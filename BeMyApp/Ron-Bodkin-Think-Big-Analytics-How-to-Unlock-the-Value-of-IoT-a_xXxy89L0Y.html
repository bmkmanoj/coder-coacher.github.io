<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ron Bodkin, Think Big Analytics - How to Unlock the Value of IoT | Coder Coacher - Coaching Coders</title><meta content="Ron Bodkin, Think Big Analytics - How to Unlock the Value of IoT - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ron Bodkin, Think Big Analytics - How to Unlock the Value of IoT</b></h2><h5 class="post__date">2016-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a_xXxy89L0Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um thank you everyone for joining me
today I'm Ron bodkin and I have been
working with Hadoop for almost ten years
now and and a little bit about my
background I'm the founder and president
of think big I'll tell you more about
think big later on but the summary is we
started the company a little over six
years ago always with the focus on
helping enterprises get value from Big
Data Hadoop and related technologies and
we've been working with a range of
companies whether it be in manufacturing
insurance retail banking media
advertising healthcare and beyond and
one thing that always struck me what
right when we got started working and
think big helping enterprises use they
did it was how much interest there was
an Internet of Things data that from
very early on we're working with
companies like network appliance and
helping them manage the data they were
collecting from devices that were
installed out and distributed locations
around the world so today I want to
share with me some of my experience in
our experience in managing data to get
real value out of Internet of Things so
with that I think we've all heard a lot
of discussion around the various B's a
big data volume variety of velocity but
one of the biggest things you know the
thing that I think is the most important
ultimately about Paquette is really the
right data the fact that you have
complex data sets with structure that's
not so simple off of varying more
frequently it can be awkward to work
with using more standard tools and
techniques so so much of the data we
think of in in order things data fits
that pattern its sensors audio video
location data images logs right so that
while size has a some bearing it's not
that's most important it's really the
variety that makes is the hallmark of
big data and why i OT data management is
so important you look at some of the
most important data sets that are unique
to io t our force time sir
data so things like sensors are meeting
or capacity data about devices products
components events dreams look not on
this is often thought of as logs but
logs will they represent one way of
capturing information about a stream of
events those can be events that are
external to a device that it observes or
records like acceleration or they could
be internal you know I OT is very much
about smart connected products that are
on the internet so the software activity
of those products alerts when they run
into problems conditions those are
really important and then there's
relational classical structure and
beyond what we think of is more more
graphical things like bill of materials
and configuration and components and how
things need to relate so those are all
important data sets when we think about
the Internet of Things now some of the
use cases there's so many use cases but
some of the ones that come up the most
often when people look at internet
things are predictive maintenance can
you use data about products that are
deployed to do a better job of
predicting failures and avoiding those
by repairing replacing updating before
there's the failure condition you know
can you make it easier to do
investigation and resolution of an issue
on the fly when something's going wrong
can you quickly search and look up and
drill down and see detail about a
complex system part in that system and
the symptoms that are creating issues
can you you know avoid the false
positives in fine of the storm of alerts
that are so common in Internet of Things
which ones are critical can you do root
cause analysis to see what's wrong even
more proactively can you do the I type
scenarios and saying well what are
anomalies are outliers or cases where
Internet of Things data tells us
something about parts of our system that
are not working well features that need
more more requirement for you is even
just understanding frequency of use to
say we ought to invest more in this part
and understand what people are doing
better because it's very popular right
so connected products give us direct
insight and how customers are you
products instead of the traditional
world of speculation around how things
are being applied so with that let's
talk a little about the changing
technology landscape in data management
you know one of the big trends that
Hadoop is right in the forefront dub is
the concept of a data lake you know
collection of long-term data containers
one or more Hadoop clusters is a common
way of implementing this of course that
capture refine and explore any kind of
raw dated scale and can feed downstream
facilities and then really the key here
is remember when I said that big data is
hallmark is variety well the reason why
data lakes are interesting is because
it's a kind of Goldilocks governance and
kind of putting enough structure enough
organization into having captured it
you're confident have complete data and
you maintain the data even as the
structures change over time and you know
something about how they were created
without all the complexity of trying to
parse and organize and fully curate the
data which is overwhelming given the
increasing amount of varieties so you
know we'll talk more about that an
example use case but the concept of data
lake is is high-level part of the data
management but I won't unpack it and
talk to you in a little more depth
around what are some of the real
patterns was it architecture we think
about collecting IOT data sources
through a variety of means streaming
data where you will have continuous
flows and message queues is a variant to
that but also there's still a lot of
data that comes into Data lakes that
comes in for management that is handled
through feeds there's lots of work of
course in preparing making consumable
data and being able to have enough
governance to know things like data is
complete and secured as well as doing
some amount of organizing data for
efficient access and downstream use from
data products so that's a high-level
around some of the architectural
elements of course we could do a talk on
this all on its self another way of
looking at it is when you think about a
reference information architecture
there's a lot of phases that go in from
data that's as it's acquired to
prepared and ultimately published and
not only is the relational data our
traditional structured data involved in
this that so is the more complex data
sets that flow in in an IOT all right so
you have data scientists that will work
across all the layers even with fairly
raw data that's just been acquired and
not structured data modelers that pic
subsets of that data and prepare it and
publish it out for broader access with
more guarantees about it for the
business to access business analysts and
then of course I t needs to have a role
in all of this so this is a way of
thinking a little bit about again high
level how does one manage the data but
will get a little bit deeper and talk
about some specific patterns you know so
there's a lot of principles around how
to manage data correctly and hit do you
know we see it's critical that since so
often an IOT deployment requires you to
have hundreds or even thousands of feeds
for the different tables and different
sources the different kinds of sensors
of producing data you need to have good
ability to secure data were insensitive
to be able to have metadata and lineage
to be efficient in how you store utilize
the data and to have a real sense of
being able to validate clean and profile
the data so that all the users we talked
about can understand and be confident
the data and make it easy to extend you
know big part of having a data lake is
being able to be agile and make it easy
when analysts when data scientists want
to extend and make more sense of
something in the data they can they can
do it on their own and doesn't require
an IT project to explore so some of the
changes are as you probably heard about
the concept of schema on read write and
that is an important concept but of
course data still has an underlying
structure so schema on read is often
really more of a short form for saying
you don't need to do all the modeling
upfront to reflect that structure
instead you could just expose as much as
is helpful for your problem and
sometimes you can even do approximate
things like instead of getting parsing
and something
back you can write a quick reg acts been
get and ninety-five percent accurate
analysis it gives you a flavor for the
data without having to do all the work
to write a perfect parser similarly I
you've got this nice ability to have
schemas that are more loosely coupled
right that gives you more flexibility in
your applications so changes that happen
upstream can be cooked with downstream
later on but you know the idea of data
modeling isn't dead that what you see is
as i showed in the reference information
architecture once you have data that's
highly valuable and going to be used for
a range of purposes you do want to model
it carefully and be accurate so that the
broadest class of people can be i can
leverage it you know and of course we're
also seeing some transference of ideas
many of the good ideas of big data
systems are being embraced by more
traditional relational systems you know
things like becoming more elastic and
working with structures like JSON to
make it easier to have complex structure
in those environments as well you know
in terms of logical modeling some of the
big changes it certainly affects peyote
or things like having Jason like
structures when the ability have nested
structure with relations arrays and maps
that reflect a lot more of the kind of
complexities of real-world problems
graphs another one having complex and
dynamically changing relationships
needless to say this is a very natural
structure to use for things like
assemblies and deployments of composite
items whether it be assemblies in an
automobile or whether it be in a
manufacturing environment or a
technology deployment of clusters of
devices and then finally working with
binary data large object specialized
data so much peyote data is of this form
that it's a complex format that's
traditionally then you have a specific
specialized program that interprets and
process is that being able to work with
and have some first-order representation
for that rather than saying it's
something that somehow outside the
system is it big change physically you
know there's a lot more options in
Hadoop
how you actually lay data out on disk
and at the end of the day when you write
data on this you are doing making some
modeling choices right how are you going
to distribute the data things like key
design or sharding distribution file
formats you know what computational
algorithms are going to work against the
data and while it would be great if
every compute engine works equally well
against every data format we know that
today that's not the case right so the
other thing is when you've got all this
complexity around how one can organize
data maintaining the integrity of the
data is really an application concern
you don't have an a Hadoop system the
concept of a foreign key like in a
relational system so if you want to
maintain consistency between two views
or two and related concepts it's up to
your applications to do it now because
there tends to be a big ship you have a
lot of storage available you tend to do
a lot more denormalization and creating
materialized views duplicate copies of
data to make access more efficient
summarizing data can be orders of
magnitude more powerful you know index
lookups are increasingly costly but
basically we think about it the time the
amount of data that you could read
sequentially is increasing with a
density of disks but you know the time
to do disc seek on a spinning disk is
not getting much faster so doing random
access needs is increasingly costly in a
big data system so you try to avoid that
I'm filing distributed systems you know
have more complexity eventual
consistency and reconciliation demands
right so even within a single cluster
that's the case and for DCP you know for
disaster recovery centers we have
multiple clusters this especially
important so with that let's talk about
some of the patterns for managing data
that apply in the IOT space specifically
there's a lot of new patterns that we
see and how one works with data in
Hadoop denormalize in facts profiles
we're really going to focus on the four
highlighted ones event history timeline
assembly distributed sources in late
right so these are all important
patterns that we see often coming up in
Internet of Things deployments that one
wants to work with data so the event
history is a very basic one it's the
idea to have a kind of fact table about
common events that can allow things like
analyzing consumer behavior across
multiple devices and a context that
could be from a wearable device or
telematics and combined with you know
access from a mobile device right so
it's a common scenario we typically see
this is being stored in a columnar
format for efficiency reasons so Park a
dark file or two popular ones in Hindu
depends a little bit on the distribution
you're using and the specific compute
engines that you're using the way this
generally functions is you join the as
was value of slowly changing dimensions
to the raw data so in our example we
might have an event one two three and an
actor idea which might be the device and
a timestamp but then we have non-league
columns about the event but we'll have
dimensions things like you know what
information like you know what was the
configuration of the vehicle what was it
what's the age you know who's the the
related account associated with that
vehicle and then another key element of
event history is having a bullet have
extension columns of data that are not
parsed or model right so kind of a JSON
like data blog that could be actual JSON
it could be stored in terms of a H
catalog description of a array of map a
map of array items or otherwise and
typically event history is partitioned
by event time buckets you know like
hours or days or five minute intervals
and can also be partitioned your bucket
and other dimensions as well right so if
you wanted to for example do analysis on
on devices or vehicles I know that you
might bucket it on
the device ID of the vehicle so if you
were going to be doing analysis you
wanted to be able to to do analysis on a
certain specific vehicle you could do it
faster if you had something else like
accounts where court you know a fleet of
corporate customers it might be very
much more efficient to be able to you
know bucket or subpartition on the
owning account so that if you were doing
analysis doing some data science work on
events you could limit your analysis to
one twentieth of the data if you had 20
buckets or sub partitions where the
specific customer who's dating we're
looking at was in only one of them right
so that's this is a pretty foundational
thing but it has again the notions of
denormalization efficient storage of not
only the basic event data for the
matching dimensional data so it's not
like a star schema if you know data
warehousing and it also maintains the ID
it's the foreign key values of the
dimensions as well as their translated
values which is very helpful when you
think about if you want to go back and
and repair errors for example if there's
ever a late-arriving change to a
dimensional value for example so that's
a pretty foundational one the concept of
event history and one that we see a lot
of deployments of Duke systems
definitely in the IOT space another one
that's also real important is what we
like to call timeline if you think about
the facebook timeline as a place where
you have a series of related activity
around an actor in an individual
timeline is like that instead of giving
you a set of events over time without an
actor scattered instead it's somehow you
have for a given actor a given device or
a given component all the events
associated with it over time you know so
that could be the history of the device
that could be interactions of it the
actor work consumer can be customer
journey
a lets you really support analysis or do
support lookups on specific items
sometimes it's be a hierarchical right
if you have a if you have a overall
device and components and sub components
you may organize the actors with a
hierarchical key so you can scan over an
entire cluster of items and look at
their behavior look at what's been going
on with them and there's there's really
some variety of approaches within that
how do you organize the data you might
have things like dat about the actor as
a whole like you know what segments are
they in or the active or they you know
at risk you know for some condition have
they just mean or they do they have
certain characteristics like they're
there in the midst of an upgrade you
then we'll have typically a lot of
different events so you'll have you know
an idea about the events some time stamp
you know fact data about the events and
again just as with event history you can
have other dimensions associated with
the event even extension data that's
unique to the event to make it easy to
extend the structure without having to
create new columns for everything this
is again typically stored in a kilometer
format and can be partitioned there's
some variations and how one lays out
timeline depending on whether you're
looking for more of a near real-time
system where updates can be done in
milliseconds or whether batches
sufficient where you're doing updates
every hour every few hours every day
maybe every 15 minutes and then another
another variant for timeline is it can
hold cash algorithm values you know if
you have a machine Lord model to say
what's on what's what's the way are on
this device and what's the risk of it
malfunctioning you know as an event
comes in to look this up and immediately
compare that to that model and
potentially update it and they're real
time as well so timelines can be used to
hold cashed out words and values this
can be a bit they can be stored in HBase
for more real-time purposes whereas if
the focus is more large
go batch computation they'll be often
stored in HDFS and made available
through H catalog two engines like
Hybris Park sequel another pattern is
the concept of event analytics so that
that is being used for things like
propensity or segmentation right so
doing actually this is this is within
the pattern sorry of timeline doing
analytics on it you can do things like
in real time you can do analytics on
timelines right so what that means is if
you want to if you want to look up and
say well let me do the app and update
you can see if this thing is likely to
be in a critical condition it needs
preventive maintenance you can look at
in real time all that happened or you
can have a batch model where you look at
all even history and kind of take the
next incremental set of events and
update the model scores for all of the
items that are in history right so
typically this gets trained from
Timeline view right so you build the
models by looking at per event data or
per device data for component or
assembly data to see which one one of
the characteristics of devices that have
problems for example fail and build a
machine learn model on those another
example is attribution right where you
want to say what's the impact of a past
event on a new event you know in in IOT
this can be things like you know what
how important is a given failure in
predicting the likelihood of a device
breaking right so this is natural to
apply a timeline view and then for
reporting an exploration there's a
couple of options one is being able to
where the amount of event data in your
Timeline view is small enough and it's
easy to use a sequel engine you can
simply use a sequence in like a presto
or Impala on top of to do but where it's
a large amount of data using deep
aggregates can be very efficient
is a different pattern that we're not
going to go into but being able to
pre-compute massive scale aggregates and
then put their missile in a cage face to
make it easy to access them we've built
a technology called dashboard engine but
there's a number of approaches to doing
that kind of deep aggregation then
finally discovery being able to do in
that thing and look up of events which
is a natural offshoot of what you've got
a timeline being able to use a and a
search technology like elastic search
you can put the key of the item in and
have pointers off to the timeline you
know this is especially powerful when
you want to have many attributes your
indexing on you just have a primary key
timeline is typically easy to use but if
you want to be able to look up based on
you know what component is in a larger
assembly you know it's helpful to be
able to have that kind of indexing for
discovery as well as you know pure data
science discovery level to do things
like spark analysis on these events so
there's a lot of different kinds of
analysis for IOT that these basic
structures support you know there's some
of the some of the key things in the
event data management in some cases
merging identities is important by
discovering that a given mobile phone
and is connected with the owner of a
personal fitness device you know that
requires some work in terms of how do
you stitch together the identities to
put them into an assembly and you have a
part that that gets pulled out of one
device put into another one what happens
there with identities archival
exploration of course governments right
so there's a lot that goes into managing
these events so we don't we're not going
so deep into each of the patterns is to
or through each of those but certainly
there's a lot to getting these right
another really important pattern is a
concept of the network right so when you
think about having ongoing status and
configuration like parts and ease of an
assembly or related items and versions
or software configurations within a
deployment which you know often it's not
just physical devices and IOT and how
they work together that that is critical
to understand but it's also one of the
software in the interactions of software
in those devices and within those
devices that are important all right so
graph databases are really compelling in
terms of making it easy Tabby's at large
scale to do traversals and do you know
complex analytics unwraps like
transitive closure you know things like
what are one what are all the parts and
downstream assembled components that
came from this batch of a production
batch right so you can do analysis
traceability to see you know if there
was a problem in manufacturing the badge
what are all the devices effective right
for impact analysis there are some
different technologies you know in the
graph world there's a number of
technologies like spark graphics that
make it possible to graph analysis on
more general purpose storage formats
like timeline at event history but then
there's also you know technologies that
are more specialized to do processing
both real-time in a batch very
efficiently in a graph database neo4j is
a very mature version of that that's
open source but neo4j has limitation
that it doesn't support graphs that need
to be in a cluster across multiple
devices it's not a hadoop technology
instead it allows sharni but only to
scale reads whereas type of DB is a
graph technology that's built on top of
I'm HBase or Cassandra so can be run on
a Duke and can be accessed and it scales
but it's less mature so there's some
different technologies but network is an
important pattern for IOT as well you
know in a common thing to think about
and the IOT world is you're dealing with
a with distributed sources of data you
don't have simple all-or-nothing feeds
right so it's very common if you've got
hundreds or thousands of devices that
you're collecting data from that you're
going to have some that are offline
networks aren't available for period of
time especially if they're mobile so you
need to know do you have all the data
right after the data
and to be able to have some confidence
you know when you're doing analysis do
you have enough enough data and what do
you do about incomplete data alright so
there's a lot that goes into that you
know control totals heartbeats
watermarking metadata so you understand
lineage and you can make decisions about
do I want to reprocess data right so a
lot of the complexity of ingesting IOT
data comes from this is distributed
sources of data and it's really
important to know if you have all the
data and to build systems that when the
unexpected happens and when you have
delays and failure conditions you had to
look gracefully and you don't have a
Code Red emergency that your system
mounts and nobody can understand what's
going wrong alright so climate a key
part of that is the notion of linked
data right data can be late because
fundamentally IOT devices will go
offline they want to power the network's
not working they get disconnected as
well as due to failures in in system
infrastructure over the land when
servers being down so metadata is
important you can do things around
watermarking so you have a sense of
what's the typical distribution of
delays and at what point are you highly
confident you're going to have
ninety-nine point nine nine percent of
the data in an IOT scenario where you
have devices to come offline you're
you're not going to be able to wait for
all the data in most cases right I'm in
if you get if you have one and ten
thousand devices that will emit data a
week late you typically don't want to
wait a week to do analysis on your data
right so you have to also think about
how do you handle it when you have truly
late arriving data what do you do is we
statements how would you reprocess them
can you report on how much did is late
you know typically there's some notion
pocketing on event time having a
secondary bucketing on delay epoch right
so if you use sometimes we'll have eager
reporting or did you make it do
something criminal reporting on hey
here's all the data we have now but we
know that that enough data will arrive
in the next 24 hours that will do a
slight Restatement between the first
estimate of what's going on
you know what's our current state of
health the devices in the field and then
okay the next day we now close the books
on pretty good data we're only a few
outliers will come in late so those are
a few of the patterns that we see
hopefully it gives a little bit more of
a feel of some of the data management
questions that come up in Hadoop with
IOT with that let's just talk through a
case study of putting this together so
in this case as a customer that we've
been working with first several years
now global manufacturer storage devices
they've produced hundreds of millions of
devices in here each containing multiple
complex components and they're
manufactured across locations around
mostly Asia some components are not
manufactured in-house but come from
suppliers and there's up to a gigabyte
of data generated during the life cycle
of testing you know manufacturing
testing deployment in the field ultimate
analysis if process problems occur phone
home and data around those devices so
some of the challenges that were there
when we started working on this problem
where there's a lot of time wasted
playing where's waldo with the data
trying to track down for engineers
what's going on this failure you know
kind of get to the data set of the tests
that happen when this thing fails in the
field and understand you know this could
be pretty significant right of you and
you have a customer says if you can't
explain to me why this particular device
failed I'm going to return a
multi-million dollar a lot of items in
the suit they're all bad that puts a lot
of pressure on engineers to track down
root cause of why to the one fit right
it's feeding time to market so the
faster you can figure out what things
aren't working and developing the
product the faster you can ship its
scale and then a high-tech manufacturing
arena the ability to ship a product a
month early is incredibly important from
for gaining market share and fur you
know amortize thing the investment in
building that product where product life
spans are typically measured in quarters
and not years which certainly decades
and customers of course are demanding
faster failure analysis wanting to
understand if things aren't working why
and in fact we're possible being
proactive and avoiding things failing in
the first place so technically
challenges around this were you know
having silos lots of data different
manufacturing facilities that were put
together a difficulty storing and
exposing the binary and other data types
that were being used in different stages
not having a platform for doing
analytics so many organizations find
themselves this way with IOT data where
they have the ability now that collected
and transmitted but they don't have
anywhere to store it and akiko I Nordic
analyze it right they don't have the
data volumes the data volumes are
exceeding the traditional data warehouse
and the first version was often only
keep do a small amount of data that we
believe will be critical and keep it for
a limited amount of time and put things
out to tape where it doesn't ever really
get used except for the most dire
circumstances so one of the key things
is that we've been able to do is expose
the entire DNA of these devices for
development and manufacturing and right
liability questing all the way through
to behavior in the field to increase the
operational efficiency and quality of
these devices in order to do that we
used a Hadoop deployment Cloudera
distribution on Amazon Web Services and
fed and data from a variety of sources
shopfloor data from multiple sites
assembly data configuration data data
from suppliers shipment data returns
data downstream testing data on devices
in the field into a platform where again
has been sawed the data stored in a raw
form and add a delay and then it's
refined so that I end and integrated
views of the entire timeline of the
device are available to make it much
easier to do analysis and then this is
fed into a variety of downstream
analytic platforms ad-hoc analysis your
failure analysis to say when something
breaks make it easy to pull the right
data to do the investigation back
screens so that models are built and
continually improved to say when things
why do they fail you know end and
traceability applications and downstream
you know summer is being fed into the
data warehouse as the data gets refined
to right answer a variety about how
questions so this is the overall
deployment that we have created working
with the customer in one of the key use
cases was the binary data and so that
there's a lots of challenges around
decoding this data the data needs to be
kept for five years for warranty reasons
there's petabytes of binary data that's
therefore needs to be retained and made
online and it's a classic example of the
need for scheme on reading the
development and process engineering
teams here will frequently change the
manufacturing and test data so and they
don't typically think about making sure
that when they're making a change is
they're rushing and trying to improve
something in engineering how do they
make sure that downstream and future
analysis will work well so the decoding
will change and it typically can't be
done it won't work to break the system
if something isn't in the right format
you expect it it's very hard on
traditional DBMS where you would be
finding errors and needing to
continuously purge data and retry and
run rather than being able to parse data
on the fly after storing in a raw format
and that's a big advantage here of the
ability to parse the other thing that's
its motor bull was with five years of
legacy data in Hadoop we were able to
use streaming to take in existing code
such as in C++ and Python the will parse
data so be able to integrate existing
parsers into the system untouched you
know with an adapter framework another
important use case is wide structures
there can be tens of thousands of
parameters that are collected over a few
months for single device so having that
kind of denormalized structure which
you'll recognize as an instance of the
timeline pattern right having that's
wide structure with all the data about a
given device making it much easier for
any
analysis and then the third is really
the ability to have massive compute to
do a parallel analysis right so you know
doing investigation of a particular
issue you can see plots of devices were
engineering teams can hundreds of
billions of test points across about 10
million devices and then the outliers
can be plotted which allowed in drilling
down and what was wrong this is a kind
of thing that would be incredibly hard
to do in a traditional system where you
could you painstakingly investigate and
look at one device at a time and
couldn't harness the power of a massive
compute cluster to do analysis and you
know the last thing that we've been
excited by is having this data and Hindu
has allowed us to hackathons of the
customer where people from various
different teams in the company are able
to participate a prototype derived as it
gets raw data and then the ideas that
are working well can then be made more
efficient so back society of agile
modeling hey here's some data that we
want to expose in a more standard form
it's worth it there's value here will
parse it out we'll take it out of the
extension columns and put them in as a
parsed first order column or maybe even
a separate materialized view for
efficient used in the downstream
application so you know a couple of
conclusions one is there's some value in
in using probabilistic data structures
to optimize right so that's a ballin
technique we didn't talk about but comes
up often you know NASA scale especially
if you want to do things like count
unique devices that might that the that
are experiencing some condition or
happens like filters to say do I even
need to look at this partition at all or
can I you know basically impacts that
and avoid you know looking at it if I'm
doing analysis on reracked events and
also there's a lot of anti-pattern so
what do you not want to do when you're
working with data IOT data in Hadoop
well you don't
treat hadoop like a giant database right
you don't want to put your data in third
normal form with lots of joins required
for all analysis or even you know more
traditional data Mart forms like star
schemas and snowflake schemas that again
so require a lot of joining and there
are some cases where you know later if
the dimensional tables are small enough
that this can be a calvin memory but
begin index lookups are slow partitioned
reads of denormalized at work a lot
better so you know the classic
relational approach does not work well
over time I think we're going to see
more of a hybrid where you'll see more
of these techniques being supported
whether with solid state drives which
still tend to be cost-prohibitive in
Hadoop environments especially at large
scale you know better optimizers you
know updates the platform you know
there's a lot of desire to make these
work better one thing a lot of people
say that I don't believe is well look
won't we just be able to cash all this
data in memory and if you look at the
growth of data even just from cell
phones you know the pace that device
data such as from cell phones is
increasing as far faster than the
density of memory in other words the
working with IOT data is not going to be
a good it's not a good bet to say oh
well just using memory techniques you're
going to have to have ways and storing
it on disk and being able to comb
through that disk efficiently and that's
why fundamental a traditional data
warehouse patterns laying out data is
not as an anti-pattern in the hadoop
world you know again before we get to
questions quick summary of us I think
big as I told you we are a big data
services company we help customers
through the life cycle of working with
big data putting together architectures
and roadmaps building data links with
not just great consulting but we put a
lot of frameworks that codify the
patterns that we've seen over and over
again to speed time to delivery doing
analytics and data science I didn't talk
a lot about the analytics but in the IMT
world so often the N willux on the data
is initially just being able to get your
arms around what's going
they're being able to look up a device
being able to do a quick search be able
to run reports over this dark and the
behavioral data of devices is deployed
but then moving on to data science
building predictive models identifying
outliers anomalies dealing with things
like predictive failure is incredibly
powerful and we help customers with that
as well as managed services training and
support so you know we work with a lot
of technology is obviously do a lot with
Hadoop spark is incredibly important
coffee &amp;amp; H face is very important in a
lot of the real-time use cases within
the new government and we see presto as
well as Impala and others is often being
really valuable query entrance on top of
the big data stack so that's a little
bit about us and what we do and then
finally you know I'll just conclude with
you know so much of IOT is about
blending data together and having the
right ways the patterns and practices
for managing it is foundational that
lead to effective analytics and with
that we've got just about 15 minutes
left here and I will turn it over to you
for questions so I see so far there was
a question can we have access to the
slide that can absolutely um I will I
would be happy if you if you want to get
a copy of the slides you can send me an
email here at this email address on that
bod can't think big analytics com you
just send me an email and say can I
please get a copy of the slides you know
I'll point you to a location to download
them all right that was one question any
other questions so the
of the presentation is awesome can you
ask to get an example on how retail
banking leverage should you can practice
like fraud detection digital media media
credit rating um the good question
there's a lot of things we see people
doing in retail banking on you know some
of the the common ones are around
understanding consumer behavior so being
able to look at things like how do i put
together data about what a consumers
doing on mobile what they're doing on
web what they're doing in this in the
branch what they're doing in the call
center to do things like better
understand what their interests are you
know what kind of upsell cross-sell
opportunities present to them
personalization as well as turn right
confining places where customers are at
risk of leaving or taking some of their
business away to be able to respond to
them more effectively that's common
fraud and money laundering and security
or awesome important use cases in the
retail banking sector and so those are
all ones that we often see so that was
the one question there's plenty of other
use cases just a few that jump to mind
that we've worked on um the question is
why would someone choose amazon services
such as Amazon s3 over Hadoop and amazon
kinesis / Kafka and you know that's a
complex question I'd like a lot of
architectural questions there's
trade-offs you know when we see people
running on Amazon we often actually help
customers run the duke on amazon right
and when we they do we are seeing a lot
more customers use the commercial
distributions clatter important works
rather than Amazon Elastic MapReduce
because when you have a long-lived
cluster there's a lot more services in
the commercial distributions you know
integration of things like spark
although there's still a real question
that the center of gravity of many
Amazon deployments is Amazon s3 and so
HDFS becomes more of a caching layer for
data that's going to sit in there for
medium term but the long-term place for
data gets synced and stored as s3 so it
is a big difference
uh between you know using a data legwear
HDFS is fundamentally the long-term
storage and an Amazon was stiff more
typically s3 now can these this versus
cough is an interesting one you know
we've seen a lot more deployments of
Kafka just because kinesis is really
compelling of everything you're doing is
on amazon it's really easy to to stream
data from servers that are doing your
event surveying or your IOT device data
on amazon and connect it to kind of
seamlessly through amazon's
infrastructure over Kinesis to
downstream analytics applications on the
other hand if you've got needs to feed
in data from non amazon environments
Kafka has a lot more integration there's
a lot more ways of collecting it in
Causton and working with and of course
it's much more portable right that's
another thing two big differences a lot
of amazon services or proprietary and
we're only on amazon whereas when you go
with more open standards like you know
patchy to do the patchy Kafka it's more
portable and works in different
environments the same time you know
credit to Amazon they make a lot of
their services work really well and
compelling and they integrate together
very nicely so DC companies saying we'll
look where we may not want to be on
Amazon forever but if we built something
quickly and it works well in Amazon you
know we're willing to have that trade
off and they have to rewrite some things
we want to move on and the next question
I God is can can I speak a bit about the
disaster recovery and security of the
big data data sets and that's a good
question I mean so there's a couple of
elements you know in terms of disaster
recovery um the way that we will
typically help customers there's kind of
two philosophies for how you have
disaster recovery you know the typical
one is to say well look I want to in
order to have ensure the data doesn't
get lost I want to have another cluster
another online system whether where my
Hadoop data's being backed up all right
so it's basically the most common
approach is to have more than one Hadoop
cluster and more than one data center
that's in production or
one will be a standby so you can either
have active active they're both active
for active standby where you have a
disaster recovery cluster that has all
the same data you can do that either by
feeding all ingesting the data into both
clusters simultaneously and doing the
processing or simply replicating you're
copying data on intervals from the
primary cluster into the backup or
standby cluster sometimes companies will
have a standby cluster made available
for other purposes like ad hoc analysis
data science depends a bit on your
service level agreement and whether you
want true business continuity where
there's very short windows where
something can go down versus the ability
to recover at all the other approach
that people will use for disaster
recovery is some form of more cold
storage of data that could mean back in
your date up to Amazon s3 and only
spitting up a cluster um that's needed
to process the data when it's in s3
rather than having a standby cluster the
title or being used for another purpose
or you know in other cases we do see
companies think well maybe we will have
some kind of a filer or a even a tape
based backup but you know filers and
take based backups tend to be a little
bit tricky to make work well on in a
Duke scale in terms of security that's
also an important consideration right so
there's an insecurity and Hadoop is
definitely something where you want to
have defense and depth you want to have
you know the classic deployments were to
dupe in the early days was that Hadoop
was a trusted environment and you
limited access to a few people to
working within the environment and had
gateways to access the data in more
secure right so you could run a high of
query for example a sequel query or
submit jobs through a gateway that check
your permissions and nobody else got on
the inside today we don't see a lot of
companies deploying it that way we see
them instead having defense in depth
where they will use kerberos so you have
strong a fennec ation so that even if
someone gets into um inside a cluster or
running on an edge node running
arbitrary code they can't do more than
they have permissions for and we're
seeing you know rapid improvement in
services like clutters record services
approach or Hortonworks ranger that
provide more fine-grained security
primitives to for access to data it is a
bit trickier in the new world because
you have this flexibility of so many
different engines that can access data
that it's not enough for example say
well I have secured how high reads data
and Hindu what if somebody downloads and
installs Apache flink all right and now
they're using a whole new streaming
engine on your cluster you haven't
secured then all right so social
security has to take that into
consideration than anything that the
data itself needs to be secured either
through a gateway or through things like
encryption and key management that don't
allow you to access data if you're not
entitled so that's a little bit about
security of course it's a worth a whole
webinar on its own security in a dupe so
that's high level next question was
thoughts about schema evolution and I
think you know those are the key part of
the patterns in I talked about in data
management are you do have scheme is
evolving rapidly right so the ability to
record data in a raw form time enough
metadata to know something like what
version did it come from so that's easy
to know you can associate this game ours
you discover later on a schema change
you can understand that you know there's
certainly when you have parts of the
model that you do need to parse out rely
on when the schema changes there will be
cases where you have to recompute
reparse data if you didn't get that that
schema change right so you know so
there's an art a lot of the value of
these patterns is it facilitates scheme
evolution and the agile approach to
working with data for IOT needs you
don't try to you're not as vulnerable
because only the critical data sets that
you know fields and values that change
that you're truly dependent on are going
to
drive more work when they change instead
of everything um when there's a
follow-on why's it not easy to maintain
scheme evolution schema evolution um
well it just depends on the pattern to
you it's right to challenge your own
scheme evolutions we have a system that
that tries that depends on exact schemas
in a given form often upstream things
change and it breaks applications it
creates you know complexity of
reprocessing or even worse you know
silent errors things that are incorrect
and not caught right um next question is
which of the design patterns will work
for smart metering applications if the
goal is to build a kind of recommender
system for the customer right and so for
that I would say you know the smart
metering applications again what you're
seeing is a whole stream of the events
associated with a customer so your your
recommender system is going to have two
parts to it it's going to have a part
where you have the history around the
metering but you're also going to want
to associate the meter events with data
about the customer right so what's their
account what's their you know their
history of payment so you kind of know
what the historical bill and usage is at
a high level so you can train a model to
do recommendations offline on all that
data and then you know as events come
into the metering they can it condensed
scored so if you start to see changes in
the behavior you can say this is a good
example you might have a need for
something like fashion ization where
it's not a single meter event but it's
the pattern over the last say ten
minutes to say hey this is ramping up in
this way it's got the signature it's
likely to mean the following event
that's happening and we have a
recommendation for how you know if
you're doing the following thing it's
inefficient here's a way that you could
for example save energy right so you
would definitely would have um you know
timeline which would be for the given
customer what's this time series of
metering events as well as event history
that's used for training
models and scoring them offline that you
can do no time updates the models and
timeline to make the recommendations
question is what tools do you use for
testing pipeline or monitoring um and
you know so we actually have built some
of our own frameworks and and our work
with a variety of different tools you
know back in enterprise schedulers
Apache IFI and beyond that make it
easier to put together pipelines and
capture the metadata in each stage and
monitor them all right so to be able to
see on the one side you know at a higher
level than a job but for a feed or
regular process what's its health is it
working and even at the other side for a
data stored to say you know what is the
data up-to-date are we meeting our
service level agreement or is its tail
and we need to in maybe investigate why
you know it turns out but there is a
fundamental fatal error the processing
code can't process this changed format
and that's resulted in a delay right so
we have something we call pipeline
controller that does that that we built
because there's pieces in open source
but putting it together is something
that we found to be incredibly valuable
so uh I think we see all see there is a
session on security at three so it was a
good leading question my one to two
minutes on security was not really
enough to cover sir security gonna do so
you can hear a whole hour on it coming
up in about an hour um which layer and
the data pipeline should do validation I
think you know ultimately validation is
important at a number of layers right
right at the point of ingestion you want
to be able to do some amount of
profiling and detection of invalid data
now typically the pattern in the big
data system is you will segment out
invalid data put it in a separate
location it could still be accessed as
raw data but at least it's somehow
tagged known to be invalid right but
then downstream is you're doing
downstream processing you often will
have more
vance validation so so for example
detecting internet traffic that's coming
from a robot that's not typically done
right at ingestion but instead that's a
smarter machine learning algorithm to
say what's the likelihood that this
activity is in fact a bot and not a
human right you know related to that
should message transport do validation
or should the end consumer do validation
you know I'd say that in most cases I'm
a fan of having more the end consumer or
more in the cluster environment you do
the validation you kind of capture the
data with the minimum amount of changes
and modifications in a raw form because
there's always the risk of software bugs
and if you try to validate data
transform it in the transport layer
you're exposing yourself to the risk
that if there are bugs you can never
recover that data now that's not boys
feasible there's some IOT applications
where you have massive amounts of data
and you just can't afford to move it
over a small network connection you have
to do some validation and even Fanning
down so it's not like this can always be
done um next question how does one get
started on understanding or getting
trained by the Internet of Things
ecosystem or start learning on a small
scale to help with a project maybe you
know and I think um I would say when
when our teams when people join our
company we do put them on initial
projects where we work with and Internet
of Things data and they're able to get
some experience you know there's there's
some public data sets out there and you
know I don't know off the top I mean
there's things like more often the first
data sets we're we're using are things
like Twitter data or stock data we do
have some public data from airplanes in
the FAA so doing a small project as well
as you know there's obviously a lot of
online courses as well and then we got a
one minute left um and I will
send the slide so if or again if you
would like send me uh let's see I'll
figure out how to share the slides I'll
get them to Francis but if you want the
slides also feel free to send a note and
again you know feel free to reach out to
me if you want the slides or want to be
in touch tweets you're welcome I hope
you enjoyed the talk today and thank you
for giving me an hour of your time I
enjoyed getting a chance to share with
you and the great questions you ask that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>