<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intel® RealSense™ Technical Q&amp;A Webinar #4 | Coder Coacher - Coaching Coders</title><meta content="Intel® RealSense™ Technical Q&amp;A Webinar #4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Intel® RealSense™ Technical Q&amp;A Webinar #4</b></h2><h5 class="post__date">2014-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hc89ThiMj_4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody my name is alex from V my
apt organizing this webinar session to
help you to support you on the intel
realsense app challenge so basically I'm
here with Eric and the whole Intel team
that would be able to answer all your
question today so this is a force and
the last session we are adding for the
challenge so don't be shy guys go on ask
question ask all the questions you have
this is the final row okay so um yeah
feel free to ask a question this station
is actually going to be recorded okay we
will put it online afterwards but yes we
have we're going to go for an hour of a
presentation Erica you here with us yep
ready go Thanks I tofox welcome which is
the webinar as we mentioned that this is
a second webinar that we have done and
so some of you may be coming back from
the first Roman are we did for those of
you that are coming back welcome back
you know the you know the game plan here
you guys have questions there's a queue
a box there just go ahead and start
firing the questions as we go along and
we'll try to get though the answer to as
we proceed for those of you who is
haven't didn't come to the first webinar
loser but this is your first rabbit I
we've got a few sides of documentation
here and then from there we'll will kind
of get into the QA that will happen
inevitably so with that I think we're
going to go into this a little bit about
me if I am the the community manager the
evangelist for intel realsense in terms
of the developer outreach and I used to
be in the Navy and I have a Twitter
account I know it's very impressive
nobody has twitter but so just in case
you wonder what I look like but more
importantly what are we here to try and
accomplish as we probably heard a few
times and seen if emails like we do have
the intel realsense op challenge 2014
going on right now and we are really
eager to get some of at least some of
the developers to be able to submit some
rapid prototypes between now and
December 17th so what we want to do here
is offer these webinars we did the first
one last week and this is our second one
so to give you folks and opportunities
try and get some direct feedback some
direct answers on some of the questions
that you might have if you want to get
some a little bit extra help or running
into some roadblocks or something we
wanted to give this as an opportunity so
obviously one thing is we want to get
you guys the knowledge that you need to
be productive and be able to finish your
your app so I said you wrap the
prototypes before the December 17 s
deadline and obviously after that
continue to work on them until the final
deadline to make sure you get the best
possible app or game into the challenge
that maximizes your chances of winning
some of those cool prizes and then
speaking of cool part is the other thing
we're hoping to do is get excited right
because we do have this wonderful rapid
app prototype supplement supplement here
right so we have the ability to win
prizes all outside of the grand prizes
just by getting your apps and games in
early so that's what we're hoping to
accomplish for those of you that for
this first webinar as those of you were
in the first one this will be redundant
I apologize but what we're really trying
to talk about here is when we talk about
intel realsense we're talking about this
is being the next logical evolution in
how people will interact with computers
when you go back and look at science
fiction shows like Star Trek or Star
Wars or most any of the shows in the
last 20 30 years you don't often see
them even touching a keyboard let alone
a mouse sometimes you might have
touchscreens very often people interact
is much more naturally with computers
right you have to for example in the
Star Trek next generation some of the
others that things like the holodeck
where people can actually interact quite
directly now obviously we're not quite
as arranged the holodeck but if you look
in this degree we've gone from being
with just type words to
clicking on mice to touching screens and
now we're going to give the point where
you can actually interact directly right
where you can wave your hands in the air
like you just don't care or you do care
whichever it may be and the computer
thanks to the software you guys are
going to be developing will be able to
interact with users and so they can do
all kinds of things something as simple
as for years and years and years when
you want the computer to do something or
not do something you add up to do a pop
up and say you want me to proceed ok or
cancel right wouldn't it be great if you
could just nod or shake your head right
I mean every child in the world leaves
that I've ever met knows how to shake
their head or not their head why do we
have to have an ok or cancel right it's
just being with us not and shake your
hands would be so much more simpler and
these are the types of things we're
getting people excited about now the
first versions will be coming out on
Windows 8 1 Android will be coming out
later on in 2015 there'll be different
versions of the real sense but for the
stuff that you get and they're working
on this is the front facing user facing
cameras working on windows and we'll go
through some details here in a second
there we go just so you guys have a full
background right this whole intel
realsense effort is part of a very large
effort for us we don't just have the one
that you see here in the middle right
which is he the face the front facing
camera that you have to work out but we
also have a rear facing camera coming
down the road and even this other
technology known as intel realsense
snapshot which doesn't allow for
gestures and whatnot but does allow for
some pretty cool interesting post
processing of pictures right be able to
actually see how tall your child is
every time you take their picture though
the hold when I was a kid right every
birthday we'd stand up very tall on the
door frame and get our our height
measured now every time you take your
child's picture you know exactly how
tall he or she is in every picture so
sorry door frame those data going away
so we have a very large effort coming
out here and this first one here in the
middle is the one that we're talking
about today because that's one of the
app challenges
depend on so the other aspect to be
aware of is that when you guys are
working in your apps or your games it's
not just about getting into this
challenge right you have a wonderful
opportunity to be one of the very first
people for a few thousand people to be
very blunt and honest in the entire
world that will get access to this intel
realsense 3d camera right this is the
actual board that's the u.s. quarter
right there you can see how small the
actual board that intel has design is is
and this is a board that we're having
manufactured and sent off to all kinds
of OEMs especially the seven list below
a sous acer any cheap dell HP fujitsu
and Lenovo either all come out and
publicly agreed that they will allow but
and proudly be making intel realsense
intel realsense systems or systems with
intel realsense built into them and this
is why you can see how small the borders
so it's going to be able to fit into the
actual bezel of all ones and two and
ones and laptops and ultrabooks and
whatever else it may be so there's going
to be a very large market and you have I
just had literally or part of the very
first no more than 2,000 people to get
their hands on this these cameras so you
have a significant advantage into it
into the marketplace if if you hadn't
read the documentation yet or haven't
many would get through all of it some
just broad sweeping generalities about
the camera it works on roughly a range
of somewhere between 20 centimeters or 8
inches to about 120 centimeters or four
feet gesture is a little bit shorter
right it should be more like 20
centimeters to 60 centimeters or 8
inches to about 2 feet but still it's a
pretty good range in volume the conical
dimensions of the of the depth sensor is
55 degrees horizontally in 70 degrees
left to right some other details right
the RGB part of the camera is a full 280
sorry 1080p RGB camera it does require a
full USB 3 interface and you do need the
4th generation Intel Core processor
which was code-named Haswell
while running 81 so we're getting some
questions coming in and it looks like
Colleen is doing a pretty good job of
answering them Colleen or magna if you
guys get a question that needs further
discussion feel free to chime in and
bring them up here so specifically what
we're expecting people to be able to
accomplish with the intel realsense
software development kit or sdk if you
really think there's four main kinds of
interactions right your hands your face
your speech in your environment
specifically from a hand standpoint
there will be 22 points of articulation
per hand up to two hands they can be
left and right immediate left and left
right and write myself and Colleen can
play rock-paper-scissors against each
other if we want facial recognition will
be 78 landmark points we have a few
pictures to support that later and we're
working on trying to do things like
emotion react recognition which is as
you could imagine somewhat of a
difficult thing and it can even do
things like pulse estimation right so it
knows your heart rate sweet recognition
is built into this because it's part of
natural interacting naturally and in
terms of object scanning and things of
that nature that is still an area that
we're working on bringing to full
completion we expect it will be out in
the full SDK early in 2015 hopefully
around january or so so in terms of what
the f ck itself can do we've already
talked about the fourth generation of
core processor the window they dot one
languages we support a c++ d sharp
javascript and java we have gotten
several feedback from other people about
other languages that they'd like to
support we send that back to the
development team and they're going to
work to try and add more languages down
the road which are now that's where
we're starting with in terms of your
development environments we support most
visual studio and eclipse and also if
you look under development tools unity
which is many ways more of a development
environment anything else we also
support things like havoc in the int'l
media SEK and the entire rest of the
list that's down here
when you look at some of the other
aspects of it right we are saying that
at this point we believe that multiple
applications will be able to access data
from the cameras simultaneously if
that's a plan and you can also use
different modalities at the same time so
for example you can do things like do
both facial recognition and gesture
recognition all at the same time there
was a question that got asked from
parents here and it was are we able to
define on gestures and the answer is as
Colleen said it yes we do have the
ability to free to start to work in that
it is definitely an area that we're
going to continue to improve on it's not
a trivial thing to make your own
gestures but it is something that we
have been planning for people to be able
to support and it will be getting better
over time in terms of the areas that we
think people are most likely to be
developing for there are five main parts
here where I capture and share in terms
of be able to do all kinds of
collaboration we're not gress scanning
an object like in this case a bird why
not be able to put into a 3d modeling
tool like maybe say blender and then
maybe even print it back out using a 3d
printer in terms of greed creativity
here its ability to move backgrounds
right so you can do video calls and
whatnot and remove the background of
where you're actually at and instead of
have that background of your office or
wherever it is actually put the topic
you're talking about in this case of
patio interacting naturally is the
obvious one right be able to just nod
your head shake your head whatever
adjusters you might do hold up a number
of fingers for them close the choice
that you want to make obviously games in
one hour always going to be there and
even things like educated right so one
example might be a situation where
people a child to be reading a book and
as they read the book the the app would
actually animate the story and let's say
it was Jack and the Beanstalk and then
the child stops reading jack will just
sort of stop and start to look at them
and maybe tap his foot like you know
keep going so well we do have a slide
that I'll get to later about some of the
emotions that we were trying to work on
diego so this is tight for a little bit
we'll try to run into that real quick
the other question was do we support
non-windows and non Intel platforms
obviously we're not supporting on Intel
platforms it is the intel realsense
technology is built exclusively for our
our processors to be quite blunt it's
hidden it's designed for our processors
we have very very very good processors
and you need to have a tremendous amount
of processing power to perform on such a
large amount of data when you look at
the volume that you're dealing with in
terms of not only having of 10 full
1080p RGB but also having a depth law
and for all that stuff that is actually
from a mathematical standpoint a
tremendous amount of data that you're
crunching and quite frankly other
processors just wouldn't be able to
handle it in terms of non-windows as I
mentioned for some of the other real
senses coming down the road that will be
support for Android but yeah for right
now the ones that you guys are working
on for this app is is for Windows
Terrence is asking why the 4th
generation Intel processor I'll expand
on that a little bit what i was saying
here is that when you look at the data
volume of a 1080p RGB and then you're
adding depth data all of that data you
really need the processing power that
you have in that fourth generation
processor to be able to not only do
things like gesture recognition and
facial recognition but also on top of
that to reveal to run a full-on game app
or whatever it is you want to do if you
try to use previous generations of
processor it really didn't have the
performance needed to be able to do this
smoothly so it's that's the simplest
answer about this is that if needed
because of the processing requirements
for this kind of operation yes oh yes
so moving along I think we're keeping up
with the questions pretty well Meg
narrow Colleen if you guys needed
something let me know we also are
supporting things for for example the
web and html5 we do think that's going
to be an area that's going to be
interesting for people will say just in
cases and give particular as an example
I got company to be able to just scan
your face and tell you what what frames
that look very good on you I and do an
augmented reality thing where they
actually make it look like you're
wearing the frames that they want you to
buy and you'd be able to get a really
good sense of how well they would or
would not fit your head also unity is
very very popular we've done several
hacker labs so far then people very much
enjoyed using the unity environment to
be able to rapidly create their devices
so that is there it does require the the
full and pro pro version of unity
because there is a lot of requirements
involved but for those people that
already have that and they've certainly
been able to take advantage of that
enjoy that I'm actually going to jump
forward a little bit here because
somebody asked a question about the
emotions so I'm going to try and jump
forward to that part and then come back
to where I wasn't a presentation okay so
when we talk about facial detection
right a couple things is we can track up
to four phases at a time knowing that
that's a face and only one of those four
faces at a time can have actually the
landmarks so you can do that kind of
interactions with them we can't do
things like head orientation we use a
fancy word to pitch you on role but
really though it means you know nodding
your head shaking your head or moving it
side to side these are some of the
examples of the landmark right things
like the eyebrow the eyes which all on
the nose line lips and things like that
and then here are the emotions that
we're starting to work on initially
right anger disgust fear joy sadness and
surprise we are certainly much better at
determining whether the person is
overall positive like happy
you know joy and whatnot or versus
overall negative angry discuss it will
not so these are the areas that we're
trying to work and getting better I'm
over eight percent humans and sometimes
I can't tell what the person's emotions
are so you can imagine how hard it could
be for a computer but Diego I hope that
answers your questions if not let me
know but these are the ones that we we
have going on right now so Richard is
saying that the he feels that the non
pro version immunity works I don't want
to get an argument Richard but when I'm
the the answer that Brian put in is is
right you know in terms of some of the
DLLs that we've had we know that people
are going to run into problems so we
want to be really clear about that if
you found out how it works then great
but our experts have figured out that we
really do need that guy there any
questions that I need to jump in on I'm
trying to read them and talk all at the
same time which can be proven a little
bit taxing be the question the longer it
takes all of us so be patient to be
wrote a long question or into the short
ones first sorry yeah so I think to the
question it richer document object
tracking and unfortunately in the
version of the SDKs you guys have and
you were using for this challenge object
scanning is not part of that FD KO
between the r2 release and I like i said
this hopefully going to coming out by
late january of next year so we don't
really have those details in it right
now because that's that's an aspect that
we're still working it to improve on ah
yeah so in terms of gestures let me jump
back to some of the gestures that are
built in for the hand recognition that
this these are the skeletal enumerations
that happen right we hold your hand up
to
the real sense it will look at and try
to determine all these various points
because it is actively scanning the
hands with ir it is able to do this much
better than say chess a pure RGB kind of
situation and then in terms of some of
the hand gestures re being supported
directly things like holding your hand
out in the big five the be a sign moving
your hand in and out towards the screen
like you're trying to tap it waving at
the screen maybe to get us attention or
something like that also the various
versions of pinching right you're to
finger pinch your two fingerprints was
your the rest of your hands open like an
okay sign and then a full hand pinch
when you're making like a duckbill type
of thing and we also have things like
thumbs up thumbs down and making your
fist so those are the 10 8 10 gestures
that we have built in to it you can see
some of them more dynamic like tapping
and waving when you want to start making
your own gesture some guidances that we
offer right if you can have your hand
open things like this then that tends to
be much better when you start to put
things in front of other things like an
occlusion like you're crossing your
fingers or whatnot then that makes a
little bit more difficult because
obviously the IR can't scan anything
through it doesn't it's not actually a
vision so I can't scan through your
front finger also holding things like a
pen or a lollipop or whatever it may be
could make it confusing and also
touching your hands to your face that'll
make it confusing so little mix in two
different modalities against use it from
a two hand standpoint right you can
definitely do two hands at the same time
we've had people do this but again as we
mentioned when you start to have a lot
of overlaps a lot of occlusions that
makes it more confusing for the computer
so you should try to avoid them if you
can yeah so when you when you when you
look at how does the camera you get the
depth data it is using what they call a
coded light it is using to be very
technical about it it's using what they
call pulse
pulse compression modulation PCM which
is the same kind of technology the
military has been using for years and
its radar than or not and so it is being
able to use that data as it sends out
the coded pulses and receive the
information back and one of the issues
that we have because it is a coded type
of our pulse is that this technology
cannot be used outside because the
natural I are the sunlight will swamp
the IR sensor so that is something to
keep in mind you should be working on
these things for to be an internal type
of thing I think it feels like we're
pretty caught up on the questions Ryan
magnet Colleen any questions I need to
jump in on our show I jump back to where
I was sort of I think you would be great
okay ah yeah feel free to jump beta for
the question that we're missing out on
here so one of the things that we
definitely want to make sure you get
aware of because we're really glad that
you were able to come and attend this
webinar with us and spend some time with
which is great but obviously it's going
to last for a finite period of time
what's it's over with if you have more
questions we have one URL that we point
you to software that in telecom / real
sense once you get to that site it has
all of the information you could want
all the information that we have
regarding real sense technology for a
developer standpoint is all contained
within since one URL things like code
sample blogs links to how to actually
download the SDK if you hadn't already
update your firmware or whatever else
you need to need it's all going to be
derived from this one URL and other
thing that we do is we do go to some
shows we go some hacker labs that we've
been doing and we do have a share your
app showcase right cool here's another
important aspect once you're done with
this challenge and you've got your app
in and that's great maybe you win the
prize maybe you don't win the prize but
you can also put your app into your show
your apps showcase and that will help
give you visibility so that when real
sense systems start to come out later
this here and certainly a lot more
rolling next year you give me one of the
very first
out there to be doing the app that you
made when we talk to developers very
often it's like I can code very well I
don't need you to help help me code what
I need is better publicity better
marketing that are better awareness
because they they say that um that
that's one of the areas that they they
sometimes feel challenged out especially
when you look at very full and perhaps
even flooded markets like Android and
iPhone as I mentioned before everybody
together camera off of this challenge or
one of the very first 2,000 people in
the world to get access to these camera
so that gives you a significant hands up
I needed you ask a question about what
are the trade shows and the labs we have
the events that we have been doing we've
posted them up on that URL right and
when we have more trade shows coming up
in 2015 we don't have any more trade
shows lesson is here but for example
will likely go to things like GDC well
like we go to things look augmented
world expo and so as those events are
approved and added to our list we will
put them up on that website um what
countries are we in we have the ones
that I've attending of a u.s. there were
five in Europe and that I know there was
at least one in I believe India won in
China and one in Japan and maybe ones in
Brazil arm what is the name of the labs
of labs are in an actual lab that's not
a not a place you can go to when I say a
hacker lab I mean instead of event that
we do for a day so for example the last
one that we held was actually in Austin
it was a place called the capital
factory and at the co-working space
there and so me and some of the other
folks went over to Austin we had
training in the morning basically this
slide deck that you see here we went
through in most of the morning and then
we gave people an opportunity to work on
some code and say at the end if they
were to be able to get some good codes
they were able to win some prizes so in
a lot of ways
these this webinar this first one in the
second one is all of the meat if you are
the main content that the hack allows
have been holding we're just doing it
now on a webinar so that people that may
not be able to get to the specific
locations like pasta or orlando or you
can get most of the same content so
obviously we'll have online resources
but one other things we want to point
out is that people that are outside of
Intel have really started to take note
of the intel realsense technology and we
feel that they've been very impressed
with it pardon my french but this this
person Luke here said that in his
opinion real sense is freaking awesome
so excuse my potty mouth but so with
that I think that's a major aspect of
the webinar that we in terms of the
actual content we do definitely want to
be able to interact with your questions
oh wow colleen d speaks Spanish I'm i
just used google translate adventure
reviews for the reply so here terms okay
yeah i would like wow i have no idea
what you just said yeah so if you gonna
post your questions just please post
them only in english we don't speak
other languages no no Klingons no
Vulcans I do a little walking but you
know okay so some of the other aspects
maybe just talking about a little bit
with besides the gestures that I've
already built in and some of the
guidance is right one of the things to
keep in mind is that the the camera
itself is going to be scanning it
rustling around roughly 30 frames per
second the other aspect to keep in mind
this is very important it is very much
like when the first touch screens came
out right it was a major shift in the
developmental guides of people that made
apps to not think of any
with users from a keyboard and mouse
perspective right the more simple fat
round buttons you could put on the
screen the better they were right the
ability to put a finger on a button and
hold it down and have other interactions
come up from that was much much better
than having some sort of pull down menu
like you might have had more commonly in
the world of a mouse and a keyboard in
that same way try not to just think of
gesture of being a turning someone's
hand into a mouse right you've seen that
maybe in the very first versions of
Connect for example is you would you'd
have a situation where lead want you to
pick a choice and so they have your hand
hover over a particular menu item and
then close your hand and then as you
close your hair and a little counter we
count down to make sure you want to
select them that is a very much of a
mouse and or touch screen type of user
interaction what you might want to do
with things like if you want to give a
user oh say three different choices have
them simply hold up one two or three
fingers right that might be a different
opportunity the more you can not think
of things in the old-school mouse and
keyboard or even touch screen
perspective but actually make the
gesture is easier like I was saying
before about nodding your head or
shaking your head if you want to ask the
person that they want to do this or not
ah yes or no head shakes and knobs are
probably the simplest and most natural
way people can interact yes or so so
Richard you're asking question about
either way to add new words to the
education Oh actually I was going to say
no but Brian arm blankey jet fighter
right hand yeah i'll follow up in
writing Eric okay good because one of
the things that we will say that while
the dictation does somewhat work what we
found is a very fixed number of words
let's say something more like a hundred
or something and I actually have a slide
on this year
right a if you if you want to have a
specific number of work but let's say
you were having a cooking a cookbook a
poor somewhat that right where you know
in terms of a kitchen environment the
only so many different things you'd want
to do right you'd want to maybe turn the
page turn the page back you know a
search and haven't listened for a
particular phrase like apple pie or
something like that close close the app
things like that the more you can have
very specific commands the better that
the the computer will to do because when
you when you're dealing with in and this
is just an illogical thing from a a sort
of computational standpoint if you have
50,000 or something like that
opportunities right over 50,000 words in
English and every time somebody says the
word pineapple right it has to compare
try to estimate the word pineapple
versus all the other things that might
sound a little bit like it like
cran-apple or something I'm not me maybe
making the best example here right but
the more you can use commands that's
going to work much better anything yeah
yeah so so Wesley's asking a question
about the eye tracking and as callina
said we're not currently tracking eyes
yet per se but it's definitely one of
the requests that we've gotten from
people is a build of track eyes that
that is a logical question is definitely
being explored for down the road but at
this point we definitely can't be
supporting it and it certainly will be
ready for the app challenge you got to
working on right now one of the things
that maybe talk about a little bit in
general and go back to the gestures a
little bit is when you start looking at
having people use gestures right again
this is going to be very much like the
early early days of touchscreens right
where people weren't used to using touch
screens and
very often outside doing a great example
with with the game Cut the Rope right
where it would show you very clearly hey
drag your finger across the screen on
the rope and you will cut the rope right
and you'd have a training aspect too
many of the games because using touch
screens wasn't originally the most
interactive most intuitive because it
was new to people right in that same
kind of way when you start looking at
doing things like even facial
recognition adjust the recognition or
whatever is you should probably look to
how some aspect of training in your game
and or feedback right so for example if
you are and this is a pictures at the
bottom i right maybe have some sort of
feedback so when the person the user's
hand leave that volume right from 22 to
120 centimeters then maybe let's say the
border or the right border of the screen
in this case when you look at low would
turn red or something right so they can
have a feeling that the hand went too
far to the right it was now out of the
field of view because obviously the
upside of the IR beam is that it's not
weird color or no one's going to get
thrown off by the downside of it is
obviously infrared is not visible to
keep an eye so people can't see the
volume right it's not some mysterious
hologram area that they know that move
their hand around inside of and so
because of that they can't tell
necessarily when their hand has left the
the field of view and so if you can give
that kind of feedback to the user may be
the border turns red or something like
that then that gives them a better
interaction so being able to train the
users and gives them feedback so that
they know that they're keeping their
hands and whatnot in the in the right
area that will probably help with a lot
of things as well Eric yeah yeah do you
mind if I diskless launch the polls
right now yeah that would be good don't
know so we're gonna take a little bit of
a break make sure we can we caught up on
the questions we've got a few questions
that ox is going to run through here
that'll be poll to come off
and now it's lunch is taken away and
start wearing the poles alright thanks a
lot Eric so yeah basically guys just
taking a very short break to ask you a
few questions so yeah we have like four
questions so it's quite important for us
to know about you and to to get these
answers so if you can quickly go through
them that would be great
you want you to vote we can see how many
a voltage so we need to get the boat up
a little bit so we go to the next
question yeah basically calling if you
if you see some question you can keep
chatting I mean
interesting so almost half the people
haven't yet started under wraps for the
challenge so Joel you're online right
here Jules you want to I hate to sound
like a cheesy game shows here but you
want to tell the audience what they
could win if they injure one of the
rapid development rapid prototypes
before december 17 okay well a lot of
you guys you to get your camera you had
a flyer in there right announcing the
early demo submission bonus the bonus is
really a thousand dollar so I q's
dollars and we have a prices for at
least 80 of these right atv developers
who can submit before december 17th and
then based on the scores you know
basically making the highest scores will
select 50 of the ice cores for the
submitters on the pioneer track and 30
of the highest scores for submitter
slowly submitters on the on the
Ambassador track so so does those are a
keeper changes again it's a very low
hanging fruit you submit the a lot of
assists stated the same kind of a
submission you do for the final demo
submission deadline but submit early
with the 17 you can avail these 8080
opportunities to get a thousand dollars
each we got a point point is that you
can submit early by the 17th and then
you can continue to refine the demo for
the final submission deadline of things
around separators team next year so
battery where yeah this is basically
closed last yes basically the last
question guys so all right from here
once we're done I think every you can
you can go on with your presentation
okay thanks a lot to everybody oh wait
are we done with the last poll are we
still working on still working on it but
soon be finished
ok
but audiences you type in questions to
be heavily all right we are done with
the poor thanks a lot okay great um so
we've covered a lot of content already
and one of the area's I want to get into
is we sort of got really close to this
but I want to finish talking about a
little bit in terms of you know what you
can do with these the facial recognition
components right one of the or at least
four of the obvious ones that we think
are likely that people will start to use
is if you've ever played a game like
let's say solid snake or some of the
Splinter Cell Games where it's not just
about a first person shooter a lot of
it's about sneaking around and whatnot
you can actually have a situation where
as a person totes their head the
character in the game will sort of
respond the same way and peer around the
corner as one type of example right
another one that's very common as you
mentioned before might be a situation
where a hat company or glasses company
or whatever or just a funny kind of
let's make silly pictures type of app to
be a situation where did my screen just
go black nobody yet we can see what
you're sharing right now okay no why
just let it kicked me out so me so much
or why looks like we're still take the
white screen for the beginning can you
give it back yup doesn't feel screen yes
now we can see thanks you know cheering
thank so it is popped out something i'm
not sure what um yeah so things like
face facial augmentation is what we're
calling it here right so you can see
what you maybe even think like a
different hairstyle right one of the
interesting aspects of this technology
because we are using the ir scanner and
I've actually had developers actually
experience dozens of real life at least
for the men right the IR goes through a
beard so if you if you had a situation
where somebody had a beer do you want to
give make an app where they would know
what it looks like if they suddenly
shave the beard or mustache whatever you
could do that relatively easy because
the IR scan doesn't get interfere with
my hair or maybe you have a different
app where you might allow people to see
what different hairstyles might look on
them by effectively scanning their head
and the quote-unquote removing the hair
which would be relatively easy and then
putting fake care or augmented hear what
everyone call virtual hair on so that we
would know what Brian would look like
with a mohawk which I've always wondered
about so Brian don't fall asleep
anywhere around me if I have Cooper's
anywhere another interesting aspect from
a facial recognition standpoint is we
found that when people play role playing
games by RPGs when I a lot of people
will spend a fair amount of time making
their avatars look almost exactly like
them just a little bit better and so in
terms of real sense you're going to have
a situation where effectively we can
very quickly jump ahead and make the
avatar sort of you know the the default
look very much like you except as an elf
or Klingon or Vulcan or whatever the
game maybe and and then you from there
you can adjust it maybe change the
hairstyle or whatever else you want to
do and the other one is what we're
calling affective computing which might
be a situation where you could be
watching how people mostly react so one
example might be a situation where
somebody makes a game for children that
you want to be a little bit spooky but
obviously don't want to give the child
on nightmare right so you want to you
want to have something along the lines
of you know a Mickey Mouse halloween
special not a Friday the 13th and so
they could be the app or the game
whatever could be watching the Charles
reactions facially their facial
expression and keep things you know you
know somewhat scary and engage but not
so much that there
they freak out and they all of that one
ah so these are the types of things
everything could be very very
interesting Richard I think your
question going to answer it really well
but just to maybe reiterate it slightly
this camera right here this board this
is a sensor is going to be built into
the bezel in other words a frame of
things like all ones and two ones one of
these OEMs the bottom start to make
these systems so the quote-unquote
consumer version of this camera won't be
something that gets plugged in at all
right it'll be built into your laptop or
on one just like webcams are today so
yeah you won't have to worry about
chords because it'll actually be built
into the system implicitly and the only
reason that we've made these cameras
available at all is to give developers a
jump start there should be no critical
consumer versions of these plugging
cameras at least that's the plan right
now is to have no consumer versions of
the camera these cameras are only going
to be used to seed developers so you
guys can get it like you said a jump
jump ahead on your code development yeah
and neither can you give us a little bit
more context as to what you mean by a
database support like your their
particular database or usage that you're
talking about well I'm not sure where
hundreds it clear obviously if you
wanted to do something like let's say
stand faces for I don't know some sort
of company directory or something then
you want to take each of these scans and
put them into a database the data you
pull off of real sense will just be like
any other kind of data and then you
could put it into any database you want
it if you have the ability to add that's
that kind of that that that data type
yeah so the yeah you can connect it
basically any database you want like i
said if these there's nothing
intrinsically don't or not we don't have
like let's say direct support for sequel
or not but all databases are basically
like anything else you're just container
is that you
stay in and so once you take let's say
of fate a face scan or some like that
that's it becomes a bunch of ones and
zeros of data so you could put that into
any database you want but we it's not
necessarily something intrinsic we're
doing yeah you can definitely use
databases in your development for sure
if you were doing some sort of story
like you know as we were saying before
about work when a child reads a book as
a child starts to read the book you
wouldn't necessarily one of the version
that I've seen is basically the child
will hold the book up to the camera and
the camera will basically look at the
title you know the cover apparently that
old adage I've never judge a book by
it's cover is wrong when it comes to
real sense you will in fact hold the
cover of the book up to the camera the
camera will scan the book and go oh
that's Jack and the Beanstalk vs
Charlotte's Web or whatever and then the
child can start reading the book once it
registers yeah which sort of wrong so
that was the version of that does the
SDK support where arm tracking and spine
tracking okay for sure I know that we're
not doing spine tracking yet that is
more of full more full skeletal tracking
something that has been requested
upstream right now we're only basically
doing the hand scanning that I shows
here so this is the 22 points of
articulation that you see we're not even
really necessarily scanning the the
skeleton if you will of the face we are
using the landmarks but that's not
necessarily like obviously there's no
bones in your nose um where that's what
justice is just cartilage so I mean
we're tracking all that stuff in fact a
lot of the things that change on a
facial recognition is actually more
muscle than anything else so if there
any plan to include the SDK into the
Windows 8 apps SDK yeah that might come
in the future but we're not there yet
yeah so
so that is a great question Djoko and
it's one of the feedback that we've had
is that the early versions of these
systems are going to be focused on
things like laptops and all and wants
right when you look at the current state
of the desktop market right now is just
a market market discussion marketing
discussion to be honest what we find is
the skin alone desktop market isn't
growing anywhere near as quickly as he
all-in-one the all ones are becoming a
very popular subgroup for number of
different reasons one basically less
systems right your monitor is your
computer your computer is your monitor
one less thing you have to worry about
one less set of power cords all kinds of
things like that some of these will
actually also fold down for us on tlie
some of these come with batteries you
can use them temporarily not even
unplugged right so you could have like a
family game night and play one of the
board games as an app on them so all
ones growing properly there have been
questions again feedback that we've
we've gotten from people being able to
put real sense k amer technology into a
standalone monitor that would plug into
a desktop so that is something that's
being explored but for right now the
only thing I can say is a definitely
plan of record is for it to be built
into all in ones and two ones and
laptops and things like that yeah I
definitely see your point Richard in
terms of you know being able to allow
from a game developer thing impulses
that I've seen people actually do is to
take oculus rift and actually they
actually use duct tape to be honest put
the camera duct taped onto the oculus
rift it made for a very interesting
application because basically the camera
would scan your hands in front of you
and I actually saw this at last year's
augmented World Expo and the oculus rift
they're basically like you're playing
your own personal version of asteroids
right and you would be smacking your
hands or at the asteroids they came to
you to protect yourself so
um that was that was interesting so I
think mega has to answer the question
about the luminosity well not the two
aspects to keep in mind about that is
the IRS ins are always you know itself
doesn't require any light whatsoever you
could actually technically make some
sort of quasi I hate these with
night-vision goggles but something like
that right there are have been times
when people have done that and you can
actually see a fairly good detail of a
face just because of the differences and
depth and whatnot but then obviously if
you wanted to do things like the RSU be
aspect like let's say you wanted to
actually see a person's face and
obviously the Lighting's going to be as
important for this is it would be for
any other webcam so lighting is is going
to be there if you plan to use the RGB
component is that's going to be key so
we've got about nine minutes left we're
getting some good questions here I think
we're having some good interactions one
of the things that I want to mention go
back to this real quick is this call
will end but our support will not right
and so if you do guys if you guys have
questions after the fact actor if you
you know two ends up with a calls over
you like oh I should have asked about
this right we do have forms right now
they're very active with lots of people
that are in the challenge that have been
asking questions and so feel free to go
to those form then ask you questions
there we have people supporting mo all
the time we definitely want you guys to
feel very well supported as Joe
mentioned we are extremely interested in
getting you guys in for the rapid
prototypes right the early submission
deadline we've got a fairly nice bounty
if you will um so jackass asking
question about can we use a camera in
non real sense and the answer is
absolutely yes we've seen we are not non
real since non-pc we definitely seen
some people do some very interesting
things we had a particular person martin
who made a very interesting robot we're
basically was a laptop that was strapped
on the back of a lego mindstorm robot
and then uses perceptual computing
camera which
what the technology was called before
God named Ross senses here and to be
able to avoid things like obstacles one
hour so we definitely see air people are
doing all kinds of really interesting
thing in the world of robotics or
another person had and obviously they're
going to continue working this we're
using the real sense technology the
gesture recognition to allow people to
very fluidly control a quadcopter aerial
drone right we had this actually showing
a few different trade shows and so you
would sit there and use your hands like
in a thumbs up position and be able to
steer there's the drone around the room
or not um yeah so the answer about a
thick jacket we're not is really really
know right so when when you when you
look at what it's going to be able to do
in terms of background removal and
things like that it can very very
clearly determine what it's seeing in
front of itself right so it's going to
know your face if you like the landmark
the landmark data right there's a lot of
detail going on this face he right here
oh that's 78 points is very very
complete and because it is not just an
RGB data but it's an actual active
scanning like I like a radar type of
thing it knows exactly where let's say
the tip your nose is right so even if
you leaned into the camera by a foot
leaning back out of the camera prior
foot it can adjust the background and
removes as you move your head in and out
obviously the jackets the same type of
thing right it's going to be tracking
your all of this stuff and removing
everything like the phase is a foot and
a half behind your nose or something
like that right need these kinds of
things are a key advantage of using this
kind of ir scanning right because it's
not just an RGB we were trying to do
rely on technologies like edge detection
and other algorithms such as that like
if i use this picture of an example
right and this is obviously a drawing
but you can see that the person that
will seem like a lady to me have that
dark blue outline right that is from an
app from a algorithm standpoint what is
often use
for previous versions of of virtual
green screens was effectively edge
detection now we're actually able to
actively scan you and take out anything
that isn't you because it can tell from
a rain standpoint from an actual hard
data standpoint that the person walking
behind you or the picture on a wall
behind you or whatever it may be it's
too far away for it to actually be you
and removes it automatically so that is
definitely a key attribute in the event
of this type of active tracking alright
so I'm going to put it back in the
sailaja so that you could have a URL in
front of you so it's going to be
ingrained into your heads and you won't
ever wonder what it was again whoo make
that I know you've done some work on the
actual performances of the SDK and
various in it arrow so we're getting
asked a question about if they're the
gain and performance by using the raw
data API versus a hand tracking a peon
you want to try and dress up sure so
specifically what we have seen with the
the camera and the SDK right now is the
performance is good if you were to use
single hand scenarios but it will
actually drop significantly if you were
to use complicated to hand gestures or
if you were to use contrasting together
with other modalities like facial
detection or watch detection so
approximately about 30 frames per second
is what you can expect to see for single
hand scenarios you can you could clean
up our she'll go to get better proposed
federal share board they're not
optimized for performance so you could
do some of your own code and get better
performance in a shambles it makes sense
all right so
yeah so one of the questions Albert's
asking a offer welcome back I remember
you from the first one was can all the
examples that we've talked about be done
in unity I really hate the word all
because it's a very conclusive thing so
I'm going to say that probably most of
them people have been able to do a lot
of things with the unity like I said I
hate to the jump on the word all and
make a promise that so that we you know
what they Swit the old adage right Rica
making promises your body can't cash
right but yeah a lot of people have been
able to do some pretty amazing things to
the unity I definitely we will be
sending out these slides out through the
distribution yeah so one of the aspects
that we're going to ask here is in terms
of some of the aspects will stand out
more when it's closer to the camera one
of the aspects to keep in mind is that
this will obviously be the actual
tolerances and accuracies of the of the
I our technology is actually an angular
not necessarily a linear resolution
right so the closer your hand is will
stay to the two that 20 centimeters the
smaller the road to actual arc length of
the angular tolerances are so that stuff
will come into play the closing of the
camera will definitely affect how it
looks so Alex with that we are coming up
on the top of the hour so I think maybe
I will turn it over to you and you can
close it out all right okay thank you
very much eric and thanks to all your
team as well thanks Colleen sex magana
Joelle Brian he with us today so thanks
to you guys also obviously in being part
of this webinar I hope well we really
hope that was helpful for you I've been
recording the whole session so I'm going
to put it online on YouTube
in the in the few upcoming hours and and
yeah I think we are done good luck for
the photo following guys so yeah it's a
final row of work so really once again
good luck and yeah any feedback are on
the any forum or anything or more than
welcome thank you very much bye-bye all
bye bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>