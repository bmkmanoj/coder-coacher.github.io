<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>[The next frontier of content creation for games and VR] by Nico Nonne and Mo Li | Coder Coacher - Coaching Coders</title><meta content="[The next frontier of content creation for games and VR] by Nico Nonne and Mo Li - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>[The next frontier of content creation for games and VR] by Nico Nonne and Mo Li</b></h2><h5 class="post__date">2015-09-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K2gltdjOZ-Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">was actually procedure modeling means
that we construction 3d models with algo
es algo Raisman with procedures and
there are small programs which you can
connect it to together to describe for
example overall surface property of 3d
model so that's happens purely digital
purely with computer and it could be
applied with a lot of automatic and now
our two were using is Houdini engine and
it's a long-term 3d content creation
tour to science 19th and this tool was
used in a lot of Hollywood production in
movie production and now we could take
advantage of this program and to modern
the 3d models easily and this too is
also for for indie developer very
interesting because they are very
indifferently its cost only ninety nine
dollar so you can just take in the
license and use it for you big content
production and the reason why we are
doing protocol made modeling is that
sometimes we don't just have enough time
to make everything by your hand
so and this is an example from a sweetie
asset in Houdini and you can see and we
we just described with mathematic
Potter's the surface properties and you
can see I can I can extrude areas and
surface from elevation properties and
step that step to create complex looking
surface this is another side and
sometimes I'll go algorithm and even
surprise you and there's always some
some errors but it's looking interesting
I I like some arrows from mine procedure
because it's always surprising this is
look for me a two little animals meeting
up in the strange planet but you get a
little bit hard time to make you content
3d models looks from real life so it's
more mine planet is more fantasy planet
so there's another pictures but the the
procedural modeling method could be also
combined with scanned data in this
example we remoted the mass surface
based on the digital elevation model 14
nada nada has just scanned all the
surface data in very high resolution but
the problem is you cannot get all of
this data in UK
it's more than three for a gigabyte size
but we combining this data with network
procedures that means you get and you
can you can look at mass from far
distance it looks quite good but if you
are going in and into the wedding it is
it's just the way the routine of the
data and just limited so we have to do
some some procedure to refine the
details you can see in the camera
looking at a mud surface and and divided
in different distance area so if the
area near to the camera you got more
polygons to describe the details so
that's another picture and we use a very
good Tehran framework it's a Roland it's
a fun French and it's very performant
and you can combine lot of procedures
with analog data like from another so
that it any questions about that but
watch procedural modeling ok one thing I
want to talk about nexus def cameras
which probably some people have heard
about the Microsoft Kinect and the intel
realsense right the thing that we really
love to work with those is because we
get point light so one of the things
that you normally do when you do like a
normal filming you just get a two
dimensional plane right just one film
but what happens if you want to include
a foam in your 3d game then it gets gets
very hard so what you can do is combine
a def camera that means a camera that
only gives you like a RGB user normal
color image but that also give the
distance from the camera to each pixel
so for instance if I look in that
direction maybe this would be a meter
away and something behind it would be
like two meters away so for each pixel
it tells me how far it is away now
roughly and so what you can do is map
the color image so the image that you
get like a webcam image onto the 3d
surface so actually you get a you know a
three-dimensional nose lips and
everything else out of it there was a
guy like a couple of years ago who did
death get that did exactly that he used
a DSLR in combination who is filming in
combination with the Kinect and those
two together give you a
three-dimensional film of course you
know it's limited because you're just
going to film from one direction so
everything behind you is just not
existence but everything that's you know
here on the front you three are still
get in three dimensions so what we want
to use it for in one project for
instance is you probably know gravity
and in gravity everything is fake it's
just 3d models and VFX but one thing
that isn't fair is the faces so they
build a really elaborate and hard hard
to build Rick to get all the peoples
spinning to get all the lighting correct
but one thing they did in the end was
just get the right camera angle of the
right persons face and then combine that
with a 3d imaging that they did
beforehand so everything was fake except
for the faces and then they still had to
bind do a bunch of things to get it into
the scene so what we want to do because
we want we're doing mostly VR content we
want to you to be able to look from
different directions and so the moment
you have a 2d image from a normal phone
camera and you look from it from the
side you immediately see that it's just
a 2d image but you go ahead and just use
the 3d imaging from the last thing and
just film the head and put it into a
three-dimensional suit so just a 3d
model of a suit and then put the face
that is filmed inside of it then all of
the sudden you get a 3d model so NVR
then in the end you can walk around the
astronaut suit look at
from any direction and you see that the
face is filmed from a real actor so you
get all the real emotions that you
normally is really hard to capture in
motion capturing because all those
little facial details are really hard to
capture but so you can combine those two
and you get filmed 3d imaging combined
with 3d models from the suit and then
then you have a really cool model for
your VR game and if you want to go one
step further and want to get rid of like
the 180 degrees that he couldn't see
because the camera was just coming from
one angle then you have to go one step
further and do something like point
cloud fusion or viewpoint video and also
Microsoft has done some you know a lot
of research recently in the into that
and what they came up with was filming
people from all different directions you
see all the cameras with death imaging
and no Moloch normal cameras RGB cameras
and they combined it in something like
photogrammetry so they're basically
combining all the photos all the videos
into images that are three-dimensional
and they get this kind of a mesh out of
it and then they're cleaning the mesh by
even going so far as to do different
kind of resolutions for different body
parts because for instance for for the
main part of the stomach or so you don't
need the kind of resolution that you
need for the face because we humans are
really just looking at the faces and
recognizing immediately if something is
wrong and there's so much detail in the
face that you have to render that they
took special care about it so in the end
you get a three-dimensional video that
means you're really capturing someone in
the studio and you're looking at those
videos from all the different angles and
you can just go ahead and put it into
your three-dimensional scene and have
someone walk around so and then you get
to the moment where you can actually
produce forms or even interactive if you
go one step further content that is
really filmed so all those kind of
holodecks where you were walking around
with you know this becomes a reality
once that kind of technology becomes
available for the public or for in a
greater audience
and on the more locust and already the
use of surfer I was telling about you
before about photogrammetry does that
and this Winston's was taken with
something like 30 DSLR cameras so you
know if you have 30 DSLR cameras handy
you could try that out yourself and film
it from different angles and get a
three-dimensional representation of the
film which is pretty cool to do okay but
we're not all no just interested in that
we're also interested in motion
capturing because you know you sometimes
need those 3d models except if there are
any questions okay thank you and print
motion capturing methods and I will
introduce you a more classical one and
we just reduce the motion in a few
points and in our case and to capture
full body motion you need a teen sends
us in our case we have a into in-house
tool who can bind a two different system
the first one is you got 18 points
that's more like Kairos Enzo's it's a
from you smartphone so it's it's very
popular in production so that means is
quite cheap and and this window has
mostly accurate rotation measurement so
we could use it zensar on actors body
without any problem with occlusion you
can imagine in motion capturing the
actual has always be seen from outside
of the 3d room in in classical motion
capture
production you need a lot of cameras
about 40-50 cameras around our area and
this lot of cameras this makes the
system very expensive like the wise can
why consisting in a lot of Triple Eight
production so we use the sensible
alternative to lowering the price but
this the result is and actually not
comparable with stripper a product
production so we we have to improve the
data and in optical tracking system with
camera you got very accurate positioning
tracking and that is what the first
system cannot do so we combined Latico
saves with the PES cameras with RGB D
data and on this kind of data this data
from this kind of depends camera very
good in positional tracking so you get
small pixels in the camera image but
this picture could be very good checked
all the time and it's another case for
those Enzo is rotation and so you need a
lot of optical reference to calculate
the rotation so why not we combine and
the data of the bose system so that is a
loosely worried about it's like i can
show you a small example
it's just it's in a random I put a grave
motion-capture data in a random model it
just it's from a kung fu master but you
can do it for funny things combined
impossible things together so in this
video you can see the small sandals and
absurd google scope sandals and on the
four corner you you can see our connect
comma wait which give us more accuracy
in position tracking that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>