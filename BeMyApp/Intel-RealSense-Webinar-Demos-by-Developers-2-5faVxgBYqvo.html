<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intel® RealSense™ Webinar: Demos by Developers #2 | Coder Coacher - Coaching Coders</title><meta content="Intel® RealSense™ Webinar: Demos by Developers #2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Intel® RealSense™ Webinar: Demos by Developers #2</b></h2><h5 class="post__date">2015-10-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5faVxgBYqvo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone and thank you for
listening to this webinar about the
intel realsense technology and today we
will have three speakers who will demo
their projects so we will have Piazza
and Julie bagging on loan from Israel
right after that we will have the
presentation from Anthony Thomas from
London UK and last we will have a
presentation by reading our mail from
Austin Texas and yeah so basically you
will have time to ask your questions
during the presentation so feel free to
do that and now yep so we're going to
start with clear and dully right now who
go into opt isn't a rocky start oh hi
everyone my name is Julie Obadiah and
I'm Bonnie Cisco Hunger Now other nieces
we develop easy to use computer vision
solutions and today I like to share with
you how we've been integrating includes
real sense into some of our applications
so one of the biggest benefits of user
real sense is being the wide
availability of this camera it's been
integrated in substantially more devices
than the rest of the dips cameras
compile and there is a lot of that and
we can gather from regular cameras as
well obviously but today we're going to
focus on our real sense ah now cows can
tell us a lot about the person that is
in front of them from eight to gender
the shape of the face the eye color the
skin color and even the use of the users
mood allowing us to provide a truly
personalized experience for each user we
can understand a reactor gestures and
body movement facial gestures are and we
can meet between the virtual and the
real allowing us to create like
completely out of the like extraordinary
experiences now i'm going to show you
the
demo itself of the karaoke application
will build um which is the second to
share them
hi everyone as how i'm going to show you
the actual demo itself first I want to
show you and I Karioke application will
build using our platform and then I'll
show you how we actually build on
without writing a single line of code so
the karaoke application basically takes
the user and immersed in into a video
clip in real time without the need for a
green screen without the need for any
special devices just realsense 3d camera
so here we have a thinkpad lenovo yoga
15 this is the meaning me here is that
healed application will run and here we
will show you the the working
environment our working environment and
they are both connected to this screen
so we'll just switch to whatever is
needed so yeah let's dig in let's let's
show you how these Karioke application
works so you can see me I mean we're
seeing a video clip I can see myself I
can sing I can record myself I can
upload my video into the different
social media sites it works in 20 frame
24 between 20 or 24 frames per second so
is pretty quick there is no big delay
there's always local a but luckily you
can actually see yes it's really awesome
also our karaoke is is something that
has been left unchanged since it was
invented in the 60s so this really
enhances the mockery of experience by
not only allowing people to feel like
rock stars but actually see themselves
and be part of like these rocks are kind
of video clip which is really cool
I don't know how to sing this song so
I'm going to go to another song that I
can actually see ya so let's go for this
one so this like every every style
everything in style has a different
template in this case this is the wrong
template you can see the Lagrangian and
I I see myself is like effect on like
post right
yeah now that that's that's another
template we have show you just a 60s
template will be the one that I will be
joined in the platform and i'll show you
how we actually build it is see again
now this one has like a TV like an old
TV effect you'll see just no way they
can you see the lines no yeah there you
go so that's like this this is like like
distorted TV lines and the 60s
background and the old TVs and stuff so
so that's a no now these must build
using our platform dog show here let me
switch it there you go so yeah this will
be the same and you can see here like
all the elements that build the actual
scene I'm just turning the I saw this
will be the human object once i put this
object onto the onto the scene i can use
it to create multiple steps so for
example here I i pressed remove
background so it will emerge the user
into the video clip but i could choose
to show tracking points for example and
those tracking points i can use to put
accessories of the present for example
bags or like a tie or change clothing or
do anything with him really so like
imagine you have a Christmas campaign
and you want to come out with like a
little video clip that makes people
Santa clouds you can dress them up put
them into like it snowing background and
you're done it's very very simple and
everything is driving drop so let's
let's create our own scene allow them
you seen yeah a ladder you seen I'll put
a background video in this case is a
background video up like a music band I
could sell it nicely but the mouse is
not exactly comfortable so here I i'll
press remove background and the person
will see themselves into the video clip
it took me exactly what a minute to so
literally simple to do now we can choose
to create something that uses for
example face tracking so throw
everything away i'll close everything
put a face tracking object in this case
i will also remove the background and i
will choose to show my tracking points
now these tracking points can be used to
for example put on makeup eyeglasses ah
I don't know you can put a hat you can
really like a personalized the user
experience you can even make people look
differently because we have real-time
effects so for example a person can see
themselves with huge eyes in the blue
face and like in like an avatar kind of
flaw of interaction so i can make build
like really different kinds of saints in
an environment that is completely
graphic very similar to flash or after
effects and so on that way you don't
have to write a single line of code and
you can create different kinds of things
that i use these kind of technologies so
yeah we also have an interactive mode
that i won't get into but it allows you
to graphically design your interaction
are using gestures oh yeah i hope you
enjoyed it i'm going back to the
presentation okay so yeah as I was
saying the applications can be or html5
or C++ depending on the environment you
want them to work if it's a it's a web
application or if it's an actual app now
I would like to share with you some use
cases for like the platform itself
virtual me rose today most of our
clients are retail clients for online
virtual me rose and one of the most
surprising features they use is the
ability to provide personalized
recommendation to their users so for
example in makeup and user can be used
it can be awkward an eyeshadow that
suits their eye color or make up the
Tudor skin tone it really like you can
create really personalized kind of
recommendation system ah I want
companies usually want to know the users
face shape so they can recommend the
best friend project the
of the use of the face of the user this
way an online store can provide a
service that is as close as what they
could provide in an actual sort of
creating a memorable experience for
users his name of every interaction i
believe and what better way to do that
than to emerge the users are making them
part of the interaction itself building
a personalized social campaign in a
publicity stunt advergames it's endless
in this example you can see for example
promo for the movie avatar and this is a
project that i really really like it was
made by an interior design student and
what it does it allows users are
physically a way to interact with one
another truth projection so in this
example you see a dad in one room and
maybe mouse away from him is his
daughter but it is still able to
minimize the distance but physically
interacting with each other through the
through that application there are many
projects of the old medical assistant be
retracting the users action or to
predict or announce any abnormalities
from the movements or helping them stay
fit reader I am exercising or
personalized physiotherapy I and so on
so today we see the potential mainly
retail advertising a little bit of
gaming but imagine the possibilities in
education training in art imagine that
vacations that could be built for
wearables for the Internet of Things our
only limitation is our imagination thank
you think you don't know what's really
cool do you have any question if you
have any questions sorry if you have any
questions for dolly and kaffir feel free
to send them to to us and then we can
tell them in the meantime now we're
going to sew Thank You dollie on with
your presentation now we're going to go
out and see Anthony's presentation
hi Anthony hello no I alright so yep
sharing your screen please yes all right
okay well alright so this is my project
so I got hold of the real site sensory
while ago I was asked to have a go
innocence even there was something
interesting I could do a presentation
the thought this would be a fantastic
opportunity to learn how the realsense
sdk works and try and do something kind
of interesting with it so the thing that
I've decided to go about doing was too
drunk a real set set set its
three-dimensional and his ability to
power three dimensional coordinates from
our from space and interface that
something else something that I found I
was looking around through I'm looking
around I was looking around through
Matlin's and they had a fantastic it's
I've got a robot arm which I thought
this is quite cool and I saw that
someone has it's actually may be here
they had it essentially the arm which is
in this product I saw this and I brenton
articles that someone had kind of hacked
it and had the USB driver so I thought
what would be a good project is to
interface the three-dimensional spatial
coordinate so you can get out of the
intel proset sensor and plug it in
something like this which you can then
move around and and yet you can you can
you can move around and you can use the
additional bonus that here to drive a
movement at the arm so I've written
that's why i wrote a small windows
application to go and do this and so I
guess the purpose of what what I'm going
to do today is run through to run to
this application what does and a bit of
you know how we've gone through the SDK
some of the things i found that with
nice and not so nice and yeah generally
how the project went okay so open this
up and yeah okay so what we can do here
i'll actually i'll share my this is
going to be
or because I've got to share my webcam
as well as this the same time but it is
here's the application basically I can
activate my arm from here and I can also
activate my camera and bind those things
those two things together so we can run
out and there's there and you can see
that i now have a arm which is a loop so
what I'll do is I'll switch to the other
camera and we will and i'll show you
that i'm basically making use of the
hand tracking algorithm within the roads
sdk to actuate this arm inside a boot ok
so let's go to the other share my web
cast ok so the results camera is
attached my mother is in my laptop a
moment and i'll put down ok so I'm going
to move my hand up and it's going to be
picked up by the resnet sensor and so my
hand has now been activated so basically
by moving my hand around his face I can
now it will now look for fall in the
hand I can do this it's just a bit more
because in my hand a little bit so
basically here ok so this is fish this
is effectively tracking my hand
movements and so this is just pulling
out the this is this is pulling out the
center of the hand position which you
can get out out the SDK and also using
some part the gesture recognition so i
can i can activate the pincher movement
here and it will also pick up this pitch
movement so what I've told that to do is
like to make the grammars at the end of
the hand
yom kippur Turles oh yeah okay so that's
essentially what it is it's coming in
and is tracking my hand and then it's
solving the solving me invest kinematic
equation of motion and say I'm coming
with yonder here I just kind of found a
quite good library that was that's
longer than actually ill be doing this
and yeah so this fundamentals need to go
in so within here this is allowing me to
then move my arm around ah yeah the
vehicle of spaces and things that you
didn't exactly that correctly but but
you can see the generator service you
can move your arm around what I'll do
now is I'll stop it and reset it and
we'll have not again that's let's go
back to the main screen and we're going
to do a little research okay okay I just
pretended the arm well then yes sir what
you find is it quite a dress up a bit
because there's no there aren't any
traditional sense on here because we're
evil bug we're actually not at the 000
we're back and that's back in 20 station
so let's go and turn it back on again
and just pop the cap over again so is
the center screen okay so this is going
to pick my hand up mix it up and then
you can find the corner behind the
cornice it will then activate the arm
so I'm so far off those days and living
across but I think that kind of gives
you a quite good just about what the
property was an answer about interfacing
the realsense sdk and getting you to
drive this robot on it's now fixed up my
second hand all the time is up to now no
need of okay so so like how do you like
what are some of the kind of main
features about the the real sets as you
can have you go that filling this so
let's go jump in too let's go jump in
too let's go jump into the main bit the
code that handles the UI stuff so in
this class let's look through so the
kind of the main there are two ways that
you can kind of interface with the with
the SDK one is you can either run a main
program loop that goes over the
basically goes over and queries for
whenever a gesture is fired or afraid
this processed and you could may see me
go around this kind while loop and then
pick out when movements have happened
we'll just just have happen to process
them they're off or there's another way
which if you look up in the
documentation the other way is you can
basically you can assign event handlers
to your session that you've created
about your session and you or just your
yes it's actually you've signed some
event and as and when there's a vet so
far you can handle that way it's the
main thing here that you're going to
have is in the case of hand let's just
roll into a drama class so in the case
of tracking a hand what we can do is
let's
jump into their method here we transform
on smooth and let's go see which one
we're going to do get back in here 15
okay so this is actually one of the bits
and code that handles when we where we
find gesture that's been five so this is
just again another event handler so we
can handle this and this handles through
this in Europe and you can see what's
what to do get the way the gestures
would pull out is there just generally
kind of the gesture jaded comes out and
it's just the name as a string and you
can handle these like this so be doing a
full pinch I've just have something to
turn the gripper on or off it's so it
doesn't suspect fourth and one of the
other things that you can do here is
also is there a sec from the hand
becomes gestures I've also from the hand
comes alert that's the other that's the
other event that gets raised by the hand
so you could also have the live with
this switch case in your book you have
here you can do whatever you want so
this will tell you such as when has
detected help detective tracked
calibrated so and so forth look these up
in the SDK ting this will show you how
to basically handle the events of when
you move the hand in and out of the
tracking borders or when it's been
tracked actually or what it's still or
when it's been found but not yet tracked
you know all those things there you can
also find that the your returned events
from the European returned events from
the hand controller but also in your
streaming video out of this and so
there's another set of events which is
also raised from the camera itself so
what you can look here is that this on
you to sample event which you can also
handle and this is for essentially
drawing the this is essentially fridge
for drawing the video stream back to the
screen and that's that's long and you
can got to handle that there you need to
ver and the other thing that you have
here is the other thing that you have
here is it is also my other module
protest frames so this is when you've
actually have your have your algorithms
process one of the frames that have gone
an that when they've actually process
some of the real reals that study that
you get out of it
this is where we essentially handle the
this is where we handle this is where we
handle the data that comes back from the
hand so we just do is we just is here
what we will do is we will it'll get the
position of the hand and then we'll run
this arm movement thing and this is what
in this method you jump do this this is
what happens when we get the when we get
the position of the hand out so let's go
and say okay so what happens when we get
our hand position here so you have a you
have a hand data object which it comes
from SD again we look in this thing here
so when you start you create a session
and you create a hand module and the
output that goes into this hand dated
the thing and you can query that hand
data to get stuff out about so when the
joints are but the fingers are where the
centroid of the hand is so on and so
forth so what you need to then do is
find the idea hands because you have
more than one hand in your in your king
of you you can have two so you need to
know which one to xfest am i doing that
you can then call this query mess in the
world and you'll get where the center of
your hand is but you can similarly do
some other stuff like I mean these are
some of some of the other other methods
that you had that you have available to
yourself here so you have have joints is
calibrated query on so I query massive
the world really finger deja so within
this kind of when you have when when you
have this I hand object back you've been
then query this for getting some of the
phone data at you yeah okay so what's
probably kind of jumped into that we can
then then do is query the Croix de masa
and world and then once we've got this
point out of it let's go back into a our
movement back into art and we go back
into our movement method so we get the
hand out of there and what we can then
do is then i have this thing which
basically then transforms that into
transforms that position of the hand
because the course is for the hand is a
bit different um yep because the
coordinate system the hand is not the
same to the quarter system and robot arm
i have this to change that round and
then
basically we need some we just run this
robot arm to go and move into to go
moving into where the position of the
hand is and I think and that's again
this is the robot arm thing it's kind of
a bit separate and not really within the
context of here but this is kind of
helping to show you some of the ways
that you can essentially pull that data
out and input and then push it into
somewhere else I found I think it was a
this robot arm was a library that I
found called test owa 535 if you search
on google has a few search on Google for
basically hacking the map and robot arm
you'll find someone who's written like
you're innocent add a few extra things
to it all rights so so what else are
some other things about this SDK that
are interesting we go into okay so what
else so yeah this is where you actually
handle your sort of when you when you
initiate and we just solely you can
essentially we can get the stuff last
that idea so it's sorts especially you
can register your event handlers here
these are there is once we have here or
just a dolphin alert countries already
interesting i'll go back to the go back
to be listen one actually because this
is how you save with you up for coming
here okay so this is basically how you
configure your this is how you configure
your your basic your session and your
sense manager so you have a session
below that you have a sense manager to
handle the real sense sense they'll just
contain within the session and then
attached to that sense manager you can
basically you can basically enable the
extra realm of modules so let me look at
so when we well I ball is an able hand
thing here that allows us to use a baton
ok basically enables the handler with
modern out of here show you so many
other ones which come out of this as
well so this thing also allows you to
learn to enable 3d scans blog face so on
and so forth so this is where you can
access the different algorithm modules
from the from the oils and cess manager
again what you're going to do here is
once you've created a hand module you
need to configure the way that those
hands are you need to configure the way
the hand model is going to act so what
you're going to do is these two things
are basically working out what the event
handler is 44 gestures and a lower
switch 5 which we then saw the which we
then saw go back to the they go back so
we use basically these are just delegate
methods that go into here and check the
on fire gesture and let's let me handle
that and then they will fart alert one
let's go to control that yes so that's
how you subscribe to the injustice and
alerts when youth when you set up look
into the configuration and that's if
you're going through the god of event
huddle model of doing this the other one
is as well as we're enabling the alerts
so these are two things that should just
find out when hand is tracking
calibrated and I'm picking that up to
make sure that my hand is in the right
position that I can actually start then
move my robot arm what's that's done the
same as you can you can enable the
gestures so these things will get past
back to you when when when the I'm Jess
your handler gets called gestures fired
out you been by the changes and then you
start and you use this in it and that
basically starts and that starts running
it and that thing starts streaming the
frames out of it and again you've got I
think there if you look through some of
the samples on the intake on the real
sex at SDK there's um there's quite a
few different types of stream types so
you can get the color what else is in
here get so yeah you can basically pull
out these happenings here depths Eddie
Colorado fry yeah okay um those things I
think I'm personally I think using I've
I did some of the demos and some of the
demo code with feet using the world it
has a lot of this and I found just doing
the other inanimate let's eat cleaner
that seems to work all right yeah so
that's pretty much kind of
how this project came together yeah and
I think it was just kind of quite nice
to see if it wasn't it wasn't I mean
once you've got have gone through a few
tutorials there wasn't too much extra to
kind of get one thing and hacking into
another so yeah I mean have a look at
this robe robe library and one of the
things that I did do it here actually
was to add some extra functionality so
you can get all the joints moving at
once because i think that the one ER you
get is stock it moves one arm and then
stops and then with another joint in the
stops but this kind of changed it so it
will now we'll go at once okay so I
think I'll just jump back to the
application again and we'll go with us
and I guess that's between joking okay
scattered some okay okay okay come on
that so you should pick up and
still a bit of affection to get to it I
think some other your gears have gone
that a bit okay well I think that kind
of illustrates doll thing and hopefully
that's given you a bit of a run round
about how the SDK works and how you get
some things of it thank you thank you
anthony you was great um so I have a one
question for you oh I'm prabhu uh so is
the source code in details available in
any website uh yeah okay can I work now
I'll just paste in the github yeah I've
got this up and github so I will just
push up now actually yeah okay I've just
put this in the chat box now we will
also send it tomorrow and you will send
everyone an email and we can add that as
well to the email coat yes oh that's
great thank you anthony was awesome and
just thank you so now um we're going to
have um Rudy's presentation um Rudy are
you ready
woody can hear us really onions just you
can just unmute yourself then we can
start with your presentation go hello
yeah is that I mean yeah yeah okay well
here we go so um let me present this
real quick all right so um thanks for
attending the talk and a great
presentation the robot arm was amazing I
really uh really awesome and the
presentation with the augmented reality
of mixing and the videos was really
great my project was bio pen I was
working on an integration with a open
source online quantum simulation and I
was going to put that together with real
sense and move some molecules around I
integrated oculus VR and this is the
tale of that journey so x 0 pen was a
real simple place where I could just
load up a molecule store data on the
back end I used the cloudkit data for
Apple's iCloud storage mechanism so this
has an iOS component as well and when
you sign in then you can then you know
store private data or whatnot but you
were going to be able to visualize the
electromagnetic spectrum is that I was
trying to denote to everyone and the
simulations don't use the latest kernel
because the latest kernel right now is
under a nondisclosure for scientific
purposes so as soon as that gets cleared
out and we publish that out we can
update the colonel on to something other
than these van der waals like spheres
because there's a hidden mystery in the
three dimensional quantum physics that
hasn't really been enlightened most
folks so whatever I trying to say here
I'm trying to say that this
a necessary tool to try to present
information to people because that
presentation is really difficult quantum
physics is a really 3d problem and so in
order to get them to immerse themselves
into the chemistry you really gotta jump
into some of this virtual reality space
so we took the gear VR and we did some
hacking we also took the realsense
camera and put them together to create
something that would teach another way
so this is all open source you can find
it at orbiters biomedical github io / x
over these were our goals we wanted to
teach you wanted to use WebGL we're
using three Jas library we had oculus
rift support with the gear VR so this is
an Android device installed inside of a
headset they can work with Google
cardboard or any any of the integrated
variants in fact we we waitin did the
hacking on an iOS device just to show
you how easy it is to push and do any of
these then we integrated the realsense
camera of course we invented a new
protocol for data transmission streaming
into the device we call it the real
sense data protocol but don't let that
fool you is nothing more than a simple
TCP stream of very simple commands and I
will go over that mr. so by a pen looks
something like a simulation to show data
this data is coming from pubchem and
this molecular search box can find
pretty much any molecule you can name as
long as the name is correct the the
matching results search feature I stuff
to implement that is a2g so what are we
talking about here this the purpose of
this is really to take these caveman
drawings and turn them into something
legible that people can actually
understand so we look at acetylene at
the bottom here and you see three dashed
lines and I'm supposed to compile that
3d quantum physics and assume that I
have known this information which is not
the case and what we're going to do is
create a point i want to write the
organic chemistry tutorials i'm going to
rewrite them in these these hyper
physics equations and i'm going to push
them out person so this is an example of
a very difficult molecule to really
comprehend the electromagnetic spectrum
of and 3d really gives us another
contribution to the understanding of
space and the fabric inside of it the
the octa nitro cubane here is a very
important molecule because it's one of
the latest explosives that the military
is using and since this is global i
would say this who knows how many
military's but this kind of information
gets very sensitive at that point and
really cool the retinol molecule when we
start talking about the electromagnetic
spectrum has this amazing property that
can only be seen in its
three-dimensional form so this tools
trying to show that what have we use to
show that we use 3gs 3g s is an open
source javascript libraries very easy to
use it takes all the difficult out of
programming 3d objects i encourage
everyone to just look at the examples
rewrite them put some information in
there try to play and have fun and enjoy
the thing that you do and love most and
maybe that's programming so whether it's
robotics or whatnot whether it's any
other sort of 3d and really encourage
you to at least look at the capabilities
of such a cross-platform browser-based
manipulation they're so very simple we
include these script sources we can see
3gs here transform and trackball
controls i created these open source
pubchem adapters that can give you the
molecular data based on a search you can
check them out there also in that open
source link but / pubchem adapter
pumpkin some 3d uh their statistics uh
for this three Dodgers related and some
other files here we initialize the
JavaScript with
a WebGL div we install a camera with two
lines of code we set a position on the
camera we push it back and then the
z-coordinate is forwarded back into the
screen and out we set up a scene very
simply we add a color to the background
and we add some fog what would take a
lot of boilerplate code is all gone now
and very simple use we add an event
listener her the keys we look for the
are key key key and we changed the
transform controls accordingly we add
and subtract to change these alpha
transform ctrl sizes we'd set up our
controls for the trackball and these are
all default parameters that are you
installed it we could manipulate had
what you need to and we needed to we
install lighting so the ambient
directional lighting are very simple
I'll abstracted away WebGL is a little
bit more like I said the boilerplate
code is gone we create a render the
renderer is the object that draws and we
find a few objects to it we go into an
animation loop here and we have a render
loop down here we render our particles
that's it nothing more so pubchem this
is uh this all this huge data bank is
you know it was built by a lot of hard
working individuals but they do not have
any money and they don't have any time
and so what's really lacking is the
exploitation of the data to do something
amazing like teach in another level so
we have to use this as an adapter in a
database that we can you know extend and
try to contribute to the community's
knowledge of auditory right there so
that's what we do this pubchem adapter
is an open-source little adapter just
grab the JSON and just brings it back
from a search term it's very simple but
can find 50 thousand or more molecules
of you know collected data so we WebGL
enable our
devices what a nightmare trying to get
these browsers on these mobile devices
to get anything running so in the end
you know at the end of the day it was
really just Android had the native
browser was capable but the device
orientation controls were coming back in
all kinds of wonky ways and chrome and
was not drawing any WebGL um it on this
galaxy s6 active you know that may not
be true for other devices but I had the
galaxy s6 active so you may be wondering
the gear VR doesn't even work for the s6
active but you know we hacked that of
course it only works with s6 and the
note 4 um but like I said this is a
journey and you can't let the hurdle
stop you so this is just a simple
example of a 3d stereoscopic breakout so
you go to your browser and the browser
will draw a stereoscopic view of some
data and that's all you need for Google
cardboard or the galaxy PR in this case
um and you are instantly immersed in the
3d zone so that's that's what we're
going for here the pubchem adapter very
simple those through grabs the Tom Adams
gets the bonds puts them in some j
javascript data we are going to match
them up so the algorithm is as follows
the elements are outlined so six means
carbon one means hydrogen you look at a
benzene molecule so we see six carbons
and six hydrogen's we now look going
across striping from the AI d1a ID to we
just match up you know el element 1 hits
element 2 element 1 also binds to
element 3 everyone's also buys the
elements 7 element 2 binds to for two
bytes 28 number 3 binds you know you can
see there's a pattern there and then you
can rebuild a molecule basically this is
what the gear VR looks like before we
take the dribble this is what it looks
like after we had to make some
modifications but it was worth it the
galaxy s6 active like I said I really
like to drop protection and all the
other cool features but this did not fit
when i bought the device
yeah I did not pay attention that much
but at the same time it wasn't worried
um what we did is we took it apart we
removed screwed by Scoob we began the
process I took a dremel and with machine
like precision tried to craft out the
dimensions without damaging the internal
components
of the day the fit was luxurious and it
was it was awesome I could step into VR
and that was really fun and it's the
window is so unpaved that we could
pioneer the whole future of the
molecular interfacing or let alone you
know just web interfacing in a 3d
vr-zone you know using real sense to
communicate and touch and really push
buttons in AVR space is this is an
uncharted land we have a new ground to
bring so of course I grab the iphone i
strap that guy in there i happen to be a
very powerful iOS developer so androids
it's you know I'm working on it uh but
uh yeah I've aya phone and with the
html5 property of the gamepad API we are
able to grab Sony PlayStation 4
controller data and input that I have
those samples online there on the
website as well you can just plug in USB
micro into your us we play station
control or any other controller really
most of them do this in front of all and
we're instantly in we can control and
send our data there here are so in
places where I learned how to do this
html5 rocks tutorials doodles gamepad
was probably my favorite the other three
used examples that had nodejs backends
which I I am NOT ready to completely
learn yet and because of that I wanted
to stick to the front end JavaScript
only so this as ice the focus was
JavaScript html5 css3 wow you sass but
you know simple simple and I will get
into note eventually so this is the
tester these mac and Windows 10 here
setting up boot camp now we're going to
integrate the realsense camera it's
another pc running with the real sense
checking out some of the demos beginning
with the rsd p integration what we're
going to do now is we're going to
transmit our data a
to a format that our computer can read
and send into our device so this will
begin introduction to web sockets so
I've been thinking about this my whole
life and trying to understand sockets
programming as a child just didn't work
out because I'm you know I like I think
it was an outlet and I can't say that
sockets programming is difficult now
with HTML web sockets with only a single
line of code I can send off WebSocket
data and instantly communicate so the
the hurdle is not quite so difficult and
thanks to mr. Darren Davies here on the
github who really just came up with the
answer the VAR connection new WebSocket
you say WS colon slash slash and then IP
address and whatever device I happen to
be on you know whatever and then colon
port would choose 9004 for example or
something connection not opened BAM you
send their data instantly what you need
on the other end of receive this now is
a WebSocket server so we chose Python
and we ran with pythons twisted and the
open source Autobahn library so with the
autobahn library and twisted and Python
in Python we we were able to get all
that real sense data as you can see to
the server in the back hand um so I
wanted to break this up and so you can
see what's happening with green the red
and the blue box here the FF hands
viewer plus the WebSockets that's
actually the FF hands viewer underscore
our SDP demo and you can find that on
the open source thing it's a JavaScript
web sockets client so it's going to be
transmitting all the hand data and
everything out of real sense and then we
transmit it in a really simple protocol
just built right on top i just sending
those text messages nothing nothing like
optimized in a binary way real sense
data protocol server and python then we
receive that on the gear VR with a
JavaScript web sockets client as well so
we're going to talk about the client on
this green in first codes very simple on
open
we set up for the connection and we just
run with the IP of whatever we're
running at we opened the connection we
send off this connection is not
successfully on a message we have this
real simple code that is just exactly
from the sample text message was see
right connection closed when we get a
connection we send our hand data and we
just take a json.stringify of that hand
data so we just pipe that over to the
RSVP server very easy it's just an
insertion right into that little bit of
code and another insertion we have the
no hands detected message we also send
off gestures so then in the RS DP
protocol you see it's built on the web
sockets really we have the following
messages we have the initialization that
closed the hand the gesture no hands
detected the face we have spoken and
spoken alert for command gestures this
is a server log look in here we're
running extremely fast greats fast
enough to get more than 120 frames per
second even um yeah so like right there
we see the little hand identifier than
the JSON blurb and that's all machine
possible so that is the middle part the
gear VR now the WebSockets client um
found a lot of problems with other
browsers here the conclusion is i need
to rewrite some of this in the CSS 3d
which won't be hard it started out CSS
3d and i just need to put an option to
choose when the renderer is not
available what about native application
development well the webview native app
window for the android just didn't allow
WebGL and the UI webview ran web GEVO
pretty well on iOS so app wrappers those
were included in the bio pen vr project
um in native OpenGL right i mean let's
get to the nitty-gritty the project
scope creep really came into play here
because i was sworn to focus on html5
css3 javascript but future iterations
could definitely have a c-sharp real
sense WebSocket sir
running to really just pipe that data
straight into the device you can go to
the URL slash this pen HTML and then
whatever identifier um you can test this
out yourself we do the stereo parameter
real simply as a parameter in the URL
for now um the controls for device
orientation this is like where are you
moving your head and you see the
molecule fixed in a certain coordinate
space mixed with head tracking will be a
very nice mark of augmented reality
mixed with some a our trackers
potentially really make a nice
demonstration there so this is what it
looks like with the iOS device hooked up
the well if you if you look at the right
you can see the molecule and very faded
figments and figures so yes thank you
for dreaming with us and and motivate
yourself you know your your pave in the
future so good job well done um that's
that's us now going to switch to a
camera mode or something to show you um
here let me figure this over okay I
should switch I don't know to switch
this thing means okay so behind me is
the UH here we go okay so
so there is an RSVP protocol working you
can see all that data up there and is
just picking up my hand data and I just
transmitted over the wire to this other
machine over here all right and you can
see in 3d space we have some kind of a
molecule it only shows up when you look
down because I wanted it to be in front
of me in the keyboard space um once you
have those stereo views and like I said
the URL is right there the sample codes
right there you can just go online right
now to orbit is biomedical get up the io
/ capital bio slept capital pen / Intel
FF hands viewer viewer okay there's
nothing wrong that's for that would
never mind but let me put this in here
and is there how much to show you I
guess I'm just gonna have to try it for
yourself um but uh yeah she is Wow yeah
whoo it's in 3d eight and have augmented
reality space okay okay so yeah that's
never fun when you're just watching um
any questions that's it thank you for
the it was great um yeah yeah people
really like to a presentation and thank
you then um did you have a little maybe
video or I know how you try to do the
Goodman right now I'm afraid I don't
have any videos just online links ok
let's already get an email tomorrow and
everyone can see yep that's not good
thank you so much thank you bye so thank
you everyone for watching we will send
an email tomorrow and we will give you
all the information we will be also
uploading the webinar to YouTube right
now so that you can watch it again and
if you miss some things again thank you
so much for joining today and have a
great rest of your day thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>