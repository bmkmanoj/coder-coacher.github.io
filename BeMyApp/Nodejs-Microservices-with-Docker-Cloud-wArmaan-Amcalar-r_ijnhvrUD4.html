<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Node.js Microservices with Docker Cloud w/Armağan Amcalar | Coder Coacher - Coaching Coders</title><meta content="Node.js Microservices with Docker Cloud w/Armağan Amcalar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Node.js Microservices with Docker Cloud w/Armağan Amcalar</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/r_ijnhvrUD4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone I'm going to be my webinar
will be starting in a few minutes see
you guys then
we have all right thank you hello
everyone and welcome to today's webinar
about building and managing micro
service applications in the cloud with
dr. Claude I'm armand i'm gel R and I'm
the head of software engineering at unum
gamba ha which is a Berlin based company
for building solutions for the future of
mobility we are enthusiastic about
future vehicles and we're developing all
kinds of hardware and software services
for the future we're also hiring so if
you happen to be interested in what I
will be talking about today we have open
positions for you I also lead a
voluntary organization called Longy
which is a software craftsmanship school
for young engineers
I love open source and here you see a
list of my open source libraries that
range from Web Audio libraries to
distributed computing to code generation
tools and well beyond so today I'm gonna
talk about what makes a good micro
service architecture I'm gonna demo a
little example with an O J's library
called code and I will then talk about
docker cloud and how it could help us
with continuous delivery and I will demo
a workflow
or scaling a microservices application
in andhaka cloud so what actually makes
a good micro service
what is a micro service architecture
anyway in the broadest terms if a
request to our servers is processed by
many little self-sufficient and single
responsible services then you make all
your application a micro services
application so what are the benefits to
this approach the may be the best the
biggest benefit is you need to only
scale the part of your application which
incurs the highest load which means you
don't have to scale unnecessary parts if
there is ever an error in the system
only that particular service will crash
and the rest of your system will be
unaffected basically micro services
consume minimal resources and they also
give way to aventurine architecture
which is a very suitable architecture to
meet the demands of current a modern
apps but the question is how are all
these even possible in order to achieve
all these benefits you should be
deploying and recreating and scaling and
managing your micro services without a
glitch without a second thought it
should be automatic like a snap
you could have a server farm of 20
servers and at any given time and micro
service could be located in basically
any one of them and you wouldn't know or
you wouldn't care this requires that
your services can discover each other
automatically for example when a new
service is deployed it also means that
their internal configuration unrelated
to the new service should be updated in
real-time it might be an IP change it
might be a port change or it could be a
change of an API definition whatever it
is they should be automatically updated
and we actually call this a dynamic
configuration or dynamic reconfiguration
of services
and finally since this is actually a
very very brittle system you should
provide high availability meaning and
that you should have multiple copies of
each service deployed on several
machines in the case of a partial
failure so here is an example
architecture on the left you see the
clients they are making requests to the
load balancer various requests are noted
by different tags and the load balancer
may distribute these requests to the
servers on the backend and due to the
nature of the requests several services
could be involved in fulfilling them you
could actually achieve high availability
by duplicating micro services here so
you see I have two whites and two purple
micro services here in the system so
basically they are redundant there also
could be event sources in the system for
example here we have a publisher where
we publish events to the to the system
in general or even to the clients
themselves so this is actually a very
minimal micro service architecture and
in in real life you would have maybe 10
or maybe 15 different services lined up
to satisfy a request all right so now
that we have the requirements and a
sample architecture here is a library
that can make it possible code is a
battle-tested and production rate
library for node.js that satisfies all
of these requirements with zero
configuration yeah that's true it lets
developers create scalable applications
distributed applications and it's
extremely fault tolerant here's a
breakdown of co-chairs features it
basically enables auto discovery by a IP
broadcast or IP multicast it forms a
peer-to-peer mesh network between your
services it also comes with a full suite
of library
to enable every use case ranging from
classical requests classical request
response patterns to more dynamic pops
up applications publisher subscriber
pattern applications and it even has a
reserved configuration client connection
through WebSockets which means your
clients whether they are a mobile app or
a web app would be notified of the
changes in your system in real time
through WebSockets without any effort on
either part of the system but this is
actually enough talk so let's move on to
a short code demo to see how it fares in
action so first let me go to the example
code so I have two applications here one
is a service and the other one is a
client so this is all the source code
necessary to build a micro services
application in node because if you
remember I said it's basically zero
configuration so here my service listens
to ping events and whenever it receives
a ping event it replies the bit of pong
and I have a client here which
periodically every 2 seconds
sends ping requests to the system to
whoever is there it doesn't care about
if somebody is there if somebody will
respond to it and it just logs the
response in return so it's a very simple
application and you see there are no
configurations at all apart from the
names which are there for documentation
purposes so that basically you don't
lose track of your hundreds of services
so let's see this in action so I'm gonna
run a service here
up and running and it has a unique ID
and I had a client here so it's up and
running it's sending pings and receiving
poems you may notice that they actually
discovered each other the server the
server the or the service discovered the
client and the client discovered the
service so they are running on separate
processes so although they are running
locally they could also as well run in
any cloud but what happens when I want
to scale I just go to another machine
and just spin up a new service and let
me clear those and here you see they
also discover each other and now you'll
see that the ukís are basically load
balance between two services here and
it's a round-robin load balancing so it
will either go to the one at the top or
in the middle and if I introduce yet
another server into the system they also
again discover themselves automatically
and reconfigure themselves automatically
now that you'll see the requests will be
load balanced between three different
services so the first one the second one
and the third one this could go on
forever and basically you could also
spin new client so there could be a new
client in system every one will also
discover it and it also can make
requests and the requests will also be
load balanced between these three
services so this is a very very brief
intro to what code can do for you in a
request response application so perfect
now we see that it's extremely easy to
run locally but the question is how do
we deploy and run this in the cloud it
has to be effortless
as effortless as possible just like
we're running locally and fortunately
with docker cloud it almost is as
effortless docker cloud is Dockers
platform for managing door applications
in your own cloud servers it basically
that lets you bring in your servers
whether they are virtual or physical or
you may give it an AWS key or a digital
ocean account or Google cloud account
whatever and it even creates and manages
those servers for you so everything is
automatic in that sense there are
basically two main ideas behind docker
cloud one is infrastructure as code and
it is basically built on top of talkers
philosophy and the other idea is the
pets versus cattle approach
infrastructure is code means that your
application and your deployment
definitions and all your configuration
about your servers and services and
applications are actually written out as
code and they can be version controlled
and whenever you want to make a change
in the system you make a change in these
code files and any management
application then goes and reads these
source code and does whatever is
necessary like it may spin up a new
server or it may scale your services or
it might introduce a new component in
the system but the idea is you can track
everything as code and you don't
actually manually do anything to manage
your servers like by a stage or anything
the second idea is pets versus cattle
and it relates to the identification of
our servers traditionally we treated our
servers as pets they had unique and
almost always funny names and we
manually SS aged into them we kept a
list of IPs and we manually deployed our
keys there so we could SSH into them and
we would manually configure them and you
know deploy for
coating them or like whenever there's a
new server coming up we would go and
manually do these stuff again or we
could have a an automated solution like
puppet to this for us but that's
actually a thing of the past now there
are no more server management and server
configuration at all now with docker and
micro services we can actually treat a
number of servers as kettle kettle means
a group of animals basically
domesticated animals which you only know
the number of and they are almost
identical you don't care about
individual animals there and pets are
you know like cats and dogs domesticated
animals in your house where they're part
of your family and you care about them
you take care of them daily so in this
case we don't know the names of our
servers we don't tend tent them
individually and they are similar in
nature and whenever a server dies we
just don't care that we create anyone
because we have our definition as code
and it's actually automatic to create a
new server of a typical architecture so
we just don't care if we lose a server
or not we just spin up a new one
as soon as we get notified of it so dr.
cloud lets us create a catalog servers
with any crowd any cloud provider we
choose you could have AWS you could have
just lotion you could have booked cloud
you could have measurer or any cloud
provider or you could also bring in your
own nodes from other suppliers other
providers or your own hardware it then
automatically manages and scales our
apps it can listen to github or
bitbucket repositories and automatically
build your application and give the new
version of your application whenever
there's a push and as a result it can
redeploy and restart your application
with the newest version so it's a
totally automated process
you do nothing but define the initial
infrastructure of your servers and
services and then just let dr. cloud
handle everything for you and if if you
host your code with get up it's a lot a
lot easier
of course best of all it has a cool
graphical user interface a web
application and a nifty CLI tool to help
ease management so basically you don't
have to manually SSH into servers and
you don't have to deal with the terminal
you can just do everything from the user
interface and it's all sync and it's
perfect whatever where you choose
so again demo time this time we will
demo a already scaled application in in
docker cloud and I have set up a an
example architecture here an example app
and example servers and clusters and
nodes just because you know demos might
fail and I want to do a a proper working
demo first before I move on with an
actual implementation so let's first go
through the code base a by the way all
of this source code here are actually
available on github and you'll be able
to find the links at the end of the
presentation and so that you can fork
them clone them whatever and play with
them at home it's totally safe to do so
so building microservices applications
with code is good but we also know that
a lot of the real world use cases
include REST API HTTP servers so in this
example we start with and barebone
Express server it's a very simple HTTP
server basically and it acts as a REST
API for
our servers for our services and our
application so here is the API server
the first thing to note is it's an
Express app it has two endpoints one is
to just say hello and the other one
basically gives you the time now if this
were a monolithic application you would
do rest at send new bait and the server
would also be responsible from actually
replying the request but I already had
micro services built-in so I have a time
request through here much like the one I
demoed earlier whenever there's a new
time request coming in from the HTTP
server it sends this time request to the
system it doesn't care if anybody will
respond of course it's our
responsibility to make sure that there
are some time services out there to
basically handle this request and on the
callback it just sends whatever the
answer is to to the client and we are
listening on port 3000 as as usual so
what do we have as a time service here
it is very much like the point server or
Pronk service we saw earlier so it
responds to time events and whenever
there is a time event
it creates an update append some
information to it and just logs it and
of course cause the callback so that the
client gets the response so we have the
source information the timing
information that we're interested in and
a random number which is there only for
demonstration purposes so the source
gives you a name time service at
hostname currently I will first demo
this running in in my own machine
locally here so there will be only one
hostname but it will prove useful when
it's running in the cloud because you
will have many many different services
different servers and it will be easy
for us to differentiate where the
response is coming from
so let's first see it actually oh that's
bad I hate the install Express hope
that's fast enough okay while that is
loading let's go back and discover what
we have for docker cloud so in order to
run these on docker cloud the first
thing I need is a docker file definition
because I'm gonna build images out of
these these services so here is an
example very simple docker file it runs
on Alpine Lea Alpine node which is no
distribution built on Alpine Linux it's
18 megabytes of size so it's extremely
lightweight and it's good for production
uses I'm just adding whatever I have as
source code to a dictionary to a folder
called source and I'm just running MPM
install as I did before which basically
gives me my libraries my dependencies I
could have many other installations here
or other tests or stuff but this is
enough for the for the demo the only
thing to notice here is in order to run
code in local cloud we actually need
some configuration actually local cloud
needs some configuration we have to use
host names and we have to use multicast
instead of IP broadcast because that's
only that's the only thing doctor
lets us use in in their system so I have
the dependencies installed here so right
well you see I have an API server and a
time service there loaded and just curl
local owed 3000 it will give me hello
world and time and you see that the time
service is responding to me with the
source time information and a randomized
number so as I demoed earlier I could
just go and create another service here
and you'll see that there load-balanced
the service at the top answers and then
the service at the bottom answers so
they're both balanced again in a round
robin fashion so let's see how this
works in in the cloud oh I have set up
an environment here as a demo I'm using
judoka cloud CLI but I've lost the demo
the user interface I think it's just
easier to demonstrate whatever we have
here so I have two node clusters node
clusters are a group of servers
dedicated for a single purpose they're
like cattle so I have a node cluster for
the proxy server or the load balancer
which sits in front of my architecture
if you remember the the graphic the
image that I showed earlier it's the the
first server the clients taught to and
then I have another node cluster called
services which will basically host my
application and currently I have three
computers three servers in
not cluster so here's a list of notes
that I have you see they have their IDs
as host names but we don't really care
what they are there are unique IDs new
IDs and you see I have three servers
deployed and a proxy server deployed and
here's a declaration of my services
basically I have a time service I have
an API server and I have a proxy now the
proxy is there because it might load
balance between various with between
multiple API servers and then the API
servers distribute those requests to
time services but the first thing I want
to do is just to purl it and see so I
have a host definition for API that
cloud that points to the proxy server
here and I'm just scrolling it and I see
that I'm actually receiving different
responses like this is coming from time
for respond the other one is coming from
time service to and time service tree if
I go on I'll just keep hitting them in a
round robin fashion I could actually
also log whatever I see here totaku
cloud service logs prime service will
give me every every log that is being
logged from those services so let me
clear them so yeah
hmmm that is actually very interesting
all right we see different logs from
time services times 4 is 1 2 &amp;amp; 3
and hopefully if there are no problems
in Doki cloud I should be seeing when
I'm hitting them but looks like there is
a connection problem right now anyway ok
so again it more or less works and if
you mind that this is a demo and if you
play with it
Oakley or in docker cloud hopefully it
will be much better so let's then go to
the user interface to see what we have
there the first thing I need is a
repository so right now I'm hosting this
code on github and I've actually created
a repository here I'm demonstrating it
again for like your demo I can make it a
private or public grapple public image
so that other people could also pull it
but I want this to be ok anyway I'm
gonna make it private public because I
only have one private and demo so here
is a repository the next thing I'm doing
is I'm creating an automated field so
I'm gonna link my github account my
github account is already linked but I'm
gonna link a repository here so I'm
defining the source repository I can
either build this on my own servers
declared in da quickly
or I could let them build it for me so I
have three options here they're actually
right now free but I guess they'll be
they'll require a subscription once
they're out of beta and the most
important thing here is tagging so by
default it builds whatever you have on
the master branch so whenever you make a
new push to the master branch it will
automatically build you see here a
notable feature it will automatically
build it for you basically this is this
could be useful for for example testing
your images testing your application
but right now I'm demoing my actual
application and my actual services so I
will move with tags here because I
prefer it but whenever I tagged a commit
with API if we create a new image and
also take it with API I will also
introduce another tag here and call it
time so basically right now I have two
different tags for my services now one
of the biggest challenges of a micro
services application is how do you
separate your services how do you
separate their source code an idea is to
basically build them in different git
repositories and then build them out of
those repositories but I think it's a
little bit hard to manage and it's a
stretch when you have tens of services
because you're doing everything from
from scratch every time you're you have
packaged chases that look a lot like
each other you and you have the same
dependencies it's just increasing the
size of your local repository so what I
prefer to do is I keep them on different
directories they are in a single
repository they share the same package
Jason but I differentiate them with tags
so whenever I'm making a new commit to a
time service or an API server I just tag
that
commit bid API or time label so that dr.
cloud is basically notified of the
change and can only build that specific
image so I want to build on my own note
and yeah that's it I could just save it
now this is basically the same screen as
I have in the webinar demo that's
already set up I set up two different
tags for two different services remember
I have a tag dedicated to each service
so and I have an API server and I have a
time service so I have two tags for them
and I have automated fields and you see
that I have already played with it today
so I can just go back and delete the
other one all right
you can also see your mode clusters here
you can also create new node clusters
directly from the GUI
you could choose your provider the
lotion Frankfurt and let's say to two
gigabytes and you could also say how
many strawberries how many nodes you
want the cloud also has a REST API so
you could also do these from the REST
API - it doesn't matter it's extremely
flexible or you could see your
individual nodes this is much like the
eternal screen that we saw earlier and
my services again or my individual
containers so I have one proxy container
which is running a chai proxy one API
server which is running a webinar demo
API tag and three time services which
are running webinar demo time tag so now
let's go back to the code base and see
the necessary configuration files for
dr. cloud it's actually extremely easy
and it's similar to doc
Campos file here you see the dr. Campos
file related to this system it has an
API server it has a time service and it
has a proxy in front of them to
basically manage and load balanced API
servers so the proxy is the only public
thing in this service and API server and
time services are hidden behind the
proxy and they are private there they
only talk to each other and they don't
have any public IP address or any public
communication at all so your system is
basically a little bit safer let's say
let's put it that way so a docker cloud
file is actually very similar to a
docker compost pile
but it's not exactly the same I know
that docker is trying to combine them
and hopefully it will happen soon but
currently we have a somewhat different
declaration here the first thing to
notice is we don't write any version or
services lines we directly start with
these service definitions so here is a
service definition for API server the
first thing to note is maybe the command
this is the comment that I ran locally
so it is the exactly the same command
that I'm running in my cloud environment
no to API server or you could just say
no the API server slash index at yes but
since its default I prefer to skip it
and then there's the image this is the
image that is being automatically built
by dr. cloud so it is code demo slash
webinar demo and as I've already
mentioned it has the API tag to track
changes to this API server source code
it is publishing a port 3000 which means
it's listening to this port and whenever
there is a new request coming for from
the clients it should be redirected to
this port it should always we start if
it fails or if for another reason this
container stops because
it is one of the one of the main
elements of my system so I want it to be
there all the time
another thing is it has a special tag
and that tag is not cluster name
services so if we go back to our north
cluster definition here you see we had a
services definition and services not
cluster and approximate cluster so I
should be deploying the proxy to this
one and all my services to this cluster
it's important because this cluster
holds the server whose IP address I'm
interested in remember I have a host
definition here API that cloud and this
DNS of this host name will resolve to
the IP address of that proxy server so
it's important to to know which server
it will go the the proxy will go so for
example the proxy here is deployed to
the proxy cluster and the services is
deployed to the services cluster of
course I could remove this and it would
also deploy to the proxy servers but I
don't want that I don't want them to be
intermingled and overlapping let the
proxy servers and alone at the proxy
container for now so one very important
feature here is the auto redeploy which
means whenever there is a new image
called code demo slash webinar demo with
a tag API just redeploy the server the
service it means it is it will kill
whatever current running API servers are
and redeploy them with the new image
which means with the new source code so
it's continuous delivery is automatic
continuous delivery basically for free
here you have the time service
definition again it has a auto redeploy
feature it has its command this time
we're running no time service it's
exactly the same comment that I ran
locally I have another image
kote demo act actually it's the same
image is the same source code but with a
different tag which means this contains
the latest changes for my time service
so the distinction here is basically
whenever I'm making a change the time
service I only want to deploy the time
service I don't want to redeploy the API
server because I didn't change it so I'm
tagging it taking that specific comment
with time so that local cloud knows to
only to redeploy this I'm Travis and it
will not really employ the API server
which is extremely cool so you can
basically replace parts of your system
with zero downtime it's again we
starting always and if we have a proxy
definition which is dr. cloud H a proxy
it is actually a special image from
local cloud which can also be used
locally but it's special for the big lot
because it can make use of the dr. cloud
API and it can automatically listen to
service changes for example whether
there's a new API server coming on board
so there is a link here I'm linking it
to API server so that it watches the
changes of API server I could just
deploy two different API servers and it
will just work it would load balance
between them and those API servers would
load balance in turn between time
services so it's a perfect horizontally
scalable solution but for that to work
for that
Doka Doka cloud API to work you need to
give it a specific role which is called
global in the clouds case if you don't
put this here it won't work it's much
like attaching the docker socket locally
here in in docker compose file we were
attaching the docker socket for
a proxy to listen to and in in dr. cloud
itself we don't need that but we need a
deliberate role a global API role for it
to listen to API events and then I'm
taking this as the proxy not cluster so
that it will only deploy to the north
cluster called proxy so one last thing
to note here is the environment variable
in the API server basically I have an a
my environment variable called virtual
host and it keeps me from keeping any
parts coltd configuration for H a proxy
or you could use nginx basically for a
proxy normally traditionally you have
multiple virtual host declarations and
configurations for those software but
with with docker cloud and the specific
age a proxy image you don't need any
configuration at all every service that
comes onboard just can say that hey I
want to be treated as a virtual host I'm
listening to this API that cloud and
whenever there's a request coming to
this host just let me know
I should be responsible for replying to
that and then docker cloud H a proxy
image is or container is looking for new
containers in the system and whenever it
sees okay there's a future there's a new
container which has a environment
variable called virtual host okay I'm
gonna set up a new virtual host and
redirect every request that's coming to
this host to this container which is in
our case the API server so this is
basically all the configuration that you
need to do for local cloud and then the
rest is basically automatic and flawless
so I'm gonna try to demo it for you from
the from scratch from the beginning so
the first thing I'm gonna do is I'm
gonna create a new node cluster called
props it to and a new service north
cluster called services to the cloud not
cluster
maybe I should yeah so okay this is the
first definition let's say
so the Kukla node cluster creates minus
t3 which means create three servers for
me or we could say in two and this time
the name services to I wanted to be
created in visual ocean Frankfurt and it
should consist of long gigabyte Tories
so if we create to one gigabyte service
for me and I should be seeing that up
here in my note clusters this is a watch
command so it watches for changes so
okay perfect here you see right now we
have an employee and then it's deploying
and hopefully it'll it'll succeed so
Tucker cloud not cluster remove services
I don't remove that one I don't need it
at the moment and I'm gonna remove proxy
to you see they're dating in real time
like it's terminating right now
services are terminated and if I make a
call request right now we don't work and
you see here I have my nodes and they
are also terminated I have my services
they're also terminated they're not
running so I have services to and now I
should create proxy I have actually no
alliance for okay cloud comment so it's
DC not cluster create proxy
to this location Frankfurt 1 and when we
divide alright so here you see I have
another server here deploying and looks
like services are deployed these sources
to not cluster is deployed so if you go
back to our stack definition this time
I'm gonna do it here I have the stack
cloud and it's not running this is the
GUI so I'm doing this so that you can
see that you can switch between the
terminal or the web application in real
time so they are not running but if we
go to the stack definition here you will
see that the node cluster names are
services and proxy now I have new node
clusters services - and proxy - so I
should change those those tags and I'm
gonna do it here I'm going to edit my
API server and here you have the
deployment constraints I'm just gonna
say I want to deploy it to services -
and save the changes that's all I'm
gonna do that do the same thing to the
proxy edit it and you see currently it's
it will try to deploy its proxy but it's
not there so it should deploy proxy -
all right
let's see if my proxy 2 is also deployed
yes I have my servers deployed I have 4
sorry 3 servers right now - on services
- and one on proxy - and the only thing
I need to do is to redeploy this cloud
or let's say start this cloud hmm ok um
there's one left time service I should
update that one - yeah Save Changes and
it is starting you could also see the
time line here and any logs that this
action causes so it's downloading my
images in the new servers and it's
creating my services basically so in the
meantime I should go back and look for
the IP address of that proxy server so
what I'm gonna do is here are my notes I
have 4 nodes and this is the note that
I'm interested in it's it hosts my props
it will host my proxy so I'm gonna say
ok cloud know inspect and kill the ivy
and hopefully it will return to me its
public IP
Oh looks like it's the same IP address
after all all right so we see our
services they're all your running up and
running on the new servers so let's see
if this thing will work it probably
didn't deploy
hmm actually I got the IP address wrong
so let's try that one yeah I never
believed the same ip address anyway so
things let's see yeah actually works if
the logs are working this time I should
follow them yep you see they're updating
in real time so oh I have log to my
services perfect now we're actually two
remaining things that I wanted em out
and that is scaling these services let's
say I want to introduce three new time
services because there's too much load
on my system what I'm gonna do is I'm
just gonna write local cloud service
scale I'm service and I think it's six
this time actually let me watch for
double cloud events here from the RB
results to the for the actions you see
they are already deployed I have six
containers here and new time responders
are actually discovered and online yeah
I'm sure it's four to one five six
perfect
I just scared my system and okay but I
still have a limited set of servers
let's try to scale our node cluster
services so that it has five different
hosts so it's also actually very simple
double cloud node cluster skill services
- and let's say five all right
here I see the introduction of tuning
services and tuning observers here two
new nodes I'm watching for the nodes
here you see they are deploying actually
and whenever they are deployed I'll just
kind of scale my service again so all
right
actually there's only one more thing
let's see the random nature okay so what
I want to do is I'm gonna remove that
random message from my time responder
and let let's demonstrate the automatic
building of images and deployments so
what I'm going to do is I'm gonna commit
this and I'm gonna push it
I'm gonna tag the last commit has time I
have to force it because I had already
tagged it back the previous commit s
time and I'm just gonna push it push
tags and force to update my repository
so let's see if this is actually
building it
so my repository list webinar demo and
yes bingo it's building the API is still
stale because I didn't make any change
to it but it's building my time service
which is perfect actually and you will
see that whenever it's built it will
just redeploy all the services that are
listening to this specific tag on the
image and bam you don't need to do
anything else you just push a tag and
it's rebuilt its redeployed
automatically to all your machines in
all your clusters and you didn't
actually have to use Travis Jenkins or
any other CI or CDA solution
it's just get a Tokra cloud and your
servers amazing technology so let's go
back here ok my extra servers are also
deployed and we will soon see that my
services here my first definitions will
be redeployed and they will be running
again as soon as my image is built ok
there we see it the build is completed I
guess yes it is successful and the
images are really pulling so yeah I
still have the random message and there
you see it here time service 4 6 5 they
all had the random message in themselves
and time service one doesn't have it um
3 also didn't restart yet so it will -
so until all your containers basically
we start with the new code base you will
have you will have two different
versions running in production in the
same time it's basically a very fault
tolerant deployment procedure and there
are different strategies for it also in
dr. cloud - so you should check them out
to find which one suits your suits you
best
so yeah in the meantime they will all
redeploy and retrigger and I will have
all of them running with latest source
code you see the time services actually
already playing right now and we could
also to container PS which would give me
the containers in my system which are
running which are terminating so you see
5 is actually updated before it had the
random number and right now it doesn't
ok perfect so this is actually the end
of my demo and today's webinar and I
thank you for joining me today for this
brief demo on docker cloud and micro
services with nodejs
um the slides are available in slides
that come in this link the codes are
available on github we have a slack
community for code which you can join us
and it would be interesting to see you
there if you could help us and this is
my contact information if you have any
questions at all related to these stuff
so thank you again thank you very much a
month
this is always a pleasure actually looks
like you did a very good at explaining
it nobody got lost and confused which is
a awesome part and yeah I just want to
say thank you very much to everyone that
participated again a big THANK YOU to
Armand as always these videos will be
online so people that are coming late do
feel free just to you know watch my own
step so if you are watching him now why
are the doors just refresh it and keep
the link as I mentioned and previous
webinars so people were coming late or
do have any questions you can always ask
me and if I'm one has increased free
time I can always just ask him to save
me to the answer back Marta - and yeah
basically
thank you very much a month and to
everyone else basically have a kid have
an awesome day Cheers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>