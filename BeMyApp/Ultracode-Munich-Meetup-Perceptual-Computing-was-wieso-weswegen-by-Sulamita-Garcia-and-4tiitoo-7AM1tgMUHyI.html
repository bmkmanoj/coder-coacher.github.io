<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>[Ultracode Munich Meetup] Perceptual Computing: was, wieso, weswegen? by Sulamita Garcia and 4tiitoo | Coder Coacher - Coaching Coders</title><meta content="[Ultracode Munich Meetup] Perceptual Computing: was, wieso, weswegen? by Sulamita Garcia and 4tiitoo - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>[Ultracode Munich Meetup] Perceptual Computing: was, wieso, weswegen? by Sulamita Garcia and 4tiitoo</b></h2><h5 class="post__date">2013-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7AM1tgMUHyI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my ego foil so my name is zulemita can
find me on Twitter so like I see it I'm
Brazilian I don't speak German I would
use oh I grow up in a call a city called
florianópolis which is X to be an
island on the south of Brazil and this
was the kind of environment that I grew
up with later I moved to San Paulo which
is a huge City one of the third it's the
third biggest city in the world and I
love big cities and where I started
working point 0 as community manager for
politan America and then I moved to
London I also love big cities and then I
moved to hot and you can see the
scenario from where I started to wear
mentally it's quite different ah on my
free time I like to sink although I'm
still taking classes to try crazy nuit
and new adventures that was my my
birthday gift for myself three years ago
and also try to do some weird escapes so
today my lovely partner went to a travel
very early in lock me inside apartment
on first floor and took my keys and I
had to find their way out and it wasn't
a something like that but it was similar
I have to so because I had to come here
to talk to you yesterday I don't know
well we gonna have a conversation later
so what I'm here today is to talk about
perceptual computing so why are we in
town investing a perceptual computing we
just released the gold fields of the SDK
list last week it's free the camera that
we work with it's not a edge not free
but the SDK which is the technologies
free so the camera is made up
creative by the SDK you can download and
use it and it's including face
recognition gesture recognition
attribution voice recognition 3d track
so the main difference between the
technologies that you guys might be
thinking or maybe have user ready with
kinetic is that kinetic is long-range
you have to have some space and he can
detect limps so it detects your arm or
your hand but it doesn't detect your
fingers this is to be used on the
desktop it's close range from 15 to 1
meter and can detect up to 10 fingers in
front of that and can track the the
motions of the fingers besides face
recognition also has some algorithm
trying to get at this point it's it's
not fully an exact but it's already
trying having a good result in guessing
gender age and some ratio expressions
like smile or closed eyes on the
tracking recognitions the gesture we
have a lot of gesture later I'm from the
show on this and this is everything like
you mentioned before to have a more
natural relationship with the with the
computer so instead of having a token
I'm also studying psychology so there is
this idea of a token when you have
something in between you and objective
you could become a bit detached from it
so when you have to interact with the
keyboard or the mouse to have something
happening in a screen you are detached
from that when you have a gesture or a
touch and it's automatically responding
to your gesture you'll be coming or
attached to this and people really
respond really well to DC direct
interactions so the algorithms can be
can be extended you can use that it can
create your own gestures and
I mentioned you have voice recognition
and the idea is to be really be immersed
on the experience is to forget about
that I forget about how exactly are you
doing but to just be immersed on
experience in using your whole body as a
control the camera is it's a 3d camera
has infrared to detect that so for some
examples you can use a regular camera
for face detection just to detect a face
to identify but for face recognition for
mapping for gestures you need to def so
you need inherit and then that's that's
where did Nick ever is this size is via
USB and it costs 100 150 dollars and you
can order on until the ground software
perceptual and the same the same place
that you can download the SDK so going a
little bit deeper on what you're going
to find when you have the sto when you
download and install the SDK so on you
already have several samples that are on
the interview and we can go through all
of them where you can see some of most
of the features and sdk and on the
sample director you actually have the
source code from all these samples so
you can see how to implement gesture how
to implement face recognition how to
implement your own gestures there is a
lot of documentation as well how to how
the face attribution work so it's open
you have several directories with the
documentation with the libraries with
some some information and so you can get
started there detector well here is the
applications then there is the SDK at a
framework of mineral interface and the
lower level I think this one explains
better so on the very low level of the
SDK you have the you have the
interactions that you have with
testicles so you have a module for
images for audio for gesture basic
analysis but this is it's a lot more
work tool to deal with so we recommend
to use the YouTube pipeline or in the
YouTube capture that implements most of
the things that you need yeah that's
that's why I also be doing singing
because I I speak so much and I forget
to breathe um everything is done native
in CPS c plus plus but you have the
c-sharp port the processing
openframeworks and the unity so we also
have on the websites on software that I
telecom we have some some training on
how to use one example of this week we
unity and if you are interested I could
sure go later where to find it okay and
please if you have any questions just
shout it out so how the gesture
recognition works so we have us a few
structures so for instance the post
images that helps you to separate the
background from the heads you have the
Geo note at our points so later I'm
going to connect up the on the on the
Decameron show for instance when you
have a hand detected by the camera you
have several points in the hand so you
have a point here on the on the wrist
you have a point in the middle that is
doesn't really mean that the middle of
your hand actually means the grabbing
point so when you
when you're closing around where is the
the point that is is being grabbed you
have one point for the beginning and one
point for the ending of each finger and
this is our old checkable you also have
identification of if-then hand is
completely open if it's closed and and
so on we also have a few gestures
predefined so thumbs up thumbs down
peace just the hands open waving a
circle swipe left or swipe right and
also there is some alerts predefined
about gestures so as I mentioned there
is Tracking's in each point and on these
structures you can also have
identification for each finger the time
the index the middle the ring and the
drinking although when you have a less
than five five fingers EK doesn't ask
you is not very precise so and most
recommended is to use just hand finger
tip if you don't need exactly one of
those fingers and also when you have a
closed hand your hand they've had the
points so this is using the three
dimensions so using that as well to
track not and so this is that the most
things that you need to know about
gesture so a simple hello world like you
mentioned if you use it by playing then
you have you have a lot of the
initializations on the holding and
everything done for have for you so you
just have to use your instant shade oh
by the way how many of you are C++
developers
yeah yeah should have a sit well so far
it's well we had this ish in c-sharp
okay so we need a question their next
time so I can prepare the Russian um
well but there's a way right Wow let's
pretend that this is some algorithm so
the thing is you have the class I flight
that is going to take care of a lot of
underlying things for you and you have
when you have the gesture as you have
the labels how you did there is some
jets to happen when you use the pipeline
you're going to receive no mysteries
gestures coming here on the on the data
structure so then you have to analyze
the label and then you find what kind of
gesture was that so you have to swipe
left swipe right up down swipe up or
down or thumbs up thumbs down and then
you do something this is again something
that you can do its to refresh the
screen and yeah so there's some some
structures that they are necessary um
I'll now I'm going to go into the visual
studio so here is the the simple code
that this is included on their own there
yeah it's official this is included on
the SDK when you download it like I
mentioned once you downloaded it I think
it placed a shortcut in here ok so when
you install the SDK you have here and
see
program intel pc sdk so here you have
everything that we saw on your own on
this delaesh on the beaner you have the
samples and on the samples you have the
source code gesture so you have all the
source code of you that you can check
later now I'm going to also connect the
camera here so we can see what it's all
about
so que onda gesture buren so what is
doing here is he redefines what its
blanket and what is getting close so the
further I go the less you recognize this
you can recognize up to 10 fingers and
track the position so if you see on the
side you can see a blue and they when I
have it the two hands in here the blue
bar that means if you disclose it or
open in the green bar so you also have
the predefined gestures and you can see
that what is he doing in you is
recognizing something and just showing
me on the top what I'm doing so how is
this doing what is is doing so this was
kind of the hello word that we saw and
if we go here on the render because this
is just a recognized just the simple is
Justin associating what is supposed to
to shade the pipeline I'll frame and
then on the gesture we can just liquid
if you want just enjoy this here and
detect what kind of gesture it is and do
something and select files or a lil you
know passage slides or do whatever we
want if we go here in the render here
you was associated every every gesture
with an identification and then on the
in their pictures and then
doing a lot of more things and any here
when rendering all this instant showing
associated pictures with with the on the
top of the screen so it's not it's not
that straightforward but it's also not
that complicated because you already
have the gestures you might have to to
work with some other structures on the
on the on the last decade but the main
point which is tracking the gesture
because to track adjuster you have to
know what is where everything was and
see whatever all the track all the
military movements and then this finder
if there was a gesture to what direction
in one kind of gesture so this is
already done um some other tricky
examples might be the face the bell the
archer good detection so we still being
checked let's see what it is oh
oh this case is just an identification
number but you can save this number and
save face and and have some yeah I
should be fun yeah I don't know what my
hair what yeah so there is some voice
recognition I it's still rickety doesn't
like Adsense but this is something that
we started to to work you know it's it's
out oh you on their website you can find
the developers and talk directly with
engineers are developed is to do any
edge west and information are asked any
questions and start using and eat the
feedback yeah what I was going to
present them is there any kind of place
you came from using a browser I mean I
beside unity so maybe plugins for all
and push one that's a lot oh my god so
there is my eyes without the other
challenge is working on the unit
you home hi pleasure to be here actually
it shouldn't be me I am NOT protecting
your garden or company but i'll try to
go as far into details if you have any
questions little bit about the
backgrounds and I've about our company I
don't have an ego slide reason for
learning some followed company so you
become your one company we actually
started the company in 2006-2007
targeting and back then we said
computing is great everybody of us does
everything there are taxes for most of
the people so we decided they should be
like a touchscreen device like a tablet
and you should have a widget interface
which weighs just say it's shopping and
you just click there and go there and
that was before iphone and ipad and
everything and the device that came out
of that drug metab something might
remember it we did it together was a
problem company in berlin he actually
did the product they did the marketing
on that didn't go that well short so in
2011 actually we decided well tablet
market is going to march into a
technological in patent war between the
large companies and there's no space for
a young technology company that
basically is focused on interaction
design so we asked ourselves what to do
and the next area where we arrived it
was actually that now the tablets are
there we believe that interaction was a
computer should become a lot more
natural so since I'm not the technical
guy on the team but so I cannot go into
the coding details what I would like to
do is give you a little bit of our
vision of where things will evolve and
one hand says I tracking about us but
actually what we do is we have econ
technology new your natural user
interaction and we actually combined all
kinds of sensors so we use a perceptual
SDK as an abstraction layer texts all
the sensors
like we are straight across that way so
we don't have to connect to different
cameras that the creative cannibal
camera or whatever engine the lenovo
device we just use that for gesture for
speech input and on the other hand we do
a lot of things I trek and basic reason
for that is that we believe that I
tracking has one substantial advantage
compared to things like gesture and that
actually doesn't fit what element also
yes the big advantage of of of gaze is
actually that you know which object you
want to interact with and one simple
example is imagine seeing when you're
having breakfast in the mall and you
want to have a button on the other side
of the table but two people are talking
to don't want to interrupt them at the
moment you start looking at everybody of
you or everybody would recognize what
you're looking at some people might even
end or directly because I understand
what you want and we believe or a new
technology today and computers are
actually capable of doing exactly that
behaving a lot closer to me to how
humans intact and so from our
perspective it's kind of actually feels
a little bit of like artificial
intelligent intelligence if you do it
right and that's where we want to alive
the reason why we combine it with
gestures quite simple because here if I
do a sliding gesture the computer
obviously understand i wanted to the sly
now if for example i have a image full
of or ever photo organizing application
if i do a gesture the computer would not
understand what image i wanted that so
if i combine that with my gaze and the
information which image i'm actually
looking it I could just flip and flip it
into my selection like throw it away I
could rotate it and I can suddenly in
tank was a lot more information or was a
lot more content that is what usually
possible with gestures well very call of
a mouse pointer and they moved to
something and then attack was
type of indirection as we call it is not
really help really helping user
experiences like I understand in front
of your connective trying to make it
work bending around to move the cursor
it's just not fun and we think it should
be fun computers can be a lot more fun
and maybe one of the reasons why we
started to work on this is because I
never long time finger typing I don't
like to use my mouse and it's just
something we that was invented 20 years
ago you more and I was put back then but
I hardly think that it's going to step
on one other interesting aspect about
what we are doing is as I said we're
connecting to all these sensors what we
decided is that to make that work we
need one common platform in between two
intact or to communicate with all these
types of sensors because for most of us
the code we're just seeing connecting to
all the sensors and one by one is is the
hassle we don't need and in many
situations we actually just want to have
a scroll box that automatically scroll
is going to look at the bottom of it or
that also will take the input if I just
sy it was my hand and I would prefer to
just what many of us would just prefer
to it to take that scroll works we work
from there rather than to bring all the
sensors in and make sure that the
interaction is the same and apart from
that from the developer perspective it's
also a user experience parent
perspective where we all want to have
one common interaction model working
with all your senses so the New York or
is is one of the ideas we have around
that if Stefan would be here we could go
a little bit more in detail how
everything is set up I won't do that if
you have any questions we'd better to
convert them but the interesting the air
is different but the interesting thing
is what we actually did is we have one
part that's completely behaving like
Nesta case or you see buying HC onions
launcher c sharp c plus plus two
see you can also use it in matlab and
java and so on so we've got all these
possibilities to use it like a standard
sdk but one of the bigger problems we
were facing is how to create enough
content the moment people start having
devices with all these sensors so we
created a simple rule based language
which allows us to define areas in for
example the photoshop application and
say okay this is the area where you work
with the content and this is the area
where you have the layers and so a
common situation you're pointing with
your mouse your ability want to do
something in suddenly you realize well
I'm in the wrong layer of photoshop so
long when you go back with your mouse
and change land go back to the point you
have to go to that exact point where you
wanted to do something and what we don't
know is I'm at that point I just look at
the lair I automatically get a second
mouse pointer we're just change data and
then the most part of automatically
jumps back on where was before so I want
to hear with that is really to make
really easy task to an able application
that are already existing today so to do
something like enabling counter-strike
where we one of them okay we are all the
running and the other demo over there to
test it later on so you can try some
light reading try some some imagine
applique also some image applications so
the interesting the interesting approach
to that is really when you have
something like counter-strike and you
want to have the computer behave like
the center area is full control of the
mouse because we want to be very precise
and very fast if somebody runs into your
side and just look at it and give it a
shot automatically will fire the first
shot there and get that person center
and to create something like that it
will take you half a day maybe a day to
create an extension onto an existing
application you have no access to the
source code whatsoever so that's that's
one of the solutions are we how we are
we want to make it really easy for
people to enable a lot of applications
and that it might not be directly
focused on you because most of you
probably are
deeper down in the development but if
somebody is really good at i'm not using
photoshop or any application there is an
easy way now to use his knowledge and
combine it with the census by simply
make enabling him to to create that kind
of interaction so whatever perfect so
now stepping a little bit away from the
variable programming side from from our
perspective dirias interesting thing in
combining all these sensors and i just
want to give you some ideas on what you
can do with a receptionist hey Otis
movie SDK and the interesting thing is
when you do not combining sensors some
you suddenly you can create user
experiences that go beyond mouse
pointers in case today most of our
programming is focused at single actions
most potent pointers move to some point
and get click there that's it but now if
for example i'm standing here that I'm
waving with my hand because I'm just
training what I'm talking to you and
whatever the computer actually should
not let typical situation when I do a
demonstration of devices and I'm sitting
there and computers in front of me
people are standing behind of me behind
me I start doing things with my hands
now suddenly when I combine it was eye
tracking the computer or for example if
i use face by permission of such as KR
we want to do it you should actually
teach the computer that if I'm not
looking at you I don't want this to
create something maybe only if I'm in
the presentation situation but that that
is things that create a lot more complex
situation actually to do to design user
interaction and the point where these
kind of device will be really getting to
the market it's still a little bit away
some of the reasons for that are that
has some challenges in the tracking
first of all but what we believe is that
this is the right point to start for the
developers to
with the technology because there's lot
of indications that this technology will
become available in the near future and
whoever understands that technology will
be able to create something we always
call it like the angry birds for
perceptual computing for financial
losses into action because touchscreens
created a completely new way of
interacting with zero with the computers
that allowed so many smaller companies
to come up with solutions that were not
there before and that were not done by
the big ones like Microsoft Adobe or
whatever usually doing the applications
but now with this change coming up it
was us preparing for that change that
will maybe happen in 12 16 18 months
from now maybe two to three years we
don't know what that's actually a short
time if you think about three years ago
then you come to a point where right now
we should start going with that's why I
really like the intelligence pushing the
topic that much and maybe just to give
you one final idea of what's possible
with in here and focus on our high
tracking since it's one of our main main
areas we are focusing in one of the
interesting thing that happens when you
use I drinking for example you go to the
computer and you you enter Intel and now
it comes up with a list of results that
grab some nights images some documents
you have about a flyball into the moment
you start looking at these documents you
ripple of you start selecting the things
that you might be interesting you
interested in and you focus just jumps
from one step that information now I was
involved both feet for around three to
five seconds can be used by the computer
to offer you to redefine or to focus on
different results like for example in
this example I'm obviously looking for a
picture which one huge was a yellow
highlight so my gaze is just jumping to
these in that situation the computer
could automatically oftenly to be
focused on yellow images with yellow
highlights and this could also appear at
a point on the display where I am not
looking at because you can notice I'm
right now looking at the bottom area of
the screen and laced on on on this
only looking at the okay the computer
cut them and resold and bring up all the
yellow images first because I might be
looking for some ways or something right
now was whether Germany sunrise
something also looking for so this is
actually what we believe will happen in
the future computers will be able to
respond to us and I hope many people
start working on because it's a lot of
work you don t have any questions oh yes
because I'm curious actually like so
you're tracking beautiful or you're
tracking you know what exactly because
for example my I ever really dark so I
don't you know like really should see my
people if I had like a light right here
or like how I school and is it out
already working like that accurately
that you can do layers of this kind of
thing like you know what company will
usually they'll show you an example that
totally works because it's like obvious
but like I'm other it doesn't work kind
of like overall or just specifically for
the tasks that you've chosen is
acknowledged disciples work together
with it was call me weak like told me
and as a mine that has been working life
I before long website how it works it's
actually it's infrared lighting and
reflections are seeing on your eyes so
it can can trackback pupils as well it
works on all about I depends on the
technology like ninety percent of the
people right now eighteen eighty to
ninety five something like that so it's
quite robust anybody is welcome to try
it behalf of human bombers that is
bifocals are not supportive so the
moment you have to focus areas in your
classes when you change your head
position suddenly that how the eyes have
used changes too much to and produced
good tracking but it should work be so
how I feel this this because you
mentioned this
find a gun selected two people truly
games you have like different layers
that I'd say you guys like I'm in the
distance how accurate is your point
silver that one for him meet me ground
why do you and right now you are
probably never even never use it for
head shop shut down if you use it
where's that the reason is quite simple
that gaze is actually or you gaze is
actually something that is very dynamic
first of all and there's some reason why
we why in general I tracking seems to be
limited to run about 0.5 degree
precision from the eye so on your laptop
you talking about ten to fifteen percent
so in many situations and that's again
where I said you have to think
intelligently how to solve things many
things for example suddenly rather than
working with a point precision using
occurs or how you got there is a lot
more relevant for example we don't have
a close buddies but if I would have a
close button up there making sure that I
exactly get the click or the input when
I look at that is really difficult but
whenever somebody has a fast path to
write up right upper corner left a pecan
apple there's a fastpass there and then
there's an input that they should be
happy happening in click most probably
it's close but not so let's trips we use
to make simple and that's also part
where we think in the new year SDK we
try to abstract many of these things do
you do like do local server lights
working and then like the comment like
how do you execute the comment just went
by just so or like if I bling and then
get the shots we did that but we did
that be annoying to
working with the technology for quite
some time it seems obvious in the
beginning it seems very tiring after a
few minutes and but how we do it depends
actually for example this if what you
see down here like the little minus
thing this is expanded view of this
scaife you know it's kind of fun or not
you see actually on the top area there's
a working area and there's a selection
of all the pictures we want to look at
that is below that so where the- right
now is we're actually in the normal
state your plus and you're all yet and
things are arranged them all things we
do this sex you know that so what
we do is Thome and these kind of actions
are triggered and I just look at that we
are more or less sure that you want to
take that button we maximize it and if
you just look at this area of minimizing
and if you make that really doesn't
matter if for example you launch an
application in Windows Start Start
screen and then we require like any
keepers so if you look at the good keep
recip is not some kind of other command
we will launch the application so
depends on how much negative impact of
false Paul or false oh don't pizza it's
not even a developer question I was just
wondering how you're making money that
Emily that is actually one of the most
difficult points right much so it's
difficult in the eye tracking yeah the
thing is and the thing is when you when
you start diving into some of the
necklace you have someone you have to
put in the Sun shoes when things get
ready for tomorrow and right now our
assumptions are too late
nice which makes it really difficult
without the quest that's why I said
start diving into it but don't bet your
life on to it right now and what we're
doing most of this project was industry
partners that are either beautiful eye
tracking technology or a continuity like
into X example of under control cases so
long and he's are quite some money from
Kickstarter right now oh he go grab yeah
and yeah it was a very interesting
experience to watch you pick start from
Germany i think it's not just kicks out
of the world launch of launched off
Joanie so looking forward to my camera
that very great ok Danny especially hard
work for I tracking no I tracking is
special I don't wanna be around over
there you can see look at take a look at
right now it's an external bar but the
question was talking about actually I
don't connect and with the with the
eyeshadow really works you have active
infrared lighting okay any of you if you
take a look at any person around you
just look into the eye you always seem
like reflections from the light of there
right now and what you do in eye
tracking is to take away all the normal
life that is in the room create
artificial infrared illumination so
you're sure where the reflection comes
from and now is a reflection that are on
your eyes and some assumptions on how
the is build you can actually track how
to estimate so does it make problems
with two people in the same room use it
in my site and two different gamers
there no in if they leg we are still
trying to figure out ways how to workers
to monitors because then you're getting
problems and you have to figure out
what's what two devices one rule is
usually not a problem one device in a
tip fairhall is a problem because they
have these strong writings on the top
and they have a lot of big forehead in
there it usually makes my problems of
demonstration public areas like
they're like seaweed or something up if
you have to prepare well I got one to
bless you sorry like can you do like one
oscillation yes you can do that so how
do i need some comment directly well
right now you can do it we don't do it
we just focus on the first person that
is close to the computer because of what
you're saying first of all I mean we
could separate with soup but a little
more to do with information may be
looking a little bit for them to future
there are deers in games where you can
play have many people playing on like a
large female screen or something was it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>