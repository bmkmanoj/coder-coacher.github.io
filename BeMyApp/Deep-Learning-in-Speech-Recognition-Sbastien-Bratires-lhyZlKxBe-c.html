<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning in Speech Recognition: Sébastien Bratières | Coder Coacher - Coaching Coders</title><meta content="Deep Learning in Speech Recognition: Sébastien Bratières - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Deep Learning in Speech Recognition: Sébastien Bratières</b></h2><h5 class="post__date">2016-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lhyZlKxBe-c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're on there so let's get started my
name is Sebastian Bhatia I I wear two
hats professionally um one is I'm doing
PhD research in Zubin garamanus group at
the University of Cambridge in the
Department of Engineering I do research
not connected to deep learning and not
connected to speech recognition either I
work on probabilistic machine learning
many Bayesian methods and my second hat
is an industry I work for a small German
company called devine we make speech
recognition interfaces for industry uses
so people who use speech recognition
systems in the industry like implants or
out on the field or in manufacturing for
instance or in logistics so we make very
custom applications for them the the
background my educational background is
in speech recognition and natural
language processing and it's always been
a subject I've been very fond of so this
is what I'm going to speak about today
let me try share the slides now so I can
get started on them ok
so you get my slides here um the thing
I'm going to speak about the thing I
message I'm going to argue me is that
there's been a change in the last three
to five years in the technology using
speech recognition so what I'm going to
argue specifically is that the state of
the art has changed from using I'm going
to I'm going to define all the terms
later on this Justin up tonight between
using words let me try and point you
what's here so Gaussian mixture models
and hidden Markov models and using
n-grams as the part of the engine that
is called a language model again I'm
going to define what this is to using in
both cases neural network-based machine
learning models so in this case here
these guys have been replaced by deep
neural net so feed forward neural nets
and engrams have been replaced by
recurrent neural networks so that's all
happened in the last you know three to
five years if you go historically if you
don't do research can argue that this
happen in the last ten years probably so
I'm going to speak about this and I'm
also going to give you hints of what's
going on among the research side of
things the very exciting things
happening there and specifically whether
still in both areas of acoustic and
language modeling the acoustic models
are now attending to be replaced by
convolutional neural nets and language
models by LSD ms and then there's a
lotta canoe thing which is completely
taken apart from classical model of
speech recognition so no longer using is
divided between acoustics and language
and let's call the end-to-end approaches
um let me just change then when I
present this like a multi see here up
they go to the next one so um SR systems
are deep down that probabilistic models
so there are parametric probabilistic
models so that's important to understand
because it means that they are big
mathematical models that will take as
input of you see some audio and then the
output something for ballistic maybe
they output a measure of a match between
the audio but the engine has seen as the
input and I have pathetic all or a
candidate word sequence okay but it is
hypothesizing that the output cookie so
you're getting probability up probably
jeans at the ugly ugly a measurable
match and because it's a parametric
probabilistic model you have those three
usual steps in the new training and
testing the model maybe you have the
training step where you're learning
parameters so your model contains or is
defined by a set of parameters members
remember scalars and these numbers you
will define based on training data then
you have a cross validation step that's
also pretty viewing all sorts of
initiatives utilized machinery methods
where you're learning what's called
height parameters so you need your legs
typically these things are the number of
hidden layers or maybe the size of your
model or maybe the type of the type of
neuron secret or rectified linear unit
or such that you're using so these are
all things these are things that are
important to understand and then
obviously the last one is the is the
recognition step where you are trying to
recognize some incoming audio
and you're outputting what I just
described before I love you at least
telling me that there's some some noise
on the line so let's see if I can do
anything about this before i go any
further lady I'm not using the
microphone that I'm expecting to you
sorry guys but that's probably for this
yeah indeed i'm not using my signal is
then sets up if we believe that so now
you
I is the audio any better now it should
yeah all right ok got rid of the noise
so let's get back to the slides here
cool
admin say it's not better
well if if there's so many of you saying
that it's better I believe you and not
the admins okay so um right back to
training and then doing some recognition
with the speech engines here so yeah so
I've detailed these steps because it's
just to illustrate that a recognition
engine a speech recognition engine is
just another type of supervised machine
learning model and it gets trained
exactly that in the same fashion there's
no sort of you know artificial
intelligence sorcery or wizardry or
magic in here it's just a plain machine
learning right the the split between
acoustic model and language model the
acoustic model is the part of the speech
engine that maps acoustics so that's a
waveform and a pronunciation model so
that's the waveform I've made a little
scribble here to designate the web form
and this is to stand for voice in its
phonetic in the phonetic alphabet the
acoustic model is is that part that is
concerned with matching the phonetics to
the audio and in mathematical terms this
is describe what's called a likelihood
and it's a probability really it says
it's the probability that are the audio
is as we observe assuming that the
phonemes are from a given word sequence
so it's probably the other way around
from what you'd expect right it's it's
it's really this sort of probably it's
not the other way around that we use it
we use it this way and and that's why
it's called the likelihood it's the
likelihood of the word sequences really
we do observe the audio and we're trying
to infer the word sequence at the point
where we do recognition the language
mall instead is the part of the speech
engine that is used very heavily in
what's called large vocabulary speech
recognition so that is the sort of
speech recognition you're probably used
to if you do dictation or if you use
your mobile phone
because you'd be using natural language
you'll be using a very wide lexical
maybe you'll be using you know our words
from a lexicon typically of several tens
of thousands of words or maybe hundreds
of thousands of words if you include
proper names and things like this so the
language model is an important part to
do prediction properly this example here
says the cat sits on the and then the
prediction model which will assign a
probability to the subsequent word to
every possible subsequent word and it
will assign a higher probability to
those words which are more likely given
the words that it's already seen so
that's the split it's a very traditional
spit in speech in speech recognition and
it's one that's been used probably since
the I don't know since the 90s since the
time large or calorie speech recognition
as an engineering problem has ever been
considered actually like when we put the
bits together at runtime we have this
incoming audio here we have a step
called feature extraction that is signal
processing reads looking at the audio
and extracting meaningful parameters
from small windows of signal here and
this is what I've noted all then it's
the features the things that we've
extracted from the audio and there's
this big search box here which is using
as underlying data the acoustic model
the language model and then the
correspondence between phonemes and
words so it's called a lexicon or
pronunciation model and in mixing all
these bits it's going through all
possible hypotheses for sequences of
words and it's doing this search it's
considering all the candidates it's a
massive search obviously that vary so
refined engineering optimization
techniques here and it's trying to find
the one sequence that is best and it's
actually as I described before
outputting arms several possible
candidates and maybe the application
itself is in a position to
choose the best candidate sequence based
on some knowledge that the application
has but that the speech engine doesn't
have right so that's the reason why here
you'll actually be seeing things like
candidate candidate word sequences and
then you know a score like I don't know
57% may be right and then you'd be
having the next one and that's only got
you know a forty three percent match
okay if you want these things to adapt
the nuts another business if you have
several more hair but you can normalize
this if you want otherwise that just
goes so that's when you put the bits
together are going back to the feature
extraction step because that's one that
enables you to understand how the
acoustic model then works and i'll be
trying to get into a bit of the
mechanics of the speech engine here
don't be afraid by the details but these
are important to understand to sort of
pinpoint where exactly deep learning has
been making this huge difference in in
performance so this is a called a
spectrogram it's the it's a very
colorful representation for speech this
is the axis for time and this is a
decomposition of the speech signal in
frequency and what I've drawn here are
the slices usually they're like 10 make
10 milliseconds slices that we make of
the speech signal and we take each of
the slices like old windows and we
compute these features on each of these
windows right and then we feed this into
a model the hidden Markov model that
consumes one of these slices at a time
and the hidden Markov model is probably
a thing that you can visualize pretty
easily viewed on computer science before
because one way of looking at it is very
much like a finite state automaton so
this is our representation for a hidden
Markov model that encodes a speech
recognizer able to recognize two words
of which I've written down the phonemes
here namely yes here and no here so the
finite state machine
an entry node and it has an exit node
and you will actually move from you know
entry node to next node to middle node
to EndNote and into the exit node to
signal that you're done every time you
move from one node to the next or maybe
from one node to itself over this
reenter loop here you will eat one of
the slices as shown on the slide before
so um this is how the finite state
aspect of things works and you will in
the in the decoding step you will walk
through all possible steps all the
possible paths through this finite state
machine and you'll be calculating scores
for each of the paths right so that
means on to Kappa key to calculate the
score you'll have a model another
probabilistic model here that is called
the emission model for a given phoneme
so here I've taken the phoneme air
that's in the middle of this yes word
and it's a model that eats us a window
of speech features so I've represented
this by just taking a slice of the
spectrogram before eats this and outputs
a probability that the audio was
produced assuming that we are in the
phoneme step or in the phoneme stage air
if you like okay so as I eat from the
side before as I eat each of my little
windows of speech here I'll be making my
way through the hmm that you can imagine
could be way bigger if it gots if it's
got many word and maybe it's got
branches because several because words
might have different pronunciations or
parts of words might have different
pronunciations now you've also got to
consider all the reentering loops so
it's a huge number of combinations
you're actually looking at and when you
arrive at the exit node then you've got
a candidate path that actually matches
the length of the expected words and
among all these you will be out while
you'll be put out putting the
probability for each of these and then
you'll be looking at the one with the
best match
this is almost the entire truth the only
variation the only complication that is
in actuality so that is in in real
speech recognizers is that hmm states
for each phoneme they're not actually
one step once one node they're actually
made of three nodes and that is because
you've got what's called a co
articulation effect which is the fact
that each phoneme is affected it's
affected by the neighboring phonemes so
the beginning of a will be affected by
the fact that there's a year before and
the end of yes it will be affected avi
of the air phoneme will be affected by
the fact that we are then pronouncing us
as the last show name so that's why
people have been splitting up these
things in three different states each of
them with their own parameters as to you
know how long you how likely it is that
you re-enter a highly likely it is that
you move on and then the parameters that
are actually in this emission model
which then is for not a phoneme but Rho
sub phonetic States or maybe I've chosen
this state here which is the first state
are in phoning air preceded by a year
and followed by Sun right but apart from
this this is really our what a speech
engine used to be like well still is in
many industrial cases still is in the
industry that's how it works so the big
big change that's happened is in this
emission mobile this emission model used
to be a gaussian mixture model so a
gaussian mixture model is nothing else
than a superposition of gaussian so if
you if you draw this in salem 2d you
know a Gaussian probability says like
we've got a bump here that's a Gaussian
say and a gaussian mixture model is an
addition of several Gaussian bumps so if
i take another bump here
okay and the sum of both it's probably
something like this thing here should
probably change color but you can see
what I'm hinting at okay so this thing
with a double bump here ah that's the
sum of the two gaussians
you
hello back we are ok sorry guys I'm not
going to name the operating system that
is playing with my nerves here let's get
the slides back on
presentation software this is PowerPoint
the waveform I didn't say anything the
waveform is not Gaussian what's go seen
in the gaussian mixture model is the
form of the model remember that it's not
modeling the actual audio its modeling
the features extracted from the audio we
can come back to this later on so wait
so the thing is then yeah big change is
this gaussian mixture model inside the
hidden Markov model then used to be used
until I think well I'd say 2012 or so as
the industrial standard as the state of
the art and then everything changed and
from then on people have been using deep
neural networks and so here's a slide
that shows what the state of the art is
so I don't have you know any special
insiders knowledge but I guess that
what's used in industry in a big
recognition software or big machine
learning companies looks very much like
this so they use that they have an
architecture uses about 5,000 or 10,000
of these sub phonetic hmm States these
things are actually called seen owns and
a typical DNN architecture will have
these five hidden layers maybe 2,000
nodes each inbreds we have as many
output nodes as xenons because we'll
have a classifying a classifier model
which will take it will take audio
features as the input and it will
produce it will classify these into a
scene own so it will produce as the
output which sub phonetic hmm states
this audio could be coming from where
the standard again is to use rectified
linear units and the cost function cross
entropy but is a technicality you don't
need to be worried about this usually
but still one interesting bit is that
still in state-of-the-art in many many
cases you use a GM m8
hmm so the old-style speech engine as a
preliminary step when you do training in
order to produce the state inventory
that you're dealing with the states of
the seals and also in order to align the
training audio you have
of with each window of audio too ok so
this our step I've been trying to do
this with just the the DNN based speech
recognizer but it turns out that it's
still working a bit better in different
cases it's in working a bit better still
using the old I'll speech recognizers
for a variety of rice of reasons so this
is what industrial state of the art is
like is like now the setup then looks
like this you've got you've got the
input here from the show the laser
pointer the input here from the feature
extraction step and then different steps
of pre-processing to get you to this to
the point where you have the input
features though these things that get
into the input layer of the DNN right
and then you feed this through the
hidden layer and you've got a softmax
output layer which will produce as their
output one of the scene owns your you're
looking at ok this um well a big say
publication I important publication that
showed that or that really define this
as the new state of the art was this
paper the deep networks for acoustic
million speech recognition then I was
published in 2012 in the signal
processing the I Triple E signal
processing magazine and what's
interesting about this article is that
very uncommon Lee these were this was
co-authored by for research groups and
that was the University of Montreal
microsoft research IBM and Google and so
these four research groups had kind of
like were seeing the same improvements
in their in their experiments nay me
they were going from these traditional
models that GMM based models and I was
seeing error rates of 23% and then they
were using deep learning based models
and they were dropping their error rates
a third or so just by changing the
acoustic model and that is something
that is totally and seen up to then
increases in performance had always been
by 1% each time or by a couple of
percent relative here okay never before
had people seen just a single jump in
performance so high at once the other
big innovation otherwise hinting to is
the one in the language model so
typically before 2010 you'd be using a
prediction function that says how likely
the current word is given that we've got
a history of previous words so this bit
here would be the cat sits on the and
this would maybe be max I would be chair
oh it would be before obviously before
it's not likely to appear in the
sentence the cat sits on the before
that's unlikely right so the probability
here will be low for before and would be
higher for things like chair or mat and
what was happening before 2010 2012 then
is that people were using a model called
engrams which are a very simple thing
they really are counts of how often have
we seen the sequence of words in the
data in training data so in text data
often have we seen these the sequence of
words and then based on this we do
counts and that's on that's the basis
for which to predict the next word ok so
the engrams and then back off models
obviously when you haven't seen this
sequence of words in somewhere you try
and guess maybe on a shorter sequence of
words what could be the probability of
the upcoming work and the new model
that's being used these days is a
recursive recurrent neural networks or
language models so there there are
pretty simple architectures you're
representing your input word by a 1 hot
vector so these are all zeros except
this black dot here which figures a one
and all zeros against this
vector usually it's as large as your
vocabulary so you can think of it as a
vector that's a hundred thousand
positions along with only 11 here and
the rest of zeros you're turning this
into a dye a vector of much smaller
dimension and then you're producing
through a softmax layer in a neural
network you're producing a probability
for the next word so this is again a
very long vector that's about a hundred
thousand long and what makes it
recurrent is that you'll be adding the
same architecture for the word before
last thing I've noted WT minus 2 then
for the word still before WT minus 3 and
so on and so forth so you've got this
recurrent recurrent neural network
language model you've got a couple of
technical issues appearing these
vanishing and exploding gradients things
so you've got lstm as maybe a better
version of the RNs and this is something
that brings you another ten to twenty
percent relative word error rate
improvement so using these two tricks
using deep neural networks in the
acoustic model and using RNN language
models you are achieving maybe a fifty
percent increase of performance and this
is along with the availability of a ton
of audio data this is really what has
been decisive in making speech
recognition mainstream owns on
smartphones really
okay I'd be saying a couple of words so
active research area very interesting
thing is people are starting to try to
do feature extraction so the very first
step in a way that is totally guided or
that is totally learnt from data so so
far we had things that were totally
engineered that's what I mean here by
carefully engineer we've got this
technology is called mfcc or perceptual
in a perceptual in a prediction or gamut
own filters mfc cs4 male frequency
capsule coefficients of barbaric
engineering signal speech processing
methods and then divert like a diversity
of methods to try and make them like
take away covariance or take away
interdependence between windows of
speech these are things that are still
state-of-the-art to be used in DNS
although in a much simpler fashion
there's no real unanimity on how to best
do pre-processing I feature extraction
when using bnn acoustic models but
what's what's currently under
investigation in research groups is what
if we feed our acoustic model with the
raw signal we don't do any
pre-processing at all and what happens
is that it seems that our neural
networks we learned to do exactly the
kind of filtering that our filters were
doing before the ones that we were
engineering as carefully as I described
so this these these are two plots from
two different methods the references of
which are here which seemed to each of
each of the horizontal lines represent
the frequency at which a given maybe
neural unit is is extracting and they're
really nicely ordered if you reorder
them then you can see that they go from
the low frequencies to the high
frequencies and they're really looking
at the frequency spectrum that is
actually interests
in speech so things that are going on
between 0 and 4 kilo Hertz right so the
majority in number of the filters here
right these guys here so maybe you know
three quarters of the filters actually
doing at the bits are interesting to do
speech recognition and they've learned
this by themselves and then the
end-to-end systems are the ones which
are do away with this hmm thing
altogether and instead they have a
recurrent neural network architecture
with an adequate structure and so they
they do away with this hmm thing
altogether which simplifies the pipeline
a great d fixes a lot of the issues
we've been seeing with hmm typically
duration modeling so how long are you
staying in a you know in one of these
states right duration modeling is is
obviously given by how likely it is to
re-enter the state often am I looping
here and duration modeling is the thing
that's not obvious in hmm um and so and
so these are these end-to-end systems
are sort of you know sort of work in lab
environments these days but the state of
the art in industry is still what I've
been describing before because we've got
highly engineered are super tuned
real-time decoders and the bar is very
high for these systems to actually beat
the existing industrial state of the art
okay so here's a here's a picture of his
a picture of inter speech in September
earlier this year that two poster
sessions going on here one is novel
architectures for a larger Kaveri speech
recognition and the other session is
speech in music analysis I'll tell you
which is the neural net focused crowd
you've probably guessed that it's the
denser one so there's clearly a hype
going on here right but you should be
you should be careful and not
necessarily you know trust the trust of
the hype because
because it's not always a good idea
there are invariant in speech in the
speech community that people are seeing
that are that don't have to do with the
underlying machine learning model that
you're looking at and that still must be
considered still must be taken into
account so robustness and adaptation
what's happening if you've been training
with a traitor training data that
doesn't fit your test conditions as in
the speaker all the noise conditions or
the channel or the topic maybe and then
issues that have to do with speech as a
much broader thing than just a sequence
of words so prosody the melody of speech
what's going on in dialogue at all and
then the you know very hairy issues of
what's happening with dialogue
understanding trying to make sense of
speech not only being able to do
dictation ie transcribing a sequence of
words but actually understanding what's
going on and then you know all the
engineering data collection footprint
latency issues so these are the real
issues that people are still looking at
and I would venture to say that the fact
that you're using deep learning the
neural net based models as opposed to
the other things that we're using before
hmm zorg a selection models or whatnot
this is this is not as relevant as the
fundamentals in speech recognition
thanks we still got a few minutes for
questions so I'm reading your your chats
here and please shoot ahead and I'd be
taking your questions as you as you type
them it
so I've got a question here has anyone
looked at vocal phase information in the
audio signature so I assume your
question means ah in the speech signal
and tell me if I'm misinterpreting your
question in the in will signal in
temporal signal in general there's
there's a phase information there's
what's being looked at mainly is
amplitude in different frequencies in
the in the spectrum and in speech
research there has been very little
attention paid to the phase information
as absolutely true and it's actually an
issue for microphone makers that were
speaking to a microphone make her a
couple of month ago who was totally
concerned about this and you were
surprised that no one had been looking
at phase in speech recognition so are
there is a bit of research on this it's
probably gaming interaction because
people think that the face is is a
valuable piece of the signal to be
looking at as well and in fact there's
been an entire workshop session at Inter
speech in September dedicated to this
it's probably quite easy to find on on
the web if you are if you type inter
speech and and when phase workshop I can
find you the reference if you can't find
it drop me an email is there a good
widely available package of algorithms
to easily experiment with up-to-date
algorithms so you're saying in speech
recognition is there a nice maybe let
software library that you can do speech
recognition do with do experiments are
very very the type of model you're
looking at and so on so forth yes there
is um i would i would pick just one
kaldi let's untie bit in its k yeah this
is a library a speech recognition
library that's called
it's pretty new it's pretty good and
it's got a lot of the new stuff so it's
got deep neural net acoustic models and
stuff like that so it's a pretty good
place to start it's a steep learning
curve if you're starting from no
knowledge of speech recognition at all
what technique would you recommend for
building audible language back from the
data so text-to-speech technology
alright so text to speech is is a sort
of sister tech technology to speech to
text clearly there are people trying to
do deep neural network-based speech
synthesis but mainstream methods are
still are based on a concatenation of
other units they're called so
concatenated unit speech synthesis
that's an entire set of different
technologies no actually I'm saying
nonsense I mean it's been a couple of
it's probably been a couple of years now
that people do parametric speech
synthesis this probably really find the
state of the art concatenated our
techniques were probably more like yeah
five to seven years ago I mean I'm less
of a specialist in speech synthesis and
speech recognition and maybe my
knowledge was a bit dated but no
parametric speech synthesis way to go
nowadays um and well you know space
speeds by a state of the art here as in
speech recognition deep learning a
couple of papers out like people been
trying to do to do a deep learning for
speech chances for that self extra
mainstream catching up with GPUs to of
course everyone is trying to to make use
of the computer kaldi yeah that's
Allison writing hi Allison trying to to
make use of the GPU computing power on
the smartphones if they're available I
don't know if speech recognition library
is running on smartphone these days make
use of the
architectures that got GPUs I'm not in
the know here I'd be curious to know
text-to-speech open libraries yes there
are a couple I can't name them off the
top of my head but you can surely find
them voice disorder classification so
deep learning applications so i don't
know if deep learning i mean you know
again deep learning is just one of the
machine learning techniques of others
among others so using a classifier based
on a feed-forward neural network is
probably you know as good as many
instances as using a classified based on
a different set of models voice
disorders are certainly an important
field of study and again at Inter speech
every year there are there's a lot of
you know a large proportion of the
papers and of the workshops dealing with
things like detecting in speech
detecting signs of autism detecting
signs of drunkenness signs of substance
abuse first signs of Parkinson disease
and so on and so forth there's a great
deal of study here because the speech
signal as it's one of the most important
bio signals is is affected by a lot of
the cognitive disorders that are going
on in our brains and so it's it's a it's
a signal in which you can of which out
of which you can read a lot of you know
what's going on here I don't know
intelligent voice but I'll be checking
that out
still waiting for more of your questions
if you have any
yeah so I guess um calling up the inter
speech website here you probably want to
go through the program on a program
detail technical program and i will give
you an example now what's going on in
the speech community okay new questions
bandwidth bins used in the FFT voice
analysis so these are based on the melt
scale you can probably Google and
Wikipedia this the male scale is a sort
of perceptual preceptory based scale
that tells you what the impact of it
each frequency band is in the audible
spectrum and it's used to indeed make
these bins in the mail frequency capsule
coefficients approach and also in the
filter Bank transform methods so we are
using bins that are sort of irregularly
spaced in the in the band from zero to
usually 8 kilohertz or so of signal
books on the subject there's a book
that's come out uh by leading that is
very up-to-date and try and conjure this
up to find this for you deep learning
that's not the one deep learning methods
an application that's just on the left
on the well deep learning pretty yeah
it's the automatic speech recognition
book right you're gonna pace the Amazon
Thank Ling here it's the book I mean so
this is a pretty technical book
it's pretty useful too so if your got
spare time and that's probably a good
place to start blogs not sure don't have
any favorite blogs in speech recognition
but you can probably find some blogs
you'll probably find either of software
makers trying to advertise how good
their speech recognition software is or
of researchers and if you're interested
in industry applications obviously then
you need to bridge the gap between these
two because they're not speaking out
exactly the same issues is the
processing of a speech recognition in
mobile processed on the device that's a
good question so different people or
different companies use different
techniques I suppose mainly the answer
is no mainly speech recognition so logic
every speech recognition is happening on
the cloud and the speech is compressed
sent over to the cloud and the result is
sent back there are there are ways and
they are indeed like on Android if
you're offline your speech recognition
provided you downloaded the correct
packages it will happen offline it will
be of lower quality so the results will
be less good but it will still you know
work to a certain extent because the
memory footprint of the models can
obviously be way less big and what you
can rub on on servers and currently the
gap performance gap is still pretty big
so it's still running on the cloud so on
servers okay I guess I'm done with my
time so I'll answer the last question
that's come up and then you know if you
find my email address then you can drop
me an email otherwise I'd say where I
guy access to the slides I'm going to
check this with the organizers you have
access to the replays of the videos
anyway
so that should be should be easy speech
recognition API from neurons to be the
best for app development so well nuance
is one of the providers for speech read
based speech recognition than many
others there's google voice IBM HP
speech mattox very many so when once is
one of them they're all pretty good
really so I wouldn't I wouldn't
necessarily say that one is totally
better than the one another they're all
very very good so probably your your
choice will be based not only on the raw
performance on a test corpus or these
kinds of numbers but probably more on
things like availability of languages
availability of features can you define
your own custom vocabulary pronunciation
can you define your own language model
so sort of inflect recognition towards
word words that you know your users will
be saying and things like this is
probably much more important than the
actual word error rate that you're that
you're getting out of the engine thank
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>