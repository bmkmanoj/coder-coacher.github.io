<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning with structure - How neural nets can leverage domain specific knowledge in vision | Coder Coacher - Coaching Coders</title><meta content="Deep Learning with structure - How neural nets can leverage domain specific knowledge in vision - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning with structure - How neural nets can leverage domain specific knowledge in vision</b></h2><h5 class="post__date">2016-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/i8GTd8cB0so" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so okay so let's get started hello
everyone my name is Charlie and I've
just recently obtained my PhD in in at
the university of toronto in the machine
learning group so hey and so today i
want to talk to you guys about how do we
do deep learning with structure so i'm
sure earlier in the day you guys have
heard a lot about how how you can use
deep learning and some of the tools that
you can use and bisexual deep learning
in your networks and so with with deep
learning that there's lots of buzz
recently that's that's beings were
generated and so the things that in the
vision speech and natural language
processing this current discriminative
the train networks are found to achieve
a very good good recognition rates from
image that to speech recognition on
android phones and Cirie things like
that however we're still not there yet
we're not where we like to be in terms
of so much surprised and not surprised
learning so so today I would like to
talk to you guys about three different
three different models in specifically
with related to generative learning so
in this case the first one is the robust
post machine and the second model I
would like to talk to you guys about
this so what's what's called deep line
brushing networks and third ones
learning with with visual attention so
for robust Boltzmann machine the
question is how do we deal with parity
with noisy inclusion so and this is
relevant to this is relevant to lots of
recognition problems because the scene
in our environments often very cluttered
with with occlusion so anything you see
it could be partially occluded there
could be noise
due to a four sample noise in
environment to do illumination
variations and so on so how do we handle
that so I'll skip the math for now but
i'll give you an illustrative example so
in this case here the if we have a face
model then the log probability is signed
by the model to the face he is the
highest so here's an egg hearing that so
basically higher the higher number the
better however if so on the left left
left image its native 551 however if we
block it with some sort of occlusion and
we see that the like lock the
log-likelihood dramatically decreases so
and basically basically what this is
saying is that the model is does not
know what's going on the density of
signed by the generative model public
model is suffers a lot even though we're
simply occluding only a fourth the left
right quadrant of the face and the rest
are complete visible and this is dis
demonstrate that the model is not robust
through occlusion which can happen a lot
for example to the sunglasses students
scarf and so on so how do we handle that
well we employed a generator model
called the robust potion machine and
this is one of the so this is a
generative model that's been developed
for maybe 15 years now and this the idea
of this is actually one of the sort of
precursor to deep learning to revival
keep learning and this is a bipartite
Markov random filled with binary latent
variables and if you look at the figure
in the middle we have visible units
that's correspondent corresponds to
pixels and then we have binary hidden
variables which sort of weeks you can
think of these as features there's edge
features or corner features or parson
object and so this is the model that
wouldn't be working with
uh it's generated generative so that
means that the model can assign problems
like the state TD to the data and so how
do we handle this so how we're going to
handle this case here okay so in this
case the right thing to do is is
basically we need to add a set of mask
masking latent variables as you see on
the right we have a set of white
indicating that we're clamping on the
white region which mean by clamping I
mean take these values as the cream
pixels clean image and not change them
and we want to impute basically fill in
in the back collision because the the
this is noisy noisy noisy area so that's
the solution won't you so mathematically
I'll with what we have to do is we had
to make a modification tour to original
the existing network in this case we're
adding three sets of pixels to each of
the original ink so see you see on right
and and essentially what that does is it
has a set of latent binary mask variable
to the model on the Left panel you see
that the V tilde represents the noisy
image of a person with white block
including the right I and V and s are
these are latent variables the model
does not have violating i mean the the
the model has doesn't know what what the
value is a priority it has to figure out
what they are and the model that uses
prior by learn on thousands of other
faces distribution of what faces look
like to them in tripoli to to infer what
the clean face will look like and the
reason one of the running for the clean
face look like that that would be very
helpful for recognition so this is a
this is the basic model that we're going
to be working with and as you see here
if you look at a structure of this model
this is going to be a slight
modification to the original research
restricted Boltzmann machine and I'll
show you some of the Oscar the map and
show you some of the cool effects of we
can do with this this model so here
here's an image of what happens to the
model during training let's let's look
at the top panel here the top row of the
top top top panel is shows the training
data which is a person with sympathetic
type of cushions a quid good of food by
three noisy block and the bottom to the
middle row and the the bottom world vns
these are inferred the model figures out
by itself what the person will look like
under know when the clean version of it
and in the bottom road s is actually
learn structure representation of the
noise so initially has no idea what the
noise look like as learning progresses
eventually figures out that noise is
will probably look like a three-by-three
grid block um and you see there's some
spec is some salt and pepper noise right
there because this is the smallest fully
propolis dick and we use give sampling
which is comes from it's a sort of a MC
MC sampler doing giving learning and as
well as in friends so yeah we have some
noise because the entire model is
stochastic and not deterministic same
thing for the bottom of the sunglasses
because the model it's train all faces
without sunglasses so it has no idea
what it thinks sunglasses is not a
particular typical face so it's only
what it does is it should we move to
sunglasses for you some other examples
this is Dina we can use the model for
the noisy and improve remove the
inclusion and it works better than other
models such as the simple rpms PCA
you know filtering nearest neighbor and
the reason what's better is because we
added the additional complexity or
multiplicative gating so that we can
actually get off the noisy noisy areas
and this requires no human intervention
in since that we don't have to tell the
the the model where the which pixels are
noisy which pigs will not are clean it
just figures out by itself using its
prior same thing for a removing noise so
the original noisy and then an old model
can can remove noise very efficiently
and here's some qualitative data that
shows that recognition accuracy improves
when you first remove the sunglasses
scarf and then you're and then you you
classify based on the clean image okay
so so that's that's a very simple so
what I just talked about very simple way
of adding a little bit of complexity to
allow us to be able to handle occlusion
and noise which is which exists in the
real real environment if you want to use
these models recognition another source
of variation is no single nation
variation right so you know from indoor
to outdoor from lighting comes from
different directions on person's face
and we know that lighting variation is
very detrimental to acura recognition
and so how do we actually add more
structure to jay directly erin s which
are simply these are officially known as
are simply distributed representations
of linear transformations followed by
nonlinearities but we want to be able to
add some structured structure in this to
it to allow to handle better better
handle elimination relations and so our
solution is to add was these things
called albedo and surface no
moonlighting hidden variables for model
albedo is sort of the surface
reflectivity of material like a paint
surface normals are are some since the
vectors which are perpendicular to the
local surface tension
specifically here here's the model we
take inspiration from Scott of my
impression reflectance model what it
says is that if you have a studio
surface the lighting that strikes the
surface the angle of that vector and the
surface normal the cosine of that that
angle is proportional to x abidos is
going to be proportional to the
intensity data camera my cat capture
this is the fists fists or physics based
model and what we do here is we put this
this operation if you Waldo in Russian
assumption into a deep leaf network or a
deep Boltzmann machine and so here you
see that the the part that's circled in
in red is the is the albedo post machine
so the generator model of what a face
will look like under no illumination
variations a deep ocean machine for
those you guys or might not be familiar
is think of us just like a complicated
nonlinear mixture of gaussians so this
is the problem stick density model in
the middle here we have a another deep
ocean machine modeling the surface
normals and displace three channels
because surface and almost the three is
a three batter and then we have a
Gaussian modeling the lighting direction
so so essentially this is what we're
trying to do is is is inverse graphics
in it in a certain certain sense and
these to be reputation combined it will
give you a face on a different lighting
scenarios little bit like machine all
machine learning algorithms we need data
so this is trained on face yet the Yale
be faced is that with three subjects and
as you can see here the some of the
faces are have heavy having illumination
variations so so we trained on which one
our model on this training is done in
stochastic fashion and after training
I'll show you some of the cool cool
things we can do here so we can do is
you can take a test subject the image
the test person by testing this person
was never the subject particular subject
was never seen before during training so
we take his one of his faces were
shattered shadow across the left top
left arm and then we put into the net
and we do inference so doing in France
by again by inference oh I mean what I
means that the model only sees one image
and it has to try to figure out what the
albedo which is just a lighting the
image of this person without lighting
and you hiding variation and a surface
one won't look like just by learning one
image and typically this is this is
known as an ill-posed problem in vision
because there could be multiple
solutions to synthesize V but however
are because the fact that we're using a
deep generator model that has a prior
this gives us extra constraint in a prop
success so we can actually come come up
the unique solution that's coherent with
the prior of what typically faces look
like and typically what prior service
animals will look like okay so the the
top row is the albedo that's inferred
and the bottom most servile so initially
a time step one it just has a mean face
that's the initial that's the initial
conditions in the initial step but then
as we run the gifts chain for sampling
from the slow Syria of this of this
distribution we come to to listen to the
two male face and the certain almost is
plotted as a color image because again
has a sort of 3d vector per pixel
location so here it quickly after you
know ten Gibbs change directions quickly
jumps in the park mall which is what it
should be doing another cool thing we
can do is is once we figure out the
albedo in surface normal because we have
a lambertian generative assumption right
we can actually do be liking or what
that means that actually synthesized new
novel images of this particular person
by changing a light directions right so
if you look at the top roll the image of
this person is basically the light comes
from sort of a lower lower left
but then these images are generated on
the right we can have more extreme Biden
variations such as the third image / you
can see the light comes from the way to
the right so and these are complete
synthesized by fire model and this is
just the demonstrate that the model
actually really captures something about
the physics of lighting lighting
variations and the reason we might want
to do this other besides having sort of
cool generative generated images is that
we can actually demonstrate that this
helps with recognition right so the
problem with recognition if you change
variation lighting variations that you
might have never seen this person hunter
and novel lighting direction and then
that would that would adversely affect
recognition rates but however what we
can do is we can we first remove these
like variations and and compare
distances and and do perform recognition
tasks in a much better collapsed space
that's that's invariant lighting and and
that's the reason we get we can get the
good performance what what what I mean
by one shot recognitions that if you
were to recognize between different
people when you only have one image pro
training so for example if you're trying
to recognize a person based on the
passport photo that we can still be a
one-shot task as as as opposed to having
10 or 20 images of the particular person
which makes the recognition task much
easier ok so this the second model
basically covers illuminate how do we
handle lumination variations in
conjunction with a generator model and
the adding structures that roughly
reason post lighting the way light
interacts with the services what else
can we do what else so what else is love
two-handed tues with that cripples
recognition well look
attention is actually the next big thing
well so to motivate why we should use
attentions that often images are very
big and high resolution and a lot of
times we only care about a small
location within the big image so in this
case we care about the face and we don't
care about the background okay I want to
be put model face model expression this
criminal to paint different people doing
interesting things but don't necessary
parts we don't want to spend the
complexity of the model wanna back up so
what do we do well we came up with a
holistic publicity framework that you
combines visual attention and learning
of generative models so in this case
here's here's our framework and I'll
show you and this is a there's a video
here I don't you guys see it but
basically this is this demonstrates the
inference process of this model so the
yellow square is the initial gates so
the model doesn't know where to look
initially so it just chooses a random
ice with an image and we have general
model faces then the tension gets
updated every creation and it quickly
zooms in on to the the area of interest
which is the face instance essentially
this sort of this is this is this idea
of modeling selective attention is its
inherently existing in this framework in
the event of this is that the tensions
actually data-driven so means that it's
where we look next it's not turned by
something else but it's actually
determined by the data what the image it
is yes which is so the general model
this is this uh this models it's what's
known as Gaussian TV network it's was
used before essentially you think this
is a model that can if you sample from
them they can give you random faces it's
there's the distribution of the faces so
here running the video basically it
so the bottom is the image the output
image the middle h 1 h 2 r are what's
known as hidden layers and these are
binary stochastic latent variables and
they're they're changing because we're
sampling them were randomly toggle so no
really loosely they can be thought of as
neuron firing very loosely but as we run
this network sample from this
distribution it generates phases of
different you know why distributing
faces so this demonstrate that the
general models is pretty good and it's
gonna be part of the attention model so
too so we for this we borrow we also
have something this is a computational
your size model called a shifter circuit
bio chosen in venison this you know this
is 93 there's other earlier work in 80's
by geoff hinton on how to coordinate
transforms but did you know the idea
this thing is that on a retina things
could change in scale and shift around
but for recognition with really needed
this object centric reference frame up
here you see a and then the social
memory or any other sort of learning
technique then just focus on a invariant
representation and i'll have to deal
with you know different size rotation
and shifts of the stimulus so here's our
proposed model so on the right is our
model we have a deeply network modeling
faces and the transformation is
represented by a similarity
transformation which means that this
there's a Delta X Delta Y the shifting x
and y the shift in rotation Delta Theta
and then Delta s is the scale and they
interact a multiplicative Lee with a 2d
similarity transformation on image so we
we have a general we have a model of X
with you down this guy's face and so
ours is a fully publishing model now
this is a proper model and when you when
you want to do to inference you would
just have a big image present a model
with the big image and it will figure
out what the faces are however there
might be some difficulties and the
difficulty sisters is that if we math if
if we do inference this model it's very
similar to some technically in image
alignment which has to maximize the log
P of U given X and B we're used to
transformation X is the the the data
click the big image and V is the
canonical face and so what I mean by
getting stuck is that said if you can
see in the video the alignment because
he uses grating descent and we use
something called hmc which it stands for
hybrid Monte Carlo the IL Ronn like is
aligned and it's hard for the model jump
into a into the global minimum which
means to chip in the template over to
the correct nose the nose cone face and
this is due to the fact that we're using
Gaussian residuals has some energy
barriers for hmc to jump out so how do
we deal with this in practice in
practice we we can use something called
a convolutional neural network for
approximately inference so bottom up and
what that this does is it allows us to
jump jump into a mode a longer distance
we're not just doing gradient descent on
the in the energy space of the posterior
so that means we can our attention can
jump many pixels whereas before I can
only sort of to this local adjustment
now you know we can we can make a big
bigger jump so we do is we take the V
canonical image B and we put into the
one stream of the convolution your
network and then we take a contact
window which is the yellow red yellow
square I see it as input this
convolutional neural network it's dis
combine the two streams combined they
mocking to get multiplied and what we
get out of this we get out the new
attention the new attention can then be
fed into again fed into transformation
you
and then we the box gets adjusted okay
and then we repeat this process for a
few times and you see that is you see
that the this squared is shifted onto
the face in very fast so that's that's
what whistles us as approximately
inference so it's using a second second
network to do bottom up where's the
generator model tends to be top down now
after that we can even use hybrid Monte
Carlo again to make fine adjustments so
so so so we have a model we have a way
to do approximate universe in an
efficient manner what is what is what is
nice the next is learning so to make it
truly like like the way human learn
right is the goal is not to simply do
you face localization because that would
be the job of phase detector but it goes
to have a generator model framework
general framework that can both detect
face localized faces as well as learn
from faces of a new data set so so the
idea here is that if you give another
person now a person that's the new
network have not seen never seen before
we won't be able to focus on the face
and then learn the distribution of this
novel person's face and so what we do is
we use that the statistical technique up
the expectation-maximization we're so
it's a two-step process and if there's a
Estep knows em step in the East up we
sample the posterior over different
locations looks like a phase detector so
you see on right and after that we can
then update the parameters of the of the
deep belief network that showed you guys
earlier to allow it to to update the
model to this person so that's that's
that's the that's what we weren't even
in learning I'll show you this so this
is the example of learning so so first
we these two data sets the calc and face
data sets and there's the CMU faces so
there
the data sets with two different people
we can do is we can learn on the caltech
faces and then we so that's the that's
temples on the the model train on top
back faces only is on the left and
that's sort of the samples its
distribution what people look like from
Cal Tech and then we can do is we can
then learn from the ninety percent of
seeing new faces and by learning I mean
that just feeding a big image without
without small crops of faces right
because that's you know with Facebook
with other sort of online media all you
have is this big images you don't have
it's expensive to actually have cropped
faces so with the tent with with our
attention model we simply feed in a big
image and the model simply then figures
out where the faces are and then learn
the street in the face in part this
policy framework and so after learning
these are on the right these fossils of
which are hired green they are so there
there's these samples come from
distribution the CMU faces the rest are
still in cotton faces so this
demonstrate that that learning with this
model can I will allow us to actually
learn from after learning we have the
model can have the sample can generate
samples from both distributions right so
we learned it was able to learn from
both c mu n and call it faces okay so
i'll show you some more inference
examples so on test images these are
caltech images right so how does infants
actually work in this model so you can
see that you can random we can randomly
initialize it anywhere different size
the rotation that's the yellow box and
as a situation time comes we're able to
to me on face so that's these are some
testing edges that there are more
okay and and so one other thing is that
with this model it's a generative okay
how the question is how fast is it does
it take out with a faces are so
basically each it can be done in real
time because the expensive part is the
proximate friends using a convolutional
neural network but but it's actually not
very big showed you before a spouse
sometimes the layer has roughly sixty
four filters and Sergey filters so can
it can be it can be done in real time
for cropping images of size 1 28 x 128
figured up so it's really fast okay so
the other thing is that inference with
ambiguities and so unlike other face
detectors that you like the real Jones
adaboost detectors we're just attacks
all the the defaced win an image this
model here is a proposition 8 of
framework so the advantage is that what
the V here late in V is you can think of
this sort of the internal internal
representation in the model so what the
what the model has mine sort of and so
everything else in these two example
left and right are the same so the input
image is the same the initial condition
of the your box is the same everything
is the same the only thing that's
different is the V right so so initially
we set it to be the person on the right
well I mean in a left panel and if you
do this the attention will focus on the
person on the subject on the left and
the reason it does this is that this
will have a lower higher log probability
right because the V match is better with
the if we focus on on the left and vice
versa on the exempt on the right if we
set the person the male person male
subject we can then the model usually
won't drift over to the right and
everything's done you know
Papa sickly kosher way okay so that's
that's the tension model so in
conclusion I talked about three
different models of how we can actually
push recognition beyond simply
describing the training so discrim the
training basically it's you give an
image it does classification it says
that which classes are here so that's
the screen retraining the first thing is
the robust we we have robots motion
machines that can handle noise inclusion
by using muffled decayed of gating to
get off noisy areas and filter them in
we have a date t plan brochure network
that can model evil and server school
mon and do crazy crazy amazing things
with lightning variation and then
finally we have to master
transformations that roughly loosely
chris month to visual attention that
allows us to focus and learn general
models of object of interest within a
big high resolution image and so these
are just a three domain specific models
that use models models that use domains
of knowledge envision obviously there
are many more possibilities for future
research so with that like to thank you
guys and takes any questions that you
might have right now
sure thanks for uh thanks thanks thanks
thanks everybody for attending you're
welcome okay question how well these
models work with various face
orientations so phase orientations is
I'm assuming that you're talking about
in that location so this kind of
notation it does work it doesn't work it
works it doesn't work as well and due to
the fact that this is a highly nonlinear
transformation right so so that's
actually excellent question and one
thing that sort of part of the
trajectory of these kind of model which
which is basically adding structure to
22 generative models is actually monkey
3d so 3d CAD models now we have the
ability to have using kinect kinect def
cameras we can have very cheap 3d model
3d data sets rtvg data sets so so so so
so we're moving to it there i have a
paper called tensor analyzers where
there's some some more examples on how
experiments with where you're actually
learning ma placated interaction with 3d
rotations but the problem is that none
of the Lamb personal ambition network
model the janitor model is actually it's
actually piecewise sorry nonpiecewise
but aluminum why is multiplicative right
so it's every single pixel you multiply
by a certain one with the lighting
direction now we have 3d rotation it's
not gonna be element-wise anymore right
so we have we have three rotation unit
actually do this video submission
dspace you can do that with 3d model and
but that's for future research okay so
next question can we implement inversion
models with out data dependency um yeah
you can so the lambertian model
basically it's just that I have a have a
surface I have a lighting direction
coming from this anywhere and I'm going
to take the inner product of this of the
light in the surface normal and then
multiply that by a albedo so the the
thing with data dependency is that what
what what I really want to do was did
you post your inference with one single
image right so just just look it up a
single an object of arbitrary lighting
orientation and try to figure out what's
the what's the albedo was the surface
normal so if you just take a look at one
pixel right if you're giving one
intensity value like one from 0 to 255
and you have to figure out for numbers
from it for numbers because we have one
scalar for albedo now have three numbers
for the surface lingual that's your post
problem right because many combinations
of phone number you know when you modify
together can okay can generate the
details day and so we have a goalpost
problem we have you have a prior right
priceless that oh I prefer out i prefer
I'll be able to be certain value of this
Gaussian distribution albedo I have
another distribution over to the surface
normal like some typically certain point
into the screen and not not not
vertically or something that and so if
you have that prior then sure then that
can you can use that to salta salta
steel post problem and we use data to
learn the prior now if you're you just
happen to know that prior then that
would work the same but that's where the
data dependence comes well okay so next
question is it possible to apply these
models on mobile it's processing part
better hardware um yes it is possible um
so the good thing about the neural
networks engineering journals that it's
very
it's via if you have less processing
capabilities let's say that mobile we
have 10 watts these sort of GPUs the
tapered Taylor GPUs from Nvidia then
what you can do is you can actually make
a model size smaller if you don't you
inference you can do it you know how to
do 10 steps you can do it with three
steps so so yeah so so we can do it I'm
about it and there's a place to optimize
as well I mean a lot of mobile nowadays
comes with let's just take take the t
power for example you can actually ran
Singh Ponte we can actually run for Kota
on but ya better hardware is obviously a
soft as the would obviously to
definitely helped a lot with the with
the processing requirements of a lot of
these algorithms um yeah so see here
next question it's possibly what you did
in cargo mobile see what yeah I mean yes
if you look at the flick hago challenges
it's using a convolution on your network
and the same thing you can do inside
using filters the photos of 64 / layer
you could use 8 or 16 instead of using
64x64 does the initial resolution you
could use smaller 32 by 32 reps on you
can have a smaller hidden layer for
final classification then brush and
models for indoor scenes yeah so so the
land Bush model fingers things might
require more research at this point and
the reason is because four faces is
roughly roughly comebacks and that's
what that's what makes an impression on
easy easier membership model is not good
to deal with cast shadows right so if
you have
shadows is not good because shadows are
not generated by simply doing inner
products between the lighting direction
and searchable the brochure models also
don't do pretty well with specularly
reflecting with infection so it means
for something something shiny like the
bike in the glasses right that doesn't
handle that well there's something in
graph is called a phone model which
handles that so for indoor scenes you
can do a lot of source surfaces can be
modeled by the impression so them a
smaller scale but if you want a model
entire scene there's some with with the
model I described there might be some
some difficulties in it in the first
thing is that we need to be able to have
a really good generator model off the
entire scene right which is which is
hard to do at currently we can have a
generator model of a part of the same
for example like object and then yeah
the first maybe stitch those things
together that's one way to do it but but
currently it's not is not easy to you to
apply to the entire scene
sure yeah you're welcome okay do we have
any other questions feel free to ask ask
any patches
sure okay well with that thank you guys
again for tenting and enjoy the rest of
the conference okay we have no question
here especially reflect we're lucky yeah
like so it's like if the phone model for
example handles it this is a coefficient
with the with highly nonlinear constant
interaction that the hell's specular if
we fashioned in that gets into more
graphics so if you're trying to do
computer graphics you're trying to
generate games resentful in that they do
a really good job with that but again
it's this is sort of what sort of like
Michelin use on on one side and you have
graphics on the other so some people
have theories that says that machine
learning is essentially its universe
graphics so you won't have a graph you
want have a start with a graphics engine
that goes from high level 3d
representation to be pixel space and
then machine learning or perception just
kind of invert that so given the the
image how can we find these these
variables um but machining is a
trade-off so so what'swhat's what would
come we should come come that's right
now is that if we don't really want to
model it does vary in a very detailed
way we just want to be able to train
something that does us something
approximately correct and and the
reasons that too time consuming it
requires too much on hand engineering
and to do that so you have to think
let's trade off as well and you have to
try to strike a very good balance
between you two because because it's to
sort of the labor some or laborious to
to to to make these details right so
because we making for specular
reflection that might work for some
cases what happens if there are other
sort of other variations right if the
surface is
the others are noise happens like
cushion or there could be other other
things that that might pass pizzas in
the environment right not like snow or
rain or something like that so so then
so then the things that you want to meet
halfway right like with with these these
huge huge illumination variations member
that ambrosia model can do you know
maybe seventy percent of way right and
then the rest we just let the bottom up
the neural network figure out roughly
what it is so it's all trade off you can
model more more accurately but but at a
cost of more time spent in here to to
programming the model and and perform
the entrance how do you police handle
reflections from bright objects um
dealing different light sources or do
you mean the like light source bright
objects or do you mean slice vs ok so um
so with Lambert remodel you cannot use
multiple light sources we can add
multiple lights in phase combined
additively um that's one way to handle
it uh now it doesn't yeah so we have
reflections bouncing off different
things yeah we can't really handle that
much that well basically we treat as
ambien ambien ambien light so that's
essentially as all the pixels in the
environment gets added bit in terms of
the ambient and the other questions that
you know you need to take it like do
this ray tracing for graphics because
you want to generate really realistic
looking images and the questions for
perception do you really need to model
it that accurately you want to do a good
job in terms of perception because it's
unclear at this point
what your advice is to hide our faces
from hey I yes it's uh I mean I think
it's interesting question it's it's hard
it's really hard because there are some
papers published now that it's showing
that face recognition performance is
reaching human performance if you're
allowed to have if you're trained on
database millions of faces we're very
subject you would have hundreds to
thousands examples so for from from
facebook if you grab every photo you've
uploaded you grab one hundred hundred
faces your face from that I can figure
out what you would look like hundred all
sorts of different conditions and from
that then it can actually do really
reacted job of reckoning recognizing you
from from now on so I mean I guess just
try not to upload as many faces on you
know Facebook or Instagram units that's
one way but but yeah it's a face
recognition to roughly simple problem
compared to get a general object
recognition and so um yeah no I don't
mean if we're disguises I guess that we
use BB another traditional way of
dealing with it
sure
you're welcome</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>