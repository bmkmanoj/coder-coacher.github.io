<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intel® RealSense™ Technical Q&amp;A Webinar #2 | Coder Coacher - Coaching Coders</title><meta content="Intel® RealSense™ Technical Q&amp;A Webinar #2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Intel® RealSense™ Technical Q&amp;A Webinar #2</b></h2><h5 class="post__date">2014-12-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UcBqcaZgGtQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">button above the 3d depth camera we have
the SDK and desiccate provides a bunch
of capabilities I'm going to go through
the some of them in a moment and in
order to you and we use all these
capabilities to provide more intuitive
and natural interactions so first I want
to talk to you a bit about 3d depth
camera so in 2013 we list the peripheral
creative camera it's the one you see on
the top and this camera like I said was
a peripheral camera was really selling
with the recent 2013 SDK for early
developers I just to get the concept of
what what's the 3d depth camera is and
just recently we released the 2014
realsense camera the one you you're
supposed to receive already received
this camera is very thin it's going to
be embedded inside the all-in-ones to in
ones and eventually laptops tablets as
well it's very very power efficient and
the goal of this camera is actually like
I said produce 3d depth data now I'm not
going to go very technical with you back
but just to give a whit just to give a
short concept over the depth data there
are several ways to get a 3d depth data
and this camera is basically based on
the time of flight technology which
means that it emits a certain signal and
it measures how much time the signal
takes to hit an object and comes back to
the camera and then it can actually
understand how far is the object
according to how much time it took it to
hit the object and come back to it and
go back to the camera so when I tell
people about this camera for the first
time usually the first thing I've been
axed ism so what it what's the
difference between this camera to
connect well like I said in the
beginning there's a major there's a
major difference between these two
cameras the first one is that this
camera is designed for close range
interactions when you could connect for
example it aims it is aimed for a full
body for a father's for a father range
but with this camera is aimed to be
interacted with the integrated inside
tablets makassar only once two in one
and which means we want the interaction
to of the user to be close to the
computer close to the compute device and
being able to interact with it with his
hand very near to the computer Vice who
are talking about 20 to 120 centimeters
and the second thing is that this camera
has to be very thin like I said very
power efficient as well because we're
talking about mobile devices and we want
this camera to go along with them and
this actually creates a major difference
between these cameras so before i'm
going to show you i'm going to actually
show you what a 3d depth data is but
just before we go into that i'm going to
i'm going to show you talk a little bit
about the sdk itself so the rental
recess sdk actually understands four
basic type of inputs the first one is
the hands we're talking about 22 points
on the hand including joints we're
talking about gestures whether it's
static gestures or dynamic gestures two
hands and more than thats multiple hands
you can understand whether it's your
right and left hand and so forth the
second one is the speech we're talking
about voice recognition and text to
speech capabilities command and control
mode or dictation mode the third one is
face we're talking about face
recognition and face detection multiple
face to have detection actually we're
talking about 78 points of landmarks on
the face 78 point like the tip of the
eye or the nose and so far and about of
course we aiming forward towards other
capabilities like pulse estimation and
so forth and the last one is the
environment and the environment is very
important because like I said all the
333 types of inputs that I've talked to
but right now we're related to
understanding the user but in order to
understand the user completely you
actually have to understand him in the
right context in the right environment
that surrounds him meaning who is he
with what is what surrounds him
so just to give you a quick list of the
SDK like very it's not it's all
completely spicer definitely provides
you two main areas so when we talk about
our value we talk about background
segmentation about 3d and 2d object
recognition understanding what objects
are around the user seen perception is
something that we are looking forward to
being in the future and augmented
reality capabilities I'm gonna give you
later on a short concept of what does it
all mean so maybe we should just have
now a quick look um about SDK itself I
don't know how many of you guys got the
camera already so I'm gonna let's see
first of all can see um p I can see I
don't have the questions tab so that
means either there are no questions or I
can't really see any one of the
questions if there are any so I want to
give you like I said quick look about
the SDK stuff let's have a look here all
right there we go I'm going to start
with the with the depth data now you can
basically see me and you can see the
distance of me from the camera take now
my jacket because it's a black jacket
and black objects observe all the signal
with the camera emits so now you can see
basically me you can see how far from
the camera if I go back you can see the
difference if I go nearer and this is
the raw depth data well actually it's a
bit smooth but let's not get into it
right now and if I show my hand you can
see how the SDK understands my hands how
do we can actually have all the point on
the landmarks I can play with my hands I
also wrote to him to check for a fist
just as you can see when I close my hand
there's a fist gesture being emitted and
I can actually play with my with my hand
you can see it's very flexible very
accurate and of course if I am if you
understand which one is my right hand my
left hand so this is just a quick car
sir
one second see now I have some questions
all right so ok let's continue the next
thing i want to show you is basically
the face tracking so the face tracking
if i open it right now if you can see
now when let me know when you can see me
there we go so you can see the 78 point
landmarks on my face you can see how
when i move the points move with me you
can see that on the right side or
whether it's on your left side the
expressions you can see when i'm smiling
the small changes when i'm surprised
basically this understands gives an
amount as a central number of that
indicates the expression that i'm
currently opposing we and also provides
the post detection whether i'm leaning
forward with our link backwards and so
forth it has the face ID which means who
am i if I'm if i press the register here
it will recognize me when i leave the
screen and go back into the screen you
understand i'm the same user so this is
the face detection and the last thing i
want to show you is actually the 3d
segmentation it gives it a really good
concept of what a 3d depth camera is so
now you can see how it segments me it
understands that I'm the object that's
near the camera and it actually hides
the entire world behind me if I my hands
yet will show only me in my hand now if
I go backwards to certain point will
actually hide me and put me in as far as
a background but if i put if i get
closer it will show only me and segments
me out from the term from the external
from that rest of the world so all right
let's continue now i want to talk to you
the next thing I actually want to talk
to you about how we use all these
capabilities to create augmenta
experiences and the first experience I
want to talk to you about is the digital
storytelling
in order to I've so far I've been
talking about about how we want to do I
want to create an immersive experiences
in non natural and intuitive experiences
and the first thing we actually want to
decide we want to take it to do it it
was the digital storytelling so far the
books and the storytelling has been yeah
well you you're taking a book and you're
having a look at it and then you decide
you start drilling you decide whether
you like it or not then this means
whether you get immersed in it or not
but we we thought well why don't we take
it to the next level why don't we have a
look if we can actually change the
context of the book and in order to do
em to better engage the audience like
why don't we why can't we engage the
person have the person were engaged
inside the book so in order to do that
we actually created something called an
immersive digital storytelling which is
actually created taking a certain book
recall we created called uncle jeff's
factory farm and you can see the book
here i'm going to show you a small demo
in a second and the concept here like i
said was having the user more immersed
inside the book so let's have a quick
look all right so here you can see uncle
Jeff funky farm all right under on the
left side you can actually see the user
holding the book and once we show the
book you can see the farm let's say this
is a butterfly this is the butterfly
scene so once you the user shows the
book a virtual butterfly appears in
front of the book let's maybe make it a
bit lower because the music doesn't
really contribute and now once the
butterfly appears you can actually use
it can actually play with it and move it
push it forward we should take it
backwards and then afterwards of course
show the book and have the butterfly go
back in into the book let's have a look
about several other scenes
i'm going to show you just name the more
interesting ones to save our time so
here in the next scene not knowing this
one just about to start there we go you
can see how the user can actually see
himself inside the book you can see
himself coming out of the computer
screen and this uses actually the 3d
depth segmentation in order to signal
the user volume inside and then we in
this scene we can see the face detection
or tracking you can see how the user
views inside the billboard and basically
if there were several users behind them
we could see how you can see how we can
switch between one user to another maybe
the left nice thing to show here would
be the portal to the put a partial to
the world or so you can see how the user
uses the book now as a portal to the
virtual world you can actually move the
book around have a look in at the
virtual world from different angles and
this actually depends as well on the
cameras point of ve and angle so let's
have a look I see we have a bunch of
questions so I'm going to read the
questions can I don't know how many you
can actually see them so the first
question is by I'm on asking is there a
link describing the hand information
received from the SDK so first of all
this is actually a good question so all
the information is being received from
the SDK it's being described very
thoroughly inside the documentation once
you install the SDK itself you can see
either the documentation folder so we're
inside SDK path so once you go there you
can actually have all the information
you need exactly what what's the road
that that others being provided how you
get the hands data and how you can use
each one the next question is um is
there way to reduce the frame rate for
less than 30 frames per second I'm not
really familiar with it um not really
sure about it actually I'm not really
sure so we have an hour quest
share about seen protection I haven't
configured my camera but can you just
point out the cam towards some object on
the table well yes and I'm going to show
that in a moment Stevie saying about
segmentation segmentation works very
unreliably on my system only about
twenty-five twenty-five percent of the
time usually I just see the faded
background is there a known problem 3ds
tag sample code well first of all no
there is no there isn't any known
problem that I know of it's important to
say though that the segmentation data
and in general the cameras data depends
a lot about the environment during so if
you have for example a reflective
material behind you like a glass or a
mirror that basically can cause noise
and interferences or of course if you're
wearing black so like I said black
actually absorbs all the signal doesn't
allow it to come back to the camera so
you don't really have the camera doesn't
really receive any depth data in any
other case you should basically get a
really you get good data decent data
like the one I've shown right now there
shouldn't be any real issue with it
let's continue them so um the what the
things you've just seen right now like I
said is how we take the the virtual
world and bring it into how it I'm sorry
how we take the user and bring him
inside into the virtual world but the
concept of the camera is actually having
the users integrate inside their
environment in the 3d world I'm sure
you're all aware of this situation where
people sitting inside a restaurant and a
couple of people sitting and each one is
actually engaged with it with the own
phone device tablet whatever and instead
of interacting with each other and this
is some this is what people were talking
about being engaged in a 2d screen but
when we look at that we actually know
that the world around us is a 3d world
it's much more appealing to our senses
and we're asking why can't we bring
people having them interact with each
other having collaborative interaction
and with their devices in a 3d
environment in 3d world so I want to
show you several examples right now and
the
one is what we call another demo with
our famous uncle Jeff it's actually I'm
going to it's made out with two videos
I'm going to break one in the middle
show you the second one but it's called
a farm on the table and in this
demonstration we show how we take
instead of taking the user and bringing
him into the virtual world we're going
to take the virtual world and take him
out into the real world so in order to
do that we use augmented reality we take
a marker and now once the actually like
we start the demo here you can see the
farm coming out on the marker on the
real world on our tablet and we use the
3d depth camera we aim it towards the
table and use it in order to you
understand the surface around it we
understand that there is a table and
later on you will see how you put some
other objects and we understand that
these objects are actually there um so
I'm going to give it a few seconds so
once we have
uncle Jeff coming out of the screen we
can actually hear there we go we can
actually see him coming out from the
farm into our table walking on the on
our real table so I took a virtual
object interacting now with a real world
walking on a table around us I'm going
to stop here and show you the retina
like another photograph of this demo
because I think it would be a better
explain Tori there we go so the next
thing i want to show you how we add some
more objects into um let's make it a bit
quieter into the environment the first
thing is actually we plug dress on on
the table so you can see how the grass
tops on the edges of the table and it
doesn't continue outside and in order to
do that we use actually the 3d depth
data we use the camera and we understand
that there is a table here and behind it
there is a different environment at the
next part we have this island right here
we want to have uncle Jeff going up on
it and rescuing the sheep that is being
stuck on it so in order to do that we
use a physical actual real ramp and then
because we have the 3d def camera we can
understand what we have a 3d ramp here
and we can segment the data we can
understand that there is a different
size and then and here now you can see
uncle Jeff climbing on it so you can
also see for example this some the snow
piling over the ramp so this is another
nice example and here you can see like I
said virtual elements of metal reality
and virtual objects appearing on the
real world and another another nice
example of it is the one you just about
to see right now we call the tractor
example we take we have a ramp here the
same ramp and this time we have a small
tractor is going to try and climb over
this Tramp and because we have a 3d
depth understanding we know that there
is a climb around it and we know that
behind there is a descent so the
tractors you can see we use a physical
engine in order to simulate and now you
can see how the tractor is having a hard
time climbing up and the moment he
climbs up is going to have
easy easy going easy way going down are
almost easy way going oh there we go so
this is one example of how we use the
environment to it then how we have
environment understanding i think
there's also a small session here
alright that's about it for now so this
is one like i said this is one example
for environment understanding and how a
person can interact we then find around
him another small another nice example
would be some something small or some
small demo we call them AR pet so in
this demo you're going to see how well
the demo does this name of itself is
self-explanatory but the point is
actually showing you how we use
augmented reality and how we edit to
document here at the experience the
occlusion in order to do that we use the
3d depth data in order to understand
where the hand is located in them in our
in the real world and then decide
whether we hide this in object or not
according to UM to its location so you
can see first of all example of how we
understand the hands location in the
depth in the 3d world and how we take
basically take it into the virtual world
and you can see how the hand can push it
the ball or play with the buttons or
definitely can also hide the ball behind
it the next thing is actually like I
said a our pet so we're going to move to
the this to the next scene sorry so here
you can see how we have an intern
environment and the user can actually
interact with it with his hands you can
play with the leaves push the buttons
push the the rocks and once he breaks
the actors his virtual pet the dragon
coming out he can play with the dragon
he can hold him and pushing him push him
around he can make him aburn the trees
hide him with his hand you can see how
he can
play with Amy can push him and then the
dragon starts firing throwing fire and
there we go and then you can actually
hold it with these two ends and blowing
apart all right so i can see we have an
arrow here pal questions let's see let's
go over a little bit before you continue
the demos so we have hi I just wanted to
know if a unity 3 integration is that if
yes one will be available so definitely
a general question I will try to give it
towards the end when i get to the unity
part but in general well let's save it
for later alright is sdk java library
again general questions about the sdk if
they're not related to the demos I try
to treat them in the end another
question we have here is about 3d
segmentation problem that we've already
talked about so again let's try and live
it towards the end other general SDK
issues if they're not related to demo I
will leave 15 minutes and then and then
we'll try to talk a bit about technical
issues all right so you've seen so far
how we take the virtual elements and how
we take the user input in having
integrate interact with the virtual
world or how we take the virtual world
and have him and being the virtual world
back here to our reality the next thing
actually I want to see I want to show
you is how we take real elements real
objects and use augmented reality in
order to have a better interaction with
them so in this case we have what we
call an AR ki we have a real dragon and
we use augmented reality in order to
show how he is firing throwing fire and
if we had a Lego tree for example we
throw the fire it you can see the tree
burning and now we can have it now we
can actually bring our fire truck and
there we go when you can see how we
augment the smoke over the fire track
and we use the water to turn the fire to
turn off the fire and this is a very
nice example of how we can have kids you
actually playing with your actual toys
and I'm still having a better
interaction with it another small sample
of concept is how we use the Ministry
ality we basically burn the tree but
when now the tree is actually
disappearing you can see how you can see
how the tree is actually disappeared
after we burned it so all right let's
continue so all right so so far what you
actually seen is how we have different
types of interactions of the user
whether it is having the virtual world
coming into the real world of having the
user going into the virtual world and so
back and so forth back and forth and
it's all very nice but like we started
to say in the beginning the purpose was
actually having the users interact with
their environment around them and not
having them interact in a very small
area so and another another thing
actually mentioned was having the users
move from one person into collaboration
having several users collaborating
together interacting together in the in
the in the real world so now actually i
want to show you how we move from the
small environment what i just showed you
is actual interaction around the small
environment into into the big event to
it to do enter a room environment sighs
and how we move from one person
interaction to us to multi to multi user
interaction so the first the first demo
i want to show you is actually what we
call arm let's have it just a second
there we go a collaborative farm so it's
actually based on the same farm story
that you've seen so far and this time
we're going to use several users
interacting together around this fund to
just seen so you can see basically
several users holding their tablets and
the market in the middle and now we have
the farm coming out and each user can
actually see the farm from you
the only point of view so we have the
same of the same virtual object alive so
to say in the real world and each user
can see him from his own different point
of view and in a moment we're going to
show how we can actually have the users
interact interact with this farm in
their own way and we have Quincy the
users building their own work together
so in order to do that we have we going
to have each user plant trees on the
table around the farm and each tree that
I want user plants both of the users can
actually see so we have the users
creating the all their own world
together planting trees the next thing
they're going to do is actually put
sheep so you can see how we have several
ship you can see that the tree is
appearing all around this is the other
person planting trees and I can see how
one user puts sheep and the next data
user can actually see this ship where
what the first user has put them so this
is a nice example of collaboration and
another example would be like a surf
course uncle Jesse's walking outside
you've already seen something similar
but they both watching them to get chef
in the same area and just a small sample
for another in track is how we can throw
tomatoes at the table so you can see how
the first user is actually throwing
tomatoes at table and the tomatoes are
bouncing from our being shot from the
user heating the table and bouncing off
the table and this sofa and both of the
users can actually do that get throw
tomatoes so of course you can take it
too many too many different areas too
many different types and concepts and
this is actually a very nice example how
we do collaboration between two users at
this morning I've been above I'm I've
been asked about this in the previous
webinar so I'm just mentioning the
collaboration itself between the two
users is actually based on the metro in
the Wi-Fi we actually we have here
another small example I think of object
recognition how we can put for example a
real chest and we can recognize that
there is a real chest now on the table
and also a tractor and now using the
object recognition we can actually
put over the ridge chest we can hide the
real chest with a virtual chest or they
put another future attractor above the
retractor and like I said you can take
it too many different areas too many
different that you can do with it a lot
of really cool stuff so let's have a
look very quick answering some questions
before we continue alright so i can see
here partial questions Get Set tracking
one of the examples so having none of
the examples deal with hands getting out
of bounds issue letting the user
feedback from available tracking area
well that's correct you haven't seen
anything like that we have something
similar actually the unity toolkit but
basically they're having the user to in
order to tell the user that the hands
are coming are getting out of the bounds
of the screen or basically out of the
camera as angles or point of views you
what you're going to have to do is for
right the use of some sort of feedback
you're gonna have to show him sort of
marker that indicates where he's on the
screen and if the mark is getting out of
the screen the user knows his hands are
outside of the screen we do something
similar in the unity toolkit I'm going
to hopefully get in get to it in time we
have our L asked asking regarding the
face detection can you use it the tech
custom gestures for example a blink of
the eyes together with his mouth open or
is there a predefined set of gestures of
the SDK detects the same goes for 3d
gestures can you detect gestures in 3v
for example a fist coming from back to
fourth so all right this pretty good
question regarding the face you we
currently do not offer as far as I know
any any gestures as you call it so I'm
really just about any custom gestures
detection like a blink and so forth you
can you can you can detect expressions
so that's a smile that is um that is
expression of surprise but not
necessarily a blink what you can
actually have is having let's say the
location of the tip of the eye
the end of the I then if from that you
can you can manually infer a blink and
then afterwards from that you can mix it
up with a data that comes from the other
let's say SDK modules like if you have
if you wanna also to understand in to us
whether the mouse is open or closed so
that's the same model like the face
tracking if you want to actually
understand some more information about
the depth of the hands you can take that
as well regarding the 3d depth gestures
so there are currently several gestures
that are being provided some other
gestures are going to have to be
manually planned for the moment there is
a gesture of the ring tool I'm not one
hundred percent sure if it's provided
already or not I'm but I think I think
that this the gesture authoring tool is
not provided along with the SDK at the
moment but it is planned for the few
common future and the gesture authoring
tools supposed to provide you the
ability to create your own gestures
whatsoever however because you don't
currently have it that's at least what
I'm assuming so what you can do is like
I said use the raw data in order to
infer about what kind of other gesture
you want to create if you want to
understand that there's a feast and the
hand is moving back and forth so what
you're going to have to do is basically
have the hands location and track it and
actually understand it's going back and
forth and also have the hands openness
and understand whether the hand is open
or closed and combine these two together
alright so let's continue so you you've
seen like I said so far multi user
interactions moving from small scale to
large scale and now I actually want to
show you how we move from let's say
table scale to a room scale so in this
demo it's a bit complicated then i'm
going to explain as we go alright so in
this demo we actually have that's there
we go we have
I'm going to use the interaction inside
a room you have three users flying a
ship actually one the one user that
that's me now as you can see it's I'm
flying the ship with my hands with my
gestures inside the room and the other
two users are hobbling are holding a
tablet or computer vices and they can
see the ship flying inside the room and
both of them like i said seeing the same
object flying in the room from their own
point of views and they can interact
with it so basically we have the shape
coming into the room from the ceiling
from the clouds the user don't really
see is sealingly sigler clouds and they
both see the street flying the room and
the next thing that happens is actually
the ship is being attacked and we have
all three users trying to the friendship
so the pilot is trying to navigate the
ship running away the ship is being
attacked by dragons and you can see the
dragons flying around the sheep and the
part tries to navigate the ship along
the as far as you can from the Dragons
while the users lab they are attacking
the Dragons throwing fireballs user
interactions inside the big room we're
not now we're not constrain to the table
anymore we can move all around and this
is not actually a small concept demo you
can see how the birds are falling the
user this is actually just to show how
we understand the user's location in the
room so you can see how the user moves
around and the birds are following all
right we can stop right now all right so
as you've seen so far we have a lot of
types of interactions we moved from from
one user interaction to multi user
interaction from small scale to large
scale and we move that we move a lot
from back and forth between the future
world in to really try to combine all of
these worlds together and actually
remove the difference actually blend
them together and have have that you we
do that in using the augmented reality
of course alright let's continue so
we've seen the multi user interaction
and now I want to talk to you a bit
about the challenges that comes along
with document reality experiences along
with all these new experiences along
with the camera and SD cater and the
data it provides but we actually have a
lot of challenges that comes along with
them for example we have a matter with
this we're talking now this time about a
lot of compute power that is required
because we are handling a lot more data
this is three three three dimensional
data is a lot more than handling two
dimensional data and therefore you have
to a handle to have a more responsive
device you have to be very aware of your
battery life you have to have a robust
platform and this is a basically what
interest trying to do to push for the
world cup of computers and into the next
generation of computers and in order to
do that we and this comes straight along
with all the new capabilities that when
we want to have a more if you want to
have a natural interaction with a
computer vice with one ever more now and
natural more intuitive interactions we
want to compute device to always listen
to always be aware of us we have to have
a better battery life and more
responsiveness and this is exactly what
we're trying to do here so the next area
would be actually the unity toolkit and
I'm not really sure how many of you guys
are unity developers so maybe it would
be good idea to make a short a small
poll just to understand how many of you
actually developing in unity actually
out there is a pole I can see that not a
lot of you so i want i will actually
tell give a brief intro just to give you
the small concert what you can do with
the unity toolkit and and then we'll
continue now just quick see we have a
couple of and questions so wherever ng
asking what's the difference between 2d
and 3d
face detection so.2d face detection is
actually legacy there is no more to the
face detection investigate the key in
the 2014 SDK you have a 3d face
detection which is a lot more robust it
can do it can handle a wider angles of
of the face when you had a 3 a 2d face
detection with space actually just on
the RGB just on the regular depth camera
and the 3d bare face detection is based
on the depth camera as well on the depth
data and so we can do a lot more robust
detection and understanding and requires
less compute power of course so this is
me basically the major difference um so
see here a small a question about a lot
of elements new webinar from the last
session so forth so those photos of you
were asking about the last second this
is the same women actually still a
session
you
you
is a mere you is it seems your problem
itself um okay special drink okay
alright so for those of you who are not
aware of what unity is well unit Lisa
gave development platform and this is
how it looks like and it is very very
successful like mean we what we actually
did with the with the resistance SDK is
provided along with sdk a plugin that
connects to unity and provides all these
capabilities that you've seen so far the
how it provides an easy integration of
all these capabilities that you've seen
so far into the unity environment so
like a set the goal the goal was
actually making it easy to create real
sense content we're talking about
drag-and-drop capabilities we want the
users instead of having the users
actually diving into the realsense sdk
code and starting to open pipes and so
forth we are what we provide and you
will see that in a moment is the drag
and drop capabilities minimal coding
required we stay aligned to the unity
interface the one that the unity
developers are already using as well and
we provide a our capabilities and many
more so let's let's have a look right
now so this is basically the unity
environment what you can see here for
those of you who are less familiar with
unity is this window right here is the
game environment the game is actually
what the user is that are playing the
game can can see and above it we have
the scene the scene window here we can
actually see the 3d world from all
around you can see here my camera for
example the point and I can actually
move around and have a look at it from
different angles and and the rest of
these windows I'm not gonna get in very
into into technical details but they
basically contains all the objects in
the in the
so the first thing I want to show you is
basically how we use the resets to
create a very easy very quick
interaction the first thing is once you
import the unity toolkit you can eat
what you will get is this folder here
Wilson security tool keys folder and
this folder has a lot of a lot of
interesting things but the main thing
would be the actions so the actions is
basically a bunch of scripts that
provides you a drag-and-drop
capabilities into into the objects in
the scene let's say for example you want
to put a certain object tracking over
your hand so here if you can see you
have a tracking action and you can
simply drag it into the object and have
these object tracking your hand and I
will show that in a second but it's
really important for me to explain for
those who are not really familiar with
unity that this is the way basically
unity works is using scripts that you
dragged into a game objects so let's
let's have a quick look about how does
how it works let's say let's create
first of all a cube you can see the cube
right here i'm going to move it two to
the center screen and may be scaled a
little bit so you can see there we go
now in order let's have a some some
light in the scene so we can see it and
let's rotate the cube a little bit so we
can see that it's a 3d cube there we go
so so far I haven't done anything
special nothing that has to do with the
reasons basically we're talking about
only unity capabilities and so and if I
press play I actually have just a cube
sitting around the world nothing happens
so in order to add now research
capabilities what I'm going to do is
actually take a tracking action for
example and throw it over the cube so
once I press on the cube you can see
here all the properties that this cube
has and we can ignore all their unity
properties and go straight into the
tracking action and what you can see
here is actually how we have the
tracking action that let's see that is
being attached into the queue so
currently we're talking about a hand
tracking and basically what I did was
actually just track and drop and I can
immediately press
a-and I don't know if you can let's help
us off to open the webcast so you can
see me as well there we go so I'm
guessing now you guys can see me so
basically what i can do now is basically
just display my hand and let's put a
focus right here and you can see how the
cube immediately tracks my hand I
haven't done anything special just throw
the crack the tracking you I were
talking about 3d tracking we're talking
about i can move back and forth in the
in the world and we're talking about
orientation as well you can see how to
keep change orientation along with my
hands if i move to our hands back and
forth if i move it to the right and to
the left and this was very simple and
now i can actually play this tracking
action comes along with a lot of
features you can play with a lot of them
basically you can constraint you can
stop the tracking in certain axis or you
can stop the rotation on certain axis
you can you have it you can invert it
you can define actually there are a lot
of love features you can play with here
along with and very easily i can switch
between hence tracking for example in to
face tracking or into object tracking if
I want if I wanted to track over a
certain object let's switch it to a face
tracking for example press play and now
you're going to see how the exactly the
same here is actually tracking my face
right now so let's have a look there we
go so here we go and you can see how
when I move my face the cube moves along
with my face I can go back and forth and
you can see the orientation as well
there we go so I'm not like I said I
don't want to go very technically with
you guys and just want to give you a
short and small concept what do we have
what does the unity took it provides so
basically what you can see here like i
said is a bunch of scripts for example
you have here like tracking action try a
third action which means you can
actually hold it object grab your hand
close it and then move it from one place
to another and release it or do
different gestures in order to play with
you have a scale action which means you
can hold the object and scale it or
rotation action means you can rotate it
from from from different angles and a
lot of a lot of action equals stuff here
you have a higher action which hides the
action you have a several enable and
disable behaviors which allows you to do
certain type of actions which means that
if this happens so disabled that
behavior like if one thing happens
disable the tracking action and so and
then you don't have any more data
tracking stops or starts so it's also
important actually to say that all these
features all these capabilities that are
providing with you to get our open
source you have everything all the code
is open free and you can it was built
that connected was a plan was for you to
talk to have it very easily for you to
be able to so you will be very tight I'm
sorry so it will be very easy for you to
change things and actually adapt them to
what you guys need so i'm going to show
you example what we did with the unity
two key let's say they are mirror is a
good example this in this case because
it provides a bunch of capabilities so
in this case we actually what we
actually did is took a lot of objects
and have them tracking over several
elements the first thing you can see is
actually tracking over my face you can
see how I put tracking over my nose my
mouth I use several landmarks and
consider tracking if I put my hands as
well we took actually bunch of objects
and have them tracking over our hands
consider orientation how you can move
over hand and open and close them and
this was done actually very very very
quickly in very simple way I'm talking
about Lester but somewhere around an
hour was just actually grabbing the
scripts and throwing them over the
objects and then playing along with the
parameters and having them having direct
parameters that fit our needs and of
course all the parameters are not
explanation for all these parameters is
being our are available free Liam under
the commutation so before I go through
several demos
actually I'm not sure if I have time for
it before I so let's go over some
questions and certain answers and if we
have time going to show you some demos
of things we actually did inside the
Indian toolkit itself so we have run G
asking if saying I do see the I are
streaming camera view example since I
are is used for depth calculation but is
there any simple use case to use IR
string directly well we haven't used the
ire string directly no I don't so maybe
you will find a planet suited for your
needs but as far as I'm aware of there
wasn't any usage that we we have done
with die or stream directly I Ruben is
asking how can we how can be how can we
have a shadow of the hand movements on
the screen while it is tracking the hand
for example I have a virtual stick in a
scene and I want to pick it pick it up
and the throw it away so basically
actually we have that in a very good
sample so i will use use this
opportunity to show this demo day I want
to show in anyways something we call
that real fishing so what you what's
actually nice to see in this demo is how
we use the camera in a different way
this type this camera is actually facing
facing up and you can see how when we
move our hands above the camera you can
see the shadow of the hand we can
actually touch the screen you can see
the ripples in the water and the point
in this game is a it's a provide
experience of going fishing so you can
see the shadow of the hand hovering
above the water and once the user guilt
goes closer you can see the shadow goes
gold closer and it is it is pretty easy
actually to do it you can use either the
the point loud or you can use the hand
tracking and imitate imitate the shadow
of the hand but what you're going to
have to do i'm not going to go very
technical again so what you could have
to do is actually take the raw depth
data that stands above the camera and
from it have a projection of it into 2d
actually I think asking can you please
provide some guidance for app
development in c-sharp so the SDK itself
provides them with all the other SDK
features are also available in c-sharp
for those of you of providing our who
are developing in C shop and you can you
can actually see in the SDK folder where
you have the binaries I'm going to
quickly show it right here if you go to
the SDK folder you have right here the
binaries you can also have every binary
that you have here you have in C++ and
in c-sharp and all of them are provided
along with the actual code so if you
need samples if you want to have a look
about somehow something is being done
simply go into the SDK folder and have a
look and of course everything is also
available in the documentation itself
alright next question so we have RL
asking just a second there we go if we
detect custom gestures coming from the
raw data how do we detect you the user
is making the same gesture user won't
repeat the exact same gesture but I
assume similar set of XYZ points so
again this is a matter of how you you
understand things in order to detect the
certain gesture what would actually
whether what I would actually look for
let's say for example the the one you
are looking for previously was a fist
being moved back and forth so first of
all you haven't beauty tool kit you can
see how this is being implemented over
there but the concept would be looking
at the closer the openness of the hand
you want to see that the hand is closed
and this is doesn't really matter on
which uses closing and where is he
closing it but you want to see that the
hand is closed and then you want to sit
it moves from the disease of the hand
changes into a certain amount you don't
want to make sure you don't want to
check whether it was from point A to
point B but you want to look at the
Delta the difference you want to make
sure the Delta is bigger than a certain
amount and they change this back and
forth a couple of times as many times as
you want the check all right so let's
see
excessive I feel it would be more
productive to progress need to we'll
catch everything up to speed be my app
you tube channel right doesn't where can
I report the reliability problem with a
3d segmentation sample this is a genuine
problem definitely not caused by the
visual so if you have any actual and
this is true for all of you guys if you
have any actual technical issues so
forces the technical channel this is the
right place to start reporting things
and asking them and I would generally
say it is either you're not using things
correctly or there's an actual problem
of course into the 3d segmentation issue
if it's a binary there shouldn't be this
this shouldn't happen that should be
actually in an actual problem all right
do you need unity pro in order to use
the unity toolkit very good a question
so basically yes you do need unity pro
no do you do to get unity currently
supports three a 3rd party plugins only
in the pro version however there is I've
heard it out ways around it I'm not
talking about illegal ways I'm talking
about but basically if you if you want
you can breeze through the internet and
see if you can i use the UT free version
some other way at the moment we don't
really have any official agreement with
the unity of how of having this embedded
into unity so the answer would be yes
what is the presenters email id well any
other questions you have basically i
would suggest you guys not to not to to
go through me but to go through a saline
so I'm sure so he's a he'll be able to
answer all the technical issues um all
right so let's
look about last thing actually exceeded
Adams here I slim more question at the
moment the last thing I actually want to
show you is how we took the unity
samples and very easily create a nice
interaction we took a Unity game this is
a prebuilt Unity game that's provided
for free by unity and we connected to
the camera the 3d editor I'm sorry the
face tracking so you can see with that
when the user moves around his head the
camera tracks along with him and so the
user can actually take the the player
and hide behind the tree and then peak
with his ad actually move his head to
the right or to the left and peek around
the tree and have the camera move around
to the right to the left and this
provides a lot more a better experience
of playing as well as when you move in
ECE and you see the camera moving with
you even if it's a tiny movements where
the tape when these tiny movements move
along you feel like the users actually
looking through your eyes and it's
provides a lot more immersive experience
with the game so this is something very
nice very easy to create and just to
give a small concept of the things you
can do very quickly with the SDK and
with unity Toki so another question we
have here how do you get a voice
recognition to work correctly the SDK
sensitive volume is too low well I get a
very technical question i would i would
recommend you using looking through the
documentation in general there's a
matter of which device you're listening
to so if the if des des kay is actually
listening to another device if you have
several microphones like a microphone in
the camera or microphone in a in the
computer itself and one of them isn't
muted so it may tell you that the volume
is too low although you put maximum
volume so this is would be a good idea
to have a look at that another question
by jerry can the recess sdk do basic
image processing filtering a fresh
holding all right so we don't provide
all with sdk and actual image processing
libraries there are several things that
are being provided for example
smoothing a smoothing model that's
provided along with sdk i think you can
see if you can see it being used
somewhere in the samples and definitely
in the documentation but in general we
don't provide a filtering of
thresholding we provide a higher level
which is the SDK itself which provides
you of course the hands tracking and
gives you the direct location of the
hand every frame and if you want to walk
even higher than that then you can go
straight into the things like the unity
toolkit which you can actually give you
tracking straight over the hand for
example so this is that any other
questions you see we're almost out of
time all right seems like these oh this
is about it so everyone thank you for
your time thank you thank you very much
for the presentation okay can you hear
me yes yes yeah okay perfect yeah so
yeah thank you thank you very much you
know for answering you know all the all
the questions we are still in or having
or some other question or popping out
but you know I think we'll get applying
by this you know offline reflecting on
the questions and transcript or
scripting sorry not the answer that
another provided and we discern we knew
this by mail for the videos will be will
be available on the demon up channel
they would also include NV sum up email
and just you know one quick reminder so
as you can see it now on the screen this
is just one slide you know to a reminder
for the one thousand dollar change which
is you know for the early submission so
you can submit your app before december
17 to be one of the 80s person that will
be selected by the jewelry in fact you
know 80s is 50 for 15 years right entity
for the Ambassador clock and this is
completely something that you can make
you know with the also participation for
the whole channel so you can submit one
first demo for this early submission and
then you can all for show you know
continue working on it and you can also
know finalize it so before February's 13
to be eligible for the final price pool
the good thing with this is that you
know we'd be having us some dim sum
sorry big event or say coming before the
the end of the week channels so if you
are one of the luckiest a key persons
you may be contacted by our marketing
stuff in order to showcase you know your
your apps that's project i would say
that's one of our big events especially
Mobile World Congress or IDF and gender
and don't know if there is any more
question regarding the contest shit so
yeah that I think the other I thought
you know where I think it would be more
easier for you guys not to follow
already followed sadies not on youtube
if you have any question i already
showed you know my email on the chat so
slim the expo ussi at in telecom so does
it take to make me out forward to the
available technical expert because you
know
thank you very much for your help and
here but you know it must really be the
person who this would be no always
asking all the questions and thank you
Alex you know for organizing my decision
thank you guys thank you everyone good
luck thank you just right
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>