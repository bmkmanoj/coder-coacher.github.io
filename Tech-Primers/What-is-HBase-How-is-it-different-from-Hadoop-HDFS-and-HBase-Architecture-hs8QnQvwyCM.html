<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What is HBase? How is it different from Hadoop? | HDFS and HBase Architecture | Coder Coacher - Coaching Coders</title><meta content="What is HBase? How is it different from Hadoop? | HDFS and HBase Architecture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Tech-Primers/">Tech Primers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What is HBase? How is it different from Hadoop? | HDFS and HBase Architecture</b></h2><h5 class="post__date">2018-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hs8QnQvwyCM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have heard about what is Hadoop and
its familiarity the de big data
processing system there is a new
database or a data store called the
Apache HBase let's see what is Apache
HBase and how it is related to harue or
how is it different from her
press the bell icon on the YouTube app
and never miss any update from tech
premises
so what is HBase HBase is based on
Google's BigTable it is a completely no
sequel datastore which is built over the
HDFS file system we'll see what is HDFS
in the next slide but just understand
that HBase is built over HDFS it is
nothing but a no sequel datastore rather
than as calling it a database we can
call it a datastore it also can use the
MapReduce computational framework to
retrieve and store data
it is completely column family oriented
so what is HDFS HDFS stands for Hadoop
distributed file system it is generally
useful when you want to scale your
cluster from one to hundreds or even
thousands of nodes where you want to
store and retrieve data we'll see that
architecture of HDFS in the next slide
HDFS is based on the Google file system
GFS Google has a white paper on GFS HDFS
is completely based out of that it uses
the master/slave architecture and it is
highly fault tolerant that is why people
go for HDFS because of the way the
replications are done by default the
HDFS the replication factor is three so
you get three copies of the same data
when you are storing something in the
history of us system so how does this
hedge TFS looks like we just talked
about history of us and HBase let's see
the architecture of how do they look
like this is the general architecture of
HDFS so HDFS architecture has a
master/slave architecture as I said the
master node is called the name node so
name node is going to be called as the
master node and the slave nodes are
going to be called as data nodes so
these slave nodes consists of the data
which is going to be stored across or in
a distributed fashion and name node is
going to help in storing this data and
it is going to track what data is stored
in with
particularly so let's take this
particular example where we have five
slave nodes and all these are going to
be data nodes so data nodes in the sense
if let's say we want to store a file in
an HDFS architecture and let's consider
the size of the file is one gig name
node will now split that one gig file
into let's say into five different
pieces and it will be stored across
these data nodes and name node will
identify okay this is the part which is
stored in this particular slave node or
the data node so this is how a file is
stored so name node which is nothing but
the master has the metadata information
of the data nodes there and which by
which data node has one data about that
particular file and the fault tolerant
feature of HDFS comes when this
particular data node is replicated in
two different copies so in our case we
will be replicating into three copies
basically so there will be 3 copies of
the data so whenever you write a file if
the file is getting split into 5
different nodes then there will be 3
different copies of the same file across
these nodes so this is how it looks like
in the traditional HDFS architecture
so his TFS is useful when you want to
store a file in a distributed fashion
when you want to store huge amount of
file not just a file it is going to be
in jeebies or tb's and you want to read
it faster when you want to store it
these will be stored or split and will
be stored in parallel same way when we
are reading it these will be read in
parallel and they will be able to
quickly read these files so this is HDFS
architecture so Hadoop uses MapReduce
plus the HDFS architecture it uses
something called yarn in order to
retrieve data and collate it and send it
but here we see only the HDFS
architecture
so now how does the HBase architecture
look like so we saw what is HDFS how
does that hedge staffers come into
picture inside HBase architecture so
HBase has a similar master slave concept
but it reduces the single point failure
of how we had it in HDFS so if you see
in HDFS there is only one master and if
the master fails we won't be able to get
the data from the slaves right so HBase
can eliminate that
in hich base we have something called H
master so H master is similar to how we
had master in the HDFS file system it
also has the concept of region service
so region servers are similar to EDD at
data nodes where if you want to scale
the nodes we will be setting up new new
region servers so you can consider data
nodes as the region servers so in H to
be his architecture there will be only
one pitch master we can have multiple
hich masters but only one hitch master
can be active at a time so it will be 1
H master and then different region
servers so the next component which is
there in the HBase architecture is the
zookeeper so zookeeper is the service
discovery pattern which is going to keep
the H master and the region servers
together so if a client is requesting
data from the HBase architecture it has
to come to the zookeeper instance so it
needs to hit the zookeeper instance from
the zookeeper instance it will be router
to H master or the region servers so
when we query data from HBase we will be
hitting to the zookeeper and zookeeper
will route to whichever region we need
to go or whichever master we need to go
so that is how Suki / helps in tracking
across so if let's say H master needs to
talk to the region servers it still
needs to come to the zoo keeper because
zookeeper W what are the different
regions which are present inside the
HBase architecture or the cluster inside
the region servers we have several
blocks so we have something called block
cache this block cache is nothing but a
read cache which is created to give
faster read access let's say there are
some frequent data reads which are
happening in the cluster so all those
data will go into the block cache so
this will be present in order to give
faster availability or the faster data
to the client so this block cache helps
in doing that there is something called
mem store so mem store is a write cache
and it stores all the new information
which we are writing until it is written
to the disk every region inside the
region server will have the block cache
and the mem store
regions are nothing but inside the
region server there could be multiple
regions and this regions are related to
a column family so let's say I want to
create a column family called user then
that particular column family will have
block cache and the mem store similar to
that you will have multiple column
family in a region server so you can
have multiple column families inside the
region server and each of these are
called regions so block cache and memory
have different regions so there could be
multiple regions inside the region
server and these two constitute or
region wal is nothing but right ahead
log so right ahead log is the file where
the new data is stored and which is not
persisted yet to the permanent storage
mem store is where we wrote something
and it is present in the cache and if it
is not written to the disk you write
'hail log is a file which stores the new
data but it is not persist to the
permanent storage so which is nothing
but the final data node the final thing
is the H files a H file is the final or
the actual storage file that stores the
row as a sorted key value on the disk so
this is how the architecture looks like
but where does this region server fit
into HDFS architecture right so we were
saying HBase is built over HDFS so this
these region server processes will be
literally running on the data nodes
these region servers will be running
inside the data nodes and inside that we
will be having regions and we will be
having H file so H file is the final
file which has the data so this is the
typical architecture of HBase at a
higher level so if you have hundreds of
nodes in your cluster there will be
hundreds of region servers inside that
you will have multiple regions you will
have block casual ammmm stores and right
ahead log or the H file inside that and
there will be a central zookeeper
instance via which we can read and write
to different region servers and hitch
master is just going to route these
requests to the region service if we
need to write data or read data then H
master will redirect our request to the
region service
the additional tasks which hitch muster
performs is it also manages and monitors
this cluster which is present it also
administers in creating these tables
right when we Wendy say I want to create
a table which master says ok I know that
ok there is already some data in
particular in a particular region go to
that particular region server and write
some data it also handles the failover
so H master also takes care of the
failover of the nodes which are present
in the cluster it also request changes
from the client if there is let's say a
schema change a metadata change then H
master is responsible for changing it in
all these region servers so H master
takes care of that as well and as I said
earlier zookeeper is going to track all
the region servers which are present in
the cluster and it is going to be like a
service registry so it will have all the
information about the different region
servers and the H master in that
particular cluster
you
so where can we use HBase HBase can be
used when you want to process huge
amount of data when the file is in
terabytes or petabytes if you have less
GB of files HBase will not yield you
more value because the number of
instances of the nodes which you have
deployed is completely high and the cost
for maintaining the HBase is going to be
extremely large that is why if you are
having huge amount of data and you want
to process them then HBase make sense
because you will be having that many
number of nodes in order to process huge
amount of information and it makes sense
when you have petabytes of information
or terabytes of information usually the
throughput is completely high the reads
and writes are faster and you can do
random reads and writes so why do we
need a patch base we could have gone
with Apache Hadoop right why did we even
use Apache HBase
how do Bri writes the whole file when
you want to write something to the disk
however Apache HBase writes only the
part of the file for example it is like
a data store but Hadoop is not like a
data store so in HBase you can modify
data inside the HBase database you can
read you can write you can update HBase
is built over Hadoop in order to
overcome the drawbacks of HDFS if you
see the HDFS architecture there is a
single name node and there are lots of
data nodes if the name node fails the
whole cluster is inaccessible however in
case of hitch base you will be helping
multiple hitch masters if one of the
hitch master goes down there will be
other hitch masters too so our purpose
and using zookeeper you can redirect the
call to the corresponding region servers
so HBase builds over the drawbacks of
HDFS and it allows faster and random
read and writes which are optimized in a
way so that is the major advantage of
the HT HBase because we want to write
and read perrolli and random
because our data could be residing
somewhere in case of Hadoop we read the
whole file so you will be reading the
whole file but in case of HBase we will
be reading the data so the data could
reside in different nodes we want to
read it faster in an optimized way right
so that is why HBase is used so who is
using his space Pinterest is using HBase
goai bebo is using HBase and in fact
Facebook is also using HBase so if you
know Pinterest has almost like 38
servers or maybe more I'm not sure what
is the latest count which princess has
there are 38 different HBase clusters
which can read up to 5 million
operations every second HBase is
completely popular and these are
different technology companies which are
using HBase in their production in one
same way Facebook as well
Facebook processes petabytes of
information using the HBase cluster they
do data mining over the data from the
HBase instance so what did we see in
this particular video we just saw what
is HBase we also saw what is HDFS how
was it implemented from where it is
implemented we also saw the HDFS
architecture and the HBase architecture
we also saw why do we have to use HBase
or why did they create his face when
they had already heard OOP and their
HBase can be used I hope you guys got a
basic understanding of what is HBase and
HDFS if you liked the video go ahead and
like it if you haven't subscribe to the
channel board and subscribe to it wait
again in the next video thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>