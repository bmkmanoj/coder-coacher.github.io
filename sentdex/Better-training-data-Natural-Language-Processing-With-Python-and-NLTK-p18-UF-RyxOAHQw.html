<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Better training data - Natural Language Processing With Python and NLTK p.18 | Coder Coacher - Coaching Coders</title><meta content="Better training data - Natural Language Processing With Python and NLTK p.18 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Better training data - Natural Language Processing With Python and NLTK p.18</b></h2><h5 class="post__date">2015-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UF-RyxOAHQw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to
part 18 of our Python with ML TK for
natural language processing tutorial
video series in this video we're going
to be talking about importing a new
dataset so why are we going to import a
new dataset one you should get
comfortable with how to do it to the
other dataset wasn't working with where
we want to move in the future so didn't
really have anything to do with that
stochastic gradient descent doing 100%
positive so don't really know why I was
doing that but we're going to move on we
want to use this I was trying to think
of a good project for us to use this on
and the best thing I can think of is to
go to like Twitter and do live Twitter
sentiment analysis we can graph the
sentiment live so I wanted to go to
Twitter I used that previous dataset on
Twitter it literally wanted to call
everything negative it was really brutal
so and I think the reason why that was
is because the text is so short compared
to the text that we trained against that
because we have a binary classifier it
was defaulting to negative so we don't
want that we want something that has a
little more certainty so we need to
train it on a some data set that has
smaller blurbs of text yet is also
labeled I just so happen to have one of
those data sets so I went ahead and
uploaded that data set to Python
programming net so that everyone else
can use it I'll show you guys how we're
going to import that data set in kind of
this way you can see to that here's
another example basically of remember
before I was telling you you can use
this on a binary labeled data set so any
data set that's one thing or the other
you can use this algorithm and so this
is here an example of us taking this
algorithm applying it to a completely
different body of text training it and
then we're going to move forward with it
so anyway that is over here let me bring
it over that we've I've stored these
files on under static downloads and in
short reviews so head there and you'll
see that you'll have a negative text and
a positive dot text we'll just open up
both but you're going to want to save
them so open them up and go save as
so then we'll view the source here and
you can see that you know byline is the
is the review so these are separated by
a new line and you can see that these
reviews are actually much shorter this
one's on
four words but almost all of these are
no longer than the width of my page
writer or at least not much longer right
this one's a period longer but you the
point these are much shorter also our
last dataset had a high degree of
volatility regarding the algorithms
accuracy that's kind of indicative of
you've probably got too little data or
you've got a bug in your algorithm I
didn't think we had a bug in the
algorithm although the stochastic
gradient descent something's wrong there
but I think that the the real problem
was indeed a lack of data so we only
have a thousand of each side that sounds
like a big number but really not so this
one's a lot bigger we've got fifty three
hundred reviews for each a little over
so that's the negatives and then here's
some positive ones again view source
split by new line there you have it so
save this and save it to the way that
I'm going to write it anyways is
basically I've got my script it's in its
own directory then we've got another
directory that's in the same directory
as the script another directory and I'm
just calling it short on our score
reviews and then that's where I'm
storing a positive text and negative
that text which is all of these reviews
so let's go ahead and get started so
we're going to come down here and
basically from this frequency
distribution right above that all
everything above that gets deleted
except for obviously this book
classifier class so just delete boom now
what we want to do is we need to define
obviously we need you know basically we
need to solve all the things that we
just deleted it okay so we need to do
all words and we need documents so let's
go ahead and delete and get started
there so first of all we need to access
the file so we're going to call this
short positive for short positive
reviews equals open and again I stored
mine in short underscore reviews slash
positive text we want to open this with
the intention to read and then we're
going to go ahead and read it then we're
going to do the exact same thing for the
negative so I'm just going to copy paste
and we're going to call this short neg
and then it's negative dot text read and
that's fine so now we're going to have
documents that's gonna be an empty list
and then we're going to say for
are for review in short underscore pose
what do we want to do well we're going
also when you split this binding line so
we'll do that here so for the review in
short post dot split by new lines that's
splitting them up by review what do we
want to do what we need to do document
start at the end and then if you
remember this documents is a tuple and
the tuple is consisting of the review
and then whether or not it's positive or
negative so these if they're coming from
short pose that means they're positive
so that's that now let's copy this paste
and then instead of short pose it's
short neg and then we want to append the
review and then neg at the end of it
next up what we need to do is come on
down here and now we're going to do
something pretty similar for all words
as we need to populate all words so
we're going to say all underscore words
equals and empty lists and then we're
going to do short underscore pose
underscore words and that's just an
equal word underscore tokenize
underscore whoops not underscore
parentheses short pose and then we're
gonna do the same thing to neg this neg
we're torquing it tokenized tokenize neg
and the ways obviously we're just we're
just converting these two words so if
you remember the NLT Kaye had like a dot
words method we don't we don't have that
functionality so we're going to just use
the other functionality which is just
word tokenize so boom done now what
we're going to do is coming down here
we're gonna say for W in short
underscore pose underscore words what do
we want to do we're going to all
underscore words dot up hand and we're
going to penndot our W lower and then
we'll do the exact same thing here right
under it so then for W in short neg
words what do we want to do all words
dot append and then W lower okay and
then we're done all the way up to all
words for the frequency distribution now
I wanted to have up
5,000 features first of all a couple
things one is this feature set is should
be faster to load than the other one
also we're gonna end up pickling
everything so you might as well have
some extra you know large sum of data
5000 seems to do pretty good so I'm
going to keep it at 5 you can do what
you want with it but I think five is a
good number so there's that also keep in
mind that if you wanted to you could
also since we're using an ltk we're
tokenize we could do some part of speech
requirement so kind of keep that in the
back of your head that might make a
difference so keep that in mind now what
we want to do is we're going to come
down here feature sets that's good
let's go ahead this is okay we're going
to also just call real quickly a random
shuffle and we're going to shuffle up
feature sets so again our data is not
perfectly you know a bunch of negatives
and a bunch of positives I'll shuffle it
up there then here the training set
let's say the training set now is going
to be everything up to 10,000 and then
the testing set will be everything
beyond ten thousand ten because this
data set has like ten thousand six
hundred and sixty-four I think feature
sets so once that is done you should be
able to run everything else so let me
save that and make sure this kind of
Elise compiles for us okay word tokenize
not found so let's go ahead and bring
that in come to the very very top and
we'll just import that so from NLT kay
tokenize import word underscore tokenize
save and run that let's see if this
works this time
just to save some time I have already
pre run this script ouch that's a really
bad accuracy there 47% enough to see the
other ones get that it's worse than what
I've gotten on average from this this
algorithm so we've possibly got some
sort of error going on in the code pose
neg let me just look through it while
this is happening all words choking on a
short closure
mm-hmm frequency distribution all words
good to could be good
let's compare over here any others
coming up now
yeah the Maddox secondary narrates
rounded it supremacy so really it would
only be oh okay here's here's why so we
left this this old training and testing
set that was our mistake that's probably
that's all certainly why we got that
really horrible first rating let me just
close this one more time let's just
close that
reopen it hopefully this time we will
get such a horrible horrible answer
there as the for the basic one I've been
getting upper 60s on this one usually
sometimes even into the 70s what what is
this why would you do this to me it's
still giving us a forty seven percent
accuracy that is no good random dot
shuffle feature says let's see
positive not text documents a c4p on
those documents dot append all words
pose for Ning for W in short pose words
all words dot append lower that's fine
fine feature ah okay I know what we've
what we've done messed up okay so I'm
fine features what we have to do here is
remember work okay um so initially we
had words was equal to the set of
whatever document we passed through well
the document we pass through now is a
big like string it's not like words so
doing this is no good so we're going to
actually have to say let's see words
will equal C words equals document and
we need actually actually it's going to
be word tokenize document so words
equals word tokenized document features
will be the same features never blah
that should be fine yeah okay let's try
that one more time so a few initial bugs
there so hopefully this one won't be as
bad let's just automatically do this
just kind of looking through everything
again just to make sure so anyways
well hopefully this will is work before
we run out another like 20 minutes
trying to debug why it's not giving me
the upper 60s um it should work
eventually and you you can actually
specify so again with since we're using
n LT K we could specify we can even grab
a part of speech tag still give us
horrible horrible numbers here I have to
debug this I suppose on my own and I'll
take it 18 this is going here we're
loading ow well that's probably our
problem action so aside from like all
these other problems we're loading this
old classifier that we you know from
long ago so yeah it only has 50%
accuracy because it's a completely
different data set but let's see let's
see what the accuracy comes out to be on
the next the next one because obviously
this with this one um let's see a we
would do this and then get rid of these
like this so no more pickle loading and
then bring this one up show most
informative future okay so that's fine
uh so hopefully this next one will pop
up in the next like minute here so so my
accuracy currently is at 50% which is
obviously quite pathetic okay so the
next one is the more realistic number
that I've been getting so let's go ahead
and run this one more time hopefully the
ninety phase accuracy percent will be
above fifty percent because the here's
the this is the current one anyway we're
not going to run through every single
one of them I just want to make sure
that the code that I've shown you guys
is the right code so everybody can
follow along if they need to so anyway
this is the other one okay that I ran
based on the code that we're attempting
to write in this tutorial and this is
basically every all the scores everyone
got so the original 69 all the way down
in the vote
last fire average which was sixty nine
point four here are some of the more
common words so yeah so that's that and
as soon as we see that this one has a
relatively positive the next thing that
we're going to be talking about is
converting this into a module that we
can use like we had kind of before where
you just call the classifier will
convert it to a module and then we're
going to call upon that module with
Twitter and pass tweets from Twitter
through the module get the
classification the classification
happens really fast
it's the loading of the data the
training of the data that takes you know
but this time so if you don't pickle the
data you'll have to initially wait
through this like you know one or two
minute loading time and then everything
will be like you know instant uh but you
can't pickle it I've already showed you
guys how to pickle so I'm not really
going do a tutorial on how to pickle
everything I'm going to kind of just
expect that you pickle so when you're
done training needs for example all of
these classifiers could be pickled the
other things that you could pickle the
speed up time would be you would pickle
documents you pickle all words you would
pickle word features so actually you
would pickle all words at this point
probably but you can pickle many
different point this this function here
is a really fast one okay so this one
came through with 64 so anything above
like sixty percent I'm contented with we
normally average at around upper 60s but
at least now I feel like the code is
accurate anyway
I'll finish talking about what what you
should pickle and then we'll cut this
video and move on to the next one but
anyway pickle that you should pickle you
can pickle feature sets you could pickle
I would leave these and then you should
pickle every single classifier pickle
them all the only one you won't have
success pickling or at least I didn't
have success pickling you shouldn't have
success pickling this uh so you'll just
have to leave it but this this pickle is
really fast because the the reason why
this took a while is just because all of
these had to be loaded in otherwise this
is actually
very fast classifier because all it does
is add up votes from the other
classifiers and the classification is
like milliseconds so so this one just
you won't be able to pickle it but
anyways pickle
you just pickle everything okay it's
making me hungry talking about all these
pickles it's honey ways that's that next
video we'll be talking about converting
this to a module again you should pickle
everything if you don't pick a lit that
is fine it'll just take a little bit
longer to load but I'm not going to be
showing you how to pickle so cuz I
already have
anyway that's it hopefully sorry about
the debugging issues but I think it'll
be best if I leave it in the videos most
people seem to be happy that I leave the
debugging in there plus if I don't leave
it in there probably some people are
going to miss it and not know what we
changed and I think at least in this
scenario it's kind of important to go
through the things that you should have
to change just so if you bring in your
own data you can do it and ideally we'd
have a much cleaner method for changing
you know data sets maybe that can come
in time but for now this this does the
job so that's it for this video if you
have any questions or comments please
feel free to leave them below otherwise
as always thanks for watching thanks for
all the support and subscriptions
until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>