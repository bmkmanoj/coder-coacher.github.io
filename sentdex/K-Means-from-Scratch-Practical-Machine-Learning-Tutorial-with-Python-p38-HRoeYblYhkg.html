<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>K Means from Scratch - Practical Machine Learning Tutorial with Python p.38 | Coder Coacher - Coaching Coders</title><meta content="K Means from Scratch - Practical Machine Learning Tutorial with Python p.38 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>K Means from Scratch - Practical Machine Learning Tutorial with Python p.38</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HRoeYblYhkg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to
part 38 of our machine learning tutorial
series I'm hoping to cover a lot so
let's go ahead and get started we've
been working on our own custom k-means
algorithm and we're going to continue
picking that up so at this point we have
at least classified all of the feature
sets and in this part here passing for
now but we already wrote the code that
will redefine the new centroids for us
and then what we're going to do is for
classification okay what we're going to
do next is we're going to say optimized
equals true again just like the support
vector machine innocent until proven
guilty here and Pampa we're going to say
is for C in self dot centroids what do
we want to do well we're going to say
the original centroid is equal to the
pre centroids Z and then we're going to
say the current centroid centroid is
equal to self-taught centroids see okay
so then we're just going to compare the
two and we're going to just say if NP
that's some of the current centroid -
the original centroid divided by the
original centroid times 100 point 0 is
greater than self dot tolerance we're
going to say optimize equals false so if
any of these centroids in their movement
move more than the tolerance in this
case 0.001 we're going to say no we are
not optimized that said if we go through
this entire for loop and optimize never
actually says it's false false we're
going to say if up to Mai's break so
that will break this for loop and stop
us from running through every single one
of those map
some iterations but otherwise we will
continue that for loop until we are done
with max iterations and then whatever
that centroid happens to be at the time
that will be the final the final
centroid so I'm going to leave propel I
will just do predict really quick
because this is relatively easy
so predict and do we already have
distances so with distances self dot
data feature set for feature set in
let's do let's just copy this line and
actually classification min distances
yeah so let's just take this copy so
predicted is pretty quick to write there
I mean so distance is equals that and
rather than feature set it's just data
and then classification distances index
min distance so then here we just return
classification okay I think that's all
we had to do so now we're going to say
the CLF where the classifier equals K
underscore means and then we're gonna
say CLF top fit X so train that bad boy
and now we're going to do is we'll plot
centroids so we'll say for centroid in
CL f dot centroids PLT scatter and this
will be CL f dot centroids centroid and
then 0 and then the same thing for 1 so
I'm going to do comma 1 and then we're
going to say I'm going to hit enter here
so we don't run off the page marker is
going to equal just an oh we'll say the
color for the centroids is K sighs will
be 150 line widths will be 5 so those
will be the centroids and then finally
we're going to say for classification in
CLF classes classifications the color of
that classification will just speak
colors and then that classification easy
enough and then we're going to say for
each feature set in CLF classifications
classification this is this is some ripe
code for some typos I'm just saying
we're going to say PLT dot scatter and
we're going to scatter feature set 0
feature set 1 marker will be an X and
then we'll say color is going to be
whatever that color choice was size will
be 150 line widths will be 5 okay and at
that point I think we're ready so we'll
do PLT - OH
see what we got okay so that's the
initial data all right so this is the
it's interesting that shouldn't
shouldn't maybe maybe that's hist it's I
don't know if that's classified
correctly anyway these are the the
initial to centroid points and it's
claiming that all of these are closer to
the red we'll see we might have made an
error there cuz I would think that's
closer I don't know I'm just eyeballing
it but maybe maybe because the Reds down
anyway but it's saying all these other
points are actually closer to this
centroid so but that was after in theory
all of those those iterations but now
let's go ahead and allow for this new
centroid okay so something worked
correctly so anyway here's centroid zero
centroid one or they might be reversed
as we found in the Titanic data set it's
really arbitrary which - oh there are
but anyway so a centroid in another
centroid and then these are the these
are the clusters that belong to each
centroid so lo and behold the code and
the algorithm works amazing we got lucky
there no type is impressive so then what
would happen if we wanted to add some
data right that'll be exciting so let's
check and test the predictions so I'm
gonna say unknowns those will be MP
array
and then we'll have it be an array of
data will do one two three whoops four
and five don't forget your commas and
then we'll say 1 3 will do an eight and
a nine feel free to do any data you want
to do it you don't have to copy me here
we're just going to throw some in five
four and then a six four and what we'll
do here is for unknown in unknowns the
classification is just going to be CLF
predict unknown and then PLT dot scatter
and then we'll do unknown zero on one
marker we'll make these stars
someone once complained that I used
different quotes and at different times
trying to use the same quote anyway I
know it's really important to most of
you
Evil's colors classification so it's
just that index value basically as size
will be one video line widths will be
five so these will plot unknown data by
that centroid and these don't these
obviously do not move the centroid cool
so you've got the original data and then
these are be like all just new data
points and again the prediction would
just be how close is it to the latest
known centroid but how might things
change if rather than unknowns being
here let's cut unknowns and and in fact
you know what I'm going to do I'm just
going to copy this this and then I'm
going to comment all of that out and
then let's just add unknowns to the
original list so it's not the most
beautiful but I'm not going to worry
about it okay so that's that's what the
data set now looks like let me close it
and it turns out that remember this was
actually a green data point but it was
actually quite close to this centroid
but and what's happened now is due to
the new group we actually have this is
now that the new centroid was actually
actually quite close to the one of these
points but it's not quite on that
point they've got a centroid this
centroids moved because if you recall
these were the three original data
points so this Centro is actually more
likely up here and I remember I think
these are three original here so the
centroid has moved a little bit there
too
you know it's kind of interesting to see
you know if you added new data what it
kind of does to that and then finally
what we can do is where are we printing
the distances for optimized if n piece
sum so this is that percent change so
what we can do is print so we'll just
print and then this will tell us how
many iterations it went through and then
also it'll tell us like how big those
are so you can kind of see how it
shrinks as time goes I'm going to
comment out that original plotting or as
it's here it is that's interesting so
huh this must be when maybe one of them
changed sides or something I'm tempted
to make like a live graph to show how it
iterates and goes through it's only like
four movements are actually we have four
so anyway the first one moves almost
500% the nice one 13% and then
apparently the next one 52% so it's up
there but unless that one of these
points probably got reclassified like
this point first on some reason maybe
got shifted in and so that made this
shift heart harder or something I I
don't know it's fat it's fascinating
it's absolutely fast it's riveting
let's close this let's say we go through
rather than see if we can change this
max iteration let's say first let's do
one we can kind of cheat this way right
so this would be like the first
iteration through our it just picks like
two points this would be actually the
second pass
so the first pass gave us about five
hundred percent so that was the first
pass and then if you change this now to
two this will be our second pass and by
our second pass you can actually see
that we've we've completed everything so
it's actually pretty quick so zero one
and then apparently two also I'm just
going to add this
really quick I was probably going to
have this on a different video but I
think I'm just going to stuff it in here
really quick the the source code here I
will put in the text-based version of
the tutorial but basically this is just
the meshing of our k-means classifier
onto the titanic dataset and seeing how
we compare to psychic learners to make
sure we get about the same result as
scikit-learn also interestingly enough
this appears to at least from my testing
calculate it at the actually quicker
then scikit-learn calculates it so I'm
not quite sure what's different about
this code in psych it learns code but
for some reason this appears to go a
little faster but anyway just to run
through it really quick we have and
maybe you know maybe the actual the only
difference is the import of like
scikit-learn might be just a heavier
import that takes a moment longer that
might be all there is to it anyway
because surely they've got more
efficient code that's cut anyway so this
is the code that we wrote before nothing
new here some notes for myself but
nothing new here the download for the
Titanic data set but mainly here this is
all the code that we wrote for the other
k-means one we're handling non numeric
data here so there's nothing there's
really nothing here that's new that you
haven't seen so I didn't really see much
point of rewriting this as a video
anyways first initially I thought I was
doing this there's really no reason to
do that so we'll fit to X actually X
there and then
because again there's not really no
reason to ever to actually train test
split with in this case of clustering if
you really wanted to know the prediction
but we would we just want to know how
well it clustered the entire data set
really so because it's not supervised
you can you can do this to get your
initial information so anyway we can run
that really quick oh wait
forgot about this there we go pull it up
here okay and actually we must pretty
high accuracy is wrong it's supposed to
be a little lower yeah that's quite the
variance there anyway so you know about
65 to 75 percent somewhere in there and
for some reason it runs a lot quicker
and scikit-learn but I do wonder I'll
have to maybe I'll add to the text-based
version or mention it in the next video
but maybe I should start the timer like
here so before the imports and or after
the imports because I wonder if the
importing from scikit-learn is bringing
in a bunch of baggage that we don't
actually need or something I don't know
but anyway it seems to actually
calculate quicker than the scikit-learn
version which is interesting anyway
that's all I just wanted to show this
really quick of us applying to it if you
want to check it out for whatever reason
I'll put the text-based version there
just didn't want to waste time you know
having a whole nother video on this when
there's really nothing to be being
covered here so you're welcome anyway
okay so that is your custom k-means
classifier so hopefully that was simple
enough I'm sure it was a nice break from
the support vector machine if you have
any questions comments concerns whatever
up to this point feel free to leave them
below otherwise in the next tutorial
we're going to be changing gears a
little bit and talking about
hierarchical clustering where we
actually let the Machine figure out
what's the best number of groups to
classify into and to do that we're going
to be using the mean shift algorithms
that'll be exciting so anyways questions
comments leave them below otherwise
until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>