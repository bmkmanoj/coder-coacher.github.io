<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build Your Own Supercomputer 1 - About Supercomputers | Coder Coacher - Coaching Coders</title><meta content="Build Your Own Supercomputer 1 - About Supercomputers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build Your Own Supercomputer 1 - About Supercomputers</b></h2><h5 class="post__date">2014-02-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/13x90STvKnQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody and welcome
to my supercomputing series my goal here
is to just get you all familiar with the
ideas and concepts of supercomputers and
then I'll actually show you all how to
build a supercomputer step by step for
as little as 80 dollars but probably
more like a hundred and twenty dollars
if you don't have all of these supplies
so that'll be fun but first we need to
know a little bit about what it is
exactly super computers are why people
use them and so on so super computers
have been around for a while they were
first introduced in the 1960s but they
really had very little purpose to them
initially they just had a few processors
kind of tied together though by the 90s
there were you know supercomputer setups
with thousands of processors nowadays
we've seen supercomputers with tens of
thousands or even hundreds of thousands
of processors that comprise them so that
gives you an idea regarding you know the
growth and evolution over time the main
idea driving supercomputers is this
notion of parallel computing parallel
computing very simply just means
calculations that are being solved
simultaneously and they're kind of it's
almost like you're solving I guess more
in depth the ideas you're taking a large
problem or question and then you're you
divide it into a bunch of little or
littler smaller parts and questions or
calculations and then each node or a
computer comprising this system is
tasked with one of these smaller bits to
the problem at a time each node is
served with a different problem and fed
new ones as they finish and they kind of
work in a synchronized fashion parallel
computing has actually come up in the
past in my videos here in the form of
machine learning it's based on the same
principles we're breaking down the
problem and solving that problem
simultaneously in its parts is usually
more efficient than attempting to solve
that same problem linearly so some
calculations can be done at the exact
same time where they don't really rely
on the previous solution before they can
like begin the calculation of the next
part of the problem
so you
got linear computing and parallel
computing so linearly it would just be
step-by-step literally and parallel
computing is where you've got multiple
computations happening at the same time
so solving a problem using parallel
computing is just also simply just more
realistic and when it comes to like
complex modeling or even things like
genetic programming where it literally
the problem you're trying to solve is an
interaction between multiple variables
at once and so when you have problems
like that it just not only is in reality
is it the case that you've got multiple
variables happening at the exact same
time uh so not only are we trying to
like more accurately depict certain
scenarios but also it just makes it's
actually easier to calculate these kinds
of problems with a supercomputer so some
of the larger fields that use
supercomputers are things like medicine
quantum mechanics weather research oil
and gas exploratory research that kind
of stuff with medicine one of the more
popular applications to supercomputers
is actually called folding so today
folding can actually mean a few things
but the original term folding in the
sense of what's called folding at sign
home comes from the simulation of
protein folding so you can actually
donate your computing time and power to
folding at home which is where you're
aiding basically in disease research and
other forms of molecular dynamics
research so it's kind of neat that you
can donate in this fashion rather than
your dollars and so on this was really
one of the first ways for people to
donate in CPU time and energy so now we
actually have Bitcoin which allows you
to do this for literally anything you
can imagine since you could generate the
Bitcoin and then simply didn't donate
the bitcoins as pretty neat so the
purpose of supercomputers again solve a
complex multivariable problem by
breaking it down into tiny parts and
assigning separate nodes to each part
along the way so what is one reason why
this is superior well besides the fact
that like literally it's easier and more
efficient to model a
problem like that by actually breaking
it down and solving for each of the
plate you know parts another reason why
this is actually useful is like when
giving a computer a problem to solve
sometimes the solution to a problem will
require waiting on a solution from
another or the problem is so let's
consider a six core processor for
example so like I have an i7 3930k and
it's got six cores well if one of those
six cores is waiting on one of the other
cores to finish its calculations so it
can take that variable and use it in the
next calculation you're going to have
what's called an idle core it's going to
be sitting there waiting so if you have
one out of six cores that's idle you're
sixteen percent idle let's say though
you have 64 six core processors if one
core goes idle that's only one out of
384 or 0.2% idle so these numbers might
not sound like very much because like if
a CPU gets in idle state for just a
moment it literally is just in like
milliseconds right but how do we
actually you know measure performance of
supercomputers the performance of
supercomputers is actually measured like
for example this i7 3930k processor it
pushes out somewhere in the 120 to 150 g
flops or gigaflops so what's that mean
so you might hear G flops or gigaflops
but nowadays as far as supercomputers
are concerned you'll hear most likely
petaflop but also there's teraflops
so petaa equals quadrillion tera equals
trillion and Giga equals billion so an
i7 3930k pushes 120 to 150 G flops so
what the heck are GFLOPS well G or Giga
it stands for what we just described
here gigas billion tera and so on
well flops stands for floating-point
operations per second now don't get too
confused because sometimes you'll see
all caps flops and then you'll see FL
Opie in caps and s underscore underscore
under case or lowercased
they are they do mean different things
so flops
all uppercase means floating-point
operations per second and then just FL
Opie uppercase lowercase s actually just
means floating-point operations so it's
not necessarily denoted by per second
because actually to calculate flops you
use flop like how many flop plural s did
you get so anyway moving along so 120g
flops means we're getting 120 gigaflops
or 120 billion floating-point operations
per second well that sounds nice but now
it's time to consider architecture see
CPUs traditionally have been built for
more of brute heavy lifting computing as
time goes by we're seeing CPUs really
getting tape being coming taken over by
their cousin who has been pretty dormant
for a while when it comes to
high-performance computing the GPU the
name graphics processing unit might not
be the best name for it anymore since
the GPU is now being used for far more
things than just graphics now the term
gpgpu is floating around now which
stands for general purpose graphics
processing unit so what gives well it
turns out processing graphics over the
years has prepared the GPU for
processing or other processing tasks of
this age right so any form of animation
on a screen is actually the result of
redrawing right in frames that's where
we get framerate so each movement you
see is it clearing and then a redrawing
on to your screen if each pixel is going
to be calculated sometimes it's not by
pixels sometimes it'll be like a model
itself and there's multiple models but
in the end it's broken down by pixel so
it's going to be calculated in each
pixels position let's say is often
contingent on many other variables so
today's high performance games like take
battlefield for example they're going to
use really intense physics engines that
calculate everything here
imagine how much physics and
calculations and variables come into
play when calculating let's say an
explosion right so that's kind of why if
something explodes in battlefield 3 if
you've got or 4 if you've got kind of a
poor graphics processing unit that's
you're gonna you're going to start
lagging because your frame rate goes
down so all these variables
depending on other things like physics
modeling you know gee what's that sound
like alright the supercomputer is
parallel computing that's exactly what
we're looking for here so the i7 3930k
processor six cores
runs five hundred and seventy dollars
new gives you a high peak of 150 G flops
Giga flops let's poke around here you'll
find it GPU let's see if we can find one
for 570 well the Radeon HD 7970 is going
to cost you about five hundred and fifty
dollars so that's close enough
that will give you about three ish for
teraflops not gigaflops teraflops so
we're talking quite the step up and
that's all basically an architecture how
is this thing set up and all that so
notably GPUs or GP GPUs are far more
popularly used in supercomputers as well
it's just like heavy processing so if
you get on like a AWS and you need like
a processing server you're probably
actually buying a GPU not a CPU which
honestly confused the heck out of me in
the early days when I was looking for
cloud computing and I kept finding all
these graphics and I'm like why do
people want to use anyway it was really
confusing but that's why I mean the GPU
is just a superior um large calculation
kind of device here so just for kicks
the number one supercomputer at least
that is known right the number one
publicly announced supercomputer is the
tianhe-2 which literally translates to
skyriver but really means Milky Way too
this performs at a peak of 33.86
petaflops
this is 33.86 quadrillion floating-point
operations per second so how do people
build these things well generally we
have two types of supercomputers in
existence you've got your clusters and
you've got your distributed systems
first let's talk about clusters this is
what you think of when you literally
think of a supercomputer clusters are
nodes that are really close to each
other usually like in the same
room and these are going to be more
efficient because there's no well
there's less bandwidth and latency
constraints slowing down data transfer
and sharing and they're generally
structured specifically for the job of
supercomputing then you have things like
distributed systems this is where you
have a network of computers spread
around hence distributed connected via
something like the Internet tcp/ip
generally distributed supercomputers are
far more relaxed when it comes to
efficiency so you'll find many of these
distributed computers right our
supercomputers they're going to contain
everything you've got computers phones
tablets desktop netbooks laptops
whatever so folding at home is an
example of this massive distributed
supercomputer so one of the largest
distributed supercomputers but no one
near the largest is folding a home and
it pushes 18 peda flops or 18
quadrillion floating-point operations
per second that's pretty impressive and
this one is dedicated purely to science
and solving disease so supercomputers
can also be used for more malicious
things one of the largest targets for
governments and supercomputers is doing
things like cracking encryption so it's
somewhat likely that the NSA already has
a supercomputer that's more powerful
than 33.86 petaflops and we do know that
China is also working on a 100 petaflop
computer and it's also likely that China
themselves has another one that is
secret you know so super computers today
are a lot like nuclear weapons of the
past where a supercomputer capable of
shattering say something like 256 AES
encryption to a large degree would
render pretty much every business
insecure country's infrastructure
insecure you could do a lot of damage if
you have that much power luckily it's
unlikely that anyone does but
interestingly enough there is one
supercomputer that is by far the largest
supercomputer in the
world it's actually larger than every
supercomputer in the world put together
times 500 and still larger and it's
growing stronger and stronger every day
see I was doing some research and I'll
probably dedicate a video to this on
this on this topic specifically but I
will just mention it here last year
early 2013 the Bitcoin network surpassed
one exaflop pretty much unheard of as
China again is trying to finish their
100 petaflop and the NSA has probably
got something around 200 to 300 the
Bitcoin network passed one exaflop
to 3/4 and today the Bitcoin network is
measured at two hundred and eighty seven
thousand one hundred and fifty eight
pedo flops or two hundred eighty seven
eggs of flops and growing and just for
measure it is February 2014 the Bitcoin
network is the largest supercomputing
network in history outpacing all other
known supercomputers on the planet added
together by far the network is very
unique as there is no central server so
normally the data and all this stuff
does come back to a central place
whereas the Bitcoin networks actual
purpose is to achieve network consensus
so it's almost like the network's
purpose is to be what a central server
would be yet not be a central server so
the idea here is that there is no
trusted third party or anyone in control
of it um so anyway if you're more
interested in Bitcoin I won't bother
anybody too much with Bitcoin and how it
works in this video but it's actually
very impressive invention but anyway I
will probably put out a video more about
the size of the Bitcoin network and what
people really should ought to be doing
with the Bitcoin network but they aren't
so stay tuned for that and the let's see
the last major part to the workings of a
supercomputer is you know how do we get
them to communicate with each other so
we you can tie all your supercomputers
together and put them on the same local
area network but how do you actually get
them to start sharing processing jobs
well this is where the MPI comes in
which stands for message passing
interface so what MPI does is it passes
messages between the nodes to keep them
working in a synchronized fashion on the
same problem and it allows us to share
resources to solve a problem so that
satisfies the basics of what a
supercomputer is what its purposes are
and all that in the next video what
we're going to be talking about is how
you can build your very own
supercomputer what you're going to need
and all that so hopefully that sounds
interesting to you guys as always thanks
for watching
thanks for all the support in the
subscriptions and until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>