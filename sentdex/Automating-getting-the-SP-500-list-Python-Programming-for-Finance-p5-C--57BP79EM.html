<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automating getting the S&amp;P 500 list - Python Programming for Finance p.5 | Coder Coacher - Coaching Coders</title><meta content="Automating getting the S&amp;P 500 list - Python Programming for Finance p.5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Automating getting the S&amp;P 500 list - Python Programming for Finance p.5</b></h2><h5 class="post__date">2017-01-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C--57BP79EM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to
part 5 of the Python for Finance
tutorial series in this video what we're
going to be doing is grabbing the S&amp;amp;P
500 data basically all of the companies
that comprise the S&amp;amp;P 500 and at this
most basic level this isn't exactly how
the S&amp;amp;P 500 is calculated but just to
keep things simple it's generally the
top 500 companies by market cap and
market cap is just the value of that
company basically the value of that
company again super basic the the number
of outstanding shares times the price
and that gives you the value of the
company so the S&amp;amp;P 500 the top 500 most
valuable companies so so first of all
whereby we get a list so like for
example like in in practice you're going
to have to find your own list so yes if
you want to use the SP 500 you can just
follow this tutorial but chances are you
might have your own list or something
else entirely who knows so the first
thing I'm going to do is just come over
to Google I'm just going to say S&amp;amp;P 500
yeah companies company well I don't
really need to type Google knows when
I'm asking so let's go over here for the
Wikipedia list of the S&amp;amp;P 500 companies
fantastic so there we have a list I
would wager this probably gets updated
relatively quickly as that list does get
updated cool so we can now view source
and basically the list view source and
let's look for 3m company so control you
to be the source a now let's find 3m
company okay so it's in table data and
basically what we're looking for is the
start of this table can we possibly find
table great so what we're looking for is
this so what we're going to do is we're
going to use a web scraping library
called beautifulsoup
you'll see some of the basics here if
you want to know more I have a tutorial
series on beautifulsoup
so if you want to know more about
beautifulsoup go you can go to python
permanent search beautifulsoup
and you'll find
so we're going to look for this table
and then every table has some rows and
table data and the first thing will be
table headers which were not interested
in but we're in it we are interested in
all the table data we can go here and
sure enough between basically the first
table data tags we can see that's where
the ticker is so we can just pull apart
this table so the first thing I'm going
to do is I'm just going to go ahead and
we're going to be starting clean here so
I'm going to import the s4 as BS just
because basically any version of any
beautifulsoup basically the whole reason
we import it just as BS is for ps5 data
for whatever someone could take sample
code and that's the only change they
need to make they don't need to keep
changing fours to fives or threes two
fours and so on import BS for that's
beautiful soup import pickle and then
import request we're going to be using
pickle again I've got a tutorial on
pickle but basically what it does is it
serializes any Python objects so you can
basically just save any object like a
variable and so on and what we're going
to do is we're going to save the sp500
list that way every time we want an S&amp;amp;P
500 list we're not going to need to go
back to Wikipedia and hit Wikipedia so
now define save SP 500 tickers tick
terms what are we going to do here so
basically what we're going to do we're
going to start off by just getting the
source code for Wikipedia it's going to
look just like this so we're going to
use request for this so we're going to
say respond people's requests those
requests get and then we're going to
just take this link here so not new
source we're going to take this link
copy that paste there we go
now we're going to say soup so soup is
going to be a beautiful soup object so
BSBA eautiful soup it's going to be soup
that comes from the response and funds
we raising request we're going to say
response that text so response that text
is the text of the source code ok so
then we're going to specify a parser we
actually PI don't need to but I'm going
to go ahead and do it
the Alex ML parser you probably I can't
remember if this comes you might have to
pick install that looks Aman
let me run it without L XML and let's
just make sure let's just see if it's
absolutely necessary table so this will
be table data so now to use beautiful
suits to find certain things we can
state the suit defined what do we want
to find you want to find table but not
just every table because there in this
page I don't think there's any more
tables that's not a table there might be
some like under here though or something
no this could be a table who knows but
on a lot of Wikipedia pages there's
actually a lot of tables and they might
have a very specific class chances are
they'd all have the wiki table sortable
so we're going to see if we get lucky
though so we're going to specify a
specific class just on this page
actually it's probably not necessary but
let's just just to be safe we're going
to say exclusion we want all tables of
class it was wiki table and it was
sortable so that's the table we're
interested in then we're going to
specify an empty takers list and then
we're going to iterate through this
table so for row game table dot finds
all TR for each TR is table row so for
each table row and then we're going to
say one onward because that first row
remember that's at amp those are table
headers so it's like the titles of the
rows or the titles of the columns rather
so here table header it just tells us
you see filings ticker symbol and so on
we don't actually need that so : so the
ticker equals row dot find all and in
each row it consists of table data
that's each column basically and we just
want that first zero list column so
right that was the most less column here
those are all of our tickers so we want
the zero with right that's the first so
that's the ticker we need to specify dot
txt because it's a soup object right now
we want it to just be like string for
Python so dot txt and then tickers got a
pen
kicker once we have that we've got a
full tickers list SB 500 let's go ahead
and save it with open and we'll call
this SP 500 tickers pickles we're going
to open with the intention to write
bytes as f pickle dot dump we're gonna
dump tickers F so we're dumping the
tickers to file F so just save that and
then finally we'll return tickers but
hopefully in most cases we'll just load
this pickle so now let's go ahead and
just run at save SP 500 tickers and
let's just just for now we'll print
tickers just so we know if it works or
not so save run it let me pull it over
here whoops
I said response dot txt somewhere here
we go txt try again there we go okay so
now we have the entire system P 5 on our
list big list I also think I saw a
warning let's see it was about the
parser it looks like for some reason I'm
not able to go all the way over to the
edge there we go
warning yeah so wants me to just go
ahead and pass L XML so I'll go ahead
and do that but like I said you might
have to actually install LS ml so yeah
but I guess I'll pass it in there cool
okay so anyways that's how we're going
to get the tickers I'm gonna go ahead
and stop it there and in the next
tutorial we'll kind of continue working
on this because we've got a lot of
things that we're trying to do here we
get the tickers then we're going to get
all the data for all the tickers then we
got a compile that data into one data
frame and start doing analysis and so on
so now let's stuff to look forward to do
you have questions comments concerns
whatever up to this point feel free to
leave them below otherwise I'll see you
in the next 4th</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>