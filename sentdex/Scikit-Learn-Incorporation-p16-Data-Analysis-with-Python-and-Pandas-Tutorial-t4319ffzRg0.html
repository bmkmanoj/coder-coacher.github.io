<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scikit Learn Incorporation - p.16 Data Analysis with Python and Pandas Tutorial | Coder Coacher - Coaching Coders</title><meta content="Scikit Learn Incorporation - p.16 Data Analysis with Python and Pandas Tutorial - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scikit Learn Incorporation - p.16 Data Analysis with Python and Pandas Tutorial</b></h2><h5 class="post__date">2015-11-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/t4319ffzRg0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody welcome to
part 16 of our data analysis with Python
and panda's tutorial series in this part
we're going to be talking about is
incorporating scikit-learn for machine
learning on our data set here to see if
we can figure something out
at least at a basic level as far as
machine learning is concerned to us
check the current valuations for you
know like the current housing price
index the GDP ESP 500 unemployment stuff
like that and if we can accurately
predict whether or not the housing price
index for wise or fall so in the
previous tutorial we already created the
label and all of that so in this
tutorial all we have to do now is
actually feed this through first we're
going to need scikit-learn so if you
don't already have it what you want to
do is do the usual pip install SK learn
or if you need to give the full path C
colon slash Python you know 34 scripts
pip install SK learn I already have it
so I'm good to go so close this and
we're ready to rhumble so we'll come to
the very top here and we're going to go
ahead and do from SK learn import I need
SVM pre-processing and cross-validation
SVM is short for support vector machine
which is what we'll be using then you've
got pre-processing that converts your
data to a range of negative one to
positive one or at least that's the goal
generally that can help improve machine
learning algorithm classification you
know accuracy but doesn't necessarily
cross-validation we'll use that to
create test and training sets so first
like I was saying in the previous
tutorial machine learning doesn't I mean
it's a complex subject but to utilize
machine learning is not complex I often
equate it to understanding you know how
an engine works or how a car works as
opposed to just knowing how to get the
utility out of the car to drive the car
okay so I guess it's Tesla and uber have
their way that analogy won't really be
that useful in like five years but for
now it is so
let's do the right estimator
scikit-learn cool so with SK Lauren
generally all you have to do is create
the features create the label feed it
through to train it and then you can
test it it's super easy the only thing
is you know figuring out what estimator
to use or which algorithm to use but
there's a nice flow chart for that start
right here do you have greater than 50
samples if not you need to get more data
but we do so are you predicting a
category yeah we want to predict either
it goes up or a doesn't do you have
labeled data yeah we sure do do you have
100,000 samples or do have less than
100,000 samples yes to linear SPC if
that doesn't work is it takes text data
no so then you Duchaine years or both k
neighbors and then if that's not working
just SVC or ensembl like random forest
or something like that so anyways we're
gonna close this and just do simple SVM
so now what we do is we need to create
basically the feature sets generally you
create those like so so we don't need
this rolling apply example I just wanted
to show it how it was done in the
previous tutorial so generally you have
your features or capital X and your
labels will be a lowercase Y doesn't
have to be but that's usually how people
do it
so X is what well it is the basically
it's the housing data so it's NPR a on
housing data but if we feed through the
label and we're training against the
label the machine learning classifiers
going to recognize that oh my gosh will
give you use use label right it's gonna
be really accurate so we have to do
housing data drop and then we have to
drop that column and what other column
should be probably dropped well we
should probably drop the US HPI future
column because again we can't see into
the future and our machine learning
classifiers going to realize that that's
like 100% correlated so it's going to
take that and
going to be really accurate all of a
sudden so we need to drop those two
columns so we will so we're going to
drop a label and we drop a u.s. HPI
future we drop both those so we're going
to convert that to an array drop those
we're going to take just one there
because we're for the drop basically so
what this is going to return is the data
frame without house without these
columns now so that's X so we can go
ahead we could like print X if you want
to see X we'll just say Y is equal to 0
for now so it works so here's X just a
bunch massive numpy array now Y is going
to be equal to oh and the other thing we
want to do is actually we're going to
say X is equal to I'm going to take all
this and then we're going to wrap
pre-processing around it we could either
wrap pre-processing around it or just do
it second so let's just let's just do it
to the line because this one is long x
equals pre-processing dot scale X again
that converts the data to a hopeful
range of negative one to positive one it
like I said before can improve accuracy
it won't necessarily it rarely hurts you
though to do that so then now what we're
going to do is we're going to say y is
equal to an umpire array oh housing
underscore data label so now Y is our
features or sorry why is our
classification or our label X is our
features so now we can train and test
now scikit-learn has a really easy way
to do that to split it up so we're going
to say X underscore train X underscore
test y underscore train Y underscore
test equals cross underscore validation
train underscore test under store split
and we passed X Y and then we can say
how big the test sizes test size equals
do
Oh point two so 20% okay so what is
this.what cross-validation does for you
where we've imported that from SK learn
what this does is well this whole module
does a lot of things but first of all we
can do a train and test split we can
throw in the data to this function here
so this is our features and our labels
we throw that in and we say how big we
want the test size to be so 0.2 means
20% so this means we'll train on 80% of
the data and then we'll test against 20%
so that gives us those now we specify a
classifier that we want to use for
machine learning we figured out it was
SVM so we're going to say CL f equals
SVM dot and we'll do SVC and we'll say
that kernel is gonna be linear so with
linear SVC here and then to basically
what we'll do to train this is how you
train a classifier CL f dot fit and then
we fit against X the train data for X
and then Y train so now it's big that's
training now we can test it so we did
print CLS score and then this is X test
Y test and that's it
so we can save and run that has a lot of
code but we can do it and at least at
the outset we're 73.5%
accurate but when you ran this you might
find your data is your value is 40
percent who knows like let's run this
again oh we got even more accurate 69.8%
accurate 75 so we're actually coming out
pretty accurate here that's a little
more accurate than I thought we would
actually end up Ian to be honest just
run a couple more times Emerson okay
well whatever I'm so apparently we're
doing pretty good let's run through the
code real quick again because if you get
data that that's that good you probably
want to recheck things we don't have
okay so X test y test we dropped housing
data so these are all basically this is
what we really want to see
um so let's take this copy let's go
ahead and print that information and
what we're doing is we're looking for a
column that would be a bad one half so
United States well that's our current
HPI so I don't need to drop that we
don't need to drop any of the states and
again we were 73.5% accurate okay we are
truncating some but that's all states
and we have United States the mortgage
thirty-year unemployment rate that's
valid GPS b5 okay well shoot
I guess we're billionaires maybe I
shouldn't release this tutorial because
we're pretty darn good at predicting
changes in the S&amp;amp;P 500 or a thousand
people enter the housing market so maybe
I'll just keep this tutorial for myself
anyway so okay fine so so we're pretty
accurate apparently at predicting of
string B prints out at predicting
changes in the housing price index I
mean we're pushing 70s the 80s on
average here every time so anyway that's
pretty cool so I guess becoming a
billionaire is plug and play I'm still
kind of skeptical but I mean we just
looked at all the columns that we were
passing through there I don't think
we're doing any cheating or bias here
but I just can't believe it would be
that easy I thought we would truly need
a little bit more data to start getting
into the 70s even but ok I accept it I
guess don't forget about me when you
invest and become a billionaire I
suppose ok so anyway that's machine
learning if you guys have questions
comments concerns whatever leaving below
otherwise as always thanks for watching
things for all the support subscriptions
till next</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>