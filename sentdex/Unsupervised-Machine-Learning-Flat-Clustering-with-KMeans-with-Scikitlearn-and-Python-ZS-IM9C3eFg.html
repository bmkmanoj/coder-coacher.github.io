<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Unsupervised Machine Learning - Flat Clustering with KMeans with Scikit-learn and Python | Coder Coacher - Coaching Coders</title><meta content="Unsupervised Machine Learning - Flat Clustering with KMeans with Scikit-learn and Python - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Unsupervised Machine Learning - Flat Clustering with KMeans with Scikit-learn and Python</b></h2><h5 class="post__date">2015-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZS-IM9C3eFg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody and welcome to another
machine learning with scikit-learn in
Python tutorial video in this video in
the subsequent videos we're going to be
kind of shifting gears a little bit and
talking about unsupervised machine
learning so on the videos leading up to
this video we were covering supervised
machine learning specifically using a
support vector machine even more
specifically using linear SVC so I was
just a really basic example of
supervised machine learning by no means
do we cover everything involve with me
supervised machine learning or anything
like that but we just wanted to cover
the basics of a project or a type of
problem that we might use supervised
machine learning for so now we're going
to be talking about unsupervised machine
learning and it's really not too
confusing the difference between
supervised and unsupervised machine
learning is whether or not we the
scientists are providing the computer
with labeled data most of the time
you're going to be using unlabeled data
entirely where you don't even know the
correct answer the only time you're
going to really use labelled data with
an unsupervised machine learning
algorithm is going to be when maybe
you're just trying to test that
algorithm but for the most part you
aren't even going to know the answer so
what kind of problems are are we
applying unsupervised machine learning
to well we can actually apply problems
or we can apply unsupervised machine
learning to supervised machine learning
problems so we could actually use
unsupervised machine learning on say
these stocks a question and see if we
could separate them into two groups now
those two groups that we separated them
into we may or may not be they may not
be under performing or outperforming
groups they would just be two groups so
that leads me to the next point with
unsupervised machine learning
unsupervised machine learning is
separated into two kind of dichotomies
as well the first one is where well both
of them the targets is to cluster data
into groups so sometimes you'll hear it
called cluster analysis or whatever
but with unsupervised machine learning
you've got two two options here you
either throw a bunch of data at the
Machine and you expect the machine to
just kind of figure it out for you or
you throw a bunch of data the machine in
you ask it to separate the data into X
amount of categories some example might
be throw a bunch of pictures of males
and females at the computer and you say
separate this please into two categories
with the goal of hopefully it separates
the pictures by male and female and then
what you can do is you can derive the
underlying major differences between a
male and a female face in a picture so
that would be an example of what is
known as flat unsupervised learning or
flat categorization or flat clustering
the other one is going to be
hierarchical clustering and that is
where you feed a bunch of data to the
machine learning algorithm and you let
it determine how many clusters there
ought to be so either this is doable
into both unsupervised machine learning
obviously taking the you know not
telling the Machine how many categories
are that's even more unsupervised like
then telling it how many categories
there are but obviously we both of them
are unsupervised examples so what I'd
like to do is show an example of each of
those here so first of all we'll do just
a flat clustering where we specify some
data and we tell the Machine hey this is
how many categories I'd like you to
separate this data into so before we do
that just some quick examples the one
that I just covered basically with
separating maybe meal and female faces
that's an example of one word we would
do that's a flat cluster some
hierarchical clustering might be used
for say genomics something like that
we're basically on supervised machine
learning the general point of using that
is to understand or derive some sort of
underlying structure behind something
that we don't quite understand yet so
it's it's using like genomics and stuff
like that because we don't really know
the structure or how they work with each
other the genes I mean and so we use
unsupervised machine learning to help
hopefully gather some insights
there is one more reason why
unsupervised machine learning is used
and that is actually to bring down or
simplify a data set into a way that
either we can visualize it or at least
more better understand it and so like if
we take for example this dog in the
background this dog has a lot of
features he's you know got fur and legs
and a face and a nose and eyes he's got
and then we could go even further he's
got you know strands of fur or hair or
whatever and then the further he's got
cells and those cells have you know
nucleuses and all this kind of some
nuclei and so we have to understand that
there are a lot of features to a dog and
initially feeding in every single
feature of something we might weigh
every feature as I do we weigh the
features in terms of importance and so
actually what we can do is we can use
unsupervised machine learning we can
feed through the features of a dog let's
say and then the features of a cat and
we can have all those features and then
some of those features are very similar
to each other yet some are very you know
vastly different and so we can use
unsupervised machine learning to
actually take all of the features of
dogs all the features of cats and kind
of simplify all of our features into
maybe two or three features maximum by
kind of making conglomerates or picking
out the ones that actually matter and
there's a specific term for that but
we'll talk about that later but um but
yet so that that's another third kind of
example of what we might actually use
unsupervised machine learning for so
anyways with that let's go ahead and hop
into the first example that we're going
to do and that's again going to be just
flat clustering so first we're going to
go ahead and import um well I was going
to import time but I don't really think
it's necessary in this one so anyways
import numpy however as NP and if you're
following along in in this actual entire
machine learning series if you're not
that's okay but if you are we're gonna
use exact same data as we used for the
initial actually both the linear SVC
example and the SVM example that we use
so it should look fairly familiar we can
use that data for this one for the
hierarchical clustering will actually
end up having to make a new data set
because it's
not possible to do it with the data that
we have anyway so we're gonna import
numpy as NP and this is purely so we can
convert our data set to something that
is acceptable by scikit-learn next we're
going to from matplotlib import or
actually I always do it this way
imports matplotlib dot pi plots as PLT
then we're going to go from matplotlib
import style and style dot use GG plots
if you're on pythons a27 and or with an
ungraded version of matplotlib you can
feel free to ignore these two lines it
is just to make the graph look a little
prettier nothing more so feel free to
ignore them if you don't have a current
version of matplotlib next we're going
to import the actual machine learning
algorithm from scikit-learn so we're
going to go from SK learn dot cluster
import k-means so for now our cluster or
our Eclipse not cluster cluster for now
our example is going to have few enough
data points that we can use k-means if
you have more than say 10,000 data
points you'll want to use k-means
mini-batch which what that's going to do
is going to separate your data into a
bunch of little batches of the data
because anything over 10,000 it's going
to get things are going to get hairy so
anyway we import k-means and now we're
ready to make a dataset so again I
assume we're I expect that many people
might actually view this video but
without viewing the other one so first
let's draw out our data and kind of see
what the data is that we're working with
and then we'll go from there so we're
going to create a scatter plot we've got
x coordinates Y coordinates and we're
needs the same data as we had used
before that data was for X we had a 1 a
5 a 1 point 5 8 1 &amp;amp; 9
that's our x coordinates now for Y
coordinates we'll have a 2 &amp;amp; 8 a 1 point
8 another 8 zero point 6 and an 11
number just going to call PLT scatter
we're going to scatter x and y and then
we're on PLT
and that will bring up the plot for us
obviously you're going to need
matplotlib if you don't have it
numpy and of course scikit-learn if you
want to know how to get all that stuff
shut just check out probably the first
video in this entire series or just go
to those the respective websites for
those modules and install them so this
is the data that we're working with here
and as you can see probably with your
eyeballs if you were going to you know
be given the task to cluster these you
would probably say okay okay we would
cluster these top three here and these
lower three right here simple enough but
what we want to have is the Machine do
it for us so we'll come down here and
we'll leave this um leave that up for
now just so we can compare I suppose but
soon we can just comment these two lines
out they're not really that necessary so
now what we want to do is actually feed
this through and see if we get the
results that we're expecting so first we
need to convert this data to an umpire
ray and an umpire ray is basically
pythons version an array is really just
like a list of lists only there's really
no such thing as an array in Python
we're gonna have to use numpy to make an
array and numpy is just a Python wrapper
around C so that's why we can get away
with that anyway moving right along
we're going to specify this as a capital
x equals MP dot array and this will be
an array of like I said a list of lists
and we have a I got an itchy net we have
one two three four five six data points
which we could construe as features so
we have one two three four five and six
and just for the sake of organization
we'll I'll just do this you can feel
free to do what you want
so the first pair of coordinates was a 1
and a 2 right so 1 &amp;amp; 2 and now we're
going to do five eight one point and
we'll add some spaces here especially
these decimals can get confusing then
one point five one point eight and then
we had an eight and an eight a one in a
zero point six
and then a 9 and 11 so that is our numpy
array of data or again as if they were
features then what we're going to do is
we're going to specify a lowercase K
means and that's going to be equal to
capital K capital M means and basically
we're referencing this up here note the
capital m capital K there so K means and
then we're just going to specify the N
underscore clusters so the number of
clusters so again this is a flat
clustering where we tell the machine
learning algorithm how many clusters we
want and we know that this should result
into two clusters so we're going to
notify the Machine hey this is two
clusters so again you know say we're
feeding it pictures of males and females
or pictures of dogs and cats or features
of dogs and cats we want it clustered
into two clusters so that's what we'll
do then we're going to call k-means dot
fit and we are going to fit the
parameter of X okay
easy enough now once we've done that
we're ready to move on and we kind of
want to visualize this so at this point
we really don't need to do much else we
could print out our information and we'd
have it but it's kind of helpful
I think to visualize the data at least
at first
so like I've said a lot in this series
even leading up to this point it's a lot
of machine learning you actually will
never visualize it yourself you may
visualize certain aspects but the
entirety of the the operation won't be
visualized but like I also said
sometimes we actually use unsupervised
machine learning specifically for the
task of visualizing highly a like a lot
of dimensions or a lot of features and
we kind of break it down simply just so
we can visualize it and get the feeling
at least that we're on the right track
but for the most part you probably
actually won't be graphing your results
but we will be here just simply for the
educational effects of it
so k-means dot fit and then now we're
going to specify centroids and so
centroids centroids are going to be the
marker of the I guess the the most ideal
date of data point based on the points
that we have
okay so when we do clustering what we
actually are doing is or at least with
k-means is we're going to be taking
these points and the way that we cluster
them is we cluster them based on equal
variance okay or degrees of variance
okay so from say the others right on the
on the plane and so the centroid is
going to be basically the center ish
kind of plot of all of those plots let's
say in each cluster so it would be like
the ideal plot almost so anyways
centroids that will be the center marker
of the of the cluster so centroids is
going to equal k-means dot cluster
underscore centers underscore don't
forget that last underscore there then
we're going to say labels and labels are
going to be literally the label that the
algorithm is going to assign so if you
remember in our first run if you went
through with the SVM the support vector
machine here with supervised learning we
labeled the data right we said this was
a 0 this was a 1 this was a 0 this was a
1 0 1 okay and that's what we used to
train the data and then we could feed
through a new data point and ask it
based on these features please make a
prediction and it would and with on the
supervised machine learning that's not
the case we're actually feeding it just
this data and we're telling the
unsupervised machine learning algorithm
label this data so this is where the
term semi unsupervised learning comes
from because what you can do is on the
exact same data that we actually label
later on we can actually use
unsupervised machine learning to to
label our data and then we can move on
to use supervised machine learning from
that point onward so keep that in mind
that's kind of semi supervised machine
learning and maybe we'll have kind of a
topic on that a lot of your larger your
larger projects are going to be
semi-supervised because machine learning
is so powerful that you might find it's
actually quite useful to solve a lot of
questions so you might find yourself
actually mixing and matching quite a few
algorithms tacked to answer whatever
your may
project is so anyways moving along
labels these are going to be the labels
that the k-means k-means algorithm
actually supplies us and ideally our
target so to speak is that it labels
zero one zero one zero one or it could
label it as one zero one zero one zero
anyways moving right along so once we
have those we can print will just print
them out just so we can see them with
our eyeballs centroids and then print
labels I don't know what I was thinking
there anyway so print labels so now what
we're going to do is we want to graph it
so again at this point the learning the
not training I on the centering the
learning is done okay
the fitment is done but we want to graph
it so now what we're going to do is
we're going to say colors and this is
going to be a list of plot options
basically and since we only have two
categories we really only need two
colors and so we're going to say G
period and our period G is the color
green period is kind of like a marker
and so it would be just like a little
period looking marker and then our
period same same story there for red now
we're going to do is for X in range of
the Len of X so length of X is going to
be 6 right we've got 1 2 3 4 5 6
coordinate pairs so for each coordinate
pair well actually this is just for X
and the range of 6 what do we want to do
well we're going to first let's just
print it out let's just say coordinate
like that we'll do coordinate : and then
capital X X so the X of X this might be
slightly confusing now that I think of
it
let's say for mmm try to think of a
better variable that we could use really
probably get by with like an underscore
since it doesn't really matter we could
do like for underscoring range you might
say this underscore a lot underscore
basically means it's a worthless
variable I actually I kind of like what
we use I a lot of people use I that'll
be good one so for the eyuth of x
that's the coordinate and then we're
going to say the label : and this is the
label that the machine learning
algorithm k-means has assigned to these
coordinates based on its findings and
that will be labels I okay and then
we're going to go ahead and use PLT
plots and then we'll do X I and then the
0 with whoops not the I the 0 with and
then the X I first if and so what are
these X is corresponding this this numpy
array so the X and then the element is
either 0 through 5 so this would be the
0 with the first second third and so on
and then the 0 with that would be just
this right if we were talking about the
0 with X and then the 0th element of
that would be a 1 so the X bar and the
plot and then the Y bar so this if I was
0 would be 2 so we're going to plot that
and then we're going to say the color of
this plot is going to be and we're using
colors for color and marker basically so
the colors is going to equal or I'm
sorry actually we're going to do colors
like this my bad so colors responding to
basically this this colors array so to
speak or list rather so colors and then
what we're going to call is colors and
then we want the labels I of colors so
if you remember we have two categories
there for the labels could be either a 0
or a 1
so we're referencing either a 0 or a 1
element in the color list so the 0th
element would be a green and the first
element would be a red so that's it then
finally what we're going to say is
marker size equals 10 and now we'll just
kind of make the marker size a little
larger than a tiny you know pin plot
basically so once we've done that the
next thing that we want to do is we're
going to go ahead and scatter the
centroids so we're going to call PLT dot
scatter
and we want to scatter the centroids and
I wanna scatter this the X&amp;amp;Y version of
centroids right that's this right here
and if you recall we have some fancy
code for using num pot or clotting numpy
arrays and that would be colon comma and
then the element and then again colon
comma any element so what this does is
within the centroids array this is all
of the zeroeth elements within that
array so this centroids this let's say
that we did actually X this this right
here of X would be 1 5 1.5 8 1 9 okay so
we're going to scatter those and those
are actually our centroids so what we
want to go ahead and do is mark them and
we're going to mark them with an X so
they're marker basically the marker type
is going to be an X the Esper size will
call that call it 150 and then we'll say
line widths equals we'll do a 5 and Z
order equals 10 so that should do and
then finally we will call a PLT dot show
ok so barring any errors which were
highly likely to have that will be it so
let's run that bad boy so this was the
original plot let's exit that now and
okay cool we didn't have any errors so
here we have all of our stuff and so
here we can see as well we have our
coordinates in our labels anyway so now
you can see that this is the cluster of
group of a group anyways it's changing
here so in this instance right these
these are our zeros and these such
probably this over and these are our
ones so these are zeros and ones and
these are the centroids ie the center
kind of marker between these points here
now
what would happen if we did the
following what if we were rude and we
said a number of clusters equals 3 now
this means that we're going to have 3
labels
so we need to add another color so let's
add a cyan and that should do it as far
as anything else we need to change so
now we're going to tell it we're going
to force it to choose three categories
run it there's the original and now it
has chosen three categories and you can
see here that this is still a category
of its own
these two are a category and then this
is a final category that's being covered
up by the centroid but it is there so
what has happened here well this is kind
of how what I was talking about before
where how this kind of algorithm works
so these three are very close to each
other there's not a high degree of
variance therefore they're definitely a
group these two are closer than these
two and certainly you closer than these
two therefore these two are paired
together and this one is all on its own
because it's basically obviously further
away from these two and definitely
further away from these three so that's
how you can kind of understand how this
algorithm is actually working on on the
background so if we said four categories
where do you think the centroids would
be please write down your answers pop
quiz class uh-huh it's whole add let's
add another argument here we'll do
yellow period and if we add a fourth
category well it's obviously going to be
these top ones right here so it close
out and sure enough yes because again
this one is further from this one this
one's further from this one and this one
is further from both of these and of
course all of these are further from
these therefore these are chosen as
their own group so equal variance so
anyways that is it for the extreme basic
introduction to flat clustering and in
terms of using k-means for unsupervised
machine learning in the next video what
we're going to cover is unsupervised
machine learning using hierarchical
classification or clustering to cluster
into groups that the
feels are the most applicable groups so
anyways
stay tuned for that if you guys have any
questions or comments on this video
please feel free to leave them below
otherwise as always thanks for watching
thanks for all the support the
subscriptions and until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>