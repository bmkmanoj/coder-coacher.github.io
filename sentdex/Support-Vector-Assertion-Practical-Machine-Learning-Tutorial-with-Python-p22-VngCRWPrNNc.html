<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Support Vector Assertion - Practical Machine Learning Tutorial with Python p.22 | Coder Coacher - Coaching Coders</title><meta content="Support Vector Assertion - Practical Machine Learning Tutorial with Python p.22 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Support Vector Assertion - Practical Machine Learning Tutorial with Python p.22</b></h2><h5 class="post__date">2016-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VngCRWPrNNc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody and welcome
to the 22nd machine learning with Python
tutorial video specifically we're
talking about support vector machines
and classification in the previous
tutorial we talked about vectors
magnitude direction and dot product and
before that we act we showed the high
level intuition of a support vector
machine now we're really going to start
digging in so first of all what is like
what's the we know that eventually the
support vector machine creates about
like a separating boundary this decision
boundary or the separating hyperplane
but like what is the what is the
operation a machine or a support vector
machine is running to give us a
classification after it's been trained
on a new point so to exemplify this
first we're going to let's draw some
vector space I'm not going to worry
about drawing any ticks or anything but
let's add some points now so we've got
our positives and then we'll have some
negatives and the support vector machine
we hope at some point is going to create
the decision boundary we'll just eyeball
it for now we're going to say that's the
decision boundary so the way that a
support vector machine once it reaches
this point is going to classify new
points is by first taking the vector
that points perpendicularly to the
separating hyperplanes so that would be
like this and this is vector W and then
let's say you've got an unknown data
point and we'll say unknown is here and
it's also a vector from the origin and
what you're going to do is you're going
to project vector u onto W or vice versa
and you're going to find out is vector u
basically on the left hand side or the
right hand side or on the top or the
bottom or whatever way you want to look
at it just depends on what your
perspective is to this hyperplane but
what side of the hyperplane is it on in
this case we can visually tell that it
if we were to project it on to vector W
it would definitely not be it would not
be on the other side of vector W so what
would be like the the operation the
calculation once we've trained a machine
learning class
fire it's going to say basically vector
u for the unknown vector dotted with
vector W unknown plus B for the bias
which will we'll talk about that soon
enough that is the equation that we're
going to run and then what we're going
to say is if that is greater than or
equal to zero if that is the case then
this would be a plus sample but as we
look at this we are relatively confident
that is not going to be impulse we can
just tell visually that it's not going
to be the case so actually it's going to
be a vector u dot W plus B is actually
less than or equal to zero
and we'll draw our lines and be good as
long as we can and that would be a
negative sample so that's the operation
that the machine learning classifier is
going to run what if what effect are u
dotted with vector W plus B equals zero
right what if it's not greater than or
less than then it would that would be
that would mean that that was on the
decision boundary so coming back to our
original drawing here piece of art we
know that the equation that we
eventually need to solve for eventually
is vector u dotted with vector W plus B
we need to find the value for that so
what is the what are the variables that
we actually don't know at the moment
well unknown we have that because
unknown is going to be comprised of well
what is what is unknown prized of right
what is this unknown vector comprised of
well it's comprised of X 1 and X 2 those
are the features right unknown is a
feature set so we actually know what
vector U is we have the value for vector
u what we don't have is the value for
vector W or the
you for be and we need both of those in
order to solve this equation so how do
we get those two values and it turns out
that that not only how do we get them
but they ask also come with a few
constraints so that's what we're going
to be talking about next to the the
constraints that that are brought
brought upon us by those two values and
what we thought was initially looking
like a really simple equation turns out
to evolve pretty quickly so we're still
on our quest to find both W and B in
order for us to actually have our
decision function and what we know so
far is something like X minus support
vector so X sub minus support vector
would be dotted with W plus B we know
that that would be equal it's a minus so
it would be equal to a negative one we
also know that vector X sub plus class s
V dotted with vector W plus the bias we
know that is going to be a positive one
so how can we make an equation possibly
to go through our data and actually just
locate support vectors so now we
introduce Y sub I and Y sub I is simply
the class of the features that we're
passing through so if the class is a
plus class then Y sub I is going to
equal a positive 1 or just simply 1 if Y
sub I is of the minus class then Y sub I
equals a negative 1 so then what we're
going to do is we're going to take this
Y sub I and we're going to multiply it
by the equations that we were using to
identify the the positive and negative
support vectors so if you recall for the
plus class what was the equation for
that it was X sub I dotted with W both
of these were vectors plus B that was
meant to be equal to one and then for
the minus class we knew that it was X
sub I dotted with W plus B equals
negative one so now we're just simply
stating the fact that Y sub I equals one
in this case because it's a plus class
and then down here y sub I equals
negative one because it's a minus class
so then what we're going to go ahead and
do is we're going to multiply these
equations by y sub I so when you
multiply an equation by whatever what do
you have to do well you have to multiply
both sides so we would be multiplying
this side and this side and same down
here by Y sub I Y sub I and what we're
going to do is we know y sub I is 1 or
negative 1 so we're just going to go
ahead and apply that to the right-hand
side of the equation first so let's say
1 times 1 what does that equal
well after multiplying this by 1 we
still have 1 so what about on the bottom
equation well we have negative 1 times
negative 1 so again we're graced with a
1 we're not going to multiply out the
left-hand side but it's still y sub I we
just happen to derive the answer on the
right-hand side and then now what do we
do well now let's go ahead and set both
of these equations equal to 0 so we're
going to say I'll just use the same
color here we want both these equations
to be equal to 0 we'll do the top one
first so how will we make this top
equation equal to 0 well it's simple
enough you just subtract 1 from both
sides of the
so one we're going to save - one here
this cancels out this was already
converted so we're all set here this is
zero after we minus one and then all you
would do is you add minus one to that
side right so that top equation in total
becomes y sub I multiplied by the X sub
I W plus B and then minus one equals
zero and then looking at the bottom
equation what happens here well again we
we this one is already done and then
we're going to minus 1 from both sides
so minus one this becomes a zero minus
one here and lo and behold looks like
we've got the exact same equation so for
both the positive and negative class the
equation to derive support vector would
be right here so I think we'll go ahead
and stop here and we'll carry on in the
next tutorial as far as ok great we've
found these port vectors what do we
actually do with support vectors once
we've figured them out so that's what
we'll be talking about the next video if
you have any questions comments or
whatever feel free to leave them below
otherwise as always thanks for watching
thanks for all the support and
subscriptions until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>