<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Unsupervised Machine Learning - Hierarchical Clustering with Mean Shift Scikit-learn and Python | Coder Coacher - Coaching Coders</title><meta content="Unsupervised Machine Learning - Hierarchical Clustering with Mean Shift Scikit-learn and Python - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Unsupervised Machine Learning - Hierarchical Clustering with Mean Shift Scikit-learn and Python</b></h2><h5 class="post__date">2015-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EQZaSuK-PHs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody and welcome to another
unsupervised machine learning tutorial
video in this video what we're going to
be talking about is hierarchical
clustering classification whatever you
want to call it basically what's going
to be happening here is in the last
video we did clustering but we told the
Machine how many clusters we wanted it
to make in this video we're going to be
talking about how to let the Machine
choose how many clusters it thinks is
the most applicable so uh with that
let's go ahead and get started so first
we're going to import numpy as NP I'm
using numpy really only for one function
now we're not going to actually be
converting to an array for reasons I'll
be talking about in a moment next we're
going to go from SK learn dot cluster
import mean shift and we're going to
import mean shift as M s then we're
going to go ahead and do is from SK
learn data sets dot samples underscore
generator import make underscore blobs
so what we have to do here is we're not
going to be able to be using the same
data set that we've used up to this
point for a sample because it only has
six data points and six data points is
too few data points to use with mean
shift it's really two for you to use
anywhere but it's just worked for us up
until this point but it's not possible
to do it with mean shifts so anyways
we're going to make our own sample and
what this is going to do is it's going
to exactly what it sounds like it's
going to make blobs of data based on
standard deviation a little bit of
randomness it's going to make blobs of
data around a point that you specify and
you'll say how many blobs you know how
many data points you want around that
point and then also you'll get you can
specify the standard deviation so that's
what we're gonna use that for and then
finally we're going to import map plot
lib pie plot as p LT so that we can
actually visualize our data so now we're
going to go ahead and say the following
we're going to specify centers variable
and it's going to be a list
of Lists now for now we'll just have two
data points to start with we'll use a 1
and a 1 and we'll use a 5 and a 5 so
those are our center points
now we're gonna specify X and a
lowercase Y and if you happen to see any
other tutorials on this topic you may
see people do X comma underscore here
and the reason why they're doing that is
this this from SK learn data set sample
generator what it's going to do for you
is actually two things it generates
sample data but not just any sample data
it's specific to machine learning and
it's generalized data for either
unsupervised or supervised machine
learning so what it's going to do is
it's going to come with labels so
whenever you see this underscore like
this when we're unpacking variables it
means pay no attention it's useless in
this example basically I'm going to call
it Y because that's what it is and
that's how it relates to the other
videos in the past but just understand
that the reason why people might
underscore it is to kind of underscore
the importance of understanding that
normally you do not have labeled data so
I don't know maybe I'll just leave it
there actually I don't know it doesn't
matter just understand what what that
purpose is so anyways make blobs I love
that joke underscore man so funny and
make blobs you're gonna say how many
blogs you want so n underscore samples
this will be how many blobs do we want
to create let's make a hundred just for
the record mean shift is thought to be
accurate up to 10,000 samples more than
that
you're in trouble so if you have data
that's bigger than 10,000 samples you're
going to need some way to simplify that
down you can actually use use use you
can actually use machine learning for
that as specifically even unsupervised
machine learning but maybe more on that
later for now our number of samples
we're going to say is 200 then centers
is going to equal well centers and then
finally cluster underscore standard
deviation is going to be one
for now so this is our function we're
unpacking to a variable that doesn't
matter and of course X which is our data
set now what we can do is we can do PLT
dot scatter and we can scatter using
that fancy code that we learned so
capital X colon comma 1 and then finally
run a PLT dot show on the data flips and
we will save and run that and here we
have our lovely data set that you know
with your eyeballs you can say okay this
is a cluster here and this is a cluster
here now we want the computer to make
that choice and we're hoping that it
really does indeed say this is a cluster
and this is a cluster and it doesn't say
well here's a cluster here and here's
another cluster and another cluster and
then here's another cluster we don't
want that we're hoping it doesn't do
that
so anyway closing out of this I'm going
to comment this out we don't need this
anymore but if you want to look at it
that's totally fine now and in fact
maybe I'll leave it up for the first
time running because it'll be a scatter
plot of the same data non-colored
because we're going to end up coloring
it now we've got we're going to
reference ms for a mean shift so now
we're going to say M s dot fit and we're
going to fit a capital X there now we're
going to say is the labels equals ms
labels underscore now please don't
confuse these labels with these labels
these are the actual labels that were
used in the generation of the data set
these labels are the labels that the
Machine has assigned to the data so
later on of course you could compare
these labels to these labels but I will
just say that that might be useless as
far as generating any sort of
performance because for example if we
put these two centers close enough like
say we generated with a 2 and a 2
standard deviation is 1 therefore we're
going to have some crossover that could
be justifiably so more related to this
than this that's just the case so just
keep that in mind that if you've got
enough standard deviation
in your samples even if they come
pre-labeled and then you're using that
to test your unsupervised clustering
algorithm just understand that it's it's
going to do what it thinks is best and
if the data even though it was generated
with an initial - - if the variance is
enough it makes more sense to be
clustered with the 1:1 so keep that in
mind
moving along those are labels now we're
going to do cluster underscore centers
and these are going to equal the MS
cluster underscore centers underscore so
these will be the X like the centroids
only not so much but this will be the
estimated centers so again it will be
interesting to see how closely the
computer predicts the cluster centers -
what the actual cluster centers were in
theory the higher we makes the samples
variable and the lower the standard
deviation is the more accurate this
valuation will be in relation to the
true starting number moving on now we're
going to have n n underscore clusters
and the number of clusters is going to
equal the length of the NP unique and
what the NP unique functionality does is
it looks at an array and it tells you
how many unique variables there are not
what the unique variables are about how
many exists so an MP dot unique and then
we want that to be run on labels and
this will tell us how many categories or
cluster types the Machine has returned
to us now we want to go ahead and do is
we're just going to print this out and
we're going to say number of estimated
clusters and we're just going to have
that be n clusters underscore so that
just returns to us how many numbers or
how many clusters the number of how many
clusters there are now right at specify
a list of colors for plotting it'll be
very similar to what we had used before
so we'll just kind of make a list that
we're about to populate now we're going
to do our period for red green period
for green orgy period for green B period
for blue then we've got C period
kay period for black ee or Y period for
yellow and then we'll do am period for
magenta now it's really possible that we
have more than these many clusters so
just to make sure that doesn't be the
that doesn't become the problem we're
going to do ten times and what this is
going to do is just make this list ten
times as big as it is it'll just be
repeating over itself that way we won't
run out of colors to choose from
hopefully if everyone's in 70 categories
something wrong so moving right along
we're going to go ahead and just so you
can visualize what's happening there in
case you haven't followed you can print
out colors and then we'll also print out
labels so we understand it's just it's a
one-dimensional or right now we're going
to do pretty similar code to what you've
seen before in the previous video so
none of this should be too to new so
we're going to say for I in range of the
length of capital X so basically for I
in the range of this cluster data that
we generated what do we want to do and
we have the labels that the machine has
chosen in a list so that will be a list
of equal lengths to X so we'll know what
the corresponding label was and now that
label will be either a 0 or a 1 so then
we can call or of two or three I suppose
if it incorrectly decides that there's
more clusters than there are and then
we'll use that number and apply it to
this list to pick what color to chart it
in so for I in Len of X we are going to
PLT dot plot and again using similar
syntax as before the X or the eye of the
X 0 the eyuth of the X there we go
anyway X I + 1 and then we'll say colors
labels I and that will be the you know
one of these in this list basically
again this is all similar syntax to
before so hopefully you're still
following there and then marker size
will equal 10 and that's it for this for
loop we don't really need to do anything
else we could print out the coordinate
in the label like we did before but we
have 200 samples so I don't really
want to do that next we're going to
split up plot and scatter the center
marker right so we've got cluster
centers kind of like centroids not the
same thing but kind of like centroids
cluster centers here and so we're going
to go PLT dot scatter and we're going to
scatter the cluster centers and this
will be using similar code that you've
seen so colon comma zero comma and then
I'm just going to copy this so we don't
have to type that all out again and now
now we'll use the number one then we're
going to say we'll hit enter here after
that comma don't forget your comma
before you hit enter there a marker
equals an X and the size will be 150 the
line widths which will be 5 and the Z
order will equal 2 and finally we run la
Tasha
save and run it and we wait a moment
hopefully not too long for it to pop up
took a while just to load there and here
is the initial plot with the data so we
can see okay we categorize this in this
but we're not really sure about what to
do with these center points we'll close
out of this ah
did we not even put into a positional
argument in there we sure did we put in
the fit X there fit X I'm looking at it
uh let me try the following we'll say m
s equals mean ship as follows try this
one more time there we are okay so now
we've got the plot these were the center
points now this time I was mostly
testing it so we'll run this one more
time and I'll explain what what just
happened I suppose so this was I suppose
attempting to import it as M s and we're
having a little bit of problem there
because we didn't we want to actually
run it run the function itself so that
was kind of our problem so it's this
function dot fit as opposed to just M s
dot fit so anyway
so now we're going to go ahead and do is
will run that one more time have this
pop up for us and so again here's our
two clusters and here's the center point
that I would not know where to apply
that so anyone would close this and
we'll see what the machine says okay
well the Machine says it's part of this
cluster here interestingly enough we can
move this over here and we can see okay
sure the number of clusters is two that
it suggests I'm into plot or print out
the center points let's close out of
this and let me write here we're going
to print cluster underscore centers so
now we can actually compare that to
these centers let's raise up the number
of samples as well so this is the data
we'll close this and it runs the test
fits and all of that and here we have a
group here and another group here and
now let's see what the centroids were
well we've got a point nine five and a
one point one two and a four point nine
and a five point zero zero so yeah
that's pretty darn close to the initial
one one five five now what happens if we
for example say let's do three eight run
that again so here we've got what
appears to maybe be a blog but you can
totally see how the center point could
be right about there see what the
machine says and indeed the machine
agreed with me that that looks like it
mostly is all one big blob and this is a
second blob now there are some ways that
we can tell the machine hey we want you
to kind of be a little more specific in
your categories or you want to be more
unlike we would call this maybe a blob
in this part of that blob and this is
pretty clearly a blog there's even white
space between them but we can use it to
create more just like a more tight
threshold basically there's a lot of
parameters that we can adjust here and
you can see why that becomes necessary
let's do
ten see what this one does so this is a
little more defined I can see clearly
three center points there so hopefully
gives us three
and sure enough it does although the
blue X we cannot see where the blue X is
that was kind of a silly idea what were
you guys thinking anyway uh so now we've
got these groups here so we can see that
it indeed chose right I mean obviously
this point could have been from this
point and all that but for the most part
we can definitely see that this looks as
we would expect and sure enough one one
five five and three ten and that is
exactly what we had pretty much chosen
so we can see that that seems to have
worked pretty well but of course we can
always fool it by increasing standard
deviation and getting the following so
how would you group this all right what
are you gonna do maybe you know apply
this as an outlier or something like
that we'll see what the machine says
right so the machine says okay this is
clearly an outlier these guys are closer
to this little guy over here so it says
yep that that's a point of its own and
then everyone else is the same so
obviously there's when it comes to this
there's a lot of tweaking that's
required depending on what kind of data
set you have so and that only makes the
most sense what we're doing is we're
throwing data at the Machine and we're
basically letting it do everything so it
only makes sense that this is the more
complex version of machine learning and
let's let's do a 0.3 just for just for
kicks so obviously that's pretty clear
and yeah it's right on the ball pretty
much and again you can see that accuracy
as far as picking the center point is
pretty darn high just simply because of
like what I was saying the standard
deviation went down and if we rose
number of samples to a higher number it
would only be even more accurate anyway
that's it for the basics of this like I
said as you can see this is only going
to get more and more complicated
especially when it comes to unsupervised
machine learning
and allowing the machine pretty much
full ability to determine you know what
is actually different in the data so we
can see that it can be quite accurate
and it's very good if you feed it the
correct amount of data and later on
we'll talk maybe a little bit more about
some of the parameters that you can
specify especially in mean shift here as
well there's a lot of default parameters
that we haven't covered so anyways basic
covering of it
pretty cool stuff if you ask me but but
yeah there's definitely a whole lot more
to cover here so anyways if you guys
have any questions or comments please
feel free to leave them below otherwise
as always thanks for watching thanks for
all the support and subscriptions and
until next time everybody what is going
on I just want to let you know that I do
plan to make a sort of project based
example with unsupervised learning kind
of like we did with supervised learning
as well as the meshing of the two for a
more of a semi supervised machine
learning and there's actually quite a
few more machine learning algorithms
that we have to choose from with
scikit-learn so obviously the topic of
machine learning is just massive so for
now I'm going to be taking a break from
it we've been doing this basically for a
month straight here so I'm going to be
taking a break for it from this for now
because not all my subscribers are
interested in machine learning but I am
coming back to it because I am
interested in machine learning so have
no fear it will be back and whenever
there are more videos there will be a
link in this video if you have
annotations turned on as usual you'll
just need to click on it otherwise I'll
add it to the playlist which is also
there's a link in the description for
that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>