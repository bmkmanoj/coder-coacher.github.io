<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine learning - Python Programming for Finance p. 12 | Coder Coacher - Coaching Coders</title><meta content="Machine learning - Python Programming for Finance p. 12 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine learning - Python Programming for Finance p. 12</b></h2><h5 class="post__date">2017-01-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W4kqEvGI4Lg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody welcome to
the moment of truth as well as part 12
with the Python for Finance tutorial
series in the previous tutorials we've
worked on grabbing the data we worked on
creating our feature said creating our
targets we returned our feature sets our
labels in the data frame just in case
and now what we're going to do is
actually do the machine learning
run-through classifier see what we get
so first of all we're going to make some
new imports here we're going to come
here and we're going to say from SK
learn import SVM cross-validation and
neighbors and then we're also going to
from SK learn dot on samba we're going
to import the voting classifier and new
random forest classifier lots of
information there
just for the record anybody who is
interested in machine learning
understanding what the heck these
classifiers are doing you'd come to
Python brunette click on data analysis
go to machine learning click on
practical machine learning with python
boom and what this series does is it
goes through each of the classifiers
explains what they do shows a simple
application of those classifiers and
then we actually write the classifiers
ourselves in raw Python so it's a really
great way to learn about them if you
want to know more otherwise you can just
do what we're doing here and not worry
about or if you already know same thing
so basically SVM for support vector
machine cross-validation so we can
create nice training and testing samples
because you don't want to test against
the same sample as you trade against you
want those to be kind of blind but you
also want to shuffle your data so
cross-validation for that reason
neighbors because we're going to do K
nearest neighbors voting classifier
because we're not going to use just one
classifier over these many classifiers
and let them vote on what they think is
best a nice way to kind of smooth out
any sort of unstable I guess
classifications possibly and in the case
of having so much so much data chances
are that's going to be the case random
forest classifier just another flash
fire awesome now going down to the
bottom here what we're going to do is
create our new and kind of final
function for doing
learning or so at least typical machine
learning and that is going to be defined
do ml and we're going to check our phone
do ml we're going to do ticker and we're
going to get our feature or future sex
labels and of course the data frame and
we're going to do that with extract
feature sets ticker we're just going to
copy that come down here and paste and
we don't need to run this anymore okay
so that our future SIF labels now we're
to create our training and testing so
it'll be X underscore train X underscore
test Y underscore train Y underscore
test equals cross bow validation this
actually cross validation train
underscore test underscore split train
test split takes X it takes Y and it
takes the test size and we'll say the
test size is going to be 0.25 so 25% of
our tech of our sample data will be a
what we'll actually test again this will
give us our accuracy once we've done
that we're ready to actually create a
classifier now like I said you can learn
how to create classifiers and what they
do and all that but if this not what
this tutorial is all about so let's just
define any old classifier we're going to
say we're going these neighbors got K
cane 8 neighbors classifier so we've
defined our classifier now we're going
to say CF it dot fit the equivalent of
dot train so we're gonna pass X training
and why train just again so we're clear
X train is this data here remember and
then obviously these are just changes
but that was then VfL values that's what
gets passed X this data is the percent
change data for all of the companies
including the company in question that's
what this data is why is the target it's
0 1 or negative 1 so 0 4 hold 1 4 by
negative 1 for Snell that's
target classified classification and
what we're trying to do is use a
classifier that will fit the input data
to the target that we're setting so
let's see how we do right away we can
find out how we did we can say confident
well not right away we'll take a second
for this to actually happen but we have
said confidence equals T last score and
we just passed X test Y test and then we
also just because I want to know C
confidence equals C laughter score will
also grab let's just do this to friction
you can see left predicts and then we'll
just pass next test my test again
actually sorry just XS so moving forward
once if you train and you're caught
you're happy with the confidence all you
have to do to further predict is we
literally do CL f dot predict and if you
wanted to not have to retrain this model
again you would pickle it so using what
you've already seen so far and you would
pickle out the classifier and then
literally to use it ever again you just
load in the classifier it's already
ready to go call it CLS and then
literally all you need to do is see left
predict and that outputs the actual
value and you can pass either a single
value here you can pass a huge list of
values it doesn't really matter so in
this case we're patched passing a large
amount of the feature sets but it's and
then it will return to us a nice list of
predictions so those are our predictions
now we're going to go ahead and do is
print predicted read I don't something
like that
counter prediction so we want to know a
lot of times with classifiers you can
get kind of overfitting or a strange
issue where basically everything becomes
one class so this will happen often
within balanced data because the
classifier is going to realize hmm I'll
actually get the best accuracy by always
predicting a certain value and so that
can be very problematic so you want to
see okay what are the predictions that
we were making and we also want to see
are they skewed at all or like because
sometimes maybe not
all of them are one but like 99.9% of
them are one prediction so yeah so
predictive spread we just want to know
what's going on so at the end of this we
can return confidence but for now we're
not going to really do anything with
that so let's go ahead and do ml with
will do Bank of America so cool so let's
go ahead and run that so what we get
hopefully Neyers so we see right away
are spread which is already in balanced
it's relatively balanced by cell but
it's imbalanced we did not predict act
or print out accuracy
I'll get back in a second let's print
accuracy here I can't believe we didn't
click print plus I should say accuracy
confidence okay
try that one more time no you can't
we'll wait so yeah these are pretty
balanced this is relatively imbalanced
read the other ones and then accuracy
point three seven seven so it's better
than just in theory randomly choosing
but actually even randomly choosing but
plenty worse than we did because this
data set just in general it's clearly
more often to buy than it is a cell than
it is a hold and we do see our
predictions at least somewhat model that
they do say okay a lot more actually no
they don't huh the first I thought they
did but no we actually got a lot more
cells than we did buys and we still did
better than the than average which would
be 33 yes definitely actually riveting
all right all right that's just a simple
classifier let's see if we can do any
better by actually coming up with a
voting platform other than just this
simple K nearest neighbors let's take
three classifiers so them all at the
problem and then let them vote on what
they all together think our best so now
will say see less people's voting
classifier and then the voting class
file will take a list a list of tuples
of classifiers by the name in the actual
classifier so first it's a list
and then the list will contain two pools
of classifiers we're going to have three
classifiers the first class fire is
going to be linear
SVC so linear support vector classifier
so what's called this LS BC and then
we're going to call this SP m dot linear
as VC so we're just referencing that
support vector machine for linear
support vector classification default
parameters next we're going to KNN for
the K nearest neighbors again we're
going to use this this exact thing right
here so I'm going to copy paste and then
finally we'll add one more and our for
work for random forests so random forest
classifier awesome so now we've got a
new classifier classifiers in task let's
see how we do save and run that come on
man what we got wrong here I'm not
seeing it is it no dude
might have to pause so because I'm
obviously blind here might pause okay so
interestingly enough it's actually up
here that I've messed up let me go ahead
and to reconfigure here because it's
actually off-screen I'll just do 18
mostly I'll say it let's see this it
sure does so for some reason the rest of
this got deleted
so anyway X Y and then 0.25 for the
train test split okay so let me try to
run that again hopefully please
good musta happened when I deleted the
other classifier or something
singleton array drew it's wrong why are
we getting all these errors now but I
appreciate this I guess maybe maybe
we'll have to add test sighs it's
probably corresponding to a different
value like a different parameter and
then while that's going oh look that we
got actually we did improve pretty
significantly there although we got way
less predictions for fer a hold okay now
just just for the record there are a ton
of parameters for each of these
classifiers I'm not going to get into
them because again you kinda need to
know how these classifiers actually work
to understand the parameters but for
each of these you have especially with
scikit-learn is kind of API on top of it
not only do you have like just the
traditional parameters that you have in
something like K nearest neighbors like
K is a value how many of K we're using a
default which I think is either three or
five but that you might want to change
that you know there are so many things
that you might want to change with each
of these so we can almost certainly
tweak this algorithm to be even more
accurate than it is right now but for
now what I'd like to also there's a
whole lot more machine learning
classifiers these are just three that I
just quickly made up not made up but
pulled out of mine you know what so the
other thing I want to talk
is okay this unfortunately this data is
just simply an imbalance it's not
perfectly balanced so that can kind of
give us a little bit of a skew in our
accuracy so even though this looks
pretty darn good and we are still this
is fascinating it's still it isn't
totally predicting just with this
imbalance it is making this slight
change on this new data and it appears
to be accurate interestingly enough
anyway there's a few things that are
wrong here I mean possibly where we're
predicting hold we might be wrong
because we didn't think it was going to
raise more than 2% but it turns out it
did you know and stuff like that you
could also be wrong here where he said
yes it's a bi and but it didn't raise 2%
it rose 1.5 percent but it was still
goodbye and same thing on the cell but
that can go easy the other way too you
know
so you got to really pay attention to
these you know the accuracy is useful
but you really need to know what happens
in practice this is just not good enough
but I do want to like let's see if we
can balance these these numbers and
still stay higher than 33% accuracy so
coming over to here rather than using
0.02 let's see if we can't come up with
some better numbers so like let's change
this to be 0.025 and let's do this as
0.025 let's see if that balances out the
data a little bit better ok yeah
definitely a little better not perfectly
balanced but close and we're still about
40 percent accurate let's see if our
predictions so real heavy on it down
this is for Bank of America you pull up
banks because well no that wouldn't make
any sense because this is shuffled data
that's being predicted again so it's
fascinating that each time this shuffles
we're getting a different thing anyway I
wonder what's going on there anyway ok
so we're on the right track still a
little bit more buys and sells normalize
up to 0.28 and then let's do this as
seven so two point seven two point eight
I'll see if we can get a nice balance
here
that's pretty good this is about for
1,400 of each thing it's still still
strong negative predictor which is
strange because I don't think Bank of
America is doing horribly let's pull up
the old Bank of America shall we let's
see okay this is just reason but yeah
it's weird that looks so often we
predict a drop oh wow yeah that's a
pretty steal the recession hurts some
people more than others
I know you guys feel so bad about the
banks alright okay so yeah so cool
that's interesting I no wonder I'm sorry
I just I really want the training to be
perfect I just I'm fighting an
interesting that we're able to balance
the spread initially and then they get
shuffled and then somehow it's
imbalanced but whatever still mostly
protecting negative one interesting I
wonder if there's some sort of issue
here that's causing that but let me go
ahead and move on
from here we'll leave that in the dust
because basically the end of the day you
can get these answers but the question
is actually like how good do we do in
practice not just how good is our
classifier everything changes when you
actually go into practice with one of
these and actually trade on one of these
because your accuracy suddenly doesn't
matter as much as as you might think so
anyways that's it for now if you have
questions comments concerns what I was
physically didn't blow in the coming
tutorials what we're going to be doing
is taking this data and learning how to
do back testing with it I thought about
doing deep learning with this data
because in the current situation it
would be pretty quick for us to throw in
a deep neural network into this but I
think I'm going to leave that out if you
really want to see it feel free to let
me know maybe I'll throw one in it's
almost certain to give us the exact same
accuracy we just don't we simply don't
have enough data to do deep learning and
do it right we would need a whole lot
more data
this but it could give interesting
results I don't know but for the most
part when I play with deep learning with
a data set as small as this one is it's
just there's no fruit sort of ice we're
going to get into actually back testing
and actually seeing how would a real
strategy deployed in the market do so
that's what you guys have to look
forward to questions comes below I'll
see you next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>