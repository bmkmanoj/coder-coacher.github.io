<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Support Vector Machine Fundamentals - Practical Machine Learning Tutorial with Python p.23 | Coder Coacher - Coaching Coders</title><meta content="Support Vector Machine Fundamentals - Practical Machine Learning Tutorial with Python p.23 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Support Vector Machine Fundamentals - Practical Machine Learning Tutorial with Python p.23</b></h2><h5 class="post__date">2016-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZDu3LKv9gOI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody and welcome
to another support vector machine
tutorial where we left off we were
running the calculation for every
support vector so we have that now and
what we're going to move into is the
calculation for the width that separates
the hyper planes that go through these
support vectors so let's just visualize
that real quickly so you're going to
have some vector space and you're going
to have some features and then real
quickly what are our support vectors we
can look at this and just know right we
know for sure this is a support vector
and then we know this is a support
vector and this is a support vector the
reason why we know this as these are the
points where the two parallel
hyperplanes with the most distance
between them would be going through but
we also know mathematically that what
what is the definition of the support
vector itself like mathematically how do
we know it's a support vector for sure
because this equation is actually
something is a more like a constraint
that we the scientist are imposing on a
support vector that doesn't necessarily
tell us that we have a support vector
because first of all we don't even know
at this stage what W is and what B is we
just simply know that we want this
equation to equal zero and as you'll
soon see we're going to go through a lot
of work to make that the case based on
the value we have for X sub I so we this
is not the equation that we're going to
figure out we're just going to run this
equation and somehow it derive what the
support vectors are instead a support
vector is the let's say feature set that
if you were to move it like let's say we
took this Plus this bottom plus and
moved it to here would that affect this
hyperplane absolutely right no longer
might that be the best separating
hyperplane it might be something else
and so so we we call this a support
vector because if we moved it it would
affect the rest it would affect the best
separation
so we know what our support vectors are
and then we have our nice beautiful
hyperplane and the objective though if
we go all the way back to that first
video was to actually come up with the
best separating hyperplane between the
data sets so that separating hyper hyper
plane can be calculated simply by taking
this right here right the the width and
then you could take that width divide it
by two and then basically from here you
could just plus that width and you would
find you would get yourself to that
hyperplane so we know that we want to
maximize that width and now what we're
going to talk about is how we how we do
that
so the equation is for width equals x
plus minus X negative and these are the
vectors dotted with vector W over the
magnitude of vector W so again what are
we wanting to do here we want to
maximize this equation and then what are
the basically the variables in this
equation well we've got of course now
here here here here but we actually
already know what this and this are
equal to because in the previous
tutorial we worked out what that what
the equation is for those values right
for X minus and X plus this is the
equation that we're using so pulling
that down we get right remember that's y
sub I X I W plus B minus 1 equals 0 and
then it was actually the same thing here
right both of those equations and if we
were to do the algebra in this example
we would find that the X plus is
actually it would come down to being 1
minus B and then we'd have 1 plus B for
the for the X minus and then again if we
worked out all the algebra we'd find out
that this entire width equation comes
down to being 2 / vector the norm
magnitude of vector W width is equal to
2 over the magnitude of W and we want to
maximize this so we know we want to
minimize that
and then we just simply rewrite we say
yeah we want to minimize the magnitude
of vector W and then for just simple
mathematical convenience we're going to
say well yeah we want to we want to
minimize vector W or we want to minimize
the magnitude of vector W that means we
also would want to mat minimize one-half
the magnitude of vector W squared right
so long as we want to minimize just
simply the magnitude of vector W we'd
also want to minimize half the magnitude
of vector W squared right that works so
we do that for mathematic convenience
that we're about to plug it into an
equation and that's just going to make
it a little easier for us so we want to
minimize one-half the magnitude of
vector W squared so we want to minimize
one-half of the magnitude of vector W
squared with constraint to our support
vectors which we have defined as being y
sub I times X sub I by W plus B minus 1
that is our constraint ah someone said
the C word there's a knock at the door
it's L'Orange he brought his multipliers
and together we're going to work on this
problem so then we're gonna have our
lagrangian and it's going to state that
the l w b equals what we already decided
was going to be our statement for the
magnitude of W and that was going to be
one-half the magnitude of vector W
squared minus the sum over alpha i where
alpha i is our lagrangian multiplier and
then the constraint we have is the
constraint that we define as being our
support vectors so that was y sub I
multiplied by X sub I
w+ b -1 and that is our initial
statement here that we need to optimize
and so again we want to minimize w and
we actually want to maximize B so why do
we want to maximize B and what exactly
is B well if we look at the really
simple equation for a hyperplane so the
equation for a hyperplane is just W X
plus B what's that look like that looks
a lot like MX plus B right for the
simple line and what happened when you
modified B well what even was be right
what B was you're just that was just
your y-intercept and if you modified
where the line intercept to the Y that
would just move it up or down the same
is basically true for a hyperplane so to
exemplify this let's draw some vector
space and then let's add a couple
hyperplanes actually we'll start just by
adding a single hyperplane so this is a
hyperplane right it's all filled in and
then what if you were to add B well if
we added B let's say B this is B we're
adding B moves you up so then if you had
another hyperplane you'd have this and
what you've done here is you've added to
B so you've just you've basically
shifted this hyperplane and B in terms
of the support vector machine is
generally going to be called bias that's
kind of why we're actually using B here
so heading back to our Lagrangian here
we need to differentiate L with respect
to W and we also need to differentiate L
with respect to B so differentiate L
with respect to W and we also need to
differentiate L with respect to B okay
so when you did let's say first we'll do
differentiate
l with respect to W you're going to find
that you've got basically it's going to
come down to being vector W minus the
sum over alpha I Y I X I which then
simply becomes when you move this W
basically but we'll just add the equal
sign here so it simply becomes vector W
equals the sum over alpha iyi X I and
I'll go ahead and make X I official okay
so then we can differentiate L with
respect to B now so we can say
differentiate L with respect to whoops
differential L with respect to B and
what we get there is going to be minus
the sum over alpha I Y I and you would
say that is equal to zero but then you
can actually just you would come up with
L with respect to B is the sum over
alpha I Y I then what we come up is l
equals the sum I over alpha I minus one
half of the double sum of I and J over
alpha I alpha J Y I Y J dotted with
vector X I dot X J and we want to
maximize this so we know we still have
this constraint optimization problem but
right away we should recognize this
alpha by alpha this alpha squared when
you see that we recognize uh-oh we had
of a quadratic programming problem so
understandably some of you might be a
bit fuzzy at this point if that's you
that's totally fine and honestly should
be expected if you're not lost or
confused and this is your first time
learning about the SVM you're a robot so
what I've shown so far is the more
formal and traditional calculation for
the support vector machines optimization
along with some of the facts and
assertions that an SVM just just has so
it's important to at least be familiar
with this stuff as you're going to see
these formulas if you continue working
with SVM's also you need to know them if
you're going to work with an
optimization library and you definitely
need to know how the SVM works if you
intend to you know make your own like we
do in the series it is also important I
think to cover the more formal
calculation of an SVM or the typical
calculation because I think it really
highlights some of the downsides and
upsides to the SVM so one of the
downsides is the just complexity both in
math but in also the optimization
problem itself the other major downside
of the SVM is that you need all of your
feature sets in memory to optimize so if
you have a really large data set this
might not be feasible like like a data
set that's a hundred gigabytes or
something that's not outrageous okay so
I mean that's a huge day set but I mean
it's not it's not too crazy in this
world and the fact the chances of having
100 gigabytes on your typical machine I
mean you might have a big giant machine
but probably not so that's gonna be
pretty hard now of course this was a
problem with K nearest neighbors as well
but this is one of the major downsides
to the SVM now there are methods to kind
of work with small or larger data sets
and you can do it with like mini batches
basically probably the most popular
choice nowadays is sequential minimal
optimization or SM Oh which was created
by John Platt actually quite a while ago
at Microsoft but that's probably now the
most common used one if I was to make a
guess but anyways either way the SVM can
be pretty cumbersome it
now the upside though is once you have
trained a support vector machine you do
not need the you don't need the feet the
old features anymore and a
classification for a new data set is
just W X plus B and not even just W X
plus B it's the sign of W explodes me
right is and not not sign as in cosine
sine and sine as in is it a positive or
negative if it's positive it's a plus
class and if it's a negative it's a
negative anyway in the next video I'm
going to hopefully clear up some of the
fuzziness if you have it here by
dramatically simplifying our problem
which we can do since we have a pretty
decent foundation I think now of the
support vector machine and its
constraints and then hopefully I'll be
able to get you completely clear when we
go to write our own support vector
machine from scratch in Python because
we are actually going to write our own
optimization algorithm in Python and I
think once you do that and you recreate
the support vector machine and do the
prediction the training and all that I
think that's going to help tremendously
at least for me I was pretty fuzzy until
I actually tried to do it myself so
hopefully it will help you as much as as
it did me so anyway I kind of suggest
you hold your questions until at least
the next video when we kind of do it
basically a recap and simplify the math
that we've seen so far but you can still
leave one below if you want otherwise
you can of course leave comments below
and as always thank you for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>