<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Text Classification - Natural Language Processing With Python and NLTK p.11 | Coder Coacher - Coaching Coders</title><meta content="Text Classification - Natural Language Processing With Python and NLTK p.11 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Text Classification - Natural Language Processing With Python and NLTK p.11</b></h2><h5 class="post__date">2015-05-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zi16nl82AMA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody welcome to
the 11th NLT k with python for natural
language processing tutorial video in
this video we're going to actually start
creating our own algorithm or text
classifier as what it's called now we're
going to be doing a text classifier for
sentiment analysis but you can also use
text classifiers for all kinds of stuff
maybe you're trying to classify the text
as stocks writing or politics writing or
whatever you want economics or anything
or another form of text classifier might
be just discerning whether or not
something is spam or a legitimate email
that kind of thing so our text
classifier is going to classify
something as either a positive
connotation or a negative connotation
basically or meaning or sentiment as a
form of opinion mining basically so
let's go ahead and get started and show
how we might actually do this because
this methodology our methodology here
can be applied to any categories as long
as they're tagged and there are two
categories so we just have two choices
there's not a degree of choices it's
just one or the other so this is spam or
it's not spam right when you have a spam
filter you've got spam or not spam you
don't have like a bunch of little
folders that are like maybe spam highly
likely spam definitely spam no usually
it's just your inbox and spam okay same
thing here we'll do sentiment analysis
positive or negative now of course with
sentiment analysis that's not
necessarily the case you might have
something that's slightly positive
highly positive extremely positive and
the same for negatives but anyway this
is just an example but you can feel free
to get your own list and your own tagged
list even or create your own list and as
long as it is a document with some words
in it and it's labeled as either one of
two labels you can use anything you want
it could be spam it can be defiant you
know whether or not something as a text
message versus a you know official
document or something like that it could
be anything so anyway let's go ahead and
get started work of course going to be
using NLT kay what's going to import
random and we're going to use that to
shuffle up the data set that we have
because right now it's very highly
ordered it's all
for the first thousand all positive for
the second thousand that's no good
and then finally from NLT KDOT corpus
we're going to import the movie reviews
so this is what I was just talking about
it's a thousand positive and a thousand
negative movie reviews so we can train
against them and they're already labeled
you know so that's the whole idea of
training data now when you read the NLT
K documentation you're going to see the
following one-liner this is kind of a
confusing one-liner if you ask me it's
kind of a wasted one-liner because it's
basically the same amount of lines if
you didn't make it a silly one-liner but
people are just in love with their
one-liners so I will show it to you
because this is how most people do it
anyways so let's go so documents is
going to equal and it's going to be a
list and it's also going to be it it's
going to be like a list of tuples
basically in the tuple itself will be
the words and we're going to call words
as features so if you follow along any
sort of machine learning tutorial
including some of my own machine
learning tutorials you'll know that
you've got features and those features
are what makes up you know the elements
of something and we use those features
to train based on their categories and
or tags or whatever you want to call
them so anyway we're going to have a
list of tuples and the first L or the
zeroth element in that tuple will be the
words they're basically the presence or
the non absence there it is of a word
and then the second part of that tuple
will be the category so this will be
anyway the list of movie underscore
reviews words for a file ID and this is
why I hate one liners because file ID
makes no sense at this point but we'll
get there and category so again it's
it's a list of tuples so you can even
see this is a list of a tuple and then
basically this is where or for the
category in movie reviews categories so
this is basically for category and
positive or negative and then again for
file ID in moon
moving reviews dot file IDs for the
specific category category okay so this
is a one-liner we could put all this on
one line we're just choosing not to
excite would be absurd and the one-liner
works basically if you wrote this
yourself you would have you know
documents documents equals an empty list
then you would do this is the first part
so it's asking you know for category
movie reviews that's the first part then
for file ID whoops let's get just this
for file ID in the movie reviews :
without the l and then we would have the
tuple that the tuple equals that and we
would do something like you know
documents dot append we would append
this tuple of course so anyway this is
actually easier to read I don't know why
ruin insists on one liners sometimes
they make a whole lot of sense they
really do make everything nice and
compact but this one doesn't so why but
anyway
documents equals that good to go next
what we want to do is we're going to do
a random shuffle of those documents
because we're going to train test and
when you train a test you test on
separate data that you did not train
against because if you train and test on
the same data that's extreme bias so we
want to shuffle the documents so now
let's go ahead and just print documents
and oops the first if element in the
documents just to see where we are to
make sure not doing anything too crazy
at this point but it should be documents
one will be a list of words and yes
there it is and then whether or not it's
positive or negative so this is the list
of all the words in this document
starting with a Denzel Washington is
among the many available and then
finally moving to the end this is all
the features and then the rating here is
positive so this will be part of our bag
of words so to speak now what we're
going to end up having to do is take all
of these words though literally take
every word in every review and compile
them and then basically we do
is we'll take that list of words and
we'll find the most popular words used
and then we take of those most popular
words which one appear in positive text
and which ones negative text and then we
simply just search for those words and
whichever one it has more negative words
or more positive words that's how we
classify it so hence the word naive for
Bayes algorithm a document one and so
now what we're going to do is now that
we have all the documents loaded we
don't need to print documents one
anymore that's a waste so I just wanted
to show it to you and now we're going to
say all underscore words is going to be
an empty list and then we're going to
say for W in movie underscore reviews
words what do we want to do w first of
all is going to equal W lower we just
want to make sure everything's
normalized so we're not gonna care about
casing in words so we're just going to
convert everything to a lowercase and
then we're just going to say all and in
fact we can just do this all words all
words append W lower so now this adds
all the words to this list and then
later on I sort of misspoke the
documents is basically what we'll do to
create training and testing sets so
we're actually not going to add all the
words from documents this this element
here I suppose basically we have the
reviews and then all of the words so
this is just just words from all the
movie reviews so this is actually how
we're going to compile this massive list
of all words then later on we use the
features of documents which are words to
compare so anyway so moving on now what
we can do is we can actually find out
really quickly who are the most common
things so first of all all words is a
list right now so let's convert that to
an NLT K frequency distribution so we
can do all underscore words um equals
NLT k3 quints rec dist of all words like
this and then we can do
this print all underscore words dot most
common and let's do the 15 most common
words and then while we're doing that
waiting for that well do we can also
find out like how many words are there
so this is still kind of coming up now
you just pull oh there we go so these
are the top 15 most common words so as
you probably recall with ml TK you have
a punctuation that is classified as
being possibly words so you can change
that if you want we're not going to
bother worrying too much about that
right now
but if you wanted to you can and then
put a few videos later we're going to
talk about how to improve the algorithm
if we wanted to but anyway you've got
commas these period a and of like these
are basically all words that there's
nothing in this list that actually
matters to positive or negative at all
so just keep that in mind now all words
eventually we're going to kind of shrink
all words but all words is actually a
huge amount of words I mean we I think
we have like you know 50,000 or maybe
even more so don't worry that the top 15
are kind of useless words that's just
because that's the English language but
just showing you that how to use this
frequency distribution mostly but you
can also do something like this print
and then all underscore words and then
like stupid okay so let's see how many
times is the word stupid pop up in our
entire corpus of movie reviews so we'll
kind of wait for this to show up I just
pull it up here and this will mark the
end okay so here we have two hundred
fifty three times the word stupid
appears in these reviews so keep in mind
that we've got two thousand movie
reviews total two hundred and fifty
three times the word stupid appears so I
mean obviously someone might write a
movie review and use the word stupid
many times that's totally possible but
you can see that quite a large number
almost 1/4 possibly movie reviews have
the word stupid in them which is
understandable because we do have you
know
a thousand negative reviews anyway are
not a quarter my bad and eight because
we have two thousand reviews anyways um
that's it for now we'll continue
building on this in the next video now
that we have all words and then we have
all the words per document here and
their category now we can actually start
to train so we can train based on the
words in each document and the category
has we can use the naivebayes algorithm
to say okay well these words are
generally positive these words are
generally negative so we can do that and
then we'll end up testing it and see how
good this algorithm actually is it's
actually pretty basic algorithm anyways
uh that's it for now if you guys have
any questions or comments up to this
point please feel free to leave them
below otherwise as always thanks for
watching things for all the spoiler
subscriptions
until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>