<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Words as Features for Learning - Natural Language Processing With Python and NLTK p.12 | Coder Coacher - Coaching Coders</title><meta content="Words as Features for Learning - Natural Language Processing With Python and NLTK p.12 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Words as Features for Learning - Natural Language Processing With Python and NLTK p.12</b></h2><h5 class="post__date">2015-05-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-vVskDsHcVc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody welcome to
part 12 of our Python with ml TK for
natural language processing tutorial
video series in this video we're going
to be building on the last video which
is kind of starting up the processing
for our naive Bayes algorithm for
classifying text as being either
positive or negative using the movie
reviews data set so moving along we've
got all the way up to this point where
we basically get all the words in the
movie reviews data set and then we've
also got some documents that we can use
for training so let's go ahead and I've
commented out these two things I'm
actually just going to remove them I'm
gonna remove this too and now we're
going to go ahead and get started down
here
so first of all we need we want to have
some sort of limit on the amount of
words so right now we have this
frequency distribution of all words and
the frequency distribution is ordered
from the most common word to the least
common word and the amount it's like a
keys and values basically and so you've
got it's a huge huge amount of words and
we don't actually if we could train
against all the words but we it probably
relatively useless so we want to kind of
limit this a little bit so we're going
to say word underscore features equals
the list of all underscore words dot
keys we're not really focused too much
on the values like how many there are
and we're just going to say we want use
up to the top 3,000 words that we're
going to check against so if you
remember in the last video we did the
top 15 well the top 15 words were kind
of me because a lot of them were empty
words like the a a lot of stuff was like
punctuation like periods commas and
dashes so we obviously don't really want
that stuff included but this will be
good enough to encompass a lot of words
that are commonly used and then we can
train against these top 3,000 words
finding out which word is most common
and negative and which word is much
common in positive reviews so that'll be
the word features now we actually want
to have some sort of quick function that
is going to find those features within
the document that we're using so let's
come on
here and now we're going to say so
define find features and we're going to
find these features by searching in a
document and that document is going to
be part of these documents soon we'll
pass them through so within the document
is going to find features and we're
going to say words equals the set of
documents so the document if you recall
is the just going to be the first part
of the tuple is just a list of words so
when we convert something a list to a
set we get only one basically iteration
of any unique element so we don't have
many iterations we just this is
basically all the words not the amount
of those words but every single word
will be included in this set of words so
we've got that now we're going to say
features B equals and this will be just
a empty dictionary for now and we'll say
for W in word underscore features so
that's for the word in these top three
thousand words we're going to say in
this features dictionary the the key
that is the word in these three thousand
most popular words is going to be equal
to the boolean value for W in words so
this is this creates a boolean of either
true or false so if the word if that one
of these top three thousand words is
within this document now this will ring
true if it's not this will ring false so
that's how we find those features and
then at the very very end of this after
we've run that we're just going to
return features so now let's test this
new function and we're going to do
something like this like print and then
we can do we want to print the results
of font not fun find features of and we
want to do this for movie underscore
reviews dot words of the following let's
search in the negative data set and the
first one is CV zero zero zero
underscore two nine four one six txt so
then
we'll do that and then we'll call
something will say feature see feature
underscore I'll call it feature sets
like this equals and this would be a
list and then it will be find features
for the reporter various review whatever
putting through and the category
category for the review and category in
documents so this one-liner makes a
little bit more sense to use for me but
basically the feature set is going to be
the find features in the categories so
we're going to end up converting and in
fact let me run this one talking since
this kind of takes a while sometimes so
we're going to end up doing is
converting the data so each status set
again each document is going to be the
review on just the words and then the
category and what this is going to do
for us is it's going to convert that to
not a review but find features so it'll
convert before it was just the words now
to be the revert the words with true or
false so be a dictionary of the top
3,000 words and whether or not they are
contained in each review and then the
categories so now what we can do is
using these top three thousand words
this is starting to build what we're
going to be able to test against and
train against is the existence of words
and their impact on whether or not
something is positive or negative so now
that we have that let's kind of pop over
here so you can see what I mean so
feature set so first we printed the find
features so find features movie reviews
you that this is doing what this little
bit right here will do followed by
category so this has a contain you know
contains basically all the words so
let's see if we can find a true most of
these are false like it doesn't contain
any of these words let's just search for
a true
see if it has one okay so those have one
up here and it's the word as before so
before is one of the top 3,000 most
common words this contains it and this
is a negative review so you might
actually find that before is far more
common to appear in negative reviews as
we go on but we'll see it may or may not
be but anyway we can close this out now
and well go ahead and cut off the review
or the under review the tutorial at this
point and we'll continue building on the
algorithm in the next review oh I should
probably be able to build the tear of
the training and the testing maybe just
the training but we'll probably get to
that point in the next tutorial so stay
tuned for that if you have any questions
or comments up to this point we kind of
ran through a few things pretty quick
but hopefully I explained it pretty well
if you have any questions or comments up
at this point please feel free to leave
them below otherwise as always thanks
for watching thanks for all the support
of subscriptions and until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>