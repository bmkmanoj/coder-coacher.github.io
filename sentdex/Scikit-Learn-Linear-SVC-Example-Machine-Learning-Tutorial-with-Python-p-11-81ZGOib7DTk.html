<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scikit Learn Linear SVC Example Machine Learning Tutorial with Python p. 11 | Coder Coacher - Coaching Coders</title><meta content="Scikit Learn Linear SVC Example Machine Learning Tutorial with Python p. 11 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scikit Learn Linear SVC Example Machine Learning Tutorial with Python p. 11</b></h2><h5 class="post__date">2015-01-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/81ZGOib7DTk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody and welcome to the 11th
machine learning scikit-learn or SK
learn tutorial video with investing and
all that some of you may be actually
joining us right on this video the
previous videos and the rest of this
playlists are by using machine learning
with psyche learn and linear SVM for
investing using fundamental investing
character or fundamental characteristics
of companies as features but I imagine
some people are mostly just interested
in seeing a very basic linear s SVC
rather example so with that let's go
ahead and jump in we're going to be
using the following things we're going
to import numpy as NP we're going to be
using numpy for basically converting to
an umpire array I believe that with the
only use of numpy actually we have a
couple others because numpy arrays have
some characteristics basically that we
wish to utilize moving right along we're
also going to import matplotlib mm dot
pie plots as PLT we're going to do from
SK learn and we're going to import SVM
for a support vector machine and then
we're also going to go from matplotlib
import style style and then we're going
to do a style die use and we're going to
be using ggplot and this is just to make
the graph look a little nicer than usual
that's that so let's consider the
following let's say we've got you know
machine learning example we have two
features that we want to consider sorry
about that my dog is shaking so we have
two features let's say you've got exes
and you have Y's and in that you've got
let's say your X values and really x and
y are separate features you're going to
see that later on we actually combine x
and y to be an uppercase X is what
you'll normally see and usually that
contains all features so the X parameter
is going to contain let's say a 1 5 a
1.5
eight one and A nine
and though the Y is going to let's say
contain to a one point eight eight zero
point six and eleven so real quick let's
do plot dot scatter and then PLT show
and we'll see our scatter plot real
quick oops um Hilti duh okay scatter X
of Y rather and here we have a scatter
plot and you know with our eyeballs we
can kind of see okay how might we
separate these things well we would do
probably a line you know a nice diagonal
line from here down to here something
like that that seems to make the most
sense but how do we do this you know
with an SVM specifically a linear SVC so
the first thing that we would do first
of all is we need to convert this into a
numpy array and generally the way that
you're gonna actually see this is you'll
have a list of features and then you
will have the answer for your label so
you would have something like this well
first let's make the the list so we've
got a list and it'll be a list of lists
and the first one would be basically
these values like you know the values
and their corresponding so the X and the
y right so we would have one two right
for the first value of x to the first
value of y so you'd have one two and
then we would have and let's go just
enter down and let's just make a few
real quick so we had one two then we had
a five eight then we had a one point
five one point eight then we had eight
eight so eight a then we had a 10.6 then
we had a nine woopsy so list there nine
eleven right so those are all our
coordinates and then what we would do is
we would say capital x equals that and
then we end up actually converting this
to an NP array so numpy array and that
converts this to numpy right and since
we kind of built this a little different
we'll just have those over so now we're
going to do we're going to label these
and so the way that we would do that is
we would say y equals and we can see we
could see with our eyeballs that those
you know when we scatter plotted these
things so like let's do it real quick we
could see that we would want to group
these and we would want to group these
well what is the fundamental
characteristic of these two things well
this one has lower numbers this one has
higher numbers simple enough so we close
this and we would we would label these
ourselves and you label the pair right
so one two let's say those are zeros
five eight would be a one one point five
one point eight we call with zero eight
eight we would call a one one in point
six zero nine and 11 a one so now we've
got our X and our Y and what we want to
do next is we're going to create our
classifier so we're going to say CLF for
classifier equals SVM for support vector
machine dot SVC and then we're going to
specify our kernel as being linear so
linear SVC and then we specify C now C
is like this kind of a I hate to say
magical number but it might as well be
there's a lot of debate and study around
machine learning and there's a lot of
unknown and really best kind of called
experimentation around machine learning
especially support vector machines and
all that so where I'm not going to touch
too much on C for now and if you want to
learn more about C kind of follow along
this series and you'll see how bad my
jokes are anyways moving right along
we've got C we'll just say is equal to
one point zero you actually don't have
to define C at all that is the the
default parameter so it doesn't really
matter now we're going to go see LF or
classifier and we're going to do dot fit
and we fit your you fit features to
their labels so why was the labels and
fit was the features now we have two
features here right but you can have
really an unlimited amount of features
so what we
I'm doing or what we're doing in this
this series is we have probably
something like thirty features so anyway
you end up with quite a few then we're
going to go ahead and because we're
going to from here we can actually do
classifier dot fit XY and then we could
do something very simple we could say
once we've done that we're ready to
predict so we could do something as
simple as the CLF dot predict and you
could predict now a set of features so
what if we predicted a point five eight
zero point seven six our goal is that
this will return a zero because it's a
lower number so let's go ahead and print
out that we'll save and run that we're
still graphing another thing anyway this
was the prediction indeed a zero very
nice uh what if we did ten and ten right
so ten point five eight ten point seven
six let's go ahead and run that and see
what the prediction is and it's
predicting it as a one so it's that
simple
once you've converted everything into
numbers anyways again if you want to
learn how to how to do that definitely
check out this series as well as
normalizing the data and actually
scaling the data that's pretty important
turns out there's a really easy way to
scale it at least but as you'll find if
you are following along in this series
actually just acquiring the data is kind
of the biggest task so moving on now
let's say you want to graph the data
just because you want to UM so the way
that you would do this is you want to
get the coefficient and what this is is
like the you know what what happens is
you've got this hyperplane you know
vector coordinates and this is pretty
much what's going to determine our
classes right because at some point you
have to draw the line literally and
figuratively and this is usually
referred to as W so we're going to say W
equals CL f dot coefficient um
underscore and then we'll say zero here
and then what we're going to go ahead
and do is like zero this gives us a few
values so let's go ahead and just print
W really quick I'm going to go ahead and
delete this above code
the original code that we had so we
don't have to keep showing that and so
here's your coefficients okay so these
are what's going to be used to create
our line here in a second
so now I'm going to just kind of create
some space here so after defining W
we're going to define a which is you
know much less usually referred to as a
learning rate and all that but we're
actually just simply using this for a
line creation but a is going to equal
Delta negative W zero divided by W one
that's just a simple you know textbook
algorithm for creating this then we're
going to say X X this is the X of our
line basically equals NP linspace and
here we just make it a range of you know
of our data so typically you could say
you know the max and min is literally
the max and min of our features we could
say something like that but we already
know the maximum would be zero or I mean
the minimum would be zero and then the
maximum we would say is really like 11
right but we'll put 12 so zero to 12
will say and then for y Y we can have a
simple algorithm as a times X X minus
CLF intercept underscore zero divided by
W one again a massive most of what you
will do in machine learning will not
involve any of this I'm just showing you
so you can create a graph people like to
see graphs but generally you're going to
have too many features to graph so
you'll never see a graph or you rarely
see graphs in machine learning
especially with support vector machines
unless you have three three or fewer
features anyway
that's that and then so now we can
actually plot this line so we can call
this H 0 and that is PLT dot plot we
plot x XY Y we plot it as a black line
so we use K for black and then a dash
for line and then label and this is 9
weighted divide let's call it div
and um we'll leave weights aside for now
but there's also a way to create weights
and you use those to even better
classify your data some data sets are
going to require that you you weight
things
so anyways now we're ready to go ahead
and plot again so so now what you would
typically see here is this fancy numpy
code here so you would get PLT dot
scatter and then we can do X cups X
colon comma 0 with uh-huh and then so
that'll be the the first element or the
first element rather not the first if
and then you we can have the X colon
comma burst if elements of the second
element right so these are the zeros so
the first numbers and then the ones are
these ok so that's what we're scattering
and then we can do some more trickery
dnesday see for color equals y and that
will allow us to color the plots
differently according to their group and
then we'll go ahead and just do a BLT -
show and in fact since we gave a label
to our line there we do PLT that legend
save and run that and now you can see we
have a nice little divider here it looks
like we're covering over where's our
other dot ah so anyway here's our blue
class and here is our red class and here
is our fancy divider so that's that is
pretty simple as you guys saw I mean the
really the the major part of the support
vector machine is basically none of this
and you basically that so we'll comment
all that out and this is really the the
meat of our SVM right our support vector
machine specifically linear SBC really
not much to it right I mean this is
pretty simplistic code there's a lot of
customization there's a lot of little
things that we could change add and
remove things like see a lot of this is
actually experimental and I would even
go so far as the same machine learning
is almost like an art form and it isn't
just the computer even even if
you have something that's unsupervised
there is a lot of experimentation and
the scientist actually performing
everything needs to understand a lot
about how machine learning actually
works so it can't all be the computer
but this is a pretty simplistic example
of how easy it can be the only thing the
only major thing I just want to bring up
real quick before we you know cut this
off is this is generally not best
practice you want to scale your data and
generally people like to scale their
data between negative 1 and 1 some
people do if there's like a really high
degree of variance you could scale it
between negative 5 and 5 or something
like that it's okay but generally you
want to scale all your data to kind of
normalize it that gives you better
results we haven't done that here but we
will be doing that in this tutorial with
our exact actual investing example but I
wanted to keep things as simple as
possible when we're first running
through linear SVC just so everyone kind
of understands the main objectives of
gathering data and feeding it through
linear SVC or a support vector machine
so anyways that's it for this video if
you guys have any questions or comments
please feel free to leave them below
otherwise as always thanks for watching
thanks for all the support on
subscriptions and the donations and
until next time
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>