<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TensorFlow Basics - Deep Learning with Neural Networks p. 2 | Coder Coacher - Coaching Coders</title><meta content="TensorFlow Basics - Deep Learning with Neural Networks p. 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TensorFlow Basics - Deep Learning with Neural Networks p. 2</b></h2><h5 class="post__date">2016-07-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pnSBZ6TEVjY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to
part two of our deep learning with
neural networks tensorflow and of course
python tutorial series at this point I'm
expecting that everyone has tensorflow
installed if you're on Mac or Linux your
installation of tensorflow is actually
super simple you just need to go to
tensorflow dot org and when you get
there you'll go to get started dude
there we go come on uh and then you'll
go down to the PIP installation right
you just run these very simple things
and these very simple things and this
very simple thing and you're done ok if
you're on Windows it's not so simple so
if that's you if you're on Windows I
have an optional tutorial for installing
tensorflow via bun to via VirtualBox so
if you need that check that out I'll put
a link to it in the description that's
what I've done this is a virtual machine
I'm not really running on a boot ooh I
just so happen to want to do this
because I have some nice graphics cards
or GPUs I suppose if you want to call
them anyway I've got some nice ones I
wanted to use my main computer I did not
want to pay for a cloud server that
would have as much power as this so I
figured I'd might as well get used to
that anyway so it's assuming you have
tensorflow already installed also I've
got H top over here if you're not
familiar you can just do sudo apt-get
install H top and it just gives you a
much more better visual representation
than top does of like here these are my
my CPUs this is my memory my ram swap if
I use it god forbid huh and then the
processes and stuff like if I wanna kill
process I'm going to be writing the code
over here in sublime text use whatever
the heck editor you want I'd be using
idle like I normally do but I don't have
it on here I don't know how to get it
and I don't really care that much to
worry about that right now and then over
here is where I'm just going to run the
actual code and that's that so
tensorflow does have I feel like I'd be
doing you a disservice if I don't tell
you but for the most part you write a
tensorflow program and then you run it
okay it's part of what makes tensorflow
as if
efficient as it is is you set up your
code kind of in the background it's like
abstract and then when it comes time to
run it it runs like pretty much
everything kind of in a chunk not like
Python like Python is a very kind of
slow compared to like really efficient
C++ because it's kind of this is like
line by line nonsense and so so what
happens in tensorflow is it's going to
take all your stuff and then go in the
background and do some things and then
it's going to come back with your
answers okay uh so you're pretty much
always going to be writing the whole
thing and outputting the end results
okay or at least in very large chunk
she'll be doing this but tensorflow does
have what's called the interactive
session and that allows you to
interactively play around with
tensorflow
in your session which will make a little
more sense down the line but i probably
won't talk about it again but you can do
that if you want like if you're in ER it
used to doing like interactive shell
like a ipython kind of stuff then you
can by all means still use ipython just
use the interactive session for like
playing around but in reality when you
actually would go to actually run this
thing you would not do it in ipython or
in the interactive session so first of
all what the heck is tensorflow like
what is it actually doing for us so
libraries like tensorflow or Theano and
stuff like that what they are at the
inner core is just matrix manipulation
libraries or even better put in like
Python terms array manipulation or not
even Python terms like sea terms I go
though a list anyway array manipulation
libraries that's what they do like so
you might ask like what's a tensor okay
a tensor is just your array like objects
oh it's just an array okay it might have
a bunch of values it might be one by
eight eight by one five thousand by six
million it could be any kind of size
array it could be a one by one okay it
can be anything you want
that's a tensor so all tensor flow is
just functions on tensors so if you can
take any kind of problem you have and
convert it to this function on a tensor
which is a function on an array if you
can do that you can do it in tensor flow
it just so happens that deep
learning is pretty much the main area
that this sort of thing is being used
because that's like the only place that
really needs to be able to do let's say
optimization on you know 600 million
variables or something or batches okay
but anyways the other thing that's kind
of cool about tensorflow is kind of if
you look into it generally it's it's
it's called a library for machine
learning or for deep learning but really
tensorflow if you really start looking
into it I would say is actually a deep
learning library because it has just
tons of deep learning functions that you
would need pre-built for you that make
it a non numpy like thing but at least
at its foundation it's more like numpy
than it is like scikit-learn it's not at
like psyche alone is so very high level
but it's good I would say it's pretty
close to scikit-learn because it's it
has a lot of helper functions but
anyways you'll see as we go on so the
first thing that we kind of have to talk
about is the methodology so python is
just inherently kind of a slow language
C++ for example is a fast language
python slow because it's pretty much
read line by line so as you start making
doing operations a lot there's a lot of
like sending and receiving and
transporting of information that goes on
and that's very inefficient especially
if you want to do processing on say your
GPU or something like that
so what tensorflow does is first you
define your your model and kind of
abstract terms and this is this is where
you're you're building your computation
graph okay
so you define your model and kind of
abstract terms and then when you're
ready you run the session okay and that
runs the graph and everything is done in
the graph everything's done in the
backend and everything's like just
happening then you get your result back
and then you're back kind of in the
foreground and you can start to do
things again so let's just kind of go
through some of the basics and I think
that'll make a lot more sense in then
when we actually build our neural
network I'll point out at least one
thing that will really drive this point
home so anyways the first thing we're
going to go ahead and do is import
hopefully you can see that well I don't
know if I can make that much bigger
we'll go with that for now
move this over imports a tensor flow as
TF and like I said the first thing you
do is you construct the graph so we'll
have X 1 and X 2 these will just be
variables okay when I these aren't even
going to be variables I misspoke they're
going to be constants and so they're
going to be T f dot constant and then a
value will say 5 and then X 2 same thing
it'll be a TF constant and a value will
say 6 so in this case in the graph it's
a constant it won't change so yeah it's
a 5 and a 6 ok but these can also be
variables and you can have like these
placeholders you'll see a lot and all
that kind of stuff but this is meant to
be just a really simple example that I
can do really quickly so there's that
then you could say the results equals
and you could do something as simple as
x1 times x2 and I'm not 100% certain
that's as efficient as what I'm about to
show you this will work but it's not as
efficient as the official way of doing
this so anyway just note that you can
get away with that apparently I've tried
it but anyway
TF dot mole for multiplication x1 x2 so
you'll notice that these are actually
just values they're not even really
arrays they're just like single scalar
values and we're still getting away with
that but in most cases it's going to be
some sort of array so you'll probably
have something like this
whoops like that and then rather than
just a simple mole it's a matte mole and
that's probably the most common mat mole
ad that you're going to see as we go
through anyways we'll just stick with
straight up multiplication ok so that's
your results now what happens if we
print that results so save this we'll
come over here I am NOT where I need to
be desktop what I call is TF Tut's or
something oh my gosh what did I call
this thing Oh a capital T nice all right
so Python 3 don't forget that 3 and then
TF basic stop I will run that and we can
see the result
ten sir at the moment it's an abstract
tensor it doesn't actually have any
value because nothing's actually run
like within normal Python if we ran this
code this would be executed right
especially if it was the original x1
times x2 and in fact let me just we'll
kind of go back and forth just because I
think it's interesting and I wonder if
anybody has a real answer if this is
valid like if this operation is
identical to this operation so if
anybody has that answer I don't really
know I just know the first time I was
playing around with it I accidentally
did that and I was like hey it still
works because at the end of the day it
still creates this abstract tenser so I
don't know but this is the official way
to do it so that's the way I'm going to
mainly do it what but I wonder ok so
anyway so you print your soul the result
turns out it's just a tensor it's an
abstract tensor in our computation graph
ok now to actually see the result you
run it in the session so to get a
session I'm just going to make some
space here this is probably a little big
to run the session you just do a there's
a few things you can do a lot of times
you might see something like this likes
s or for session equals TF dot capital s
session ok that begins the session or
actually it well sort of begins the
session but gives us mostly or such
variable and then what we can do is we
can say we can actually run this session
so we can say s dot run and then results
ok so we say that whoops oh sorry oops
in there anyway so that run that ok and
first you get the abstract tensor which
was this result that we printed and then
after that you actually get the the 30
and just for my curiosity I probably
detest this before but I just want to
know and I'm just really curious I'm
yeah yeah maybe someone that my son has
the answer to that one I don't know
anyway it seems to be the same results
but I wonder what like a speed test
would give us the same answers I think
you can get away with that I think you
can do like a few other things but
anyway I think you're supposed to use
the TF um methods but anyway so that's
one option right that run that starts
the session and actually runs the graph
so no computation actually took like
happened like this just kind of
to find a model that would multiply 5 &amp;amp;
6 but no process actually ran to run 5
and multiply 5 &amp;amp; 6 until we ran the
session ok nothing happened so so that's
that the other thing which we can do
probably should have done it but uh-huh
when you're done with the session you
want to do is set stuck close look at
that wasn't very much information I
think we'll be fine but we'll just run
it okay and it's just like a file object
right you you open it you need to close
it ok and really every connection object
ever now luckily what you can do is you
can you can do the following so rather
than doing this this is probably what
you should always use so with TF dot
session whoops don't forget catalyst
with TF session as it goes SS Wow cess
there we go now what you could do is
print a session results and this is much
like you know like with open and stuff
like that it will just automatically
close when it's done so you don't have
to remember to close the session now a
couple things um one thing to note is
like let's say we said rather than
prints s dot run result we can say
output equals cess run results and then
print the output instead run that
nothing changes but then what if we for
example like what if we what do we try
to access output are we going to be able
to access output is output part of our
computation graph in our session or is
that like something else entirely
something else entirely right that's a
Python variable right so don't try not
you have to kind of like remember that
you've got your computation graph which
is like this abstract graph and then you
have the session which actually like
does stuff and can output stuff and it
can also modify tensors within that that
session and then within that graph and
then you can actually save that back to
like Python variables again and start
referencing them
so you can print the output but you
wouldn't be able to like like like you
wouldn't be able to print cess dot run
results or you want to be able to run up
like this like you wouldn't be able to
do this actually you would because you
solve you know it you'd be out of sight
outside the session so once you're
outside the session right you would get
that because you attempted to access
that closed session okay
anyway um just kind of keep these things
in mind like the separation and stuff
like that but mainly the big thing to
take away here is that you you have like
this this computation graph where you
kind of model everything so the number
of nodes in our network in the number of
layers and the starting values and stuff
like that is going to be built into our
our computation graph and then we when
we run the session we will run that
session with an optimizer that will then
go through and start managing all those
weights and tinkering with them and
taking the outputs and stuff like that
based on whatever cost function that we
built into orbit or our computation
graph and all of that that's all going
to run and be modifying the weights and
such for us so the point where we're not
even going to code that we don't have to
code the logic that's going to go
through and modify those weights it's
just going to happen we just have to
tell tensorflow like this is what we
want you to do we want you to minimize
this cost function and stuff like that
but as far as like everything else
that's involved tensorflow pretty much
takes over from there which is I think
crazy anyway if you have questions
comments whatever with this code this
hopefully is basic enough but if you
have any questions or whatever feel free
to leave them below otherwise in the
next tutorial we're actually going to
build a neural network will probably
model it in the next tutorial and then
we'll do this session because it
basically when you build something in
tensorflow you it's kind of in two major
parts right build the computation graph
build what's supposed to happen in the
session and that's it like it's these
two major chunks so in the next tutorial
we will
build the computation graph and kind of
model the network and then probably
after that we'll actually run it and all
that so questions comments concerns
suggestions whatever fillion below
otherwise as always thanks for watching
thanks for all the support subscriptions
and until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>