<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Streaming from Reddit - Python Reddit API Wrapper (PRAW) tutorial p.3 | Coder Coacher - Coaching Coders</title><meta content="Streaming from Reddit - Python Reddit API Wrapper (PRAW) tutorial p.3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Streaming from Reddit - Python Reddit API Wrapper (PRAW) tutorial p.3</b></h2><h5 class="post__date">2017-08-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XmOA1k4RTrc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to
part three of the Python reddit API
wrapper or project Oriole series in this
video we return amount is streaming from
reddit so at this point we basically
just done everything kind of
historically as if you were viewing
reddit but as you may know sometimes
when you post stuff on reddit you'll get
like an immediate reply back from a bot
or or even like some of the like the
auto mods and stuff like that but also
you might want to maybe you're wanting
to keep a database up-to-date or maybe
you're trying to set up like an alert
for something who knows there's all
kinds of reasons why you might actually
want to stream reddit rather than
looking back historically also streaming
for a lot of applications is going to be
less like API call intensive than it
would be to you know keep making API
calls constantly so um so let's go ahead
and cover how to actually stream it's
actually super simple you just add a dot
stream in front of everything so for
example what we could say is for comment
so in this case we're going to still
continue with subreddit but we're just
starting some new new text so for
comment in subreddit dot stream dot
comments now you can also do dot stream
on other things if I forget to mention
it you could do it on a specific even
submission if you wanted anyway the
comment in subreddit stream comments
what do we want to do let's go ahead and
we can say print actually let's do try
to set how I want to do this let's say
we want to grab a parent ID and that'll
be the string version of comment dot
parent and then what we want to do is
for commenting subreddit that string
comments so what we're going to say now
is we're going to say like basically
what I want to do is grab let's say like
we want to grab all the replies to
comments so we need a parent ID and
there might not be one because I
actually think top-level comments don't
have a parent ID that is thread
so I might have been in correcting my
statements before but anyways let's do
it let's just in case this in a try
accept exception me and we're going to
pass people are gonna be so mad now so
that's a parent ID now let's say let's
say like the submission I hate to use
the word submission so that's like a
thread original equals reddit.com ents
this will let us search for a specific
comment by ID so we can search for it by
the parent ID so cool so we've got the
original comment and then what we're
going to say is so we can just print it
out so it's just different parent brand
original and actually original again
would be a a pro object so we need to do
dot body on that and then print reply
and reply would be comment that body and
we can even we could be a little better
actually rather than accept exception is
e let's do draw dot exceptions dot draw
exception as e well we're a little
better but not much so so now we can go
ahead and stream on these comments I
think that's probably good enough we
might have to change the Python
subreddit but we'll see
so some of these probably had no parents
is my guess did we print parent before
hmm it's weird that we would get to
parent it would be totally empty because
you wouldn't have gotten an ID I
wouldn't think or are they really empty
now it's just jet it takes time to
generate me no those are empty it's kind
of odd I'm not quite positive why a
parent would be empty like that anyway
we're getting parent here shouldn't be
hard for you to provide 40 character
string
although blah blah blah and then you've
got your reply I doubt you'd be able to
crack it
even with an EC Tunes okay how big of an
easy two incidents bro welcome to Reddit
so okay so that's how you could stream
in comments now one thing I'll bring up
that I didn't realize out of the gate
was or to a couple things first of all
don't forget that every time basically
you've got this function call that's
another you know strike against your ear
your total query limit basically your
API call limit which is a extremely
unfortunate dirty API calls per minute
mu that sucks
now initially you might think it's
actually much larger than 30 especially
if you like use it a little bit stop and
then use it a little bit again it's
almost like like I don't know maybe they
average it over the course of an hour or
something like that but initially you'll
get a huge spike and then it slowly will
level out to an average I'm just knocked
over to copy an average of like 30
requests a minute so and I forget where
that is that's somewhere in the docs
apparently I had to google it to find
that limit but there is that limit which
honestly is pretty low especially when
you don't provide the parent ID I can
understand why you wouldn't want to
supply the parent content or something
like that but seriously if it has a
parent ID give it to me so that's kind
of that's kind of a bummer like all
requests are equal
so hey not happy about that but anyways
so this here is an API call this here is
an API call because your extractant you
know you've got to go find out that
parent ID this should not be an API call
but it is it's a feei call and then the
stream
I don't know how frequently it sends you
new data but it's some somewhat
frequently but what we can do just to
kind of show you this though I'm going
to go ahead and just comment this out
that's that's where I hit all three but
apparently I didn't and even this in
fact I'm just going to delete all this
so like comment that body this we want
print Conklin body it's not a function
call
try again bro okay so this is just a
stream comment so anyway if I let it go
just as fast as it wants okay it's
actually kind of stopped um that's
probably because the Python subreddits
kind of slow-moving sub right so let's
change this to something faster news
striking this I expect up know what this
is news to me oh man super tempted to
just pass entirely let's go to politics
that's kind of error I don't want to
debug right now sorry guys
let's try again maybe cuz I'm what no
this is nice to me
I've string this without any problems
the hell's going on I'm going to
I do the worst thing you can do me see
if that still does it like this is like
an idle issue no I don't know anyway if
someone has a solution to that head
error let me know I was disgusting
anyway okay encase it in full exception
just to let this tutorial continue on
its roll
anyway this is streaming contents from
now politics if I went back the news and
printer news is a much larger subreddit
than politics
let me check real quick poof so 22,000
people in news right now politics has
stromal please Wow 33,000 it's actually
bigger than news is interesting anyway
um okay so that's streaming all the
contents as they come in it has a stream
in like a cluster so it looks like maybe
it's just like one update every few
seconds but if you're paying a full
attention it's actually a cluster of
like a bunch like I don't even know how
many this is but like let's see okay
this ends with like these Wow
thanks for helping me prove my point
guys okay that one bolt up pretty high
okay socially not too many when I'm
screening this in the past like it
seemed like it was coming in as like 20
at a time and now it's kind of early in
the morning so maybe this is not like
the time for Reddit come back around
like 6:00 p.m. Eastern or something and
read it might even be down so anyway
closing out so not only can you do like
a stream of comments you can also stream
an actual just the submission so for
example we could say subreddit dot
stream and then we can do submissions
this one might be a little slower but
less it's not going to be comment well I
guess it would be now we'd have to do
title so so we're just hitting a bunch
of exceptions if we're passing them
silently because we are good programmers
right if a for
in subreddit s transmission through
submission dot title let's at least sort
of fix our problem there save run surely
whoa
so when you go to do the stream it'll
like kind of populate back for you a
little bit and then it goes now I'm
hoping to see an update here surely
somebody's going to submit something to
politics you guys could do it I guess
not
anyway just know it's flexibility so I
think that's it for the subreddit or
rather the reddit API wrapper if you
have any questions comments concerns on
it feel free to let me know otherwise
what I plan to do with this is I wanted
to make a chatbot now I was going to
stream everything from reddit to a
database but reddit did not appear to be
a fan of what I was doing so I have to
come up with another means now I did
find that just in case just for sake
clarity the the project Oriole is over
but if you are interested in pulling a
doorknob data from reddit and you don't
want to make reddit mad you can say
let's say it's 1.7 billion reddit
comments or something like that
he just Google that so here we go so
there's this post that was made two
years ago this guy pulled her on we did
it without making reddit angry okay
somehow so it's every publicly available
reddit comment for research although I
don't know who this this person is stuck
in a matrix me all right this might be
someone very someone affiliated with
with uh look at that karma so I'm going
to fill you with reddit I don't know
someone someone can feel free to educate
me anyway somehow they have all the
reddit comments so anyways at this point
it was up to 2015 so it's a 1.7 billion
comments everything now if you scroll on
down on
here someone also posts that they have
it all on bigquery so Google's bigquery
you can click on this comment here boom
someone is telling us hey they loaded
everything onto bigquery now if we go to
bigquery Wave root I'm just going to
check to see if we got any updates you
can see starting at 2005 we've got a
table but it actually goes away to 2017
June 2017 so I feel like I'm under the
impression that this is going to keep
updating maybe it'll be like two two
months lagged I'm not really sure but
anyway so it's even more data and reddit
has only grown like reddit is one of the
top ten websites I mean it sometimes it
comes in and out of the top ten but I
mean it it's a huge website so as its
growing I mean like probably 27 2016
maybe 2015 probably has just as much
data as 2005 to at least point 10
probably does anyways all the natives
here on bigquery
you can also what I was showing you
initially was you can download the
torrent of 2005 to 2015
actually I believe it's not even it's
not 2005 I think it goes back to 2007 I
might be wrong somewhere now this was
loaded onto bigquery if I go back one
more time
yet here and he says that it is
obviously it's one point seven billion
comments and it's like one terabyte or
two terabytes somewhere in here I swear
he says it's only a one terabyte but
then he says here you wanted to get a
digital ocean box with two terabytes of
bandwidth oh I guess so people could
download it yeah it's like 150 gigabytes
somewhere here it has the extracted size
I think it's a terabyte anyway but yeah
you can torrent that grab that and just
have a bowl or grab just a month of that
data if you wanted
anyways just throwing that out there for
anybody's interested and so you don't
run in the same problems as I run into
so I think that's it
questions commenting below low weight so
the next tutorial</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>