<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multiprocessing spider example - Intermediate Python Programming p.12 | Coder Coacher - Coaching Coders</title><meta content="Multiprocessing spider example - Intermediate Python Programming p.12 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multiprocessing spider example - Intermediate Python Programming p.12</b></h2><h5 class="post__date">2016-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N0ph2a6Vd7M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on welcome to part 12 of
our intermediate Python programming
tutorial series in this video we're
going to be using multiprocessing along
with beautifulsoup as an example of when
multi-processing could be advantageous
so if you don't have beautifulsoup
one thing you can do is come to Python
programming net you can type in
beautifulsoup
enter go here and come to this tutorial
and either you can just read this pip
install beautiful suit for pip install
XML make sure you have those two things
if you need to know some of the basics
of beautifulsoup
you can go through it's a pretty short
series there's only four tutorials and
really you could probably be fine just
one in two would be good enough anyway
make sure you have beautiful soup and L
XML so or a parser that you want to use
in place of L XML if you want so what
we're going to do is we're going to run
through a really quick and that's so
quick it'll take us a little bit but I'm
hoping to just do it all in one video an
example of using multi-processing with
beautiful suit for let's do it
from multi-processing we're going to
import pool we're going to import BS for
SBS we're going to import random we're
going to import requests and we're going
to import string random in strings so we
can start with a random string I'll show
you that very quickly requests so we can
actually make the request on to get the
source code bs4 to read the source code
and multi-processing pool to do the
multi processing so we're going to
define a random starting URL what this
is going to do is return to us three
random characters and we're going to say
those three random characters plus a
comm or really HTTP plus the three
random characters plus a comm basically
is going to be our starting URL the URL
zero so to speak where we're going to
begin the parsing so just a random
three-letter com domain we're assuming
that they're going to have something
there a lot of them are probably
heart but we're going to go ahead well
we'll have some handling just in case
there isn't anything there so let's go
ahead and do that so we're just say
starting equals and then we're going to
do empty quotes dot join random dot
system random for a nice true random or
as cuz it's going to get not true but
choice and we're going to choose string
dot ASCII lowercase underscore lower
case for underscore in range three
pretty long line pretty sure that's
going to violate the seventy nine
characters but we don't care we're going
to hold ourselves to that arbitrary
thing moving along that's our starting
our starting three characters so
basically it's going to choose some
random character some random lowercase
characters for the range of three so
three random lowercase characters then
we're going to say the URL itself is
going to be this join and we're going to
join a should you be colon slash slash
the starting and calm and we'll let our
spaces there will be Pepe there now
whoops now we just returned that URL so
that's just going to be some random URL
we can do URL e
URL grant well there we go our starting
URL CDW com all right so that's how we
can get random URLs now one thing that
might happen as you poke but basically
what we're going to do I suppose we
should explain exactly what we're going
to do so we're going we're going to
create a spider here and the main task
of a spider
a spider might have multiple subtasks
but the main task of a spider is to go
to a website find all of the links on
that website and then go to all of those
links right and just slowly spiderweb
out on to the entire interwebs okay
spider
now while you're on those websites maybe
you're going to do some stuff but we're
not going to code that part we're just
simply going to code getting a ton of
URLs and actually like crawling crawling
around so anyways
random starting URL we got that now what
we're going to say is define handle
local links so many times you're going
to find that on a website they they use
local links so you might have a href
that is like this like and that's that's
going to the contact page of your
website which is you know in our case
with CDW that was going to be CDW that
corresponds a CDW com slash contact but
the actual reference is just to this
right and your browser just knows what
that means but our when we go to make a
request via requests to pull data and we
feed that as the URL we're going to get
an error so we need to handle for those
local links so we're going to say if
link dot starts with a forward slash
then we're going to just return join and
we're going to just join the original
URL to the new link else we return the
link simple enough let me make some
space now what we're going to do is
we're going to define the really the
crux of this and that's going to be get
links and we're going to get those links
from some URL and then we're going to
have some tries and we're going to have
some exceptions except we'll come to the
accepts let's talk about the try first
so first the response is just going to
be requests get whatever the URL is then
we're going to say soup equals BSD a
youthful soup the response text value
and our parser zel xml then we're going
to say body equals soup top body that's
just the body information basically so
we're going to hope
avoid nav nav bars but we probably won't
a lot of times a nav is part of the body
some websites even have multiple bodies
I'm I'm not looking at my own source
code but I'm peeping net might even have
it or Python program net PP net and the
cool kids anyway um blinks equals and
here is where we could have a really
long line and I'm just going to break it
up so we don't but we could but we're
gonna have basically three links equals
statements here but anyways link get and
we're looking for the href so link I get
href or link in body dot find all tags
that are a tags so that's all of our
links but of course we need to do a
couple of things we need to handle local
links and possibly encode them but
anyway now we're going to say is links
equals handle let me just copy this
handle local links URL link a for link
in links yes good I was blanking on
where we're where we were actually
acquiring URL but I'm like okay ask um
okay now Oh finally I suppose just in
case we'll add in a quick encode so
we're going to encode string link dot
encode ASCII or link in links hopefully
that's good enough if so we'll return
the links so what this is going to do is
we're going to turn a list of links that
we find on any given URL and we will
handle four local links by appending or
adding or joining the original starting
link to the kind of local link great now
what happens when things go wrong so
first what might go wrong
one issue is maybe there are no links on
the page that's a problem so if that
happens this will this will just be
basically we'll be searching for a body
to find all and that will be none there
will be nothing there so it's going to
be a nun so we're going to say except a
type error as E and we're just going to
print E and then we'll just print a
message to ourselves that's like got a
type error probably got a nun that we
tried to iterate over good enough if
that happens we just return an empty
list and that's acceptable to return an
empty list for our program now we're
going to say except maybe index error as
E and we're just going to say print E
and again we're here is basically we
probably didn't loops did not find any
useful links returning empty lists turn
50 list makes more space here some other
errors that we might have is who accepts
attribute error as e3e and then again an
attribute error might be if we don't try
to think here cuz we got none that we
this would be if we try to iterate over
none this would be if we are attempting
to iterate through the links I would it
get us and then here attribute error in
this case I'm pretty sure this would
also be an error if we threw basically
no links anyway there's a lot of
handling for if we don't if we don't get
links or we get bad links or strange
links but anyways we'll just say likely
got
for links so we're we throwing this okay
and if all of those fail you probably
ception
as he have like a general exception this
will catch all other exceptions and you
probably should print string or or even
just let this exception hit like not
have handling for that exception because
here we've got okay we understand that
these might be errors that might happen
there's no list we've got we're trying
to do something with none and we can't
do something with none here we're trying
to assign an attribute and we don't have
that so here you might not want to have
this exception but there still might be
an exception like maybe a connection
error so if we take a while in URL or
actually we're using a request but if we
take a while with requests to make the
connection we might actually throw an
error there so I'm going to leave this
area here but just know that in most
cases you probably wouldn't want to but
in this case we have like an outside
environment that we can't predict and
we're making the active choice here that
since we can't predict that outside
environment we've done our best to
predict basically these things if
something else happens that we just
didn't predict will still print the
string e we're still going to return an
empty list which we forgot to do here
and then you know maybe do some log log
this error right or something like that
save it to a file or something that you
can look at don't let this just totally
pass without any sort of notification to
you okay good enough now let's do the
main so we're going to define main we'll
say how many basically this will be how
many how many links do we want to kind
of start with let's just say well do 50
P will equal pool and we'll just say
processes equals how many so we'll have
50 processes
then what we're going to say is parse
underscore us this will be a list of
links that we want to parse that will be
random starting your URL for blank in
range of how many however many links we
actually want to have now we're going to
say data is equal to P dot map get the
links link for link in parse us so what
we're doing here is we're saying parse
us this is going to be a list of URLs
basically a list of three character URLs
and then we're coming down here from
basically this will be the data that we
basically the same as what we did in the
previous tutorial where we're the you
know the return was like a list of 0 2 4
6 8 and so on this time it's a mapping
and it's going to be lists of URLs that
we get for all of the links in parse us
you might be able to guess already that
what we're going to do is parse us this
starting value is a list of URLs what
are we getting back from here with get
links a list of URLs
thus this could be a process that just
recursion is a list of lists so what
we're going to say is now data is going
to be equal to URL or URL list in data
for URL in geo papel list how's that for
list comprehension what is that doing so
for every URL in each of the mini URL
lists and then for each of the mini URL
lists in all of the URLs that we have
which is data we're saying
what we want to do is now have a new
list which is just those contents so
this is a way of taking a list of lists
and taking the data from each of those
lists in the lists and putting into a
single list fantastic okay so now we've
got a single list we're happy P close
and then what might you do at this point
well data would in theory now become
your new parse us so maybe you would do
a while true here maybe you would tab
this over maybe you would say parse us
equals data of course we've got a slight
multiplication and data there but it's
okay and the process goes forever but
we're not going to do that we're just
going to run it one time but that's how
you would do this forever and then want
to say with open and let's just say URLs
dot txt with the intention to write as F
F dot write these string data and
finally if Li equals
may we run main great now again as we
have learned in the past if we run this
via idle we just won't see any output so
including in the output that we would
not see is all of these errors so let's
go ahead and see just how many errors we
do run across so we'll open the command
window and let's run this again I'm
going to type the full path to Python
because I have multiple versions of
Python you probably just need to type
Python let's do it Oh invalid syntax
don't get this what did we forget here
that looks legitimate to me link for
Lincoln parse us why is that invalid
syntax it is in line 49 data equals P
map really though why is that why is
that a syntax a come show me link for
link to Oh
inner I wonder why that thinks that's
the syntax error but anyway we are
missing a comma great let's try again
request is not defined B requests with
an s requests no matter if this is still
no okay try any there we go malwarebytes
is getting angry at me okay we've got
some errors here but nothing that we so
none type has no attributes start with
good thing we got none for the links so
probably starts with was the URL that we
thought or something we just didn't have
starts with there here's another one
that we just haven't handled yeah max
retries exceeded with the URL caused by
new connection error from the request
stop packages so yeah we got some
stranger error there so you could add
these errors these new errors that we
haven't been seen we can see that we're
like hmm okay yeah that's understandable
like we don't we don't care about that
error so we can let that error go away
or you know pass so in some cases you
want to basically only have exceptions
for things that you have purposely built
the exception for so you know silently
passing exceptions is kind of is one of
those things and pretty sure it's in the
Zen of Python you at least definitely
don't want to say this
pass all right that's not Zenith python
but probably like if you're running and
silently passing does include even if
you're saying print string EE in your
code but you're running it by a cron job
you're never seeing that so if you're
not going to see the print statements
make sure you do log the air regardless
let's open URLs text and sure enough
what we have in here is a ton of URLs
and a lot of these URLs are three-letter
URLs hoping to find one is not a
three-letter you yeah oh one of them
sent me to malwarebytes thanks to
malwarebytes so one of those URLs was
not a good one and I did see your
malwarebytes got a little angry with me
moment ago Oh unless I'm missing it all
these are the three-letter URLs I kind
of want to run it again just and just to
prove that eventually you will get off
of your original three-letter URLs yes I
think I also have like these links here
let's run it again and let's see if we
can get just curious and maybe we could
maybe I'll if this doesn't work this
time I'll pause it it's getting angry at
me again I wonder which which website is
not the website we want to go visit with
hey man whore bytes is not like that
that's you do you follow this tutorial
your own risk because we're just
visiting random websites
this is bigger URLs text well this one's
bigger this one's a at least um uh no
its kWh okay get me okay here here haha
we made it to Twitter let's zoom in
where are you Twitter
there we go Twitter we got Twitter we
got some LinkedIn here we got YouTube oh
man's game over this spider has
discovered the Internet yes P calm it's
a good three letter domain D Rd I'm sure
there's some more in here but anyway our
spider made it onto YouTube so we're
good there's some more malwarebytes we
really curious what website that is but
anyway there we go we got two network
solutions comm so we'll lesser-known
plays err calm poof that's got to be a
valuable domain name alright anyway
there you go a quick simple example of
applying multi-processing to beautiful
soup because parsing takes a lot of time
but in that case we just parsed 50
websites in a relatively short order and
got the links to all the stuff that we
needed and actually at one point in time
it's not that we just we didn't just
pull links I mean at one point we had
their entire website so you could do all
kinds of stuff there are certain legal
issues when it comes to storing people's
content especially if you go about
sharing that content on your own website
and so on so what necessarily suggest
that you crawl peoples websites and repo
stuff but anyway that was you know 50
websites we downloaded all of their
information like we have all of their
information here so that's that's
actually a pretty large task to be doing
and then of course we're pulling apart
that information grabbing just the links
throwing away the rest of the stuff so
that's actually a fairly tedious process
not really sure how long it would take
to do 50 websites
in inline with each other rather than
you know processing this in parallel but
probably not that fast especially since
a lot of the time you might just be
waiting for a connection to timeout
right so just one of those connections
timing out probably would have taken
that entire time anyway that's all for
this tutorial if you have questions
comments if you have improvements I
really just ran through this really fast
so there's probably some improvements
that we could make what I tried to
include some things like join and stuff
like that and of course list
comprehension so hopefully that was not
too tedious for anybody anyway questions
comments leave them below improvements
let us know in the next tutorial we're
going to be jumping into object on your
programming so stay tuned for that I
will see you in the next tutorial</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>