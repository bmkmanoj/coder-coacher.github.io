<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Preprocessing cont'd - Deep Learning with Neural Networks and TensorFlow part 6 | Coder Coacher - Coaching Coders</title><meta content="Preprocessing cont'd - Deep Learning with Neural Networks and TensorFlow part 6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Preprocessing cont'd - Deep Learning with Neural Networks and TensorFlow part 6</b></h2><h5 class="post__date">2016-08-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YFxVHD2TNII" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody welcome to
part 6 of our deep learning with neural
networks since Flo and python tutorial
series in last video where we left off
we kind of got familiar with our data
set familiar with our means how we're
going to go about actually passing this
and creating a pre-processing this data
to actually fit through a neural network
and now we're actually gonna write the
code that does that pre-processing step
for us so so far we've done our imports
and those were explained and we've
defined the limit Iser we've explained
how many lines and all that and now
we're ready to begin so the first step
that we have to do is we need to create
a lexicon so the way that we create this
lexicon again is at least at its very
most basic level we have we just it's
all of the words that we find in our
positive and negative dick sets so to
start we might say the lexicon equals
and we'll say just an empty set here and
then what we're going to say is with
actually let's say for for file in pause
neg that way don't have to write the
same code twice in a row and really this
should have been this price should just
be a list but I'm not whatever hey poor
file this is not important for that so
then we're just i with and then with
open whatever that file is with the
intention to read as f we're going to
say the contents contents of this are is
f dot read lines and then we're going to
say for L for like for line in contents
whoops
contents up to however many lines we
were going to read of to honing lines
we're gonna say all words equals word
underscore tokenize l and then we're
gonna say lexicon plus equals list of
all looks like we type ode sure did nice
autocomplete for the save their words
there we go alright so
um so let's just going to read our data
in and stuff it into a lexicon but of
course this lexicon currently contains
like copies of words and all that and
then the other question we have is do we
actually care about every single word or
do we care like about like specific
words or is there like some sort of
range or something and I'm going to kind
of put forth that there is a range that
we we actually care about so outside of
this for loop once we've populated
lexicon with every word that we've come
across the first thing we want to do is
we're going to limit eyes all of these
words um in fact we might actually might
be better off to like first do I'm gonna
go ahead and just delimit eyes them
first but um you'd probably want to like
you can chop away words first maybe I
don't know yeah because the logic that
we're going to use to get rid of words
you'd really want them limit eyes first
anyway we're going to say lexicon equals
and then we're going to do this one line
or for loop limit Iser limit eyes limit
Iser to limit eyes eye for eye in the
lexicon okay so this is just our way of
kind of stemming them into legitimate
words and then we're going to do word
counts w underscore or w w underscore
counts equals counter and then we're
going to apply that counter to our
lexicon and this just gives us a
dictionary like element so word counts
might look like this like you know the
and these gonna occur this many times
and then um you know and probably occurs
let's save twenty five thousand okay
something like that okay it's a long
dictionary like uh object thick it looks
like this okay and then what we're going
to say is we're going to say and in fact
let me comment out that redefines
everything okay so for W in word counts
what do we want to do well we're going
to say if and then we're gonna throw in
some hard-coded parameters here sorry if
1000 is greater than word counts um W so
what we'd like word counts on a word b
so b
word counts thee the value would be the
actual count like how many occurrences
did we have so word counts W it needs to
be less than a thousand and then we're
going to say greater than fifty okay you
can kind of tinker with these values but
I'll at least explain why I'm choosing
them here in a moment so first of all we
need L 2 L 2 L 2 is empty for now and
then L 2 dot append W and then before I
forget this function returns return L 2
okay
so this creates a lexicon l 2 is the
final lexicon but what's happening here
is basically we're saying like we don't
want super common words because super
common words are like the and of or
these kinds of words
um look okay so so we don't get the
words aren't going to be valuable so
those are just going to like blow up our
model more than we want right so so the
this lexicon is literally going to be
that input vector that input vector is
going to be the lexicon in length so
ideally we want the lexicon to be as
short as possible so that we can have a
decent sized model like in terms of like
three layers and like I don't know a
thousand or something nodes or cells in
that layer so we want to be able to make
lexicon as efficient as possible so
we're going to say like we don't want
super common words because we don't
think they're a value but you might you
might say I don't know like because the
thousands like 10% of the data set so
maybe in 5000 or something like that but
we'll go we'll stick with a thousand and
then we're going to say like the really
rare words we also don't really care
about 50 might be again too big of a
number for your tastes might be wiser to
use a smaller number but we're going to
stick with 50 and we're returning the
lexicon and we'll see how that goes but
again you can kind of tinker with this I
think as you will see soon enough this
does not matter much what's going to
matter is something else so once we've
got the lexicon we're ready to use this
lexicon and maybe a you know classify
feature sets so how might we go about
actually classifying feature sets
first we're going to have some sort of
function it's going to do that for us so
we're going to say define sample
underscore handling sample handling is
going to take a sample of some kind and
it's going to take the lexicon and it's
going to take a classification so what
is this classification supposed to be
and then we're going to say feature set
equals empty and then we're going to say
with open whatever the sample is with
the intention to read as f contents is
going to equal F dot read lines kind of
like we had before and then we're say
for L in contents up to how many lines
so like that so for each of the lines in
our lines we're going to say the current
words and this will be word tokenize and
we're going to tokenize L dot lower
I think we'll do a dot low right here -
I'm pretty sure it won't matter but I
will we definitely want the we want the
student be same that's for sure
so anyway we'll keep that there so those
are the current words and we're going to
say the current words is now the limit
Iser oops limit eyes our limit eyes eye
for eye in current words and then we're
gonna say features equals that MP zeros
and that's going to be the land of
whatever the lexicon is which we passed
as a parameter and then what we're going
to do is we're just going to iterate
through our words and then set the index
in this features array to one or we're
going to use plus equals one because I
think that makes the most sense but it
really probably does not matter at all
but anyway forward in current words if
that word dot lower is in our lexicon
well what do we want to do well we're
going to say we need to find out what
the index value is so index value equals
lexicon no jerk Lex there we go
lexicon dot index and then word dot
lower so we're going to search in our
lexicon for that word dot lower and this
will return basically our index value
there and then we're going to take that
index value and say okay great so the
features whoops features or the feature
at index value that is going to be
either equal to one or you could say
plus equals one so depending on like if
you're reading an entire like paragraph
or you're using this on like a huge
paragraph or something plus equals one
might actually make more sense but since
we're using actually pretty small
strings this probably is going to have
absolutely no impact to whether you use
plus 1 or plus equals 1 especially on
this data set size I don't think it'll
matter but anyway you could if you you
know use whatever philosophy you think
is best then we're going to say features
equals the list of features and then
we're going to say feature set feature
feature-set dot append now we're going
to append a list of lists feature set
features and ten the classification
whatever that classification is so at
the end of the day a feature set up here
is going to be this huge thing that's a
list of lists where within these lists
you're going to have this thing that
looks that maybe like this right this is
that that a hot array so to speak that
has like each index of the like at the
bag of words model basically are some
more features and then you have the
label which soon you will see is like
either a 1 in a 0 or a 0 and a 1 right
so the zeroth element here is the
positivity and the first element here is
the negativity so 1 0 is a positive
sentiment sample and then a 0 1 would be
a negative sentiment sample so that's
our feature set that's why that's going
to look like and then when everything is
all said and done we want to return to
rent or turn that feature set okay so I
think we'll cut it here and then in the
next tutorial we'll we'll go through
actually the actually I think that we
only really have like one more function
so I think I'm going to go ahead and
have us write this next function I I
don't think you guys will mind so unless
we have like a thousand errors ok so
what we're going to say next is define
create feature sets and labels so so far
we've kind of got a high level some high
level functions that are going to do
these things for us but now we actually
need a function that's going to pretty
much actually like do it and put it all
together for us so so create features is
great so now what we're going to do is
we're going to pass a positive negative
and test size and this will be zero
point one so we're gonna say a 10%
test size lexicon is going to be able to
create lexicon and we pass pause neg and
then our features will be an empty list
and I'm going to say features plus
equals the return of sample handling and
then we want to pass in the pause text
and the lexicon lexicon and then our
classification for positives is 1 0 I
forget if I see I did say pause in a
text or actually save this ok and then
we're going to the same thing for
negatives so I'm just going to copy this
line paste neg dot text 0 &amp;amp; 1 and then
what we're going to do is random dot
shuffle features features so um we need
to shuffle these for sure not just for
like testing and stuff but and not just
for like statistical reasons but
actually for the neural network because
remember a neural network basically
you're like I say everything was
positive I don't really think we've
shown it but hopefully by the end of
this tutorial
or this like mini-series on implementing
this data and testing it you'll see what
a neural network is actually doing and
you'll probably will recall before the
question was from the output of the
neural network the question was does TF
dot Arg max of let's say the output does
that equal the TF dot R max of our
expectations right that was the final
question of you know did we actually get
in get the prediction right now what
does this actually look like so if we
actually looked at this like the values
of output and expectations like let's
say let's consider this case whether you
just have like these two output nodes so
we're going to ask does the output equal
expectations so let's say we're
expecting a positive it would look you
know TF organics here would be 1 0 but
the output here is actually probably
more like this or something it's
probably not this big it's more like I
don't know that and
that okay and this is true yes okay uh
but this is obviously not a one in this
office in a zero but in those kind of
one hot output structure you really just
you just want to know what's the maximum
argument that's kind of how strongly we
feel that this is positive right so
close how it works so if you didn't
shuffle basically you would train the
network to pretty much try its hardest
like if you were feeding positives and
you said you wanted this you basically
train the network to do like this right
and then like I don't even know like
negative 9 mm right because the network
could be like okay yeah basically
everything just needs to be like really
weighted up this way like this you would
just try a network to do this so you
definitely want to shuffle the data not
just for statistical reasons but also
just because that's how the neural
network models going to work it's
literally going to be shifting the
weights to try to make these arguments
are max functions be true and yeah
anyway mmm so that's why you definitely
want to shuffle the features here then
we were going to say his features equals
an MP array features okay
need this to be an umpire array and then
we're going to say the testing size is
just equal to the integer value we want
a whole number here of test size times
the Len of features okay so this is 10%
of our features whatever that whole
number is that's what we that's what we
want our testing size to be now we're
going to say train x equals the list of
features : comma 0 this is just some
nice numpy array notation that you can
actually get away with when you say :
comma 0 basically you'll say you've got
an array and it's like a 5 and a 8 okay
and you've got like a list of these
things let's say you got like this okay
so features : comma 0 and like let's say
you have a 5 a and then a 7 9 okay when
you do this : comma 0 stuff basically
you're saying I want all of the 0 with
elements so this here would be you know
5 7
okay and in reality these are
our actual features so again our feature
set is like this and then actually in
each one is your features and label
right and then you have a bunch of these
like this okay and then our features if
you recall or actually their own little
array and they're like these little one
hot arrays I'll just do this and then
the label itself is I don't know 0 and 1
and so on and so when we say we want the
list features 0
we're just taking all of these right all
of those like the true feature sets not
the labels ok and we want to take those
all the way up to the very end software
updates on of testing size that integer
value so basically we want to do this
for all of our features up to the last
10 percent okay then we take this and
I'm just going to copy that paste and
we're as I train y equals that only we
want actually you know the first
developments there and then when I copy
these and then rather than train X it's
test X and rather than train why it's
test Y and then it's actually negative
testing size to the very end so the last
10% and then at the end of the day we
want to return return train X train Y
test X and test why okay so that's
pretty much where everything's going to
come together for us I guess I need to
get rid of this before that causes
problems actually might not actually got
a problem there I don't know anyway uh
and now we're going to go ahead and run
this bad boy we're gonna say if name
equals main and this is just so the room
we could reference this later on in our
script if we wanted but if we call this
script into action it's just going to
run this instead for us and then what
we're going to say is these values here
equals and then it's equaling create
sentiment feature sets
so this function and then this function
we pass a pause text whoops
pause Oh text neg text else we're using
all these local um these local values by
the way make sure when you execute this
script you're executing it from that
directory don't be executing yeah anyway
pause text neg text and that's it right
we just passed positive yeah and the
test sighs we're just going to leave
that because we we wanted that default
parameter there and now what we're gonna
say is with open and then we're going to
call this sentiment set pickle as WB or
yeah as f rather and what we're going to
say is pickle dot dump and we actually
want to dump all of these values and
we're going to dump them as a list and
we're going to dump them to obviously F
so this creates a pickle of all this
information for us because we don't
really I mean like we only need to run
this function one time or this script
one time to have all these elements and
then we can pass them through our actual
our network so let's save this and run
it and see how many errors we have so on
me try to make this smaller so I can
pull this up cool and so first we're
going to change directory into desktop
TF tuts I think that's where we want to
be right yeah and then we're going to
say Python three create let's see what
happens okay
it's triple equals okay err number one
cancel those back over here did we print
I wonder if we printed the length of
that lexicon I meant a print Len lexicon
but I don't think I did
yeah I think for right above l2 it could
be useful to just print the line of l2
just so you know like how how big this
is
print whoops print Len l2 it just kind
of gives you an idea of like how large
but we didn't get any errors and I
promised to mning our pickle is here
yeah here's our pickle
that's about 140 megabytes in size so
it's 140 megabytes as you can see like
this this is how you can know so they
like to run this we're going to use up
one hundred forty megabytes for sure of
RAM just for this and then that's not
even including our neural network model
and that's because of the size of our
lexicon for the most part so Python 3
agree yes we're on that one time
hopefully quickly we'll see the lexicon
value here or the length of our lexicon
so 423 so that means every input input
vector is going to have 423 elements to
it okay so so that's actually you know
quite large as opposed to like the
initial string data like opening up
pause text for example well this first
line is actually kind of long I might
it's definitely not 400 characters
though it's you know looks like a tweet
almost and then a lot of these are very
very short short lines
so now 423 and really 423 is actually
pretty small um overall but yeah so okay
so we've got everything now
pre-processed so now we're basically at
the step where we were with the eminence
data set there are actually a couple
things left changing the script like the
batching and all that because they had a
nice little batch function written for
us but hope that will be a really easy
hurdle so anyways we're pretty much well
we have we've pre-processed all of our
data and we're ready to feed it through
so that's we're going to be doing in the
next tutorial if you have any questions
comments concerns whatever if you're
getting an error here anything like that
feel free to post below I'll do my best
to help you out otherwise until next
time
maybe I can't seem to get this to stop</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>