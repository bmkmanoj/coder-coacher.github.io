<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robots.txt - Search Engine Optimization Tutorial part 5 | Coder Coacher - Coaching Coders</title><meta content="Robots.txt - Search Engine Optimization Tutorial part 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Robots.txt - Search Engine Optimization Tutorial part 5</b></h2><h5 class="post__date">2013-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c6Jg1j_8EYI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to part 5 of my SEO
tutorial here we're going to be
discussing the editing of your
robots.txt file and some use of regular
expressions to make this a little bit
easier as we discussed in the previous
video it's really important to specify
what URLs to not crawl in order to keep
the indexed content of your website
updated frequently or even at all so you
want to keep in mind as well that even
if you suggest it's more of a suggestion
than a forceful action that you'll be
taking in this robot Dex text file so
what I mean by that is it only suggests
to Google or a robot not to crawl a page
it's not gonna prevent it from doing it
if it wants to
nor is there like any legal obligation
tied to it right so there's something
you really don't want crawled by Google
you need to put it behind some sort of
pay wall or something like this that
Google like literally won't be able to
get to so if you just say hey please
don't visit this page that's not good
enough
so anyway with that let's go ahead and
dive into the robots.txt file now if you
just make a website yourself totally by
scratch you you're not gonna have one of
these I mean some hosting plans I think
will just automatically slap one in
there for you
but it really just depends so the
robots.txt file if you don't have one
you just straight up making it it goes
in the st. it's like in the same
directory as your index dot HTML or
index dot PHP or whatever your index
page is you want to have that in that
same directory so as you'll see we can
go to robots.txt and visit my robots.txt
file so what this means is you've got
the user agent what this says is what
user agents are we gonna allow to
actually like view this this this
website and this star here is probably
what's going to be your little favorite
little tool here it basically just means
anything right so it's I'm not even sure
I would say it's a regular expression as
surely it just means zero or more
repetitions for regular expressions as
far as that's concerned you can't really
use any fancy regular expressions in
here so just keep that in mind then
really the only thing you need to
understand is that the star or the
asterisk means anything so for example
user agent we're allowing any user agent
to view of this website a good example
would be to disallow various user agents
or you could actually just specify all
user agents you would want and if they
don't match that they would be thrown
out so in some of my tutorials I show
you guys like how to use Python to visit
websites well when python visits a
website it's user agent most likely is
going to be URL l URL Lib - and maybe
sometimes your python version right so
so when websites see that they can just
be like nope I don't want you visiting
my website and they give you like a 404
or something like that so right I might
be like a 500 and I don't forget what it
is I don't see it anymore because you
can edit your user agent so it's not a
big deal but that's just an example of
what you can do anyway so this one this
robots.txt file says any user agent is
able to visit my website so this is
including all the robots now you use
disallow to not allow something to view
various pages so so obviously this WP
dash admin it's a link that exists but
we don't we don't want the robot to
visit it because it won't even be able
to visit it WP dash includes same thing
it's not gonna be able to visit that
stuff just keep that in mind you can use
that that way like for example this is a
search function right here so basically
it's disallowing any uses of the search
bar right because that would result in
an infinite number of searches possible
so this will block all of that from
happening at least with the Googlebot
right or were actually well any user
agent and any robot that comes through
here trying to go through all of our
links isn't gonna waste its time with
that so just keep that in mind also if
you don't you know if you don't have
that again all you have to do is if you
have FTP access or any sort of hosting
access at all you just literally write
up a robots.txt file
and you know put all this stuff in it
you don't need this right here it's it
can be suggested to put sitemap : and
this just tells it where the sitemap is
this is not required here but you can
use this and just literally just type
this stuff out put it up and that's all
you have to do it's very basic but again
very highly recommended especially if
you have any sort of any sort of dynamic
content if your whole webpage is static
it's not as big of a deal I would still
put user agent and the asterisk there
but obviously don't this allow anything
so anyway that's it with robots.txt what
you need to edit and why you would want
to edit it as always thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>