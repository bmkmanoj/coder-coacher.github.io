<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Custom K Means - Practical Machine Learning Tutorial with Python p.37 | Coder Coacher - Coaching Coders</title><meta content="Custom K Means - Practical Machine Learning Tutorial with Python p.37 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Custom K Means - Practical Machine Learning Tutorial with Python p.37</b></h2><h5 class="post__date">2016-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H4JSN_99kig" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody and welcome to
part 37 of our machine learning tutorial
series leading up to this we've been
talking about a whole bunch of machine
learning classifiers but specifically
clustering even more specifically flat
clustering even more specifically
k-means clustering so with flat
clustering k-means the idea is that you
the scientists get to tell the Machine
hey I want you to separate this data set
into K number of groups or X groups but
in our case with k-means k number of
groups so we've used k-means but this
time we're actually going to build a our
own custom version of k-means and to
recap what k-means is if you visualize
let's say we've got two-dimensional data
so envision a two-dimensional graph with
some some data points on it we pick any
two data points to start from and what
we do is we pick any two data points to
start from we measure the distances of
every other point to those data points
so we're saying those first two data
points are starting centroids we measure
all the other data points distances to
those points and then whichever one
they're closest to we classify those
data points as belonging to that
centroids class and then we take the
mean of both classes and the mean of
both classes becomes the new centroid
and we repeat this process over and over
and over until the centroid stops moving
and from there we can say the data is
clustered so with that to begin one
thing that we're going to go ahead and
do is take some code from part 34 if you
don't have this code if you haven't been
following along for some odd reason you
can go to the part 34 on the text-based
version of this these tutorials or you
can go to this version will have at
least this code there along with all the
other code we're going to write so you
can go there so I'm gonna copy basically
this all of this so copy that
come over here paste we aren't going to
be using scikit-learn sky means that
would be kind of silly we'll go ahead
and just show this just once and I'm
going to get rid of this stuff so we're
just keeping basically the imports the
definition of X scatter is just to show
and then the color stuff will just
I'll do that so real quick just to look
at the data set if you aren't familiar
with it or forgot this is the data set
should be pretty simple we're hoping
that we can create an algorithm that
automatically determines when K is equal
to two anyways automatically determine
Tate there's a centroid here a centroid
here this is cluster one zero will say
and this is cluster one something like
that okay
that's objective anyway we'll see so
finally make some space and we'll start
off by defining our class and this will
be a class K underscore means and we'll
start off with an initialization so
define in it and here we're just going
to pass self k equals two tolerance will
be 0.001 and then max hitter for Max
iterations will say is 300 so what's
going on to start here is that we are
we're going to say the tolerance is
0.001 that's just the tolerance is
basically how much that centroid is
going to move and this will just be by
percent change max iteration is
basically just how many times do we want
to run this before we're like okay
that's enough so I've never seen a data
set that you that failed within 300
usually it's like 10 or less iterations
or so depending on the data set but
usually it's very quick to actually like
find the centroid to a pretty high
degree but sometimes I suppose it could
take could take a while with maybe
really high dimensions there's an atom
anyway so now we're going to define some
of that starting value so if that K is
just going to equal that that K value
then we're going to say self dot
tolerance will equal tall self dot max
itter is just max inter also this is not
just arbitrarily chosen this is copying
scikit-learn so I could learn here
actually might have one more zero I
can't remember but anyway the other ones
fine but hopefully you never actually
hit three 300 but anyway so now we're
gonna have the
mine fits this will have self and data
for now pass it will also have a define
predict and then this will also will do
self and data here as well and and we're
just going to use the same sort of logic
that we used before so up to this point
you would never predict on data that you
trained against but with a clustering
algorithm or at least this specific one
well the what happens at the end of the
day when you've trained is you just have
these centroids right so if you pass the
exact same data that you trained against
through it really doesn't matter it what
matters is it's the centroid so so
you're not really cheating by passing
through data that already exists I mean
you might be sort of I don't know but
but for our cases you would be cheating
if like you were trying to test it based
on that but that's not really what we're
doing like the hope of clustering is a
little different than classification
with classification we wanted to know
how accurate we were right we clustering
we just simply want out we're just
hoping that those groups are made and
exist right so so it's just a completely
different completely different but it's
a different objective so at least in
this case we're not really cheating by
passing in the exact same data through
prediction so that's kind of what we're
going to do so um because really we just
we're just hoping that it makes the
groups so to start with fit will leave
predict empty for now will populate that
later but for now we're going to go with
fitment so to start we're going to say
self dot centroids and that for now is
just an empty dictionary and then we're
going to say for I in range of self k so
that's just for iron range of two for
example so we might get a in our case
two will be zero one we're going to say
self dot classifications or rather
centroids we're not ready to classify
just yet selves out centroids
I equals data I okay so so we're just
iterating through data which in this
case will just pass X so we're just
saying the first two centroids are going
to be just this in this again if you
wanted it to be random you could just
random shuffle the data set and then do
this exact same operation it should not
matter
that's why you do have tolerance and Max
iterations and maybe maybe if you didn't
optimize for whatever reason by the time
you hit either of these you might
shuffle the data and then try again but
you I've never had a data set that I
that this didn't work on so I don't know
but I'm sure it's possible so you could
you can I don't know have something
coded like that if you wanted now we're
going to do is for I in range of self
dot max iterations here we begin the
optimization process and don't worry
it's much simpler support vector machine
ones as you've probably already figured
out actually
but anyway self dot classifications now
we're ready it's an empty dictionary for
now and this is just going to contain
the centroids and the classifications
are basically but you'll see hey anyway
for I and then again we'll do in range
of self dot K we are going to say self
dot classifications hi equals an empty
list so what's going to happen is self
that classifications will the keys will
be the centroids the values will be the
feature sets that are contained within
those values so then what we're going to
do is and in fact maybe what I'll do is
since we have that we could probably
just graph that pretty easily so maybe
what we'll do when we go to actually
graph all this and show it is actually
not do the prediction we can use predict
truly predict new data so we'll do it a
little more formally correct but again
it really doesn't matter but yeah we can
do that because we'll just reference
self dot classifications as an attribute
and then everybody will be happy
so anyway so we've started it's a empty
list now we need to populate that list
so we're going to say for feature set in
X greetings I'm from the future so as I
was reviewing this video I notice that
I'm calling this variable here X don't
call that X the only reason it's going
to work is because X is defined above
this class but if you were to import
this or something like that
you would get an error this actually
needs to be data so in this video in the
next video I'm calling it X and I get
away with it but just know officially it
really needs to be data because that's
what we're you know undefined in it or
defined fit rather we have self data and
we're passing X as data so keep that in
mind first we need to calculate the
distances and the way we're going to do
this is a little little fancy but it's
not too bad
so for feature set next we're going to
say the distances equals and then it's
going to be a one-liner for loop and
we're going to say NP LnL norm to be the
norm of that feature set - self dot
centroids and then centroid for centroid
in self dot centroids okay so all we're
doing is this is just creating a list
that is being populated with K number of
values right because for centroid and
self doubt centroids that contains K
number of centroids so 0 and 1 so the
zeroeth index in this list will be the 0
basically the the distance to the zero
with centroid right and then the first
death element will be the distance from
that data point to the centroid one
basically okay so there we have that
we've got distances and then basically
from here the classification
classification is going to equal
distances index so what's the index
value of the min of distances boom done
finally we can say that feature set now
belongs to that clip that centroid so
self dot classification or
classifications in the classification
and then dot append feature set so keep
in mind every iteration what's happening
we are clearing out the centroids right
because the centroids are going to
change forever I'm sorry the starting
centroids aren't going to change my bet
put a for every iteration we're going to
clear out the classifications because
each time you move the centroids
because centroids basically you're
always going to have like a 0 &amp;amp; 1 index
or key rather but the value is going to
change so you're always going to have
the same number of centroids but here
for the classifications that's going to
change every time the centroid changes
so we empty that out and redo the
classification every single time now
selves that classification comes with a
pinch feature set
we're basically we're done there so
we're done iterating through let's see 4
I make sure we're not going too far over
so this would actually be still yeah
because you're still in the iteration
basically and then what we're going to
say is the pre of centroids is going to
equal the Dix of self dot centroids and
we basically have to do this because
object inheritance I suppose but if you
just said pre centroids equals self dot
centroids it would always equal self dot
centroids so as we what we're trying to
do is we're trying to compare the two
centroids so we can find how much
they've changed so we can use this
tolerance value but if you just said pre
centroids equal self that centroids it
would change as self that centroid
changed and which is obviously no good
so now what we would do is I'm going to
add this but I'm actually going to show
the first iteration without actually
having this but we're going to say for
classification in self dot
classifications I'm going to say pass
but initially what we're going to do is
self dot centroids classification equals
NP average and then this will be the
average of self classifications
classification and then AXI is 0 all
this is doing is it's going to take that
basically array value and it's going to
take the average of all of the
classifications that we have so I'm
going to take the average data set or
the average it's going to find the
centroid for all of the values that are
of that previous centroids
classification so this is finding the
mean of all the features for any given
class and then it's re that central
like this de frida finds the centroid
now okay
and that's why first for the beginning
I'm going to pass and just have this
commented out so you can actually see
the first step how it creates a centroid
and picks all the closest points and
then you'll see what uncommenting that
out what change that ends up ends up
making and I think for now I'll cut it
here and in the next video we'll
continue and we'll probably be able to
finish this but don't take my word for
it we'll have to see what happens in the
next video but probably in the next
video we can finish the entire thing and
actually see if it works because predict
is pretty quick and the rest of what's
left of this is not too bad anyway so
anyway you got questions comments
concerns whatever leave them below
otherwise as always thanks for watching
thanks for all the support subscriptions
and until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>