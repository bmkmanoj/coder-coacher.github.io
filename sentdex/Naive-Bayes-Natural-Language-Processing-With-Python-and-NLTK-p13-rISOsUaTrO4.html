<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Naive Bayes - Natural Language Processing With Python and NLTK p.13 | Coder Coacher - Coaching Coders</title><meta content="Naive Bayes - Natural Language Processing With Python and NLTK p.13 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Naive Bayes - Natural Language Processing With Python and NLTK p.13</b></h2><h5 class="post__date">2015-05-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rISOsUaTrO4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to the
13th NN ck with python for natural
language processing tutorial video in
this video we're going to be building on
the last video and actually complete our
naivebayes algorithm to categorize
things as either positive sentiment or
negative sentiment so first of all the
process usually goes as follows you're
going to have when you have a data set
like this that's a labeled data set
you're going to have two things you're
gonna have a training training set is
equal to something and you're going to
have a testing set equal to something
you have to split them up and you don't
want them to be the same data because
that creates a lot of bias so the
training set we're going to say is
feature sets and this will be the first
1900 feature sets the testing set will
be feature sets and that will be 1900
and on so the first 1900 will train
against and the second 1900 will test
against so when we train we take the
data we feed it to the algorithm we say
here are the words here is the words in
the top 3000 list of all words these are
how many times these most common words
appear in negative reviews here's how
many times they appear in positive
reviews if they appear significantly
more on negative reviews that probably
means that word is very important to a
negative review and vice-versa for
positive reviews so with the training
set that's how we train it and then with
the testing set we feed through the
feature sets and we don't tell the
Machine what category they are we ask
the machine to tell us what's that
category the machine tells us the
category and then we compare it to the
known category that we have and we see
how accurate is the machine does it get
it right more often than it doesn't and
if not well you've screwed up so anyway
that's that so we're going to be using
this algorithm called a naive Bayes
algorithm so this is a type of
classifier that's going to use the the
actual just the Bayes algorithm which
works on very strong independent
assumptions on for each feature and
that's actually where it gets its naive
name
basically and people sometimes people
call it stupid Bayes because it doesn't
it's not really I mean like logically we
can think through this algorithm we can
see all of the flaws that are contained
within it but we can actually find that
we have pretty good results with it and
because the algorithm is so basic it's a
really short algorithm and it doesn't
require a lot of computation we can
scale this algorithm to like massive
proportions because it doesn't take much
processing so then the Bayes algorithm
is pretty simple you're going to have
basically the posterior and that's just
like you know the likelihood equals and
it's going to be prior occurrences a
times likelihood and then all of that is
going to be divided by the current
evidence and that will be the the
likelihood of something to be positive
and then you would do the same thing for
something to be negative and that's
basically all it's done so it's a quick
algorithm if you want to know more about
that algorithm I highly suggest you just
go google it and read like I don't know
the Wikipedia page and maybe some
articles on it and figure out more how
it works but it really is actually quite
the basic algorithm as far as learning
algorithms are concerned but it's again
not necessarily the best algorithm it is
just scalable and it's really easy to
understand so once we've gotten to this
point now we're ready to actually create
the the classifier that we want to use
because we've got a training set we've
got a testing set and now we're ready so
first of all we have an LT K already
imported so we'll just continue using
that and we're going to say our
classifier classifier is going to equal
n LT K and we're going to use the naive
Bayes classifier algorithm we're going
to dot train and we're going to train
against our training set I'll just copy
that and paste it now first of all what
we can do is we can do a couple of
things first of all we can check
accuracy right away so we've trained it
now we can check now we can test it so
we can just say print and we'll say a
nice
baize algo accuracy : comma and then now
we can use NLT KDOT classify dot
accuracy and we can run that and what
you the parameters that you pass through
here are going to be what classifier are
you using and what's the set that you
want to test against that's testing set
and then we'll just multiply the answer
by 100 so it's in a percentage and we'll
even denote this is accuracy percent
okay so then so we can do that but Walse
add one more thing to it we're going to
say classifier that show underscore most
underscore informative on our score
features and I'll show the 15 most
informative features and let's go ahead
and save it right now let me just check
to make sure on the major syntax errors
I don't think so so let's save and run
it and we'll talk about this as we're
waiting so this will print we don't need
to print this anymore I'll stop that
from happening so that's gonna hog up a
bunch of space real quick but anyway now
I want um so this trains it this runs
the actually and what this is going to
do is tell us like the most popular
words on both sides and whether or not
they're a tend to be positive or tend to
be negative so it looks like probably is
updating now right so first of all our
naive Bayes algorithm accuracy percent
we're given here is 89% now I will add
that depending on your samples and
actually let's go check and make sure we
didn't screw anything on this is
actually a higher number than I normally
see with the pay's algorithm you might
find that yours is as low as like 60 or
maybe even lower sometimes I've seen it
hit 59 and stuff while I was setting
this up so just understand that this is
highly volatile and don't be like
ashamed if you don't hit 89 you probably
didn't do anything wrong um anyway uh so
now we can look down here and we can see
ok what are the most common words I
can't believe 3000 is one of them I'm
actually kind of shocked I want to go
check and make sure this is a little
area that we use 3000 here but that
shouldn't make any
me any difference at all anyway so we
can see here one of them are more
important the most important one is this
word slip and it appears 11.6 times more
often than it doesn't a negative review
3000 is a negative one trying to think
of 3000 like why would 3000 be one
anyway they've got symbol seamless
idiotic makes a lot of sense this is a
unimaginative
okay these these ones make a lot of
sense idiotic appears 8.5 times more
often than it does in and they are in a
positive review unimaginative same thing
7.7 and so on so we can actually end up
using this later on down the road to
change the algorithm slightly but as you
can see none of these contain grammar
and that's why grammar just didn't
matter we have such a large data set we
have 3000 most common words that grammar
is of course not on here because it
isn't an informative feature so we can
actually kind of use that to our
advantage and since I'm talking let's
let's rerun this one more time before I
cut this tutorial off I'll be really
surprised if we get another high 80s
return so hopefully we'll get something
a little more reasonable because it's
just uncommon that you're going to hit
that much accuracy but anyway so we'll
just wait for this and then we'll talk
about this so yeah 66% like I said it's
not as accurate as you would as 8989 is
unlikely so anyway we'll talk about how
to improve the accuracy of this or at
least improve the reliability because
one of those two is really important
it's nice to have both but we definitely
want to have reliability over random
high accuracy points so anyways that's
it for this tutorial
if you have any questions or comments
please feel free to leave them below
otherwise as always thanks for watching
thanks for all the support and
subscriptions and until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>