<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tables and XML - Web scraping with Beautiful Soup 4 p.3 | Coder Coacher - Coaching Coders</title><meta content="Tables and XML - Web scraping with Beautiful Soup 4 p.3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tables and XML - Web scraping with Beautiful Soup 4 p.3</b></h2><h5 class="post__date">2016-10-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sAuGH1Kto2I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody and welcome to
part three of our web scraping the
beautiful soup mini series in this
tutorial what we're me talking about is
scraping tables and if we have time XML
documents so let's jump in I'm going to
go ahead and delete from here and
looking at this here so this is our
table that we're going to try to parse
so looking at the source
just in case anybody's not too familiar
with HTML table lling basically is gonna
start with a table tag and then
everything in between table tags has
like TR tags for table rows and then
within the row here we have a th for
table headers as the header of the table
then the rest of this is just TD tags
for table data okay so we're going to
try to pull just the table data
information from here so the way that
we're going to do that is first by
defining table so table equals and in
this case you could do you can do this
like a couple of ways so remember before
like for the navbar we said soup soup
nav right you could do the same thing
soup table right so we can print table
save and run that and we get the table
information we can also say table equals
soup dot find table so we're just going
to overwrite it there okay it's the same
thing so you can use those two those two
ways and at least so far we're not
seeing any difference close this and I'm
gonna just come out that first one if I
remember to all run it with both just so
you can see at the end but anyways soup
define table so now what we want to do
is we're gonna say table underscore rows
equals table dot finds all because we're
going to look for all the table rows
right we could do we could do table that
TR or table define TR but it's just
going to find one of them we want all of
them then we're going to save for TR in
table rows what do we want to do well we
won't find a table of data now so we're
going to find table data between the TR
tags so TD would equal TR dot find
all table data tanks now the row we're
just going to make a quick one liner for
loop here so it'll just be I dot text
for I in table data and we need to say
equals that then when we're all done
let's print the row and let's run that
great so we get all the table data here
you'll notice this one's empty that's
because that's the table header and that
doesn't have TD tags it's the table
header but it was between table row tags
now before we progress any further I
will just show you
pandas version of grabbing tables I
think it's a lot better so I think if
we're talking about scraping tables I
definitely need to show this this is
what I'll usually use so if you don't
have pandas you can pip install panels
but it will take a long time to install
so if you don't have it and maybe it's
not interesting yet to you or whatever
you don't have to grab it pandas is a
data analysis library and if you are
interested in pandas tutorials I have a
bunch of them so what I'm going to do
real quick is up at the top I'm going to
import pandas as PD and then I'm going
to comment all this out and then I'm
just going to say the follow I'm going
to say D F's for data frames equals P D
dot read HTML and then we just pass the
link in there and what this is going to
do is it's going to go to this website
and whatever you put in and it's going
to try to it's going to parse all of the
tables that can find and return a list
of data frames because there might be
multiple tables now we're going to do is
for data frame in data frames let's just
print D F dot head actually it should be
short enough let's just print the entire
data frame cool so we get the whole
thing now we could say when we go to do
the read HTML we can say header equals 0
and that'll make the first kind of row
the header yes okay so that's how you
can use it with pandas and read tables
and
I think that's so much more simple you
can of course convert this to a list of
lists like DF values to list okay you
could do that if you wanted but it's
much more easy to sort manipulating
running calculations or whatever on a
data frame than it is just a bunch of
lists anyway I thought I'd show that
finally let's get on to the XML
documents if you're not familiar with
what an XML document is usually you're
going to see these in the form of
sitemaps I put a link to it at the
bottom here sitemaps are basically maps
of all the URLs on your website okay so
there will be some information here but
as you'll notice there it's just between
tags so XML was meant to be slightly
more human readable okay so it's like
human and machine readable so here you
can see basically all the links for
Python programming dotnet now a lot of
times people use sitemaps on like news
websites and stuff because this is where
the newest links you can find them so on
like let's say you go to Washington Post
or something like that and you go to the
Washington Post sitemap that's going to
have all the links for Washington Post
so let's go to Washington Post see what
we find it's probably at the bottom I'm
just going to get Wow there's no no okay
okay it's probably RSS yes I agree dang
it I thought this is going to be quicker
why they like hiding it now this
Washington Post sitemap it's probably
like really obvious I'm just missing it
okay
so here's one sitemap at least this is
just their main site map but usually
news websites I'm not going to waste
much of time looking for it but usually
news websites will have sitemaps even
for specific topics like politics news
or whatever so if you wanted to have
some sort of bot that was constantly
tracking news you would just track those
sitemaps so closing that out let's talk
about reading the sitemap now so doing
this is there's like one slight
difference in the soup but I'm going to
copy this and in fact let's just do it
up here I'm going to paste it I'm gonna
uncomment this out and rather than parse
mEEMIC parse face it's going to be
sitemap XML and then rather than using L
XML there when we create the soup we're
going to say XML then what we're going
to do is just so we are confident here
let's print soup cool so we got
everything we need and we know that okay
it's between URL tags and you might want
less modified so a lot of times they'll
have dates or whatever so to figure out
how by visiting this link already you
could use the date like on a news
website but may basically what we're
interested in is the the location tag so
pretty super cool so here all we would
need to do is for URL in soup find all
location print you our URL text okay and
these are just all the Python program to
net URLs so that is all the beautiful
soup also I just realized I never did
this second version of table so let's
just run that quickly so using just dot
table or whatever you get the same thing
okay just wanted to show that I've
always been able to use those
interchangeably I wish I could tell you
what the difference was I'm sure there
is one because there's it wouldn't be
there I don't think if there was no
difference but anyway if someone is the
difference feel free to comment below
okay so that concludes the third a
beautiful soup web scraping a tutorial
if you have questions comments concerns
suggestions whatever prefer to leave
below otherwise till next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>