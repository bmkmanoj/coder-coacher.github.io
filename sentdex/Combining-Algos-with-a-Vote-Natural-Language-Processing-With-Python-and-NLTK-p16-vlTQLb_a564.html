<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Combining Algos with a Vote - Natural Language Processing With Python and NLTK p.16 | Coder Coacher - Coaching Coders</title><meta content="Combining Algos with a Vote - Natural Language Processing With Python and NLTK p.16 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Combining Algos with a Vote - Natural Language Processing With Python and NLTK p.16</b></h2><h5 class="post__date">2015-05-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vlTQLb_a564" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what's going on everybody welcome to
part 16 of our Python with ml TK for
natural language processing tutorial
video in this video what we're doing is
taking all the classifiers that we just
built and kind of putting them together
to create a voting system so each
classifier gets one vote in our new
classifier that we're going to right now
is based on the voting system between
all these classifiers in whichever
position whichever category positive or
negative gets the most votes by all of
these classifiers that is the one that
we're actually going to choose this
should not only raise our accuracy by a
few points it should really raise our
reliability very nicely but also it
gives us the ability to add in another
parameter and that's going to be our
confidence parameter and this will be a
parameter based on how many votes right
who got the what percentage of votes so
if the score is a hundred percent right
where all of the classifiers said hey
this is what we think is the case then
our confidence is 100 percent if you
know three out of seven said something
was negative and four out of seven said
it was positive our confidence is kind
of low it's you know four out of seven
so anyways that's what we're gonna be
doing here we're going to build this new
classifier so this is going to be kind
of like our own sort of classifier and
this is you can use this methodology
actually to create any classifier you
wanted it just so happens that this
classifier that we're going to make is
the you know compilation of all of these
classifiers so and in fact the
classifiers that we want to use we
already know we don't really care about
SVC because it was giving us inaccurate
numbers so we're just going to toss SVC
and so that leaves us with one two three
four five six and then seven for the
original classifier so we have seven
classifiers so that should be good so
we're going to come down here so that's
a nice odd amount and let's go up to the
very very top we need a couple of things
first we're going to go from
LT k dot classify classify we want to
import classifier I so you can basically
this is so we can inherit from the NLT K
classifier class and then finally we're
also going to from statistics import
mode and this is just how we're going to
choose who got the most votes we're just
going to take the mode okay so easy
enough now coming down to the bottom
well we'll just build the class right
here and actually we really should build
the class up at the top that's just
where it should that should be so we'll
just put it up here I guess so what
we're going to say now is we're going to
save class votes underscore class
actually priced abilities boat
classifier in this vote classifier is
going to inherit from the classifier I
class from NL to K by the way if you
want to know the basics of classes I do
have a couple of class introduction
videos also there's a crash course for
classes a class for classes hilarious
anyway I have tutorials on those too if
you want to know like if you don't
understand what I mean by inherit from
you might want to check those out now to
find in it and if you don't know what
this means basically this is just a this
will always run so the init method on
any class is used because when you
invoke the class the init method will
run whereas the other methods won't run
unless you call upon them so this one's
self and then for our args will have
classifiers so we're going to pass a
list of classifiers through our vote
classifier so that'll be that
and then self dot underscore classifiers
is just going to equal classifiers so
our classifier list will just be
whatever list of classifiers we pass
through our vote classifier class now
we're going to define our classify
function or method rather and this is
just to be kind of synonymous with the
NLT K classifiers that way we can call
upon this classifier and say vote
classifier classify just like we can
with any of the other ones
accuracy won't really matter we're going
to basically already have that anyways
it's just that we change classify
because we have to kind of define our
own classify because right now
classifier doesn't actually do anything
so classify a self and then features is
what we pass through here so this is
just the same as anything else we pass
through the features so they can be
classified so this one's pretty simple
we'll start with an empty list of votes
and then we'll say for classification in
or I mean for classifier in self dot
underscore classifiers what do we want
to do well we're going to say their
votes so V equals C dot classify based
on the features so now for each
classifier we're going to get the vote
then what are we going to do with it
well we're going to do votes dot append
whatever that vote was and that's that
Ella and then at the end of this
function we just met it rather we're
going to return the mode of the votes
who got the most votes now the other
thing we can do is we can define a
certain post call this let's call this
confidence define confidence and again
so features we really this will be
almost identical the classify class we
could actually include admit but I don't
really want to do that I want to leave
classify identical to the other
functionalities of classify so that'll
be that votes and then we'll get rid of
that original return and we'll say the
choice votes this will be equal to votes
count whatever the mode of votes was so
this counts how many the most popular
vote or how many occurrences of that
most popular vote were in that list and
then we can do something like this we
can say confidence we'll just call it
con flag is comm equals so we don't
conflict with this coffee quills the
choice votes out of the length of votes
so this is just how many of the chosen
category over the length of votes so
this gives us a you know certainty
basically we could multiply by 100 I'm
choosing not to do that at this stage
because none of the other n ltk accuracy
classifiers and stuff
they use the you know zero to one so
we're going to kind of stay with that
and then so we've got confidence and
then we'll return Oh easy enough so now
what we're going to do is we'll come
down believe this one here we'll come
all the way down to the bottom here and
now what we'll say is we're going to say
voted underscore classifier equals what
we call it vote classifier it equals
whoa oops vote classifier and then
remember what did we say where the
parameters well this is args so it would
just be a big list of them so we're
going to use basically all the
classifiers we have so we started with
just the regular classifier here just
our basic classifier we had that and
then we had this mnd classifier then we
had the Bernoulli classifier then we had
the logistic regression classifier then
we had the stochastic gradient descent
I'm just going to stuff it right here I
don't feel like going to the end of that
list then we had linear SVC okay just
don't forget your commas otherwise
you're going use syntax error after
waiting like a few minutes
and then new SVC pays comma good that's
that now what we can do is we'll take
this right here copy this paste and then
we'll just say voted classifier voted
classifier class buffer voted classifier
accuracy percent n LT k dot classify dot
accuracy of what we want to do the
accuracy now the voted classifier I'm
just going to put this down here with
some separation just so we can see our
new code basically so this will be this
is our new classifier with the voting
capabilities so we'll do that and then
we can also finish this up with a sort
of print
and what we can do here is we can print
some examples because I want to show you
guys the confidency thing and then we
can kind of move for we'll use this and
watch them move forward soon and
generate real-time sentiment analysis
rather than doing this kind of
historical and testing anyway
classification : then we'll do comma and
then we'll do something like this voted
underscore classifier classify and then
we want to classify something it so will
classify we'll just do testing set will
do the zeroth element and then 0 again
so that will be the classification and
we just test is good and then we'll do
comma and then we'll do confidence and
this will give us our confidence and
actually make this one percent
confidence percent : comma voted
classifier dot what do we call
confidence I think they called it
confidence then we returned comp but I
just want to make sure confidence yet so
voted classifier dot confidence
confidence um and then again the same
kind of testing set we'll just do this
if it is testing set 0 0 whoops this
okay and in fact let me make this a
little smaller hopefully this will all
fit on the screen yes
ok our massive list doesn't but this one
does anyway good good good everything
looks good to me and in fact we should
be able to get away by doing this so
this will make it all fit on the screen
yes okay so this is our new voted
classifier then we can this is this the
general percent based on the testing
stamp the entire testing sample and this
is just one example so let's go ahead
and save and run that and then I'll
leave we're just going to leave this
running for a moment and as it starts
outputting results I'll show it on the
screen but then we'll come over here and
let's just kind of let's make a few more
I just want to make sure the initial
code is correct at least so now we'll go
pace let's just make a few more of these
just so we can hopefully see some some
of more examples so this will be a 1 2 3
4 5 and then we'll make this a 1 2 a 3 a
4 and a 5 so this will just kind of run
through a few examples for the
classification and the confidence
percent hopefully we'll see some
divergence we may or may not but anyway
this is outputting some results now so
the initial naive Bayes 64 we're seeing
some other percentages here basically
everybody's ahead except for stochastic
gradient descent linear SVC is doing
really well I seem to recall linear SVC
does fairly decently it's one of the
higher ones let's see we still have new
SVC to go through and stuff but anyway I
was hoping this would go slightly faster
the voting process is probably pretty
long too
Conner's rerun it there we go okay so
here's an example that first one our
classification was negative but the
confidence percent was whoops we forgot
to multiply by 100
basically 85 or 86 percent let's uh
let's add our multiply by 100 real quick
and then we'll run this one more time
you get a few more examples but you
should probably be getting the idea at
this point
okay space please based okay hmm so with
us at this point basically this last
line here this classification negative
confidence what you should see this is
the code right here that we will use to
move forward so testing set remember
testing set is merely a list of features
and whether or not they contain those
top three thousand words and then that
was trained against you know whether
those top 3,000 words are more commonly
found and negative or positive reviews
so this is just a list of true and false
it's basically a set of you know true
and flies the dictionary of the key
being the word the value being true or
false if it's you know whether or not
it's in this document so what we can do
moving forward is we can read really
anything we could read news articles we
could read tweets we could read blog
posts we could read comments anything
and as long as we pass the the words
through what what do we have to pass
those words through we just have to pass
them through find features as long as we
know about word features we need to pass
them through there as long as we've
pickled our algorithm otherwise every
time we'll have to kind of do this
process luckily this process isn't that
long anyway
but you could pickle the entire thing
and say that but I think you probably
still want to tweak these parameters
maybe do some more training and stuff
like that it looks like we yeah we
outputted this so let's just let's check
this out real quick so here's we've got
a one position positive the confidence
is 71%
this negative negative everyone agrees
negative 71% negative everyone agrees so
as we start to go on you can kind of
decide what confidence score are you
willing to accept the answer otherwise
you can just kind of toss it as bad data
so is it better to have 80% accuracy but
get literally you know half of the
results or is it better to have 70%
accuracy but get you know 80% of the
data set you know so that's kind of up
to you a lot of times you can kind of do
a little bit more analysis to decide
which one you want but anyway moving
forward this is basically all we have to
do now we just need to get a data source
whatever that data source is and pass it
through the fine features and as long as
we have this word features either
pickled or in our memory we can start to
generate this exact output on any body
of text we feed through it so that's
what we're going to start working on now
is kind of feeding any body of text so
probably take to Twitter maybe connect
to a Twitter feed and start outputting
some analysis on you know some sort of
keyword so anyway stay tuned for that if
you have any questions or comments up to
this point please feel free to leave
them below otherwise as always thanks
for watching thanks for all the support
and subscriptions
until next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>