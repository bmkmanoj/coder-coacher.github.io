<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scikit-Learn incorporation - Natural Language Processing With Python and NLTK p.15 | Coder Coacher - Coaching Coders</title><meta content="Scikit-Learn incorporation - Natural Language Processing With Python and NLTK p.15 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/sentdex/">sentdex</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scikit-Learn incorporation - Natural Language Processing With Python and NLTK p.15</b></h2><h5 class="post__date">2015-05-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nla4C-VYNEU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is going on everybody welcome to
the 15th Python with NLT K for natural
language processing tutorial video in
this video I'm going to be showing you
guys how we can marry the NLT k module
with the scikit-learn module so the NLT
K module is what it is it's a natural
language toolkit it's not a machine
learning toolkit by any means but
scikit-learn is so we've built into NLT
k even though I know TK does have its
own naivebayes algorithm so does IKEA
learn sake learn also has support vector
machines it's got its own classes of
naivebayes and many different versions
of the naivebayes algorithm it's got you
know linear regression it's got a bunch
of other stuff that we can actually use
like will also use stochastic gradient
descent for example so anyways that's
what I'm gonna be showing you guys today
is how to make use of all of these other
algorithms that are available via the
scikit-learn module now you will need
the scikit-learn module so if you don't
have that you can I have a tutorial on
it otherwise you can just you should be
able to get by with just pip install SK
learner so I can't learn I forget which
one it's probably in this tutorial
though so if you go to Python program
and I just got to start learning data
analysis machine learning and go to the
second tutorial here and this is just an
example of using it yeah you see these
would be all the things that you would
need for scikit-learn so that's going to
be numpy scipy matplotlib and
scikit-learn
should be able to go and use pip to
install all these you don't know how to
use pip and again start learning control
F for pip come on down here and here's
the tutorial on installing things with
pip now that's that so once you have
scikit-learn now ready to marry these so
if you have any problems installing and
stuff scikit-learn you can the pose here
post on the scikit-learn installation
tutorials otherwise we're going to go
ahead and move on so the first thing
that we need to do is we need to have
the NL TK libraries so to speak of what
we need to marry these things so we're
going to go from n LT k dot classify dot
scikit-learn
Import s-k learn capital C classifier
okay so this is part of the NLT K module
okay but it's basically a wrapper to
include the scikit-learn algorithms
within the NL TK classifier itself so we
can do the same things that we've done
down here where we basically say you
know NLT K dot classified on accuracy
and we can pass any classifier through
here including the psychic learning
classifiers so let's go ahead and import
a bunch of some scikit-learn classifiers
will do the NAT naive Bayes ones first
so from SK learn dot naive underscore
Bayes we're going to import let's do a
multinomial naive bayes now multinomial
that just means it's a multinomial
distribution so it's not a binary
distribution of data so this could be
many categories so this one may or may
not be very accurate this one will
probably be a pretty volatile one then
we've got Gaussian naive Bayes and then
we'll take in the Bernoulli knife face
feel free to google any of these that we
go through I will just basically explain
them but if you want to know more about
each individual one eventually I plan to
go through socket learn and just knock
out all of the algorithms because useful
to know which one is going to be good in
various circumstances also every single
psychic learn algorithm comes paired
with a bunch of parameters you can use
the default parameters these are general
parameters that are pretty good but it's
good to know what the parameters are and
how adjusting them will change your
success so keep that in mind that
despite the success that we get with
these whether it's good or bad
you can probably improve that success by
at least ten percent by correctly
choosing and not using the default
parameters and scikit-learn also
actually does have something built in to
help you automatically choose your
parameters but this is not a cycling
tutorial this is a
Tek tutorial so if you want to learn
more about scikit-learn I do have basic
tutorial on scikit-learn I just showed
you how to get there to do the
installation check that out if you'd
like also like I said eventually I'll do
tutorial basically on every single
algorithm from scikit-learn
and that should be pretty helpful
anyways we'll be using the defaults so
let's grab that and then how do we
actually use these so we've got
multinomial Gaussian and Bernoulli well
it turns out it's actually pretty simple
to kind of bring these all together so
let's do um let's just scroll down to
the bottom here
and you can see where well this is kind
of like the old stuff that we've used
we'll get rid of this we'll leave this
one here just because we'll call this
original naivebayes algo accuracy
percent and then now how might we
incorporate some others so let's say we
want to use the multinomial been our
yeah multinomial naive bayes binomial
Bernoulli haha and then we'll do
underscore classifier that's going to
equal the SK learn a classifier and
we're going to use the multinomial and
be class here okay so that creates now
we've got our classifier and now we can
do the exact same thing as as we did
before so let's see well we're using we
were in okay so we changed our code a
little bit but here's the original code
I commented it out so this is the code
for n l TK classifier training it's just
classifier equals and then well first
way to define the classifier but then
but what we can do now is we can train
this with just dot train because now
it's an NL TK classifier so this is how
we converted it to its n l TK
classifiers with this SK learn
classifier wrapper around around it so
now we can do something like this we can
say n n m and B so just copy that paste
dot train what do we want to train it
against well the training set so copy
paste that in and then we can do the
exact same thing basically right here
let's just copy paste this line let's of
course edit this
and we'll just say actually leave most
of that we'll just copy this and then
replace this with that and then instead
of classifier we're using this
classifier now the M and B classifier
and then we test it against the testing
set everything else stays exactly the
same so then we had a few more right
we've got multinomial Gaussian and
Bernoulli so let's just let me just copy
that come on down here paste and let's
highlight this copy paste paste and now
we're going to do let's just do this
copy paste and then for Bernoulli copy
paste do this copy paste and then this
is Gaussian and be paste the classifiers
of Gaussian and be so this one's almost
done let's take this paste
okay so Gaussian and be classifier
equals SK learn classifier gauzy going
to be good good good very newly NB
classifier copy that paste paste here
and then here paste but get rid of the
classifier part and that should be it
drinks it and maybe Oh what we need to
do this let's copy paste copy and paste
okay so now we can train against
basically all these let's go ahead and
save and run that make sure we're where
we want to be without any errors before
we continue on while we wait for that
actually what we can do okay so the
original naive Bayes sixty-two percent
what we can do is while waiting on that
let's go up here and now what we can do
let's do from SK learn dot linear
underscore model let's import logistic
regression and the SG so test so
stochastic gradient descent
classifier there we go classifier good
so we'll get those two and then also
let's do from SK learn dot sv n:m for
support vector machines import SVC just
support vector classifier the linear SVC
and then new SVC linear is just linear
SPC nu is so you can specify the number
of support vectors so then so these are
just like a bunch of algorithms and I'm
not wasting your time just showing you a
bunch of new and extra algorithms here
we got an error here which one was that
on I was on let's see we had that error
on NMB will comment out Gaussian for now
let's not use Gaussian we'll come back
to that I'm pretty sure maybe that
doesn't work with the SK learn
classifier conversion so we'll come at
that out for now let's keep this let's
just run it again and make sure the
other one doesn't have an error but MMB
you can see actually did better 75
percent than the 62 but you should know
by now that this number was so volatile
is anything from 59 to 89 in our current
tutorials so you should know that 75 may
or may not be reliable anyway as I was
saying before let's see if we can make
it through here I'm showing you all
these algorithms because we're going to
actually use most of these and then
we'll use a voting system to vote so
each one is classifying something as
positive or negative but what we can do
is if we have let's say five or six or
seven algorithms we can make it a vote
we can say okay what are the most
algorithm say about this sample and so
one that should raise up to basically
the mean of the most accurate most
accurate ones we'll toss out some of
them that are clearly inaccurate so
it'll do that but it also will create
reliability for us as you know and I've
seen that's important so anyway so we've
got linear SPC new SVC that's good so
now we have all of these and so I'm just
going to copy this because I'll forget
otherwise and we'll come
down here let me just paste them and so
we want basically all of these so let's
just comment that out and then we'll go
copy and then plus that five more one
two three four five okay so now we'll do
logistic regression classifier paste
paste paste paste and then paste will
just occur aggression classifier good so
we've done that now SGD see classifier
copy that paste paste paste paste and
paste um essentially you might want to
actually just use a for loop and enough
classifiers anyway we're already too
deep guys pastes paste paste paste and
paste so if I'm not missing one that
would suck uh what was the next one I'll
see linear SVC next so linear SPC linear
SPC paste paste and paste and I think
the last one is our new SBC yes sir so
again all of these have their own
parameters so it's really important that
you understand that we're using defaults
right now so for example new SVC you can
specify you know the new so the number
of support vectors and I think the
default 0.5 but you there's a lot of
parameters most of these have like ten
parameters so you can really customize
these quite heavily so keep that in mind
and check them out if you want to go in
advance like I said I will eventually
have tutorials on each one but I don't
right now but you can go to this eye kit
learn documentation hopefully find them
so one quick look over before I start
running this SVC good good good good
good everything's looking good logical
okay let's just run - it happens I am
put this up so now as we're waiting
again what we're going to end up doing
after this is we'll have basically a
list of accuracy so our initial accuracy
here is 58% pretty bad but as we go down
the list here of accuracy so nm mnb we
got 65% we'll keep going here and what
we can do is take a vote on each single
one so the next video we're going to be
talking about is how we're going to
actually build a class that will take
each classifier and it will basically
make these classifiers each one what you
will need to have an odd number of
classifiers you could do an even and
have some handling for that but we'll
just purposely use an odd one so
whichever classification positive or
negative gets the most votes that's what
our new classifier we're going to call
it like vote classifier that's what our
new classifier will say the answer is so
um so that should give us one a slightly
higher accuracy because we'll probably
throw out the ones that we're finding to
be rather inaccurate or unuseful here so
support vector machines at least the
basic support vector classifier is
basically useless this number is going
to be pretty reliably low it'll be lower
than 50% most of the time often so we
found we'll find that that's this one
we're just going to toss out anyways but
of all these other ones basically we'll
be able to take a vote and see which one
gets the most votes and this will create
a reliability in our accuracy so we'll
actually have some reliability but not
only that we can also create a kind of
like that objectivity score from the
Center wordnet if you looked in that but
generally when you have a sentiment
analysis algorithm you've got you know
or any sort of text classification
you're gonna have two factors you're
going to have the ending score and then
you'll have another thing that is like
the certainty of that score so if if if
we have let's say we've got seven out
and six out of seven algorithms say it's
positive well that's a pretty confident
score so so that would be a relatively
reliable score but what if three out of
four said it was negative and four I'm
sorry three out of seven said it was
negative in four out of seven said it
was positive well that's a little less
reliable we're a little less confident
so you can generate a confidence II
score as well so anyways that's that so
we ran through all these classifiers we
can say see that actually quite a few of
them did better than our original
classifier in this round at least we got
you know 65 65 70 71 69 so when we
averaged these we should find that we
get you know upper 60s we're obviously
going to toss out the basic SVC but
again this is with the default parameter
so I encourage you if this is the path
you want to take to maybe start tweaking
some of these things again this is Anna
scikit-learn tutorial series it would
take a really long time to talk about
each one of these so I'm not going to do
that but if you find that you want to
dig in deeper I highly suggest you do so
anyways that's what you have to look
forward to as a voting system basically
so we can get some reliability in a
slightly higher score but also we can
get some confidence out of this so
that's what you guys have to look
forward to if you have any questions or
comments up to this point please feel
free to leave them below otherwise as
always thanks for watching thanks for
all the sports subscriptions in it till
next time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>