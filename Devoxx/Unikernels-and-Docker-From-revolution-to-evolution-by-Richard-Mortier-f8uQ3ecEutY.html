<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Unikernels and Docker: From revolution to evolution by Richard Mortier | Coder Coacher - Coaching Coders</title><meta content="Unikernels and Docker: From revolution to evolution by Richard Mortier - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Unikernels and Docker: From revolution to evolution by Richard Mortier</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/f8uQ3ecEutY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello everybody thank you for
coming to this talk my name is Richard
Moore ta I'm going to talk a little bit
about you know kernels and docker from
revolution to evolution as the title
says my background is that I'm faculty
at the University of Cambridge in the
systems Research Group and thanks for
acquisition of a start-up with which I
was involved I also now work with docker
what I'm going to do is I'm going to
introduce the notions of yoona kernels
what unique kernels are where they came
from what we try to do with them some of
the benefits of them and then I'm going
to go through some of the ways that we
starting to see you know kernels being
used in practice and hopefully show a
couple of demos that will that will at
least give you a flavor for what we
think might be might be possible here
before I start and bearing in mind all
the lights that's shining in my eyes so
I can't see who here has heard of unique
kernels most people cool who here has
used yoona kernels okay almost nobody
and who here has programmed in Oh camel
ever even at school okay a few got
handful what about other functional
languages like Haskell and things all
more Haskell programmers that always
slightly surprises me okay and that's
question who has used docker
yeah and docker for Mac or Windows
specifically lots of people okay cool so
a starting point one motivation for this
is software today the process by which
we build software we build software
locally typically so we develop on some
kind of local device local laptop server
desktop machine whatever but we deploy
that software remotely so you push it
out to the cloud in order to actually
stick it out there so that other people
can use it and remotely is becoming a
slightly odd word to use there I guess
so some of these devices here on the
writer what would normally be referred
to as Internet of Things devices
anybody recognize any of them
yeah just shout out I'll repeat it yeah
so the scales is pretty pretty popular I
think that's some kind of Fitbit like
device as well there are things like
this that you use to track various
things you know you're into the
quantified self-movement for example you
track various data about yourself
there's a nest thermostat there that's
pretty sort of pretty popular I don't
know if I could say common yeah and
these are all Internet of Things devices
these are devices that have become smart
they need to run software ok so then
it's not just the case that we're
selling things now we're selling things
that embed software in them and some of
these things are becoming I guess the
word remote is not appropriate intimate
might be better so I think the two
things on the bottom right there you've
got an internet-connected pacemaker and
an internet-connected insulin pump I
think it is so these things perhaps you
would think twice about putting in your
body to connect you to the Internet
certainly in the light of recent the
botnet events there and so on so we are
potentially building ourselves a problem
here we're going to embed smarts and
beds software and all of these devices
and there's gonna be you know trillions
of CPUs as the slide says as the phrase
goes and they're gonna be embedded
everywhere in the environment about it
around us and all these employment is
going to inherit inherit the existing
system issues that we have so security
and speed are gonna be problems they're
going to be running general purpose
operating systems typically things like
Linux at the moment and the resource
footprint as well it's a problem there
these are operating systems that are
designed for server class machines
essentially and they're being carved
down and carve down and stripped down to
try and fit them into these these small
devices they're going to be connected to
the network but often because of the way
that networks now work because the way
the internet has evolved I they're not
actually very well connected to each
other so you often need to go out to
some kind of cloud service to come back
in to talk to another device on your
network it can be difficult to talk
directly among these devices and then
finally and perhaps most worryingly of
all and many of these things are going
to be deployed without any professional
management so it's going to be us or our
families or relatives or friends our
kids who are going to be deploying these
things and it's not clear that all of
them are going to be full
train sis admins I possibly in that many
people will not be fully trained to
seven minutes before they start
deploying this stuff so they're not
going to be kind of ready able and
understanding how to do firmware
upgrades and how to manage the
deployment of a set of devices and
ensemble devices in the home I don't
know maybe we'll we'll employ a whole
new set of plumbers who will turn up and
be able to do firmware upgrades but
maybe not so these are new devices it's
old software that's going to be
generational books and in particular a
number of these bugs are going to be
very very difficult to find and fix so
even if we find and fix the bug in the
software finding the device to apply the
upgrade to accessing the device to apply
to upgrade to is going to become
difficult
anybody see I only came across it
earlier in the week there's a paper
that's come out about hacking Phillips
hue light bulbs and being able to
remotely exploit a firmware bug to in to
install your own firmware on those light
bulbs once you're within range of them
the in the paper I believe the statement
is made that you once that's done the
device is bricked and you can't replace
the firmware you can't repair the device
by upgrading it again this seems
worrying this is not a good thing I
think so there I write these devices
going to connected they're going to be
well connected that can be unmanaged
they're going to be difficult to a
graden fix so that's the Internet of
Things that's the sort of terrifying
current and future world we're moving to
what has happened in terms of cloud
service deployment on the other hand has
recently been quite productive I think
at least there seems to be a lot of
interested in it's getting a lot of
ground so we have we've moved from the
idea that you have a monolithic piece of
software that you put out there and it
does everything
that's one big system and we've gone
through this process of taking that
monoliths those monoliths and breaking
them up into pieces excuse me splitting
them apart and ring so-called
micro-services architectures so you take
the components in the monolith you split
them up into pieces and so you end up
with a lot of smaller simpler pieces
rather than one big complex piece of
deployment deployed software and docker
is one of the technologies that's really
made that take off as an approach it's
really made it easy to do that this
notion of being out of build ship run
sort
we're anywhere and this is the kind of
way the way it goes about it right so
you have the doctor engine sitting on
top of your OS and it essentially
multiplexes the kernel to a set of
containers the containers are kind of
more than a process but perhaps less in
some sense than the VM so they've got
they're sharing a kernel underneath but
you tend to be deploying them in such a
way that you are you're running a very
small number of processes within each
container so each container is very
special purpose
very single purpose you don't have to do
it like that but that's the recommended
way that I think that that these are
deployed there's a sort of problem
though in these micro services which is
that it's really the tip of the iceberg
no matter how simple the piece of code
that you want to run is that's going to
consume those api's it's got a
dependency on this OS and the OS kernel
is a big piece of code right so I don't
know exactly how that was counted but
debian 5 is 65 million lines the Linux
kernel is over 25 million lines of that
whatever a line of code means it's a
substantial component there's a lot of
code involved in the current than its
kernel so you have this sort of iceberg
effect you've got this tip of code on
the top which is the code that you want
to run that's your actual application
it's going to generate you revenue or
yeah make you happy or satisfy your
friends or whatever it is you're doing
it for and then there's all the code
underneath that their operating system
is insisting you need so you need to
have a kernel there you probably need to
have Gigli PC but we need to have all
these are the large complex
well-engineered and so far really
reliable components but nonetheless lots
of code involved in them and so you have
this traditional software stack where
you've got lots of components underneath
including a kernel and you're only using
probably small parts of them so there's
only some pieces of those libraries and
some pieces of our kernel that are
actually relevant and your kernel may
have a number of file system drivers
built into it but if your application
it's not exercising those your
application doesn't need those file
systems it's not using that code it's
just sitting there in the kernel so
you've got this this effect that you
have a very very large complex software
stack of which which is tends to be sort
of always available by default you're
only using small parts of it and so the
claim would be and I think this is borne
out by
of continual recurrence of serious
exploits and vulnerabilities in a number
of these components that it's more like
this kind of cracking effect that your
code sitting on top like little boats on
the sea and at some point there's going
to be some kernel bug discovered or some
Lib C or I don't know open a safe bug
whatever it might be discovered which
means that people can now get into your
system and do bad things to it okay so
you have this effect that all this stack
that you've been relying upon simply
because of its size can become a problem
it can become a very contain
vulnerabilities that would be a problem
and the claim I think would be at least
the claim we would make is that the sort
of the problem here the underlying
problem here is the complexity it's just
the size is to so many pieces of code
means that you have such a complex
configuration for how you configure and
if you configure a full Linux install
now there's lots of things you can I
don't know how many options there are
now in a kernel build but there's lots
of them I know that much and there's
lots of configuration files that you
will have in on a running system you
know LS in stash Etsy for example
there's lots of stuff in there there's
lots of things running lots of different
ways of configuring all these all these
programs and so it's complex it's
complex to get right there are also
cases where duplication of the
functionality will lead to inefficiency
so you have lots of different layers of
multiplexing for example you might have
some kind of green threads use level
threads library you might have I don't
know JVM some other kind of virtual
machine runtime environment which
provides a threading notion as well
that's possibly multiplexed on top of
inside your container to be multiplexed
onto the kernel this kernel threads
involved maybe that's running inside the
virtual machine if it's out there on
Amazon for example and C then got Zen
doing some kind of scheduling so there's
lots of layers of scheduling involved
there in that case so you're getting
sort of lots of potentially
inefficiencies due to this kind of
multiplexing in fact many of the images
that are being used to support these
kind of deployments are quite large in
comparison to the actual piece of code
that you wish to run so you look at a
standard abun - image or a standard
debian image for example and they can be
substantial tens megabytes hundreds of
megabytes meeting gigabytes and so it
takes a little while to beat them take a
little while from to come up it takes a
little while to deploy them
and then finally this point I've made
already that we got more lines of code
me just means a larger attack surface
even if they're reasonably well
engineered and well trusted lines of
code that have been there for a long
time people still find bugs in them as
one of the profs in Cambridge describes
we still build code using 1960s
technology and he would like to see as
moved to 1970s technology which is to
say ml instead of C but there's still
there's old code sits around and it sort
of got latent bugs in it so overall it's
kind of complexity in these systems is a
problem and this was there yeah I guess
you'd say Rep well some people would say
revolution what goes around comes around
a lot of the technology nurses built on
ideas from the 1990s so this idea of
unique kernels so the Unicorn is the
idea that you're gonna take your
application and compile it and Link it
into specialized operating system that
includes only the functionality
necessary to run your code on the target
platform so you'd no longer have a
general-purpose OS that sits underneath
you try and minimize that as much as you
can you take your application code and
in in the compiler tool chain and the
linker tool chain you pick and choose
the bits of system functionality you
need and those get linked into your
application and you end up with some
kind of binary image that you can run
that essentially you can boot so in the
first instance of this that we built
this would take a no camera program and
turn it into either a POSIX binary and
elf binary that you just run on OSX or
Linux or it would turn it into a Zen
virtual machine image that you could run
with Excel creates the an config
whatever equals okay so you you're
trying to pick and choose among the bits
of system functionality you need and
Link just exactly those into your into
against your code so you end up with
this very minimal image that comes out
at the end so it's application code
being added to operating system
libraries producing this standalone
unicode there's a Wikipedia entry now so
it must be the determine we have taken a
number of inspirations here when we did
this work in particular the library
operating system work for those who are
familiar with it does anybody ever read
anything about or heard about exokernel
one or two scalped so these nemesis even
that was the Cambridge one so these were
library operating systems that came out
in the I guess the middle mid to late
1990s so the idea was that you convert
your operating system kernel instead of
having it running as a shared service
instead of doing the microkernel thing
and splitting it up into a number of
concurrently running services which
provide shared services just smaller
ones you take your your shared service
kernel and you convert it into a set of
libraries and then you link those
library against your application and
allows you to do things like accounting
resource consumption through
applications much more precisely because
you no longer have to account it to the
kernel you would get to account it to
the application those services were
linked against anyway unique kernels
take application code link it against
the libraries you need and then you end
up with this this self-contained image
it's not an intent of ours when we
developed unique kernels to reinvent the
general-purpose OS so we were not trying
to remake Windows or Linux it is this
notion of just providing enough just
enough system software to run the code
you want to run so its specialization
all the way down in sort of iteratively
specialized that these things the
application developer should be in
control of this process it's quite well
aligned with the DevOps approach so you
sort of the the developer is in charge
of the process as it kind of trickles
down and you get towards the thing that
you can a deploy in the end and in I
think all the ones that aren't aware of
anyway certainly the morass a unicameral
system with which I'm most familiar it's
open source libraries so it's taking
component taking system functionality
implementing it as a small service
specific library and then releasing that
as an open source single under some
liberal license typically is see for us
because it's very important here that
the system pieces of system
functionality are widely used and get to
be widely tested you only want to have
one of them you don't have everybody
implementing their own file system
everybody implementing their own network
stack okay these are these are complex
things to get right so put them out
there make it easy to share them so I've
mentioned a few times mirage has anybody
heard of mirage apart from me just
mentioning there ya want to see okay
so there's lots of others Niraj is the
one I'm most familiar with it's a unit
Colonel framework built in no camel most
of the Unicode frameworks tend to be
land language specific so you look at
something like Hal VM is that Haskell
one link is built in Erlang big include
OS as a C++ one
Viraj has been self hosted the website
has been self hosted at Mirage door IO
since 2009 that used to be open Mirage
to org but it was still the self hosted
website so this is a unique onal web
server that contains all the content
that's being served at that website
hopefully as far as I speak about this
those components that he used to create
that unique onal now distributed there's
lots and lots of small libraries so
they're not all in that particular unit
kernel but there are now over 100
libraries that we've got that implement
different bits of functionality there's
several from scratch protocol
implementations so we've done our own
TCP stack we've done our own key value
stores DHCP lots of these things I'll
talk a little bit about one of them in
particular which is the TLS stack so we
have a clean room intimate
implementation and TLS among the
benefits are that you have reduced
attack surface and reduce resource use
excuse me
and more predictable scheduling another
benefit of this is that you get to
relatively easily retarget your code for
different platforms because all you have
to do is reimplemented layer libraries
that you're depending upon so the
initial versions of Miraj would target
elf binary is running on linux or OS X
or Xen virtual machine images but we've
had various people do work for example
we've contributed somewhat to the Zen
port to arm so that in particular the
minimal support to arm so you can now
target Zen alarm as well as then on x86
with Mirage ena kernels we did some work
to get a prototype of a kernel module
API so by linking into this particular
kernel module you could essentially
insert your application code into the
kernel of the system you're running on
and so you can target quite a quite an
odd platform there perhaps the strangest
one of these is the target for
JavaScript so thanks to the old camel to
JavaScript compiler we could take
the colonel and again by linking in
including various libraries that were
necessary you can take your eunuch
colonel and compile it to running the
browser if you want to do that we don't
really have an application for that yet
but it seemed like an interesting thing
to do some quick numbers these are from
a somewhat older version of Mirage but I
don't think that they're on
representative so if you look at the top
left plot the memory size of the running
image is can be made a lot smaller using
Mirage there than even using quite an
optimized Linux paravirtualized
vm there if you look on the right-hand
side we had DNS server
these were sizes of the on disk image
that was going to be booted and if you
did if you use the bytecode compiler and
a dead code elimination you get these
down to hundreds of kilobytes for DNS
server web server and so on so you can
you could really optimize some of these
things one of the nice things here is
that we can take advantage of all the
work that's going into the old camel
language itself with some very smart
people working on that and so when they
come out with some new optimization you
just rebuild the unit kernel and deploy
it and usually it gets better in some
way smaller faster whatever one of the
interesting ones we did a DNS server
where we managed to with the first
implementation of our DNS server we were
only twice as bad as bind sorry twice as
bad as NS team as that wasn't great but
then again taking advantage of the fact
that this is a high level language it
was about six or seven lines of code to
ID memorization into the DNS server and
that brought us down to be that improved
the throughput I should say so higher is
better on that pot on the middle pot on
the right I'm higher is better so that
allowed us to do better than NSD and
considerably better than bind if you
want more information about any of these
results they've been published in
academic papers as a paper in
ACM ass floss 2013 that describes the
initial Niraj system and there's a paper
in USENIX NS di network system design
implementation in 2015 which describes
some of the later work we did to make it
much easier to boot
unique URLs very quickly system called
jitsu just-in-time summoning of unique
URLs
I said I'd mention the TLS stack so we
rebuilt TLS down we built a new TLS
stack and we tried to test this in some
sense by putting up what was called the
Bitcoin pinata
so the TLS stat was a new stack written
on a camel from scratch specifically for
use with unity kernels it's about six
months of code by two guys Hannes and
David and it's men art and a big caliber
the code size is less than about 10,000
lines so that compares favorably with
most existing implementations of TLS
almost all the protocol is written in
safe code so it's almost all written in
our camel there are a couple of bits of
C because you otherwise you end up with
a side channel through the garbage
collector where people can attack at the
encryption as it goes on it's not
vulnerable therefore to heartbleed
starbug's in an attempt to test this
rather than simply asserted although
it's it's not a an exhaustive test we
built a unique earn all that served up a
little webpage which explained that it
had embedded in that unit kernel 10 big
wallet that contained 10 bitcoins and if
anybody was able to break it they could
steal that that wallet of identifiers
and take the bitcoins this was up for
well over 6 months
there are certainly over a hundred
thousand attacks on it nobody took the
bitcoins some people donated some
bitcoins which was nice but nobody took
any we suspect this was people testing
if it was a real wallet if you think
about what was happening there imagine
trying to do that with the Linux kernel
module that you embed some bitcoins in a
kernel module you embed some bitcoins in
Apache and say that people come and
break this and you can take this it
seems like this would be again this is
not proving anything but it seems like
it would be more likely that somebody
might be able to do that there are more
avenues to attack that I'm a hundred
thousand attacks on this and it seemed
to do ok so I've talked a bit about
unicorns technology did that make sense
to everybody anybody at the point of
unique URLs yeah some people nodding
some people are not moving at all it's
late so what I'll do now is I'll talk a
little bit about where we're starting to
see uni kernels actually being used in
practice certainly coming into if not in
very wide scale deployment
certainly started to appear in products
so top left is a story if you go to
Unicode org by the way this is a site
that's trying to build a community
around the unit kernel approach so
there's listed then linked there about
ten maybe twelve now different unit
kernel projects and there's stories of
hearing there when people are starting
to use them practice and so the top left
is actually a prototype implementation
so this is from Erickson where they've
used mirage OS yuna kernels to build the
system that allows them to implement
network function virtualization on one
of their platforms the idea is that when
a packet hits the switch a unit Colonel
picks up that packet examines it and
then instantiates the protocol you can
occur all that's necessary to service
that request and then what's that
request is serviced once that that
packet stream has been dealt with those
unique URLs can be killed there was a
very early example of this in a in a non
nfe situation by magnetic a stud where
as part of the jitsu work where we had a
website where every URL triggered the
booting of a unit kernel that served
that page and only that page and you
could browse the website and unicorns be
popping up as you browsed and being
destroyed afterwards a similar thing
here but for something a bit more useful
namely processing streams of packets
flows of packets bottom left one cyber
chaff this is from Galois so they have
products that involve using xavier
munich kernels to provide essentially
many many fake targets inside a network
so if somebody didn't manage to get
inside your network and they're starting
to port scan to see what to go to next
what to attack next they get distracted
by having lots and lots and lots of
these little lightweight unique kernels
that had sort of thrown up his chaff to
slow them down and give them sort of
meaningless targets to go and hit the
one i'll talk about most i guess cuz i'm
most familiar with it is the docker for
mac and windows which i believe is in
name i'm general release rather than in
beta and so these are ways to use
doctrine on your laptop so there's a
unique kernel technology embedded inside
these some of the libraries that we use
in those products are from the unit
kernel world
so the question was really how to run or
the question this these products are
trying to solve this is the sort of
evolution phase I guess so rather than
thinking we have to throw everything
away and start again not to start taking
these technologies and slowly fitting
them into existing uses existing
deployment solving existing problems
with them so how can we run them as
containers seamlessly on RS x and
windows today that's been you know about
to date till about six months ago that's
been VirtualBox and a Linux virtual
machine so for de la with docker for mac
and Windows we can make that a much more
seamless transition much more seamless
usage of docker because we can translate
operating system invitations on both
sides between the container and the host
essentially so you end up with Windows
and Mac applications that just run
docker containers for those who've not
seen there will be a short delay what I
can't use keynote for those who've not
seen this by way of example this is
invoking docker to build my blog my
website locally as a jackal site so you
can see dock the docker run command here
which is sort of running jackal to
instantiate to create the blue create a
block build a blog and then serve it and
then hopefully localhost 8080 you
probably can't read but localhost 8080
on this device is now serving at that
site um that's not really that
impressive this is that impressive but
still if I move a new post into there
into the directory that's being built
then you can see the regeneration the
site occurs and so there's now a demo
post up there if I bought you all my
typing if I put something here I can
write that and again regeneration occurs
and that goes there so that's all the
inotify stopped working correctly
between the container and the localhost
filesystem which was beautiful
and so quick demo docker for Mac similar
things on Windows what's going on there
how are unicorns being used so the
virtualization is being used is using
hyper kit to the RSS hypo sx hyper kit
framework based on X hive which based on
bit FreeBSD is beehive it's sandbox
friendly so most the processes are
running as local non-root users there's
an embedded lightweight out based on
alpine Linux distribution it's optimized
for fast food and it allows us to
provide a self-contained docker app so
you can do drag-and-drop install the it
needs to have
I think pseudo permissions only to
install SCIM links so that you can have
the docker command-line tools being
served out the docker app and it need to
get auto updates and all these are the
good things so this is the kind of this
sort I don't get it mouse pointer okay
down at the bottom you can see the kind
of hypervisor framework is a mediating
between the OSX host the linux host then
inside the linux host there's a bunch of
containers and then there are some
drivers that manage I threw that through
that sequence a couple of them are sort
of specifically were someone interesting
so for networking we had a problem that
we had to deal with custom VPN software
on the host that made it difficult to
get the bridge to work because the VPN
would take over the bridge and then we
didn't get to use it so we produced a
thing called VPN kit which uses some of
the Mirage networking libraries to
essentially do reconstruction based on
the Ethernet frames that are flowing out
of the container into socket calls on
the OS X host so we're sort of doing a
reverse there's proxy or something I
guess you'd call it so we've got packets
flowing through we want to pick those
package so up turn them back into the
socket calls that they would have
generated them and then feed them out
through the OS X host and that means
that we play nice with the VPN now
because we're just more socket calls as
far as its concerned we also need to be
able to publish ports properly and we'd
like those to be exposed on localhost
without needing to go and work out what
the VM is IP addresses allocated as
interface and all that kind of stuff
that you'd have to do traditionally and
so again we can forward the container
port requests on our sect service which
then bind them properly on the native
interface so we've got control of the
i/o that's flowing
negative or flowing between the
container the linux host and the the
local OSX host file systems so as i
showed in that little demo we want to
transparency share arbitrary OSX
directories into the container so we use
a fuse forwarding layer and again we can
translate the Linux file system calls
into the OS X equipments so we can map
quite easily between the two worlds
between the Linux world sitting inside
the VM and the the OS X host
similarly for inotify
so we can have something that observes
in the host when I notify is being
invoked and can pass that into the
context into the Linux host which can
pass that then expose that there in the
standard way to the container I'm sorry
can have I notifying flowing correctly
through these this sequence okay
somewhat under I think that's a use in a
sort of a non-standard way I guess you'd
say so this is not you know kernels
being deployed as Yoona kernels this is
Unicode technology being taken and
embedded inside products one of the
things I'm involved with in the computer
lab in Cambridge in the computer science
department is I think although camel
laps so this is an organisation's
attempting to improve the state of the
art in terms of the tooling particularly
in a platform that a camel provides as a
language and so we have a block this is
a wiki a standard kind of media wiki
site
it runs hours for containers hosted on
docker cloud so we have a my sequel
container we have a media wiki container
which is PHP and Apache we have a TLS
tunnel to give us a TLS endpoint so we
don't have to trust the container that's
got MediaWiki and apache in it and we
have an HTTP to HTTPS redirector the
redirector is a mirage Ãºnico so this is
a you know kernel that's been built it's
a few tens I guess of lines of code that
is able to listen on the appropriate
port on port 80 receive connections and
send the redirect to port 443
so that it's getting redirected to tier
I'm in the process of turning the TLS
tunnel into something similar so that
then that can receive that can be the
TLS end point can receive the request
from the redirector and then can pass
the the request through into the
containers that are running Apache and
MediaWiki and my sequel this gives us
some benefits so the small piece of code
here that runs the redirector doesn't
have any other things running inside it
there is no shell to attack
there is no Lib C there is nothing it's
kernel it's just that piece of code
which is necessary to redirect HTTP
request to HTTP requests similar thing
will happen with the TLS tunnel it'll be
just that piece of code it's necessary
to listen on the HBS socket and be able
to forward request through onto
something out of its back-end into the
traditional containers receive the
results from now and Ford them back out
there will be nothing else in there and
it will be running your camel TLS stack
so we hope it'll be less vulnerable to
attack in terms of how this happens
this is currently a bit of a hack I have
to admit but it does work so we have if
I could type a build script which does
some stuff basically what we're doing
here is we're pulling a base container
we're linking some code we're using that
base container to build another
container image which will contain the
unique kernel and nothing else there
were some invitations of docker and all
sorts of stuff going on there you can
look at this repository if you're
interested ask me afterwards not tell
you the name of it or indeed there we go
that's that's where the repo lives so
that that dot build on run again for
good look
that dog build command built a container
image will do when it finishes no yeah
and if I then run that container it also
build the run script helpfully if I then
run that container
I can issue occurs the localhost on the
standard HTTP port and I get back they
read redirect from that
so this is a this is the piece of code
that's doing the outcome of the piece of
code that's doing that and if I look in
source there's lots of stuff there which
is not important this is the
configuration so it's not very long it's
mostly only because it provides a few
command line keys this is for handling
the command line keys all this code and
then down here is the actual
configuration that goes on so this is
saying the Unicode is something that's
going to invoke a particular module and
a camel speak that modules going to need
to provide it with a clock and an HTTP
server we also when we start the thing
off down here we need to provide it with
an HTTP server there which is
constructed from a network stack which
is a generic ipv4 network staff that has
TCP available to it it's not terribly
interesting that's the other thing
called redirector so this is the actual
code that's doing the redirect so they
goes just over a screenful there so
we've got something that goes
redirection we got something that acts
as a server and then we've got an entry
point which actually kicks things off so
it invokes the server and tells it to
serve using the redirect function I'm
not going to bother trying to explain on
camel syntax for those of you know it
hopefully that is not too offensively
bad as code goes and for those who don't
know it at least you can see it short if
you've got questions about that again
ask me afterwards so we can make it
hopefully simple to build unique kernel
containers there's a lot of other
infrastructure that would be nice to
have around that so it'd be good to be
able to tag these differently and be
able to use them and be able to
reconfigure them perhaps as part of the
build process so a bit more control of
that would be good but at least services
are as a quick working example of the
sorts of things that might be possible
to do with them and it sort of serves as
an example of the way that docker is
moving into all these different
architectures so it's no longer just
Linux containers on x86 you can have
different arca CPU architectures being
supported x86 arm power etc and
different operating systems the windows
container support is been announced
recently Windows height of e containers
and as works in progress to build bring
other things into this
fold the idea will be you can cover all
of this space using a single tool chain
by way of has anybody tried the multi
arch support in doc of the Mac know
anybody it's quite neat I suppose so
if I run this particular that's not
available if I run this particular image
the new name will report that is running
on an arm even though it's actually
running on here
and so that's invoking using pin format
misc and qmu in the background to make
things look like an arm in this case if
I could remember the container image
that might be able to do that for
PowerPC as well I can't remember it's
not my head but it's sort of a seamless
way of being able to do development arm
development for example using an x86
laptop you don't have to set up a cross
compilation and such things in the same
way as you might do currently to do that
to make that work which is nice so this
is I guess this is the conclusion so
this is what we're heading towards
hopefully building shipping running
software without really needing to care
too much about where it comes from so
then it's containers windows containers
are already supported and soon we hope
to get unicameral supported as well and
the containers they're the things that
have been managed to the things in the
sort of chalky red outlines so
traditional Linux containers you've got
applications running in containers that
will share a kernel often sitting on top
of the hypervisor because that's the way
the cloud works and windows contain a
similar thing but with Windows and then
a hypervisor but now hopefully with Yuni
kernels we can bind the OS more tightly
against the application and simplify
that picture further and have things
using just the resource they need and
not having to use all this other stuff
but they're never going to
they never actually invoke and this is
all turning into containers of this
service platform for those who would
like to hack on some of these components
we open-source some of them at oh s con
this year so hyper kit VPN kit and data
kit which are used in the dock of a mac
and windows products are also now
available for people to take and use and
build upon themselves if you go to the
docker github organization
listed there as repositories and with
that I might say thank you and ask if
there are any questions if I can see
anything okay I've got I think one over
there on the left that's what dr. for
Windows allows you to do yet
if I hadn't broken the fusion virtual
machine I've got one here I'd be able to
show you that by running doc of the
windows inside the EM the infusion
on-site in on this in the end I think
that should be possible that's not
something we would support right now
okay it's one that
so the question is what's the link sorry
for not repeating the first question the
question is what's the link between this
the things I've shown essentially the
unicorns I've shown in the small devices
so I think there's kind of two there may
be more in the future the future is hard
to predict it turns out the the first
one is that thanks to the multi arch
support it's possible to set up build
processes and build pipelines for doing
I owe t device development using a
slightly richer environment like this
because I can make this pretend to be an
arm and I don't have to learn about
cross-compilation so we hope that that
support might be might make it easier if
people who want to build those images to
start with and secondly yes we would
like it to be the case that we can start
to compile the unique thermal images to
run on those devices we've done that so
far in terms of devices that have enough
capability to support Xen because we can
just target center arm so I have an EU
project for example where we've done
that fairly extensively to build mirage
unit kernels that will run on cubieboard
2 sand cubic trucks because they have
the necessary hardware to to run Xen
easily we had a prototype in the lab
where somebody had ported their camel
runtime to boot directly on a Raspberry
Pi which unfortunate doesn't have the
necessary hardware to runs then so that
would be a I think a small matter of
code to extend that sufficiently so that
Mirage could then target that and so you
could have a single image then running
on your Raspberry Pi which was a mirage
Union colonel but all those things that
that thing is still a prototype so it's
not something that that's that's
straightforward to try yet to try it and
but certainly targeting Xen on arm is as
easy as any other mirage target ok
so the question is is it not
contradictory is there not some sort of
mutual conflict if you like between the
approach is taken with docker and docket
containers and unique kernels so I would
say no I would say that you know kernels
are on a spectrum that traditional Linux
containers at one end of and docker is
trying to make it easy to build and run
all of the things on that spectrum so it
might have started within its containers
but now it's supporting all these other
things as well
yuuna kernels kind of take what you
might think of as a container approach
where you build a container you have a
docker file that specifies what's in it
and sort of just trying to push that a
few steps further so that you're now
invoking your language specific
compilers and linkers in order to build
the output image so you can maybe
leverage some of what those tools can do
to get smarter about what goes into that
image and be sort of strip it down even
more but it's not fundamentally opposed
to the motion of having containers the
idea of showing that oh camel labs
MediaWiki site was to try and say you
can take an existing deployment of
containers and you can start to insert
unique URLs where it makes sense with
that you don't have to throw everything
away and start again you can sort of you
can add them into existing deployments
potentially does that answer the
question maybe any other questions so
into any of unit kernels I've built you
can't because I didn't build SSH into
them so there is no legislative support
we do have a no camel ssh implementation
although it's quite old now and as a bit
rotted and if that was brought up to
date it would be possible - I think that
might even be one of the pioneer
projects we've got on the Mirage wiki
and it would be possible to build to
compile that library into your uni
kernel and then you'd have SSH support
but right now no any further questions
okay in our case we don't thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>