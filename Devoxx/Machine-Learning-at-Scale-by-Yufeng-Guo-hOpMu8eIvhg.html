<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning at Scale by Yufeng Guo | Coder Coacher - Coaching Coders</title><meta content="Machine Learning at Scale by Yufeng Guo - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning at Scale by Yufeng Guo</b></h2><h5 class="post__date">2017-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hOpMu8eIvhg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right how's everybody doing today
welcome to day three I guess day four
depending on how you count it of devoxx
my name is yu Fang I am a developer
advocate at Google and we're gonna chill
a little bit see if any more people
filter in feel free to move up lots of
seats everywhere but thank you for
coming we're gonna be talking about XI
learning how to scale it in the cloud to
make it something that's useful beyond
just training for trainings sake right
oh I made a model so so yeah first I
want to make sure we're on the same page
in terms of defining what is machine
learning so my super simple fide version
of what is machine learning is that it's
using many examples to answer questions
that's it
I think that's yeah got it down to five
words it's in all the long different
kinds of definitions I've seen I think
it's up there I'm on the shortest words
because I mean I'm kind of running out
of words to use right and in this quest
you the important point here is that
needs to answer questions you can have
all the examples in the world but if
you're not using it to answer questions
and you're just training a model then
why are we doing it you know at all so
that means there's two sides to machine
learning and both are equally important
on the training side is where we do all
those examples get all the data cleaning
up loaded in wait for it to train way
towards the Train still waiting for it
to Train and then we get to the
prediction part that's the fun stuff
right that's where it's actually useful
to your users and so that's the
answering questions step and you can do
both of these steps on your local
machine right you can train it and then
you can run a little command line and
then see oh look at that my accuracy is
awesome it's over 95%
but then you still need to solve the
problem after having gone through
collecting the data cleaning it up
loading it in training training training
the last thing you want to be caught
doing now is let's go figure out
infrastructure scaling making an
endpoint quotas so go to the cloud what
if that could take care of that for us
so that's kind of the overall narrative
that we're gonna go through we're gonna
use tensorflow
I assume since I put it in the title
that you guys at least have heard of the
word at least once and before we go on I
wanted to take a poll real quick show of
hands who has either run a red light or
knows someone who has run of the red
light all right that's so so everybody
who didn't raise your hand look around
okay now you've all known somebody who's
run a red light
great so now that we're all in the same
ballpark of of knowing people would run
red lights have you ever thought about
how they caught you we ran that red
light some amount of you probably got
away with it right but some of you
couple days later sitting around check
the mail or what is this from from the
State Department oh and of course you
got a traffic ticket how did they catch
you was there a traffic cop standing
there on the corner saying oh there goes
a car light still green there goes a car
light still green all right lights
turning yellow someone's running for
he's gonna make it oh he made it
oh this guy didn't make it well I mean
let me use my eagle eyes my goal Legolas
eyes and and catch his license plate as
these zips down the road and then jot
that down in my little notepad did that
happen to anybody by any chance so
traffic cameras are kind of like machine
learning right there they're doing the
things for us that are repetitive there
did we lose the we lost traffic cops on
the way I suppose but now they're doing
more useful things right the computers
are going through all these images and
they're figuring out which ones are
running red lights getting the plates
off
for them and you might have a human step
down the road right to verify things
make sure the pictures are you know
being detected correctly so that story
aside back to tensor flow so the tensor
flow is a open source library for
building graphs computational graphs
it's particularly good at doing machine
learning but you can do it for other
things as well you can build these
graphs to just do math addition
multiplication more complicated things
to and it's at its core is C++ and over
that is the execution engine of a
cross-platform system that is still
being added to more and more and then
distribution and then over that is the
various api's and the one we end up
interacting with most is Python right
but even that Python API is a low-level
API you're dealing with the individual
nodes of a graph you're saying all right
put a 5 here and then multiply with the
input over here and then link it
together
so there's higher level API is built
into tensorflow
there's the layers API that helps you
build entire layers at a time the layers
of your network and then if you won't
even go higher than that you can build
entire basically internet works at a
time and just configure your training
all your different tuning variables and
you can choose like learning rate and
optimizers and whatnot right and you can
still go higher we have canned
estimators so that will take care of
choosing all those things for you to and
that's we're gonna focus on today
because it's a easy to use and it lets
us focus on the actual story of scale so
let's walk through the kind of canned
estimator this algorithm that we're
gonna utilize today a little story for
you
it's almost as exciting as the traffic
light one so let's say you know we're in
Silicon Valley here you you're starting
a hot new startup you got a great idea
you're gonna predict what people want to
eat they're gonna type in chicken and
you're gonna predict oh I know you want
chicken pasta or you want chickens and
waffle
right and so you start out and you say
I'm just gonna launch it right it's all
good I'm gonna match what they say just
do word search and well as you might
suspect the results were less than
spectacular because sometimes they don't
want just anything that has to work
chicken in it so then you go on to
creating your v2 you create a linear
model a linear model is great for
memorization it's great for tying
specific associations and so your app is
getting some traction it's doing pretty
well but you use there's a pickie right
it's memorizing so every single time
they say chicken they are getting
chicken and waffles because your
training data happen to have a lot of
chicken and waffles in it so sometimes
people say I want some Italian food and
they don't want spaghetti and meatballs
every time so you try to generalize you
say I know I'm gonna use a deep neural
network because that's all the hotness
right now and what's great about a deep
neural network is it helps you
generalize what it's doing is it is kind
of casting your data the data that it
has into a higher level higher dimension
space where it can say shrimp fried rice
and chicken fried rice are kind of
similar in its own kind of space right
so that's you know 816 dimensional space
to your choice really and so you can see
that there in the visualization but no
good deed goes unpunished sometimes
they're too general if you say you want
I can never remember this an iced decaf
latte with nonfat milk then you want an
iced decaf latte with nonfat milk so you
still need to be able to memorize a
little bit and generalize so would you
both this is a model known as wide and
deep it combines the ability to memorize
with the ability to generalize and this
is it was a paper that came out of
Google research and
is kind of a decent starting point for
any sort of structured data a problem
you have because you can just drop in
the pre-built model and just train it
against your data and because it both
memorizes and generalizes it's fairly
robust you have a selector at the top
that basically chooses which side to go
on
depending on the data coming in on that
prediction
so that's what we'll build today so
that's end story time our data set is
just as exciting as building a food app
it's census data yeah okay
so we got about 32,000 training examples
and the task is to predict whether or
not given various demographic data
points somebody will has a income of
over 50k a household household income
over 50k this was from the 1994 US
census so we got columns like age and
education and marital status
you got relationship gender capital
gains occupation exactly the things you
would expect to see the point I want to
emphasize here is that we have both
categorical data and continuous data and
so as a broad kind of division of those
two those two kind of have different
treatments if you have something that's
continuous let's say like capital gains
where it's just it could be any real
number versus something where you have
very distinct values no 11th grade 12th
grade college you know PhD et cetera or
you know single married divorced widowed
then you you have to treat those columns
of data differently those features
differently and finally what we have our
last column which is our income bracket
of literally just it's just a string
that is the greater sine 50 0 5 0 K or
less than equal 50 okay and so we'll try
that gets transformed into just a 0 or 1
for predictions purposes so 0 will mean
less than 1 will be higher than so back
to our little architecture pseudo
architecture diagram we have the
training in the prediction because we
only have only
32,000 rows of training data it turns
out that it can run on my laptop in a
reasonably decent amount of time and so
what we're gonna do is just have that
training happen locally what's neat is
that on the cloud side you can take the
output that model that's trained from
your local machine and push it into the
cloud so we'll show you how to do that
and that's that's the cool part is that
you can mix and match at your leisure so
I've got here a ipython notebook for
those of you guys not familiar with that
it's basically Python in a browser and
you can have these cells which you can
choose between markdown and code and you
can run this code in the browser so the
first thing we'll do we'll do some
imports will CH you know make some
arrays or lists of just the string names
of the columns in our data and we create
a function here I'm going to kind of
move through this part you don't need to
get every detail here mainly because
like the focus here is on scaling the
training so I'm just gonna kind of walk
through this the this second piece here
is defining a function that will map the
CSV data to something that the model can
understand so it's simply creating a you
know it's called an input function so
it's literally just mapping those inputs
for us the next thing is creating
feature columns now remember how I
mentioned there were categorical columns
and continuous columns so when you have
the categorical columns in tensorflow we
it's referred to as a sparse column
versus a dense column for continuous so
for the sparse columns what we're gonna
do is we have two choices if we know
exactly the values in our data set that
align with that column in this
particular case the gender column
happened to only have male and female as
values then we can define them directly
we can use sparse column width keys and
is this big enough for folks I can make
it thater
font is okay you can see it not for yes
and if you don't know or don't care to
enumerate all the different string
values of a column then you can just let
tensorflow do it for you so you do it
with a hash bucket and you can choose a
hash size of based on what you expect
the values to be just so you don't have
collisions and so this saves the actual
tensors that's what these kind of
wrapper functions do they create just
places for the data to flow through
later and so what will combine these
together in a moment let me just run
this cell and then we do the same thing
with continuous columns continuous
columns are straightforward they're just
real diode columns that's all you do and
then we can also choose if we wish to do
some transformations we can combine some
columns together and you can also alter
the columns little some of the regal
value columns if you care to bucket eyes
them let's say age right you say you
know I actually I want the model to be
aware of the fact that you know 18 to 25
is an age group I care about then 25 to
30 through 30 is another issue so you
use this boundaries argument to do that
so that's also a nice little feature so
that turns your age a continuous column
into bucket eyes age which is
categorical oh and then we have these
cross columns where you combine two or
more categorical columns into kind of a
super column and so if you know ahead of
time
for example that education and
occupation are probably related somehow
and you want to basically say this and
this then you can you know kind of do
that ahead of time and again we see that
hash pocket-size so the intuition there
is basically you can combine the or the
orders of magnitude of the two existing
columns so that's why some of these are
bigger than others and then did I run
this so it did so then we can finally
pull them all together what we have here
is all those variables from the wide
column just in a giant array and you can
see here we have the actual categorical
columns up top followed by age buckets
education occupation these are all our
cross
it's right below it then we have our
deep columns right because we said we
want to do wide columns and we have the
deep columns the deep columns you need
to do this embedding we talked about how
a deep neural network can kind of place
things in this higher dimensional space
to see what they're related to that's
what it's doing with categorical columns
so you see here we have all our
categorical columns again these are the
same variables as we saw above in the
wide column section and then we can
choose a dimensionality this value is
kind of something you can tweak I'm
choosing eight because I'm running on a
Macbook and I don't want my CPU to blow
up so we do that and then finally we
have these which are our continuous
columns from earlier those real valued
columns so combined them all together
into one giant array so now we have wide
columns and deep columns okay so we're
ready we've set up all the data we've
created all our columns and I ran that
cell just creating a model with the
higher level estimator can decimeters is
super easy it's it's all up in this
section and I have there's three going
on here and you can choose between them
so our wide and deep is at the bottom
it's a deep neural network linear
combined classifier but if you just want
a linear classifier that's there too and
the arguments are pretty much just the
same thing with the deep one requiring
the hidden units which are just how many
nodes do you want part layer so the
number of values there is each layer so
that highlighted bit it's two layers
first one has 100 and the second one has
50 notes and so when we combine them you
know I happen to choose 170 50 25 and
the other arguments are literally the
same so this function just it takes a
string argument to specify which kind of
model type to use just to make it easy
to rerun and so but you know if we were
to say let's create me a wide and deep
model so this creates us a model but it
doesn't train anything right it's just
all that son of we did
all right everything's set the board is
set and then finally we can train it the
training is literally as simple as
taking the variable we had was just M
that we saved the model to and doing m
dot fit it's so short that it's easy to
completely overlook it so you have fit
it's gonna run 32,000 rows of input for
a thousand training steps the batch size
that happen to pick what's 40 in this
example the data is local but I also
have it hosted in the cloud for
downloading if you wish did I run the
yes the notebook is running okay
I want to make sure it is actually
trying to do something so this is done
great so see this is why we didn't put
in the cloud because when you put it in
the cloud there are definitely certain
overheads you got to spend up your Brya
virtual machines load in the library
load in the model if you have multiple
machines you got to do it for all of
your machines then network them together
then charge your data to each of them
and then hope that they come back with
the answers so we did one deep we did a
little evaluation here which is as
simple as you expect right m dot fit
followed by m dot evaluate and once
that's done once we have a train model
we run this little ditty to export it
this is fairly new code in terms of
libraries and there's kind of this was a
bit of a struggle to get working but now
that it's here you can take it in and
use it so a lot of it is reproducible
like easily translated to different use
cases so what we're doing is i have this
little utility function that's just
mapping the type between strings and
floats so some of the data are numbers
the continuous ones and some of them are
strings in a lot of cases those are kind
of what the only two data types you have
strings and numbers may be events also
and so I just had an array at the top
that was these are the kind of columns
so if the column is categorical return a
string type and what this function does
here serving input function the reason
we need this is when we want to save our
model for training
cloud the kind of final mile the last
mile that you have to solve the problem
you have to solve this mapping future
inputs when your users are saying hey
you got me a prediction for this data
you need a map that to something that
the model can accept and that can
sometimes be different from the
functions you need it for training
because in training that input function
that we wrote at the top that pulls in
your entire data set figures out
batching and does the training loop and
it's essentially a generator that spits
out data in large chunks so this allows
you to you basically define something
very similar but you have the
flexibility to define something a little
more something different if you wish and
so one argument is for a the actual
inputs into the graph the the tensors
and then the feature placeholders are
the values as presented from kind of
from the web so let's say it's a JSON
request and then it converts to tensor
and then you can map it how you like and
then finally there's just a export saved
model so again you see that M making an
appearance so we did end that fit we did
end up value eight finally we can do m
dot export safe model this outputs a
file that looks C where did I put it
yeah so we've put it in exports more
than okay so it outputs something like
this it's a saved model dot Pb Pb stands
for proto buffer basically it's a
compiled binary that represents the
graph and then we also have a variables
folder that contains the trained weights
and what we do is we literally can
upload those you can literally upload
your model directly to well this is a
failed attempt for the first time but
this one is the the same folder that
we're seeing and it's just those files
it's just the saved PB
and if the internet will cooperate and
that would be wonderful
oh okay so this is tiny probably okay so
there you have it again right the save
model so we put that in cloud storage so
this is Google Cloud storage it's just
blob store in the web in the cloud and
then what we can do is we can go to the
Google Cloud machine learning engine
it's a mouthful you can just call it
cloudy melon Jan Oort cloud ml and
create a model a model in this kind of
system is simply a wrapper for versions
versions are what are interesting
because that means it what it is it's a
version is the actual model and models
is just a wrapper to help you group them
together probably could have come up
with better names there but so we can
create model let's say we just create
one for for now just to show you so
deadlocks
and we create so they literally create a
model all you need to do is supply a
string name of your choosing
so that just creates a model but of
course there's nothing there right all I
did was enter a string that's not gonna
do us any good so what you want to do is
go into your model and create actual
versions so when you create a version
let's go to this one you give it a name
a version name so you could say you know
V 1 or V 0 dot 0 dot 1 because you're
really really not sure so version name
should start with a letter and
apparently you can't use dots so let's
just call it that and then you give it
that path to the folder that we had
before something like that so you can
either browse for that or they think I
need GIS and random tip here because
this tripped me up really bad when I
first did this you need the final slash
and if I would paste it correctly that
would be great but you do need the final
slash so what I found out was when I
click through the UI I was like duh
it's like oh this one and then it's like
hmm there's a slash so you need that
slash if I delete the slash it will
complain if I add the slash it will work
and this is I get returning on
dictionaries okay so put the slash in
and then you can click great so now it's
now it's actually putting in a model
right and that's a in operation right so
there's a hole if I if you go back to
the one I already had in place there's a
whole list of operations here right
creating models and whatnot then you can
look at the details of that and you can
see for example if I said oh I want to
create another version that points to
the same folder then I can go here and
pull that out right we have that
location again it should be noted
however that the folder is not where
it's reading from when it's serving
you're doing that when you create it and
it's gonna copy it out so you can delete
it you can put other stuff in there it's
not gonna automatically pick that up you
want to do a new deployment so this way
you know it it's kind of pythonic in
that way right you tell it exactly what
you want and it doesn't do unexpected
behavior so I guess the the moment we've
all been waiting for it is actually
seeing it do a prediction so if I can go
past all the LS as we did doing a
prediction
what we had was there's a census that
JSON you can see it's you know
reasonably responsive this is doing two
requests at once it's doing two
predictions you could of course just
have one at a time and there's a REST
API that this commandment was calling
but this is like a useful tool for
development I have here so we talked
about my all creation yeah so this is
kind of the way you would represent the
inputs it's kind of similar to a CSV but
you know it's the JSON version of that
and this these are the two that we put
in and then so when I ran that this is
kind of what we see over here but this
is bigger you get back a couple of
different values in the API form it
would come back as JSON and so you get
the actual logistic value
so these are the values that correspond
to this column on the right and then you
can see that the probability is this
first one would basically be predicting
it's the zero right it's the 0th value
and this one's predicting you it would
be the one position index one and so
that's why these values here are 0 and 1
you go flip them next time so it's not
looking like it's a ordering or a list
of the indexes but it's actually if I
had more it would just be like 0 1 1 0 1
0 and you can do batch prediction as
well I'm not going to do a batch
prediction live here because batch
prediction is really meant for when you
have larger batches of data not 2 or 200
but maybe like a thousand or more
because we do that it really shows up as
a job and that incurs with it all those
spin up costs it spins up multiple
workers loads up your model in all of
them as well as the necessary libraries
and then it's shards of data across them
and then it runs the model predicts and
then it comes back together again and
the UI for this is kind of neat in that
you know you're not stuck on the command
line the whole time once you've executed
the job you can go in and you can see
some details about it and they have a
link that go takes you directly to the
logs so so that's always nice to you
know just to keep tabs on things and oh
yes and with batch prediction you get a
file back or files back rather than just
on the command line so with instances we
just said predict me this okay
break me that okay got it with batch
prediction it's really thought of as a
job you it's maybe something you run
once or twice a day and then you can
have these results they just put them
back into cloud storage for you at a
location of your choosing and let's see
this is the JSON version if you did
instance so it's the same exact thing as
you'd expect and earlier I talked about
how what we did this kind of model where
we trained on the local machine and then
we did our prediction in the cloud and
that we have them basically this models
this saved model dot PB file
which is being used as the intermediary
and because tensorflow is open-source
you can basically load it to wherever
you want and you can kind of flip and
swap them as you wish
which means you could also if you wanted
train in the cloud let's say we actually
had a massive data set and you had a
long-running training that you wanted to
run so you can flip these now why would
you want to flip them the reason is
because on the prediction side maybe we
could replace this laptop with a phone
you could have a train model deployed in
a mobile app for instance that is
running locally on a device rather than
having that device call out to a
prediction API if you wanted we could do
both so you could certainly have a fully
cloud on both ends but you have the
option allottee to design it differently
if you want because you have this
intermediary model between training and
prediction and so I think this kind of
realization help me understand a little
bit more about like how you can leverage
this architecture a little more and and
it's a lot more flexible than perhaps it
looks at first because it's like Oh put
it in the cloud like oh no no my stuff
is in the cloud but if you wanted to
train locally because your data was
sensitive and you don't want to upload
it to something I understand that right
or if you had a GPU farm at home in your
basement for some reason and you did all
of the training in there and then you
said gosh my startup is really taking
off I can't I don't want to also spin up
an entire data center and and host this
myself or even like get a bunch of VMs
write the wrapper code to wrap the model
to serve it right the you know API
gateway and things like that and just
throw the model over the fence and it
works right so thank you for coming I
think we're sort of I don't know how the
schedule thing is is playing out it's
they're late yeah yeah so maybe we can
say some time for for questions or if
there's anything you want to see more
detail especially in that code run
through that we kind of glossed over
happy to do that I know it's I'm a quiet
day but we have a question awesome
exactly right so the API is is also it's
a REST API yeah
it's a good question so the question is
around using the prediction service for
your model for images because everyone
likes to predict whether or not there's
a kitty in their picture so with images
what you would do is you would basically
for encode it and so that the the
request would then just be a serialize
string and what that would mean though
is in terms of thinking about your data
right and and how that's being fed into
the model your model needs to know how
to read that through properly so that
would be a fun example to work through
as well so thank you other questions yes
so the question is around the size of
the data set versus the size of the
model created the data set really
dictates how long it takes to train a
model the model itself you can think of
it as there's some description of the
architecture of the model of like okay
here are all the places where you know
this is the structure of it the data
flows through this way and then the
weights define you know the various
multiplications and additions and
whatnot that happened in that model but
how much data you pour through and the
training that happens that updates those
weights inside it's still the same
structure with the same number of
weights those don't change no matter how
much data you put into it so that output
model is entirely dependent on the
architecture of the model itself not the
data that is fed in the amount of data
that is fed in right so some models are
quite large one example if you're
familiar with the inception model which
is what Google is kind of vision the
wide vision model that trains on the
scale of about eighty four megabytes
when it's done the alt when it's all
said and done like in terms of the total
size of the model file and the weights
now I actually gave a talk about this at
Google cloud next was that two weeks ago
specifically on training the cloud and
putting it on a mobile and there are
ways to compress it down to about 1/4 of
that so now we're talking about 20
megabytes for what is a really large and
sophisticated model so it's kind of an
upper bound
and those techniques are evolved around
turning your weights into kind of
quantized values so instead of having
one zero point one one five three and
zero point one five four maybe those are
the same value right so so it turns your
graph a little fuzzier but by having
lots of values essentially be the same
it allows it to compress so it turns the
the specifically terms to 32-bit
floating point numbers to 8-bit floating
point numbers and you don't lose too
much accuracy amazingly enough that's
the the unexpected result it's because
neural networks are designed for fuzzy
and puts in the first place so you can
get up get it get away with it which is
nice
other questions
yeah so so what we saw the first
question around fast predictions right
because you know you're kind of handing
over your model and hoping that it works
and so we saw it I mean now it's warmed
up but the first time we ran it I you
know the entire talk I didn't run any
predictions right so everything is spun
down to essentially zero and over Wi-Fi
and then it comes up because this is you
can think of it it's running on one
worker node right so the model is kind
of warm a node is kind of warm and as
long as you get some requests every few
minutes it'll stay warm and as you get
more it'll actually scale up for you
and there there are also ways to
manually set it but with manual if you
set the number manually it won't scale
it'll just hold it there so if you get
you know tons of traffic it's not gonna
go anywhere but it also means that you
kind of are reserving yourself a number
of workers but in general and especially
when you're starting out using the
automatic scaling seems to work fine
yeah so that that's one of the things I
wanted I'm in the process of making is
something to kind of load test this and
then give it a give it a run for its
money right and just start it throw it
throw a request at it all day long and
seeing seeing how how hard it can get
hit yeah
so the question is around whether or not
there is a overhead incurred when
loading a model into a mobile device so
with both Android and iOS and I guess
also Raspberry Pi there's kind of
dedicated runtimes for each that are
also kind of compressed down and then
when you you would package it into an
app so it would be loaded at applo time
so then it would just always be there it
wouldn't be every time you needed to
make a request it would be there I mean
you could you could write the code
however you want the one that I did in
in my talk was a I have a camera scanner
and I trained it to recognize different
kinds of peanut butter cups like candy
as well as like one stick of gum
randomly just to for variety but it
could like recognize like Reese's Cups
and like it's like Justin's peanut
butter cups and like milk chocolate
versus white chocolate just like the
packaging right but ya know the fun
little thing but it's a scanner right so
it's continuously I think I was having a
poll once every second so it looks like
a camera app but then there's an overlay
that displays what it what it does I'll
probably add a link to the slides
since the time like as a point of
interest yes one more okay ah yes the
pricing I am no sales rep so oh yeah
also just for sad thing with the output
model where they go okay the files like
when I said like gonna make the batch
ones dump files out like it's just the
text file with the values in it so it's
not something crazy just wanted to let
you guys know that okay let's do yeah
let's go to Google clouds machine
learning homepage hey pricing so this is
for training this top portion and
there's different tiers for the training
portion because you can the basic tier
is literally just one machine and what's
also worth pointing out is you can train
locally right when you're just
developing you should training locally
if you have a huge data set you should
make accurate kind of some-some
representative sample put it on your
local machine and trained locally
because you're trying to get your code
working you don't want to be pushing
your code up and then waiting and
waiting waiting and comes back he's like
oh the library couldn't be found that
you wanted to use it's like great like
Oh syntax error you you indented wrong
it's like no you don't you don't need to
be you know the development cycle should
be fast so that you should do locally
and then we were ready to go to the
cloud then you can do your whole data
set or half our data set and you can
maybe start playing with different tiers
of compute so the standards here and
premium tier the reason why they look so
expensive I think standard is something
on the order of 4 for 16 core CPUs all
wired together and then premium is I
believe oh no it's 10 and 100 I think
visit but we can look at the details
it's a bunch and then the basic one is
just one and then there's also the GPU
tier where there's a K 80
attached to the and then on the
prediction side what's neat is like I
think maybe this is the side you were
wondering about I guess both are
interesting between zero and a hundred
million requests per month it's ten
cents per thousand requests and you know
that includes all the auto scaling and
stuff and then plus 40 cents per node
hour so that's like if you're just like
running in a whole bunch like each
little snippet kind of adds up so it's
kind of the present story yeah
so the question is around whether or not
this $0.40 is in addition to the node
that you have running so you know how I
when I made a request I just did it and
then it just came back right
that's not your node I didn't spin up a
virtual machine in Google compute engine
and say run it here for me it's more
similar to like App Engine or something
like that but that just like handles all
of the infrastructure under the hood for
you because all I did was provided a
file or a folder with files in it and so
this is actually just it's 40 cents per
node hour and it's not saying like whoa
you did one request that's one hour
right it's it's per little snippet
billing and then so yeah
so let's see multi-part question you're
the the basic GPU tier is one machine
with a nvidia k80 attached to it right
now we're working on getting other types
of GPUs in as well and then with the
other tiers especially the the more
quote unquote expensive ones you can
it's just the CPU but if you wanted to
do networked distributed which is using
distribution tensorflow yep you can use
this custom configuration and so there
you can choose I would like 33 CPUs
please like and I would like half of
them to have GPUs and the other half not
it's is literally just whatever you can
dream up we can do it for you these are
basically just aliases you can think of
as like oh you just want to be able to
pick one of these you don't really want
to think about that like you can but if
you if you want to pick something
specific by all means I'm gonna jump to
the actual pricing page because you guys
have lots of questions about it speaking
of lots of questions about it other
questions about it or anything else
alright well if there's no other
questions I have tensorflow stickers up
here for your laptops if you want if you
have other questions that you want to
follow up with feel free to swing by and
yeah happy to get in touch so thank you
again for for coming I can chat with you
more about pricing effort</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>