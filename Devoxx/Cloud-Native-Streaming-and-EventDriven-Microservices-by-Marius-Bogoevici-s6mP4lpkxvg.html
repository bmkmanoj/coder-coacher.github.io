<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud Native Streaming and Event-Driven Microservices by Marius Bogoevici | Coder Coacher - Coaching Coders</title><meta content="Cloud Native Streaming and Event-Driven Microservices by Marius Bogoevici - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Cloud Native Streaming and Event-Driven Microservices by Marius Bogoevici</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s6mP4lpkxvg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right
lights are on I guess we can get started
thanks a lot for attending this morning
and joining me for this session where
I'm going to talk about a couple of
projects that we develop in spring
portfolio spring cough stream and spring
called data flow mainly and I'm gonna
try to talk a little bit about how they
came into existence what is the
philosophy behind them what do we want
to achieve with them but then it's gonna
be more organic and I'm gonna try to do
this demo driven we'll build they're
actually gonna build this applications
and deploy and run them to actually see
what the developer experience is right
so my name is Mary smoke ovitch I've
worked for pivotal I'm I've been a
spring integration contributor for a
long time I've been working for in the
spring XD project for the past three
years and over the past couple of years
sorry and I've been in the I've worked
in Spring Club data flow and sprinkles
team since its inception so essentially
that's a long title that we have up
there and I'm gonna try to get very
quickly to the points that's kind of
it's our to our thinking around this
problem of high throughput low latency
data processing right which is generally
known as streaming so essentially the
goal is when we talk about streaming
you'd normally talk about processing
large quantities of data as fast as
possible and usually the term is brought
up in contrasts to batching right
instead of actually storing data for
accumulating it actually process it as
soon as it arrives and we've arrived
there through a few pathways one of them
is basically just because we can
batching has been a long time on like a
very the most efficient solution for
processing data but capabilities both in
terms of storage computing power memory
and so on and so forth have made it
possible to actually bring the moment
where the data is processed closer and
closer to the moment when it's a quiet
right the other pathway is this more of
a enterprise integration meets big data
type of scenario where you get like
instead of having for example sporadic
events car has broken right you get
preventive maintenance you get a series
of events that actually the application
process is continuously and tries to
infer and tries to predict where the if
a failure is incoming and try to take
action for before that right so in these
types of scenarios what I'm trying to
say here is that it's not only the data
volume that matters is the fact that
it's data that's coming in kinkiest
right and it's data that relates to each
other so you process for example a
stream of sensor information you
actually want to money there are not
only individual values but you want to
monitor trends you want to basically be
able to process data in the order in
which it has arrived you want to process
data in in aspect like that has been
acquired in a specific type type time
window to actually identify a trend in
there and we've seen basically
transitions like we've seen a transition
even in in terms of messaging middleware
design so like more traditional crabby
thank you for example or active in queue
or like your traditional enterprise
messaging system starts being starts
giving way to new these lines more this
distributed log type of this line like
Kafka for example or cloud managed
solutions like Google pop solving
Kinesis and our choice in terms of micro
services is not necessarily driven by
the let's break the monolith
kind of approach there's no model is to
break but these types of high-volume
data processing solutions lend
themselves inevitably to distribute it
approaches right to a distributed
architecture so in a distributed
architecture we find that building these
distributed topologies that consists of
production-ready
applications that can be deployed
independently they can be developed
independently by teams that can be
organized organizationally aligned which
is basically the definition of
microservices is the way to go and
obviously there are other requirements
that drive us there we break the
application in multiple parts because
the multiple steps of the pipeline might
have different requirements I certainly
like compute-intensive
unit might require multiple instances a
certain data enrichment unit might
require multiple might require more
memory and so on and so forth so the
ability to tune this these parts
independently lends it's a very very
well to the micro service model and
obviously we're processing like
processing data that's arriving in an
indistinct units so message like the
messages the unit of processing is kind
of a as a very very natural choice here
right so when we talk about these these
topologies they can say they there are
they're constructed from micro services
that talk to each other via messages as
opposed to the more traditional a HTTP
or I wouldn't say even HTTP like request
reply kind of a approach you get
something more that's more similar to
the UNIX pipeline where basically every
component processes data and passes is
downstream without waiting for a reply
and that gives you a number of benefits
like the decoupling discoverability you
don't need to know where the next step
of the pipeline
you don't know who it is you don't care
it could be multiple applications that
process the data if they have an
interest into it in parallel so you can
actually have these like very
sophisticated pipelines just because you
don't put any knowledge on the producers
line and who is your consumer if it's
available right now if it can process
and that also brings us an interesting
idea which is that at the end of the day
when you look at let's say business
driven events streaming right event
sourcing the notion that an application
notifies the other parts of the topology
if something happened and they act on it
and they eventually become consistent
with the with the starting point is
structurally maybe not functionally but
structurally is no not necessarily
different from let's say pure data
processing pure analytics right you can
take a look at the two like the two
images and they're almost the same right
applications communicating over
publish/subscribe channels and basically
having disability of adding new
components into the mix and of course
there's a topic of cloud native right
turn that's very popular these days what
do we mean by cloud native in this
context we really mean that when you
have a distributed topology when you
have this distributed topology that
consists of independent applications
these platforms like all the platforms
that are on the page and if and others
right they build on top of that have
this have this ability of managing
resources of monitoring your
applications state of scaling allocating
resources given like managing the
resources that go to a process like
memory CPU and instance gallons
obviously to a degree or another
so you can do things for example in
kuala foundry that you cannot do in yarn
or you can do things in kubernetes that
you cannot do in mesas and so on and so
forth so there is a degree of which
these capabilities exist in the
different platforms but as a rule we try
to take advantage of what's being
offered to us as a function of the
platform and we try to tailor our
deployment strategy and our application
design towards that as opposed to kind
of making that a byproduct of our
deployment strategy and you'll see that
when we're going to talk about the about
the application so let me run a quick
example which kind of should give you a
quick idea of what I mean by this so I
have Kafka running on my laptop and I
have a data flow server and I'm gonna
start the shelf
the shell is one of the ways that you
can actually interact with the spring
cloud dataflow server right we're gonna
talk exactly what it is and how it's
built and what's happening underneath
but what what I really really want you
to pay attention to is this notion that
I can go and say I want to deploy a
stream that receives data on an HTTP
endpoint let's say nine eight and
ingests it and writes it to a file that
is
oh sorry it's an instance so
basically sprinkle of dataflow manages a
fleet of applications that are
registered so basically what happens is
you register some artifacts and labels
them and you associate them with a
reference to an artifact that exists in
a in a repository like maven for example
so we can actually do a bulk import
which is what I'm gonna do now and since
I'm gonna use Kafka I'm gonna import the
variants that use the kofta binder so
now if I go back to apps you see this
entire list of other applications let me
see if I can increase a bit so you see
all all kinds of applications in here
right and the ones that I was trying to
launch our HTTP which is the HTTP source
so we have an artifact and there is a
file sink it's a little bit down there
so I'm gonna create this and what
happens if I monitor the processes that
run you can see that I have my data flow
server running and I have two new two
applications that are executed the file
sync and the HTTP source and if I were
to send some data
basically it's sent to that HTTP
endpoint and I will see it in detail
this
right so I can see it ingested in the in
the file and I can say I love that box
again and I can see it ingested in the
file and that's not exactly a very
interesting use case for what it does
it's a very interesting use case for
what happens under the scenes under the
scenes you see basically two separate
standalone applications as I said the
phallus in Kafka and HTTP source Kafka
which are launched by the dataflow
server based on a blueprint that has
been given to it the stream definition
right and they communicate with each
other over Kafka right and they exchange
messages and that's basically built out
of the building block so let's see what
how these two things are actually put
together let me destroy the stream first
I'm going to destroy the stream for
example I should see that the two
applications are basically gone so the
server basically manages the life cycle
of these applications and locally it
means that you've runs their separate
processes when we were going to move to
the cloud you'll see that it actually
launches applications in in the target
platform so the three projects that make
the spring collide cloud dataflow egg
ecosystem are sprinkle off stream stream
call tasks and screen cloud dataflow and
each one of them performs a specific
task
sprinkle stream is a programming model
for writing event the event driven
applications that make the that are part
of the pipeline so the HTTP endpoint and
the file ingestion endpoint are
basically spring cross stream
applications sprinkle task is the
programming model for writing
short-lived applications batch jobs if
you want and sprinkle of dataflow is the
layer that basically orchestrates them
and deals with running these
applications on
the different target platforms so let's
start talking about sprinkler stream
first it's basically it one one more
point that I'd like to come by which is
is interesting here is that they by
themselves have been the result of
breaking a monolith not sure how many
how many of you are familiar with spring
XD for instance not very not many spring
XD has been a project that has tried to
create a platform for data processing
using spring batch and spring
integration and at some point around
2015 we came to the conclusion that we
had the right idea in terms of things
like abstract stream definitions our UI
management ey the unified monitoring and
everything we're great but there was an
approach that actually didn't scale
quite well which was defining our own
containers for running the modules it's
not an accident we actually moved
towards a micro service architecture
towards 2015 when spring boot and spring
cloud really become mature and sort of
validated the notion that you can build
production great micro services in the
spring like me the spring ecosystem
right so we kind of moved towards that
as well and this is basically the result
of that approach and of course recall
stream builds on top of spring boot is
usually built on top of spring
integration there's often a question
what's the relationship between spring
costume and spring integration it's
being negation obsolete or something or
something like that it's it's a question
it's a wrong question Springs
integration is performing exactly its
role that it has which is to provide the
enterprise application integration
patterns that are used inside the
application to provide the connectors
for middleware and there is an entire
set of applications that are built with
spring cloths treatments bring
negation that address integration with
different middleware systems in fact all
the portfolio of out-of-the-box
applications that spring cloud dataflow
comes with like even the even the file
sync or the file source or the Cassandra
source or the Cassandra sync sorry
or the gdb C source if you look there's
an entire ecosystem of applications that
are built there we provide them out of
the box to be used with but that they're
actually spinning the creation
application
what does sprinklers what role does
sprinkles stream play it provides the
application model it provides a unified
API for writing applications that are
scalable and structured in distributed
topologies so that's our focus
everything you will see in terms of
features are really geared towards that
right and that means what it means
pluggable middle abstractions I don't
want to write my application knowing
what messaging system I'm talking
through why is that because I care more
about how did how did how two
applications communicate with each other
then the pipe that they're communicating
through so we take upon ourselves to
abstract out certain aspects of the of
the middleware for the benefit of
actually having this uniformity between
applications right it works for a
certain class of problems it works for
the majority of problems that we are but
we are trying to solve obviously you can
always go to a lower level and try to
use the integration in the middle of our
integration directly if that's what you
want but when we talk about apps talking
to each other or that kind of approach
then there are other things that you
have to consider how the messages are
are for example marshaled in a way that
they can be they can be consumed on the
other hand transparently right without
me having to know exactly what encoding
they have how did it be how they mean
serialized and so on how to distribute
data between multiple applications right
this is comes more into the realm of
let's see opinionated primitives what we
call I mean we're going to talk a little
bit about what they are a spring
complicate stream application in a
nutshell consists of a application core
it's a basically a middleware neutral
core it's really you just describe how
your application interacts with inputs
and outputs it can be done in different
ways it can be done through a spring
integration has a spring integration
application you can do it through like
pure spring messaging constructs and we
actually have support for for reactive
programming as well since version 1.1 so
you're gonna see immediately how that
works and obviously the goal of this
uniformity really yeah so all these
applications basically have interact
with the outside world with a number of
through a number of predefined inputs
and outputs that describe that are
basically connected to the middleware in
this kind of uniformity allows them as I
said to be structured in in these like
very large distributed topologies right
where they focus this pluggable model
that you get from from other realms like
circuitry or something like that and
they're designed in such a way that the
goal is not to be able to talk to each
other but sometimes to talk to
applications which are not string boots
and sometimes to talk to applications
that are not even java at all
central to this is the binder
abstraction so the binder essentially is
a component is the heart of the
framework it basically when you program
and we'll see exactly how that that
works you actually describe your
channels you declare them and then you
let the binder which is a SPI really an
implementation that's specific to that
middleware to come in and create those
channels and inject them into the app
and connecting to the to the messaging
middleware right and their controls
through spring boot properties
so really the programming model consists
only of adding an enable binding
annotation with a number of channel
specifications and a binary
implementation on the class path and
that's pretty much it and the binder is
like we have a number of out-of-the-box
implementations for binders we have out
of like production ready right now in
release mode our Kafka and dravot in
queue
we have under development support for
JMS so active and Hugh and solace and
Google pops up right so we're looking at
other implementations if possible but
these are our existing right now in
different stages of development so let's
build a let's build a couple of
applications that actually interact meet
each other I'm going to create a new
project what's being initialized there
so you can actually create it from start
spring day home let me call it source
sensor choice
okay and all I have to do it really is
to specify that a version of the binder
that I want to include in this case is
going to be streamed Kafka it's very
different directory the whole example
already built like I have a version
already build of it
but rather go through at least to the
process of creating this application so
that we can actually take a look at how
what happens so basically to turn a
regular spring with application into a
spring cloth streaming application
really is a matter of putting enable
binding annotation and adding a number
and adding a number of interfaces that
describe the channels that the
application you're going to use and spin
call scene comes with the with a number
of out-of-the-box interfaces one of them
is a choice which basically declares
that it has only it's an application
that has only one output channel right
it's basically kind of the start of your
pipeline and so we're going to say this
application is a source you know we want
to send some data to Kafka so how are
you going to do that we're gonna say
well I'm gonna use spring integration
for this I'm going to say inbound
channel adapter and the channel the
target channel is source output so you
can use this interface actually in two
ways
the interface is actually instantiated
as a proxy by spring cloud stream and
injected into your application so you
can all the wiring and get the channels
directly as like you know type safe
manner or certain spring integration
components for example refer to two
channels by name so they also registered
under that name as beans
so I'm gonna make this ass worse I'm
gonna say well I want to send some data
and I'm domain-driven design developers
so I don't like sending raw date I want
to send them and send domain objects
frame I want to think in that terms so
I'm gonna say right and that's perfectly
fine okay new sensor data and I'm gonna
say sensor data would be an object that
carries temperature and a an ID so I can
create it here I can say private
okay so now I'm going to have a
constructor as well so that I can create
it's easier and gonna say in a Simpson
random data really not very efficient
but it's good for a demo okay and then
that's all fine but where does it does
this actually data actually go so by
convention this goes to topic were
rabbitmq topic exchange or you will pop
subtopic whose name is exactly identical
to the name of the channel but you can
actually customize that so we can use
spring boot properties for it so you can
say spring cloud Stream findings output
sensor data and there's another thing
here that's kind of interesting you see
I'm sending pojos I'm sending Java
objects how do you how do these objects
actually get sent how do they get put on
the wire well that is a bit interesting
because the way I put them on the wire
dictates how what what degree of
interoperability these applications have
I can send them for example has
serialize data I mean not Java
serialization or not you don't want to
do that but you can use something like
cryo for instance right the drawback in
that case is that you have to have the
class on the other side and basically
every time you change the class you have
to update the code and the other end and
so on and so forth and this is exactly
that what you don't want to do with
microservices so you want to send it in
a different format right so we can say
something like spring cloud stream
bindings output content type for example
the application JSON and that will
basically put the data on the wire as
Jason right close to you end up with a
Kafka specific property for example to
force the partition count to a larger
number so that we can do some scaling
later on so I'm gonna run this and it
runs and you don't see doing very much
right
and I can create another module there's
actually receiving this data right
okay this was the same process
okay this is gonna be a bit simpler I
just need to declare okay this has this
is a different this is a receiving
component right so it has to have an
input channel and just as I have one for
the source I have one for the sink which
is a interface that declares a single
input channel here so I can say okay
with the data that's coming from this
sink just block it and I'm sort of
assuming that I'm getting a string over
the wire
I'm gonna get to get a little bit
fancier in a moment
Oh
the other thing that I need to do here
is to specify what the destination is
right so the nice thing that that spring
boot gives us in this case is that it
allows these properties to be over in my
runtime so if I deploy it in an inn like
for example when Club dataflow deploys
them it actually overrides these
properties okay unfortunately I don't
have a very let me do something else
for a moment I run this in the command
line
I'm going to skip the tests then let me
get the quicker
never skip tests and you see basically
some data arriving the JSON that's
actually been written by the previous
application right so that's pretty neat
but actually it will want to write a
little something more elaborate I want
to do something more interesting than
just moving data around in different
formats so maybe I should try to process
this data in a different way
how about calculating some averages
right so in this case I'm just gonna
head to one of the applications that
actually is is already written which is
this sensor average appliquè sensor
average calculator right and what's
interesting about this application is
that unlike this sensor receive
application that processes each
individual message as it arrives so
basically that's how you declare your
processing strategy and you describe a
method that gets invoked each and every
time you process a message from that
input channel this calculator actually
uses a reactive programming model it's
not necessarily reactive in the way it
interacts with the messaging middleware
we'll get there at some point we just
wrap the current spring integration
channels but what this allows you to do
is it allows you to reason on the maybe
I should increase the font a little bit
it allows you to reason around the the
processing of the data not in terms of
as I said individual message being
handled by actually a flux of messages
coming in and to declare functional
transformations over that flux of
messages and to compose these functional
transpose to compose these these
functions write in a functional matter
so you see here an average calculator
it's quite a beast in a
certain sense but once you get a hang of
it it's quite understandable what it
does is it windows data with an interval
of 20 seconds with a sliding window of
10 seconds so basically every 10 seconds
you get data from the last 20 seconds it
groups it by sensor ID and then it
applies a reduction function to
calculate the average of the data over
that interval of time right and as I
said it's kind of complex but on the
other hand once you get used to it it's
much more it's much easier to reason
around the transformation compared to
the case where you would think around
this in like if you would try to write
this by storing messages somewhere
calculating how much time has spent and
so on and so forth it's so much easier
so much logical right so we've adopted
this programming model for ourselves so
let me just try to plug in and run this
average applications so what I'm gonna
do is I'm gonna configure this average
calculator to get data from the sensor
like from the sensor data endpoint and
pass it to averages and another thing
that you notice here is that when I
describe the data that's coming in I'm
not describing it as a string I don't
handle the transformation the the
unmarshal link of the data at all I'm
really handling the I'm leaving to the
framework to deduce that it's Jason and
extracting from Jason how does that do
that when this application actually
sends the message when the when the
source application sends the message in
this format and declares this was here
it is and declares the content of the
application JSON there is an internal
protocol that basically attaches a
header to the
message right and the way it's attached
it depends on it depends on the the
middle way like some messaging
middleware support headers some others
just taking the case of Kafka for
example we just envelope it in inside
the message right but still on the other
end on the receiving end you get a JSON
object with the information that it's
it's application Jason and you can it
can extract it and that allows you to
use different that data format so for
example you can do the same thing for a
ver we have support for a ver we can
just declare that your data is utilize
with a ver and automatically is gonna be
converted like that on the receiving end
if you have a ver on the classpath it
knows how to extract it and convert it
so now let me change this application
real quick actually have stopped it so I
can put its like here I can launch it
with
so I'm going to launch the logger again
and I receive no data of course I don't
receive any that because there is no one
actually writing it so let me start this
application
Oh
so this is in a different binder
and in about 10 to 20 seconds I should
see some data coming in here
there we go so you see this is basically
the component displaying the average
values and you know I kind of see that
they're correct because they're all
floating around the value of 50 right so
that's all good but what happens if I
want to scale this up right so if I want
to scale this application up I can for
example just launch another instance of
it but if you remind if you remember
from the from the what it was talking
earlier all the connections in spring
classroom are pops up so if a large to
instance is I need to make sure that
they are both part of the same group
right so basically if I want to do that
then I can declare this property which
says they are part of the group sensors
and if I run two instances for example
of sensor average application
you know you can see basically that when
I started the second instance
unfortunately this is very very small
you can see that basically Kafka
reassigns and reallocates the partitions
to the two applications it really
transforms them in competing consumers
that's a feature of Kafka on the other
hand if I'm looking at the results here
it's gonna become slightly confusing
first as you see the two applications
send data based on the data that they
get from the sensors but there is a mix
of of basically of values there is an
overlap why because the original
application sends them blindly out there
it doesn't know like it basically does a
round-robin sending of data across all
the instances there are downstream so
that's exactly what not what I want
right I really want to what I really
really wants is to be able to control
and make sure that all the application
all the data basically goes to to a
specific instance I don't want to
control to which instance it goes I just
want to know make sure that it's
actually put together so I can put here
on a property which is called partition
key expression and where I can figure it
like that
I can tell it basically tells the
producer that it should sense all the
data with the same payload ID but with
the same I hold the sensor data with the
same ID to the same target partition it
could be basically in case in the case
of Kafka it's the same topic so I'm
gonna restore this
and I should see no overlapping here
because the data has actually been been
sent correct so basically this kind of
this kind of control over how data is
distributed across the wire over how how
multiple consumers basically participate
in in the same structure is part of the
features of Spring cloud scheme you get
it out of the box with the framework
right so you see right now that it's
like data is more it's it's correlated
better right you don't get these
overlaps that you used to have before in
the previous example so I'm not going to
belabor the point too much with the with
the slides basically they kind of show
you what you've seen so far I'm just
going to be informative for a visual
representation of what happens instead
I'd like to move to the second topic
which is just as important you've seen
you've seen spring clog data flow right
initially in the in the previous example
what does it do it basically takes these
applications that I've just written and
it orchestrates them it allows you to
provide these stream definitions ESL's
that describe how these applications are
chained together you've seen that I have
to specify for example information about
the topics that are being used even
information about the mic they say the
middleware configuration if I wouldn't
be running a localhost
so that's pretty tedious when it sits
when you actually need to when you're
required to do like to deploy a large
topology by the same measure launching
individual applications you've seen that
basically for launching two instances I
actually had to go to the IDE
basically launched two separate
instances and manage them and monitor
them and I'll do all that so instead of
this kind of approach dataflow basically
what it does it's basically taking the
definition that that you you give it to
it like HTTP PI file that we had in the
original example it has a registry of
applications and it finds them and it
launches them where does it launch them
on a platform it's has an abstraction
layer which is basically the string
cloud deployer project that has
different implementations for a number
of targets so essentially it's being a
recall data flow like the orchestration
layer that transforms the DSL and does
the launching doesn't exactly care about
what the target platform is every spring
Cloud dataflow server instance obviously
comes with an implementation of of the
target of the target platform right so
there's an implementation for Cloud
Foundry for example if there is an
implementation for yarn there is an
implementation for kubernetes right it's
an implementation for missiles I think
there is an implementation for openshift
right now so someone in the community
has written one right so it's basically
it's it allows you to I mean someone has
written for example one for four Nomad
right it's very organic it's growing
very organically and very makes it easy
to take let's say the high level
concepts are around orchestration and
basically try and kind of port them to
to a platform of your choice right I
mean what you get essentially when you
execute that stream and you when you
deployed it is this graph of
interconnected spring boot applications
right that's all you hear and sprinkle
deployer basically supports different
types of resources right it doesn't
support only
uber jars it also
supports darker images and different
locations for resources so you can store
them like it has it can use a maven
reference if you have a maven repository
you can use docker hub you can use for
example if you deploy in kubernetes HDFS
if you're deploying in yarn so it's it's
an entire variety of things that
actually hides under the scenes so that
spring cloud dataflow can actually
concern itself only with a high level
high level of abstraction right and kind
of not meaning to belabor the point too
much but essentially when you deploy
this is an example of deploying a couple
of streams and what happens is that you
get these boot applications talking to
each other over the message broker right
and it doesn't matter what the platform
right thing eventually yes in terms of
externally interaction it has basically
a restful api that is used like the
shell that i've been actually
interacting with earlier it has a a web
UI the dashboard that you've seen
earlier as well and obviously because it
has a REST API you can write your own so
very often like users just write their
own groovy scripts for example that
invoke an HTTP endpoint just perform the
rest will call passed the JSON for for
deploying a stream and that's it for
automating your process and that is
completely decoupled from the deployment
strategy right where the deploy SPI
actually takes the job of deploying it
into the target platform and basically
being since we've gone so far because
probably we should probably go and
actually see a deployment into an actual
cloud right because you know local
machines are dangerous for the code but
even more dangerous for their for
running applications so if you don't
want to do that you want more security
and
I'm gonna do is we're gonna deploy in
Cloud Foundry so I have this account of
BW s actually you can get one and try
the same thing yourselves and I already
like to save some time I've already
deployed the Cloud Foundry version of
the dataflow server and I've deployed a
number of also before the talk I've
deployed some versions of the of the
applications that I've just demo in a
maven repository right so now these
applications have been built with a
rabid binder so what we're going to use
in this exists in this example is some
the same spring club data flow sprinkles
few applications that you've seen
running with Kafka running now with
rabbit without any change than just the
dependence the binder dependency so
because this is running in the cloud
they actually get a pretty nice URL
right and again I can see I can see my
applications running here I can see the
like I can see for example the source
the sensor source being deployed right
and having this maven URL associated
with it
the average calculator should be here
somewhere
there it is and the sink is actually
somewhere as well in any case we're
gonna go and create a stream and this
time just because for some variation I'm
going to create it from the UI
so I'm gonna say sensor gonna Paulo
since or I'm gonna take the three
components I'm gonna drag in and connect
them right this makes a stream
definition so as you see here I have no
applications running then except the
server and I want to create the stream
did you call it sensors again and I'm
not I don't want to deploy it yet
I'm just gonna create it right and if I
go here I see this stream already
created with the three applications that
I've just deployed piped together right
and obviously it's not very obvious that
they are not that the framework actually
takes care of connecting them like the
pipe symbol basically means that the
framework will provide that framework
and by the framework I mean screen clock
data flow will provide the information
what's their common topic over they
offer which they talked because you've
seen the earlier example that actually
I'm do that man I'm doing that manually
and this part of the application but
that's what happens actually we
configures them so that to use some
common connectors and with the stream I
can actually deploy it and in deployment
properties I can actually enter all
kinds of properties that configure the
applications that run but more
interesting I can say I want two
instances of sensor average and I want
three instances of the reporter and I
logged data they crazy right and I want
the sensor source
position to use them
I'm gonna deploy it it takes a bit of
time it says failed but actually it
means just that the status hasn't been
updated yet
and you can see the three applications
basically being started and deployed
right and you also see that I get the
number of instances that I have
specified and I get the I get if I had
for example I could specify other things
like memory for instance and so on and
they're starting right now so probably
we're just going to be in time - they're
not crash they're just they're starting
slower and that therefore they have
their reported as just their wine
you know let's just see you either
basically reports them as until it
updates the state so basically I have
three applications running and if I were
to look at the logs I would see the
basically the average data flowing
through right and the salient point that
I was trying to make here is that the
orchestration layer they all flow has
actually taken the application and
you've seen the application basically
that I've started deploying at the
beginning of the presentation taken to
the cloud configured the platform
actually takes care of their health if
they were to fail for example for any
kind of reason it would restart them so
you get this this monitoring for free
right from from all these platforms
they're deployed with the required
settings in terms of count and as I said
you can set other things like memory and
so on and that's an important point
because when you actually start like
deploying three applications is easy
deploying a pipeline that has ten
applications each one with so and so
many instances is complicated right so
when you think of it like that's pretty
much the the moral of that of the focus
of this presentation is like you think
about large graphs of applications and I
hope I showed you how we try to solve
this problem with spring cross stream
and spring cloud dataflow for data
processing my time is up unfortunately
so I think and if you have time for a
question or two but if not then I'll be
at the pivotal booth and I'll be happy
to to talk to any of you who have or
have questions and want to follow up on
this so thank you for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>