<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning to fly by Krzysztof Kudrynski and Blazej Kubiak | Coder Coacher - Coaching Coders</title><meta content="Learning to fly by Krzysztof Kudrynski and Blazej Kubiak - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning to fly by Krzysztof Kudrynski and Blazej Kubiak</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UOVT9T64kXI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is what a Kubiak my
name is Christophe kudrinsky and this is
a drone it is in the box and the word
consists of two groups of people people
from the first group will immediately
take the drone out and start to enjoy
flying while people from the second
group will immediately download the API
maybe even make their own better one now
there is absolutely no shame to be part
of the first group I have real friends
and that would do this and they are nice
people myself and was a we are proud to
represent the elite after over 2 years
of experience with the drone we have
absolutely no idea how to fly that thing
so if you worried about us between 9:00
and 5:00 p.m. we work on extremely
exciting project in TomTom image where
the main idea is to enable cars to drive
by themselves and our main contribution
here is turning a set of images and the
point cloud of lasers into a consistent
three-dimensional representation of
roads available globally after work we
spend our time of families what they
would wash myself with mine and then
when normal people start to think about
meeting friends and going to sleep we
dive into the world of technologies and
we have similar interests computer
vision robotics virtual reality and
artificial intelligence around midnight
watch a walks to sleep and I sleep to
walk this project is driven by passion
it was created during time instead of
meeting friends and instead of sleeping
and in this project the drone will learn
to fly and it is learning that skill
based on what it sees and it will see
the word using a camera so the starting
point of our project is a calibration
gel board so you can make a lot of
photos of that and then use simple
mathematical algorithms to find the
model of distortions and remove them
from their camera in fact almost every
project in computer vision starts with
calibration chess boards and then you
can practice with detecting chess boards
on the screen and then tracking chess
board in the video these are the hello
words of computer vision however if you
are here to see the hello world example
it might be a little bit disappointed
we cannot disagree that detecting chess
boards and tracking chess boards have
great applications so useful in real
life ranging from detecting of freaks
still playing chess on the chess board
up to detection of people walking on the
street
with a chess board and the list does not
end here
however our inspiration is a bit
different what you can see here are the
fragments of videos from the web of
drones racing and amazing to fill that
speed and when you realize that these
drones were actually controlled manually
there is one word that is screaming in
your heads I'm sure and it is
irresponsibility
let's analyze one case this is a frame
taken but drawn during some race and the
task here is simple what control needs
to be applied to fly safely and there's
a lot of time to apply that control to
think about there's so many frames but
if you are starting to worry you are
doing the right thing because although
there is still hope we can make it you
know exactly how is it going to end if
you analyze this case again you will see
there was so much time to react
similarly to any other cases where
which ended in a silly clap crash while
there were tens of frames ready to
extract the proper information tens of
frames begging for salvation
together with was a we decided to put an
end to that the user should only click
when they want to fly and it is the
machine that should calculate proper
low-level controls to safely fly towards
that place and today we will all have a
walk through all the steps you need to
consider while implementing that on the
machine and we will show you our
examples how we did that we will show
you our test not from the ruins like
that not from the forest and and parking
lots but from our modern TomTom office
built and opened this year but if you
look at this carefully you cannot miss a
similarity especially if you look at the
right perspective we have absolutely no
doubt that this building was built and
meant to be our training ground so let's
talk about our solution and for our
inspiration let's again analyze this
simple image we all have to agree that
we humans are amazing
everyone here immediately and without
any effort is able to spot and track a
safe place worth flying to everyone here
immediately and without any effort is
able to intuitively plan your next
actions everyone here immediately and
without any effort is able to understand
the environment spot the collisions and
act accordingly what is even more
amazing this is already amazing but what
is even more amazing is that at the
exact same moment using Ziggler exactly
same brain we humans are able to plan
our next meal and perform deep
considerations about life but general
intelligence like that
is not yet possible according to the
science so will not talk how to teach
the drone to cook or understand life but
we will think how we humans would
analyze these three tasks and how to
implement it on the machine
and because the machine can be perfectly
focused and absolutely dedicated to
these three tasks then by just adding
enough processing power we will quickly
outperform humans so these three steps
will be the plan of our presentation and
the plan of the development of our
system so let's start from the beginning
in order to start the project you can
use whatever equipment you want as long
as you can control it from your software
we started with a parrot ar.drone tool
which you can communicate over Wi-Fi and
there is plenty of open source available
in the web for that we used open source
Java framework called
yeah drone it's under Apache 2 license
and from the hundred thousands of lines
of code from a drone in this project we
used one the move command in move
command you need to say what field needs
to be applied to accelerate the drone
and then Ascension and rotation speed
and these three parameters will be
derived directly from the image which
for convenience will be decoded using a
C++ ffmpeg bit either from the internal
camera of the drone or the external
GoPro for example and then this decoded
image will we will analyze using Python
libraries like open CV or new PI and
everything will be orchestrated in the
beginning that was the plan in Java so
this simple architecture the simple
design is a starting point and you will
see it evolving while our new ideas
approach and why while we are developing
the system but you can think this is a
nice start to start think about the
logic and the first thing in the logic
is the first level of autonomy which you
want to apply which is
tracking and because we are fans of
human brain we would like to think how
we humans would analyze this task and a
nice example would be when we go to the
club full of people and we find
something catchy something that catches
our attention for example a heart but
for a start let's imagine something more
familiar imagine that
but you must imagine it really hard
imagine that instead of the heart we
have a chess board now this is not this
does not happen really often but if it
was the case we would have a nice
algorithm ready to accomplish our task
however sometimes it happens that you
have to track something else and it is
this particular moment because you don't
know what you are going to track you
know it when you see it and this is at
this particular moment that this defined
region gains some description it might
be the distribution of colors inside it
might be the shape of the heart itself
or we may be directly tracking the flow
of pixels in the image and all of these
approaches can be easily implemented on
the machine and the good news is that
you don't need to do it on your own
because somebody else already
implemented that and it is available for
example in the library called
open city so in open City we used min
shift algorithm so min shift algorithm
is just in simplification it establishes
distribution of color inside a given
region and then shifts and scales that
region to keep this distribution of
colors constant time shift is very
similar but it also is able to apply
rotation and then optical flow measures
locally the motion of pixels around the
specified object from these algorithms
optical flow for us gave the best
results so let's see how it works in our
case so what you can see here is our
graphical user interface so this is the
frame seen by the drone in our office
and you can see our automatic controls
applied to it so the bottoms on the
sides will show what till the arrows or
on the bottom shows what still needs to
be applied from the left and right arrow
on the bottom shows what how height
needs to be changed in order to make
that and then this will be in this
moment it will be all zero because we
are not yet avoiding obstacles in this
level of autonomy so let's focus on the
circle in the median middle which is
what it will be tracked in the beginning
it is in the middle but when we start
flying we will click on the table on the
left and this small table will be
tracked and finally the arrow on the top
is shows what rotation needs to be
applied for the drone in order to be
focusing on the place where it tracks so
when we finally start flying you see the
optical flow tracking the table and you
see a strong desire of the drone to turn
left and this desire will be turned into
action when we in our graphical user
interface start number + number one
which is number which is autonomy level
1 so orienting ourselves versus the
object and this is what happened and now
the drone is floating freely and
tracking and orienting itself versus the
object so this is the first level of
autonomy we have that in the second
level of autonomy there is not much to
to talk about because we will calculate
the rotation apply it so that we are
orienting ourselves versus displace and
then we add some forward motion to
actually fly it or displace we did some
test
the results were expected without
collision avoidance you cannot do too
much so this is how we arrived at the
third level of autonomy which is where
the fun begins because in order to avoid
risk we need to understand the spatial
content of the image and having just the
2-dimensional image this is quite hard
so again let's think how we humans would
do it and the first answer that comes
into our head is probably for the power
of our eyes so let this diagram be a
simplified viewing system the blue
rectangle is the place where the image
is formed for example the retina of our
eye and then the dashed line is just its
principal axis and a single pixel on
this image tells us very limited
information about what we are looking at
to be more precise it tells us the Ray
on which this object in the world lies
but we don't know how far it is until we
have another eye which is quite common
and in this eye we find the
corresponding pixel and then using some
mathematical tricks our brain knows
exactly where exactly we are looking at
and we can also implement that on a
machine so is stereo vision an answer it
might be it might be but then we would
need to take out our mono camera and
replace it with a stereo vision system
much heavier and probably much more
expensive so before we all go out
shopping let's think let's let's ask
ourselves a question is it really
necessary are we human hopeless if we
have one eye only obviously not because
when we move our brain is fed by
multiple images and we remember these
images
it can be mathematically proven that if
in between these two corresponding
images we find seven corresponding
points in general non-critical
configuration we are able to find our
movement and also reconstruct the
three-dimensional structure which we are
looking at our brain does that and also
we can write an algorithm to do this
this will be a video of my chair and in
each frame the algorithm will find
characteristic points and corresponding
characteristic points in the next frame
and from that it will build the Fulda
structure so you can see it on the right
and thus this approach is known under
many names it can be it is a structure
for motion multiple viewer construction
visual slam you can use OpenCV library
to have some basic reconstruction you
can find nice projects in the web or you
can read nice book great books about
this topic and write your own better
code and if you do it right it was just
perfect especially if you move
horizontally versus the object which
were you are trying to reconstruct but
if you are moving towards the object you
might have a problem because moving
towards such movement brings very little
additional projected information in fact
this information is so small it can be
confused with the error that you have
from the from your sensor from this
calibration and then you may think that
your free dimensional world is flat
which is not true it is even worse if we
rotate because if we rotate then this
movement provides zero additional
information because all the Rays meet at
the focal point anyway so this is quite
unfortunate because in our solution in
our problem in our project the drone
usually flies towards and rotates until
it meets an obstacle so this is the
worst case so however cool this
methodology might seem
this is not the technology of our dreams
another similar approach would be to use
optical flow itself if you focus
yourself on the door handle in the glass
while movement while moving you will see
that the door handle moves much slower
on this image and the glass because of
the perspective and you can use that in
order to get the three-dimensional space
again but again here in order to make
that happen
similarly to the previous example side
motion needs to be dominant in your
movement so the first thing that comes
to overhead so while we fly we can
deliberately add a little bit of
slaloming motion a little bit of side
motion and for us Polish people and our
friends in Russia adding a little bit of
salami path would be perfectly excuse
but we would not win the race that way
so are we
don't to fail absolutely not and again
we will draw inspiration from the power
of the human brain because we humans do
not need two eyes to understand the
world around us we do not mean even need
to move around we can stand still in one
place with only one eye and still have
absolutely no doubt what's around us so
how do we know this from experience from
billions of images fed into our brain we
learned how to understand objects their
relative sizes their arrangements and
live shadows with this lifetime
experience of looking at the world we
humans need absolutely no tricks to
understand the space and as amazing as
it makes sound thanks to the recent
advancements in artificial intelligence
we can share this experience of the
machine so we will
teach our drone our machine to
understand the space using deep
convolutional neural network in a minute
Roger will tell you more details about
different version or neural networks but
if there is anyone here absolutely
unfamiliar with the concept of machine
learning the main idea is that instead
of providing a set of nice rules
algorithms and formulas to detect some
object we design a mathematical model
which will take all the inputs for
example all the big pixels in this
example and create output level and to
make this outfit label correct we will
slide this model from the backwards with
examples with known label and then this
model will adapt itself so that it's
reactions to pixels and the combinations
of these reactions and combinations of
combinations and so on will finally give
the label which we are teaching this
model together you can read a lot about
AI but today we want to suggest
something much better you will all be
part of an amazing experiment in this
experiment you will be the AI and I will
teach you how to perform a simple
categorization task as every network in
the beginning you have absolutely no
idea what you will be learning and I
will give you examples with labels one
by one and your brain will shape what
the categorization task is and hopefully
finally when I give you the nice final
example the test you will know the label
are you ready for training ok
let's start
is number one label image number two
label image number three libel image
number four label the training time is
over
and now it's time for a final test this
is a question for 1 million and allowed
answers are yes and no so please raise
your hands who vote for yes thank you
now please raise your hands who votes
for No
thank you very much
the results are five thousand thirty
five four yes and one for know ladies
and gentlemen this is
a correct answer we have just
successfully detected so far thank you
very much for your participation it was
really important for me I'm very happy
we managed to be the winners today but
there is however something disturbing
there is this one guy who was blinded by
some other less important features also
available in these images and this is a
threat if there were more animal lovers
in this room we would all miserably fail
at the technical ugly sofa seriously
this is a threat in artificial
intelligence because formulating a nice
well-formed unambiguous training set is
an art and you will never know exactly
what the network learned so you usually
need crazy amount of training data to
make that and as a designer you have to
decide if you can afford getting so much
training data because maybe maybe it is
possible that using only one example you
with your own intuition you can design a
set of rules and create your own
handcrafted algorithm to to make the
task possible at a lower cost
in our case finding a handcrafted
algorithm would be a pain because our
task is from having a two-dimensional
image find a third dimension a depth map
of it here it is shown as a depth map
where the black is smear and why is far
and if we have this for every incoming
frame it would be possible to spot the
black places and this will be the
obstacles and apply proper control to
avoid them it would be nice if you have
that and we will train our tip
conversion
neural network to perform this task for
us but of course in order to do in order
to train that we need training data in
other words if we want our machine to
get a desktop for every frame we first
need to give hundred thousands of
examples of this job done right so how
to do it the first way would be to take
a ruler and for every pixel in this
image
measure the distance between the camera
and the place in the real world repeat
that for every pixel we have the DAT map
and then repeat this 400,000 of examples
our rough estimate for that would be
three hundred years so we decided to use
a different approach we will be using
our structure from motion algorithm
without any stress in the beginning we
can do whatever movement we want so we
will be moving sideways to create our
grunt roof and then we will move towards
and backwards from some object for
example a pillar hence the name deep
will are learning and we will be moving
precisely according to a well-defined
scenario between zero and four meters
from the pillar all the time the same
and this is this is how we would get a
nice death map for that but this data
because it is structure for motion
algorithm we don't know what the scale
of that is it is a random scale it is
floating in order to fit the scale we
will find the histogram of the depth in
time
locate the pillar inside and then
normalize this to lie between zero and
four meters and then apply this
normalization back to our images hence
having a nice ground truth in the metric
world and only train our network to be
honest we are in love of our solution
but
you have to know there are two solutions
which would bring you faster to to your
aim and the first would be to use
internet will download the grant roof
prepared by someone else and then train
your model and the second would be to
use internet and download the already
trained network so now watch I will tell
you more about the deep neural networks
and our architecture and I will finally
tell you how our example works hi
everyone sorry for my voice so stuff
that we really enjoyed the box
it was nice yesterday but to the topic
it is great thing to buy great things
from scratch however it is a bit wiser
sometimes to use existing solution
especially if it fits really well to
your need are you preparing a great demo
for devoxx and you have limited time so
will deepen your network for for dev
estimation and we find it in Internet
and what was very nice
not only the network structure but also
Brett reynad model was available to
download from the github and it was
available in the format with second one
in 4/10 of flow what was really nice
outdoors also prepared some Python
scripts if you read it you can really
really check how to how to play with the
network how to load the model feed the
network with your your custom data it's
very simple so it is very nice and now
we could just use use that stuff but we
really like to know details under the
hood and we also like to share it with
you okay there is one fig when we
download all the stuff and we did first
us with our data whether we were a bit
scared
because the model size was around 200
megabytes it was loading in five minutes
and this estimation took around 40
seconds so it was far away from very
tight performance but fortunately in our
office under my desk there is very
powerful machine with two nvidia gtx
titan titan x cards and on that machine
we could perform it with 100
milliseconds which was very nice so
let's go to the details so solution can
be described in just one sentence deep
fully conversion on your network with
upscaling layers fine-tuned from
residual network see it is a bit
terrible so I will try to translate it
to human language so let's start with
neural network so probably everyone has
seen such image it is just classical
fully connected your network where
neuron at any layer is connected to all
all layer all of neurons in previous
layer there are connections connections
are whited during training process you
are changing quite to make make the
network correct prediction so why can we
use our drone take the photo and just
fit fits our social network with with
with images we image pixels so imagine
how many ways we would you have in that
network so we have 100 by 100 pixels and
free channels and in next layer we have
neurons are connected to every way every
neuron previous layer we have three
hidden layers so we end up with almost
billion of words so it's very it'll be
very difficult to Train such Network it
will be so many degrees of freedom that
Network couldn't couldn't converge to to
the solution that's why we need to
switch to conversion and
work and I will tell you how it is
better why it is better so it is but
it's it bases on operation called
convulsion this operation takes the
frame on the on the left and the colonel
sometimes called filter on the right and
it just perform it calculates products
products values between pixels in in
frame and in kernel and and adds them
together and put in in our food image it
will be easier to understand on the
video so interacting over put 7 for each
9 pixels we convolve it with the kernel
and put placed output in the output
image quite simple next operation that
is using conversional networks is max
pooling it is used for del scaling image
but it should also save all all the
store information that will be useful
for further info layers for for future
extraction and classification so it
basically for each 4 pixels it does take
maximum value and put it in the output
image so operation very simple but it
works very well and last operation
important is unfolding because we fall
for upscaling because we don't know what
to put in every pixel in upscale image
with us put the values from input image
in left in left corner for each 4 pixels
in output inch it looks maybe that it
shouldn't work but it's very useful
activation function in conversional
networks we use rectified in our unit
this is basically linear function but
for negative values they are saturated
to 0
this function was introduced in 2000 and
it is biologically and motivated so
scientist things that our brains works
in this way so let's see how
conversional layer looks like so we have
a free channel image for example we have
free kernels so there is also a separate
color for each channel we perform
convolution sum together apply apply
activation function and we have feature
map if you would like to have multiple
feature maps we need to use multiple set
of scans now I think this is most
important slide I will compare what is
difference between conversional layer
and fully connected layer I think this
thing is skipped in most in most
presentation and it is most important in
fact so when when we count the number of
ways in first layer we have three
channels 3 by 3 it is only 27 ways if we
would like to have for example hundreds
of feature maps you would have two
thousand and seven hundred seven hundred
weights on the other hand one fully
connected layer we will have around
three thousand in the three hundred
million ways and we produce only one 1
output lesson so conditional layer may
be treated as feature extractor because
with very limited number of ways it
produces a huge huge number of features
huge output and on the other hand fully
connected layer it uses many in many
ways but it aggregates features and this
aggregation aggregation nature of of
full connected layer issues in
classification ok
there's also one more thing which is
it's called attention to the structure
or local structure so in a full
collected layer because every pixel is
in output is connected to every pixel in
in input the network might think that
appearance of the rubbish bins might be
somehow related to for example
appearance of flower it's crazy in the
conversional layer all features are
really local maybe in some local small
small local context sometimes context
might be a big bigger but is still local
this is huge difference this is typical
picture of conversional Network there
are few conversions with aunt my aunt
blinks which is the subtract instruction
part and a few connect fully connected
layers which is classification but these
days is quite popular to remove
classification parts completely
exchanging it with and other
convolutions and links and with that
network is called fully convolutional
layer and at the end of the networks
there is just one pixel that gives you
the value which is the output of your of
your network so we cannot use that
network for our task because for for the
image you would have just one death
prediction but we would like to have the
prediction for every pixel in input
image that's why we need upscaling
layers so in yarding some upcoming
layers which are basically conversions
inter lift with unlink and we have full
full resolution depth fine-tuning it is
quite difficult to train the full deep
network from scratch that's why many
people uses retraining network does cut
off some some layers of inks and other
layers and fraying
trying the network again for another
task and the authors of this network use
this approach so wave a to read all
Network 50 very cutoff classification
part they added upscaling players if we
trying it network previously training
for classification anything like
bicycles cuts and everything by training
it again for dev estimation at the
results are very nice so in the middle
we see grunts whoa which is basically
labeled data set and on the right we see
that predictions which looks very nice
formed so I will christoph continue okay
let's come back for a while to our
initial system design which is already a
little bit complicated with the
components for convenience written in
different languages and now when we add
our deep neural network prediction it
becomes even worse because not only we
face portent everything to a linux
machine with a cuter support but also we
cannot just call Python scripts
independently every time a frame comes
because this would kill us if we do this
every time of course obviously it is
solvable and there are people capable to
manage all the possible pieces and make
them work together except opportunity
but for those who cannot the word brings
microservices
yes this was a joke a good one for those
of you not laughing well this is not
microservices according to the
definition probably but breaking our
design into client-server components
made our life much easier now the
headquarters is the Python application
which communicates with a ffmpeg service
to get the decoded image
then it uses the cutest service to which
when given a frame will return a depth
map in the Python application we have a
graphical user interface where we can
apply commands manually but also here we
can press one to free to give autonomous
level to the drone and it will fly by
itself and then based on that depending
on that the control self server will
serve automatic or manual commands to
the client which communicates directly
to the drone so having all the puzzles
solved and explained it is time for to
show you our ultimate demonstration and
we are very happy to be able to show
this this video view because during the
day of recording we had a fire if you
don't know what a fire is a fire is were
is a set of failures that every
self-respecting developer should
experience during the day of delivery
now you will not see these problems on
our video because we have perfectly
stable delivery platform statistically
proven to be PowerPoint so you can watch
it without any frills ok but seriously
the our results were quite nice
everything works smoothly and we only
had some problems with the communication
with the drones or frankly speaking this
is the best out of 10 videos probably
and others are impossible to watch so
let's watch this one so on the right you
see our drone flying but what's more
interesting is on the left the applied
automatic control you can see that so in
the bottom you can see the output from
the deep neural network and when it
turns deep blue will find this region
and we apply proper automatic level
control so everything is done
automatically the only thing we done in
the beginning we we showed what we want
to fly which is shown by the circle so
the red circles tracks the window for
which we curiously flight
words without any other manual control
to the drone because this is the best
and only video that we have I will play
it again so that you can watch it in
peace
so according to the task defined in the
beginning of this presentation the user
only selects the place where they want
to fly and it is the machine that
understands that learned to understand
the three-dimensional content of the
world around it through the
two-dimensional picture and it flies on
its own which is the conclusion of our
today's lecture but before we finish we
we were joking a little bit and if we
offended anyone while we try to be funny
please accept the following
rectifications first of all dogs are
more lovely than sofas second not all
chess players are freaks
in fact if you have a nice traditional
just bored chess can still be a nice
game and finally irresponsibility is fun
there are only two rules to remember in
your office please watch out for the
lamps and the second one if you happen
to record a video on which your drone
flies and crashes into the workspace of
your boss never show it to the public
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>