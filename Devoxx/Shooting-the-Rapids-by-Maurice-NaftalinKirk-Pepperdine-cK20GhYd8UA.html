<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Shooting the Rapids by Maurice Naftalin/Kirk Pepperdine | Coder Coacher - Coaching Coders</title><meta content="Shooting the Rapids by Maurice Naftalin/Kirk Pepperdine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Shooting the Rapids by Maurice Naftalin/Kirk Pepperdine</b></h2><h5 class="post__date">2015-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cK20GhYd8UA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody enjoy the first day wow
that sounded enthusiastic did not no no
I kept on you can do better no enjoy the
day that that's so cliche okay so this
is a redux of our University talk
there's a bit of a quite a bit of
overlap there's some stuff in here we
didn't cover but you know we figure 3
hours on streams if you can live through
that you can live through anything right
okay yeah yeah so don't come back for a
second time right so this is me and
really my role in this whole talk was to
talk about the performance tuning
aspects of what we're doing and the the
performance aspects of it and that's a
slide about me and there's more stuff
there I guess associate and then we have
more Isola forward through here so yeah
he's the language dude so ah and not not
my dream what if you know about me it's
the author co-author of the best ever
book on Java generics author of one of
the books on java lambdas those things
and that's it okay right so you want to
cover thee well these are just these are
the things we're going to talk about in
the talk so this to let you know there's
going to be some background on streams
and Landers because we always find one
we're talking to any audience just now
it's kind of a difficult time Java 80s
in the course of adoption so some people
in the audience already know quite a lot
about streams and lambdas because
they're working with them and other
people have read about them a bit maybe
but need to be reminded before the talk
so we have to ask both sets of people to
be patient one because we're going to
give some background and then the other
because we're going to go fairly fast
once we're given the background and we
as part of the background we introduced
an example there's some log processing
code that Kirk needed to needed to write
and use for is for his work we look at
the performance of that we talk we're
going to talk about the effect of
paralyzing it and why we want to do that
we talked about the
issue of how we can get efficient input
of data into a parallel pipeline and and
about the advantages and disadvantages
of going parallel it's not always a good
thing to do and how it works when you
get away from this the these toy
examples where we're just sitting on
with our with a single parallel
application on a single VM in a host
that's doing nothing else and start and
Kirk we'll talk a bit about what it's
like in the real world right as a
warning for you yeah we will be there's
some benchmarks I did a lot of
benchmarking for this one just for to
see what things are going on so this is
my benchmark warning for those of you
can't read in the back it says professor
is M Pizza Pinsky prove that the squid
is more intelligent than the house cat
went posed with similar problems under
similar conditions okay there you go so
the point of this joke is I always
always wondered why skirt put this side
up in his talks I mean it's a good joke
right enough yeah but the what the what
the warding is that we actually have a
lot of trouble with benchmarks and we
don't really feel like the results are
reliable until we've subjected them to
some pretty free harsh tests not that
that doesn't happen often enough with
benchmarks and I'm afraid it doesn't
actually happen often enough with ours
because those tests include rigorous
tests for statistical significance and
also publish publication and peer review
and under the pressure of work we don't
always get to that and therefore the
results that we give us sometimes
provisional and the subject to
confirmation is that a fair way of
putting it absolutely that pause tells
you everything next okay yes I'm with us
so a bit of background about a bit of
background about lambdas for people that
aren't that aren't that aren't
completely familiar with them one of the
ways there are various ways of
introducing lambdas if you're not used
to them then one way of relating to them
is to this absolutely bizarre Java
construct called an anonymous inner
class so here we've got an anonymous
inner class which is an implementation
of the pred
interface predicate interface has got a
single abstract method on it which is
which is test and essentially what we
what we have here it's something that
really is a function because what it's
going to do is it's going to that method
if you if you apply that method you're
going to take it you're going to take a
match and you're going to return a
boolean result so it's it's got a single
input in a single output but there's a
huge amount of apparatus there's a
ceremony that that prevents us from
using lots of these things as we're
going to want to do in in the same in
the same piece of code because basically
it would become unreadable because it's
so big if you take away all the all the
boilerplate the only things that we're
really interested in here while we're
declaring a variable which is of the
type of the interface and actually the
only really important information there
is what the parameter is to the to the
to them to the method and what the and
what the result is what it's going to
return we don't even need to know the
name of the method if if it's a
predicate is the type of the declaration
is a predator is an interface which has
only a single abstract method if it's
only got a single abstract method then
that must be the one that's being called
and so so to turn that anonymous inner
class implementation into a lambda all
we need to do is to just put the bits of
it together we're going to need some
syntax the at the arrow there to to
separate the lambda the function
arguments from the from the function
result but that's it that's it so that's
that on the right hand side there is a
lambda and it's just it and it's nothing
more than a function of from arguments
to resolve it's not always not always
necessarily pure it can have side
effects but quite often it is pure
nowadays so the example that we've got
Curt wanted to processor a GC log file
and he's going to use regular expression
to pick out the numbers there's lots of
lots and lots of lines that look like
this and he's going to pick out the
numbers from here and what we want to do
is to work we want to summarize it using
a new Java 8
called summary statistics and you can
add and in this case doubles to it if
you can pick the doubles out of there
and successively add them to it you'll
then definitely when you've finished you
ask the w ask the doc this double
summary statistics object for its output
well for its representation and this is
what this is what you get it'll give you
the count that some them in the max and
the average so that's that's what that's
what we're headed for here so we can do
we can do that in old school code that's
just an imperative code and and and what
we've got going on here is it code that
you code the code that you would very
often see you want to talk about this
you've got these oh yeah okay sure do I
know you didn't like this feature so
i'll just talk about it myself yeah so
really what I wanted to do is go through
this bits of code and say okay if we're
going to convert this to a lambda
expression what are the features in here
that we need to keep and you know can we
identify you know what what their role
and purpose is here right so we have
this thing here and really that acts as
our data source so if we're going to set
up a stream then obviously we need a
data source for the string over here
this is really just mapping the the data
from the data source into in this case a
matcher and then typically what you're
wearing what you're going to want to do
is after you do the mapping well is
we're going to filter out all the bits
that we don't want so we're left with
the bits that we do want and with the
bits that we do want we you have to do a
little bit of data conversion here so
I'm going to map it and and change types
there from string to double and after
that we need to somehow do a reduction
or in other words collect the results
and and this is where the double summary
statistics is going to come in so how
does that all map into Java 8 streams so
the idea of Java 8 streams this is these
these built on top of lambdas but
they're actually conceptually they're an
independent idea so I did the idea of
Java extremes is the
sequence of values I like to call them
and it's not me it's not my invent I
didn't invent the phrase but it's a nice
idea to think of them as values in
motion they're a little bit there a
little bit like an iterator in the sense
that they they deliver values of lazily
but they're not like an iteration since
they they're not backed by any by any
specific collection they're just really
some of their dynamic data structure
along which along which values move
though they have three parts to them
there's a source there's a number of
intermediate operations and between
these two you can the stream will be set
up so we've got here some code at the
bottom there which is like abstract code
we might one one source of a stream
could be a collection so if on any
collection impair implements a stream
method and that produces a stream and
then you can pass it through a series of
intermediate operations so the vertical
ellipsis layer is meant to imply you can
each intermediate operation transforms a
stream to another stream and eventually
the the stream is terminated by a single
terminal operation the terminal
operation is the one that actually makes
the values flow so streams that streams
are lazy in the sense that once when you
set them up the Sun is still no
processing will take place no no
processing the actual values from the
source takes place until they're
required by the terminal operation so I
think of it as a terminal operation
pulling the values at pulling the values
along the stream how do you get streams
well the the platform api provides it
provides you with lot the platform
classes provide you with many many ways
of generating streams note no doubt that
by far the most common use case we'll be
using collections as the as the source
of streams and the and the absolutely
the the mainstream use case for which
the whole API was designed was the most
bulk processing of data so the notion is
you're going to take values out of a out
of a collection and traditionally that's
via iteration well this is now going to
chain
change and you're going to use you're
going to use the collections a stream
sauce and then typically you do lots of
stuff to them and then you dump them
into another collection and that's that
is the kind of mainstream model but
there are many other ways of providing
the input data for streams and we've let
I've listen there's a few of them are
there but if you want to know that if
you wanna know the whole catalogue now I
can suggest you a good book to buy yeah
i think you know you know as color ii to
this particular slide here is that
you're probably going to want to go back
and read the javadocs know you don't
want to go and read my book right sorry
that's what you wanted to get maurices
book and read it and and just review the
eight guys because there's a lot of new
stuff added in Java 8 that actually is
there to support all of this yeah okay
over to you or me okay you got in the
imperative stream part okay so yeah so
so we had that piece of road that was
the imperative code and really what we
want to look at is you know what does it
look like a stream code right well there
it is right in all its glory I think we
see it used to be shorter but Heinz got
a hold of it and he decided that he
liked the longer version so so I stuck
with this one right so you know what do
we have here well we have a data source
so I'm going to read the lines from a GC
log file file start lines is one of
those methods which will actually
generate a stream of strings from a file
yes exactly right so you need to hand it
a path and then when you do this it's
just going to create a stream of strings
and then I'm just going to hear there's
my intermediate operation so I have a
map filter map and map again so in this
case I'm going to map to a match your
filter out things I don't want I'm going
to grab the group that I want so if you
look at the pattern there's a capture
group defined in there and soak rapture
the valley from that convert it from a
string to a double and finally right
what we'll end up actually doing is
hitting will hit the terminal operation
now you might notice there's a strange
syntax in York says
stop time colon colon matter right those
things are known as as method references
and there's four different types of
method references that we can use so
this is another shorthand that we can
actually use to just condense that the
text yeah yeah it's basically the
easiest way of understanding method
references is simply as a shorthand for
for lambdas so in it so in this case for
example that map that that the method
reference which is the argument to the
map it could be expanded to a lambda
which would say for each string of for
each element of the of the stream
calculate the stock partners topped the
the match method returns so we return
the match your method of stop time
patent right ok and then there's our
terminal operation here which is
actually going to collect everything
into the was a double summary statistics
yes yeah that should be eclectic ok so
Mike made my rather can simple-minded
animation of how streams work it shows
shows 11 I think fairly fairly useful
thing so we got this idea that a value
comes from the source and what you see
happening here is that that the all of
the intermediate operations are
performed on each value so this is
different from the kind of stage a
stepwise way in which collections are
traditionally processed typically you'd
iterate over all the values of a
collection performing the first
operation on each one and then you
desperate over them again forming all
the operations on the second one well
you quite often have to do that you
don't have to do that with it with this
in this case there's we have the
advantage of not having to have any
intermediate storage and we get a
performance advantage from effectively
fusing the intermediate operations
together so to each time they're being
called by the these values are being
pulled down by the terminal operation
which I've called it a reduction because
the idea of a terminal operation is
generally in some way to fuse the values
of that are coming down the stream so
that if that should be this very wide
variety of ways in which the value is
coming down the stream
can be collected together so the most
the most again the mainstream use case
is taking the values of the coming
downstream and to dump them into a
collection but there are other but there
are other things you might like to do as
well arithmetic if there were our earth
metic values you might can multiply them
together or in this case get the summary
statistics if there were strings you
could concatenate them there's really
quite a wide variety of terminal
operations there you can write your own
consumers to you can write your own
clique you can write your own clicks
with terminal operations of all kinds
the whole question of reduction has
actually been opened out for the the
stream API reductionist functional
program at the production in the way
that functional program is used to think
of it turns out actually to be too
limited an idea for Java because
functional programmers don't have to
deal with mutable values and then
reduction in the traditional way won't
work for mutable collections so they had
to invent something new this idea of a
collector which will which will in a
thread safe manner will take even
possibly concurrent values and put them
into a and put them into a non
thread-safe collection it's a very
clever piece of work and the collector
API is a kind of major part of the
stream API and it it's very good it's
very powerful it's very nice i really
like it and i've talked about it quite a
lot ok so over the years that perform
all right that's the question that's the
question ok so the old school code here
is just like 13.3 seconds and the
sequential code was 13.8 seconds so i
had about 24 million lines in the file
and so it was just running on my regular
laptop here and since we're doing about
the same workload we should have about
the same amount of you know that should
take about the same amount of time which
it does and you know what can you say
well it's the stream is the streaming
code seems a little bit slower and was
consistently just marginally slower but
from my perspective it was a lot cleaner
code and a lot easier to read now there
was another advantage to the first time
is running these benchmarks I actually
may
aid and well I I made a different choice
as to how to read the file for the old
school code and so it turned out that
the sequential code was about it's about
twice as slow as the as a streaming code
and when we looked at it I realized oh
yeah okay the sequential code is using
niÃ±o whereas i was using just io
classes so i would say that they've done
a lot of performance work inside the
streaming you know with the streaming
codes in the labs and stuff like that to
make sure that you get really really
good performance so they're they're not
going there you know by using it i'm
actually not going to make you're not
going to make the mistake that I made by
using some of the slower older stuff
that's let you know thats hanging out in
in the jdk so so anyways well 13.8
seconds for this was okay you know of
course the question is when we're doing
performance tuning is like yeah can we
do better right and we might actually be
able to do better if the workload was
parallelizable you want to say something
about that well yeah there's the I guess
probably everybody knows by now that the
one form or another the one phrase or
another the free lunch is over so the
speed of cause isn't the speed of our
processes it's not by and large going to
increase a lot well it might might
increase little but it's not going to
increase a lot Moore's law though still
is operating and the result of that is
that we're still getting more and more
and more transistors on every chip and
that's leading to multiple cause there's
a lot there's a lot more to say about
that but the fundamental story is that
whereas in the past we could always rely
on hardware innovations making out
making our single processor run faster
we now have to find some way of making
use instead of distributing our workload
over multiple cores and that turns out
to be well that turns out to be
impossible for almost impossible for
java programmers to do certainly
impossible it without help from the
without help from new framework from the
platform
classes and in fact also from the
diplomat JVM what we native programmers
who are using native code they can
decide which core their different
threads are going to run on but we we
can't what we have to do is we have to
rely on the JVM and the platform classes
in the JVM to cooperate so that when we
spin up multiple threads each thread
will be will be what will work on a
different core and therefore we'll get
through the the work that we have to do
faster because it's being done
simultaneously so that's that's the idea
but there's many difficulties that lie
in the way of it so for example it's got
to be possible to split to split the
data into different segments so that
each thread can work on them
independently that in itself is it's
quite a problem and there's going to be
and there's some other problems that
we're going to we're going to come
across shortly because Kurt's going to
be talking about them so the idea is
that we're going to split the data
process each segment and combine combine
the results and that actually fits with
an API that all that was already present
in the in Java from jdk 7 which was the
thought join framework and that and in
fact it's the thought join framework
that the part of the parallel streams
sit on top of so here's the primitive
animation of how parallel streams work
the idea is that if you imagine these
values X 0 2 X 3 not now with each one
representing in fact a segment of data
because you wouldn't do it just for for
for single values then unpredictably and
an in a random way the the different
threads will we'll work on the debt on
these different segments of code and and
at some point each each of the values
have been transformed and then filtered
will arrive back at it will arrive at
the terminal operation and I've
deliberately made them execute in the
wrong sequence so that you don't get an
idea that they're regularly all going to
pass across at the same time you can't
rely on the feds aren't synchronized the
threads that are executing the different
streams in here you can't rely on them
being synchronized and so you've no idea
really
how the at any given moment where the
data is all you all you can do is
observe it at the end and see that it's
arrived in the right order it does make
the collector at the other end more
interesting to write that it's and it
certainly does I mean in other talks
I've described how to and in and of
course in the book I described how to
how to write a collector it's not
actually that difficult you laid down or
actually they lay down some of the rules
and I laid down all of the rules as to
what would make a collector correct but
yeah it's not an entirely trivial
exercise so so where does the weather
data come from I've described this this
idea of the source it has to be it has
the four parallel streams to work the
data source has to be effectively split
the the every stream is sourced by
what's called a splitter ater which is a
kind of cute name that combines the two
functions that are required one of them
that the tide has got to be Scott to be
splittable or it's got to be splittable
if possible because not all data is
effectively splittable and it's got to
be iterated over and that's it
definitely has to be iterated over so so
the basic idea is if you've got a data
set like this then then a splitter ater
will cover a range of the data set in
this case we've got we've got to split
raters they already cover it's already
been split and the two split raters
cover the top and the bottom and the
bottom of the other data set but if we
have maybe four cause that we want to
execute on we would need to do a further
split as well and so now with now now
we've got four cyl iterators each one
covers the range a quarter of the range
of the data and now what it's not worth
splitting any further so each one of
these segments can be worked on by an
individual thread so the different
colors represent a different thread and
the threads will now iterate over the
over the values and unfortunately in
this case I haven't made that haven't
taken the precaution of making sure that
they iterate in an unpredictable way
which I did on the previous diagram here
that they're marching neatly in step
across those segments which is not
something you can rely on at all okay so
how do we make this happen in the in the
in the
a stream code that we had it's really
easy they the slogan was we want to make
when when the stream API was being
devised well what they want that we want
to make parallel ism explicit but
unobtrusive so there it is it is
explicit and it's only obtrusive because
we've got a big green patch over it it's
really really easy to as you think to
make your code to make your code run in
parallel in fact too easy hey that's
that's there's an argument if there's an
argument in favor and I just say that
there's a lot of thought that has to go
into megan deciding if you want to use
that method call there or not and that
is I guess the purpose of the rest of
the talk is to try to say okay
absolutely do you really really want to
do this because if you think that you
really want to do it then you know go
ahead but okay go on then so as Maurice
said this whole you know but soon as we
put the parallel you know the method
call on to the screen and there's a
splitter ater that will actually split
the source what we're actually going to
end up now doing is basically delegating
all the work down to fork joint now for
coin was introduced in Java 7 in Java 8
we actually get all of this wonderful
work that takes makes good use of it
because quite frankly before this fact
it was exceptionally difficult to use as
a matter of fact how many people here
have tried to do a fork joint how many
who have been successful yeah okay there
you go there was a lot of hands that
went down after that so yeah yeah you're
in good company no worries here right so
okay so one of the things that fork join
you know they had to make a bunch of
decision so the first thing is that okay
how big is the pool going to be well if
we assume that the pork joint ask our
CPU bound then it doesn't really make
any sense to have more threads than
coors so and we might need some Headroom
for doing other things so let's make the
number of threads equal to the number of
hardware cores minus one right so
if you're on a fork or hyper threaded
that means you get it like a defective
kors it will set a pool size of seven
threads well you should say the reason
for the minus one is because the
expectation is that the submitting
thread is also going to be pressed into
service yeah that depends on whether you
use an invoker submit that's why it's
that's why the size it like that though
yeah well okay so so okay so so we're
going to so we have the parallel keyword
in there so the next question is how
does that perform well there's our old
numbers and here's our new number when
we go parallel so that's nine point five
seconds so I was running on my eight
core machine and it's 1.4 you know 5x
faster but not 8 x fox 8 x faster right
you know i would expect all 8 cores to
be chugging away on this and actually
they were but you know so the question
now becomes you know why is it not so
much faster right and you know when when
you come right down to it we've run this
with a whole bunch of different
benchmarks and actually we've seen it
used into production environments and we
really get a mixed bag of results here
sometimes it works better sometimes it
was about the same like this one's about
seems a bit faster but you know nothing
to write home about and sometimes the
results were absolutely worse and so you
know that leaves us with a bunch of
questions you know when is it worse one
is it better you know which really says
you know when should we paralyze and
when is leaving this as a serial process
actually the better choice and of course
the answer depends upon where your
bottleneck is ok so we're reading a file
where do you think the bottleneck is in
this case it's not in CPU right it's on
reading the file I think Dmitry shouted
that out first yay ok so in general if
your bottleneck is not to CPU in some
place
and you're really pounding that other
resource hard adding more threats that
the situation is actually going to make
your performance worse you don't want to
add more threads in that situation you
want to add fewer threads right in some
cases going to single thread is the
right answer now having said that we ran
into some strange examples during the
university session that we're somewhat
counter to counter intuitive until we
actually could work through the logic as
to what was happening so so that says
you still might want to bench this my
benchmark this in your environment to
make sure that you are making the right
decision because you know having
multiple threads go after a seemingly
overtaxed resource can sometimes fill in
a lot of dead time and and I can
actually have a positive effect okay so
where's their bottleneck well you know
we ran a simple profiler over the top of
it there it is that's son I know I
that's a bad class to be using now isn't
it these were all the all the benchmarks
were done in Java nine so but without
the jigsaw extension just just to let
you know right so an i/o operation you
know not a surprise so um you know that
was using one way of doing it but in
Java nine they actually use this thing
called a file channel line splitter ater
which I'm gonna talk about home which
coming up Maurice is going to talk about
because it was up yeah he had some
involvement in the whole thing so okay
I'll let you go I'll let you explain his
wonderful work here cut so I I mentioned
that they that that some stream sources
I mean in general it's kind of obvious I
think that some stream sources will be
more easily better splittable than
others so you can imagine that an
ArrayList is going to be easily split
because it's easy to find the midpoint
of it and divide it up in the way that
the previous animation showed but other
things are going to be difficult because
like with a linked list how do you find
the midpoint of it well the only way
you're going to find the midpoint is by
a tree is by iterating over it and
that's clearly going to kill the
the benefits of being parallel streaming
io is an example of something that was
really very put that splits very poorly
because you've got to get it essentially
line by line off the of your motive of
your i/o device my my anima my picture
that is that we may have we may have
parallel streams but it but it in fact
what we've got happening is the the data
is being fed into the stream essentially
by an iterative process being parallel
doesn't it really doesn't do you any
good yeah we saw you drip feeding would
drip feeding into we've got this kind of
giant water main that would carry our
data away but we've got a very we've got
a really tight tap that we can only just
turn on in a very drip-feed way so
actually actually I was working on a
different example for when I was when I
was writing the book on streams and I
was also looking for an opportunity to
explain how to write us how to write a
splitter ater that used the use there
was a kind of real-world example that
didn't they're actually require the
splitter ater to be written because in
fact they've written splitter raters for
most of the platform classes that need
them and i came across file i/o so i had
so i had the idea that we could do
better if we were to read the her to
read the file into a memory into a map
map back buffer because with memory
mapping you can treat the file and disk
is essentially as though it's already in
memory so you can find the midpoint for
it or you can find a split a split point
for it and you can do you can do that
and basically by random access so the
idea of the line of the line splitter
raters i called it but it's now in the
it's in jdk nine as the as the file
channel line splitter ater it was it was
reimplemented and considerably improved
by by Paul Sanders for jdk nine but the
basic idea behind it is you get you take
your entire file put it into a mapped
byte buffer and then you can treat it as
as
as you can treat it as though it were in
memory and you can find them you can
find the mid mid point simply by taking
some random point in the file and
searching for a and searching for a line
separator this hugely oversimplifies the
situation because of course depending on
your char set the the lines the line
separator may not be back slash n if
you're in if you're in some of the some
non ascii character set then you'll
you'll need to do sometimes much more
elaborate searches so there are
differences in the implementation but
the basic principle is this when we
found a split a spittle point we can we
can divide the data up and create a new
situation the new splitter ater gets the
bottom half of the data the the old one
reduces its coverage to the top half and
now we're ready to go with the and and
this process can be repeated until we've
got chunks of the data that are suitable
for a single thread to iterate over and
and this and that's where it is now in
JD Kane and is a file channel line
splitter ater right ok and so I think
the numbers on this one is that we got
an improvement maybe not the improvement
that we were looking for again but but
we do get like a 2x improvement on on
the old numbers so so this actually did
work better now there's some
restrictions here right I think you can
only use a utf-8 or the car remember
which house has his implement right but
it certainly is usin is utf-8 right and
but it's not all character sets are
supported no no if he if we come across
a character set that isn't that isn't
supported and I think maybe utf-8 utf-16
I'm not sure right then what happens it
just falls back to the old way of doing
things and the old way of doing things
is something called an iterator splitter
ater which was the which is the default
for collections that can't for any
stream source that can't be effectively
split and basically what it does is it's
sort of like iteration but there's a
heuristic which makes it a bit better
and that that was the default before so
you know worse off if you've got an
unusual character
and Paul is telling me that he really
welcome contributions from anybody wants
to implement it for other character sets
okay good so now what I want to do is
actually move to like you know we're
file IO is the problem to try to see
what happens when we actually get this
thing into memory so we actually are
just reading it from memory so the you
know the the M map technique basically
Maps the file into the process so as
more you said you can treat it as
effectively in memory because it's just
going to be paged in as you access those
pages which is much much it's it's a bit
faster than reading a file I mean the
whole point is that I Oh subsystems are
well optimized reading a file from left
to right and they and everything will
get buffered up very quickly so the
advantage is going to end map files
sometimes aren't as big as you might
expect well the biggest single advice I
mean the big advantage that you're not
going to get away from and it's actually
it's actually symbolized by the fact
that Paul although he although uses the
map pipe buffer to find the split points
then reads directly from the file so
he's so he's definitely believing that
iteration over a file isn't isn't that
much slower right but the point is
you've now got four threads or eight
threads simultaneously doing it feeding
into the parallel yeah bye yeah then and
that was a good point so so anyway so we
read all the data into into memory into
Java heap and we just said okay so you
know so now what does the picture look
like right and you can see the again
here so here's our times right so it's
9.4 seconds nine point nine seconds and
2.7 seconds so we're 4.25 you know four
and a quarter x faster better again but
still not eight times faster right so
you know we're getting better but you
know not quite the performance
improvement i would expect and i don't
think we actually ran a profile on this
one to know why so I but I what I
suspect is happening here and what we've
seen with some of the examples that
other examples that Maurice had was that
there was just a lot of overhead and the
in running for joint there's a lot of
setup and there's and there's a lot of
tear down that you have to do and
sometimes the setup and teardown costs
can actually get in the way and so
really one of the decisions we have to
make when we go parallel is you know can
we justify the overhead can we amortize
away the cost of all this overhead now
Doug Lee wrote a paper where he
describes this what I call c pn q
performance model and really what the c
pn q performs model is doing is trying
to come up with a cost model that says
okay you should you be running this
particular workload in parallel or is it
better to run it in cereal now you know
i should add here that there's a lot of
just a lot of systems that are really
really fast like in low frequency
trading systems they are all single
threaded they're all single threaded
because it's just much faster to do that
than to actually incur the cost the
overhead of going parallel and you know
all the costs of doing data data hand
off the handoffs between threads now if
you look at these particular numbers
what we're saying is that the you know
the overhead or the fork joint framework
is from the Oracle engineers according
to Morris is about 100 microseconds
right so if you're if then if the cost
of your operations by the times by the
number of elements here is not in that
range then your overhead costs are
actually going to exceed your processing
costs and that's probably a good case
when you might not want to parallel eyes
all right so as a rule of thumb we're
going to say that you need about ten
thousand elements in a segment in order
for it to me in order for it to be
worthwhile to actually parallel eyes the
workload anything smaller than that then
again that's a pretty rough rule of
thumb i mean probably it
it what is important is the product of n
times Q the intermediate operations have
got to be expensive enough that it's
worth distributing right over multiple
cause and the problem the reason why
this rule of thumb of 10,000 elements
comes up it's because it's actually
quite difficult to estimate what Q is
it's usually fairly easy to know how
many elements you've got to process but
it's quite hard to estimate how long
it's going to take to process each one
yeah you wouldn't apply this rule if you
had some very expensive intermediate
operation if each if each thread had to
calculate you know a 10 deep in a chess
position that would be expensive enough
to be worth parallelizing even if he
only had very few of them the matter
should be expensive enough because that
was not well that was a fairly simple
matter with a single man yeah well what
yeah i'm not i'm not convinced i mean
when on the example that example they
were looking at before the matter I mean
it's kind of unfortunate regular
expression matching in Java although
it's inefficient isn't sufficiently
inefficient possibly yes for this case
yeah because when we actually did do
some profiling either the matter did
show up in the profile but it certainly
wasn't the dominant cost in this whole
this whole equation right so again you
know this is the type of analysis that
we went through when we're actually
looking at the problems and it's trying
to find out okay is this thing actually
expensive enough that it's going to make
actually a good demo right so we don't
have any demo failures not that there's
any demos in this talk but now nevermind
okay demo sniffing and right okay so and
we have other gotchas to write like
frequent handoffs as i mentioned before
well first off it puts pressure on the
thread schedulers right and you know the
estimate is that a threads you know a
contacts which is going to cost you
approximately load eighty thousand
cycles right which means that if you
have to do a context switch to hand off
a piece of data to another thread that
just cost you eighty thousand cycles you
can do a heck of a lot of processing in
eighty thousand cycle tsunamis like this
it takes League do like an ad yeah
exactly it's it's the you know it is
quite a bit of processing that can
happen with eighty thousand cycles right
the other thing too is that if you start
using a lot of threads there's other
things that can
that that happened for instance if
there's any maintenance going on inside
the JVM then it's got to bring all of
these threads to a safe point right so
time to save point is often a very
limiting factor in the performance of a
lot of applications so you'll you'll
have the time to save point then you'll
have the actual maintenance staff and in
some cases we've seen gaps like a
ten-minute GC pause might result in a
half second application stall just
because some thread decided not to
cooperate so it didn't get to a safe
point and or the thread scheduler at the
other end possibly running in a
hypervisor environment all of a sudden
decides that you know okay this thread
is just not going to get started again
so all of a sudden you just get these
very very long long pauses that are much
longer than necessary having said that
we certainly don't want to have too few
threads so we run into situations where
people have made the pools too small so
everybody's waiting for a thread to get
processed and you have all of this free
hardware that they could actually be
using you know you pay a lot for the
hardware you might as well use it right
so you know so you want to make the
thread pools big enough or you certainly
want to make the task big enough that
you can get good saturation the hardware
but you don't wanna make them too big so
that you saturate the hardware and you
know just to see the effects of this we
wrote I wrote this little thing which is
trying to simulate a server environment
so what do I have is I have an executor
up there and the executor is size 10 and
what I did was I just you know dripped
jobs basically parse jobs into this
particular thing so there's our lambda
in the min the middle of this right so I
said okay let's execute this particular
piece of code right so you know what's
the workflow here so right the first
possible to arrive will come into the
work you know the fork join work worker
thread pool and it will just completely
consume all of the threads right so that
means everybody
coming in afterwards is going to have to
wait for threads as soon as some of the
threads become available then we'll
start seeing all these jobs also and get
mumble jumbled up inside of this work
queue so and some will come out really
quick some will come out slow and it's
you know so then results all of a sudden
get very unpredictable I mean this is
not really a well-ordered cute ordering
at this point is not guaranteed right so
you know your tasks are going to collect
are going to be active and they're going
to be collecting dead time waiting for
threads in order to be able to complete
this thing right main effect system will
be stressed beyond capacity and that
means if you look at the profile again
right you know in this case okay our
match is well up there on the profile so
in this case it is relatively expensive
to what we're doing but you can see that
underneath here we have framework code
that's running and that's running at a
very high percentage also so you know
it's it's a you know the bottleneck here
is pattern matching but streaming
infrastructure isn't far behind next
problem right but that this exposed is
this whole thing of what we call the
tragedy of the Commons and you know
Maurice liked wanted to put this long
explanation well anyways Adam you don't
have to have a long explanation if
people haven't heard of the tragedy of
the Commons how many people here haven't
heard of the tragedy of the Commons oh
that's quite a few actually okay I give
it to you you will you win this argument
that's it's it's not it doesn't take
very long to explain that the tragedy of
the Commons was this I it wasn't an
Asian name given by this just look at
the decision that took out after yeah
this yeah this is the idea idea is
you've got a common land and a lot of
different heard owners of grazing
animals and it's in the interest of each
of the owner of each herd to put more
animals on there if they're saying to
themselves well the honors already a
hundred cows on this field what harm can
it do to add another one and but
obviously if everybody just acts in
their own interest in that way without
considering
group interest that without allowing the
group interest to dominate then the
result is that the land becomes over
grazed and so obviously this is this is
this has been take it's very
controversial but it's been taken as a
model for all kinds of human behavior
including obviously climate change yes
absolutely so in this case we're going
to borrow the idea and we're going to
talk about tragedy of the Commons here
right so um what do we say we have a
finite amount of hardware so we don't
have an unlimited number of hardware you
don't live in an infinite compute
environment we only have I only have
eight cores I only have 16 gigs around
if I grab at all there's nothing left
for anybody else right and you know and
if everybody tries to grab at all then
all of a sudden you have a problem and
we've seen applications that actually
have tried to do this they said oh great
you know early adopters of Java 8 they
you know they drank the purple kool-aid
and they said okay we can parallel eyes
all of our streams they put it into an
application server environment and what
do you think happened everything just
went upside down very very quickly right
so and if INEC very quickly they wrote a
said script to basically set out the
parallel dot out of the code so that it
was a stream right so yeah so like I
said before we just you know we
simulated this right and the first
result comes out 32 seconds afters
remember in the parallel case we're
getting like to point was at 2.3 seconds
something like that right 32 seconds the
fastest one we got her there was 9.5 and
most of them took somewhere between 20
to 30 seconds to actually come out some
were in the teens but you know quite a
few of them around 20 22 23 seconds not
only that the system time on this
particular piece of hardware was really
really hard to the point where I thought
it was going to have to push the power
button to actually shut the whole thing
down before my machine started melting
it was you know it was the with the high
system time basically what it meant oh
is that the the application itself was
pushing on the colonel
it's telling the colonel I need to be
doing a lot of stuff sorry you need to
do a lot of stuff for me and and as you
know the colonel is going like you okay
and it's like burning up the CPU just
trying to get all of the ILO operations
done so obviously not a good situation
so server environments don't look like
an environment that you would
necessarily want to have boy called
rampant or abundant parallel ism inside
outside of the parallelism that's always
already there so that was our in-memory
variation so we preloaded log file
submit at the toss and that seems to be
a repeat right so I think in conclusion
you know do you want to do the
conclusion gosh okay i'll do i'll let
you didn't like it you can interrupt me
yeah so so generally speaking it's not
often going to matter very much the
difference between sequential stream
performance and the imperative code that
you you've converted it from all that
your is your alternative to right I'd
like to something that we haven't said
very much in this talk because it's been
a performance-oriented talk is that the
advantages of going over to using a
stream code are not limited to
performance at all and in fact they're
not primarily performance the clay our
claim is that the stream code that you
saw there if you haven't seen it before
then it looks unfamiliar but if you have
seen it you'll know that once you get
used to it it's easier to read and it's
more maintainable it's more concise and
typically the operations are more
composable so we're going to get better
api's out of it and so the big
advantages of going over to using stream
stream code which is more functional and
style an imperative code don't depend on
performance and so in if you can't go
parallel I think that's not something to
it that's not something to worry about
by and large you should still be
thinking about writing in a more
functional style and using stream code
to do it so the first bullet point is
very relevant if it's roughly if the
performance is roughly comparable and
performance isn't a big issue then
forget about it
and just right nows code if you still
don't have the performance that you need
and you think that you may get a boost
by going parallel then you need to check
your task your use case to ensure that
it's suitable so the task has to be
suitable in a sense of the it has to be
splittable the the the different parts
of it don't have to depend on one
another because remember the values of
the going down is those parallel streams
can't interact with one another at all
it's got to be expensive nicely we've
talked about this the intermediate
operations have to be expensive enough
to balance out the overhead of setting
up the faulty own framework in the first
case and the in the first place and the
and also the overhead of merging the
results of the parallel streams together
I haven't had time to talk about that
but the reserve stantial overhead there
as well so I talked about no into task
communication needed I think probably
when I was thinking about the task is
suitable I should have mentioned it's
got to be efficiently splittable the
data source has got to be suitable and
the environment has got to be suitable
so Curtis talked about that really it's
not going to work very well if you're in
an environment where a lot of other
processes are competing for the flip for
the hardware so there's really what the
result is I think now I started writing
and started writing the book innocent
witness kind of haze of enthusiasm for
parallel ISM and actually put parallels
them into the title in a in a way that
would have been less prominent if I had
waited to finish the book before her in
time before where I produced before I
wrote the title so I did the performance
test at the end which was a big mistake
because I learned a lot from doing them
and one of the things that and what I
mainly learnt is that although this
though automatic parallelization it's a
big advance it's fantastically useful in
the cases where it applies it's those
cases are not that common it's a
relatively it's an important niche but
it's still in each so that's basically
that's my conclusion I guess we could
add we could add the bullet point at the
end oh that's right ok so yeah that so
there's an additional point that you
know we were sort of thinking like okay
do we talk about it do we not talk about
it and that's the fact that the fork
joint pool is a cue
if you have a cue in your system the
first thing you want to do is make it
observable you know you want to know how
long or how frequent things arrive at it
you want to know how long they sit there
in the queue you want to know how long
it takes the you know the your server
your application to actually service
that particular tasks that's sitting in
the queue and the reason why you want to
know this is because that's information
that you can actually use to
intelligently tune the size of your cues
and there's some theory behind all of
that now the fourth joint pool itself is
unfortunately not very well instrumented
and we've actually done some work to
actually add some instrumentation to it
and added an MX being to it so that you
can actually get this information out of
it and and and start looking at it but
that's just in an open a adopt OpenJDK
project it's actually not in the the JDK
proper but you know one of the things
that we're suggesting is that we try to
make the the you know the JDK more
transparent so that we can actually get
this type of performance metrics out of
it so that we can actually do a better
job tuning it and you know I don't know
if you put left the URL up with our
bunnies and we have some experimental
code that if you're interested in
looking at it it's out in the public
domain okay well people should contact
you for that yeah they kept contacting
it so that just send me a tweet on
setting right it should have been on the
next slide which is slide about
resources so a very useful resource if
you're thinking of if you're weighing up
the advantages of going parallel is this
web page which was written by ugly who's
the who was the main author of the fork
joint framework and he discusses their
the different issues that are involved
in deciding whether or not you're going
to whether or not it's worthwhile going
parallel and so and the other resource
that I obviously can't refrain from
mentioning because he he a bit sometimes
there's only one here well of course I
because I have because in it I discussed
the lot of design issues around around
streams but I also consider performance
issues as well erica is that when i
bought it
right okay so that's the end of that's
that's the end of our talk and we've got
five minute five minutes left for
questions yes we always like questions
thank you very much imitation oh yes
there is but it's so the question was is
there a way to to use a custom for joint
pool for parallel streams yes there is
it's an unsupported accident of the
implementation but you can actually set
up your own Fork joint pool and then
submit the task to it yourself if you
submit the lambda then it will then with
the parallel stream in there then it'll
actually use that fork joint pool by
awesomest sample code someplace yeah you
submit it and then then it'll get it
gets you know the splitter a derp we'll
split everything up and then the fork
join will actually flow from that the
reason why I wouldn't you know I think
think twice about doing this right there
might be some conditions where you'd
want to do it but think twice about
doing it right the fact that you've a
common thread pool means that you have a
global basically you have global control
over how much activities going on the
machine if I start firing up my own Fork
joint pools all over the place I can get
myself into trouble very very quickly
right so this notion to have in global
control i think is actually a reasonable
one however some of the examples that we
ran into in the university session which
were quite interesting that's actually
use case that basically contradicts me
right now in those cases I would think
that it would be better for them to set
up there I i'm not sure what we had was
some what we had to do something wrong
we have to confirm all at 2i there's
some more benchmarking I want to do on
that and the idea was you're interested
basically the idea was that the guy was
running a fork joint over a socket reeds
and yeah it's a weird idea but it seemed
to be working for them so one of the
conditions I've that I've tended to say
was necessary to take good advantage of
parallel streams was that you should was
that your tasks should be CPU bound but
while we're hearing in the University
session one
people were getting good results with
tasks that would not in fact CPU about
yeah but in this case the threads were
spending a lot of time blocked right and
so I think the the advantage of their
getting might might be the same type of
energy get by using an niÃ±o with a file
selector type situation yeah thanks any
other questions up there oh I think
thought I saw him No oh right over here
yes sir how do you make sure your code
still works with parallel streams how do
you test it well well actually the
actually the problems not very different
from any other from any other testing
problem really I mean the point is i
guess that anytime that you're testing
code that works that relies on
concurrency you are going to have to the
your testing to make sure there's an
absence of race conditions essentially
the only way the way that code that uses
parallel streams is going to go wrong is
because you're breaking the rules the
rules of the framework relies on in
order to avoid race conditions so in the
same I mean it it's not like this is an
easy question to answer but it's not a
different question from the question
that you normally encounter when you're
writing when you've written concurring
code you're just going to have to test
under under a wide variety of different
conditions and loads to try and ensure
that the that you've that you don't have
conditions that depend on the relative
performance of threats the one advantage
that you do have it's not a testing
advantage but it's an advantage the
rules for using the framework safely or
actually laid down in the documentation
so you can have a higher degree of
confidence that you're avoiding problems
because this is a relatively high level
it's not like you're writing wait and
notify code the rules the rules are
pretty clear and they're at a level that
application programmers can implement
relative relatively easily yeah my test
for the i mean when i did the benchmarks
I had a QA test in there to make sure
there is getting the same results right
and it was just basically I I use the
double some you know collector
and double summary collector and you
know i just used it in sequential use it
parallel and it was giving me the same
results so i was i was happy i mean the
the thing is here is like you it's not
like you have a symmetric workflows here
right you do have symmetric workflows
right so and it's not like you should
have race conditions and things like
that that you have to guard against if
you have any of those in the code then
you're doing it wrong you know to begin
with so you should get a bug all right
yeah ok any other questions for the day
I think everybody's ready well not time
time's up anyway everybody's ready for
beer I think yeah thank you very much
for you enjoy your evening enjoy your
beer and throw your time here thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>