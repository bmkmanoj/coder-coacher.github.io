<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction to Apache Kafka by James Ward | Coder Coacher - Coaching Coders</title><meta content="Introduction to Apache Kafka by James Ward - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction to Apache Kafka by James Ward</b></h2><h5 class="post__date">2017-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UEg40Te8pnE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm James Ward I'm a developer at
Salesforce and this is intro to Kafka so
let's dive in oh and feel free to ask
questions throughout the presentation so
don't we won't wait till the end so ask
them anytime
all right so sometimes this is kind of
what our data integration feels like
it's these rats nests of wires and you
know I'm responsible for that one wire
and I've kind of lost track of like how
it intertwines with all the different
systems and so this is what we're kind
of and a lot of enterprises this is what
things look like and it's become a real
challenge to move forward as things have
gotten more complex so what is that
complexity what is what are the reasons
why we kind of got to these complex
integration systems that we that are
kind of out of hand so here are some
challenges one is that we no longer
really have a system of record for a lot
of our data if you just look at
something like a contact object it could
be that part of that comes in from a
lead system part comes from a customer
portal part comes from somewhere else
right and so we're trying to like munch
together all these different data
sources for each application and the
type of view that it needs of a
particular piece of data so that creates
for some significant challenges not
having a single system or record anymore
for a lot of our data synchronization is
hard on my phone it can't even figure
out how to synchronize email sometimes
and that's a pretty easy synchronization
problem or should be a pretty easy
synchronization problem when we get into
these deep graphs of data it becomes
really really complicated to do the
synchronization really error-prone a lot
of challenges around that then scaling
our ETL is been really hard for a while
we tried to just put bigger machines at
our ETL problems and hope that it could
keep up with the amount of data that we
are processing but we what we really
want to be able to do is scale our ETL
processes horizontally but most most of
the time
weren't able to do that for a variety of
reasons then processing of this
integration data can be really error
prone there can be fields that we didn't
expect to have the data that they had or
not have the data that they had parsing
is it can be really err prone so these
are some of the things that have kind of
led us to that rat's nest of integration
complexity so what I want to talk about
before we dive into Kafka is like like
so how do we address some of these
problems and then of course like how
Kafka does it so the first big thing
that we're beginning to move to in these
integration architectures is using
events instead of tables as the source
of truth for the state of a system so
you can see my wonderful drawing there
of an example of this where let's say we
want to post to slash contact a REST API
and then we want to do a put to do an
update of the data and so we're going to
do is we're going to feed those events
sequentially through a thing and then
from that thing we can feed that data to
other systems so we can feed that data
to a typical relational database that's
going to turn those events into a
creating an update and only keep track
of the final state or we're going to
feed that data into some some process
that's going to like do a count or suit
do some type of Business Analytics
against those events we could also hook
this up to search indexers or to Big
Data data stores or to whatever but what
we want to do is we want to look at the
system of truth as being the actual
events and keep those around so when we
take those events and we start keeping
track of all of them and making that our
source of truth then what we we another
thing that we need to be able to do is
go back in time rewind back so let's say
that we turn off our processing because
we're doing an update or there's a
problem we need to be able to go back in
time to where we left off or maybe go
reprocess some of these events and so
that that thing needs to not just be an
event stream where the events are flow
and through but it also needs to be a
ledger and a ledger is something that we
can go back in time and get the old data
from so in the third way that we address
some of these problems with with our
integration architectures is that thing
really needs to be distributed from the
outset it can't be something that we
scale through adding faster Hardware to
has to be something that we scale
horizontally so of course you probably
guessed it
that thing is Costco and we'll talk
about in a second but one of the things
you might be wondering looking at that
is oh that kind of looks like a
messaging system like we've been doing
that with messaging systems so why not
messaging systems for this there's some
issues with with using messaging systems
to address integration in those ways
first is ordering is ordering really
guaranteed all the way to the
consumption side in the message system
maybe maybe not it's it's not always a
guarantee that we can rely on and it's
pretty important to have ordering as a
strict guarantee when we're relying on
that vent stream if we get that create
event after the update event then that's
not going to make any sense right and so
we really need to make sure that
ordering is guaranteed in our systems
can we horizontally scale our messaging
system maybe maybe not and so that can
be a challenge with how we scale our
integration processes and all the things
that hook onto it it's pushed the right
way to do things might be might not be
right it's with push we we have some
challenges like what if we have
different speed consumers and one
consumer can process things really fast
and one process of them really slow how
do we deal with those sorts of
situations how do we do back pressure
which is kind of the new cool thing and
reactive programming how do we do back
pressure with push it's actually pretty
hard pull actually is a lot better way
to apply back pressure to a system so
those are some of the reasons why a
messaging system may not fit to be that
that data store that we want so let's
talk about
so Kafka can be that thing it can be the
the kind of central hub of our
integration architectures and other
things where we can hook all these
different systems feed all those events
in and we can then hook different
processes to them different data stores
so what Kafka actually is is it's an
event ledger so it's a ledger so it's
keeping track of all the messages that
come in its distributed by nature and
it's redundant so it's creating copies
of these events as they go through the
system so sometimes you'll hear that in
Kafka LAN called a distributed commit
log and if we break that down a log you
know just like you have a log of stuff
you know of events in the system and
then commit log
these are append-only we're writing to
them and then distribute it across
multiple nodes and scaling horizontally
so that all sounds great but you know
does that distributed thing actually
work there's a lot of benchmarks out
there around Kafka and it does turn out
that yep because of the architecture
that they've used to implement the
distribution we do get linear scaling
with Kafka so as we add more loans with
more nodes we linearly scare scale that
and Kafka because of some of the ways
that it's implemented we'll talk about
in a few minutes we can get near Network
speeds of events processing through the
system okay
so that's enough on the pitch of Kafka
let's dive into how to actually use it
what it actually is so some of the
fundamentals here are that Kafka uses
messaging system semantics so you this
will seem very similar to a messaging
system and that was intentional they
wanted to see him approachable for a lot
of people and most of us are familiar
with messaging systems so you'll see the
same terminology used in Kafka as you'll
see elsewhere
but there really is a completely
different way that it's actually
implemented underneath the covers so
you'll see producer-consumer topic those
sorts of things
clustering is core in Kafka you would
never run a production Kafka instance
with only one server that just
not at all makes sense in the Kafka
world it's really intended to be
distributed horizontally across a bunch
of different nodes so we'll talk about
how how the actual nodes come into play
in a few minutes and then Kafka has
durability and ordering guarantees so
it's going to guarantee durability based
on either time or amount of data and
then it guarantees ordering we'll talk
about how it does that in a little bit
so that's kind of the core of what Kafka
is all about is those those features but
what are some reasons that we would
actually want to use Kafka let's go
through some of those so first is modern
ETL change data capture so we can
definitely use Kafka to be this hub this
new like integration hub for all the
data that's flowing through our systems
and we can hook ETL processes to those
we can use that for change data capture
so that's one use case data pipelines is
kind of the more modern use for Kafka
and really kind of the use case that
Costco is created around was this idea
that I'm going to have this hub and I'm
going to hook all these things to the
hub so I'm going to hook my search
indexer I'm going to hook my machine
learning processing I'm going to hook my
big data storage I'm going to build
everything off of this this cost github
so I've got lots of different producers
feeding data into Kafka and then I've
got lots of different consumers that are
doing all sorts of different things with
the data that's the data pipelines use
case another common use for Kafka is for
big data ingest so what we a lot of
times in our systems we need something
that can be a big buffer so it can
receive like trillions and trillions of
events and be able to buffer that data
because the downstream systems may not
be able to keep up with the stream of
data through the peak times and so we
want this buffer to be able to really
quickly get that data record that data
and then allow the systems behind it to
catch up go back in time if they need to
those sorts of things so those are
primary use cases for why we'd use it
let me show you a quick little demo of
little Kafka app and this is this is
kind of like a data pipelines
application so it will give you an idea
for some of the data pipelines stuff
that we can do with Kafka so first off
I'm running my Kafka server here so it's
running just on a single node that's
great for development but and don't ever
do that in production and so my Kafka
server is up and running locally and
then I've got a play framework web
application that is running and
connected to that Kafka system and then
I'm also going to start up just a
console consumer which will listen on a
particular topic in Kafka for messages
and we'll see as those messages come
through so let's start up that and we
should start getting some some messages
coming through here in just a sec ok so
let's go to our actual app now so what
I've done is tried to create a
ride-sharing like application where I've
got a driver and you see that it
geo-located the driver there and then
what I can do is I can place a writer
here on to onto the map somewhere and
the driver would go over to their screen
now sees that writer and if we go back
to here we'll see that now I'm
broadcasting these messages from the
browser to my server and to Kafka where
it's saying hey I'm a writer
right now we're listening to the writers
feed of data and it says here's my Latin
long and here's my status is available
ok so now as the driver let's go select
that writer and we're using some cool
map box technology here to to figure out
the route between the driver and the
writer and you'll see that now the
driver is heading towards that writer
and what's really cool about this which
is some fun JavaScript to write is it
knows that the route will take 4 minutes
and that car will actually take 4
minutes to move along that route so that
was kind of fun ok but then on the
writers view we switch over and we see
sure enough there's the the driver
headed towards me ok so all this data is
flowing through Kafka in this case and a
more extended version of demo I hook it
up to Cassandra for the big data data
store so we're tracking all these events
and
Sandra and have those there for Korean
and those sorts of things later
I'm also hooking up to flink for
real-time analytics and all those sorts
of things but wanted to show you the
basics there's a little bit more than
just a console log Kafka and these some
of these server technologies they're
kind of challenging to demo because it's
you know it's just text right how do you
make text look interesting so that's why
I came up with that coober a little demo
is there any what is it
UI so yeah so there is a UI that
confluent provides it's like a
management dashboard is one way you
could do that you can do the console one
as well I'll talk a little bit later
about Heroku Kafka which is Kafka in the
cloud as a managed service and they have
a way to introspect the events through
their UI as well so so really anything
can be a consumer all that this command
line is is a consumer on Kafka and then
posting what it gets is messages out to
the the console so you can make anything
be a consumer to takaka and get those
events and visualize them yeah okay so
that's my little cube wrap and just a
fun way to show okay great we hooked a
driver UI
and a writer UI together through a
message system that was built in Kafka
okay so it works that's great
let's let's go on to some of what Kafka
actually is and then we'll dive into the
code behind that demo in a little bit
okay so first part of Kafka is records
you could also call these events or
messages so different terminology for
the same thing but what they are is a
key value in a timestamp those are the
pieces of a record they're immutable so
once you create a record you can't ever
change it so there's no like update
operation in Kafka there is basically
only insert or append and so once we
create a record it's written essentially
forever with some caveats we'll talk
about in a little bit append-only so we
can only append events into the ledger
that's our only action and then they are
assisted so it actually persisted to
disk - one of the cool features of Kafka
is that it really does a good job of
optimizing storage because in some
messaging systems that have durability
they'll have a copy of the data in
memory in the application and a copy of
the data on disk and Kafka says let's
just basically do everything against
disk because disks have great caching
now or the operating systems have great
caching around disks so let's just use
the disk right and so really the cost
key application is kind of almost a
lightweight wrapper around just these
operations writing to files on disk ok
so they're persisted so we would also
call this a log record part of this so
we have producers and consumers so we in
Kafka have brokers is the name for a
node in the cluster and a producer
writes records to a broker and then a
consumer reads records from a broker so
it was actually an important thing
because Kafka is not actually doing push
the consumer connects to a broker and
asks for blocks of records basically
we'll talk about how that works in a
little bit but it's not actually doing
push the consumer is asking for records
so and then we use a leader follower for
clustered distribution we'll talk more
about how that works in a little bit so
inside of Kafka there is topics and
partitions so a topic is really just a
logical name for one or more partitions
so in Kafka we'll name a topic like I
have one in this application called
writer and then I can assign one or more
partitions to that topic so the
partitions are what actually gets
replicated so the replication is not
really at the level of the topic it's at
the level of the partition then ordering
is guaranteed only for a partition so
when I'm writing to that writer or
driver topic I'm writing to a particular
partition within there
and my ordering is only guaranteed for
that partition if I do for some use
cases need ordering across the partition
then I can use timestamps as a way to
handle that it gets a little bit tricky
like what is the timestamp actually mean
for my use case is it the time the event
was generated it's at the time it was
received into Kafka there's different
times that you can associate with an
event but Kafka allows you if you need
to use timestamps you can choose which
one works for you you can have Kafka
automatically timestamp it when it's
received you can have or you can provide
your own timestamps so up to you but
ordering in terms of the sequential this
is is guaranteed for a particular
partition okay
so then there's the offsets and the
offsets are the way that we keep track
of that ordering and and deal with it so
the offset is what's called in Kafka is
just the sequential ID that gets
assigned as soon as I write a record to
a partition so whenever I do a write
it's going to say okay what's the next
integer essentially or next long to
associate with that message so you'll
see that in a given topic I'll have the
same offset IDs across my partitions but
I would never have duplicate offsets
within a given partition so then it's
the consumer both Kafka and the consumer
worked together to keep track of these
offsets so when I'm a consumer I can ask
Kafka and say give me my offset what is
my offset and if it's the first time
I've ever connected to that topic then
it's going to say your offset is like
zero right but if I disconnect and
reconnect on ask Kafka again what's my
offset it's going to there's a few
different ways that we can tell it to
which offset we should start from which
or which one we left off that so the
consumer and Costco worked together to
deal with these and we'll talk more
about message the guarantees for
delivery and a little bit for consumers
so but some of the benefits which we've
talked a little bit about for having
these offsets ideas is that we can
replay so we can always go back in time
depending on our durability window how
long do we specify these events will
live and that can be based on time so it
could be days weeks months it could be
basically infinity if you have enough
disk space or it could be amount of
storage that you're using so you maybe
want to say after a terabyte then I want
to start flushing out off the back of of
this partition so the event to one of
the other benefits is now I can have my
consumers at different places within
reading through the actual messages so
one consumer could be it offset five and
another consumer could be all the way
down at the head so that's how we
support different speed consumers with
Kafka those are our offsets so let me
zoom in a little bit to that one so
let's say that we have four brokers in
our cluster and we have one topic so
that topic is then partitioned into four
pieces so here's how our actual
partitioning works for for message
production so we're going to produce a
message and we produce a message to a
given partition within a topic so when
we send a message we're only going to
send it to a given partition so you
could so what that means is that we're
going to write to a leader of a
partition so remember there's a leader
follower architecture in Kafka so when
we write we're writing always writing to
the leader of a partition so that's an
elected leader I won't go into the
details of how leader election works and
stuff but we write to the leader and
then what happens is that there are
followers that will then replicate that
data out to other nodes and we can do
this partitioning based on we can either
do it manually so that would be actually
like given telling it which number
partition we want to use or we can
partition based on a key as well and let
Kafka handle the actual partitioning for
us
so what we're setting when we set up a
topic is we set the replication factor
in this case our replication factor
would be 3 which is saying okay we have
three copies of the data and then one of
those is going to always be the leader
so in this case we would have our four
partitions each would have its two
replicas and be be scaled across the
cluster of machines so kafka will take
care of auto rebalancing the partitions
across to all the nodes in the cluster
okay so then
oh and by default when if you just
produce a message to a topic it's
essentially doing some kind of
round-robin across the partition so you
can definitely use do message production
without doing any partitioning and in
that case Kafka is going to do the
partitioning automatically for you or
you can do it manually specify the
actual like number of the partition or
based on key okay so then consumer
groups are how we do scale out of
partitioning and dealing with partitions
on the consumer side so I've got my
partitions right and I've got a group of
nodes that's all going to be consuming
data from a topic but I really don't
want if I have let's say two nodes and I
get a message into partition zero on
server one I don't want that message to
be delivered to both nodes that are
processing the same data I want to have
the scale out so that I can add a bunch
of nodes to do processing in parallel
but I really don't across to what we
call a consumer group I really don't
want to have that message delivered
multiple times to multiple nodes within
my consumer group so Kafka takes care of
that for us through consumer groups so
really what a consumer group is is this
logical name for one or more consumers
and the message consumption is then load
balanced across to the different
consumers within consumer nodes within a
consumer group okay so that's how we do
the partitioning on both the production
side and on the consumption side so now
here's the fun part let's talk about
message delivery guarantees so with
Kafka we
different ways to look at delivery
guarantees before we go into that any
questions about the the production or
consumption partitioning I know that
gets kind of tricky yep yeah yeah good
question so the followers are really
just there in case our leader node goes
down then the cluster will reelect a new
leader within within the cluster and
then once we bring up a new node to
replace that one actually we don't even
need to bring up a new node Kafka will
automatically see that okay my
replication factor is 3 I've lost a node
so any of the partitions that were on
that node now need to be set up on on a
new node and so Kafka will automatically
manage that for us but but so we always
have a leader over partition our
production and consumption is always
against the leader and the the replicas
are really just there in case a node
goes down so that we have the data
basically a hot standby we can flip over
to we can flip the leader over to that
one in the case that a node goes down so
they're just there for for that use yep
it is yeah so there's there's a bunch of
different configuration options for how
to do that but yeah they are doing a
quorum it's it's like a modified quorum
algorithm for for that yeah so good
point thank you so the the whole cluster
is using zookeeper to actually keep
track of the state of the cluster yeah
okay
let's talk about message guarantees so
there's guarantees different guarantees
on both the production side and on the
consumption side so let's talk through
each of those so for producing messages
to Kafka the the default way is just to
say async so with a think there is no
delivery guarantee I have no guarantee
that Kafka actually received and and
recorded a record when I send it so for
performance that's going to be the best
right that's going to be the highest
performance no acknowledgement of
being received the next thing setting we
can tweak is to say okay I want to not
actually acknowledge that the message
was received or want I do want to
receive acknowledgement that the message
was received from just a leader and not
just received but actually committed to
the file that's underneath Kafka so
that's the next option and then the
following one is I want to guarantee
that it's not just received by the
leader but has also been received by a
quorum of the followers of that data so
if you really need to make sure you're
not going to lose any data then you'd
want to go with that third option so it
certainly depends on your use case and
so you can always this is just when you
produce messages you can change this
setting to fit your use case so in the
case of like IOT sensor data that you're
getting every half second or something
you can probably just go async and you
know if you lose some messages that's
okay right but if you're doing like
financial transactions and you probably
want to go with the third option where
we really make sure that that data has
been committed to the leader in the
quorum okay questions about production
the producer delivery guarantees yeah
there are some some blog posts and some
information in the documentation on on
the different trade off obviously there
is a trade off right with each of these
there's like orders of magnitude higher
latency so with a think it's you know
network speeds basically with the next
one I have to actually receive and write
it to disk so I don't know what the
actual numbers are but however long it
takes to receive and then write it to
the disk and then the quorum one it
depends on the topology of my network so
if I need to wait for a quorum of my
nodes to actually do the right then if
I've got high latency between my nodes
or between maybe some portion of my
nodes then that could could have a
pretty dramatic game impact so the third
one is really based on network topology
second one is probably based on how
saturated my node is and and then disk
speed and then the first one is you know
yep yeah so you can pull up all the the
diagnostic information on this through
the the confluent console yeah
any other questions on that okay those
gone to the Consumer Guarantees this is
where it gets pretty fun because this is
what most people are interested in is on
the consumer side what happened so the
default is at least once so the way that
that works is remember the the offset ID
and Costco is keeping track of where my
consumer is in reading through its
offsets so the default is that I'm as a
consumer going to ask Kafka for a block
of offsets and I Kafka is going to wait
and record that offset the final offset
that I got to until I've actually told
Kafka hey I got through all those
records right so this is at least once
because if processing fails midway
through that then Kafka only knows that
I started at the beginning it doesn't
know how far I got through that block or
records so then it's going to then read
potentially redeliver some of those
messages so that's the the default the
next option is at most once and in that
case what we do is when we ask Kafka for
that block we like right at the
beginning say okay I got all those
records and so what that's saying is
that all right if next time if something
fails halfway through then Kafka is
saying okay I'm just assuming you told
me you got through all those records so
that's why it's at most ones
so those are the two easiest and then
what most people really want we'll talk
about in a second but what most people
get to and use is effectively once so
with effectively once we're doing at
least once delivery but then we have
some way to make sure that we're not
going to reprocess that data in a way
that that would be would alter the state
you know by by processing the same
record more than once so easy ways to do
is that if you have an item potent
service then that's you know so it's not
actually mutating any state or we can
use a
a unique ID for this event then we can
basically guarantee that okay we've
already seen this ID we've already
recorded it or maybe we recorded it but
it's not going to actually change the
state again then we can get effectively
once processing so that's pretty common
in our systems is to do effectively want
now exactly once is what everyone like
says that they want but most people now
agree that you can't really do exactly
once processing it's basically
impossible there is a way to do it in
Kafka so with Kafka you can keep track
of your consumer or of your offset so if
you're keeping track of your offset then
what you can do if you really really
really want exactly once and let's say
you have a transactional database system
so what you could do is take that offset
and you could put it into the same
transaction as your gate as the data
that you're processing and so if the
transaction fails then the recording of
where I am in the stream what my offset
is is also not going to be updated right
so if you can roll all that stuff into
one transaction then yeah you could you
could potentially do exactly once
delivery and this is true not just of
Kafka but of any any type of system that
does message delivery exactly once is
really hard unless you have this
transactional system that you can do it
against but at least with Kafka you do
have that concept of that offset ID
where you can keep track of how far you
got through processing okay questions on
that one okay so I want to highlight
some of the cool features of Kafka let
me check my time cap some of the cool
features of Kafka and then we'll dive
into some code so log compaction is one
thing that Kafka does it's really nice
so if I in my case we look back at my
data here my my writer has been picked
up and my writers lat lawn isn't
changing right and so it would sure be
nice like as this data is streaming in
I'm writing every single one of these
two Kafka but would it be nice to not to
through all that disk space if they're
just duplicate records and so with
Costco we can do log compaction how that
works is that there's there's kind of
like a sweeper that's coming behind the
at the end of the log file if you've
turned on log compaction it's an option
and it'll come through and it will sweep
through and compact down similar the
records with the same key down into a
single record so it's just preserving
disk space is ultimately what's
happening so we do then have gaps in our
offset IDs in this case but kafka deals
with that so if you ask for an offset
idea that has been compacted it's going
to choose the one that it was compacted
into because these are just sequential
IDs yeah it does yep so so this is it's
essentially a background process that's
running on your files that is doing this
this sweeping from the backside it
doesn't impact your it doesn't directly
impact your production or consumption
because that's all working like off the
front of the file whereas this is
working off the back of the file so it's
just going to be more disk read and
write overhead essentially on your nodes
so yep there is a trade-off to it for
sure so it depends like you know what is
my data look like do I really want to
compact down these records that are the
same to save disk space or you know if
you're not dealing with like IOT streams
of data like you may not even need to
turn that on so it uses the key of the
record so if there are sequential
records that have the same key that's
what gets compacted down into a single
record so the values could actually be
different this is I think you can
override this behavior with settings but
I think the default is is if the key is
the same compacted down if they're
sequential and it doesn't matter what
the what the actual value is okay so I
mentioned this earlier but disk not heat
so a lot of messaging systems that do
any kind of durability they basically
have two copies of the data the one in
memory that they're working on and then
the one on disk the
there you know in case things die and
what Kafka does is it just uses the disk
and they're kind of I think revelation
around this was that operating systems
are already keeping a cache of disk
reads and so why have two caches of that
right you can have the disk cache that
your operating system is doing and then
you can have the cache and memory in
your application and Kafka said let's
just basically use the disk cache like
once these reads are in the operating
system read cache then they're super
fast and why replicate those into memory
in my JVM so it really uses the disk not
the heap to be much more efficient and
uses the operating system disk caching
to be efficient about that so another
cool feature here is page cache to
socket this is a modern I think it
started on Linux may be solaris or
something but is this optimization where
you can have this really fast pipeline
of data from from disk to a network
socket so instead of always going
through Kafka in the case of like
replicating data to other nodes it
doesn't necessarily have to go through
the JVM process it can basically just
take a chunk of disk memory and pipe
that directly to network so really nice
efficiency for how it actually moves the
data through the network there are
certainly times where it does need to go
be read into Kafka but but for some use
cases like replication doesn't need to
you can just copy straight from disk to
the network socket super efficiently
so then balanced partitions and leader
so Kafka is is always going to keeping
track of the state of the cluster and
doing this rebalancing of of the leaders
so remember they're for each partition
there is a leader that was elected and
it could be the node is starting to get
kind of overwhelmed with the number of
messages that are coming into it and
maybe because you know one partition is
really noisy and another one isn't it is
noisy so maybe we should move the less
noisy one over to a different node so
that it's performance isn't degraded
because of that
one partition so Kafka will do that
automatic rebalancing for us and will
automatically rebalance the leaders as
well because we have these replicas of
the data we can essentially at any point
in time elect a new leader of a
partition and then all of a sudden the
the producers and consumers for that
partition will connect to the new node
automatically so the Kafka client is
using zookeeper to understand to get
events on like okay a new leader has
been elected now connect to that new
leader that's all happening under the
covers for us so one of the challenges
that we may have is a particular
producer or consumer may just like
totally saturate a node and if if that's
happening we may want to put some quotas
around like okay how much how many
events per second can particular
producer consumer work with so that we
don't set full each saturate nodes in
the cluster so that's a nice feature so
in another nice feature of Roku Kafka I
work at Salesforce and we provide a
service through Heroku for Heroku Kafka
so that's how I got into Kafka was like
I didn't want to set up and manage the
zookeeper clusters and the caca clusters
I just let her oh could do that and so
now I can just use Kafka and not have to
worry about the management of it so
pretty pretty nice thing there okay so
let's talk about the clients for Kafka
so the JVM is the official one and
that's because Kafka is built on the JVM
but then there are other clients for
really anything you can imagine at this
point so you can definitely find find a
client out there for Kafka for whatever
you're using as I've talked about the
clients are pulling based on the
consumption side which is pretty
different than than most typical
messaging system consumers so the API
for Kafka is really pretty basic this is
the raw kind of basic API of Kafka's I
have a producer that sends data and then
I have a consumer that can fetch the
offset confess to the offset forgiven
partition and can fetch the records
based on the offset so really simple API
underneath it all
but what I prefer to use with Kafka is
something called akka streams and so
I'll give a quick little overview of
akka streams so that some of the code
I'll show you in a minute will make some
sense but what akka streams is is it's
an implementation of reactive streams so
if you've heard of rx Java or akka
streams or there's a number of other
implementations out there there's a
standard that a bunch of different
vendors work together to create and then
there's implementations of this thing
called reactive streams so what it is is
source sync stream processing stream
programming so a source is something
that produces events and then the sync
is something that consumes the event so
one of the really nice features of
reactive streams and of akka streams is
this idea of back pressure that a
consumer can actually put back pressure
back on the producer so that the
producer can we can basically tell the
producer hey slow down I'm not ready for
more records and that can make our
systems more resilient and can help us
deal with ultimately deal with spikes in
in things much better
so that's built into akka streams and
reactive streams so what I'm using is
something called reactive Kafka which is
a naka streams implementation or wrapper
around the kafka client so it has the
back pressure as the source and sink for
akka streams that hook to Kafka so it's
pretty nice akka streams pretty simple
API when we start with with the basic
stuff so I can create a source in this
case I'm just going to repeat hello
world and then I can have a sync which
in this case just prints out whatever
the sync gets and then I create a flow
which links together a source and a sink
I can chain sinks together I can combine
sources all sorts of different things
but then when I run this flow it's just
going to continually repeat out hello
world so in this case no back pressure
or no fancy stuff just a basic example
okay so that's our quick little intro to
akka streams so let's go take a look at
some of the code here
or for the the coober demo oh and I
should put this up real quick all the
code for this is on my github and it's
called coober for Kafka uber creative
I'm a creative one okay so in my code I
have this Kafka helper and we need to
provide Kafka with a serializer and a
deserialize er so the serializer
for writing records deserialize ER for
reading records and we need to do it
both for the key and for the value so in
this case I'm using JSON for my
serialization you probably wouldn't want
to do that in the real world because
Jason is pretty heavy there are some
better options like Avro protobufs
something more compact and and
ultimately much more performant is what
you really want to get to with Kafka
Jason is Jason serialization is really
slow so for the type of volume that we
deal with in Kafka usually we don't want
to use text-based serialization so but
for this demo it works really well okay
so I've got my serialize err and
deserialize err also I have some stuff
that helps me connect to Kafka because
in this case I'm using SSL certificate
since I have to deal with some of the
stuff therefore that okay but then yeah
that's right so so I would recommend
that in production you may want to use
SSL on Heroku you have to use SSL and so
that's that was what that code was doing
was specific when I'm working locally
I'm actually not using SSL but when I'm
running on Heroku I do use SSL so that
was just some plumbing to set up the SSL
connections and deal with the
certificates and that sort of stuff yeah
thanks okay so now I've got my little
Kafka class here and this is Scala code
sorry if it's a I'll try to walk through
it for those of you that aren't familiar
Scala but I create my producer settings
and my consumer settings and these are
ultimately just just configuring for for
a given producer and consumer what are
my serializers Indy Syrah
Lazer's and then there's one other
little thing here oh and what server to
connect you now one other thing here on
the consumer side is my group ID so
remember my consumer group is a group of
nodes that's going to make sure that
we're only only one node in the consumer
group is going to get delivered that a
message so that's what the consumer
group is doing is grouping nodes in this
case I only have one node so I'm just
generated in a random UUID is my
consumer group okay
so then I've got my sync and this is
where I'm actually using that reactive
kafka library and I create a plain sync
which is my sync is going to be the way
that I get messages into Kafka so I'm
going to send a message to a sync and
then reactive Kafka will send that
actually to Kafka and then my source
takes a topic and then what it does is
it subscribes to messages on that topic
from Kafka so I've got my sync in my
source and that's how I communicate with
with Kafka so now here's where the part
that kind of pulls it all together is I
have two WebSockets so the driver UI has
one has a WebSocket the writer UI has a
WebSocket and what I'm doing is just
wiring together these two different
messages message feeds so on the writer
I'm broadcasting messages about my
writer position so this guy right he's
broadcasting his position in his status
that's the writer topic and then the
driver is broadcasting its position
right and so what I need to do is the
driver needs to broadcast its position
but then receive the writer information
and then the opposite right and so what
I do is I set my sync as being the in
this case this is the driver WebSocket
so I want to produce messages to the
driver topic and Kafka and then I want
to read messages from the writer topic
Kafka so that's my source and then I set
up my WebSocket with a flow that links
together the sink and the source so
that's what ultimately drives the that
two-way communication across that
WebSocket okay so pretty simple to kind
of wire to get
those things but let me show you one
other quick thing here if you dive into
the cuber code there's a whole lot more
here there's connecting to this cockatoo
Cassandra but then I think the flink one
is pretty cool so flank is this great
real-time stream processing engine it's
open source it Apache and it adds
out-of-the-box support for Kafka so in
this case what I want to do is I want to
keep track of what's my average wait
time that someone is waiting for the car
to get to them and I want to just keep a
rolling average so what I do in here is
this is a little flink client app where
I'm coming to the driver topic and then
I'm just keeping track of my route
totals so how many how many routes which
is a connection between a driver and a
rider
how many routes have I had and then
what's the total number of seconds and
so I'm and then I can calculate my
average from that so what I do is I take
my stream from Kafka I do a little bit
of parsing and partitioning of that and
then I do a fold on that so the cool
thing about flink is that I can run this
on a bunch of different nodes and it's
using some of those features of Kafka
with the consumer groups to actually do
a fold which is a aggregation
computation across all these events and
in this case I haven't done any
windowing and flink like I could do
time-based when doing or number of
Records base window and those sorts of
things in this case it's actually just
recalculating every single time an event
comes in so probably not what I'd want
in a real real world case but but that's
it and then if we run this it's just
going to print out the average every
single time a record comes in so so if
link is pretty cool stuff sorry didn't
have enough time to go into that didn't
plan on that today because it takes a
bit longer to go through the flink stuff
but but you can go check out the code on
my github and go explore all that ok so
that's your quick intro to Kafka and
high-speed I think we're just about out
of time but if there's any last
questions feel free to surmount yep
yeah good question so you can download
you can download Kafka like as a zip
file and you can start up your Kafka
server locally what I actually did in
the case of this coober app is I just
because Kafka is just some jar files and
maven central I just sat dependent set
dependencies on Kafka and then I start
up the Kafka process as part of my build
so that you don't I don't have to tell
the when you go clone the coober repo
you just run the build and can start the
Kafka cluster from there and it
downloads everything it needs so the
other option would be download their zip
file extract it start up the server and
you don't do it that way but if you go
to the cuber repo you'll see all the
instructions for how to how to get
started
the coober way yep cool ok I'm going to
wrap up so the next speaker can get set
up but I'll be around if anybody has any
questions so thanks thanks for having
hopes that useful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>