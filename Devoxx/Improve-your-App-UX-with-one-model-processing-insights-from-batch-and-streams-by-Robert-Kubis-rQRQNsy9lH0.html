<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Improve your App UX with one model processing insights from batch and streams by Robert Kubis | Coder Coacher - Coaching Coders</title><meta content="Improve your App UX with one model processing insights from batch and streams by Robert Kubis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Improve your App UX with one model processing insights from batch and streams by Robert Kubis</b></h2><h5 class="post__date">2017-05-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rQRQNsy9lH0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone can you hear me perfect
I heard there is like quite some
competition in the next room in terms of
popularity of speakers so I appreciate
that you found your way here Who am I
I'm Robert cuckoos I'm a developer
advocate for the Google cloud platform
so we basically understand our job being
a bi-directional connection point of you
guys into an our organization so you can
always reach out to me on a toaster or
see on Twitter or through github or
through medium or on other channels we
love to collect feedback in terms of
like what we can do better or ideas that
you have how you use our products we
love to provide them whatever I got to
talk today about is a new approach of
data processing and you might have seen
the discrepancy in title of my slides to
the one that is on the schedule so I
wanna try to address that really quickly
so if I see or look at user experience
to improve user experience there are a
couple of steps that you have to take
first very simple and different methods
that you can do that is collecting data
and I'm not going to talk about
collecting how you collect data today
second step is analyzing the data that
you have collected and then taking the
insights that you have out of this
analyzation and applying them in a way
of like improving your application now
today's talk is about looking at like
what that can be it can be on the one
hand stream like events that come in
that you collect be the click stream or
any kind of telemetry data that you
collect for your application or can be
batch where you basically collect data
for time frame and then you process data
in a batch mode and today I'm going to
talk about analyzing data that is the
topic so if that's not what you expected
feel free to choose another talk I
wouldn't be offended
but first I want to go a little bit in
the history to get everybody on the same
page like where we are coming from now
if you look at data processing at Google
there's like a long history of different
kind of frameworks and methods that
we've developed over the year to cope
with the amount of data that that is
collected and as you can see here the
the timeline starts about 2002 and
Google 197 the reason was we were really
really scrambling the first couple of
years we didn't have time to write
papers or think about novel approaches
so by the time that we could actually
breeze a little bit and think about like
okay how can we approach data processing
in a better way that's where Jeff Dean
came up with a model library sale so all
like all software engineers are doing
the same things on over again and they
are dealing with with disabilities terms
in a way that they shouldn't be they
shouldn't care about like how our data
processing programs are running and they
shouldn't care about like dealing with
failures and things like that and
resiliency so you came up with a model
and formulate that as MapReduce which
takes care of its like failover and
things like that so what is MapReduce
basically MapReduce is a batch
processing batch processing of data so
you have a big batch of data you split
that data up into chunks you apply a
mapping function we pick out a key out
of out of these chunks of form yet each
element in that chunk and then you
shuffle around and then you have
so-called user no it's where you put
these keyed elements and then the
reducer applies like an application or
any kind of coping function and then you
have like your your insight at the at
the bottom and you slaughtered somewhere
now the problem with math and reduce is
usually what you do is you don't end up
like you're writing one program and then
you have your answer usually you
to pipeline these these bad programs and
again like what ended up or what they
ended up with was in like a lot of
developers did the same work of like
moving data around from one batch
process to the next one from one
MapReduce job to the next one and
repeating and repeating over and over
again the same same kind of data so they
wanted to come up with a framework where
they deal with this as well and that is
for bloom Java comes in so 2010 we
publicized the paper about from Java
which basically gives you a higher level
API to write map introduced shops and
pipelines of multiple revenue map
interviews jobs so it's basically half
of transformations that you write it's
still bad and the code looks a little
bit like this so you basically have a
pea collection that you can process in
parallel you you apply the the
depositing which is basically my map
function where in parallel say like okay
I want to have the pausing of my
elements and then you do a grouping
function which is easy down there the
mean double where I basically group
perky these elements together and then
you can write that in in multiple as
movable different sessions and have your
output at the end now room table was
great and we taught that to every
noogler at Google and everybody started
writing flume Java the problem there's
still a couple of problems if you look
at data data is really big and the thing
the problem with this is like you're not
really done at that point like
processing big data like if you see like
you chunk this up like by week or by day
or maybe by hour but the problem of this
is like you you do the calculation and
then you have to wait for that amount of
time till you get your answers and you
want to have answers faster so the very
first problem of batch is really the
latency that you have of like processing
your data depending on the boundary that
you set like the batches that you put
like the boundary that you set for your
data the how you split up your data
that's basically your latent
see at least that you end up with of
course on top of that is the processing
time that you have so another problem
that you see is if you do this
artificial splitting so for instance per
day or per hour you end up splitting
your for instance your user sessions so
if you want to get insights about how
users are using your application that
usually doesn't adhere to these
boundaries that you artificially set and
split your data into what you end up
with is incomplete data and then if you
calculate that you basically get concur
incorrect results so what you're really
looking at in data is it's actually
unbounded it's continuous and unbounded
we somehow have to deal with this
continuous and unbounded large amount of
data that you are collecting now a
really problem there as well is that
usually data is not in a way that it is
happening and immediately in your
processing machine all right there can
be delays imagine you have like a game
that supports an offline mode and your
players go on an airplane and they put
it in an offline and play on the
airplane and then they arrive in San
Francisco 11 hours later and they've
turn on the phone again and now suddenly
all these events are streaming into your
system now they are like ray will
delayed from when they actually occurred
to and to get Rose correct results you
have to basically deal with this delay
alright so how do we do that that's
where mill via comes in
so mill real basically looks at like how
do I work with streaming data and the
millet paper basically came up with a
directed acyclic graph of elements where
you basically stream elements through
and then you do per element calculations
you group them into so-called Windows
and then you do calculations at the end
and mirror for you managers like state
and persistent flow of all these
elements so first of course like what
can you do element wise transformations
is filter that you filter based on
certain key elements
and then you educate them in Windows
that can be time windows it can be
session windows and it can be like here
when time based windows so here's like
you where you see already that we do
this differentiation between processing
time and the read time where we say okay
we don't do the windowing based on when
the elements arrive in our machine we do
the windowing based on when these event
have happened and then put them into the
window now there's a challenge at that
approach the thing is like how long do
you keep these windows open all right in
the end like if elements come in like
hours later and you want an account for
that do you keep this window open for
hours what happens in that case is again
that you you have large delay in getting
results and insights out of your data so
you have to kind of find the middle way
like when do I close the window what do
I do with like late arriving data and
we're gonna look into that how to do
that and what I mentioned before like
another day another way to window your
data is sessions where you basically
don't use a time window but you use
session windows and again you have to
decide when you close it so if you look
at event versus processing time in this
graph you see like processing time on
the on the y axis and event time on the
x axis and we have this finished line
that goes like up there now if you find
any events in the lower right part of
this graph please find me afterwards I
get you good paycheck so if you find any
element in the lower right part it
basically means that you can look into
the future you processing your data
before it even happens so so the idle
world is really like that the elements
are on that line that would mean that an
event happens and this directly
processed so you have no delay between
processing and actually event happening
now the reality of course looks
different and the reality looks more
like the red line that you see there
which we
call and watermark and the watermark is
something that we use basically to make
a decision when we think that the
majority or all events have arrived in
the system so that we can close windows
so what are the motivations for
streaming versus batch so there's three
main areas that you're looking at first
completeness second latency that are
addressed
like batch mode latency or swimming
latency and then costs like how much
money do you invest to get a low latency
or completeness correct correct results
now have a look at a couple of different
example or use cases here the first one
is a billing pipeline and I want to
point out one thing here we have a graph
which shows from not important on the
low-low and on the y-axis to important
on the higher end of the y-axis now if
you see in the middle the low latency
bar if that is lower doesn't need low
latency it actually means that low
latency is not important
drips up a lot of people me as well in
the beginning so I want to point that
out so in the billing pipeline what is
important is completeness do you don't
want to overcharge your your customers
and you don't want to under charge them
because that's best bet for your
business right so you want to really
make sure that your billing pipeline is
correct so that's really important now
it doesn't really is not important that
you charge some day after the dip the
cost secured or if it's like two days
all right so latency is not so important
in that pipeline low cost again is not
that important you want to be correct in
this pipeline so you want to invest some
money that you are actually correct in
that now what is nice is for user
experience for instance is if your users
see on what trajectory they are in terms
of the costs that they are occurring now
that is something where you want to have
like an estimate the accuracy is not
super important that still should be in
the ballpark of where they end up you
shouldn't have a trajectory of 100 euros
and then or
miss Fong and then end up with a
thousand at the end of the month but if
it's like 950 versus 1000 it's not like
like a big difference but you so you
want to be somewhat correct you want to
have low latency so the district err you
should show like pretty close to when
you are cure the cost so the person sees
actually the trajectory and you want to
have low cost in the sense like you
don't want to invest too much money into
the pipeline it shouldn't eat into your
margin like all the way and the third
use case is abuse detection now for
abuse detection complete list is not
that important low latency is very
important so like if your systems are
abused and incurs cost for you that you
cannot charge anybody for so you wanted
to take that as fast as possible now of
course you don't want to interfere with
your customers and that then set you
basically constantly name your correct
and relate and pop our customers as
abuse customers so still like somewhat
importance in terms of like that is
correct
but low latency is much more important
in that case and low cost in terms of
that you can run that like really really
fast but again you're also net not that
important you want to invest some money
because if you prevent abuse it
basically didn't it doesn't cut them
your margin alright so if you look at
historical systems how that was solved
like these use cases that we looked at
you look at sources something to call
lambda architecture where you basically
look for the correct results where we
needed so for instance like billing the
billing pipeline you used like a hadoop
system for instance or spark system
where you calculated your billing in in
batch processing now for your life
estimates or for you abuse pipelines you
use like an apache fling or apache storm
which like a stream processing the
problem of this is your your developers
or you have to learn through systems
right you on the one hand learn do per
spark and on the other hand you learn
fling and storm to accomplish both poses
challenges now it would be nice to
actually combine that in one and only
have to learn one programming model and
that's where data flow and the patchy
beam comes in so a Petri beam and data
flow combines basically these two worlds
of batch processing and stream
processing into one program model so you
have to learn one program model and you
can both can do both so how does Apache
be more data flow data to do that first
before I go into the details I want to
talk about what Apache beam is so Apache
beam is our data flow SDK that we
donated to the Apache foundation to
create an ecosystem which allows you to
write pipelining like data processing
pipelines which are portable so you can
basically write the data pipeline and
veggie beam and you can run it on one
spark you can run it on flink you can
run it soon on gear pump or you can run
it on cloud dataflow so this moves
basically the login you can go where
your pipelines run the best and it
builds a higher level abstraction so
it's a set like this education has a set
of high level abstractions and API so
that you can use to build your pipelines
which makes it much easier and you don't
have to deal with machines or things
like that you don't have to deal with
failures or resiliency that's all
handled for you all right so let's take
an example of how how dataflow or PG
Beam the programming model achieves
correctness in an streaming mode as well
so here in this example we have for
instance a game that you play and these
events happening throughout the time and
we have the real timeline there that you
see there now the when event number 3 is
like accurate around
12:05 and also after 3rd between 0 4 6
and 12 or 7 and the rice in the system
shortly after 12:12 or 7 so it's pretty
close to the real timeline and then you
see other ends like the rent number 9
which happens at over 1 and arrives in
the system at 12:08 so there we have
like I said
minute delay now there are four
questions that we want to ask to to get
correct results and what the h-e-b model
rules about so first what you know
already from from a map introduce job is
like what do you want to calculate so
that is your like do you want to do an
aggregation about like team statistics
or player statistics how many points the
player has achieved over time frame or
if multiple players are playing in a
team how much points did the team
achieve so that is like your what next
is where in the end time do you want to
calculate your results so the various
basically what do you do your how do you
construct your one event time windows so
for instance I want to have like the
team scores for everybody between 12:01
and 1202 so one minute window and I want
to know like who was the best in that
window the third question that we ask is
random processing time missus
materialized so that deals with like
when I'm I omitting results when I did
one do I give like results aggregated
results downstream to like systems that
I can display or if you look at like
applications like somebody that can act
on the results of this aggregation and
number four is how do you deal with the
refinements so there are multiple modes
of refinements that you can look at in
that model one is if you have very large
windows so you can look for instant at
an hour window you don't want to wait an
hour till you get results right like
we're talking about streaming system
we're talking about real time more and
more we want to have like lively
dashboards but we still want a window
like we want to do want to use one hour
windows so the way that you can do is
you can like emit for instance early
results or late results and we talked
about it like how you do that in in the
beam model all right so let's start with
with the what what very familiar to the
ones that have used MapReduce you can
basically do a parallel do on on
elements you can aggregate you can do
composite of aggregations and
elementwise transformations now in the
dim beam
model how does that look like so we have
here for instance P collection that we
read from some kind of data source that
can be in beam you have very many
collect connectors that you can connect
to that could be a cough car I could be
cloud pops up it can be anything it can
be file for instance in in batch mode
then you apply your calculation so that
looks very familiar to what we have seen
for for flume Java and then you do your
excitation and because I want to do life
simulator I going to show what how that
looks like in B so in my demo I that I
show you and like throughout the
throughout the talk I have a little game
where you get assigned the color and you
get four fields and you can hit on these
fields and I calculate a success rate
like how it successful you are in terms
of like missing or hitting the right
color and then we calculate also team
stuff so if everyone wadi of you joins
in to play this game we see like which
rich team bins for for today so what i
get as an element every time that you
hit one of these fields is basically a
time stand of when that happened the
color that you were playing for and an
ID of your player and i stream this data
in through through pops up now the first
question that we ask is like what we are
calculating so I want to calculate two
things first I want to calculate the
player statistics so that is basically
this where basically pull out the player
ID that I give you that we see here then
I group with everything that I have
windowed by this player ID and then I
calculate the player stats so we can
have a look at this very simple and you
see there that we support over it and
lambda is already basically I pull out
the team
I also pull out the device and the color
and then basically calculate the stats
it's very pretty
straight forward alright so the second
thing that I calculate here is the team
statistic that we see there we're
basically just calculate basis based on
the team color now when do i omit these
results that's where we come to the next
question but before that so if you if
you basically do that like just ask the
question of like what am i calculating
then we have like at the traditional
batch processing you better really
process everything and you get
yourselves at the end which would be
here in that case at 12 12 10 but I want
to get my results earlier I want to cut
down the delay till I get my results and
we have talked about there a couple of
different ways to to window this in this
case we use fixed windows so let's see
how that looks like so what I apply is
to my incoming stream or batch data I
create a window so I in this case I
window that in through two minutes for
the game weave into it into five minutes
so I apply this window and then say what
I what I want to do for that window in
systems I want to have the sum so now as
you can see in this crawfish so you have
our slices of 2 minute windows but we
still see that we have to wait till the
end of everything till we get actually
our results we're still not admitting we
still did not cut down on the delay and
when do we get ourselves the only thing
that we did here is we now know who was
the best which team was the best in that
two minute window so we get with nice or
results with it more like likely result
in the sense that we can receive the
variation but we don't get the results
earlier so that's where where the Venn
comes in when do I wanna processes so
when do I want to actually trigger when
do I want to emit the results and as we
can see here again we take this
watermark which is a heuristic based on
when the elements come in like we have
it's based on the source that you're
using I can give you a
eristic when you're emitting the results
so you go through and then once the
watermark is hit you start admitting
these results so how does it look like
in beam you basically define triggers so
called triggers and you have different
kind of triggers that you can use the
SDK comes with a white set of triggers
and what you use here is I want to in
this example I want to trigger a result
after the watermark basically passed so
I use after watermark pass the end of
window and everything else stays the
same so how does it look like in this
example now as we see that the watermark
basically passes and we are meeting
results much much earlier so we can see
basically much early like which team is
leading and we don't have to wait
anymore to like the entire thing is
finished so how does it look for our
demo so what I have here is basically in
the beginning I'm reading for my pops up
and because I might want to take you to
some calculations at a later point the
first thing that I'm doing is I'm
writing them to bigquery so just to
archive them so I stream basically the
data the raw data directly into into
bigquery so now the we saw the first
like we want to win do that so the first
thing that I want to do is I want to
window these elements that come in in
two 5-minute buckets
now there's the next thing that I want
to do is I want to add the triggers so
that was my when question and the first
trigger that I want to do is I want to I
want to emit results at the end of the
window now this we just saw all that
gives me resolve once but it doesn't
account for like what if I want to have
earlier results what if like events come
come later we saw in the beginning that
event can be unbounded and can be
delayed so I still have to like to find
somehow like what do I do with data that
come in comes in late and that is where
we come to the fourth question how do
refinements relate
so what we want to do is on the one hand
if my windows are very long I want to
say like okay I want to do some early
firings so you see here with early
firings after processing time where I
say like ok after one minute I already
want to emit a result so one of my team
starts emitted entry every minute so
this is an estimated result the next
thing is if elements come in late what
do I do with the elements that come in
late
so you see here with late firings after
paint that element count at least one
that basically Allah tells me like when
do I trigger when late results come in
so after pain basically if I started my
window if if I close my window if there
are any elements coming in in this case
already if only one element came in I
already trigger again the calculation
basically the coupang now you can define
like maybe you don't want to emit a new
result every time an element comes in so
you can basically say okay I only
aggregate again after each five results
or you give a certain amount of time and
then we have to define like we don't
want to keep our windows open forever
because at some point we would basically
run out of memory and our streaming
pipeline if you would keep all the
windows open forever so you need to
basically define also the stop boundary
when we close the window and then
basically if then the events come in
later after that we basically discard
the window so that is where if allowed
lateness comes in where you basically
say ok in this case I want to close the
window after 10 minutes if any event
comes in after 10 minutes after I close
that window I just discard the event now
the last thing that you have to define
is what you actually do when you trigger
one sense if the late triggers are
coming in are you doing action
accumulating fired panes which basically
means you keep all elements in your
window and you do the complete
aggregation again so if you have for
instance in some
when you do with late trigger you do the
complete calculation again now you have
other trig of modes where you basically
only do the difference so if a newer
element comes in late you only emit the
difference to the prior result that you
had that's not always possible for all
calculations so you have to see which
one you use again anything else stays
the same so you don't have to change any
other parts of your code so that is
where the how comes in so in our case we
do and I have two it's like some typing
here all right
so in our case what we do is I want to
have like five minute windows
I got a trigger after it's closed but I
also want to very early like I don't
want you to have to wait five minutes
till you see your player stats or the
team starts so what i'm doing here is
after processing time past first element
pain which basically means after the
first element in that window after coop
the first element into that window that
i defined my five-minute window ten
seconds after that i am it's the first
result and then every 10 seconds i am it
results for that window so I get like
throughout those five minutes I get
every 10 seconds an estimated result I
also allow for late firings and I also
want to already emit results new result
updated result every time and late
element comes in and I allow for this
window to stay open five ten minutes
after these five minutes like after the
last element that I grouped in this five
minutes and I do accumulation so every
time I do the complete aggregation again
for all the elements that I have in my
window so now hope that this pipeline
works actually I going to be I going to
try and deploy it
so this
comes all with amazing support and
everything but well yeah of course
okay I have some see we have to does
anybody spot the mistake that I have in
here we know the trigger comes in is
that should be fine
start up that aside then I close this
9:54 see did I put something oh I know
what is alright I worse cause fingers
build success and seems it's doing
something
alright so what you see here is knock ah
ah ah ah okay I know what I'm is
and it's one thing I didn't clean up my
demo workspace before rookie mistake so
what just happened is basically I pushed
all that that pipeline it was packaged
and put sent to dataflow now I can name
my pipeline and what dataflow doesn't
allow is that I pushed pipeline with the
same name twice so I also took of course
to cancel my pipeline first before I can
submit it again and because I don't want
to wait that long I just give it a new
name and just apply it while the other
one this is cancelling now the thing is
what you don't see here is what I did
not have to define or anything you can
respect my code like to everything is I
didn't define like how do I deploy my
machines or whatever I install on the
machines it's all done for you behind
the scenes it's the only thing that I
defined here in my in my arguments that
you see here is that like okay what run
on do I want to have so you can use a
direct run or if you want to run it on
your laptop or you can use like its walk
run or for instance or gear pump runner
so depending on the underlying system
that you have where you want to run your
pipelines you define basically the
runner that you're using the second
thing that I want to point out here is
that I said it's streaming so you like I
show in a minute is you can do dreaming
and batch so in this case I have
streaming pipelines I'm putting that the
true and then I say how much machines I
want to have initially so what you can
also do with dataflow is it supports
auto scaling so you can basically put
boundaries on how many machines you want
to have and then the other thing is like
the zone and and if I want to update my
pipeline so I could have like let my
pipeline run and just use the update
through and then I would have updated
the pipeline that would have been
another option of what I could have done
so let's go a little bit smaller here
that you can actually see you it's
so you can see here that I've deployed
my pipeline and in the data flow console
you can see like all the steps the
logical steps that we are doing so we're
reading comm pops up we've been doing we
were also eye archiving the bigquery
then we are keying by team and calculate
all the stats and write it to pops up
and bigquery and the wiki by player and
write that to pops up into bigquery so
let's have a look at the B game so if
you want to play and I would be lovely
you can go and scan that QR code or type
in that short link down there in your in
your smartphone smartphone and I'm going
to do the same as well
right so what you get is basically you
will see it on your phone but you get
basically four fields of four round
fields and of course make sure that you
as well so it looks a little bit more
beautiful on your phone then here you
can assign the color so in this case
like I get green and then I can start
clicking green and if my pipeline is
running it should at some point I should
see a result coming up so that is the
estimated result let's hope the demo
works see if the pipeline is running oh
it's still starting
alright slowly starting the system
negative five seconds that should be
fine yeah okay cool all right so you see
like I'm pretty bad with 38% there we
don't have anybody that plays red
unfortunately
okay so what you see here is like we do
the division by five minutes but as you
can see the updates are much faster so
that are our ten ten seconds triggers
that we update this data now what I want
to show quickly is I'm using the exact
same things for my petrified line and my
bad pipeline is really really small
because I don't do the windowing I'm not
working with dreams so I don't have to
do the windowing so in this case I read
all like as I mentioned that I archive
all the data so I read from that table I
keep my team I group it by team and that
I calculate team scores so now it would
but I'll probably take too long to to
run this but I gonna show you really
quickly how that looks like in the end I
am over ten minutes I have ten minutes
left cool so let's see how that looks
like for patch pipeline so this starts
my battery pipeline and as you can see
what I made here is the streaming again
I don't do an update here but my last
batch pipeline finished so it should
still succeed and all the other things
stay the same as I used for for my
streaming pipeline again if we look at
the code here I completely reuse the
code of my streaming pipeline I didn't
do anything else like if I learn the
system through our off I basically can
do batch and stream processing in one
model and that's exactly what we want to
achieve with a patchy beam now what I
going to use to show you how the data
looks like is bigquery
so let me open up bigquery quick
the big here is our data warehouse
analytics tool it's basically a column
store where you can process and and run
queries over terabytes of data in
seconds now the first one what I want to
show you here is we have this collection
color smash and here we have like our
all the data that comes in if you look
at the preview you see here the only
stuff the only things that I collect is
the timestamp the UI ID that I generate
in the application the team that you're
playing for the color that you hit and
the device that you that you had if you
play it on desktop or on mobile and if
you go to last we should see a lot of
Mobile's here yes so you see also here
the time stems is pretty accurate
through what we see right now now the
player stats is where like all the data
that I aggregate and calculate I on the
one hand push it pops up so that you can
see it on your phone and that could be
for instance like if we go back to the
improving your your user experience you
can basically put a builder pipeline
where you take this insight to improve
the to guide your users to certain parts
of your application or you show certain
promotions based on what the user is
doing your application so how does this
look like so what I'm collecting here is
basically again the timestamp of the
window when when that triggered the team
how many points that team had and then
the ratio the device and when this
firing was so you also can inspect and
get the information off the triggers was
this an early trigger or was it an
on-time trigger or was it a late trigger
so you get that information downstream
in your systems so you can handle these
these elements these results in the
right way and the same here the the team
starts where we basically calculate the
the rate the hit ratio per
so let me have a look if my pipeline
finished if my pipeline finished that I
probably can show you yes so it's
finished so we can check so one of the
very said I want to show let's use this
one so what I do in this query so what I
put s and an element in my knee make
that a little bigger now the bench if I
do batch processing I don't have any
timing information now I know triggers
or anything so I basically populated
that field with all time and now you can
see like that I get just calculated
let's go to the last okay it hasn't come
through it said I basically calculated
the aggregate of all time so I have done
this game like a couple of conferences
and in total we have over 40,000 I
believe elements now in the system so if
you look at the system we have like
41,000 events that happened and now I do
the all the aggregation and see
basically what is the mean hit ratio and
we can see that red is currently the
leader and the all-time calculations
back to the slides
so basically fetch pipeline was reading
from bigquery running it through data
flow and writing it to the query again
now I mentioned earlier that you can run
beam models on multiple on multiple
runners we have data flow fling spark
apex gear pump is coming and many others
you can see what like in this matrix you
can see basically the capabilities based
on the questions that we ask so the what
where when and how and what kind of what
is supported by the different runners
underneath
so with this I want to open up for
questions and thank you very much for
your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>