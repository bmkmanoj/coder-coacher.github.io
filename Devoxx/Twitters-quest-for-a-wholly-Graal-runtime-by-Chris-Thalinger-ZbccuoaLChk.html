<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Twitter's quest for a wholly Graal runtime by Chris Thalinger | Coder Coacher - Coaching Coders</title><meta content="Twitter's quest for a wholly Graal runtime by Chris Thalinger - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Twitter's quest for a wholly Graal runtime by Chris Thalinger</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZbccuoaLChk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome welcome to Derek's my name is
Chris I work for Twitter I live in
Hawaii that's why I'm wearing the
Hawaiian shirt and my hello hat to bring
some some Aloha to you yesterday was a
very nice day I really enjoyed it today
it's a little yeah you know so this talk
first of all we we have a vm team at
twitter and it's not a big team and it's
like four core people and there are
three g's a year in two years and
there's a compiler guy and that's me and
I'm not sure if tony is on the periscope
but there's a shout-out to Tony in
Boston so if you're going to tweet about
this talk I would appreciate if you
would give some left to our Twitter team
using that hashtag and so what's the
what's the goal of this talk the name
doesn't really give away a lot what it's
really about but I'm doing this talk for
two reasons so first of all show of
hands who knows whether the JIT compiler
is okay yeah that's what I expected who
knows what growl is okay yeah good so I
explained that a little bit so growl is
hotspot has to cheat compilers called C
1 and C 2 and growl would be like a
replacement for C 2 like an optimizing
compiler but it's written in Java so
that's that's the main difference to C 2
C 2 is written in C++ and growl would be
written in Java so I'm playing around
with guava for quite some years now and
so we are using ground now at Twitter
and an out of you know walk you through
I'm working at Twitter for a year and a
half now and I'll walk you through the
experiences I had and what we found out
the bugs we found and eventually you
know the savings we are seeing just by
using that new compiler so one reason
why I'm doing this talk is I
to encourage you to try it I'll show you
what we did and I'll show you that you
actually can use it in production but I
would appreciate if when you go back
home to work next week that you you may
be downloaded you try it out maybe in
production or just an experiment or
whatever you want to do because what the
compiler needs this what drug needs at
this point in time is it needs some
exposure to to customer code to become a
production writing compiler that's one
reason why I'm doing this talk and the
other reason why I'm doing this talk is
to save money and that's the important
thing
for Twitter and that's the reason why
I'm here so I think everybody in this
room knows what Twitter is I don't have
to really explain that but it's a huge
distributed system right we have many
many many micro services I don't know
even how many but I guess thousands and
but the most important ones are tweet
service right that's reading writing
tweets that would you see in when you
open up your app user service has to do
with all the user data associated with
the accounts and timeline service it
builds up your timeline and social graph
is the interaction you know the graph
between the users who you fall in who's
following you so these are the four main
core twitter services and so one service
runs on many tvN's thousands of machines
and thousands of JVM so it's it's a huge
scale and we have multiple data centers
so if we can somehow save CPU we will
save a lot of money because just off the
scale and so Twitter in general likes to
do open source we use a lot of open
source but we also give back a lot there
is like this page the link at the top is
I think is outdated doesn't work anymore
but there's this Twitter github that IO
it lists all the projects we've open
sourced over the years so if you need
something go
grab it growling general is is a good
fit for us because it's open source so
there is there's a github repository you
just grab it you're cloning it build it
and then you can plug it into Java 9
let's see and we are trying to
contribute to growl as well
I'll give us a brief example later would
I what I unfortunately still working on
because I don't have time to finish it
up but we'll get there we also have our
own JDK built it's just easy for us to
have internal you know modifications
smaller things I mean there are the
biggest ones are the chapter 43 backport
which a check for the 243 we also call
it JVM CI the JVM compiler interface
we've included that in JDK 9 but you
can't download it in the JDK 8 you so we
back product that so we could use since
we're still running an 8 we could use
Gras in in our own 8 build so we and
then we have obviously a copy of
growling it which is really a one-to-one
clone of the github repository and
that's just because you know the build
systems they don't have access to to the
internet and you know all this right
that's that's what companies do and then
we have something called contrail most
people probably know what JFR is it's
just a reimplementation of that but that
will be you know moot soon anyway
because Oracle decided to open-source
JFR so yay for that and we can either
upstream some of our code or we'll see
we'll see what we do we had I think
that's not accurate anymore we've had
CMS improvements but I think we up
streamed all of this so we try to
upstream as much as we can because it
lowers our merge burden right if you
have a lot of local changes new emergent
new new JDK rootless it's just it's just
a pain so we we try to obscure them as
much as we can
in general about why gras is number one
c2 is a very old and complex source code
base and the learning curve is really
really steep when I started working for
Sun back in the day and I started to
work on c2 it literally took me 2 to 3
years to fully understand what's going
on it's a really complex piece of
software and just over the years it
became more and more complex so what
what happens is you know sometimes
people leave and then we need to hire
new people and these new people come in
they are used from their previous
experience or yeah I'll look at a source
code for like two weeks or two months or
something and I'll understand ok that's
not happening with c2 I can promise you
and what what then happens is that these
people the new people were hired they
get frustrated because they think
they're stupid which they're not but it
takes a while so and then people leave
before they actually get really useful
for us so we need to find something
better that's easier to understand where
we can hire people again and they are
willing to work on it that's a very
important point here then there were no
major optimizations in c2 anymore the
last couple years everything that
happened was mostly driven by Intel
because they were coming out with new
CPU instructions you know AVX avx-512 on
all that and so they were writing do
people know what intrinsic SAR intrinsic
sand the compiler okay
so it's basically a piece of handwritten
assembly code optimized for a particular
instruction set and and then the
compiler when it sees is let's say I
don't know what's a good example we have
an intrinsic for a string dot index off
right you searching for a string and if
that's there's a version of that in the
core library it's written in Java but if
you would compile it it's rather slow
but if you have by vector vector
instructions you can implement it much
faster so that that's what it is you
hand write some assembly code and then
you replace that in the compiler with
with with this assembly code and and
that's what Intel did right new new
instruction sets wider instruction sets
and they had to rewrite some of this
these instructions of intrinsics and and
that's what was going on that's cool but
there was no major improvement of I
don't know you know c2 has all the
optimizations you can think of it has
loop optimizations at s obviously in
lining it as an escape analysis
implementation you know Auto
vectorization you name it but we have
this for years it's like no one went in
and said oh let's rip out the whole
escape analysis implementation and write
it from scratch no one did that and what
I think at least and I've seen it one of
the main reasons is it's super hard to
do that if you touch something in situ
you usually break something so in my
personal opinion
c2 reaches its end of life a long time
ago it's just it's it's basically in
maintenance mode for many many years it
just works well it's a it's a really
good compiler right most people are
happy with with the performance of the
compiler people are very focused on GC
because that's the main issue right
memory but most people or all of the
people who are using Java they don't
care about the compiler it's like this
black box that just magically works it's
just a pain for the engineers to
maintain it so we need something better
and one choice one possible replacement
is grown and I I mean I truly believe
that is a good replacement otherwise it
wouldn't be doing these talks but it's
so much easier to understand for I think
mainly two reasons is number one from
ground up it was designed
modular not in the Java 9 module way
but basically a module in Rahl is is a
package let's put it this way right and
there's a there's a screenshot that's
rather old but you can see there are
there's some architecture dependent
packages and then there are some
architecture independent packages and
the bill says to make sure you don't
have circular dependencies and the build
system makes sure an architecture in the
pan and package doesn't depend on an
architecture dependent package and on
things like that right things that make
sense so you're not screwing up the code
and so growl makes sure you're not doing
this and then also very important is the
inlining in draw and the escape analysis
the implementations are number one in
learning very different and escape
analysis is just a better implementation
of escape analysis you know c2 has
basically an escape analysis it's
implemented as the paper describes it
and then growl has an advanced version
of that which is called partial escape
analysis I'm not I'm not going into
details here but let me just tell you
it's better and and we'll see that later
so bugs I I started running I think it
was mainly the tweet service I started
running a service a Twitter service with
crawl and then you know we found a few
bucks and but I think I'm listing for
bucks but these were all the bugs we
found okay there was there was nothing
else after that so I'll briefly go
through these the details are not too
important I just want to show you what
our experience was so there was a there
was a bark and it complained it couldn't
compile a method because it it wasn't
always our compilation which means on
stack replacement also not really
important what it is but so you couldn't
do that because it had locks and the
exception looked something like this
as you can see it's the compiler is
written in Java at the very bottom the
frame you see it's J decays VMC itit's
that's the chavvy mci part it just says
our compiler method right
the compiler is just written in Java and
you see you see what's going on and
these are this on sec replacement phase
just say for a reason okay so I
discussed this with an engineer at
Oracle Labs and and we said well in
general it's not good that it's
happening but it's not a big issue
either because by default you're running
in inter compilation mode people know
what tier compilation is okay so as I
said earlier hotspot has to compile of C
1 and C 2 and and tier compilation means
since C 1 is is a very simple compiler
design for throughput so it compiles
code quickly so that you you can move
away from the interpreter and run on
native code but it's not really
optimizing it a lot and then C 2 does
all the optimization at a later stage
and so the tearing means you chimp from
interpretation to C 1 compiled code and
then later to the C to compile code
that's that's what tier compilation is
and in this case the the highest tier
it's called tier 4 in in hospitai situ
tier or the growled here
if the grout just failed for that
particular compilation and what would
happen after it failed for for a bunch
of times
hotspot would just flag that particular
method on that tier and not try to
compile it anymore and would fall back
to a lower tier you would still end up
with native compiled code it's just not
that highly optimized and in this
particular case the method was not
important it was just lukewarm so it got
compiled after a while but it was
nothing you know it was not it was not a
service method or anything it was just a
core a core library method that that was
compiled not that important still would
be good to have this fixed right because
it could be an important method and in
fact the bug is closed because someone
fixed it with you C 14 changed files
thousand lines of code so it wasn't
right a big change but technically the
buck is still open so if you go
if you go on the link like the park 128
you will see it's still open and the
reason for that is after it was fixed a
tried to run it again and I saw these no
unstacked replacement note generated
once right the the one at the bottom is
nothing important but the top to so and
I just discussed this with Tom a little
bit and he says well there's a little
piece of logic in situ Esparza which
allows access to static fields within CL
in it for class and so on and so on so
the issue there is while we are
compiling the static class initializer
where the compiler tries to access
static fields but since the class is not
initialized you're not really allowed to
do that so c2 has some logic for this
that the methods called use a parse
static field okay
in CL in it okay so since we're the
compiler we know we can access it you
just have the compiler needs to take
care of that right by default that's
that's really not the case but and the
the comment that the bottom is is very
important here because it's just better
to check now than to D optimize as soon
as we execute that's the issue
so you're compiling a static class
initializer usually they are not
compiled at all because they're just
executed once so this one is doing a lot
of work so it's probably important to
compile it because you're starting up
your application and since it's doing a
lot of work you want to happen that
quickly but if we would compile it so
there would be optimized as soon as we
execute the natively compiled code we
wouldn't win anything would have spent a
lot of time compiling it then we execute
it and then we the optimize and fall
back to the interpreter again so we need
we need this little piece of logic in
growl as well which Tom did you see it's
it's one changed file with 30 new lines
of code so it was really just a small
piece of logic that was missing and this
one was very painful
because the details again not really
important what's important is I saw it
as only after a couple days of running a
service which was hard so uh you know I
was looking around and I think I was
playing around probably the command-line
arguments and things like that and
removing some to see if it made a change
and then I I saw that we are using this
agent called hipster hipster is an
open-source project on github and it
it's a heat profiling agent okay that's
important so he profiling always means
your instrumenting some code
instrumenting bytecode and in this
particular case you know since you're
you want to do a heat profile so you
have to annotate or instrument the new
bytecode and there's something called
I'm not really going to explain it here
but there's something called snippets in
growl it's very similar to intrinsics
what I explained earlier but snippets
are yeah they are they're written in
Java let's put it this way they're
they're not written in in handwritten
assembly but they're written in Java so
in this case it was this boxing snippets
double value off that used double dot
value off which has a new bytecode in it
and this new byte kid was instrumented
and that's what tripped growl off that
suddenly some of its own code get
changed because the way it's treating
snippets so you know tom and dr. we're
discussing this issue and then dark
finally said okay it's probably time to
bite the bullet which meant a pretty big
changing raw so it you know 65 files and
2700 new lines of code so that that was
a big change but it was very important
because in a you know an ending in a
benchmark setting or an experimental
test setting you know really using
agents but a lot of people out in you
know custom must do so we did so we have
to fix this and it's good that it's
fixed so from now on if you if you have
an age
and instruments especially new bike as
it should just just work and this one is
I was running like I think again the
tweet service and I was looking at the
log files and the large ones were just
growing at this at this rapid pace and I
thought okay there's something wrong so
I was seeing these connection it's a
exceptions like flying by and the
problem is if you have Micro Services
and you get any connection exception you
don't know whose fault it is right it
could be York your own service the one
you're running or it could be someone
else who's not accepting your connection
or something like this right sorry at
this point I really didn't know what to
do to figure out where this is coming
from I was pretty sure it was the
compiler or something went wrong but I
didn't know how to really access that
that issue so what's usually the best
way of handling a situation like this
you just drop it right you go and do
something else maybe later you'll get
back to it and and it will just come to
you and then I thought okay you know
we're using metaphor so let's just run
the native for test suite okay so I did
that and oh look the buffer test fail
okay
right let's see why that's happening
turns out there are methods in the core
library that are called reverse bytes
like there's an integer dot reverse
bytes and there's a short dot reverse
bytes right it's just swapping bytes and
that one was broken which is ridiculous
because it's used everywhere but in but
in this particular case it just failed
so Tom said okay I'll fix that easy fix
five files because I think he added a
test case and you know all this but
basically was a one-line fix so that was
closed and that was it these were all
the bugs we found and the last one let
me go back September 12 2016 so it since
more than a year
we are running this stuff now and we
haven't found another bug so for us the
bralette compiler is production-ready
contribution as I said we Twitter is
trying to give back to to open source
projects and I was I thought okay what
could I do what would make sense in in
the short term and one thing that that
we have now and when I when I wrote the
slides it was before I think Java 9 was
released but now it's it's already out
and chocolate 9 has compact strings
right and unfortunately the way it's
implemented in the compiler is because C
2 had an issue with optimizing the the
byte to char conversion and for them
back right because they tried the
engineers tried to write it with unsafe
which would have been my preferred way
because then it would just work with
every compiler but it they couldn't get
it work with C 2 so they was equally
fast as with handwritten intrinsics as I
explained earlier so unfortunately they
did that and these handwritten
intrinsics are highly optimized right
there using sseo AVX avx-512 whatever
your cpu has and so we need these
intrinsic synchro as well it's it's not
it's not rocket science basically you're
taking handwritten assembly code and
just poured it over from C++ to Java it
does exactly the same thing it's it's
tedious but we have to do it so that now
with 9 compact strings are as fast on
unrolled hasn't been on C 2 so I was
working on that a little bit and at the
top you see I did the string compared to
intrinsic first because it's the biggest
one so I started with that one and when
you look at the third row it's like the
string is pretty big of like 4k but you
see 2,600 nanoseconds and then at the
bottom with the intrinsic sits just 126
so for short strings first row doesn't
really make a difference right because
you're only comparing like one bite okay
what can you up to my stare but
I mean gets a little bigger you see even
second row you can already see an
improvement so I'm just showing you this
so that that this is just but the exact
same performance at the bottom then c2
has so the intrinsic just just works
it's just you know we have to do it
unfortunately so I'm still trying to
wrap that up doing these talks now so I
can't really but we're working on
something so now let's get to to the
real important part and and these are
the results that we are seeing the
numbers okay so I was talking about the
tweet service a bunch of times and I was
just picking it because it's a very
integral part of Twitter and it's easy
to run so that that's why I was using it
it's a finagle thrift service so finagle
is something that Twitter wrote at some
point in time it's a I copied the
description here it's like an extensible
RPC system for the TVM used to construct
high concurrency service I don't know
what it is the important part it's
basically entirely written in Scala
well it's 92% of Scala code in the
repository that's important most of our
services are written in Scala and this
is also important for the numbers that
we will we'll see you later
so the test setup I have I started out
to do benchmarking and testing in in our
production cluster but the problem was
it turned out that all the crosstalking
and this it kind of screwed with the
numbers a little bit so I got my own
dedicated cluster where with exactly the
same hardware same machines like a bunch
of machines same hardware and the way I
set it up is I run the experiments all
in parallel one configuration set up per
machine and all the instances receive
the exact same requests so that's
important because if you have a tweet
read request of a tweet that's one
character long or 140 or
today since yesterday 280 that makes a
big difference of how much you have to
process how much memory you have to
allocate on all this so it's set up it's
running at the same time same requests
it's dark traffic so it's read-only and
we were using JVM Zi 0.30 not that
important
the gravure j'en was 0.22 I think now we
are at 0.29 but it's it's pretty much
the same I think if I would rerun the
experiment I would see exactly the same
thing the default setup is tiered as I
explained earlier with c1 and Gras
so I say here growl truth bootstrapping
is not an issue for us the bootstrapping
in general growl is written in Java so
bootstrapping here means since it's
meta-circular if you start compiling a
method like an application method it's
hot and you want to compile it with Tier
four and then growl kicks in and wants
to compile it since it's Java code when
the compiler method it will compile
itself because its own methods will get
hot okay that's that's the bootstrapping
part in in the default tiered setup
there's a configuration switch that you
could change but the default drawl
itself will only be compiled by C 1 and
the reason for that is because C 1
compiles much faster and then it doesn't
take away so much time to compile your
application but for us for the micro
services you know we we do rolling
restarts of all of our stuff so it
doesn't really matter if it takes like
10 or 20 seconds long to start up a
service it really doesn't matter and if
it would be an issue for you for example
you could take a OT in chopper 9 and
just a OT compile growl we just we just
can't do it because we still run on 8
but again it's not an issue so it
doesn't really matter okay so the graphs
all of the graphs are 24 hours it's a
day a day of tweet
reading and using you will notice it's
not cut off there is no y-axis and
that's unfortunately on purpose because
I'm not allowed to tell you so I can't
tell you how many requests per second
one particular machine receives and all
this but I'll I'll put the numbers in
ratios later so that we can see what
we're actually saving this this light is
just to show you what configurations we
have we have in blue c2 than we have
orange grove then I threw in JDK 9 to
show you there's no real difference in
performance between 8 &amp;amp; 9 so if you run
on 8 it's a little tricky you can make
it work there's something you can
download if you run a nine it's much
easier to do and you'll get the same
okay so the tweet service is using
parallel GC so we're seeing here
scavenge cycles and it's a moving
average of 60 minutes so it's it
smoothed out otherwise it would be hard
to see but this is how many spec
scavenge cycles we do I said earlier
growl has better inlining and better
escape analysis so what we want to see
here is less cycles that would be good
and we do so the escape analysis can
scale ax replace more objects and so we
are not allocating as much memory and
then we don't have to collect this often
that's good we see C 2 inch JDK 9 it's
roughly the same and growl and JDK 9 is
exactly the same as an 8 and this is
between two and a half and 2.7 percent
less cheesy cycles it's not huge but
it's something right if you could save
two and a half percent of cycles that's
already something multiplied by
thousands and thousands of machines
would be good so let's look at this when
I talk to service Onis they they're
monitoring their services closely right
they look at all how much memory do to
use is there is there a memory leak how
much CPU do they use and so
a bunch of them came back to me and say
oh my god we use more old champ and I
thought okay yeah how much is it it's
it's roughly like forty megabytes and
where this is coming from is from the
fact that growl since it's written in
Java it has some state it's a java
application that's running in your TVM
so it has some state and it's this forty
megabytes that's the state but it's not
it's not in relation or you know if you
have a 100 megabyte old gen or a two
gigabyte dalton it will still be 40
Meg's and it will not grow over time
it's just it's just basically a hit you
have to take and you would you you're
taking the same hit with C - it's just
native memory and you don't see it but
in on the RSS you would see it okay
there's also oh yeah I should mention
this I don't have a good graph for it
but meta space owls it uses also a
little bit of AI also uses more meta
space between remember correctly between
like 10 and 30 megabytes something like
this it's just because again it's Java
code you have to load classes you're
loading in methods it it needs to be
somewhere so and it's it's it's
something but it's not a lot so CPU time
that that's the important one use the
CPU time we want to safe there right
that would be good so we do that's great
you don't see yet how much it is but
we'll see in a minute then c-to-t can I
it's likely faster but I think that's
only a measurement issue here sometimes
inlining decisions and profiling data
are different from run to run and so I
think that's the case what's happening
here and then crawl and JDK 9 same s
with 8 so as a ratio here now now we
have a y-axis okay you can see how much
that is and it is 11% so we just by
replacing C to withdraw running for this
particular service
we're saving 11% of CPU utilization and
that multiplied a thousand and thousands
of machines is a lot of money
so today the tweet service runs 100% on
Raw in production so if you tweet it in
the last I think we started running it
100 percent probably around tune if you
started if you tweet it in the last
couple months it was running all through
crowd compiled code and also the other
service that I mentioned earlier for
example the user servers in the social
graph service they are they're also
running 100% on growling production so
if you anything you do today
that's not quite true but the most
important things you do in your Twitter
your Twitter app or on the web is going
through crawl code and I think we didn't
lose any tweets did we did a crash no I
don't think it did so it works fine one
I think at least for me important
question is if it's the escape analysis
that that gives us all the CPU savings
we saw earlier we save a little bit of
of GC cycles but it was not like it 11
percent right so but I know that the
escape analysis is much better than what
C 2 has so let's see if that's where we
where we get the winnings and so if we
turn off as scape analysis for both
compilers and I have to do a separate
run for this so this day looks slightly
different than the one earlier from just
showing C 2 and growl here and again
scavenge cycles we just make sure we
really have turned it off and yes we did
so we're not saving any cycles here with
growl and the CPU time looks something
like this ok we're still saving that's
good that means that just a better
inlining implementation already safe so
CPU
and again as a ratio that's roughly like
half so even just the inlining already
helps you how much money is this well
the problem is I can tell you but I
thought we'll do a little thought
experiment here and we just make up a
company just to get an idea of how much
it is some people in this room might be
running their own data centers or or by
a cloud service or something you guys
know how much you spend for it
right so you can say okay if I could
save 10% of that that would X amount of
dollars that would be great but the you
know for other people's let's just do
this so I was I was googling some cloud
service providers and I found all these
and they list their numbers in in
different ways so I calculated it out
it's like between 72 dollars a year per
CPU core and 210 so what can we learn
from this choose wisely
because you can save a lot of money but
just choosing the right one
so I averaged it out because again it's
just random numbers we're making stuff
up here so let's say it's 127 dollars
per year per CPU core so how many cores
are we talking there's there's an
article from 2014 so it's already three
years old that Amazon has probably two
million service which is ridiculous I
mean the the one company says 1.5 and
the other one Gartner says it's 2
million and then they say 2 or 3 regions
8 total sets Microsoft has 17 regions
you know so millions of service not
often I'm probably running Java but a
but a bunch of them so Twitter is not as
big as Google or Amazon but with our
size is still pretty big your company is
likely smaller than Google's so we just
pick a random number let's say you have
10,000 service whatever
and each CPU has usually 24 cores let's
say 1 $27 127 dollars per year per CPU
core times 24 times 10,000 is 3 million
bucks
you would have to bathe 3 million
dollars to run your data center
basically and if you could save 11
percent of this I would be three hundred
thousand dollars I would be great that's
maybe I don't know one or two more good
engineers I don't know how much money
you get paid but so can we get a little
bit more and that's how you know there's
not a lot you can do with escape
analysis but we all know that inlining
is the mother of the optimizations so if
you can just make inline slightly better
maybe we get a little more out of it and
if you can inline more escape analysis
also works better so I took an afternoon
to just play around with some inlining
parameters with Rawls policy like three
switches that you can just play around
with and so I did that a little bit and
I was running this actually at the same
time as the first experiment so that the
graph looks familiar
oops I didn't want that here we go so
it's I call it experiment again
dedicated machinery receives the same
amount of requests and you've seen that
before
oh that's good so we're saving a little
bit of scanning cycles here how much
like one and a half percent more that's
really good with just you know tuning an
afternoon CPU time let's see yeah that's
great that's exactly what do we want so
I'll race you again so just by playing
around with it I got two more percent
out of it yet this shouldn't be done
manually obviously so but the the new
buzzwords on machine learning and you
know what so this
should be done by computers right I just
wanted to see if the Inland policy
that's in Gras if if it's suitable for
doing that if it could get more out of
it if we if we would invest something
and and in fact for this particular
service yes and it's what two percent
more how much was that like sixty
thousand dollars that's great and I take
that I don't get it but I'll take it for
you
I'm assuming you don't have your own VM
team so something like support is
probably important for you growl at this
point is inch ATK nine and it's only in
JDK nine because the äôt feature in
JDK nine is using raw to compile native
code and this is also I have to I have
to put this on the slide because that's
only the case for Linux x64 so if you
download it on a Mac it you can't do
this but on Linux you can it will be
different with the next gvm release
because then they have support for mac
and Windows as well I think so yeah I
can't remember now but I think the next
release will you can do it on all the
platforms down but one thing the äôt
feature in jdk 9 is an experimental
feature actually that's how Oracle
promotes it what it doesn't tell you is
that you can actually use Gras in there
as a JIT compiler as well you just have
to know how and this is how so you turn
it on you say okay I want to
experimental VM options and I enable
that JVM CI and I want to use the JVM CI
compiler that's all you have to do and
you run and grow then earlier this year
like end of August there was an
innocent-looking review flying by on the
mailing list and it says something like
build growl regardless a ot built and I
thought okay yeah that's interesting why
did doing this and if I assume in a
on the text it says we are planning to
have growl as experimental JIT compiler
on Linux in next release so when I when
this mail was sent we weren't on the six
months yet so it was still what he was
talking about was JDK 10 it's a little
hard now to know what that means in the
new system it could be the release in
March it could be the one in September
but what I'm trying to say here is at
some point and probably pretty soon
Oracle will say okay you can use this
expert as experimental check compiler
they will announce it like this still
not the support you would like to have I
guess but it's better than nothing right
g1 was experimental for the longest time
and people were just using it for for
the people who we're actually seeing
better numbers so this is pretty good
and then on top of it I don't know if
people know about Project Metropolis but
so John Rose proposed that project in
late September czar not too long ago and
the important part here is it's down
here for using the Gras compiler and
äôt static compilation technology to
replace the hot spot server compiler and
possibly other components of hotspot so
what this project is about is to replace
parts of hotspot with Java written code
let's say what one project or one
feature we were talking about for the
longest time is it's the bytecode
verifier the byte could verify you it
would be so much easier to implement in
Java
than in C++ and so you would write it in
Java you would a ot compiler you would
link it into the VM and then yeah there
you have it but the important thing here
is the hotspot server compiler that's
exactly what we want to replace we want
to replace it too and that's what
project metropolis is about and this is
I think the voting is already over and I
think it got accepted so that's that's
an existing project in open JDK these
are all signs that Oracle's
tending towards you know it's it's
always difficult to say what Oracle's
doing but it looks very much that
they're jumping on the band
and I want to get rid of c2 and and use
growl in the future so I have to say
this basically I mean what we've seen
here especially the eleven percent of
improvement and the amount of dollars
we're saving it's great but the main
reason we are seeing this is because we
we do Scala so Scala and I've talked to
or collapse engineers about this a
little bit because the they they were
interested in this in general and they
have more time to do this stuff than
it's just the one person at Twitter
right Scala has it has a much more
polymorphic nature than Java does and
the way grah does inlining in particular
it's much more suited for that that's
why we are seeing such big improvements
for chava it's a very very different
story so when you when you run something
with grog when you run java code with
gras it could be like a 50-50 chance if
it's faster or not as I said earlier c2
is a really good compiler it's been
tuned for 10-15 years and it's been
tuned so that it works with a wide
variety of different applications so
it's very hard to come up with a new
compiler that's just better and
especially better in every case
still there are I've seen Java code that
runs pattern crawl I've seen Java code
that runs worse so I have to be honest
here it's just you have to try it you
know and and the things we can run is at
Twitter as I said it's basically all
Scala code we have a handful of Java
services and we tried these and we
didn't see an improvement we saw the
same performance which is already in my
book a win because if we want to get
away from c2 and we can replace with
something on the source code level
better
and we have the same performance that's
a win so we could just move over and
everything would be fine no one would
say oh my god it's running twenty
percent slower doubt that that would be
bad but that's not the case so for you
you you have to try it if it's lower
it's worth reporting if you see a crash
definitely report a bug so this whole
thing about the difference between Scala
and Java in this case and how to run
stuff I think that topic really needs
its own talk and I'm actually thinking
about doing this the next time
so we'll see about this and with that
I'll leave you with this and tweet a
little thank you any questions yes oh
yes there is because basically I haven't
mentioned this but the last year and a
half I wasn't doing any software
engineering I was doing social
engineering because I have to convince
service owners that's a good idea to use
a compiler that no one else is using in
production that's the reason it's it's
hard to convince people right you find a
few people there they're willing to do
it they like shiny new things and oh
yeah let's try this and and the good
thing is we're running the most
important services with it going beyond
these and these are also instance wise
the biggest ones there you know all the
other ones are much smaller so it's it's
a lot of overhead for me to go there and
explain it and then have them run on the
GNU Compiler the next step would be to
do enough testing and then just flip the
switch and run everything on well more
questions yes
so we we use the open source version
0.22 I had it early in the slides so you
can see something if you look closely on
application start yes obviously because
you're compiling with growling growls
using Java heap memory they so for us
it's it's it was never an issue because
our our heap sizes are pretty big and
what happens is that your application
during the first I don't know 1 minute 2
minutes where most of the compilations
happen
it hasn't ramped up fully yet so it's
not using all the memory and especially
in our case with the micro services that
that accept requests before it they
aren't fully up they don't accept
requests right so it in in these first
couple minutes it doesn't matter how
much we memory we use for compilation
well first of all I don't have it I'm
not associated with Oracle app so I'm
just using this stuff the way it's
implemented in situ is a is a very
static heuristic let's put it this way
like you have all these limits and you
know you have this bytecode size limits
and then assert byte codes are not even
taken into account or you know the other
way around actually so you have all
these issues in situ you know about it
and growl it's slightly different so
what they do is they there is they
calculate scores for methods I can't
explain to you right now how these are
done but there are some scores and then
they you they don't look at by code
sizes but they look at graph sizes so
you know when you when you when you
project a method and and thus the graph
is like this big you can kind of see how
big the resulting code will be in that
that's how it's done
I would say so yeah yes
yes there's a there's a flag that says
in line everything it literally in lines
everything but it's it does not use way
what it's called what what they would do
I think we also did that in JDK nine the
GNU Compiler control compiler control it
growl does not use compiler control
because it doesn't hook into that
machinery so I don't think you know
there's I think the still the old style
of doing if you do compile command
something something in line that might
work you need that ok yes yes it can
so it like c2 does that too right it can
it can in line through in work dynamics
and through the whole lambda form and
then in line basically your eventual
colleague into the caller so that all
the stuff in between gets folded away
that steps would see to does and grow
the state as well yes almost yes almost
as I said earlier I didn't do further
experiments in production because of all
the crosstalking and we we utilize our
machines as much as we can so in
production I think on average we're
seeing eight percent you know with all
what's happening on the machine but it's
still you know it's a lot I was a
question up there stuff yeah
it yes lightly it does so we are not the
improvement is in cpu utilization
because the service is still receiving
the same number of requests it's not
getting more so the the front and that
basically sends up the request doesn't
know that it's using less CPU so it's
just using less and then what's usually
happening is since you know when when
especially when escape analysis kicks in
and and you in line a little bit more if
you don't have to allocate an object you
don't have to execute that code and then
if you if your scale arise it then your
values live in registers so just the
access is quicker and all this so the
code is it's likely it's more tight it
usually runs a slightly faster and that
comes out with that the the P latency
times are usually a little bit better
times up but maybe another question no
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>