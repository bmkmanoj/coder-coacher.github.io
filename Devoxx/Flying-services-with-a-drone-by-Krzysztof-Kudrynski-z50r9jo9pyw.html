<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Flying services with a drone by Krzysztof Kudrynski | Coder Coacher - Coaching Coders</title><meta content="Flying services with a drone by Krzysztof Kudrynski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Flying services with a drone by Krzysztof Kudrynski</b></h2><h5 class="post__date">2018-04-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/z50r9jo9pyw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello first of all thanks a lot for
choosing our presentation out of all the
other options that we had today we
really appreciate that
this is washing we are my name is
Christophe kudrinsky and in our today's
talk entitled flying services with a
drone we will be talking about surprise
surprise flying services with a drone a
few words about us we came here from
wood from Poland where we work on an
amazing project in TomTom which is
self-driving cars we are building
three-dimensional very precise map using
a lot of sensors this is mostly our
expertise here in slider point clouds
and this is why probably we are choosing
this kind of topics because this is what
we can do so generally in this project
we want self-driving car to drive safely
that's why we built a map out of all
these sensors then we create some
localization and genes for that and
using different technologies including
artificial intelligence probabilistic
robotics we are trying to make these
self-driving cars know exactly where
they are on the map by different kind of
localization techniques so this is
generally what we do and out on our
daily basis we try to make the car drive
safely and securely and comfortably on
the road by itself but the problem with
the cars is that they don't find and we
think that flying is awesome
everybody wants to get high
so today we will talk about flying and
we will mention our two projects
concerning the drone so here is the
short agenda for our presentation first
you will talk about project number one
and then we will go to project number
two well it all started our journey with
his drone started with TomTom
competition where we were supposed to
follow the running person using a drone
we got a drone sponsored and that's how
we started to do play with that and the
nice stuff was that the finals of the
competition took place anounced in
Amsterdam city so famous from the fact
that you can absolutely legally visit
windmills
announced the vision of flying was
growing really strong in us we were
expecting this to be a journey into
conquering in the skies into going
evening among reaching the stars and we
were almost right it was a story about
fighters the story about constant
irritation about struggling about
falling and rising from your knees a
story about determination and separation
we are professionals and as
professionals you know you need to have
a certain plan so you start buying the
draw and open the box you learn by
manually you gather some characteristics
and then you start or incremental
development testing developing testing
developing you all know that but at the
same time we are enthusiasts and as
enthusiasts we started with a final
design
having that we bought all the
electronics that came into her head and
then we bought the drone we quickly
rushed with the development and it was
going really great after celebrating the
first successful version of our code we
finally decided to open the box the
drone was inside so we added all the
electronics and we gloriously went for
our first test in the field which turned
out to be the last test our first drone
with the next drone we decided to learn
to fly manually and then we had to say
sorry into the professional approach
because it has his reasons so generally
in this project our system was based on
parrot ar.drone - and you can
communicate with it over Wi-Fi and just
plain of open source which you can
download one of them is Java ya drone
framework but you can just control the
drone from your software and there is
quite a few interesting code code lines
which you can use one of one of them is
the comment manager for example in the
interface of the drone where you can use
the Move command and apply parameters to
it to tilt to the left right front and
change altitude or rotate this is
probably the most important comment that
we use but also there is this navigation
data manager where you can add different
listeners so that you can listen to the
state of the drone like what's the wind
pressure for example there are special
sensors was the hide from the absence or
the rotations accelerations from the
accelerometer and now your task would be
to build like system based on probably
state design pattern where you listen to
your state based on the navigation
managers or the listeners you get your
state and based on this state you decide
what control to apply to the drone so in
our case the task was to follow the
runner and film it so the system
consisted consisted of two things the
drone and the runner which from the
perspective of the system was just a
mobile phone in his pocket so what we
had was GPS obviously involve this type
of equipments of the telephone and drone
had the GPS so we roughly knew where we
are but then we also use some Bluetooth
I began to know a bit better the
distance and then finally we were using
computer vision so analyzing of the
image to detect the person and improve
the angle at which we are looking at
that person so finally we have this
small almost rectangle in the middle
where we think we should look for our
running guide so having these positions
we were able to apply a turn rotation
and acceleration so that we find
ourselves in a position much better for
filming the runner so the system was in
fact a mobile phone application
listening to the drone state over Wi-Fi
and based on this state returning some
commands to improve the state even more
for better filming and processing you
can look at the more or less at the
application but it well it is quite
complicated to see for for the short
amount of time what is quite interesting
we have also the Safety Officer
application which you can use as a
observer for what is going on and you
can manually kidnap the drone in case it
wants to kill the runner for example
we are enthusiasts of graphical user
interface we know that the graphical
user interface should be both beautiful
and very functional we didn't have time
for both so we chose just beautiful what
you can see are nice buttons for manual
flying that you have a button for
autonomous flight mode which you when
you run the drone will actually start to
manually follow the runner and for those
who are maybe a little bit disappointed
by the graphical user interface we
fortunately have our safety officer
application which looks much better
beautiful or not our system worked
our initial enthusiasm costed us life of
three rounds and countless losses in
equipment part
fortunately with our injured drone we
succeeded in following the runner for
its entire track in Amsterdam which was
a reasonable achievement but instead of
reasonable achievement together with
Pasha we prefer extraordinary
achievements so we decided to continue
with a project number two that's how it
was born so from the junk of the three
injured drones we created a drone of the
last chance the new
these ambitions were to create a map
based on what the drone system its
bottom camera localize ourselves in this
map and then use this map to perform
some more interesting operations from
the high level like please strong bring
me the glass of water
thank you that's what we wanted to
achieve and we tackled all these
problems with various succeed rate but
we will try to show you our algorithmic
details and what we're using and showing
you some videos from how it went but we
will focus mostly on the show we will
say and some of these and in order to do
that our first step was to make a simple
java application which you can see on
the right so what you are doing you
control
you continue browsing see the map below
using some transparency and blending so
if you froze the photos you can then
ship the photo rotated and so on so that
it please the map that's how which you
can see here which is not very nice in
the beginning so we had to play a little
bit with blending between the frames
which after some time got much better
and better as you can see so how we were
tackling that so this is a diagram of a
drone flying over some high objects and
the dots show the time the places were
the frame was taken by the drone and
actually a single frame captures very
limited angle view of what it sees below
and it is not aware of the object's
height below it so in fact what it
captures is some perspective of this and
this perspective is not in
synchronization with other perspectives
in fact the only place that is fitting
our map will be sorry the place exactly
below the drone and this is usually the
center pixel of our image if you are
flying quite smoothly so what we did we
created a mask which is one in the
center and going smoothly to the edges
and now having this masks for every for
every frame we can add two frames by
simply weighted averaging of the frames
and their masks and when we add this
frame we have a blended map which looks
very nice and then we can add another
frame because we maintain the
accumulation of the masks behind so this
is a nice mechanism to make it possible
to black
new frames one over another and get the
smooth map so this is what we were able
to produce nice and smooth top boom up
manually which took obviously a lot of
time so what we wanted to do we wanted
to automate that obviously and we wanted
to start with something simple and we
started with some brute force algorithm
where instead like in this job
application the user was clicking trying
to find the best fit visually we will do
it automatically so in some having some
frame already in the map we will in the
reasonable neighborhood of this map we
will check all the possible shifts and
all the possible rotations and choose
the one that has the most correlation
with the map below if we perform some
calculations so assuming the drone can
fly three meters per second it can turn
90 degrees per second and it can change
altitude to one meter
let's say for a second and we would like
precision of several pixels we can
quickly calculate that in order to do
this we would need millions of operation
per second so that would make thousands
of operations for real-time that will be
just too many so we decided that we need
for sure of course you can come down
with the number of correlations with
some nice tricks but if you want to make
it work in real time you need to really
choose wisely where you are going to
correlate and then you need to be fast
while correlation so how to be wise I
will tell you later
and now watch I will tell you how to be
fast
okay I'm going to be so pissed off set
our goal was to find very fast and very
like really method to localize frame
captured by drone camera in the mouth so
when we take the picture and we put put
it on over the map and we consider when
possible locations we would like to
assign confidence to each location and
do it very fast like 2,000 locations per
second and would like to have very
smooth confidence which means that when
we go out from the correct solution we
should decrease smoothly and when we go
back to the place where frames theory
fits perfectly in the map it should be
very high so how we started we started
very very simple solution based on
template which is basically some ink
corresponding pixels in the frame and
something products of piece of I was
right
however simple in this unfortunately it
was not fast at all so we perform one
hundred locations per second it wasn't
enough for our entire solution so in it
another try but you wanted be smart this
time we know that we reduce the number
of acceptable process and we would like
to review some information that we
already process and we find my solution
which is it is distance how does it work
responsible so we detect edges detector
is fragment check how close by army so
as you see in the picture blue arrows
shows pixels so we need to just iterate
of iterate over edges in the frame and
some stances for the closest edges
so what is important these maps are
great in with our time and far are
created only once and reused for our
possible location ok so let's see where
we are she performed like 3000
transformations per second it was very
nice for for our solution and obviously
much faster than the plain magic
now a few words about the video because
there was not nice surprised that the a
drone framework that we use for
controlling the drone didn't have
support for the video stream in high
quality so we had to add it
so we downloaded a fabric library we
compile it for Android for Windows we
found some pre-built binaries we learn
how to connect to the stream and and
render of just three and four for Java
we expose it through G and I so you
could use that in our application there
is one more thing important in our
project there is one thing that could
spoil our recreation of the map and
these are distortions so or lenses
suffers with some distortions these are
errors of lenses and there are some
equations that could be used to remove
these sources from the picture but to
use these equations we need to find some
parameters and these parameters finding
this parameter is called commoner
calculation when you calculate chimera
we also get focal length of the camera
which can be used to calculate his
material solution of the map so when we
know the focal and we know the filter of
view of the camera and for particular
altitude of the drone we know what is
the size of the mix-up so how we
calibrate the camera we take a few
pictures of condition chessboard we find
corners
fortunately there are nice a nice
functions in OpenCV to do that to find
corners and also hydration itself and
when we try to make straight things
strike
so straightness of edges and the picture
is the objective function for
calculation and after copulation Omar
our map looked much better and after
blending multiple frames it were fitting
very well
now Christopher will tell you how to
device
hello again
so when we have limited the time of
collection we can now limit the number
of correlations and how can we do this
well we don't need to find like all of
the possible shifts that we can imagine
but we can use the fact that in the draw
will have some sensors like
accelerometer or gyroscope so we know
more or less where we fly we have some
possibilities where we think the drone
fly and only there for these assumptions
calculate very quickly the correlation
as you are telling that would improve
because we will not need to calculate it
everywhere else however simple it might
sound there's a huge theory behind that
it's called probabilistic robotics and
it needs about 600 pages for a brief
introduction but I will not sign it to
you today instead I will tell you a
story this will be a story about our
beautiful piece of machinery which we
used it's called particle filtering but
it will also be a story which would
could happen to any of you to you you
and myself and because of the reasons
that would later become more clear we
will call it hangover so imagine this
day when you wake up and you don't know
what happened yesterday and you don't
know where you are and you are yet too
afraid to open your eyes so in your
imagination you are trying to spot all
the places that you might probably be
and your first belief is not very
satisfactory for you so you decide to
finally open your eyes and from what you
see you feel it must have been nice
yesterday but now you can update your
belief about your state because the
spots on the drifting on the ocean have
now almost zero probability and the most
probable are those who are on the coast
on the rivers on the near the lakes so
only these will remain and then you
decide well and take a walk and you walk
for maybe five or ten minutes and what
you still see is the beach so you can
update your observation your your belief
about your state by saying that you're
no longer in the rivers or lakes so
these are only the places near the coast
that will remain with high probability
well decide to walk even more and you
see guys with guns so you are sure that
you must be somewhere near the border
and only these places will remain the
guys have guns so you decide to turn
away and now you will be working with
the coast on your right-hand side and
finally you will see a tourist and
looking at him carefully you will find
out and you will know for sure that you
are in Poland
so what was actually going on here we
wanted to have our our belief of what we
are we wanted to have our position
orientation and velocity and what we had
was some previous belief about where we
are in form of the distribution of
particles and now what we were doing for
each particle in this previous belief we
were applying some control the corner
was for example our how we felt that we
are moving maybe we were counting our
steps
maybe we felt how we rotate and this we
applied this to every of this particle
and then we had this a priori belief and
for each particle of this moved cloud
with the measurement with the control we
were applying our measurement so for
each of these particles we were
assessing probability particles remain
in our actual belief and we continue
this approach recursively well there is
we could talk a lot about about particle
filtering the nice things is for example
in comparison to other bias filtering
methods you have arbitrary distribution
possible particles with for example
Kalman filtering you need to say it's a
Gaussian and then you are now not
possible to assess many places at the
same time of particle filtering you can
do it all so scalability is very nice
scale if you have a mobile phone you
want to compute your position you will
use like 100 particles it will take some
time but it will finally converge if you
have a I don't know a server with a lot
of processing power you can use
thousands of particles and then it will
converge fast to your solution and
finally there is a recovery mechanism if
for some reason you got lost with the
Kalman filtering you will never find
yourself back into
the right place but here you can add to
your probability like random samples all
around the place and finally some of
them will fit with high probability and
it will get the answer in this place so
this is a very nice mechanism
implementing that is really not not a
challenge what is a challenge is to
design the state in a way that it's it's
really working and even more the noise
model definition I will tell a little
bit about that so our state for our
drone was actually the position on the
map the velocity of the drone its
rotation and in fact the height which is
the zoom of our frame versus the map and
our control was acceleration already
from accelerometers change in rotation
from the last belief and change in
height from the last belief and then
obviously the measurement was the frame
itself and how it fit the map so now you
can you can look at this and say well
that's that's quite easy we can
implement that we can implement state
control measurement just take the ready
particle filtering and go on with our
algorithm unfortunately it's not so easy
I will show it on I will show you it on
the example of acceleration so even
though you can think you can take
acceleration directly from the
accelerometer this is what you see the
drone is actually controlled by angle so
if you apply a tilt backward the drone
will gain acceleration forward and it
will start to move in the forward
direction but if you look at if you look
actually on the acceleration that is in
the sensors and compare it to the
acceleration that we really have for
example we measure it with a ruler with
photos so we have some current
raishin which we know for sure happened
and then on the in the sensors we see
something that was totally nonsense and
it obviously has no sense because what
we would like to have is acceleration in
the X Y direction but what we have in
the sensor is actually the acceleration
in the drone reference system but then
you once said okay we have this angle
also in the sensors we have pitch
control in the sensors so we could just
compute some trigonometry and get the
real X X Y acceleration
unfortunately the gravity is there and
it is huge and even a small error in
pitch and roll would leak this
gravitation into our x and y direction
and that would be a trouble and it turns
out that the pitch and roll sensor in
this drone is very cheap and it suffers
from significant error so what we had to
do we had to calibrate this sensor so we
were flying in one place for about say
one minute and during this one minute we
were sure we were not changing our place
which means our pitch and roll should be
mean zero it was not it was floating it
was some function of time so we could
deduce this function for for one minute
fly in one place and then calculate this
drift that's happening and apply the
correction to our buffer and have the
correct correction function for flying
in the future and then we could apply
this to calculate the acceleration which
is now leveled acceleration the problem
is that the acceleration sensor in this
drone is very cheap and it suffers from
significant error so what we had to do
we had to have this accelerate
in the buffer and then calculate the
drift of the acceleration itself and
then we had a corrected level
acceleration which you can use in your
future flight which now looks much
better and it has some sense but as you
can see there is still some differences
it's not very big differences between
the grunt rows but still it is and if
you look statistically it is quite
significant it's up to half a meter per
spore second error which is quite still
quite high and what is even worse there
is a lag in time between what the frames
is and the acceleration sensor because
of the Wi-Fi and because of all other
stuff and lags are killers for particle
filtering it is better to say we have no
acceleration at all than to say we have
acceleration and underestimate its error
so if you sometimes face this problem
that you are under estimating something
there is one nice thing you can do you
can exaggerate it and that's what we did
and it worked
so there is quite a lot of other
discussion that would need to be taken
for how we dealt with orientation how we
dealt for getting the zoom out of the
ultrasound sensor and the noise model
associated with that and how we dealt
with no installation when integrated
from acceleration through the velocity
to position but there is you can read
the book for that there is also little
time for for this our system
architecture the thing that that maybe
will be very disappointing for some of
you some some of you might might
actually find it hard to believe it even
possible but we did not use
microservices
we use a single monolithic application
in Java with some threads inside and it
was working what's quite interesting
here is that behind the drone interfaces
we had real drone implementation which
we use to really fly get some logs
orient map the staff but at the same
time we had this we wrote this
simulation implementation which took the
logs that we previously recorded in
field and we could apply it many times
to debug to check the problems improve
our solution if you ever dealt with
writing application in real time and
trying to debug it and on the hardware
and try to debug it and then the flying
hardware trying to debug it you know
it's really hard and thanks to this
solution we made it possible and
reproducible and quite fast so I really
encourage such solutions so well now you
know how it more or less works you know
what we are trying to do what is our
motivation for this and now let's let's
see the demonstration how it really
worked like we are not going to fly in
here some some of you already asked me
if you are going to bring the drone and
fly and for the reasons that I showed
you before it's not a very safe and it's
not allowed in fact so we would really
like to fly and kill some of us sorry
not so in the video you can directly see
the interface of our desktop tool in the
photograph you can see
because we are and you can see while it
flies again on the same places if you
look carefully you see them updated
using our blending algorithm so every
time we visit another place we are
actually updating it to have it because
it has a platter of personally done
before this is done using this weighted
averaging in a second in the top right
you will see we changed the camera now
we see there how much the drone sees we
see our friends
working there and we got spotted behind
bar behind the wall these girls were the
it was 10 o'clock p.m. they were here
illegally in the office we had a
permission together with Bosch I for a
romantic night with the drone
so we continue our flight now will be
very interesting you will be flying over
the test and we will see clearly the
update a better perspective on top view
so we are creating this map in the
places where we are before scene from
the worst perspective it will get
updated so it is not very smooth and
beautiful but this is something we will
end for a feasibility study - in our own
project to tweet such map you can see
see see samplers and problems but it's
generally what we wanted to achieve but
well is it the end obviously not because
now when we have them up and when we
know where we are on it what we can do
we can use the map to give to the drone
our own high level orders so because we
know where we are and when we when we
click for example on our target the map
knows where we are and where the target
is so it could know where it should fly
so it would know what control to apply
to makes us closer to our target the map
will know this in fact it does not know
exactly where we are it knows the
particles where we are but we could
apply this every particle and calculate
the resultant and this way we would be
able to go towards the target
automatically we check that with a nice
mission what you can see here we will be
trying to localize ourselves a nice
thing to to notice will not know where
we are so we will just wake up the drone
and fly in one place and the particle
filtering will be looking for all the
possible places on the map and finally
it will start to converge to the place
where it is then it is programmed to go
to target one pick up the dark cargo
then go to Target - and land with the
cargo so this is the mission
okay you will see much more loud than
before in the camera now because this is
plated from a different camera which led
some program to provide life but still
you will see after calculation a lot of
particles and you see how they slowly
converts to some places which are more
and more beautiful numbers also but
finally when it converges the map will
take control over the drones and the
draw will start to light towards the
target number one not very smoothly but
surely and now the pickup mission is
manual so we didn't implement automatic
pick apart what you can see our
top-secret technology for the pickup
this is Magnus hanging on a stick tight
and where you will see how the magnet
goes into the penguin
fly and sometimes obstruct their title
sensor and the draw always will
miscalculate where it is how you will
see how we deal with the problem is very
interesting but now let's focus on the
recognition which needs a lot of
attention
well that's the key
particle flying towards target number
two we are landing miscalculation again
conclusions we broke the glass
we broke the drone
we displace the ceiling but we managed
to localize ourselves we went to target
number one we picked up the cargo we
went to target oh and maybe not too
smoothly and not in the right place but
we deliver the cargo to the ground so
well however easy or however hard it
seems to you that we were already very
satisfied with what we achieved it was
all a feasibility was working quite well
so we were very happy with that
and this is probably something that we
expected when coming to this
presentation from the title but now I
will show you something that you did not
expect I will show you our toilet idea
if you don't know what toilet idea is
toilet idea is when you go to the toilet
and you have an idea our toilet idea was
about blending so as I told you in the
beginning we were using this top view
perspective as Center mask for each of
the frames so that it blends smoothly
from the top view now this is how it
more or less looks like but what would
happen if we delivered liberally apply a
different mask to every part to every
frame we would see for each frame in
fact it will be focused on a different
perspective than the top perspective and
we could apply it for all of the frames
this different perspective and create a
layer of the map scene from a different
perspective and then calculated for a
lot of other perspectives we would have
this stored somewhere and now we could
play with these layers
to recreate a different perspective and
when we are actually when we are wanting
the top two perspective you will be
infinitely far away and we will use just
in fact this is the simplest case what
we already did in fact we'll just use
the center perspective for that to see
the top view map but now if we move
towards the ground so if we zoom in we
will start to play with other
perspectives so we will try to create a
virtual view of as if the drone was here
but in fact it was never here how it we
will calculate that because we have the
perspective so some of the image will be
created from part of the image that
scene this place
parts of the image will be created from
a different perspective that was in this
place and will be able to recreate using
it to some image blending the virtual
image as if the drone was here while it
was never here so we tried to make this
visibility we in fact wrote all the all
of the components the problem was
without with precise localization and
pitch and roll collection it was very
very sensitive to these errors and we we
really found it hard to correct it with
our particle filtering it was not enough
precise and it was it would take us a
lot of time to implement that this way
but we at least wanted to prove its
feasibility of the solution so we moved
our toilet idea to the kitchen and in
the kitchen we prepared our feasibility
prototype set up where we will be
creating photos of our scene by manual
camera that we will be making photos
with very precise level and very precise
height and we will know how it is so we
can be we will be using that this
prototype
scenario laboratory conditions were very
carefully prepared it consists of a
ribbon attached at one side to the
kettle and on the other side to the
microwave oven supported my milk and
using this setup we were able to create
11 perspectives in fact the center
perspective and two extreme perspectives
on the left and right you can see on the
photos and now we will be playing with
that to create recreate what we want
during our zooming and browsing so what
you can see here is that with a standard
map while swimming in you will never
know that below a chair there is
spider-man magazine which you can
clearly see what is zooming in with our
3d browser giving you locations about
the drone has never even dreamed about
so zooming in and browsing is now
available in 3d and maybe it's not
perfectly smooth but we are quite happy
about it because while there is a lot of
complicated structure from motion
techniques segmentation and so on in the
literature we use no no none of these
processing techniques at all we just use
simple image blending and to our
knowledge no one has ever done this
before us so that's why we are quite
happy which is more or less the
conclusion of what we did so we were
talking a little bit about out and
almost follow up then the mapping using
the drone localization and 3d mapping
all available in 3d for selected
locations
so well this was a hard time it was very
tiring for us our project in free time
after work and it was really tiring for
us but considering these moments when we
felt unit creation
thank you
we are here til the end of the day you
can ask us questions later so you can
ask some questions now if we have time I
think we may be one question go on okay
the question is what would happen if
there will be a moving object somewhere
we will have it blurry unfortunately
this is something we this is a happy
case again we have a lot of projects
like this we try to make them work in
some simple scenario if it was our
professional project we would of course
deal with all of that but this is just
our playing for fun and we assume
simplistic scenarios for that so static
scene but saying that I think due to
redundancy of the scene it might be
still possible that we will get a good
result because the the real problem
would be if actually the moving object
will be centrally below us and moving
with us that will be the worst scenario
because this is where our main focus
will be but if it's somewhere else
probably with the other focus we will
get rid of that in the in our blend
Calgary okay last question
sorry can you repeat again okay the
question is could we use the Google map
API to get some map of locations we
specifically target this for interior
mapping because we want to focus on
using accelerometers and because
otherwise we could use just GPS fly very
high it would be much easier but and
this is probably easily solvable and
there is already plenty of solutions
what we wanted to do something more
interesting or more difficult so flying
inside were there are different kind of
problems and no GPS and no external map
okay last question
in fact we had a yesterday we had
another talk about object avoidance in a
different technology but we didn't use
it here because this was the first
project didn't the talk in yesterday was
the second project but generally here we
didn't use it but we have it implemented
somewhere else okay if you have more
questions we are very happy to talk to
any of you outside you can tweet us or
email us if you want to hear more but
obviously it would be best if you just
talk to us today which will be very
happy about so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>