<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Massive Parallelism with GPUs in Java by Adam Roberts | Coder Coacher - Coaching Coders</title><meta content="Massive Parallelism with GPUs in Java by Adam Roberts - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Massive Parallelism with GPUs in Java by Adam Roberts</b></h2><h5 class="post__date">2017-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CIjdipU66qw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to talk about today is how to
use GPS from Java and I work at IBM
hoesley in the UK and I've been given a
pretty fun task for the past few years
is to try interesting new projects in
IBM see if they'll work or not see if
they'll fail and go from there well the
test that was given was to explore using
GPUs for the first time well talk about
these sorts of things today I'm going to
share all the bad news as well and you
may even see a live demo and what a
house gets started as well completely
from scratch
so there are few disclaimers to mention
that's the obligatory bigger disclaimer
that I'm not endorsed by anybody and
information is accurate as of murdered
this talk so less than a week ago again
some more this game is for you I'll make
sure to have a lot of companies in this
stock and I'm not affiliated with any of
those and there's a final one which is
just for fun I accept no liability for
the kind when I share today
this was all done on my development
laptop over here and went through all
the mistakes in the process graphics
adapters something changing the BIOS and
crashing plenty it's maybe we get to see
some of that as well experiments with
lots of threads were done again that was
all to my laptop crashing plenty of
times and messing around of the graphics
driver on your only development machine
at work is not a very good idea usually
well won't cover is an in-depth look at
your alternatives so there are quite a
lot of them and I'm going to talk about
them in much detail those are worthy of
their own separate talks please like
OpenCL things like tends to flow deep
wedding 4j etc also there's a debugging
and profiling I won't go through too
much detail there I'm going to help you
get started from nothing so using GPUs
to get a speed-up and really impressive
programs so I get these weekly emails
minute video and always you know using
GPUs to use really cool things you know
it's a come
and asteroids and helping find you know
cancer I sort of think those are for you
to go off and research for yourself I'm
going to say how to start using those
sorts of applications from scratch and
the Java basic is going to assume that
you know all about Java already so where
is a mobile Java used again a blue slide
here but it's very very popular
so what IBM we have we have our own j9
VM which is used in a variety of places
so where the mainframe at the bottom
right so there's e13 for example
although analytic suffering so Apache
spark which I work on as part of my day
job for the past two and a half years
things like Hadoop begin sites WebSphere
things that make a lot of money for IBM
they're all relying on a good Java
implementation so if we can bring GPUs
to Java we can get a knock-on effect as
performance improves and then simply
experiment here let's say you casting
Java you know it's the GPU at all
simple experiment gets to use parallel
streams and see how many operations we
can do it once before it falls over
so there's stuck over for question of
used here and the code it's right there
and we know basically run it with that
num threads changing so at the moment
it's just set to five less who happens
so much number of elements to process to
be 50 so remember we've got an array go
back to this one let's see it's from a
of 50 elements and five threads what
happens that's all fine took a while
about 10 seconds and no problems we're
actually trying to process things fast
which when I use those other threads at
once next experiment 50 threads floating
screw up in here they should instantly
really good no issues at all ok let's
see what happens next
no man once it's a 50,000 number of
threads set to 10 I'm changing number of
threads by keying your system property
by the way for the fork I will show you
how to encode I'm changing it this way
system dot set property
and then it's basically Javadoc
Languedoc for common dock parallelism
that I'm changing you can made this
number real really big this happens to
your machine so just go back to
experiment number of threads 10 we find
one thousand threads okay that's what's
getting a little bit noisy now it's
starting to take off a little bit the
fans are going but I'm not crushed yet
and then we created a little bit silly
here and do fifty thousand threads let's
see we'll all happen so that sorry
really loud it was shy to take off no
native memory to create anymore new
threads that was a big problem I can't
smell it or process I'm pushing things
we'll see over time did you nothing the
mouth was stuttering around I'm trying
to click the X I got no chance the JVM
rememba clever here so when you getting
out of memory on out of native memory
exception the Roz agent for reliability
will kick in and try to create a bunch
of dump files for us and that wasn't
very good very useful because I'm I've
got no memory to create him in the first
place and may have been a bigger group
of trying to create files that it can
create new forks it's got no native
memory left we brough his crushes but I
made a presentation in I've lost well as
the work that was good firm I said I
can't control see no new processes
either and then I have to do what tries
to get a prince green I pressed a button
and that was it basically rage quit and
just rebooted that was trying to use
laws and laws of threads on this p50
laptop what we can do is use GPUs to
coach because some of the threads at
once if you've got the hardware and
that's because we will struggle for what
I've just shown you you can't run 50,000
threads at once on your didi little
laptop like I've got I'm going to tell
you about the spec as well in a few
slides the use cases always share these
common themes so machine learning is
pretty hot right now
and so is just you know cognitive
computing in push and what we want to do
is create models for machine learning as
quickly as possible I'm just going to
take a week to get back
model to predict some predict something
that's no good we want some real time at
the model creation or real time use of
those models to get good results back
for predictions so machine learning GPU
is really good fit I want to get the
results fast and I want our wait a week
to get my results I want to execute many
threads as possible the other thing is
you can only have a certain operations
which I'm going to talk about today as
well large amounts of data so there's no
point in using GPU if it's you know a
kilobyte of data you raise tiny I
mention that later on as well like I
said great for machine learning so if
you're wanting to use machine learning
from Java give this stuff a try so where
the GPU is getting used already them
well recent press releases so is a
alphago basically a go champion of
plenty of GPUs and CPUs as well is there
this Titan supercomputer of sorts and
CERN is supposed to using GPUs and this
yoke vijaya of supercomputer for quite a
lot of money and data bricks and now
it's diagram be using GPUs of spark as
well so few pictures they're battling
left is a tesla card and those are the
ones a little bit different from those
in your laptop which we're gonna mention
soon lots of this recent AI versus poker
win like chiffon recently so using 64 of
these and video per hundred GPUs to win
at poker and there's a recent Amazon
announced Minh offering GPUs as a
service and these Nvidia emails I always
get them at most I don't read them the
best wins are among a folder called
Nvidia and never click on that folder
but it mentions some good things so deep
wins come by asteroids take some road
days have deep learning then fighting
skin cancer
look breeding AI the live Cajun wearable
for the blind as well see and gonna find
these in your own time and then a lots
more success stories so what does make a
GPU actually good let's talk about that
so there's my little diagram at the
bottom GPUs are more like these
I'm doing lots of the same tasks at the
same time CPU is your your stronger but
more versatile a dung beetle so these
are the strong guys that can do the hard
operations and different branching curve
links do that fine but your GPUs are
more alike well I'm a bit steep like any
one operation at a time
secondly one operation I can do it
across many many cores at once
that's what GP is it good for our Megumi
using CUDA in this example and that's
freely available we can melt from the
nvidia website there's also open CL and
you'll need to know quite a bit of
better C and C++ but it's not exactly
the same so when you launch these things
called kernels you'll be using their
these look arrows to launch the kernels
a mission very very soon there were so
drivers by the way on Windows Linux and
IBM's Power Systems so what can actually
get what type of GPU I'll say there were
two main types your graphics adapters
you can book a march 20 so the sort of
thing that might be into this laptop
maybe you can integrate your card maybe
it's just a small nuts or pathway Nvidia
card but memory amounts may be two to
four gigabytes it's not much less than a
thousand posting cause decent clock
speed and these might be again in your
laptops what I've used is this scrap top
here I went in well seven three you know
decent amount of RAM 32 gigabytes it's
an 8 core CPU all xd4 filesystem and
good at some point 5 is what I've used
there is good at 8 now I've used 7.5
here then you've got the the big beefy
curves the HPC cards so these are the
ones you're gonna put into your servers
to do your massive workload so I might
test it fine on this laptop but what I
really want to do is put it in a server
with some Tesla cars in to get the best
performance over way more memory so
let's say to wait for gigabytes of
memory some the best ones more of these
little course called cuda cores so a few
thousand and the maximum performance she
might get would be in there teraflops
so think of this as your own
your own little supercomputer but it's
only good for certain operations no team
will be good for this and you're limited
as well by the PCI Express bus you got
to get your data basically from the CPU
and memory so your normal Ram onto the
GPU and that can take a while and then
you need a decent power supply as well
about 300 Watts let's say for the GPU
and here's what you can actually do so
I've got my laptop here and I ran device
query which you get with the CUDA
toolkit these are all attributes I have
for the GPU and the key ones there I
would say number of multi processors a
number of CUDA cores and the maximum
amount of memory so being to care about
most of that just memory amounts how
many cores and what actually is the
model this is a Quadro mm 1000 M from a
laptop and it's a band of tests as well
that's all about how much data you can
pass
she's a GPU how fast is that between
GPUs it's very fast between your host
and the GPU not as fast so I've got some
real numbers there on my laptop some CPU
information so there's some comparisons
I've done in this depth again it's 8
cores pretty good for a laptop and I've
got through 2 gigabytes of RAM as well
so what can actually be good at while
you're dating is conform to these soft
characteristics you know it's opposed as
well as other primitives at once so in
songs double shots on floats maybe doing
some matrix multiplication maybe
computing the dot product in machine
learning as simple transforms finding a
pattern and the operations keep it
simple so without any branching and
complexity which I'm gonna mention again
soon so that's why it's good at and then
what is it not good at well automatic
operations as well is what a GP is we're
the best at where it's not good at is
data it's not self-contained so if I've
got to keep column the GPU and go on
back again I'm backing them back again
they won't go for performance because
you got to keep transferring over the
PCI Express
then you've got the operations which
won't be good at either and that is
things that ants do with those other
numbers basically so if you're touching
any files by the idea if using the
network got me to do that on your GPU
and not good idea little creating
objects creating new ones for example
putting Malik's in your kernel or new
terrible idea involving object creation
along exceptions again terrible idea and
using threads for different instructions
at once I'm going to show you the kernel
decent kernel and you'll see what I mean
by this how to actually use a GPU all
the basic principles that you all need
to know let's to see you've got one big
integer array and we want to process it
some some way on the GPU go declare a
new variable I'm going to call it in
style
my dates on the GPU and we love our keep
some space from GPU using CUDA mallet
I'm going to pass it in as a parameter
so with a cudamalloc and the address of
my data on GPU members is it complete a
new variable you've got your own array
you've got you know hundred elements in
there I've got to go out on my way and
say right declare a new variable in star
my date on the GPU and then create some
space on the GPU there is a hundred
elements I know that in it's got me four
bytes each how many bytes Taiwan I'll be
400 you know copy it across using that
command
kudamon copy we've hosted device passed
in and then going to process your data
in a kernel so our method on the GPU and
then you gotta copy the result back and
that's all fine no Jeff Watts over here
so far by the way that's just all the
GPU code here's what a kernel would look
like so sent down in this case I've sent
down two arrays I've got in star array
one in star array two and I can access
each gentleman in my array using this
built-in variable called thread ID X dot
X in this case that thread ID X 2x will
offer some increment basically when you
read on the GPU if I want of 2056
threads it will start out at zero and go
through two five five so I can easily
access
array elements by using this notation
global what does actually mean that is a
function we can call from the host side
from the CPU there's also a device and
host as well house dates are actually
arranged I could access it well I
mentioned just a second ago and
basically a run every one in parallel we
can access each gentleman individually
by using you know thread RDX directs in
this case and I colonel were run on a
grid a big sort of 3d grid and different
blocks of threads and I'll mention this
on the next slide in start just with
regular C programming it's just a
pointer to be used on the GPU and that
thread ID x2 X will be also increment in
to access your elements what you can do
is there posting you know t56 feds only
of t56 elements it's out more blocks so
I might have parmesan 12 elements to
process out of two blocks to five six
threads each so the more work you have
you want to increase the amount of
blocks and I'll show you that in a
minute
so things to know mostly processes these
are what's really important on the GPU
and these actually do do the work so the
more multiples you have the more
Federman
at once then there are and is the kernel
we talked about earlier how many first
cannot really one can I'm wrong you know
million threads
I saw the look it's a multiprocessor
counts terms by a limit you find that
limit with the device query program that
are run earlier so you can't run you
know billions of threads here it's four
times 2048 on my laptop and then a tesla
k ATM so the really good GPUs will be
able to process many more so 26 times
that amount how much you actually need
to know them so i'll kernels get
launched on a grid what is that good
it's a free representative of how
threads can run on a giving GPU and
remember all your kernels get on the
grid like
many many blocks and may different
threads per block so you want more
blocks for more workload and the GP
function on these grids and then you
basically figure out how many blocks and
31 video SMI tells you what your game
it's a and device query as well so when
you're running your program how much
firm I'm actually using I used in video
SMI it says you know one gig out of four
gig to Gig off twelve gig etc so in
video SMI comes with the toolkit that's
really useful and a good starting point
I would say is pick a number of threads
maybe 512 and you think we should block
amount being on how big you were cord is
here's a little bit of Java code to say
how to do that for yourself so let's say
you know that that's blue
let's say let's process a million array
elements I would say two thousand blocks
is enough and thousand twelve threads
will do all of your work for you if it's
really smaller might be only one block
so you can use that jab quite at the top
to figure it all out for yourself so
this is a very simple example using only
CUDA I say really simple it's quite a
bit of code though you basically have a
a dot C file just a normal CUDA program
this will be and we can say how many
elements you want to process here
because say you say five and then you've
got a simple kernel that I've shown you
earlier and and we have the main method
below you've got to go through and
basically allocate an array and then
fill the array with elements and there
you can see the cudamalloc call the CUDA
mem copy call and then the actual kernel
on vacation number of blocks in this
case will just be one a number of
threads will be two five six there in my
name is no bounds checking here so I've
only got five elements to process see
five six threads so we can get away with
that with you have to print out what the
elements are it had a big junk data or
ghost zero there's no data yet and the
GPU for those extra elements only the
first five elements was on a five now
array
I've been copied across so you really
add some bounds checking here in your
kernel rich
if if the daddy extra X is less than
elements to process or num elements in
this case and you copy all the memory
back at the end
do your CUDA free and you're done that's
what you have to do if you're writing
your own kernel and the GPU Jarvis
gather even worse than that so what you
got to do is bring the java world again
now and you're an integer array on the
Java heap
Laska it's called my data and all you
want to do is use the GPU to process it
well we've got to create a native method
you put nobody in there Newman tation so
random native method of creating java
they've got to write the C code or C++
code with a matching signature let's go
of your native method so again it's
gonna be a pain then he go to your CUDA
v just created allocate space on the GPU
so enough for that workload and copied
AIDS all across and then process it in a
little kernel you've created copy it all
back at the end and then in Jane I
release the elements so you actually
update the elements are on the Java heap
at this point you've probably given up
on life it's very boring you know what
I'll do this stuff this is got at you
ages and then in Java you've got your
data so updated that's the long process
we've made it quite a lot easier than
that though looking example you need all
these files do it this way so I've got
it's a simple script to do it for you
you need a Java C on your main file if
you need to method in there you do Java
H and then you would basically include a
bunch of different libraries for J and I
then do nvcc which is a mid years of
compiler slash driver program and you
wanna share the library so I call it
devoxx to s so in your java program
you'll actually call dev box there so
we've gold library connected for GPU
that way then you've got to clean up
after yourself then finally you get to
when your Java program again the Java
lab your path is super important to
actually find your native library and I
quite often Gus a lot of segfault so I
- - xrs to disable all of Java's
viability functions basically sick
generate code them and want one dump if
something goes wrong so I can use - 6 RS
in this case
Java code here we've got the native
method at the top I mentioned earlier no
implementation details just native
method they measure processor and again
just taking taking area so this is the
hard way header file you'll need not
very good C++ code which corresponds
with the header file is all here so
again you've got to get the array
element so you've got to then call over
GPU valishia elements and you've got
about the CUDA code and it just takes
forever it's gonna skip all of that
check the results well it's actually
after all that work and we've actually
added 10 to all the elements it was a
really boring kernel I had to go through
all of those steps that's the hard way
what's the pitfalls of this by the way
and name mangling can be a problem so
you could just use extern see a boost on
so object dump and those parameters are
useful figuring this out so here you can
see some name mangling going on for that
add X int function look at that weird
prefix underscores that 28 not good we
use extern see to resolve that then you
might get seg faults X in weird memory
and the man either you've not actually
allocated on the GPU yet I'm trying to
access it you'll get plenty of seg
faults so straight away bear in mind you
in the unsafe world now with of CUDA C
so you to check of your memory accesses
make sure your points are still valid
that use things like gdb printf
everywhere not very good so I have some
prints here debug this this seg fault
and we can see get on hurl exception
check all your memory accesses that'll
be the first thing to do remember to
call your kernel in this strange way so
you know Fieros and there was left there
was that way as well
so yep adds add a bounced check in for
sure and check return codes
so all these QED functions can return
accurate arity and if you check its
success that means it was good it can go
wrong for those different reasons simple
way this is the easy way to do it you
are sitting job as much as you can and
the different projects you may want to
using Java so I've actually spark let's
say and we get our check in we get type
safety we get debugging tools a real
very useful for us and you get profiling
tools as well we get a nice JIT compiler
but get garbage collection all that fun
stuff and portability to an extent if
you're play by the rules and then your
focus we have taken that I'm going to
talk about next so I've changed to java
class library itself we've changed the
way the JIT compiler works
we've changed invited a new API called
cuida for J and we change spark itself
to use GPUs again spiral running JVMs
so I'm taking you through all the hard
stuff so far no more of that I'm getting
quite a lot easier so you want to sort
some numbers okay fine you can sort some
numbers here's how you can do it you add
one jar option you had there - D flag to
enable GPU and all this will do is stuff
on the left there so we'll say is it big
enough is it you know bigger than let's
say 25,000 elements if it's one element
to process now go away
boom use my GPU for that if it's too big
let's say it's to building integers and
you've got your memories too small nope
no can do it slow GP available you've
got none okay fine CPU you go and then
finally after all of that it will be
sent down to the GPU and you eat some
pretty good food point improvements so
you can see here around 10 X in this
case for sorting for integers so softly
not that exciting this is just making it
easier to actually use you said one
property basically then you've got the
JIT compiler modifications we've made so
the JIT will go in and basically
optimize your code from Java bytecode
ouncer native native code
different stages of JIT compilation
happen so you've got the cold level
which is barely optimized that was
interpreted and you go all the way up to
scorching so super optimized code what
we can do is look at your code that's in
this format so I'm using the inch dream
range here and I've got a for each group
going on and this will identify this
pattern in the code and say okay that
looks good when fine I'm a CPU but you
specified this option minus H to enable
GPU and then we can generate the GPU
code for you send your code downs for
GPU and you get the results back so
again I'm making it very very easy to
use GPUs without all of that hard right
in the j'ni code binding that I had to
file the make file no more of that so in
this case we specify actually option and
we're going to see what happens and
there are some numbers from that as well
on my laptop so this is all on here and
you add the option and we get these
prints so I'm going to basically
multiply two matrices together so 2048
elements it's also for your elements
let's see how much faster it is so we're
done saying up on the CPU for two
seconds not that good you win it again
about 41 seconds so Verge it's in a
better state maybe it was only cold now
it's lukewarm let's say and then on your
next iteration the JIT will be pretty
hot
there you go down so I'm going to take
in one second again that son this laptop
with not a very good GPU and a pretty
good CPU with their eight cores so we're
making it really easy as GPUs you get
all these prints and even tells you
which line of code are identified and
sent down to the GPU so in this case
here I've got a line number yeah it says
their uncle 39 if you just go back you
can see on this case 139 is the inch
stream dot range dot for each so the JIT
compiler see in the code and optimized
it for you
so actual performance numbers for
anything bigger than that I'm going to
tell you about now we're actually going
to compare 160 CPU threads on a decent
IBM power8 CPU pensee plenty of ram in
this case versus one pretty good GPU I
mean see what happens next so this is
the actual speed up amounts on various
benchmarks so again this is order of
magnitudes faster previous one was
around 40 X in this case we've got
between let's say you know 10 about
5,800 quite a lot faster than one finish
might expect but even the faster than
hyung-jun 60 CPU threads and that's on
quite specialized hardware as well a
parallel system the IBM provides again
inviting one jet option you giving it a
parallel for each loop it's speeding up
for you so here you can see the
benchmark names so you got matrix
multiplication you've got a sparse
matrix
there's Jacobi TD 2d and Conway's Game
of Life again you give it a J option
they'll be faster so what the advance
just DISA proach well you want to stick
to the Java world so you know your Java
code fine and then you've got a chip in
your laptop you give it an option ice
cream you need to learn lots of
different recording techniques and what
we can do is we can optimize this for
you as detailed on the right gentleman
PTX code think of it as a virtual
instruction set for GPUs and in theory
no coaching isn't needed by the Java
developer it's just warming up the
option now there quite a few more things
to mention we'll handle all your copying
of data for you and the JIT will take
care of the alignment and cash
management it's optimized that for you
as well there's also multiple devices by
the way so the field of my patience here
the GPU memory isn't an extension of a
Java heap so I've got my dates all over
here and the Java heap I've got get it
all across to the GPU that is a known
limitation
yes you still got a copy across and
that's handled for you the jet
compatible check it a days to these
criteria points you can only do it for
primitive types and one-dimensional
arrays of those types no Skylar variable
access no native methods can be sent
down either you've got a lambda and
you've got an e2 method in there that
won't work it's something for doing what
supposed to say with integers long
stores etc and again you can't be
creating new heap objects inside your
lambda to send down the GPU that would
not work yet based heuristics so we're
going to only send it down if it's a big
enough workload which we can figure out
and this will depend on different
factors that are mentioned just then and
it's quite conservative at the moment so
if you want any feedback in there's no I
mean change the heuristics based on how
big your work Lord is if you're seeing
better results King let us know as well
and then to observe if it was actually
used you specify that a verbose option
so at minus XJ enable GPU that will give
you all those prints are showing you
earlier that was mine at this bit of
code that's an it service GPU you can
override all the heuristics yourself
using the end force option and you
actually combine them as well or make
sure you use quotes otherwise paschal
come in and I think those iron is
actually a pipe so that's how you can
combine your options and I'll say just
give it a go for yourself and if there
are any issues you can raise a PMR
against us or develop a work she can go
on the forums and here see questions
directly we use in nvvm in spirit
representation language showing all of
this code for you and the scenes there's
the second way to do this to create
forge a API so you've got a for each
loop that's fine now you want to do
something a little bit different what
you need to do is write a kernel
yourself then that I've shown you
earlier you've got that global function
and so you write your kernel then you
create a fat bin which basically a fat
binary use an MVCC to build that fat
binary they need to basically avoid a
module at the bottom here of that
compared fat by
then you can call your kernel functions
directly from Java and what you need to
do is use these different classes to
basically copy the memory there or to
create a buffer ritual basically if you
use a clear 4j API you're doing sobriety
j'ni code it's an abstraction layer so
you've got these different things for a
CUDA device CUDA stream exception
handling is all done as well so you can
capture q2 exception and say olive oil
coming out of memory on the GPU I tried
to locate a massive array I've caught
the exception and if they're in a safe
way so you provide all of those classes
all there in the JDK and that's all free
of charge so you can go to
developerworks and get the JDK GPU
support today how am I actually use the
commuter forge a code in this kind of
way so again this is just pure Java code
over here and I've got my kernel at the
bottom right so you've still got to
write that kernel but the Java code here
would actually work and load up that
kernel saying your data there this nest
native code to write than without it at
the end of this talk there is a full
code listing but I would just say yeah
find this talk reported online that
should useful reference points go back
to memory it's be useful them I might
have about machine learning earlier and
this would be you've got a really good
Java program all works well I want to
speed it up big time your stat using
CUDA for Jake or the GPU Nabal lambdas
to get that extra performance boost
again you do some profiling first to
figure out which method you want to
actually optimize there's no point
spending you know 10 hours writing a
really good colonel if you're gonna call
one method that just you know it is done
like that there's no point
so you can use Health Center for that
August port you know print lines and
timers in a lot of places you can change
the heuristics your self restarting for
example you're going to say oh I said
25,000 that's too many elements
50,000 is better or maybe you want only
10,000 ohms you can configure it I'll
see a benchmark it for
self as well and just be aware of these
limitation on the GPU is only so much
device memory on there so you may need
to chunk up your ish your problem into
small pieces so I send down the first
one million elements process them so now
our next four million process them
that's not the best thing to do because
you've got still transferred between the
host and the GPU every time but
sometimes you need to adapt it's a
terabyte of data for example you can get
GPU to one terabyte of memory what we
can also do is improve existing projects
so we're kind of patchy spark in my day
job and a few things that spy it's good
for and I say machine learning graph
processing umm sql-like syntax for data
frames many other things so we can
easily things like Kafka for streaming
what you want to do is you sheepy use
for the machine learning capability in
SPARC so are floored work and get
performance boost that would be an end
game so different API is for it as well
the main point number here is that
sparks the full run in JVMs if you
include the JVM you will improve your
spot performance and what different
algorithms are I mean spark itself so it
headed ones that I know and what she
optimized a few of these we've delivered
the Austin least square optimization
there so the way I'll say only squares
works let's say you've got a table and
in this table it's got user ratings
maybe is all the movies that I've seen
and I rate them from one to five there's
also a genre there as well so I might
have for example them I might have think
I mentioned it on a couple of slides
here we go I might say this is forever
for bands and actually atom and then a
cool band one and then a rating and then
George comes along and he likes the same
sorts of a same sort of bands
the Mystery's your armor and then there
is a new rating here so a cool band for
how can we infer that adam law so like a
cool band for we'd use alternatingly
squares see this for you again that's
delivered in spark itself and this is
something we can use GPUs for so we're
going to go back for a second I
mentioned the other coup algorithms are
in spark itself so there are things you
might have heard about today - your
machine learning so there's our state
and least-squares basically filling in a
matrix of sorts recommend a recommending
products
maybe it's an amazon maybe it's a
Spotify or YouTube videos and the
different elements of clustering so
k-means we're really a popular yes can
your sheep use for k-means and then the
classification algorithms I guess it
regression this is where you basically
want to plot a line of best fit for your
training data mediator comes in and you
can see if it's above or below that line
of best fit a good case might be a spam
detection a more serious case might be
for health care it will be what are the
chance of Adam having a adverse drug
reaction of these different types of
medications if it's above the line you
will say okay it's a positive positive
he won't have any adverse effects below
the line nope don't give it to Adam a
very terrible idea again these
operations can be done in parallel and
therefore a GPU eligible that's the
whole thing least-squares works cause
motion and you can infer based on all
the data what Adam might also right so
there's some issues here because what if
you've got a big massive lose moles of
data and right at the very bottom of
your big data set is something that you
want to you want to infer so once we
infer that Adam will also like this this
band by a zac Zwick it's right at the
bottom of our CSV file and the GP zone
got so much so much memory you can only
handle one gigabyte of data for example
how would we deal this problem well the
algorithm itself also in these squares
and with GPU will handle this for you
you actually can figure you can chunk up
the issue into smaller sections of
parameters to say okay gentleman
all the ratings for the first 150 by
data then the next fifth and then next
to the next fifth you don't need to do
it all at once
so we've open-sourced the actual code
for all of this at that repository at
the bottom that explains how to an altar
and uttering least squares on the GPU
for yourself using the spot an hour
approach it's decent speed up this was a
twelve thousand a CPU taking you know
over six hundred seconds so eleven in a
bit minutes
compare that to using two GPUs and it
was running on over forty seconds so
straight away you can see it makes a big
difference now let's say it's a solution
of like I don't know deep warning for J
or maybe it's tensorflow
something you need to use to get a
machine learning model
well the real value comes from using
GPUs in the first place
to get the results fast again just open
source open source implementation and we
use two quite beefy GPUs the k8ttie
series in this example here again all
open source if you once know about
altering these squares itself the full
paper is that link as well and to use
this by the way it's pretty easy in that
you specify one parameter which is these
spike that I'm a little too ALS one and
then your work will be sent down to the
GPU so how do you actually do this
remember it but start of a talk I've got
the hard way of doing it we did this I
was at the same time and therefore we
have access to a cooler for J and
spreading the word about that very well
in IBM so a pretty big company and it
takes a while for news from one
department so it gets one from another
and we figured out that these guys we're
doing this as we were doing create a
fake J in the lambdas and we said hey
maybe that would be a good example to
show off the GP capabilities they said
yeah sure can you test it for us so I've
did quite a lot of testing a sort of a
and the end of this approach or by your
right you have to write the CPP code in
C++ code we did this because it was only
one method
they needed to change in spark so if it
was right screaming application about
fifty methods quitter for J lambdas are
the way to go it song one method you can
get away view at the Hardaway we'll do a
build script of the header file so this
was doing around the same time and
that's how influenced her and again
that's all open source and in our
packages for app actually Sparky and
download as well so we're basically
giving it away
welcome actually do to make even better
for spark itself so it's just sending
generated code from spark and changing
the generated code to be a good fit for
the GPU so remember that it's only good
for some operations so in shuang's
doubles floats arranged in a certain way
so let's say contiguous order if we
change the way catalyst works so they're
cordial optimizer in spark you can
basically offered arbitrary functions in
spark for a map for a filter for a
reduce for us or the sort of thing
central GPU for benefit that's what
we're trying to do at the moment are in
veteran I'll show you a few pull
requests we've done to enable of this so
this the endgame here is you've got
spark you've got a GPU great your mind
faster thanks
so what are actually one color that's
able to use auto Cindy or GPUs and what
that needs to do is have these
characteristics mentioned here so add a
tux in a linear fashion no external
method calls not doing silly things so
now you know file handling and creating
new objects simple process in with a
counted loop if it's in this format
great st. done for GPU so a big change
in where catalyst works to enable all of
this a few issues of this the way then
date acceptance at the moment isn't
ideal so we've got some solutions or
below she's a new data former in a
purchase back itself what you want to do
is generate code and in this way at
runtime that will simplify our
operations to arrange for data in that
contiguous where I mentioned earlier
but there are a few challenges of that
and what we've actually had done is got
these pull requests quite a lot of them
are already in spark quite often been
merged or will be merged and these
aren't yet using GPUs so stage 1
arranged to spark code it generates been
a certain former that's a stage 1 to
stage 2 bit is that we look at that code
in their JIT compiler and say that looks
good for a GPU I'm gonna go ahead and
optimize it for you sit down for GPU you
get a performance result so R at phase 1
at the moment
Pulver quests here and I'm page 2 as
well and you just by actually
rearranging their code in a certain way
we're improving spike functions already
just on the CPU so know who's I'm a GPU
yet some quite good performance
increases what suits me if I do care
about spark what I mention do which is
just you've got this machine running
library you've got the hardware you may
as well they use it I thought we had
everyone care and that's his ongoing
word that you could then apply different
projects it's maybe it's optimizing
tensorflow
I did win for Jo or the libraries see if
you can get them using the GPU more
often than the out of the moment some
challenges see of GPU Pergamum you have
a stripped it a lot by the PCI Express
speed so you can only you know send down
some mushy peas send down so much data
to the GPU at once rising a kernel by
the way is very difficult so it was a
project called root beer and in the git
project information it says expect to
write between three to five source
source code lines per hour so right in
the kernel might take you a while and
fed in this non-trivial the different
memory types as well I'm a GPU to the
shared memory
there's also texture memory there's a
limited amount of registers as well seg
faults or cleanse occurs you're in the
unsafe world now getting us access
arbitrary random pointers to random bits
of memory and there are many GPU
developers out there compared to other
programmers so for example them in IBM I
would say
bubble Gastner Hong Joo the less than
100 they learn how to use GPUs of you
know nearly 400,000 employ years so if
you know about Java you know about GPUs
it's probably pretty good for you
what other videos out there to talk
about this to an extent for GPUs but not
really on Java and GPUs so it's kind of
why I'm here today and the CUDA SDK
itself and that changes quite a lot but
one thing the Agata is backwards
compatibility so I can use credo 7.5
with the same code that I've used
include a 5.5 which might have been
around for you know four to five years
so I would say profiling you know
obviously get doing it debug in really
quite difficult and more for the
environment and what could possibly go
wrong wasn't other things if J and I
going wrong you know Java your CUDA code
bad C++ code that'll be a popular one
bad design you work close tiny what's
the point using GPU maybe should use
spark instead maybe for even being Java
because you see plus plus bear their
brief gigs just finish there are lots of
things from projects using GPUs in Java
out there these ones you should be aware
of I would say open CL you yourself CUDA
and tends to file system ml j cuda is no
alternative did wedding forge a pretty
good and video table is like c UD n n
cube blasts and thrust and that bruh P
as well this is the AMD alternative
using GPUs from Java how many of your
time was that one zero okay so no need
that they're different projects using
Java are out there and GPUs and we'll
skip the debug an example long story
short is that up Cunha but I thought for
a week because I basically used all the
memory and lots of seg faults I wanted
questions on this very quickly
so I'm gonna be in the IBM booth in
exhibition hall if I know about GPUs or
Java or spark come and find me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>