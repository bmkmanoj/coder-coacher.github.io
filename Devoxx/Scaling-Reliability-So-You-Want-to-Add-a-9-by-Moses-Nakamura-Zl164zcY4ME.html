<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling Reliability: So You Want to Add a 9 by Moses Nakamura | Coder Coacher - Coaching Coders</title><meta content="Scaling Reliability: So You Want to Add a 9 by Moses Nakamura - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling Reliability: So You Want to Add a 9 by Moses Nakamura</b></h2><h5 class="post__date">2017-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zl164zcY4ME" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright I'm going to get started more
people comes and they're not going to
miss much in the first part ok so I'm
Moses Nakamura I work at Twitter so
first things first this talk is getting
reliability so you want to add a nine
and in this context the nine so we're
talking about our nines of reliability
or talking about like say you want your
success rate to be ninety-nine point
nine percent we would say that's three
nines an example maybe when to add
another nine that would be going to 99.9
nine percent success rate I'm Moses
Nakamura I work at Twitter I work on
open source project called sneagle I on
i'm on twitter my handle is add MN
nakamura i would appreciate if you
followed me because i'm paid directly
proportionally to how many followers I
have so the core idea of this talk is
that as your organization scales up the
reliability challenges that you're able
to deal with and want to be able to sort
of need to deal with for your business
are going to change also and I'm going
to talk about the different phases that
your organization or the different
classes of problems that your
organization is equipped to deal with
and wants to deal with change over time
and as your organization scales up and
different ways that Twitter solve
problems with this specifically in the
RPC space and many of these problems we
solve this needle so if you want to take
advantage of our solutions for them to
free to go check out the project
so one question is who cares about
reliability Twitter cares a lot about
reliability I think probably if you have
any kind of business that's providing
some service to your customers you're
going to care a lot about reliability
Twitter cared a lot about reliability
because in some ways when you mention
Twitter a few years ago actually more
like seven years ago at this point
people would think of the fail well
which is this nice full image which
basically was a sign that twitter is
down again and that's not really what
you want your business to be known for
you probably wanted to be known for
whatever your business is actually
supposed to be doing and not not being
able to do whatever your business is
supposed to be doing so one question is
how do we measure reliability there are
many different ways we can measure it
one of them is up x might be a up time
per quarter up time per week where you
just say how many minutes were people
able to get deep whatever content they
want from me versus how many minutes
brother over that period this is up time
is probably the most common unit of
measurement and usually when people are
talking about nines are talking about
nine set up time there's also dollars so
for example if you're not able to hear
like twilio and you go down for an hour
and people trying to pay you for your
stuff but you can't or you need to
refund people for something that they
already paid for those are dollars that
you've lost only thing is going to be
success rate the mean time between
failures and the mean time to resolution
so I inches finagle briefly before
finagles and extensible RPC system and
we use it for every service at Twitter
which means that pretty much every
service has a bunch of reliability
guarantees built into it from day one
and finagle has grown somewhat
organically in some way from the top
down we saw that as we were scaling our
organization we weren't able to scale
the reliability concerns as linearly as
one and two and single became a tool to
help us do that okay so the first step
that your organization is probably going
to take as far as managing failures is
going to say teams must own their own
failures and it seems like a pretty
basic thing but in many organizations
this isn't really the case like
sometimes Deb's will put something out
and then someone else like ops or DevOps
has to go and own the failures usually
what this ends up meaning is that their
end up being a lot of background noise
failures and just weird hardware things
I might have in your data center that
just go on just go ignored because it
isn't such a bad experience with
customer but as you force teams to own
their own stuff then they'll begin to be
able to look at this stuff more deeply
so some examples of background noise
failures are gray failures like for
example transient failure heisenberg's
which are things which are non
deterministic temporarily overloaded
hosts for example when you have weird
load balancing like you're using a
random load balancer and crashes bad
remote hosts are usually because of
hardware failures or your remote or a
remote peers getting into a state and
hasn't been shut down properly yet
so let's say we have a probabilistic
failure when the caller talks to the
receiver it's going to succeed with
ninety percent probability what are some
things we could do to deal with this so
I could you do that what would that
entail what's an example of that
yeah so if there's a failure just sort
of no have like a good backup plan
basically another thing is if this is
really probabilistic we could just try
it again if it fails the first time but
we don't know whether or not it's going
to fail the second time and we could
just do it twice and they're going to
have a higher success rate so in this
case we added two more retries and
suddenly moved our success rate from 19
to 39 s like a pretty simple approach
that I can have pretty good impact reach
highs are of course not always an option
for example if you have a non I'd
impotent right and you don't know
whether you're right happened or not you
don't want it for example bill your
customer three times just because you
weren't sure whether or not the right
happened another thing that can happen
is sometimes when you go one room up
here which is having a bad day for
example one remote peer that's in a bad
state then probably you want to be able
to route around that here instead of
continuing to have them sell your stuff
so one thing can do is just count how
many times a remote peer gives you a bad
response and depending upon how many
times that happens change change how
often you talk to each of them so in
this case there's one room up here which
keeps on giving you bad responses and so
your clients notice this and say okay
I'm going to stop talking to them there
are many different strategies for
failure detection it's like a very
widely studied field I would say we've
had pretty good success just keeping
track of success rates with an
exponentially weighted moving average
and you can do that either for like a
number of requests or some duration and
there are lots of different ways of
tuning this
depending upon why your use cases are
okay so let's say that there's an
individual host gets overloaded what
options do you have so this one said use
the other two that's a really good idea
that's sort of going back to what we
were talking about before where you
figure out okay we're instead of talking
to the one that's bad we're going to
just ride around this one sometimes is
difficult to the client to tell how
unhealthy or remote peer is so so if
this server has better information that
is getting slow then other than the
client just load balancing and saying oh
hey I don't want to talk to the slow one
then the server also has the option of
saying hey don't talk to me so typically
this is called a neck and there are lots
of protocols which have these built-in
for example HCB too has our estis built
in which can be used is next rst to lots
of different possible error codes
there's one which is like enhance your
calm you can use as hey I'm overloaded
and your client can have better
information rather than just oh hey that
room up here I've is weirdly slow that
they shouldn't talk to that one is best
so beyond these sort of basic background
failures that happen all the time once
you have a team that's dedicated to
resiliency then suddenly there's other
kinds of things that we can akal this
begins to become more of an issue as we
scale up our stuff so one example is
lots of the micro service architecture
right now is pretty popular this is very
neat but as we add individual components
if each app has a ten percent chance of
failure if suddenly each tier has a ten
percent chance of failure then our
overall success rate is going to go down
to like sixty percent source it isn't
too good but then if we're able to do
something like a treat rise then
suddenly we can go up to a much more
acceptable to go straight like 99.6
which is not great but is there's more
we can do basically this just means that
we have a smaller and smaller room for
error that will appear as really user
visible another class of errors that we
might want to focus on other than just
the background noise failures are
failures on edges or where we introduce
some change into the system which might
be a new deploy or it might be turning
on a feature toggle or your client
deploying or the receiver who you're
calling deploying all these things can
introduce lots of changes into your
system and it's useful to be able to
have tools to deal with these kinds of
changes one example is when your remote
peers are deploying if you don't know
how to handle it gracefully they just
can't do graceful shutdown for example
then you might see lots of errors also
when you get new code sometimes those
bugs in it
I've stopped writing bugs but for some
reason many other people at my companies
to write bugs I've tried talking to them
about it but they're not that responses
okay so one question that we might want
to know is we have the caller in this
case we have three instances of some
service and they're all talking to three
instances of some other service what
kinds of things might want to know about
what's going on here so here are some
things we have this service a talking to
a service B which then talks to a
service see and what's happening here is
a makes requests to be B makes requests
to see see isn't able to pan back a
successful request to B so B does a
retry to see which is then successful
and then be responds act a so what kind
of information might be useful to know
in this scenario
Dylan say something okay
so when one example is number of
internal retries that's a good idea
anything else like if you were
instrumenting this what kinds of things
might you want to know about what
happened in this if this was your
service okay why did it fail that's
another good example maybe we'd add
logging for exceptions or something like
that anything else sorry yeah what was
the request payload so if this isn't a
particularly high throughput service and
we might be able to just log every
single request or if there was an
interesting question might want to just
log the request payload for those
specifically or sample them or something
like that those are all good example
some other things in my where now is the
latency of a given request so maybe the
latency of the overall request then also
the latency of individual requests and
as you mentioned before be useful to
keep track of how many retries we did
how many failures we have on these
successes we have and if you have a
distributed tracing system we would also
be nice to just keep traces of all the
stuff in locks so observability gives us
much more power when trying to
understand our services instead of just
trying to reason matter from the code in
like hmm this is what should be
happening we can actually inspect what
actually does happen so server bility is
pretty important to have in order to
keep your services life we want to have
metrics like time so you stuff which you
might get in something like data dog or
graphite logging and in theory you can
persist your logs and should be a pretty
cheap way of keeping track of lots of
events tracing distribute tracing is
enormously useful if you're able to set
it up although it's requires
often a pretty consistent environment
it's really useful have your
configuration information for example if
you know there's a bug with services
which are configured in a specific way
then instead of having to go and like
redeploy every single service you can
just go and find which ones are
configured in that specific way and fix
them and also it's useful to know who
your remote piercer okay so for people
who still rate bugs sometimes they find
that there's a bug in their release what
are things that we can do to mitigate
the impact of these bugs and releases
rollback that's a good example so what
you're imagining you go and you deploy
to every single node and need to see oh
my website is totally down and then you
do a rollback okay is there any way
where we can avoid getting all the way
to the point where your entire website
is down so an example is given here is
do a canary deploy deploy to only one
and watch what happens it's a really
good idea so we can do a canary and if
that carrying falls over then you know
probably you shouldn't roll out to the
rest of your ear services you can save
doing the full rollback you just have to
roll back the one canary so I mentioned
before that sometimes you can have
failures while you're rolling so imagine
that we're doing a rolling restart here
so we're taking down this one receiver
and wallets down the college don't know
the receiver has gone down yet so they
keep on sending requests to it even
though they're going to fail so instead
of consuming send requests if the
receiver is able to do a graceful
shutdown and drains requests and let
clients know that it won't be able to
handle requests anymore then we're able
to have a much neater solution here
so in this example first there are lots
of requests which are being sent start
sending responses then we have basically
go away I'm going to die and then the
client is that no more of its request
will be satisfied actually in this one
this was the responses weird because
it's dead and this one says actually go
away and again we use HP to or Twitter
as a protocol called mucks which also
has this built-in you send the go away
you can say this is the last request
which I'm able to satisfy ok so after
some point we begin to get into the
realm of the kinds of problems which
possibly only dedicated teams are going
to be able to help with another way in
which when you scale your organization
up you begin to keep on having more
problems is people are basically plug
producing machines and as your team say
doubles in size over four months or
whatever team growth you're so proud of
your bug production will also double in
size over those four months so another
way in which we need to be able to lock
down resiliency errors more and more
effectively so some of the kinds of
issues that will have won't be necessary
planned events like these kinds of edge
changes i was describing before
sometimes will be unplanned events one
example would be an overload like for
example you get written up in the New
York Times or your carry on good morning
america or some like that and suddenly
everyone everyone is there
didn't realize how much attention you
agree beginning another example can be a
retry storm when where because yours the
remote peers are all failing you send
bunch of requests to them and they fail
then you retry them and then that causes
the overload to be worse and so it fails
even more requests and ends up being the
sort of awful positive feedback loop
there can also be a hot shard problem
one sort of weird thing that Twitter
once ran into is not sure if you've seen
this movie castle in the sky before it's
a popular anime film from the info late
80s Early 90s and in Japan when they air
it there's a tradition that when a
specific scene comes on at that very
second everybody tweets this magical
word of destruction which destroys all
computers in particular destroys
Twitter's computers so that's like a
weird event that can be difficult to
predict unless you're watching the Japan
broadcast planning which we now do they
can also be poisonous library deploys
which are necessarily caught immediately
and there's also be problems when you're
mean times recovery is slow where if
there is this a sort of unexpected
problem and you aren't able to recover
quickly then that can make for example
your uptime much much worse
okay so I mentioned before we can have
these retry storms where there's some
level of overload so you send more
retries which then makes the overload
worse one way in which you can solve
these is having a retry budget where we
say no more than X percent of these are
allowed to be retries so i can say i was
able to send like 80 success for quests
i'm going to say only twenty-five
percent success for successful after
successful requests are allowed to be
resized this limits the overall overload
that your clients and send to a remote
peer another example is this cluster
overload case where for example some
people are want to go Russian say a
magical word which destroys computers on
your website and there's just a bunch of
natural traffic that is too much for
your service to handle what are some
options we can take in this case
so one example would be rate-limiting
what might rate limiting entail yeah so
some kind of load shedding one sort of
simple way of doing this is if you
already have retries built in you know
this is going to be a temporary overload
then you can have exponential back-off
and jitter added to your retries another
thing is as you mention before we can
shed load so here's the case where we're
sending bunch of requests for not
getting any responses back as we expect
if every single remote peer is saying
hey I'm having a bad day then your
client can know maybe I should back off
a little while and start dropping some
proportion requests so in this case we
might say because we're only at we're
only seeing like an eighty percent
success rate why don't we try dropping
ten percent ten percent of our traffic
and see if our remote peers recover any
case where they're only able to handle
up to like ninety-five percent of the
traffic that we're sending them this
would allow them to recover up to ninety
five percent if we guess ninety percent
that's still better than the eighty
percent that we're seeing right now
something that you might run into also
is if you have for example if you have
bad neighbors who are stealing a bunch
of CPU time from you if you're in AWS or
something like that or if you're running
something which says that if you try to
be a bad actor and take so much CPU time
then you're going to be throttled then
you can also tell people to back off
based on that so in our case we often
have services get throttled when I do a
spending too much time doing garbage
collections which sometimes indicates
the GC spiral so we use this as a
basically a sign that says hey I'm about
to go into a GC death spiral how about
you stop saying the request so i can
stop doing that so we have a little
thread running the back of a process
which just goes in checks and says okay
am i being throttled how badly am i
being throttled and given that should I
start asking I Ramon appears to back off
so another example of something that
might happen is imagine that you deploy
some new service which can send
poisonous requests which can poison the
rope here but also will appears down the
line because of some new feature that's
been turned on or some insidious library
feature which has appeared like a
ticking time bomb in many of your
services already you don't know which
ones and we want to turn it off but in
order to figure out which ones have been
deployed with which code it'll be very
labor-intensive these kinds of issues
can be solved neatly if you have feature
toggles for your library code as well as
for your regular stuff so you can sort
of imagine we're sending all these
requests and because of some new feature
we've turned on the second one is
poisonous and so because of this the
remote peer is only able to send one
response after the poisonous wine it's
not able send a new stuff by having a
toggle that we can turn on or turn off
when we have a new feature we can have a
little more confidence when we're
deploying them so one example might be
instead of turning on for every single
remote service for every single service
you could turn on for like one percent
services at a time or five percent or a
ten percent another example might be
when you notice something is bad you can
just use it as a kill switch so that at
runtime you say okay this feature is not
working out for us it's killing a bunch
of services we're just going to turn it
off and figure things out
another example of a way that you can
help avoid this problem is going back to
what we're saying before about carrying
you have different staging environments
where staging nodes only talk to other
staging nodes you can help make this a
little bit easier to work with so
imagine that you have some new release
of your software and first it goes out
to staging where first the color goes
and poisons the receiver and you can
serve CEO staging really had a hard time
after we sent up this poisonous
librarian it ran for like a couple days
so another way that we have of helping
with this is we have this naming system
built in to finagle called wily and one
of its tools is these D tabs which are
things you can annotate a request with
which will say for example like hey this
should only go through staging servers
that way instead of having to manage
your staging environment totally
separately from the pod environment you
can simply annotate certain staging
certain requests as being specifically
for staging without having to say and
like these environments will be totally
sealed off from these other environments
you can just do it with your naming
system so one thing is every day I
mentioned today is things that are built
in to finagle but these are of course
not the only ways in which trader helps
itself stay resilient and basically
we're constantly working with many other
different teams in order to get this to
work all together and everyone is
thinking about the reliability of the
website overall and how we can all work
together we talk a lot we work a lot
with the observability teams so that
we're collecting the right observability
metrics but also we're not collecting so
much that it's going to bankrupt where
we also work with the automation teams
that go in
make tools so that for example when you
notice that a service needs to be
restarted instead of having to go and
wake somebody up to go restart that
service you can just restarted itself we
want to make work with the deploys
people to make sure deploys work the
right way and don't go crazy there's a
great coordination team which makes sure
that when there's a big incident things
get coordinated properly and people
aren't just all running around doing a
lot of duplicate investigation there's
the site reliability engineering team
which is making sure that reliability
best practices get spread through the
organization and we also make sure that
we're all practicing for the stuff so
that when there is some natural disaster
and whether that's getting some kind of
big boost in traffic or a literal
natural disaster like a hurricane coming
and like striking your datacenter that
we know how we're going to behave in
that case and that we're able to handle
it another big thing here is failing
collaboratively so it's difficult for
your company to make progress if
everyone is sort of failing by
themselves and not telling anyone else
about their failures because it means
that you're never able to learn from
other people's mistakes you're only able
to learn from your own mistakes so one
really good thing that we do is we have
just like a global list of all incidents
that's just like one big juror project
you can organize it however you want but
on the finagle team I can go and see
incidents from every single other team
and why they happened and try to infer
whether there's anything so could have
done in order to make that incident less
painful for our end users another thing
that we mentioned before is coordinating
things so we want to make sure that the
resiliency efforts that
we're doing in order to squash a
particular kind of incident isn't also
being duplicated by some other team like
for example the deploys team which is
investing a lot of their time in order
to solve the same problem instead we
could better we could better provide
resiliency by saying okay you deal with
that thing which is really your
specialty and I'll deal with this other
thing which is my specialty another
thing is making sure that you're
empowering individual teams in order to
make the changes that are actually
necessary in order to improve resiliency
if people know what the problems are but
can't fix them then it's no point in
knowing how you could fix the problem
another thing is making sure that you
keep history and institutional knowledge
is sort of one of the most difficult
problems of maintaining institutional
knowledge one with the most difficult
problems of running company but it's
really important and really valuable if
you're able to do it and sometimes
that's just telling war stories at like
a monthly event where people remember oh
yeah there was this really bad thing and
maybe we should make sure that can ever
happen again or maybe that was why we
had that one extra check that we just
removed last month maybe we shouldn't
have removed that maybe we should add it
back in another thing is making sure
that your post mortems our blame frames
but people are able to be honest and
don't feel like they have to sort of
cover like redirect blame or cover for
themselves when they're trying to get
down to the truth of wine the incident
happened and how you can print trapping
in the future alright that's all I got
any questions
yeah so the question is what does the
tulip project i work on which is finagle
do to help witness and so just from the
beginning finagle is a system for having
one service talk to another service so
it handles everything from connection
management to load balancing to naming
to service discovery and failure
detection and all these things so the
API that gets exposed to an end user is
I have this thing which I can put a
request in and from there it goes to
some service some service some
individual note on some service and you
can get a response out of it and one
example of where this sort of opaque
abstraction helps us is I send requests
which then gets load balance to a node
which I know is healthy and is probably
going to be fast and is in a data center
that isn't sailed over preferably in my
data center and if it fails to make a
request because of some dumb reason like
for example we were enable a style of
TCP connection we're just going to retry
that automatically there's some retries
which you can't do automatically but for
everything where we know that we didn't
actually write anything to the wire will
just do it automatically so we also have
lots of connection pooling stuffed baked
in and at the protocol layer we have
been acting stuff like for example if
your throttle duel just started acting
automatically or if you can also
configure oh I don't want to have more
than this number of requests coming into
my server and if too many requests are
coming in you can just start knocking
request
so you know we also have draining
built-in and all kinds of other things
so finagle is sort of taken care of
almost everything that you want to do
between two stateless services the
question is what are the language
bindings for it and so finagle is
written in Scala so it's anything on the
JVM so one approach that people have
taken when they wanted to use finagles
feature set but didn't want to be on the
JVM is there's a company called linker D
which basically turns Nagle into a
sidecar where you can then talk to it
from any language you wish it just is a
little process that runs alongside yours
ya got Oh are assuming that your weapon
vas data center and bottle service
singularly service machines and of
course a more redirected to another
subject doesn't work or finality the
center doesn't work when I say only have
three that even the canary might be
tricky because that someone takes one
sort of my only negative way yeah so the
question is how much of finagle assumes
that you're in like an enormous
environment so Lisa Twitter we have some
services which are enormous like on the
ice enormous for Twitter so on the order
of like thousands of hosts but there are
also some services which are in death of
scale and finagle is used for both of
them so we expect that it should behave
well often people have fewer problems
when they're at a smaller scale which is
part of I was talking about before in my
talk so I would say that you won't see
as big benefits from using finagle but
definitely it can still be useful and
there are different ways in which you
can configure it if for example you're
only talking to one remote here although
some pieces like load balancing won't be
that useful for you
yeah so the question was it's nagel on
the client or on the server and it has a
piece which it which can be on the
client side and it also has a piece
which could be on the server side and so
using that people have constructed the
proxy which she described which is
linker d so yeah it's all three yeah so
smegal was designed to be usable with
many different protocols so originally
it was intended to be pretty extensible
and still is soliciting so twitter has
made eight work with memcache d Redis
thrift initially Redis was a
contribution from tumblr we added a
protocol we wrote called mucks just for
in data center RPC we also have HTTP 11
issue 10 and HP to which is still
somewhat experimental and other people
have oh and also fill my sequel and
other people have written bindings for
post grads and proto bluffs and other
languages or other protocols good
and yeah so the question is are you able
to inspect the message as well as the
protocol and finagle exposes different
api's depending upon which protocol
you're using so like for example it
would be it would be somewhat elaborate
in order to support having a climb which
could speak both to memcache g server
and also to a thrift server so when
you're talking to memcache d you go and
get whatever information you need from
i'm testing so sometimes we're not going
we're probably not going to give you
like the raw bytes but if you wanted to
infer what the raw bytes where you
probably could and if you do need
something where you have raw bytes then
you can build that yourself with me
so the question is is finagle extensible
and does it have to be extended from
Scala or can use any JVM language so
it's definitely easier to extend in
Scala but people have extended it in
Java oh and it is extensible people have
extended it in Java and we're constantly
trying to improve the Java bindings
sometimes Scala unfortunately doesn't
make it that easy to make really solid
Java bindings but something we're
working a lot on and if you go and try
it and find that you're not able to
please file a ticket with us and we'll
help you fix it as far as other JVM
languages like groovy or JRuby or JSON
or whatever other JVM languages are you
thinking of I haven't tried with
anything so my suspicion is if you're
able to do with Java and other languages
have good ffi for java then it should be
totally doable but I don't know for sure
yeah so the question is suppose there's
like a JavaScript server or even maybe
like you know like in the web browser or
something like that which wants to or
got on your phone that wants to send
messages along how would you have a
participate in the sort of snaggle
ecosystem or Twitter ecosystem every
want to call it and that's not something
we have a super amazing story for right
now to be honest so one of the reasons
why we're working on this proxy or so
other than linker d we're also working
on something similar which we want to
help make other services built in other
languages be able to communicate and
participate in the Sinaiticus system in
a much simpler way so right now when a
new service gets built at Twitter we can
say oh just do it on the JVM that's easy
but if Twitter acquired some company and
they're using go and they're using grp
seeing and whatever other things then we
want we don't want to have to force them
to rewrite all their stuff in Scala I
mean we acquired them as we thought they
were what they were doing was already
good so we just want to give them thin
shims that they can then use to
participate so I think that this proxy
or a sidecar idea has a lot of legs and
it's worth investigating at least for us
you might find that's the case also as
far as things on the client side it's
not going to be easy to add a sidecar to
whatever web browser you have or like
web application you have probably so
sir
yeah so the question is if you have some
client how can you have them like add
headers that will have you propagated
throughout the entire system and
basically we just have like a list of
headers which are special and client
people who are writing clients will add
them so that might be like hey I want
this to be traced just like debug equals
true or something like that it's just
something that people know yeah but in
future we could write client libraries
that would just add those edges for you
but we don't have that right now and
clients have like pretty diverse ways of
talking to the servers because there's
not just whatever you have in like your
android client or your iOS client but
maybe you have other kinds of clients
and maybe you also have somebody who's
running an iOS from like four years ago
so that might be like a totally
different climates so when we're getting
into clients that you don't control
that's getting it's like a pretty tiny
territory
okay well seems like that's probably it
thanks for being here I'm just going to
hang out after the talk if you wanna
chat</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>