<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Back to BASICS - Back-pressured Asynchronous Scalable Immutable Composable Streams by Viktor Klang | Coder Coacher - Coaching Coders</title><meta content="Back to BASICS - Back-pressured Asynchronous Scalable Immutable Composable Streams by Viktor Klang - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Back to BASICS - Back-pressured Asynchronous Scalable Immutable Composable Streams by Viktor Klang</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v3msiBIny6s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody can you hear me all the
way up there welcome to my session we're
going to talk a bit about going back to
basics and I'm going to mean that in a
couple of different ways so I'm Victor
clang I work for a company called
typesafe this is my twitter handle if
you feel like you want to have opinions
alright so one thing that is fairly
apparent right now i would say and
please see if you disagree is shout and
say that you disagree but i think that
software is becoming increasingly
interconnected so it's no longer a
single person executing a program from
start to finish and inspecting the
output but programs are actually being
composed with other programs and running
for the benefits of other programs many
times without even having a person
involved at all in the execution or
inspection of the results of a program
do you feel like you agree with that
awesome that's good that's the axiom of
the talk all right so what is a system
we're programmers rewrite systems create
systems or rather do we encode systems
because the system is a set of things
working working together as a part of a
mechanism or an interconnecting network
a complex whole right what does that
even that's sort of fluffy right what
does that even mean well I think
technically we are systems living within
systems living within systems living
within system so everything you sort of
like systems all the way down we have
complex things which appear very very
simple so what do I mean by that well if
you look at a system
its purpose is typically very simple
take a watch for instance a very very
classic round wristwatch its purpose is
to let the wearer or someone else that
asked what time it is to know what time
it is but if you've seen how these rich
waters wristwatches are implemented you
might know that there are hundreds of
small small parts that makes this very
simple purpose come to life so the
system itself could be extremely complex
but typically the purpose of the system
is simple do you agree with me is it too
early okay so complex inner workings
things working together to provide some
sort of functionality or do something
that means that they're collaborating
right so components that collaborate to
form a system to do something so what's
interesting as I mentioned before is
systems within systems within systems so
a team a developer of development team
could be a system that lives within a
larger system which is called the IT
department which lives within the larger
system called the company which lives
within a larger system called the state
which lives when within a larger system
called the world which happens to be
something that lives within a planetary
system which lives within a universe and
so everything is systems within systems
one observation is that tends to be that
components are as simple as feasible but
not simpler so there is a local maximum
of component complexity because they
have to do exactly what they're supposed
to do good enough so that they aren't
replaced with something else but you
can't come can't replace something with
something that is worse perpetually
because that will just degrade your
entire system so there tends to be an
optimization strategy going on I think
first and foremost programming is not
about solving problems at all
programming is about encoding
solutions for a machine to execute so if
you cannot solve the problem without
programming you can't solve the problem
at all because you'll have to encode it
for the machine to understand it and in
order to be able to encode it you'll
have to solve the problem does that make
sense because I know as a developer it's
so easy when you're confronted with a
problem to think I'm going to need this
framework I'm going to need this thing
I'm going to connect those things and
I'm going to do that thing and I'm gonna
ship that thing but perhaps it makes
much more sense to just take a step back
and say hmm interesting problem how
would I solve this if I hadn't got a
computer or a programming language
because there was a time before
computers were people solve problems so
if we accept that programming is the
encoding of a solution for a machine to
execute we can solve problems with our
program so what is the most common
developer task any guess drinking coffee
that is a very common task Oh looking at
logs reading pods that is a very common
task as well
sorry updating tasks writing code
thinking thinking is good the end what
about receiving inputs is that common
receiving some sort of input to do
something with can we accept that that
is a very common task cool
how about transforming some data like
you get some input and you need to
transform it in some way
like do something with the input or we
get it sounds fair enough
how about producing outputs right
processing yeah so we have inputs
transformation outputs very common dev
tasks receive the inputs do some
transformation producing modes first
that sounds pretty cool what if there
was something that lets you do this that
would be that would be great right so
let's talk a bit about streams so just
before everybody has like their own 100
million different definitions of the
word streams there's just it's undefined
streams so let's just start from from
blank and let's define streams so what
is a stream the source of information my
inputs perhaps some transformation of
information and perhaps some production
of outputs like a flow of information
does that make sense can we keep it
general enough stream some inputs to
some outputs
most commonly what we do is that we
don't just take the inputs and just
output the inputs because we're
typically not in the sort of the file
copy industry but some do so I'm going
to talk a bit about akka streams which
is an implementation of the idea of
streams and what was important when we
started doing our constrains was what is
our streams how is it supposed to work
we want to make it easier to receive
inputs transform data produce outputs
what kind of characteristics do you need
to have in order to make that a good
thing or a pleasurable experience but
also interesting stuff like how do you
operate this how can we make sure that
people don't get into bad situations
buzzword bingo but these are sort of the
core core words behind akka streams so
we wanted to have a representation of
transformation which is immutable
because it means that it can be shared
it can be passed around in your program
it can be reused you can reason about it
we use reusable is good right you don't
have to write the order processing logic
eight times pretty good we don't have to
fix the bugs eight times either so if we
have something that is reusable and
immutable it would be pretty awesome if
we could compose it as well so we can
take different parts and connect them
together like pipes like in plumbing put
things together another thing that we
feel is important and I think you also
feel it's important that we're able to
leverage the computational power that we
have access to so we want to have a
library or a tool that allows us to
exploit having multiple cores for
instance but that means that we are
entering concurrency country which means
that we need to have some sort of core
nation need to make sure that this works
across these threads and once we have
something that can leverage multiple
cores it's fairly important that it's
asynchronous because otherwise if you
start the processing of a stream and
it's synchronous you're going to block
the current thread until that stream
processing is done so imagine for
instance that you are in the business of
taking sensor data and collecting that
and transform it and insert it to some
database because you want to calculate
statistics or clean up data that could
be a potentially unbounded source of
information so your current thread will
be stalled for an unbounded amount of
time so if we can create something it
would be really good if that was
asynchronous so we could reuse the
current thread for something else
possibly another stream and of course
what do we want to focus on but I think
that is transformation because as I said
before just taking bytes in and bytes
out not doing anything with them that is
a very very slim use case so what is the
actual problem here so we want to do
Streamy want to take inputs transform
them produce outputs leverage a lot of
course and potentially even going across
the network and do all kinds of cool
stuff so what is the core problem here I
think the core problem is to get data
across an asynchronous boundary there
have things working at different speeds
potentially in different locations how
do we get data from A to B how do we
make sure that we coordinate that your
sequence is easy we just hand it off
from the stack and it just works but if
we want to be asynchronous we want to
make sure that we solve the problem so
what happens if the receiver can't
receive as fast as the sender since is
there no back pressure here because if
you have a fire hydrant and you want to
drink from it it will be pretty nice if
you could deal with only drinking as
much as you want to drink rather than
drinking from the fire hydrant right and
especially in a fan-out situation where
you have multiple streams going out how
do you deal with a back pressure you
can't do the raccoon method it does not
work so the real challenge is to do this
with back pressure and as we want to be
asynchronous we need to do in a non
blocking way we can't block the current
thread so when we started doing this we
sort of figured that we were definitely
not alone in having this problem so how
many of you have heard about the
reactive streams initiative Wow very
cool that's almost like half of the
audience so we started checking with
others in the same space do you also
have the same problem are you also
working on a solution or are you willing
to work on a solution for this problem
and so we did and it worked and it's
done and it looks like in the last JDK
nine early accesses that is going to be
in JDK nine that's pretty successful I
think set out to solve the problem but
we managed to solve a problem in in in a
good way which means that it's possible
to integrate in the JDK
so if we want to compare different
methods of dealing with back pressure or
not so we have some requirements that we
want to achieve or live up to and we
have push and pull which are the most
common ones so if we want to support
potentially unbounded sequences of
information so you have potentially an
unbounded source of inputs we want to be
able to support that we don't want to
just have bounded once or fine at once
and both push and pull as a as a way of
solving this actually solves that
because in a push situation the sender
just sends the information doesn't
matter how much information there is he
just pushes the information and in a
poll scenario the receiver pulls
information and it doesn't matter if
it's infinite or not or unbounded or not
but then again as we said before hmm we
need to support the case where the
sender might be slower or the receiver
might be slower so send a run separately
from receiver yeah we can have the
sender run there and we can have the the
the receiver run somewhere else it's not
tied together tightly coupled we could
send something we could push something
over and that will pull something over
and over a network so here's what I
talked about before we need to have the
rate of reception may vary from the rate
of sending so we don't want to want to
force force them to always work in
tandem or it only works if they run in
tandem which is true right we can just
if we push we have to drop or if you
can't receive it will just have to drop
and when we pull we just pull what we
want so we'll be able to pull when we
need more information but the problem
becomes if we won't have requirement
this is we only want to drop elements or
drop data
as a choice not by necessity so we don't
want to be forced to discard information
well that doesn't work with a push model
right so the the sender that pushes the
information has no idea of whether the
sender is receiving it right he just
pushes doesn't make a difference it
doesn't know that it's getting dropped
and if we want to have something that is
high speed
well then pool doesn't really work
because if you need to pull every
element out of that stream if you think
about it from a message passing point of
view you'll have to send one pool
message to the sender and the sender
then responds with the data so now I
have two messages for every single
element of data that's quite a bit of
overhead what if we can solve this if we
model it as supply and demand that was
our general thought so we have supply of
data and then the receiver has a demand
for data so it's like going into the
store wanting to buy ice creams perhaps
you say I want 10 ice creams as your
demand of data and then the store clerk
is responsible for making sure that they
deliver 10 9 screams hmm should make
sense right so if we model this in the
as I said before with the first thing
argue with naming I think we spent like
two months argue with naming so it's
called a publisher the thing that sends
information and the thing that receives
information is call a subscriber and the
publisher publishes data and the
subscriber demands data two flows of
information
hmm
so what's the difference between this
and the pull model we're just pulling
information well as I said before we can
demand something more than one we can
say we want ten ice creams we want long
maxvalue ice creams so we can amortize
the fact that we're asking for data so
one interesting observation we did is
that if you have a splitting situation
or a broadcast situation we have
multiple subscribers to the same source
of data you're effectively merging the
demand if I have to send to a and B and
I only have demand from a then can I
send now or how long do I wait until I
send to them because I can't let a
diverge too much from B because then
I'll have to buffer all that data so
it's essentially a a merging of the
demand saying that when I have demand
from a and B I can send information
downstream to both what's interesting is
the inverse as well so if you're merging
two streams of data you're splitting the
demand you're saying that I want one
from you and one from you but I only get
one element out all right
everybody with me so far
anybody lost I can't see you up there
but if you if you shout I will I will
make an effort so the interesting
observation here is that it becomes
effectively a push model sometimes right
if the subscriber wants more information
than is available from the publisher it
means that the publisher can always send
so there is always demand so I can
always send effectively a push model I
can just push when it's the other way
around it becomes a pull model because
the publisher cannot publish unless
there is demand so when the subscriber
more data he pulls quote-unquote the
data out so what's super interesting
about this is this all changes and
switches automatically during runtime
because as the subscriber or publisher
is faster it switches between these
modes so it all balances out over time
and as I said you can batch the demand
and say instead of say I want one I want
one you can say I want ten and then I
want five and then I want a hundred and
batching the demand means that you're
essentially allowing the publisher to
say okay you want a hundred so I'll
prime and create a hundred you're able
to make decisions based on how much
demand is downstream so you can think of
it about it as a sort of a dynamic
push-pull switch is automatically
between push and pull that well the the
the user having to do something so how
does that work what's the protocol what
does it look in code so this is the
entirety of the interfaces in the
specification this is Java this is one
slide pretty awesome keep in mind though
that this is not an end-user application
developer interface or or specification
this is more of an integrator
specification because everything that
implements these interfaces and follows
the specification they will just work
with everything else that implements the
interfaces and follows the specification
so this is there's very little API
because you have multiple different
languages on the JVM you possibly have
multiple other languages they want to
integrate with everybody has their own
flavor of API some style how you do
things everyone has different
capabilities so by not constraining the
specification with API choices you can
distill the specification to the Behrman
and if you only have the bare minimum
it's much more easy to reason about so
what you have is a publisher a
subscriber a subscription and a
processor and the reason we have
processor there is because it's it's
very hard in Java to say publisher and
subscriber as a type so it means that
everybody would create this type because
a processor is something that takes
input does something with it and
produces output seems like a fairly
useful thing so having that as a
standard type make sense and as you also
can see when you request information in
in the subscription request is the
demand so you're saying request a
hundred elements does this look fairly
clear so just to illustrate a bit about
the mechanics the subscriber places
itself into the SUBSCRIBE method of the
publisher the publisher creates a
subscription for that subscriber and
passes that to the unsubscribe method of
the subscriber so that's sort of a
handshake I'm interested in data here
here's how you get data pretty simple so
how does the data flow start well this
was a subscriber has to ask for data
right before the publisher can send data
so it requests one or aches from its
subscription and then the publisher is
able or willing and and allowed to send
an element on the our next method of the
subscriber and of course there is no
relationship between having to request
and having to get deal next so you can
call request multiple times to aggregate
the information
and then the publisher can then send
that information make sense fairly
simple and we can work with some more
information we can send some more
elements so in the city in the case
where we have an unbounded source of
information that just produces the
number one forever there is no natural
end or completion of the stream right
but a lot of streams have some sort of
completion so how do we complete a
quote-unquote street well if we send the
information and once we know where the
publisher knows that the information is
no longer available he can just call the
uncomplete method on the subscriber
super simple but in the case where there
is some sort of catastrophic failure
where we aren't able to send any more
information from the publisher something
happened we're not able to produce
anything we'll have to let the
subscriber know that we didn't complete
successfully it's not end of screen this
is some error that happened something
blows up the publisher just calls on
error passes an exception down there and
then the subscriber knows what happened
super super simple right so if we go
back and we compare these things we can
actually see that we address the push
case because we don't have to drop
information because we request
information and also we offset the cost
of pulling by amortize in the demand
right so we ask for a hundred we send
one demand message for a hundred
elements
so just to give you a sense of the
implementations we have ARCA streams
project reactor rat-pack slick vertex
there are integrations for several both
the databases and i/o libraries much
more on the way and what's interesting
about this is that if you have a
database and it has a reactive streams
interface you can connect your reactive
streams based thing with that and you
get the transitive properties the
backpressure without having to do
anything so let let's talk more
concretely about our streams so one of
the core components in our streams or
sources flows and sinks what is the
source anyone something that produces
data yes very hopefully the name sort of
reflects what it does that was the
intent that very fierce naming this
naming discussion again and a flow is
the equivalent of a processor right so
it's something that takes input and it
has one open thing on the left side and
they produces something one open end on
the other on the other side and then we
have sinks and sinks consumed it produce
output does things with data and when
you connect these pieces together you
can connect sources two sinks directly
you can connect source via flows into
sinks once you have all the open ports
closed or connected it becomes a
runnable graph now once you have a
runnable graph you can out run it
because it now has everything connected
together so let's do a quick live demo
all right Scala code everybody okay
awesome so let's say that we want to
have some source of information we want
to connect it through some sort of
transformation and we want to pipe it
through some sink we want to do
something with it
so let's say we have a source of one mm
sorry oh we can't see it ah that's not
reasonable let's push it can we push it
out there
can you see it now that's amazing
alright so what we want to do let's take
something that is less perhaps complete
can everybody see this are we good
slightly can we zoom in here are we good
now yes awesome so let's say that we
want to do something we want to have a
source of information and in this case
we have a source of the numbers one to a
thousand fairly simple right but this is
a source so this is now immutable and we
can refer to it and we can reuse it so
technically if we want to do
transformation let's say we want to take
all the numbers and times two because
that is a very useful thing to do we
have a flow events and we want to map
that and we want to take the number
times two right so now we have a flow
that takes int and take forever int it
does times two in the end everybody with
me so far
awesome all right we have a
transformation we can refer to this
transformation because it's immutable
what if we want to have a destination so
let's say we want to print these new
awesome numbers we want to produce
outputs so we do for each and we do
print line right so for everything this
is a sink takes things and calls this
print line so sense things to system out
so if we want to make this run we have
sync via transformation to destination
we have connected a source via
transformation to a sink I want to see
if this actually runs you believe me I
should probably don't deal with that
right print it out numbers times 2 we
good so we have these pieces that we can
reuse we can connect together and we can
execute when we want to so technically
what we have here is a runnable graph
and we can run them how many times we
want
and this thing actually returns a
runnable graph so let's just do this for
example so now we have three different
runnable graphs that run right
is this magical or does it seem like
useful things or both both all right so
we have a means of describing
transformation that we can reuse that
allows us to have back pressure you see
nothing here that relates to back
pressure right it's completely
completely hidden behind the scenes all
right let's go back to the all right
one thing that I find extremely useful
how many of you have ever tried to
implement a network protocol yeah fair
enough
how many of you have ever taken things
from a file had to decode it and then
had to do some transformation and
re-encoded put it into another file
right
yeah like CSV do stuff with it produce
another CSV same thing right taking that
thing with the CSV and the decoding and
doing stuff that is a protocol right we
do something with it and we do the
reverse on the other side a
bi-directional flow can be seen as
something which has two directions one
input in one direction and one output in
that direction and one input in another
direction so it's sort of upside down so
you can flow through things through it
in one direction and flow things through
it in the other direction what's
interesting is that since it's one thing
it can short-circuit so think of how
many of you have ever used
TLS in Java how many of you are okay are
you okay
you're okay
TLS is a very typical example of this
because TLS does things like
renegotiations and stuff so being able
to short-circuit between the inputs and
outputs when you need to is fairly
useful what's interesting about
bi-directional flows is that you can
stack them on top of each other like
Legos so you can write one by a
directional flow to do encryption and
decryption and then you can do one
bi-directional flow that does encoding
from domain type to a byte string or a
byte buffer and vice-versa and you can
plug them together to form a new
bi-directional flow that decryption
creates domain objects very powerful way
of putting together different stages in
a protocol so before I said something
about graphs graph seemed fairly useful
right how many of you use get most of
you so use graphs all the time but the
most common form of graph that is used
is an acyclic graph so it doesn't allow
back pointers so it has a directed flow
and you can encode this innaka stream is
fairly very simple but there is this
other kind of graph that I call the the
what graph because oftentimes if you do
have a stream processing solution that
only allows DAGs and you need to have a
feedback loop within your graph and
you're not allowed to you output it
somewhere and then you have something
that reads that output up in the stream
so you sort of hide the back channel
that sort of violates all the
assumptions around your graph so that's
that's not a good thing
so since cyclical graphs have their uses
I think it's better to have an
abstraction that supports both a cyclic
and cyclic graphs
you don't have to do this what thing
where are you trying to debug why it's
suddenly breaking because it's writing
to some file somewhere that it's being
read up for up in the graph and ARCA
streams allow you to compose graphs and
there is an optional setting for
allowing cycles so that allows you to if
it can do some helpful things like you
created a cycle you didn't enable this
this is probably a bug sort of helpful
but in order to do an interesting graph
we need to have both fan ins and fan
outs we need to be able to take multiple
sources of information for instance we
want to be able to read from multiple
files and join those files and produce
something with that or we want to split
the information and send it to multiple
destinations very very common both
broadcasting and routing and and
essentially if you do any composition of
programs as I mentioned very early in
the presentation by calling out to
multiple services and joining the
results of those services very very
common so you need things like merge you
need things like broadcast you need
things like zip sit with and all get all
kinds of very core components so that is
also supported and what's interesting is
that it's now available both in the
alright where did you go where did you
go mr. IntelliJ can you see something
all right so let's say we want to do
something with a graph
so here's two different examples one is
we have a source from one two hundred
and we want to sip that join another
source which has 100 to 199 and what we
specify for that is a function that
takes the first value and plus it or add
it to the second value so we're
essentially adding two streams together
using using add so that takes two
sources and joins them to produce a
single source so from the outside if you
pass this source around you won't really
see that this is what it does as a user
of this so you can really compose very
very complex sources and expose them as
a very simple source but what you can
also do using our our graph DSL if you
want to have full freedom you can wire
things up yourself so if we want to
create a flow like one input and one
output from a graph so we create a graph
we have a builder B so we can wire
things together we add a sip with stage
in our graph and it's behavior is that
if it's given to int it will keep the
second in the right one the left int and
the right int and it keeps the right one
and here we have a source that takes
once every second so it sends an element
every second and the element it sends is
unit or just a placeholder right we
don't care about the value we just care
that we get one value every second we
can take the clock which is a source and
pipe that into one of the inputs of the
sit so it flows from the clock into the
encierro port of the SIP so a sip will
is a y-shaped thing it has two things
to sources it connects to and one output
right so if we've plugged one of the
sources all that remains is one input
and one output that is unconnected and
we can return that as a flow with the
remaining input and the remaining output
and was what gets exposed here is now a
flow so we've hidden the fact that there
is a clock attached to this flow so
technically what we can do here is we
can have our sibling of of two different
sources we can pipe that through a flow
that only runs once every second and
then we print every value that that gets
out there so let's just try that all
right this is funny
right tick-tock-tick
right and all these are reusable you can
all refer to them and compose them let's
not leave that running all right back to
presentation city we actually
so all these fan ins and fan outs are
composable which means that you can do
some pretty fantastic things you can
create very elaborate graphs of
processing connecting multiple merges
and multiple splits and multiple zips
and essentially create your own little
quote unquote streaming processing
framework what's interesting is that I
found that doing i/o is pretty
interesting right receiving inputs from
the outside and producing outputs right
but I found that once you have something
that takes input from the outside and
produces output to the outside and it's
demand driven you can't really produce
anything unless the receiver or the
output is willing to output so it sort
of becomes oh I instead of Iowa because
it's driven by the output so as I said
before materialization or perhaps I
didn't even mention materialization so
when you run a runnable graph we call
that materialization because we take
this immutable description of
transformation and make it come to life
so we materialize it and materialization
has some pretty interesting properties
the materializer itself is pluggable so
you can decide what materializer you
want to use so you could think that in
the future there could be materializes
that do extensive rewriting of the graph
or does some validation of the graph or
even takes the graph and analyzes the
graph and sees ok this part of the graph
I can run on that machine this part of
the graph I can run on that machine over
there right so not only scaling across
course in the same machine but also
scaling across machines so acha streams
really separates the what from that and
when right so you have the what which is
the immutable representation
data Nihao is doing things with the with
the graph DSL or composing things
together and then you decide for
yourself when and where you want to run
this so all of these concerns are
separate so you can think about the
sources and sinks and graphs as a sort
of a blueprint for transformation and
the materializer we ship with is using
akka actors to drive the entire
processing so this is also as I said so
what's interesting about akka actors as
a driver for this is that akka actors
process one message at a time which is
pretty good and it's okay to send
messages from multiple sources but the
actor itself is a single consumer of
these messages and the current overhead
of an actor in akka is about 450 bytes
it's fairly low compared to most strings
so that means that we can run millions
of them on one machine and technically I
think that we can run about two and a
half thousand nodes using our cluster so
till one half thousand oh it's times
millions of actors per gigabyte of RAM
means that there is a lot of potential
scalability if you had millions of
course this could take advantage of that
I didn't really show you this but every
single runnable graph has a result we
call that the materialized value for
instance if you want to have something
that connects to a TCP socket you might
want to return what port it got assigned
for instance or if you write to a file
you might get a future out with the
number of bytes that were written things
like that and that value is returned
when you materialize so that that is
typically a future so Illustrated if you
have a graph and this is a cyclic graph
right because f
F's output connects back to sees input
does this make sense
this is like a flow of Lewis chart
everybody with me have different stages
we plug them together here is the code
for that in the graph DSL so if you look
at the bottom part right if you look at
that part doesn't that look very
surprisingly similar to this one if
you've ever used a graph processing
thing before and have to wire things up
it's almost impossible to figure out
what it does just by looking at the code
so we've been working quite hard and
trying to find ways of making that kind
of code more maintainable so I will
actually
all right we have 11 minutes we should
be good for this are you guys ready for
some more life coding or are is this too
early or before lunch what you ready all
right let's do it
all right let me go back here we do
something that will require a bit of
explaining so let's say that we want to
do something with files because that is
very very common let's say that we have
a file called words anybody know about
words words it's good file like that
file
it's delimited using new lines right so
new lines is good so we want to parse
words and do something with it so we
want to parse files so we defined the
delimiter for the file and I'm going to
read this file out of my own resources
because I like having things checked in
so the reason why this is a synchronous
file source is because have you ever
tried to do a synchronous file IO in
Java no yes no using nettie so in the
JDK doing this is pretty hard so this
uses IO Java IO
so this is sort of in-your-face
synchronous but we want to read a file
so we have a file source and that file
source is in my Resources directory of
the project and it's named words pretty
simple right so at this point we have a
synchronous file source of this file but
this is all bytes at this point because
we don't the program doesn't know
anything about the structure of words so
what we need to do is we need to pipe
these bytes that we read off the file
and do some parsing
so what framing does is that it has a
method called delimiter which takes
bytes input and does parsing make sense
right so we give it
what kind of delimiter it's going to
look for we're saying that the maximum
frame in this thing the maximum thing to
parse is int max value now I know the
English language and there's probably
not that big words but just just to be
sure I could pick the German probably
actually that generates some more
variance in the word length but I know
also that the last word
in words does not have a delimiter after
it so we want to allow truncation so if
the file ends early before it finds that
the limiter we want to include the last
word as well right and once we have
chunked up the words file into actual
words we still only have bytes so what
we want to do is we want to make sure
that we take these bytes and create
strings out of them and what's good is
that the byte representation that we're
using here has a utf-8 string method
that creates a string using YouTube
utf-8 out of those bytes so at this
point when we have the source here we
have parsed and we have words we have a
stream of words you feel the power we
have a stream of words but let's say
that we just wanted to do the reverse on
the other side and just output the words
file into a new file so we have a strip
we have a source of strings now words
let's just skip this part for two
seconds or we can actually go here so
when we have strings and we want to
output the exact same thing that we read
in now we want to create byte strings or
byte buffers
from these strings and what's good about
that is we take strings in we create a
byte string and we want to attach other
limiter to each of them so now we have a
bytes representation of the string with
a new line after it and we can then pipe
that into the equivalent destination
right so we want to create a result dot
text in the same directory that includes
these things does that make sense
we've just read stuff out we have
amazing strings and now we want to get
rid of them and generate a new words
file but it's pretty boring to have this
as a demo because it's essentially a
file copy operation right we're in the
file copy industry right now right it's
not that fun demo just file copying so
let's say that we want to do something
we want to output some sort of progress
I want to be able to demo we don't want
to tail the file and just so wow it's
things in the file so let's say that we
want to attach a reporter to this
transformation we want to output results
here while the thing is running so we
want to create some sort of progress
meter thing so we create a flow of these
strings of words none of these word
strings and we call an operation called
conflate and what conflict does is that
it detaches the backpressure
so the the side can progress
independently from each other so what it
will do is if there is no demand from
downstream it will just keep doing a
transformation to sort of create an
intermediate result so if we are not
able to output things in our progress
bar we want to count things so what we
do here essentially is we count the
words as they are processed but it's
completely detached to what we want to
do with the
does that make sense we just allow it to
do stuff while we're waiting to run our
progress bar we want to do a summary and
the progress bar wouldn't be that fun if
it just outputted the information about
the current progress throughout the
program because it would just flood the
console so we want to just have it run
every 25 millisecond so we don't do like
every nanosecond or something
so we sip the summary of the number of
words processed with the clock and we
discard the clock tick we're not
interested in the clock tick we're only
interested in the left part which is the
count so now we have a stream of the
current count of words that we have
processed we would actually that's not
what we have what we have is we have a
stream of updates or what was processed
since we last checked but that's not
really helpful because we need to keep
track of the current progress not only
what happened since the last time we
checked because that would just like 1%
3% 1% like that would just be confusing
so what we want to do is we want to
aggregate the progress so that it it is
increasing so what scan does is it takes
a zero value which is the start value
and we start out with the long zero and
for every update of the number of the
count of words that were processed since
we checked last time we aggregate that
to that count so now we keep track of
the current count that we have processed
are you with me still no yes no yes all
right so now we have a stream of the
current number of words we have
processed and what we want to do with
that is we want to just print that out
to system out as lines read and the
count
right so we have a source and now we do
also to because this is a sink right
we're putting this into for each so it
just has one open end and no output so
it's a sink so we want to do source also
to progress and then to destination
so it's essentially sending information
in two directions so what this should do
is this a small file and this is a fast
thing so as you can see it prints out
the number of the count of words that
were processed before the thing exit and
the result was this number of bytes were
written to the file and if we go out
into into this exit presentation we do
we have words in the file
all right if you want to try this out
there is a wanna go out but we also have
a milestone one of Toronto which has
some pretty pretty cool improvements if
you want to know more about reactive
streams for the jvm this is the link and
thank you for showing up and your
support</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>