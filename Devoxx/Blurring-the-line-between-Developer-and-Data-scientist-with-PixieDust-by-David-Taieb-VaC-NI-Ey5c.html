<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Blurring the line between Developer and Data scientist with PixieDust by David Taieb | Coder Coacher - Coaching Coders</title><meta content="Blurring the line between Developer and Data scientist with PixieDust by David Taieb - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Blurring the line between Developer and Data scientist with PixieDust by David Taieb</b></h2><h5 class="post__date">2017-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VaC-NI-Ey5c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I know it's early today Tuesday morning
everybody is trying to settle in the
conference but we're going to try to
have a great great presentation here so
hello everyone my name is Dale eat i am
i'm in the st sm at the Watson data
platform developer advocacy so long
title but basically my role is to talk
to developers because i'm a developer
and listen to them try to understand
their problem the plane point give them
some ideas and content that we can use
into their project so i want to start
with a little quick of show of hands who
is a data scientist here just one and
who is a developer all right so I'm in
the right place so basically I'm going
to make a set of claims and hopefully
understand why you are here today let's
start with a set of facts like same as a
climate change if people deny they live
in another planet basically the first
claim is that more and more companies
are making better business data driven
decision about their customer it's quick
critical for them it's survival this
life of that they have to make
data-driven decision the end the good
news is they are doing they are drowning
in data they collect a lot of data every
day but the bad news is also they are
drowning in data so about ninety percent
of the data that they collect whether
it's unstructured or structured is not
leveraged they don't derive the
necessary insight to make the right
decision and we have to recognize that
solving the data problems of tomorrow is
not going to be done by scientists to
low by data scientists alone this is
where developers which means you have to
come in there is many reason why you
have to get involved into data science
first of all data scientists are really
really rare breed and the developer have
to get on the bandwagon but also the
data scientist in his ivory tower has
come down this hole
sepal the data scientist as a as a
someone in his castle has come down and
it has shifted from the elite few to the
empowered many so developer which when
you must get more and more involved with
data science they must move from the
traditional stuff I application
standalone application with three tiers
data middleware into more they are
pipeline oriented where data and
analytics are integrated are fused into
the application and that's what we're
going to talk about today so just a
quote of a Stephen which as i like i
like a lot which is the developer of the
new king maker companies to be
successful they have to have a very
strategic role for the developer they
have to develop a strategic that's what
IBM and other companies are doing
nowadays with setting up a very large
and functional developer advocacy to
talk to developer to understand their
need when they build services platforms
they have to talk to developer
understand what what their needs are and
what the pain points are and make the
platform that we are building the
services that we are building simple and
accessible for everyone and that's a
really important point that I'm going to
talk about today that the whole thing is
that friction is in the enemy of
innovations and developer who are
struggling to get and consume api's or
services will not do their job
adequately and it will go somewhere else
they have the choice because of the
explosion of open source and other
components people have now the choice to
go and choose other platforms so
therefore we need it's imperative to
understand what the developer go through
when they use a particular API or cat
form so this brings me to the to the
actual topic of this presentation which
how do we blur the line between
developers and data scientists they have
to collaborate but right now there is a
big silo big boundary between them they
use different tools they have different
techniques they have different
backgrounds and and this is a big
problem today
because communication doesn't flow very
naturally so what I want to do is I want
to start with a story that we all know
too well and we're going to see what I'm
talking about and when you see the story
um you will you will have a disclaimer
which is all the characters and events
that I'm they're picking in the story
are entirely fictitious but any
similarities to real life is actually
intentional so let's meet then the
developer then hold the master degree in
computer science a young guy you know we
like likes having fun with his friend
have a beer once in a while is a full
spec web developer it has many languages
of choice that you will recognize
yourself in Java nodejs html5 css3 he
likes working with data no sequel sequel
doesn't matter he's interesting to the
protocol of the web rest Jason and so on
but it has no major experience in big
data or data science for that matter his
motto is a lazy guy like you know we
should be the best line of code is the
one that I didn't have to write so
that's been let's meet Natasha which is
the data scientist she holds a PhD in
data science she has five years
experience boss with the two years plus
with the company she's very experienced
in the data statistics machine learning
she she she likes Python and our but
software engineering is not a thing so
she's interesting to the data itself
she's not so much interesting to the nth
Caucasian building the data pipeline
around it her motto In God We Trust all
others bring data so she really data
focused and then oriented so Monday
morning they all work at the same
company and they have a surprise meeting
with the vp now we all know that when
you have a surprise me with a vivir it
cannot be a promotion hey it has to be
something different and yes they have an
urgent need from the demarc from the
marketing department to build an
application that can provide real-time
sentiment analysis on Twitter data
because they are starting a new campaign
for a new product launch whatever
and they need to understand what people
are saying in real time here's a few key
constraints that those two people do to
a degree you will have they only have
six weeks to develop it the target
consumer is the line of business users
it has to be easy to use even from not
for non-technical people so you see
already the conflict with the day the
big data gory details and make it it is
ease of use it has to be accessible
through the web and you must scale out
of the box you have to be able to
analyze thousands and thousands of
tweets in real time wow that's a big
order for two for those two guys so they
start then of course start asking it is
going to work with with Natasha it
doesn't know a lot about data science
what is data science exactly so data
science is the intersection of three
rings bubble I have this Venn diagram
from from the website it's the
intersection of a rigorous data science
math and statistics with found it into
really mathematical rules hacking and
expertise so data science is a science
but is also a black art you cannot learn
it in two days right a data science is
using a set of tools from mathematical
and and so on but it's also a feeling
walking with data next time we need to
acquire this experience it's almost a
black art and being able to derive
inside from structured and unstructured
data especially when the data is huge
what the data is not clean you may have
some outliers it's a really really
really difficult problem and we all know
that and that's what Matt natasha is
going to be critical in this particular
endeavor but at the same time Ben is
going to be the one who's going to build
the data pipeline the infrastructure
connect to Twitter and so on and the two
of them have to come together they have
to creating subfertile and all smooth to
work together and then do some data
science and Natasha being able to call
API is very quickly
so Natasha asks i don't know about
apache spark and apache spark basically
has been suggested by the vp as the next
big thing so let's take a quick look at
what apache spark is apache spark is
nothing more than a simplified
programming model to help you do
parallel computing without the gory
details it's very simple it's not simple
in the sense that you know you can you
can do it without some experience in
programming but it's simple in the sense
that it abstract out all the necessary
management of the cluster of node
talking together you basically write a
set of code without thinking that this
code is going to be cut into pieces and
then distribute it across a cluster of
nodes and then all those nodes are going
to synchronize themselves run together
in parallel and then combine themselves
into into a set of results it's one of
the fastest growing it is the fastest
growing Apache project today let's talk
a little bit in more details what it was
in it it's composed of four component
you have spark sequel which give you a
sequel engine a sequel layer on top of
big data which is pretty cool so you can
if you know sequel you won't be
disappointed here you can use your
sequel skills and you can build you can
get your data from unstructured
structured and then move them together
into a coherent layer where you can
query the data using sequel it has pop
streaming which gives you access to
streaming data data constantly streaming
in in real time and we're going to use
that into the our application remember
we want to be able to connect to Twitter
and in real time get some some seed of
tweets and analyze them it has a very
comprehensive and powerful machine
learning library so set of libraries
that allow you to abstract out on a
bunch of all the supervised and also
provides training create models very
quickly just by following the same
programming model that you would use
using regular data which is really cool
I've used it I but
blogs there you can refer that I can
refer that to you after the this
presentation where it's really easy to
build a bunch of classification models
on a large set of data and it has also a
graph component that allow you to do
grass parallel computing which is
extremely extremely cool like I said
spark has a very big traction and those
are numbers a little bit outdated but
it's keep going and trending where the
number of commits on github for this
project is growing exponentially it's a
very large community just to give you an
idea the sparks are made that they hold
every year has been going so much that
this year they are moving to the Moscone
Center from from the hilton so because
those it's growing so fast but what's
interesting for us is how do i conceal
spark right so you do have to have some
DevOps in there setting up the nodes
setting up the driver and in how as a
developer how do I consume that so you
have two ways of consuming spark as a
badge spark submit application where you
create your application and you submit
it as a bad job you wait for the results
come back and analyze them but you also
have an the interactive counterpart
which allow you to to interactively
connect with spark and start coding to
what we call a notebook and we're going
to see what that is constantly having
feedback of what what's happening and
adjusting iteratively so this is this is
very compelling for a Natasha and Ben
they want to go fast and they want to go
interactive they want to sail fast this
is this is perfect let's look at their
their brainstorming a little bit what
does Ben wants to do it needs to work on
the data acquisition from Twitter and
enrichment is going to use a tool that
we're going to see a service to try to
analyze the tones and the emotions into
the each tweet he knows java very well
but um we think he doesn't have time to
learn fighter oh
poynton and is actually is actually
willing to learn scallop which is a
language that is close to to Java let's
look at Natasha she was going to perform
the data exploration and analysis she's
very good at that she knows Python and
are she's not familiar with java overall
scour she likes pandas numpy and she's
looking to learn spark a PI spark to be
precise because she expect the same
level of a PRS and she wants to work
iteratively with the data however Ben
will need to do some data exploration to
Natasha will need access to the API
those are constrained how they they
going to merge them and they're going to
start collaborating together on the
project so we talked about notebook as a
way to consume spark but what is it
exactly anybody knows what notebooks are
just as a show of hand all right not a
lot that's great is this no it's not the
notebook that you see on TV it is also
not the movie a notebook is basically a
web interface it's a document-oriented
interface that allows you to to express
yourself in code it has what they call
it's a linear like a table that has
cells each cells can hold either
markdown look for expressing some drugs
some descriptions and also code that you
can run directly by running the cells it
will go and communicate with the with
the spark cluster run the cells that
come back with the results right away
within the notebook so each cells has an
input and an output in the input you
write the code you run the cells and
then come back and you see the result
the results can be text can be charts
can be anything you want so it's a web
as UI for running apache spark console
command this it's easy there is no
install required once you've your
notebook created you can create as many
notebooks pages as you want and it's the
best way that I know of
start working with spark and big data so
we're going to talk about that in in a
minute but first there's multiple
notebook out there and the one I'm going
to talk about precisely today is Jupiter
so by far the most popular notebook that
you have is all the notebooks are there
a Zeppelin and and so on but Jupiter is
an open source notebook for interactive
data science it supports all them the
main language python java scala node
even have high school and many many
languages and it stays over 2 million
users for ipython today there's multiple
way to host the books you can do it
locally today if you want it now you can
after the call we I can tell you in a
about 30 minutes you can install
notebook and spot on your laptop and
start working with with sample data
right away but also on the cloud there's
multiple provider IBM is one of them
with IBM data science experience that
you can go and without having the hassle
of installing a spot cluster you can go
and start running and create example
right away in stand and get started so
how does it work with big data analysis
basically you have your notebook your
web interface it tough for Colonel right
here in the middle with apache spark
support the kernel itself talks to the
cluster so there's a bunch of node
allocated for your jobs and then you
have services that you can wrong things
like statistics mad machine learning
plotting cognitive api is anything that
you need to do your job ok and then the
data itself you can bring it from any
sources you want it can be amazon s3 it
can be local file system it can be rest
api it can be anything you want it's
really really open to the point where
you did i can live anywhere you want as
long as it's accessible to some form of
API you can bring it together bringing
to a spa cluster merge it and then clean
it and then do some ETL into it is
really really powerful that way alright
the books are powerful for data
scientists
but Ben is still uncomfortable with it
they seem complicated because everything
is expressed in code and he looked a
little bit at it and just to create a
little chart took a long time of the lot
of code for him so there is a very
popular library in Python called
matplotlib I don't know if people know
that matlock lib it's great very
powerful does a lot of cool stuff but
just to create a simple pie chart or bar
chart it takes like 20 lines of code and
most importantly it takes about 10
minutes even if you've done it before it
takes about 10 to 20 minutes to go and
look it up to write it again so if all I
want to do these two basic exploration
of my data I cannot keep writing these
things Ben doesn't feel that something
you can do and that's where pixie does
come in like pixie dust is an open
source library that was started by my
team and has grown tremendously that's a
big adoption into the community of
Jupiter notebooks it basically
simplified right now the main goal is to
simplify basic data exploration the idea
is that if you want to create a chart a
basic chart you should not have to write
any code it's already complicated enough
to do your job if you want to do some
basic exploration you should not have to
write any code so let's do some demo
what so we have here a very simple demo
that i'm going to show you where i'm
cleaning a data frame data frame by the
way is a one of the data structuring
spark it's a it's a abstraction of
holding a very large amount of data in
this sample i create a very simple data
set but imagine this with the trillion
records it's exactly the same it goes
into the closer span span fan out all
the task and come back the ideal is that
if i will explore the data that i just
got from twitter from from facebook from
my system of Records own the company or
whatever I should not have to write a
lot of code and the idea to have one API
call this plane you could you do you
call display on an object and I should
be able to do things and that's what I'm
going to show you right now so I should
be able to interactively have the system
understand the schema of my data and
offer me an entire graphical interface
that allow me like Excel to go and start
plotting things but with more power
because I can do it on the trillions of
Records okay so let me go back here this
is a I said before we have two ways of
doing notebooks local or roasted i'm
going to show you IBM data science
experience which is the IBM ID for
notebooks and spark basically you sign
up there's a 30-day trial I think
there's a free free tier where you can
work for free and indefinitely but you
get only two nodes for the cost or not
not a big big one and then you don't
need to know what's happening in behind
the scene it will start the spark
service for you a spark instance with 2
to node and you'll be able to do some
data science it's organized into project
and and I said also by the logo I didn't
lie i'm going to show you after that we
have a script that allow you to install
locally spark skala notebooks Jupiter on
your note on your local laptop in less
than five minutes it's all automated so
the way it works is we have projects you
can you can you can organize your
notebook into project and you can add
collaborator you can collaborate with
others so if your developer
collaborating with the data scientist
you can all collaborate on to a set of
project so you see I have Michael
ability like a bunch of project here and
so the box I prepared a very simple
thing so here I have two notebooks that
I started to to create last night when I
open it it opens in edit mode I can see
a lot of information about my notebook
and I click on this little edit and that
what happened he has reopened the
notebook but this time connected to a
live cluster okay so now it studies
already started my
spark I have a spark cluster waiting for
me waiting for my command here you have
your cells each new rectangle is a cell
I can run the cell here using this
button so i can even insert a cell this
is all standard Jupiter notebook I can
make the cellmark down and I can have
for instance starting my city's markdown
notebook and when i run it i get some
cool mark down here i can do HTML I can
do everything I want so the first thing
you do is your import pixie dust this is
the library that you can install from
from pi PI when you're going to start it
it's imported and you tomm then like i
said i'm going to use Spock API a very
simple example of creating a very basic
set of financial data categories number
of customers I can see here I can give
my schema the year the category in the
number of unique customers just to show
you so i go i created and you see when i
when i run it this little cell turned
into a star meaning it's running but
it's so fast that you can you did even
seal it and then now i have a valuable
for DF which is my dear friend so now my
valuable data frame GF is holding all my
data and not what i want is is is
visualize it so i'm going to do this
play DF i run it and here you can see it
creates creating a chrome forming
analyzing the data and showing me a
bunch of menus options this the set of
options is letting the data right now
let's hope that I'm not succumbing to
the live demo thing
I shouldn't take that long let me let me
do one one thing and sometimes it
happened I was just rehearsing before
and I've put in two bites Ted I'm going
to stop the colonel quickly I'm stopped
I'm going to open the reopen that that
that's notebook so now I've stopped the
colonel so it's creating a new kernel
you can see here scouting the colonel
it's creating a fresh a spark instance
it remembers what I so he remembers my
life type so the good thing about new
boobies I can save them and share them
with other people so I can go and say I
bought pixie dust again reham recreate
my data frame a simple data frame and
then once that done so you see the star
so it's a fresh colonel it's reassuring
everything so it takes a little bit
longer the first time I'm done I can go
and do this play again and this time
hopefully I can see my data yes so here
I have the first view which is a preview
of my data so which is always a good
thing to do I can see the schema right
here it's all interactive this is by the
way not part of tripodal proper this is
big cities who's doing it then i can say
i want to organize my data into a child
so i can go to bar chart it would give
me a dialogue where i can i can see that
the different column that i have and i
said okay i want to see I want to put a
year on the on the x-axis I want to put
the unique customers and I want to do an
average okay when I say okay all of this
if you wanted to do it in matlab it
would be like 20 or 30 a line of code
here i have a very simple easy way of
visualizing my data i can cluster it by
category and have a better view by year
on what's happening and I've been able
to do a lot of
this data exploration interactively
without me writing one line of code so
that's the first ball right you have
data you don't know what it is oh you
want to explore it you want to see it
get a feel for it pixie dust you can do
that very quickly provide you with other
type of of charting it even had multiple
renderer so if people are familiar with
my plot lib this mud floated here but if
people are familiar with bouquet which
is another very popular rendering
anybody knows my bouquet so the Canes a
very very popular chart engine because
as opposed to my portrayed matata
bistatic it gives you an image with
bouquet it gives you a d3 rendering so I
can see the same thing with bouquet but
now I can go and zoom zoom in when you
have lots of data I can go and zoom
within within my my data I can save it
locally as an image or as SVG and so on
and so forth I can go and change the
type of cooping I can be stacked and you
see how fast it is now but because pixie
does has cast a lot of things I can do a
subplot to see different ways of saying
of all my data by year and so on and so
forth so all of this we've provided with
six basic charting but you also provide
an extensible API that allow you to
write your own so if i wanted to go a
little bit further i can go and say
pixie dust has something called sample
data which allow you to get some more
meaningful data very quickly and here
for instance i can say let's let's look
at the home the million dollar home in
New England so i can say pixie dust
sample data of number six and look what
it's going to do here is going to go and
download the file the csv files create
an actual data frame for you and i can
simply say display of
home and get the exact same thing this
time on my home and just to show you a
different type of charting we also have
mapping with with partner with matte box
which is a which is a a company who's
doing a very interesting mapping
technology I can go and say I have a
longitude I can drag the longitude I can
drag the latitude and I can drag a bunch
of other data on the value price and
maybe number of beds I can give my token
and what I do that it will basically
automatically create the necessary
artifacts to show you a very nice
mapping rendering of you of your data
and here you can see I can see d the
actual data points for each of those in
a very cool mapping environment we have
also other options to give you like a
density map which is also very popular
to understand like a hit map where
whether the data is so just to give you
an example here of how we were able to
very quickly encapsulate a very powerful
set of technologies into a notebook and
bring that as a conception to the
developer the developer doesn't have
doesn't know doesn't need to know how
Mudbox work or matlock live work or
bouquet work the developer can go and
focus on its own needs so let's go back
to the presentation so I've shown you
about a bunch of a simple example but
that doesn't really that doesn't really
answer our problem of the shore so I
showed you to do mapping of course we
talked about extensibility can can we
write our own yes there is a very simple
api's that's using that boy you can
inject your your HTML and we give you
it's a templating engine called ginger
jeje as a Python templating engine where
it's very easy to learn it's like jsps
and you can you can follow the actual
infrastructure to write your own engine
so here for instance i created a very
simple my own table to display data and
just a quick classes here and there and
then you get a new menu directly added
into the pixie dust chrome ok you can
encapsulate that into your own python
library put it on 2 pi PI and then have
your users just basically use it as they
would use any other library I'm okay to
use Python but I'm really more
comfortable with scour so how do I solve
that trouble right Natasha and I need to
work together natasha is comfortable in
Python Ben is contributing scholar of
course notebooks support scholar but
it's a different note book instance so
we don't want to have been working on
its own scholar instance and Natasha are
working on their own Python instance and
then somehow meet in the middle well the
good thing is fix it us works with
scholar as well you can run your python
library into scholar notebook and that's
one of the big big advantage of pixie
dust is that a python comes with a very
reasonable system of of libraries to
chart and if you are only wanting to
learn to work with scalar you can you
can do exactly the same thing as for
example here of a scalar notebook
invoking pixie dust in the same library
here i'm using another round oracle
seaborne which is more focused on data
science itself with line of regressions
and give some correlation and so on but
you exactly do the same thing okay so
let's go back to our problem here what
is our architecture or architecture is
very simple i have twitter it's a bunch
of API i bring those tweets into spark
using spark streaming I call Watson tone
analyzer which is one of the Watson
cognitive services on bluemix I can send
those dare I say P I so sweet one by one
and remember it's start so it's going to
be clustered
it's going to be really fast send them
to Watson get back a bunch of score and
reach the data that i got from from
twitter and then have Natacha due to do
the data science very cool so we're
going to divide the task then is going
to go is comfortable in Scala that's
fine is when a walk in skala is going to
implement the spark streaming connector
to twitter is going to call wasn't too
analyzer for each streets and it's going
to return a data frame a sparkly
referring to Natasha who can then do the
data science natasha is going to use
pixie dust package manager which is
another feature of pixie dust to import
the jar that Ben has written install it
and then use the display API of pixie
dust to start doing her her
visualization a quick word on point to
an analyzer tool analyzer is one of the
27 plus cognitive services available on
bluemix it's completely rest pains you
can go and try for yourself today you
basically send some text into jpeg
format it will return you with a very
rich set of course including anger
conferences nests happiness and and try
to analyze the tone of the text that you
sent try to date it's available you can
connect to bluemix and it's free free
trial you can you can try it for
yourself so let's look at the
application itself I mean this is the
best part that I to talk to people so
remember what happened I'm going to go
back here I'm going to go back to my to
my project I'm going to kill the colonel
just to be sure good last time I had a
little problem i'm going to start fresh
for you guys i'm going to go and edit
this twitter sentiment now remember what
happened then wrote a jar that connects
to to twitter using sparks trimming for
each tweet send them to Watson tone
analyzer come back and and bring back a
data frame now this is in Java
scala sugiyama natasha is in Titan she
needs to be able to call out the code
from bed which is not written in title
and that pixie dust has another feature
called the scallop bridge allow people
from Titan to invoke any scholar code
that they want ok so here at first and
fourth pixie dust like I told you like
like the beginning of everything and
then i used the package manager right
here to install the jar into my Titan
environment I need some financials so
don't take a picture those are real
credentials so I've got to put those
credentials and this is important into
Python valuables you see I get my
username password secret whatever
although the credential for Twitter and
all the credentials for Watson tone
analyzer into a title invaluable but
then I need to invoke bands code and
this is where another feature of pixie
dust is that it is able from scholar to
access valuable that I would define in
Titan and that's what I do here you see
here Twitter consumer key i'm using it
right there into scala as is that's part
of the magic so i'm going to run it and
what it does here is it it runs the code
run the sparks trimmer i said to run it
for 30 seconds only because we don't
have a lot of time but in a real
scenario you will let it run for a
longer period of time so here is going
to start running and you're going to
start seeing here at the bottom the
result of the string is connecting to
Twitter it is going to start collecting
tweets each tweets here is sent to what
some tone analyzer and augmented so I
get the text the author the dates from
Twitter and then when you analyze the
tweets you're going to see all the
emotion score for each of the tweets so
let's let it run a little bit so you
seek it collected common 67 records and
and so on for 30 seconds and
the end i'm going to show you that I can
do some data science on that tweet of
that data frame analyzing everything
that has been done so it's done I can
run another piece of code to create my
data frame and you see another feature
of pixie dust I showed you how I could
use Python variables into scholar I can
do the opposite as well I can define a
variable in skala and using in Python
which is really powerful so here I
created you see a new data frame I call
it underscore underscore yep it has to
be called with underscore underscore to
tell pixie does that it's available you
want to transfer to buy them and you can
see the schema right here that contains
other at the top the field given by
Twitter and at the bottom all the
emotion enriched by Watson and here I'm
back into vital now i can simply use
display on the tweets to show the
results of which can work
I don't know why it's not working come
on if there's not some time
it's relevant just to show that it's
it's real code all right when this
happened what I do is I will start the
colonel I won't go I won't make you have
to go through that again but one thing
that I wanted to show you so I'm going
to do that aside the colonel there
we're doing this we're doing that see I
can run those those cells they will be
queued up and in and there will be added
so let's go back for now while it's
doing this let's go back to the
presentation I'll show you the demo and
we are back we are back to the to the to
the VP so they are very happy they got
what he wanted we can connect to Twitter
we can analyze what's going on we can
get we can coalesce group the tweets by
hashtags and understand has an
aggregation of all the emotion that
people are talking about this particular
hashtag does it is very happy they did
at a very very short amount of time but
there is still a problem they said as a
key constraint that they wanted this
particular application to be usable by
line of business user who are not
thanked ago so that's a problem right do
youdo you cannot expect people to see
all these this mumbo jumbo code so what
can we do we pick sit us there so pixels
at yet another feature which call the
embedded app you can build and embed it
up directly into pixie dust and you can
encapsulate all the things that you've
seen into into a nice application so
let's see it in action I can simply have
people put their credentials like this
and then colors simple Twitter demo this
is an extension to fix it out that I
wrote and have the entire thing that you
saw in to decompose into code into a
nice UI that Lana
as user can see and and apply there
directly in real time and if all of this
is written by using the API that I
showed you he composed into the notebook
I put them together into a very
user-friendly interface that people can
invoke with one call so let's try to see
if I can make it work so I'm back here
okay there's some errors but let's let's
go back directly into here so i'm back
here i'm going to show you the actual
versions of the spark streaming working
into a notebook so first of course I
need to put my credentials and then
simply call Twitter demo and that's
going to show me I'm going to make it a
little bit bigger this user interface
that looks like a dashboard that line of
business user can be comfortable with so
you see what we've done here we went
from notebook and a powerful tool for
data scientist notebook as a tool that
developer can be comfortable using and
we even push the envelope to try to see
a maybe line of business users who have
no idea what data science are not
technical that could be comfortable
using as well so in this particular case
I have this interface I can I can
provide some keywords like let's say my
favorite is Trump of course is the most
prolific tweets and let's say Russia
that's something that they talk about if
I click on start streaming is invoking
the sparks trimmer that I showed you
before but you know to the user
interface and you can see here is
starting to get records and it's
starting to analyze them send them to
Watson and in a second you're going to
see multiple things happening on the
left you're going to see the tweaks that
match trunk and Russia you can hold over
and get
actual the composition of emotion for
each tweet so you can see this tweet for
instance has 75% extra versions and
Joy's 52-percent sadness each one is
twenty percent and in the middle it will
show you a real time glass of all the
hashtags that people are most talking
about and it's analyzing in real time
and giving you a very nice view of what
the hashtag that people are so if you
are the chief marketing officer of the
company and you want to know what people
are talking about when you're launching
a new product and you want to do it in
real time you configure your bill
Twitter sentiment analysis with some
keywords and you run it you will in real
time will start telling you a lot of
information about what's happening and
remember what I said at the beginning
data-driven decision people have less
and less time to make better business
decision this type of tools who sits on
your sample but this type of tools can
be deployed very quickly and give the
tools to to the executives of the
company to start making decision in real
time that are data-driven but I cannot I
don't have to stop there what I showed
you here is an example there's API that
you can build your own application if
you want to but in this particular case
i can stop trimming and when i stop
streaming and I go back to notebook
guess what I get access to a data frame
and then I can give that data frame to
my data scientist to do some further
data science this is not only
application driven where the data is
lost the data is captured into a spot
cluster and then when I go back to my
netbook you will see that it says here a
data frame with the tweet has been
created and I can bring display on the
creature fulfill its gonna work this
time I ain't working and the tweet that
videos created
there they are I can go and start doing
some some things with it I can't even do
some distribution of things very good
into a pie charts very quickly this is
my pie charts okay so let's go back to
the presentation I think we have a few
minutes left so I show you the whole
thing it's okay success neither VP is
happy and bending attached idea to keep
their job conclusion notebook we think
have the potential to help break the
traditional silo between data science
and developer it's at the intersection
of the team's the tools and the assets
and tools and effort like pixie dust
allow you to bridge the gaps you can
learn more you can try it for yourself
like I said on IBM data science we have
a blog post that that teach you start
step by step you can go down there we
have a lab Adam Cox here which is here
is going to run a lab on pixie dust that
allow you to get a feel for yourself or
you can do it locally as well we have an
automated installer and all the
information is in there those are the
resources I have a list of URL that you
can go to learn more about notebooks
spark data science spuyten pixie dust
and so on one word about other things we
have other programs around spark and big
data we have a a catan going soon around
the SETI the search for extraterrestrial
life and spark and Adam here is the
person to go talk to you if you are
interested to participate it's you can
register not going to be in June 10 and
11 in San Francisco and finally if you
if you want you can also text pixie dust
to this number here to enter the drawing
and you're going to get a t-shirt and
you'll get also access to a blog post
that explain a lot about about pixie
dust
in more detail you can go into the
documentation and so on so I have a few
minutes now for questions and I'm going
to bring back this particular one in
case you haven't added any questions
before we call this talk was that too
technical was that okay yeah it was good
okay yes yes we have that in the in the
in the blog post okay it's right here
yes so basically to write you own
display you need two classes you need to
you need to inherit from a class called
display when you create you close your
own class and you will net you inherit
from display and then inside the display
you have one method called do render do
render all in all it needs is a HTML
fragment that's going to go inside the
output of the outlook sell the HTML
fragment you can use direct HTML if you
want or you can use ginger templating
which is much better much easier to do
more complicated things all of this is
explained very well into the
documentation but here you see an
example of a simple table and I use the
ginger code programming language to
start picking apart each and every row
of the entity and start displaying it as
a for loop here for role in entity and
then I start displaying it one by one
this little notations are probably brace
twice it's a way to inject Python code
within HTML and generate the HTML
automatically of the second class is
just you describing where you want you
men you to go so here you return a
little chase an entity that says I want
this new menu to go into the table has a
title of new sample and icon of sh
nie that's it with this with a little
decorator on the top pixie dust is going
to detect that and then add it inject
into the menu and you will see this when
you I showed you the chrome the first
menu was alone right now I see that
there's to the data frame table which is
the standard Exodus one and the one that
you just created it will automatically
and allow make it available people can
click on it and then run your code right
there yes absolutely pixie dust is by
the way open source so if you go to
github com ID mcds lab pixie dust you
will see into the display directory you
have a chart renderer and you have all
the renderers right there and we talked
about my old not mad dogs man clock lib
and here i have my bar chart display
this one is a bit more complicated but
you can study it you can ask the
question if you want and here you see
I'm using nothing more that the standard
matplotlib it's all in there I have used
some kind of manipulations and so on but
there's no magic in there I'm just using
visit us is not itself a rendering
engine it's just dedicating to your
favorite rendering engine and
encapsulating it so that it's really
easy to consume and your the question I
think we are out of time so thank you
very much I hope this was valuable to
you and then you can come down to the to
the boot when we can have more
conversation and hello world live on
pixie dust thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>