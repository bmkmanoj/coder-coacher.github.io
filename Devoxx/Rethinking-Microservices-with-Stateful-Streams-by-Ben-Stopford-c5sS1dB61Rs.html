<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Rethinking Microservices with Stateful Streams by Ben Stopford | Coder Coacher - Coaching Coders</title><meta content="Rethinking Microservices with Stateful Streams by Ben Stopford - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Rethinking Microservices with Stateful Streams by Ben Stopford</b></h2><h5 class="post__date">2017-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c5sS1dB61Rs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Birgit you play kit do it make sure how
we doing devoxx everyone I'm a good time
yeah it's some clashing my first time
here it's a cool little conference so
today we're going to be talking about
and I get two of my favorite things when
are we talking about services and
streaming and really focusing on how we
can blend these two concepts in a way
which is hopefully quite enlightening
and hopefully quite useful so yeah my
name is Matt Bennett upward I work at
compliment and who's heard of compliment
okay who's heard of Casca Apache traffic
oh okay so we are the company that sits
behind the patch Kefka so it was founded
by the three people that built Casca
originally at LinkedIn if you haven't
heard of Kafka we're going to be talking
a bit about it not massive amount but
casco is a kind of message broker but
it's one that's sort of designed to the
Internet age so it allows you to ingest
very large amounts of messages and very
scalable and therefore tolerant way and
has this sort of stream processing API
and we're going to be using these two
tools as we rethink how we can build
services service based applications I'm
using staple streams so this is me I'm
actually my day job as an engineer I
work on the distributed log which is
sort of messaging system piece so let's
just just so who builds mug services
applications who's thinking about
building like service applications okay
video that's got a lot of you I'm doing
who didn't raise their hands but
everyone must have some interest in this
stuff so whatever Microsoft is really
about so I think this is so a lot of
people it's sort of splitting the
monolith some idea that you have some
monolithic application that you want to
split that's how I see that used quite a
lot that's certainly it from certain
inclination point of view that makes a
lot of sense for me it's very much about
this idea of autonomy so adding you can
build you can split a single application
but not have each service be autonomous
and that isn't really a microfiber sir
the essence of Micra services or SOA and
the sort of the key point I think is
this idea of services having autonomy
that ability to basically live their own
lives independent of one another have
their own release cycles right so each
service wants to have this concept of
autonomy and this is this really
manifests itself by being independently
deployable so if you have a set of
different services we want these guys to
be able to deploy themselves
independently of one another so that
teams don't end up getting kind of
caught in like lockstep releases and
that kind of thing so this actually is a
relatively difficult thing to do in
practice in some contexts because this
but but it's worth something worth
fighting for because this independence
so ideal independence autonomy gives us
a lot of value allows us to scale so
typically if you're using cash to
actually one of the main reasons you go
for it is because of its ability to
scale but actually in this talk we're
going to be focusing less on scaling
from the perspective of like millions of
users or petabytes of data which you can
also do to we're really going to be
focusing on the idea of scaling in kind
of people terms and complexity terms as
your system grows so it allows us to
scale really in terms of how a service
grows and is written and become spread
over many different teams where you have
sexually people teams of people
interacting with one another and if you
take this right off you step right back
companies are inevitably collections of
systems
um maybe applications they still have to
work together and start to work with one
another to some degree or other so in
quite so who works in a big enterprise
many enterprise people banks those kind
of things with a handful so they they
they're an example of like a complicated
set of applications that have to kind of
communicate with one another and
typically they're like be strapped
together with STP file transfers or
maybe an enterprise messaging system but
each application will be quite typically
quite focus
on a sort of isolationist goal you know
kind of head down so the thing about
services is they kind of forces to think
about more than just our application
itself they make you think about both
and the internal world as well as this
as well as well as this Pacific this
their external world so an external
world where we live in an ecosystem of
many different surfaces as well as this
kind of internal world where we reign
King so this this idea of Independence
it sounds can't good and useful 1s
aren't something that we might desire
but it actually comes at a cost
this is kind of the rub so think about
just some standard program you're
writing in oh oh well in an overworld
might have an order subjects in the
statement objects and we have you know
our design principals teachers spats we
have state and ba behavior inside our
object and we want to hide that certain
behavior by encapsulating it with an
interface where we have a method get
over those orders and this we confine
this interface so there's basically less
surface area for other services to
couple themselves to so that's the idea
the capsulation
States back to kind of Parnassus in the
seventies and that leads us to this idea
of loose coupling so this is a nice
thing right you'll think this is a good
idea everyone kind of I hopefully would
understand that this is a good idea and
if you're building a sort of monolithic
application or in fact actually a
distributed model is also then that's
actually pretty easy to do because
regardless of what we change so we have
to change that orders object in the
statement object we can sort of make
those two changes and in a singly
deployable app we can just pull it and
everything is kind of fine make changes
to both sides of the interface it's
harder to do when we actually have
independently deployable surfaces
because it changes any changes that we
have
I have to be syncronised not in a sort
of narrow technical sense of innocence
of synchronizing teams in the real world
so basically what we can take from this
is that services work best when
requirements are isolated to a single
bounded context so single sign-on is a
good example so say we have assert we
have a service which has a very simple
intimate interface like authorize a
single sign-on exists in a very tightly
bounded context it's very unlikely that
you are going to create a business
service service that you write which
involves a requirement where this
interface will change it's pretty much
isolate and isolated concern so SSO as a
service or service has lives in this
very tightly bounded context but
business services we build business
services they tend to be different most
businesses to share the same set of the
same set of facts the same core data set
so if you work in in online retail the
the customers the orders the product
catalog these core data sets are going
to proliferate through many of your
different services it's not like SSO is
those fine so out here it's got like
it's nice neat bounded context in for
most business services their futures are
much more tightly intertwined so this
leads us to a sort of interesting
potentially interesting observation
which is we need to encapsulate right we
know this we need to encapsulate to hide
internal state so that we can be loosely
coupled these loosely coupled so we can
protect I'll get ourselves against
change in the future I'm going to expose
alone so state but actually we still
need the freedom to operate on different
data sets just a slice and dice
different data sets to actually do our
job so unlikely when business gives us a
new requirement we want to be able to
iterate quickly and solve that
requirement without having to get other
services involved too much
from is it data systems have little to
do with encapsulation in fact data
systems are the complete antithesis of
encapsulation your database is a
beautiful piece of technology because it
is able to contort the data it holds
into a whole range of wonderful
different like wonderful wonderful
different concoctions pretty much
anything that you can think of you can
sort of get a data base to represent
your data as so we think about this idea
of data on the inside and data on the
outside of our services you know if we
have okay we have our nice neat service
data on the inside we have a interface
which is basic hiding data so that we
can we can we can we don't expose too
much state we don't get too much
coupling so the data outside the visible
data is quite small with a database have
this massive and providing interface it
can sort the data inside into a range of
different things
and that's their beauty right if you're
exploring data that's what you want but
it's not so great from the point of view
of building systems that need to move
independently of one another so this
leads to this data dichotomy data
systems are about exposing data but
services are really about hiding it
lissa some degree and who's heard this
sort of piece of advice micro-services
shouldn't share a database that's kind
of a a common and good piece of advice
the reason it's typically a good piece
of advice is because databases are a
very very rich form of coupling right
not only are they wire shared mutable
state and we spent many years trying to
remove shared mutable state from our
applications and sharing a database
certainly if you're sharing tables in a
database it's just going back to that
right it's like the richest form of
coupling you will find we know this
right so typically we'll wrap databases
in services we encapsulate that sort of
that's our normal route out of this
problem but actually there's in practice
because we all know this intuitively
a couple of things that we do instead
and they lead to a couple of different
conclusions first one is that we wrap
our database in a servicing state which
hides that amplifying interface okay so
we have a state run outside dead on the
inside nice and the interface looks
great where it's really well um couple
of things then happen either one first
one is that we kind of just end up over
time it looks really nice and neat when
we draw in a whiteboard and we do a
proofs concepts but over time we kind of
add different parameters to the method
well maybe we add extra methods to get
out more specific data set so we start
to kind of slowly erode this nice level
of decoupling whilst also sort of
redefining some concepts of a
declarative language in some kind of
exposed interface and so these services
end up looking like slightly with kooky
homegrown databases which we've kind of
wrapped some kind of service interface
around and these don't say the message
get longer and longer and longer and
that's kind of one path and that tends
to actually be a hard one to evolve
because every time the state of service
has to change has to change and then
other servers have to wait for that
change to happen and then everyone kind
of gets annoyed so yeah and data data
itself the volume of data inside the
dirt service as that increases that puts
more emphasis more problems for the data
service itself to serve a particular
load but actually data amplifies this
data service problem which is another
kind of downside as you evolve an
application but then there's another way
that it goes which is that business
services kind of get hacked off with
waiting for these data services to sort
of get more advanced or implement their
requirements so they end up working out
somewhere just get all the data then get
all the data they can do what they want
right that gives them the autonomy
so they just get all the open orders and
they I create I've got all the data I'm
just gonna I'm off this leads to a
different problem
this leads basically to data erosion and
this is actually something that Kent
tends to manifest itself in big
companies or bigger systems and it's
actually very very difficult to fix and
the reason is is that basically these
flows of data we've got multiple mutable
copies all around our service
architecture and they evolve
independently and they basically fork
from one another and the reason it's
really hard to fix is that the software
in each service ends up becoming
dependent on the idiosyncrasies which
are the very idiosyncrasies which you
actually kind of want to remove so once
de to erosion get slightly locked into a
system it's really hard to get rid of so
the more mutable copies we have more
data will diverge over time that's
basically the synopsis this kind of
leads is this cycle of inadequacy all
right so we start over here we've
written our who charted up our service
architecture on a whiteboard we've done
a proof of concept it all looks really
beautiful we've got our nice neat data
services we've got our stateless
services were working with the with the
UI and etc etc a service contract comes
a bit too limiting and we kind of work
out whether we can make we can make
changes together to broaden the contract
we might iterate on this a bit if we're
really in lucky we've built a sort of
kooky shared database in which case
that's going to hide the cost there's a
lot of money to scale or somebody's just
going to pull the purgin plug and we'll
go back into a redesign loop or
alternatively we can't change it we'll
just end up moving all the data frac it
schedule the data which case data
diverges different parts of the company
start disagreeing on cool fundamental
facts people start shouting each other
everyone gets fired and there is a
redesign so this cycle is basically the
background of the it's the thing that
sort of holds up a large part of the
contractor industry from what I can tell
so the interesting thing about this is
is basically three forces which compete
and these these these forces are
fundamental its accessibility the
ability to get it fader to do what you
want so you can get on with your job
coupling the lead to protect yourself
against the future and then divergence
this problem of datasets moving so is
there a better way
well actually the I'd like to say that
there's just like a bit of technical
fairy dust you can sprinkle on or magic
all the way it's not quite like that not
quite that simple reason it's not quite
that simple is that these problems are
actually fundamental there's not much
you can do about them there's no cure
solution all you can do is pick a
different point on the trade-off curve
so to go a bit further I have to explain
a couple of couple of concepts which
some of you will be familiar with but
basically have two types of system in
the world pretty much requests driven
and event-driven kind of the two core
design patterns for systems and what
these three three classifications of
commands events and queries or sects
entually how we can build these things
together put put these things together
and the definition is kind of important
and somewhat subtle so a command is you
asking for something happen like
processor payments and typically is
associated with a response so there will
always be a state change in association
with a command but it happens after you
do the clamp you do a command there's a
state change the pavement was processed
an event is a sort of after the fact
thing so in response to something I why
I process the payment or I sent out a
fulfillment request and both of these
are associated with the idea of state
change but they're they sit on other
sides at the same claim a query is
another you can just off see look
something up what's the contents of much
of my shopping basket
the important thing about a query is it
has no state change associated with it
and it's important to think about these
things differently and it seems quite
obvious but they do actually how we
compose these things together has quite
a big effect on the systems that we
built so anyone heard of like event
sourcing CQRS we've got a few so that's
that's essentially um a practice which
is based on separating these different
type of events and treating them in ways
that suit their particular their
particular form the interesting thing
about these two though is that we can -
but because we we grow up in most of us
grow up programming and imperative
programming languages this idea of kind
of asking someone to do something is
very natural
all right so we asked another module can
you please purchase as payment for me
get a response so a natural way of
programming if we program with events
it's a little bit different because you
just sort of say I process that event
just journal it written down and then
it's actually the onus is on the next
service to work out whether or not it's
going to respond what's it going to do
and that actually it shifts the control
away from services that are doing
something to reserve says they have to
respond this is this is a term that
could receive a driven flow control but
they leave that this this event-driven
idea leads to a very different kind of
architecture so most notably it allows
us to attack this idea of coupling so
coupling in a in a in a request response
pattern where we're using we used
exclusively commands and queries we tell
other services what to do or ask them
questions and this leads to this pretty
high level of coupling I'm not going to
substantiate this here but there's quite
a lot of academic work and which kind of
back this up
in an event-driven system we incent we
don't talk to different services we
interact through events I basically just
journey what we've done and we services
respond to those events never actually
invoke a service directly the reason we
do this is it leads event broadcast
methods lead to this very low form of
coupling and this is these persons will
have like 20 years has been implemented
for in many different systems until gets
done with the bus broadcast channel so
when you bring an order is created it's
like this will be distributed that
events will be distributed to anybody
who's listening in the traditional way
of doing this is basically to churn all
those all of those vents into a database
and query that database in place that's
the traditional kind of event-driven
architecture and the reason this is has
lower lower coupling is that basically
state is just broadcast these events
which affects the state because the
things that happens are just broadcast
everyone has a copy of all the state so
they can do whatever they want right
they're totally autonomous all right
it's like the libertarian view of data
so I was gonna you know even at Capital
Ventures in you the capsule kind of
changes this a bit I just say it doesn't
like solve the problem but it just gives
you this different trade-offs and it's
this different trade-off that I think is
really interesting so casco if you're
not familiar with it is a streaming
platform how does this log that I work
on distributed log which you can think
of as just a scaleable messaging system
you also have streaming has like
connectors to get data in an hour and
traditional consumer but the bit we're
both interested in here is a streaming
engine and streaming engine basic allows
us to do some pretty smart things over
this stream basically if the properties
of a database with some extra attributes
to allow us to reason about infinite
streams of data so I need too much in
this detail details spice say we can use
Casca is a backbone for communicating
between services some use it actually
with pretty much any person but the best
pattern is to use an event-based pattern
like an Avengers in architecture when we
were talking about before and we can
naturally low bounce services we can
provide for tolerance that's just going
to be built in both of our services and
of category itself we can keep data in
kacica long term you can put petabyte of
data into catacomb even with like
compactly public's money and that kind
of thing and that's something that's
happens in production every day at most
big Internet companies we can rewind
that log back in time and we can also do
this slightly more advanced thing of
representing both streams and tables
inside Casca itself and that's important
when we talk about stream processing
so basically scaleable fault tolerant
concurrent strongly orders presenters we
have these different properties which we
can leverage to communicate between
services but really the essence that
this for me is this idea of somewhere to
keep data which sits between our
services so we can leave data in Casca
long term and that has if we think about
that adventure of an architecture that I
that idea of just publishing States and
state being stored in basically
databases in different services you
still do that if we want to but the nice
thing here is that because we can keep
State inside the broker services don't
actually have to do that they can just
cash effectively cash of any data set
that they need this kind of allows them
to essentially be kind of stateful and
sort of stateless at the same time so
the way we do this is by adding stream
processing so stream processing not
familiar with it it's just a mechanism
for joining different streams of data so
we have maybe different streams like
these conveyor belts coming in with
different data sets in them and we will
combine them inside this stream
processing engine and we'll output
another result and actually so k streams
which is the interface for Casca for
doing through processing is actually a
sub
ah that is a stateful screen processing
that's slightly different because it
will allow us to output something either
as another stream or as a table it's
actually that duality that makes it very
useful what's one of those that makes it
very useful
so stream racing engine is just a query
engine much like a database you can take
a view you can join various different
streams together you can add predicates
aggregate aggregates it's sort of
typical kind of declarative interface
but the actual engine itself so in here
here in this example we have classic or
at the bottom and we've got an inbound
stream so an infinite stream which we
will chop up with windows chop it up and
wind with windows so that we can reason
about it and then we have another stream
which we're representing as a table this
might be for example a street and
infinite stream of orders and we might
be we might be joining that to a table
of customer information because the
table is just a stream that has a key
applied to it so three any stream that
has a key we can just put we can just
fold into a table and that's essentially
done so this is this is all done inside
the API of your service which basically
allows you to join these different data
sets from different streams either
streams or stables so this is Catherine
ends just kind of different view this is
our event driven service we have a lot
of domain logic being fired off a set of
events and we can join to a set of other
data sets if we need to so we can kind
of enrich join in and often we do things
like I get aggregations can actually
manage State as well but I'm not going
to go into that so much then that final
point is that when we if we want to
create a view you actually kind of just
creating this idea of a materialized
view either materializing it as another
stream all materializing as a table that
we can query so if we think about that
event-driven sort of request response
like that the two types of communication
essentially going to use the stream for
the event-driven side event sourcing and
the table to allow us to layer in
request/response on top so service may
join together instead of orders or set
of payments instead of stock you might
have some legacy app CD seeing data into
that to get it in there in the first
place we would obviously have other
services working on top of this um so
that this is where the key streams
interface is working in it will actually
the overflowing data to disk users a
thing called box DB under the covers
which allows us to basically blend
streams and state as we need to soak up
of two core patterns very simplistically
at a very high level for building these
kind of services time the first type is
that event-driven service so we're
taking a set of inputs we run our
business logic which can be as simple as
just a very lightweight function or it
could be a sort of complicated
application and we'll output some other
set of events which further further
elongate that processing the second
person is the queries so in the send the
second pattern we're doing the same
thing we're joining a set of events but
we're actually taking a view it's
exactly the same the previous one but in
this case instead of materializing it as
another stream we're going to
materialize it as a table which we can
then query so that can either be the
context of we have in the context of
like we have a UI service and we just
want to run queries we might for example
expose this to the rest or we might have
a WebSocket which pushes events up to
that interface or this could actually be
a legacy application running in the same
process so our view is just something
which we siksa sighs our legacy
application which it queries or gets
triggered for
so it's a bit like traditional approach
but we have these extra tools which
allow us to basically manage date in a
much more lightweight way in a way which
is really just a transformation a top
that those those those shared set of
streams so we have this log the flow of
events and we have this query engine
which is sitting inside our services
embedded in our services sitting a topic
now data storage plus query engine
equals database basically of thoughts
anyway so they'd is living in Kafka
query lives up here we can actually read
them right so we can actually write to
this state store here and it will flush
the data back to Casco and then carrot
all of its durability guarantees so we
effectively have a kind of database
right data storage per square layer only
the database is slightly different
because the functional piece apart from
being event-driven is separated from the
storage piece which is not quite like a
normal database so you can actually have
one storage layer and then you grew it
in many different services so we have
this kind of shared database right one
shared set of canonical streams which
every service is reacting to but many
many query engines embedded in each of
our different services this is kind of
interesting this is effectively a
database inside out
anyone seen martin Chapman's talk on a
database inside out no good talk there's
a link to at the end so this idea of
turning a database inside out is quite
interesting we can have historical sort
of tens of set of Streep's and then set
other query engines embedded in each
service why is this important going back
to our data dye economy before
because it gives us control each service
has autonomy over these data sets it
controls how it queries it it controls
when those queries change not a shared
database in that sense but it does have
shared data sets so that we're always
referring back to this canonical set of
streams so as we said before Mike
receives is shouldn't share a database
the reason they should share a database
- database is because a database is
shared mutable state it represents the
highest form of coupling this isn't a
normal database in fact if we think of
that event broadcast pattern that sort
of traditional avenge of an architecture
which we can prove is have the lowest
form of coupling all we've actually done
is added a database engine here which is
dependent on an immutable set of streams
categories represent some useable data
sets and the nice thing about moving
immutable data sets is that they're
they're effectively invariant in time as
one of the order is the same so
inheriting the same coupling that we get
in this architecture but we're also
leveraging this idea of a shared of
shared mutable streams sorry immutable
streams so what this is doing is is
giving us a way to actually focus on
data that lives between our services
whilst decentralizing the responsibility
for querying allowing us sitting
encapsulate that inside our services
where we can get a level of autonomy and
change
so we're centralizing and immutable data
set we're sort of have a golden copy we
know that that's not going to affect or
that won't affect coupling and we're
decentralized the queries so the most of
is basically to share a database let's
turn it inside out gives us much better
decoupling on autonomy so it's kind of
reflect back on those so there's
basically four different ways that we
kind of covered here will be it loosely
we can just share your database if you
just have a database in the middle of
the application from a coupling
perspective we get two frowny faces
because it's shared mutable state so
it's a very great date is just their
integrity great there's just one copy
served interfaces like we got rid of a
mutable bit basically and encapsulated
the interface a bit so we get one for
that still going to have a hard time
mixed in changing that interface
accessibility is we is again because
sets we have to wait for that interface
change divergence again is great we get
happy face for that one because we've
got a single source of truth event
broadcast smiley face for autonomy
because you've everyone's got to copy
the data they can do whatever they want
but not so good for accessibility
because it's ephemeral you have to like
track it over time and not so good for
from the perspective of divergence
because you have effectively many
mutable copies of data and then this
kind of log back streaming approach
which is really just a middle ground
where we retain assets we get to retain
autonomy because we own the way our data
is processed in a kind of event-driven
way but we also have accessibility
because we can use we can dip any
service can just write a new service
just dips into the stream of events go
back in time if it needs to
and then from integrity perspective our
views are always creative on top of this
canonical set of streams so it's good
from an integrity perspective also
so services like this tend to kind of
have this sort of type of pattern where
and this is kind of quite important what
we're doing is we're building building
services on a event-driven backbone and
then we're kind of layering in request
response where we need it all right so
rather than just like starting with
request response everywhere we're
starting with a vendor even the most
decoupled pattern and layering requests
what we need it so here we've got like a
web tier
using CQRS to split commands some
queries and then they're effectively
event sourcing into Kafka and then we
have maybe a couple of other depart
departments which would internally have
event-driven services promoting that
kind of cascade of events so these these
typically decouple from one another in
real in the real world they're kind of a
synchronous process in the real world
and likewise and then internally we
create views inside it's inside each
bounded context which allows us to do
things like things like drive through
eyes or do more traditional request
response important point now as you
start event-driven and then we layer
request response on top so um
good architectures in my opinion have a
little to do with their starting point
certainly you can draw something that
looks good on a whiteboard you can often
do a POC which will work well but it's
actually the evolution of a system that
defines whether or not is any good like
a can a system evolve effectively over
time that's the real measure of a system
so if we're going to build services and
we want those cyber systems to grow we
want to build on a framework which is
inherently decoupled and layering higher
coupling interfaces where we need to so
we want an ecosystem which will embrace
shared data but not in a way that
promotes divergence in a way that kind
of keeps things sane and together
cohesive so we kind of want to break
this tie of just many many respects many
many services telling each other what to
do querying one another in a sort of
tightly coupled way and instead design
for the data that lives outside our
services as an entity but just without
using shared
or stayed so set your data free data
should be available data should be under
your control so the summary of this is
vendors and services is really we
broadcast a set of events we retain them
in the log so we rely on events over
create over commands we compose
functions with a streaming engine so we
can give those very very simple programs
actually that the vote is very
functional style ecchi anyone used
atomic yeah it's quite similar in
concept they're really just composing
functions over a stream of events and
using them to simulate both asynchronous
and synchronous flows and then you build
these views where you need to so you
need to build a view to add a view
so actually I should also add that it's
also very common to just create a view
in a streaming engine but you might just
push that data into a database if you
need to have a interface which supports
data discovery basically so materialized
view is typically something that you'll
access by key or by scan you've done the
query ahead of time just like you would
an in database but asynchronously so you
build the layer on these views where you
need to and that's essentially it
there's a set of principles which kind
of encapsulate this type of system so
Wimpy we want something where we can
start simple lightweight and full
tolerant so we need we're designing
around a fabric which will give us the
ability to kind of boot the service not
have to worry about where it is coming
from but start in the event driven way
start lightweight dab stay lightweight
we just want to start that way what
data's be immutable so we're building
sort of retentive shared narrative which
journals how the system evolves over
time when is to be reactive when it's
leverage asynchronous so the idea of
composing functions together with an
emphasis on process
in recent events rather than this sort
of batch style paradigm um evolutionary
didn't really touch on this today but um
one of the nice things that falls out of
this type of pattern is you don't need
you only need to use the data you need
to use at this point in time and take a
relative it you can trim off just the
need the data that you need today
because it's very easy to go back and
get other attributes or other streams
that you might need later to kind of
sort evolutionary approach to the way
that services are built where they only
really take the bits in you today and
then I guess fundamentally decentralized
so that's that receiver driven flow
control where each service decides
whether it dips into a string don't tell
each other what to do no good services
certainly in theory so yeah there's
about a few references the blog on this
topic on the confluent website actually
I wrote and there's a few more to come
out in a series often captain's turns
database it out inside that's a really
good one Pat Helland on immutability
changes everything you know one read
that paper yeah one that's that this is
this is another great paper it's it's
actually written in in the context of a
sort of a slightly older world I'm by a
database academic but it's a really good
read I really recommend reading that one
and leave and this one and then Jake
reps the log and they are on the vent
sourcing both kind of good reads this so
these will be published on my website if
some if you want to get a link and
that's me and you find me on Twitter and
I said or in the compliment blog thanks
very much for listening it's a real
pleasure to be here and if we've got a
few minutes for questions if anyone has
any and that thanks very much
anyone got a question sure hello say if
I have this a service string backbone
and if I had some data that is
confidential can she only be accessible
by a subgroup of those services then how
should I handle that yeah it's a good
question so the you basically just use
the security features so you can pretty
standard set of security features where
you can set up access to topics or
different commands on the cluster so
typically you do this with topics spread
the you spread the data well you either
have topics which have specific users
are allowed to read or write to I'm all
alternatively another way of doing it
you just set up a streaming job which is
to create a new topic and then you can
sort of divide datasets that way that's
often better if you need to sort of
subdivide a topic because actually it's
only some subset of the data that you
want to have control over
so basically security will give you this
data set is secure and the translation
of the topic will allow you to basically
strip stuff out so you can do basically
row-level security so make sense and
yells hey chop here
when I installed Kafka I found it quite
complicated to configure with zookeeper
with number of replicas with data
directories and are there any plans to
make this like easier managed or um yes
sometime I'm sorry to say I'm sorry if
it was difficult to get bigger I mean I
mean the default chest assessing should
work like buy out-of-the-box
shouldn't happen I mean you shouldn't
have to tune the default settings unless
you doing something that's unusual
well not unusual actually but you should
better get going with the default
settings the zookeeper comment is a
common one we can't talk about a lot we
may well embed because most people a lot
of people will run zookeeper just on the
same server as Casca so in theory you
could just have a start of the booty to
kiba just in the same process typically
you wanna make sure is go distant data
data's are actually but we you could
also do that so that might happen that
would more likely be some sort of
complement in the console version I
think VA shouldn't be too difficult I
mean the it is a distributed system so
typically you will run it you don't want
to lose messages you will run it and
with at least two nodes running so it
relies on replication for fault
tolerance by default but yeah it there
is AM there is a move to I guess you
have a department which is looking at
trying to make the whole process slick
it and easier definitely has a Silicon
Valley feel to it I mean if you install
it the default yes it works fine and you
can start a play with it but if you stop
to think about a production environment
let's say not in a standard cloud
distributive but in a yeah and discrete
service you would have to write like a
manual for your customer to instead of
the bride with the number of replicas as
a number of
yes so there was definitely an element
of the fact that it there is a bit of a
learning curve if you want to move away
from the default the defaults are now
ready to change from the Diesel's
recently but the the defaults are good
for most people there are a lot of knobs
and one of the reasons for that is that
there is a broad range of use cases so
there's typically at one end as users
that one very strongly ordered very
durable systems and at the other end
there's their users that want
effectively sort of eventually it's not
eventually consistent but systems that
will loop that will favour availability
over losing data so I've been just
always running never go down if the
worst happens I'll just keep going lot
of stuff for fit data to keep the system
running and there are sort of settings
around that offices high throughput
settings into those Hooper settings so
yeah I I do appreciate that it's it can
be a little bit difficult to configure
as in there's a lot of settings to tweak
it's bit like the JVM in that way but
yes that is something we're looking at
any other questions
how's one round here - Frank you chose
the water management payment scenario so
I'll go with that
and so I've got my one alarm I'm buying
a book then and it's a it's a website
where I have an internal account so just
how to update the account I've already
I've already put them
I've already got a hundred bucks in the
account so it's just updating that so
the sequence that I see that happening
is and I to the website component that
connects to Kafka saying I want to order
this book that is going to be fed to the
autocomplete order fulfilment component
that feeds back to Kafka feedback -
that's going to feed on to the account
component which it going to update that
okay so that's the sequence that you're
talking again to the website component
so I've got multiple orders order
components
because it's a distributed system and
one isn't sufficient and and I'm doing
my order in Japan and the order
fulfillment ones are based in the US and
Europe and so how do you how you dealing
with the race condition and then you've
got that distributed global account how
has that managed so the race condition
between what exactly he looks order
fulfillment component and those are
going to get the no no no native area so
that's how you deal with it they
wouldn't both get it
so Kafka will deliver to only one yeah
and how does it know the only one is up
it's funny this is like I should talk
about this more she's actually my cool
it's the coolest thing I my favorite
thing in Kefka so Casca actually has
although I have to say when we when we
first built it I was I looked at the
second of it-- really this is
complicated but um Casca actually
provides a level of clustering to its
clients so basically it the way that
works is that your data is partitioned
and it will give so if this if there's
two services it will give half of the
partitions to one instance of the
service and half the partitions to the
other one and then it will manage the
movement of that partition those
partitions when one of them goes down so
you have like complete high availability
wealth the ability to route datasets to
certain systems globe and what globally
and it will not be chemically go because
you typically run cattle globally so we
do run the cattle go buddy but you'll be
you're you would you would you would
you're connecting I mean typically in a
global scenario you would want cap
clusters in each region connect them
together and then you'll actually be
managing their distribution on each of
those local nodes but the workflow can
work globally because it's just an async
is workflow but the really nice thing
about that is actually not the that
cross swing thing is that it provides us
from ordering guarantees it's actually
quite hard to get that in a messaging
system messaging systems depends on the
spec but it allows you to basically
manage
high loads and have very strong ordering
guarantees while spreading spreading
load across different services is it and
the reason for this actually for its
width is because this actually comes
from screen processing so you need these
things in stream processing a lot of
these are just things that about through
purchasing and fallout means that which
happens to be useful for services that
make sense it makes sense it just makes
it not quite like we can just go ahead
an emotional yeah there's less take it
offline so for it so there's a set that
said there's a set of posts that's
coming out which kind of really goes
through this in this means pre long
there's like 10,000 words or something
to describe you all this stuff I'm
trying to do it in 40 minutes and cool I
think we're at a time at a time or we
got any more yeah there anyone would
start a question I'll be outside
thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>