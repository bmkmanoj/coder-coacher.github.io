<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Exploring open data with BigQuery by Jen Tong | Coder Coacher - Coaching Coders</title><meta content="Exploring open data with BigQuery by Jen Tong - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Exploring open data with BigQuery by Jen Tong</b></h2><h5 class="post__date">2017-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7yI2zAJjwfI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everyone how's it going
happy happy Monday afternoon a couple
deep dives in how's everyone feeling
great yay we're gonna be talking about
some bigquery stuff so I'm like fun yay
we're actually gonna be poking around
some open data with bigquery because
open data is always fun and it's easier
to get your hands on cuz corporate data
people hold tight to their chests anyway
before we get into that though let me
tell you a little bit about myself
hi I'm Jen hi I'm a security advocate at
Google focusing on cloud platform and
the security parts a little new
previously I spent my time as a
developer advocate shifting a little bit
more towards information security stuff
but we're not talking about that today
so just pretend I'm a developer advocate
still so as part of my job as a
developer advocate I come and tell funny
stories about cloud computing to find
people like you so you can take that
knowledge and go build cool stuffs but
that's only half my job the other half
is hearing stories from all of you the
good stories they're not as good stories
you know the things we could be doing
better and I take that back to my
product team so that we can make our
stuff even better because everyone likes
better tools and things so I'd love you
to hear from all of you if you ever have
like a concern or comment or just
something cool to show me and a great
way to reach me is on Twitter I'm MIM
encodes on Twitter or if you want to
contact me via some other means you can
find links on my homepage mimic calm or
there'll also be all the code from this
talk and the slides and that stuff will
be up there too so here's the plan
for today we're going to talk about data
in general in the abstract then we're
gonna do the first thing you do with any
new Big Data tool we're gonna count some
stuff as counting is fun it's a great
example it's a simple way to kind of
visualize scale
then I'm gonna take a few minutes to
dive into and explain how that counting
thing works and we're gonna finish up by
applying it to do something useful so
I'm like a plan yeah okay let's do it so
first let's talk about data in the
abstract one of the bigger thing that's
kind of separated the soft drive worked
on in the last like 10 years from 10
years ago is the amount of data we're
just working with today we're
accumulating more data than we ever have
before you know whether you're working
at a start-up or at a company that makes
soap or something we're still
accumulating lots and lots of data from
sensors from data gathered from our
users that kind of sauce and all of us
know in our hearts that all that data
we're collecting is super important that
it's the information that gives us the
competitive advantage that it's like
it's the thing that makes our business
more successful like for example if
you're working in you know if your
company makes soap you can use that data
to make your supply chain more efficient
or something like that
everybody has lots of data to deal with
and there's something I spent a lot of
time wasted a lot of time with is a
developer is I'll have a bunch of data
and I think I'll have something I want
to pull some insight I want to gather
from it so I'll spend a bunch of time
writing a bunch of MapReduce jobs and
feeding it on into a dashboard it'll
take me like two weeks of engineering
work and when I get to the end of that
story all I've realized is that my
assumption about that you know that
dashboard I wanted was wrong it was not
actually useful data at all and that's a
huge bummer and that's the kind of
problem that we've dealt with a lot at
Google because we deal with lots of data
we have lots of things like log log like
data things like that we have to deal
with and sometimes we want to get
answers about the data quickly so that
we can you know troubleshoot a problem
you know something that's misbehaving or
maybe some performance degradation that
we want to get to the bottom of quickly
sometimes we need to dig through a lot
of that data really quickly and we don't
have time to to build out via
traditional means and when we have
insights like that sometimes we want to
share them with the world we've done
lots of math to help get us to that
place and we share a lot of our
our secret style are not secret anymore
because we publish it a lot of the
insights we have in the nerdiest way
possible we publish them in journals
science journals and math journals so
that everybody can read them and it's
cool and it's something that we've
started a long time ago when we
published papers on PageRank and
MapReduce back in the 90s and we've
continued on until today over the years
where we started to publish papers that
describe the the math that underlies a
lot of the tools we've developed with
great names like flume Java and Dremel
all of our wonderful internal names and
what's really cool is when you publish
math it turns out people read math
papers I like reading math papers turns
out other people do too and what happens
is when you publish the math behind some
big data problem you're solving people
who'll turn it back into software and
open-source projects will spawn around
this man if you're if the insights you
give are useful and that's just what we
saw with a lot of the stuff we've
published like our BigTable paper gave
rise to actually a whole bunch of things
but I think Apache HBase is one of the
things that's most true to the original
implementation and other things like
Dremel gave rise to Apache drill and
it's really cool it's cool to see all
that stuff coming off the ground and
helping people solve real problems but
we always kind of liked our own
implementations a little bit more and
over the years since we published those
papers we have little tweaks here and
there that we've made so one of the
things that we started doing when we
started really focusing on Google cloud
platform is taking those internal
products that we were still using and
refining over time and making them
available as Cloud a for services so
what we did is we took our old products
or internal products like BigTable we
repackage them up we cleaned up some of
the things on the API that we've learned
since we originally wrote them all the
bad parts you don't want to work with we
gave them a new logo because
everything's got to be crammed into a
blue hexagon so we did that we blue
hexagon ionised
all the things and sometimes we gave
them outer names like for example
BigTable stay BigTable because that name
was cool some of the other ones I
we're a little too internally so Dremel
for example became a bigquery and that's
what we're gonna be diving into in a
little more detail for the next 20 or so
minutes we're gonna talk about Google
bigquery one of my favorite tools for us
to quickly get insights from questions
we have about data we've been working
with so I could tell you all about it
but first I think I will show you
because everybody likes demos right so
let's count some stuff exit out of my
slides and I think this is like the
hello world of just about every big data
thing I've ever done is counting words
and Shakespeare how many of you have
counted words in Shakespeare before when
starting to use some new tool Oh only a
couple people it's something I I seem to
encounter a lot it's like the hello
world of any big data thing I ever ever
encounter so that's gone over - this is
a big query web interface this is just
kind of a place to hack around to stuff
and make it a little bigger for easier
seeing and um we can run queries here as
you can see bigquery you work with it
you get to look at information using
sequel the sequel is a wonderfully
expressive way to examine data and here
we've learned something so we're right
now we're looking at the public data set
for the words of Shakespeare and we're
counting them we're counting the number
of distinct words as Shakespeare used in
all his plays our query oh wait I forgot
something actually cheated a little bit
cuz I forgot to disable caching because
you know that's no fun if you just see
my cache data from earlier so I'll run
the query again so you see that it
counted all the words in Shakespeare
it took about two seconds it processed
1.2 megabytes of data so it's actually
kind of slow I could do that a lot
faster just on my laptop with the word
count tool but a count it's exactly two
164,000 distinct words yay but we want
to count big things right so we're gonna
have to leave the world of classic
literature and move into the modern
world and one of my favorite data sets
to play with when experimenting with new
tools is Wikipedia data because
Wikipedia made all of its request logs
aggregated request logs and like editor
logs and a whole bunch of other stuff
available as
data dumps up until about mid 2016 and
it turns out they're a very popular
website so let's count all of the
Wikipedia request data for one hour in
2015 so this one process is about 46
megabytes of data a little bit more it's
a little bit faster which is surprising
and it counted to 39 million so 39
million worth of requests during that
hour but you know what's cooler than a
million a billion so let's count a
bigger chunk of logs logs are available
both in one-hour slices and in one month
slices so let's count all of the
requests that came in in the month of
May of 2015 so it took about 2.9 seconds
a little bit longer we processed 43
gigabytes of data so a thousand times
more than in the previous query and we
counted to 20 1.1 billion requests so
the numbers are starting to get kind of
big but you know it's clearer than a
billion a trillion and so I downloaded
the logs originally as one month slices
because that's what they were posted at
so I could do a query like this where I
Union all the different logs together
and do a query against it but I can also
do a wildcard query which is a lot
easier to read so let me paste it in so
I'm just doing here is looking for all
the data I imported that starts with
this string Wikipedia page counts 2015
and they're all organized you know
there's just a substring after that it's
kind of like a like query on a bunch of
tables and this one took seven point
seven seconds we processed 497 gigabytes
of data and it counted up - did we get
to a trillion not quite we've got to 286
billion which is still a big number to
count - so we're on the order of a
trillion but you know it would be like
madness what would be silly to do on
that would be to run a regular
expression on each one of the rows in
that data set this is the kind of thing
that would make a lot of my old homebrew
relational database based stuff sad so
let's take a look but let's not do
dinosaur let's do dev ox
so what this query is doing is it is
going back to all the requests logs I
downloaded from 2015 and it's looking
for all the requests to pages with a
title that includes a case insensitive
match on the word devoxx and this one
processes a little bit more data it's
processing 3.5 terabytes of data in this
query and that's because it's not just
looking at the requests numbers that do
the aggregation it's also examining each
one of the titles as it goes through so
it takes a little bit longer to process
so process 3.5 terabytes of data in 30
seconds so you can see the query time is
relatively flat as a data size increases
we discovered that thirteen thousand
nine hundred and seventy requests in
2015 were four pages about a box that's
a lot for a conference that's pretty
cool okay so we just counted to some big
numbers how did it do that because
that's the kind of thing that would make
a lot of systems that work within the
past sad well so I was entering a bunch
of sequel queries into a web interface
so you think like hey it looks kind of
like a relational database but no it's
not at all then one of the reasons they
are are go back to when we first started
developing bigquery we wanted to use
sequel because it's very expressive and
a lot of people use it but databases
have a lot of hard things in them that
make them good so if I'm going out and
I'm searching for a good relational
database I'm gonna be looking at
features like how does it do insertions
because I'm going to need to change my
data how does it handle locking you know
likelike sequel Lite is great but the
locks are rather large the whole
database usually so not not always great
for highly concurrent systems how do how
does indexing work what are the
different indexes available and how does
it use them to solve problems how does
the caching work cuz caching is
important for high performance you know
how does it do all of its query planning
how does it use those tools to evaluate
a query and come up with an answer
quickly and these are all great things
that a lot of relational databases have
developed very for very you know very
far and done really cool stuff with but
they're really hard to do
so when we were developing dremel such
bigquery we decided not to do any of
that stuff we're just gonna throw it out
cuz it's hard and we're working on data
a little bit different we're only
working on historical data the data is
not mutable while we're using it so you
changed some assumptions but we still
want it to be fast cuz it's all about
getting our answers quickly so instead
of doing this kind of stuff to make it
fast we decided to rely on disks because
it turns out we have a lot of disks like
we have access to a whole bunch of disks
and if you spin them all at once you can
get your answers really fast and this is
one of the cases where like a lot of
kind of those those cloud services can
can take significant advantage of
resource sharing because as it turns out
having one hard disk for five thousand
seconds costs about the same as five
thousand disks for one second when you
get to share those resources across a
bunch of users but getting that answer
in one second is a lot more useful than
getting it in five thousand seconds so
we have a lot of hard disks spinning we
also store the data a little bit
differently because as I mentioned we
don't run out we're working only on
historical data we're basically
processing log like data which means we
don't have to worry about that data
changing underneath us because you know
when you're writing or using a real
relational database it tends to store
the data a row at a time
because usually gonna find one row and
then change one of its neighbors or
you're gonna need to look at one of its
neighbors what we're doing with law-like
data that's not usually the typical case
typically we're interested in a subset
of the columns at any given time so the
first thing we do when you import data
is we slice it down by columns so this
has a couple of significant advantages
one is when I only care about two out of
a thousand columns I have to load less
data which is great because I can load
it faster then the other advantage is
compression because when I store things
in the column next to each other it
turns out that similar data ends up next
to each other very often which is great
because I can compress it really well
and they're not so much concerned about
the space because disks are you know
space is cheap these days I'm
more concerned with the throughput
because I want to get that answer as
fast as possible so compression allows
me to read the data off of the disks
even faster so once you've taken your
data and we slice it up into columns
then we take it and we smear it off
across a whole bunch of disks is at
least you know story on a big ole
distributed file system in a way such
that one slice of data may end up on
many many many disks simultaneously and
this allows us to use multiple disks at
the same time you know like a raid array
to read that stuff faster because speed
speed is a thing we care about so with
the data stored let's take a look at how
a query executes so here's a query very
similar to the one I just ran where we
counted not dinosaurs what we counted
dev axes and Wikipedia logs this is a
typical kind of simple query you might
run a big query so let's take a look at
how it is executed across that you know
column energy columnar datastore
difficult phrase so there are a few
different kinds of nodes that are used
to execute a query run bigquery you have
your mixers and they are the smartest
nodes they can process basically they
can parse the query and they can break
it down they can also recruit other
mixers to help them they can also do
shuffles and sorts which is one of the
things that ends up being Network
expensive so they're the most
intelligent nodes in the whole lot
hierarchy there are also leaf nodes that
mixers can recruit and leaf nodes can do
simple filtering they can also find
information in the storage system so
when our query comes in it's going to
hit a first mixer it's going to find a
mixer that will be its root mixer this
is the mixer that's responsible for
giving the single result back to you the
user that mixers initially going to go
and it's going to go find other mixers
to help it solve this problem so it's
going to parse the query and it's going
to start to peel off the layers
especially if the query contains
multiple sub queries which is a common
pattern it's going to peel out those sub
queries and is going to pass them down
to mixers below and that layer of mixers
for more complex careers might recruit
another layer and
earlier so you might end up with several
layers of mixtures but for the simple
query I just showed you we're just gonna
have to eventually get to the point
where it's broken it down to a pretty
simple request it's going to find a leaf
node that knows how to access that data
which will then access it on storage so
it goes all the way down to the bottom
so then from storage we're going to push
up the the columns that we care about
just the whole columns that point
because every queries basically doing a
full scan so all the data from that
column goes up then it's going to go to
the leaf nodes which would do kind of
the first round of simplification
they're going to do this is where like
typically reg X's will happen and a lot
of those does first round the functions
will happen so at this point this query
has about 5.4 billion records going up
for that one month of time then after
we've done that first one it's going to
go up to the mixers and the mixers if
there's any kind of sorts that have to
happen which will happen when you do I
contains operations or deduplication
anytime that there's a sort happening
underneath the covers or more complex
operations happening that's when the
mixers will take over so for this
particular request this the the bottom
layer of mixtures takes care of all of
evaluating that reg X and filtering it
out because the way this red text
function works in sequel is it adds a
column when the red X is true too true
so it's filtering out all the data that
doesn't match the reg X and then it's
doing the first round of summation but
there still might be many many many
mixers running in parallel now so at
this point we have like 10 partial sums
but we're still we're pretty far out of
the reduction phase and then finally it
gets packed up to the the root mixer all
those events will go back and they'll
get aggregated in one final event the
room maker often does the same thing as
one of the mix you know part of the
works of the mixer a layer down to us in
this case summing up our requests into
one bigger sum and then a single record
gets emitted at the very end yeah now we
know where answer it was like 17,000 for
dead ox which is cool okay so we've
counted some stuff we've kind of
explored briefly how it operates let's
do something a little bit more useful
let's use Wikipedia to get a movie
recommendation I think this one because
it's fun and it's also
something I worked on once upon a time
just some like straight code and it took
me a long time to write something to get
any anywhere near useful data but it
turns out that it's pretty easy to do it
in sequel because sequels nice and
expressive so let's get a movie
recommendation oh wait I still have some
real slides I'll go back to real side
for you before before we get back into
the code and this is how we're going to
do it we're gonna stay in Wikipedia data
except this time we're gonna look at a
slightly different data set another
public data set this one is edit data
edit data is smaller than request data
but it does have one notable difference
it can give us a movie recommendation
and so what you can do that is because
Wikipedia editor data is not always
anonymized a lot of the Wikipedia edits
are tied to a specific user a Walden
user and a lot of those users edit but
they have two pages particularly like
movie pages so one thing we can do is we
can look at all the people who edited a
particular movie and then as it turns
out most of them don't just stop at one
movie they edit many movies so we can
look at the other movies they care about
we can aggregate that up and then we can
get a a pretty good recommendation for a
movie based on how much passion someone
has to keep making the same edit to a
page about a film so the first thing we
have to do mostly to make sure the the
query will fit on on a page on the
screen is to pick a great movie so I'm
going to pick hackers the 1990s film
which is awesome so let's go to bigquery
and the first thing we're going to do is
find the internal identifier which stays
consistent across all the logs for
hackers the movie so we're going through
over doing the light query on hackers
and then the light query on a film and
this is something you end up doing a lot
in open datasets it's kind of filtering
out drunk data that you don't care about
this Wikipedia namespace limits it to
just actual Wikipedia pages and not like
the discussion page or user profiles
that kind of stuff so we found our film
page and it has an internal ID of 264
one seven six so we have that we can
copy it we can also see it's it's a
pretty popular page 9-hour 731 edits is
a lot so it's a pretty controversial
movie as far as movies go and Wikipedia
the next thing we're going to do is
we're going to take the same data set
and we're going to mash it back into
itself so let me give the career run the
first inner query which will run first
is going to go through the Wikipedia
sample data on the editor page editor
data it's going to find all the edits to
hackers of the movie it's going to
filter out the anonymous edits and the
botted it's like the wrought Ematic
rollbacks and all that stuff and it's
gonna give us a list of all the editors
for that movie then we're gonna query
against the same data set and we're
going to ask for all the different
titles and IDs and edit counts for all
the other films they also edited so
we're doing a join that way and as it
turns out when you do a query like this
on Wikipedia data you get a lot of the
same movies coming up over and over and
over again particularly Fight Club I'm
not sure what the deal is a psych club
but it's incredibly controversial among
Wikipedia editors turns out almost
everybody who's edited any film page on
Wikipedia has had some involvement with
the Fight Club movie page I don't know
what's up with that but we can solve
that with another query so we can do a
query that will discover the most
broadly popular or from an editor
standpoint than those controversial
films so we can take that same data set
and look at it again and what we're
doing here is we're just finding again
we're filtering out discussion pages and
that kind of stuff and we're just
finding that the 20 most popular the
most controversial films and getting a
list of identifiers so then we can take
those two queries and we can put them
together and make one giant query which
looks intimidating but it's built out of
some pretty simple pieces we already
discussed this ones just finding those
common edits and this one is removing
the 20 most controversial films and we
get a list of recommendations it does
not include Fight Club we get at about 7
seconds later about 13 gigs of data
after that and there are some things not
too surprising in year
Craig should probably make my little
controversial film a little more
aggressive Zachary comes up a lot too
but this one kind of came up by surprise
Mystery Train when I first started
hacking out the squares anybody see in
the film Mystery Train a few people I
had not seen it before I ran this query
so let's go take a look at it and it's
that we gave PD a page so the first
results probably gonna be a Wikipedia
page and it turns out it is a 1989 in
the penitent ology film with a budget of
2.8 million and it was kind of a flop
1.5 million at the box-office so a
pretty small film but it is exactly the
kind of film I would want to see if I
love hackers the movie which I do so I
liked it a lot so our recommendation
engine basically worked and it's
something that if I had written from
scratch would have taken me quite a
while to write but we wrote it in about
30 lines of sequel code and only a few
minutes and this is a great way to kind
of evaluate one of these things because
it query bills by the amount of data you
process so it charges about five US
dollars or I guess for 30 s euros for
every terabyte of data you process so
this this one costs some number of cents
to do each time you process it so it's
not the kind of thing you would want to
deploy to production especially because
it's still taking 7.5 seconds per
execution but this is a great way to get
started to evaluate if that thing is is
a correct question to answer have I
structured this question correctly have
I set the thresholds on the
controversial films at the right spot
are there other things I need to
compensate for before I go and I spend
two weeks building out a much more
efficient much more rapid heavily cached
pipeline for all of my users so it's a
great tool for finding that that kind of
initial answer or an answer you just
want to answer once you know a question
you just want to answer once so ya move
your recommendation so a few minutes
left so I'm gonna wrap up
so that's bigquery it's one of my
favorite tools for hacking around with
big data it's a great way to answer
questions quickly that you do not even
know if you should be asking it's a
great way to kind of set yourself up for
building a more complex pipeline in a
later
implementation and you know just just
doing other interesting things the other
cool part about bigquery is you can get
mash up any arbitrary data set with any
arbitrary data set worldwide so you can
take you could feed in your own logs and
you could combine them with Wikipedia
data if that was useful or one of the
other many many open data sets we have
available and it also has a free tier so
if you want to tinker around with it
there's a whole bunch of open data sets
in addition to the Wikipedia set I just
showed you the first terabyte of stuff
each month is free so every query except
the how many devoxx in 2015 request
falls under that free tier which is
pretty cool so yeah so thank you very
much of a few minutes left for questions
and I definitely do reach out to me if I
can ever help with anything related to
this or other Google API stuff it's
always great to hear from people so
thanks so much so we have a couple
minutes left if anyone has any questions
but what was the cost of the dev axe
query so I'm doing I'm bad at
conversions here so I'm going to answer
in US dollars I hope that's okay so we
processed 13 terabytes of data so that
comes to about 17 US dollars so if you
have many many many terabytes of data
you want to go through and that's
actually like if you look at the full
data before it got ingest it into
bigquery it was like over 100 terabytes
of something like that it's some huge
amount so if you have that much data you
need to munch through quickly it might
be worth 17 dollars to get that answer
immediately but yes it is definitely a
product where you will want to set up
your budget alerts before you play with
it too aggressively you can when you
spend thousands of just at once because
that would that query put in particular
spun about 5,000 discs to get the answer
you can burn through lots of code or
pretty fast
any other questions nope another
question back here
so the question was where did the data
come from so I can actually go back
there are one of the cool things about
bigquery is you the person who owns the
data and the person who pays the career
actually builds separately so it's been
very easy for people to upload data and
make them publicly available so the data
I showed you was birthday two I uploaded
for preparing this talk but there's a
whole bunch of data that is available as
open data so that dataset for example is
set to publicly available permissions so
anybody can run queries against it but
there is a great reddit community read
of bigquery open datasets datasets this
is actually the best list of open
datasets available there's some that are
officially uploaded by Google and
maintained that are like streamed in but
people have uploaded tons of datasets of
all sorts of stuff from weather data to
like hacker news comments and all that
kind of stuff and they're all available
here and you can combine any arbitrary
data set with any other if you have an
interesting question about like how do
code commits correlate with alcohol
purchases in Tennessee stuff like that
so thanks um one last question these so
how is caching done caching is generally
done for for the results at the end of
it so if you're how does if you want to
cache aggressively you'll want to do
that in kind of a client layer above it
a lot of people will use this as a
back-end for tableau or something like
that and they'll do most of their
caching there but generally it does not
use caching anywhere near as extensively
as like a real relational database would
so don't don't really count on the
caching too much so that's all my time
thanks a lot for the great questions and
thanks for your time have a great rest
of your day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>