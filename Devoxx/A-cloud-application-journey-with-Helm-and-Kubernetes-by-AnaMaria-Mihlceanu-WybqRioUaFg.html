<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A cloud application journey with Helm and Kubernetes by Ana-Maria Mihălceanu | Coder Coacher - Coaching Coders</title><meta content="A cloud application journey with Helm and Kubernetes by Ana-Maria Mihălceanu - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>A cloud application journey with Helm and Kubernetes by Ana-Maria Mihălceanu</b></h2><h5 class="post__date">2018-04-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WybqRioUaFg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone thank you for joining
me today in today's presentation I will
use the journey of a cloud application
in order to provide you some useful
information on how to ensure that
applications that exist inside of
containers are resilient to changes or
can scale while using kubernetes also I
would like to point out some things
related to my experience as a developer
with communities and helm and how helm
has helped me to deal with things that
are not so Java developer friendly or
developer friendly in general but first
of all a little bit about myself I'm
Anna I'm the only female speaker from
this conference as told in the keynote
this morning I'm an application
developer by profession and I am i oka
practitioner in my spare time this is
where I get some that control enthusiasm
yet some other parts of my control
enthusiasm comes from what I do in my
daily job where I try to understand how
things are working into and while coding
and to understand how I can control both
my code and also what's not in there
under my control this is why I have
became interested in containers and how
to use kubernetes as a developer and
also how can I manage the communities
while developing my code and make this
experience not so tight to do
communities directives directly first
the agenda of today so today why helm
and kubernetes some applications set up
because we need an application to deploy
with kubernetes Cabrini's and home
deployment of that application obviously
and so retrospective and some key
takeaways for today so thirsteee
why helm and communities in the cloud
application context well communities
helps us manage containerized
applications in a cluster environment
that's that's definition we kind of all
know it so but its goal is to provide
better ways of managing relate to
distributed components across varied
infrastructure so what happens is that
in the context of your application the
one you're coding daily you're doing
constantly changes
to that and you wanted to be able to
sustain grow to have flexibility in
terms of application evolution business
needs because he's growing or disaster
recovery your automatic updates and so
on so in order to have more more good
things with your application you just go
ahead and add some cloud to it well we
have now cloud application and you're
doing nice code however you would also
like to have a pattern of doing the
deployment and this is where you're
doing DevOps so the idea of doing DevOps
is the way of patronizing the way you're
doing that develop the deployment sorry
but nervous it's a way of applying
patterns in the way you're doing the
deployment and the way the model the
reason why you're doing this is because
you would like to be able to have
multiple deployments per day or to have
a rapid feedback on what you have done
and how would that work in while
interacting with other environments yet
this might look enough however if you
would like some portability then you add
containers and docker and if your
application is having like multiple
micro services so you're using micro
service and you have a lot of those and
you will each of those you're using a
docker container it can become a hell at
some point in time to orchestrate all
those and you need some kind of an
algorithm
in order to have a good orchestration of
that however kubernetes comes to help us
and offers some out-of-the-box features
like auto healing and sometimes auto
scaling and other nice features and it
helps us achieve increo buying ourselves
with how exactly are we gonna do that -
to the ladder with the docker containers
and after applying darker and
communities and so on
there's one little pain that we have as
developers deployment with kubernetes
means making manifest files with
kubernetes directives a our community's
primitive or generic primitives and
while you're doing this for each release
you should model
by that file that deployment file or
multiple deployment files and this is
pretty difficult to do as a developer
and pay attention to that all the time
and this is why helm was invented so
that can help us both us the developers
and the operation people to be able to
manage all those deployments in a much
smoother way and did not pre preoccupied
on applying of values to each of those
deployments all the time and in today's
presentation I am going to take you
through application docker izing it
communities and then hope we have time
for everything so in every journey we
need some prerequisites before going
ahead and doing that journey so as
developers we need an IDE a platform as
a service account with kubernetes
cluster support and you have an example
in the parenthesis but you can choose
any cloud provider that offers you
kubernetes cluster support or you can
run it on localhost but it might take
some resources source code repository
account configured use your github or
any other source code repository to keep
your code there an account in a
continual registry and you can use
either a private container registry as I
have in the presentation today or either
your docker hub account optional a
docker environment running on your local
host and kubernetes command lines and
helm and Tylar install so the kubernetes
and helm command lines install because
I'm using Windows this is my set up for
today so I have the BX dot X a which is
the IBM cloud command line together with
helm and cue basically install and like
on my computer and the way I'm doing
things today is because I want to run
from a single command line all the
things I need to manage with kubernetes
in my cloud and all all helm and
everything else so all these trees
sitting next to each other it makes it
smoother for me as a developer to
interact with kubernetes directly in the
cloud so let's go through the
application setup now this is not a very
difficult application so from
application perspective it's a simple
Springwood application
that's managing the book acquisition I
didn't want to just go into too much
detail with the application and I think
mark in the morning has showed you how
to make a spring good application very
very easily so it just book acquisitions
with three rest controllers very easy
and typically for this kind of
application this small application you
would require some kind of a deployment
descriptor to run it in the cloud so to
deploy it in the cloud and typical
deployment descriptor would be as
follows for the Cloud Foundry based
clouds Cloud Foundry itself or IBM cloud
which is Cloud Foundry based and using
this manifest I can smooth the deploy my
application in the cloud so you can see
here I'm using a database service I'm
just applying where's the host name for
my application the path to the ward how
many is this is I want to run what's the
bill packet that I'm using and so on
however we talked about portability so
if we want to port this to let's say
another provider that doesn't know very
well how to interpret this one how do
you do it well first thing you would get
rid of the one that is not being seen by
the other provider and you can get to a
development scenario where you just get
the java application and you need
something else to help you in to
deploying it and this is where
containers come in action and can help
you so for today's develop a scenario
and we're going to have the book
acquisition application as described
earlier available in a docker container
which and the image of that of the
docker container is going to be in a
private image registry using a database
that's not inside the communities
cluster it's outside of the kubernetes
cluster it's in the cloud so it's a
cloud only be managed service by the
cloud directly and I want to be able to
talk with this service that's outside of
my cluster in a very smooth way and to
be able to question it and to store data
there and do all kind of operations
there and then I want my applications to
be here and in the kubernetes cluster
and I also have a smooth orchestration
of my deployment using hell
so let's dig in first thing that we need
to do for a containerized java
application you need a docker file so my
my example of docker file is as follows
and i would like to point out that's
very important have an official release
Java image us-based when you're doing a
docker file for for a java application
there are multiple official repositories
we can get an official release Java
image in my case I use the IBM java and
it's very good it's very important for
you to have it because you have to be
able to accept the license without
having any doubts on what you're
accepting there and that you're using a
general product then I'm just added my
coat in a wrapper just created I did
everything in my folder code and run LVN
clean package using little small hack so
when you're creating a Springwood
application from star dot spring dot io
you get an archive with the application
and together with that archive if you
get some scripts called maven wrapper so
those scripts called maven wrapper can
help you and when you're doing the
docker image and you can run those
scripts with ma with the following
command here in VN w clean package - P
war so this is like I would run ambient
clean package from my local environment
however I'm transferring this to the
rocker fall and saying ok use those
crews because they are there they're
provided when I'm creating my Springwood
application so why not use those and
build my war then I'm just simply
renaming my my coat into op war and
stating genuine entry point to the - how
to start my application when when the
docker file will be will be ran so
that's pretty much it the commands are
standard and the last part is inspired
by the guides of spring boot and for
docker and the first part is partially
mine because I've used some scripts
already provided
but it's a way of doing things however
if you plan on doing differently there
are some other options you can build
your application locally with the
unknown and the inkling package with
your profile you want it and do NBN
claim in style or whatever you would
like else then you can include the
result inside the image and that's it
and then you're gonna push your image by
yourself so you are gonna do a
management by yourself and what's
happening there now this is good when
you're starting your project and you
want to just get acquainted with the
technology and see how things are going
for you if you understood how docker is
working for you around the long term you
have to think how would a pipeline a
DevOps pipeline would transpose these in
order not to be bind it to what you're
doing locally so option 0 still the
better one but still option 1 is good
for ramp up option to manage your docker
image with maven there is an official
memory PO that gives you both maven and
an official release gel image so you can
take any of the official release travel
images and you can again run any and
clean package - P war and create your
java application and again the commands
are pretty straightforward and easy as
you would do it in the local environment
from the comm online you would say Jeff
o- jar app dot war
so you started from come online this is
how the docker image would be created
for you when this is when this is being
built and later push so these are just
options you can choose whatever you
would like for for your for your
approach personally I would choose
between option 0 + option 2 and said
option 1 would be just for a rapid ramp
up and then make it available in the
cloud so for docker image availability
as mentioned before the first thing you
do is have to build it and then push it
so you build your image and then you
push it to the image registry and you
can do this as many times as you want
so however is develop
or you might get tired I'm doing this
constantly because maybe you're doing
some mistakes in your code in your java
code or you're doing some mistakes in
the docker image god knows it can get
tiring to just go and say build push
will push all the time and also you
would like to interact with the cloud so
by interacting with the cloud I just
used bxc our build - tee and this this
command just does for me build and push
in the same time from the come on line
so I am building my daughter image but
I'm also pushing it to my private image
registry because I don't want to do
these - all the time I just use this one
I use what the cloud gave me so I'm
helped by my side of the cloud and then
I can inspect if everything is ok with
my image and it's there just to check
it's always good to check your things
then you think okay I want to automate
my work so to automate your work you
would create a script that I want or
wrote to and you would source it in your
pipeline so this means that you're not
going to execute manually all the time
you need to do a deployment you just got
a source which you created before with
those two commands from upstairs or just
two from here and this is how you could
automate your working you can put the
pipeline to work on each commit that
you're doing an itch bush that you're
doing you are just continuously running
your updates so there's no need for you
to run manually those things and to
update those manually so this is a quite
neat thing and it gets rid of a lot of
pains from from developers perspective
how much work will be needed in order to
maintain an up-to-date docker image then
next thing after making an application
making a docker image available would be
communities and deployment
just a quick history for kubernetes and
some communities concepts so right
typically in a kubernetes cluster like I
have here example what you have two
types of nodes you have the master node
and worker nodes and when you create a
kubernetes cluster you are deciding ok
how many worker knows do I want to have
my cluster this is when you're saying I
want a three five ten one and the
masternode is very important because it
places continued workloads into the pots
of the worker notes so the master is
controlling the workers as everyone
knows and how is this things happening
is because we have the API server which
is the management hub for kubernetes
master node but not only so the API
server is the mastermind behind managing
the entire cluster and this API server
is using the etcd which is a data config
for free communities and there you can
store all the configurations related to
your cluster this is like a dot
properties from Java but transposed to
to kubernetes and then the controller
manager is the one that is ensuring that
the cluster desire state matches the
current state of of the of the of the
cluster by scaling workloads up and down
so this means that the controller
manager is continuously checking that
your state that you're having now is the
one that you intended when you did the
deployment and when you are going to
describe how your pods are doing and
describe how your services are doing
then you're going to get some
informations in responsive like current
status and desire state so you're gonna
see both what's is now and also the
desire state and desirable is to have
the same values in both so current
states to be matching the desire state
and this Control Manager is saying how
is the state going to the API server and
communicates directly with the API
server also the scheduler is the one
that is assigning were close to specific
notes in the cluster so these two as you
can see here are directly talking with
the API server then the API server acts
as a translator to the worker notes or
to the Kubla that's here in the master
note and translates those commands to
the cubelets cubelets are the ones that
are receiving PHA specifications from
api server and managed pods running in
the same host so one thing I wanted to
mention to you is these two pots share
the same host so every time that this
cubelet receives informations our post
specifications from the api said what to
do something it will it will manage
these two only so noting that there is a
mixture of cubelets randomly in the
kubernetes
and since we're talking about pots both
are top level api objects that are
running containers there are the
smallest compute units and communities
and pause are actually containing your
containers your talk our containers and
these pods are helping you to have a
group of co-located containers inside
them you can associate volumes they have
a unique network IP and they have a lot
of options on how to govern the
containers from inside them then these
pots can be filled with containers by
using deployments deployments are those
that are used to schedule pots with
actions so you can do reusing
deployments you can have a replication
management and this means in in terms of
kubernetes to have like to have like
ensuring to ensure that the right number
of nodes and containers are available
for the requested workload then to have
pots killing or rolling updates roll
back to previous version so you can if
the current state of a deployment is not
stable you can always roll back and go
back to a previous state when you were
stable or you can scale the deployment
to facilitate more role mode you can
also clean up all the replicas that are
not needed anymore so you have replicas
that you don't need
more in your environment you can just
clean those by using certain policies or
you can pause or resume your deployment
to apply multiple fixes in your pod and
then resume it to start a new rollout so
it's like okay I just realized I've done
something wrong on this deployment I'm
not gonna go through everything I'm just
gonna stop it and then go back to my to
the state than where I was when I
stopped it and another thing that's very
important for us for developers is are
the services so the Services offers a
stable of virtual point to access what
is in the pot so we have containers in
the pods and we need a way to access
those containers and services are a way
of knowing what we have there so the way
to access our deployed applications and
in order to get to services we can use
selectors and I'm gonna show it to you
when I'm going to do the deployment dot
yml before going into deployment we need
some communities pre deploy stuff so we
need to create a cluster verify it and
access it and I'm gonna show you some
command lines some commands for the
command line but you can do it also
visually from the catalog from a web UI
so a typical command to create your own
cluster in IBM cloud would be BX es
cluster create followed by a lot of
options but the most important ones are
the name the cluster name and the number
of workers so how many workers will you
assign to that clusters to that cluster
and then you would verify the cluster by
using the X clusters are in a VX ES
worker so you just query if things are
okay with with what I just created and
this is way to change sure that you have
done the right thing if you're working
with the come online and then you can
access it by using VX es cluster config
either this all right there Cuba Italy
config use context for the cluster that
you have just created so you need to
position yourself inside the cluster
from the command line in order to be
able to control it with kubernetes
command so this is meant to interact
more with it directly from the command
line from
from your localhost and also this these
kind of commands are useful to be doing
to be done in scripts for the cloud put
into the very pipeline and then you just
set okay in my case because I have
windows or exports the cube
configuration so I'm just placing myself
okay I want to use this cluster having
this configuration and then I can just
go and see what's how my cluster is
doing to a dashboard using Cubase Italy
proxy this is pretty much it with doing
the clutter with um updating with
creating the cluster then in my scenario
I was stating about an application
application that is being somehow able
to get data from a database that's in
the cloud so it's not in the cluster and
in order to be able to communicate with
that database that is outside of the
cluster what I am doing is that I'm
doing yes yes the xes cluster service
bind so I'm just binding my cluster to
that service why I'm doing this because
I want to be able to those two see each
other and to know that they belong to
each other or actually that the service
belongs to the cluster and the Kuban a
space is very important because in a big
cluster where you have multiple worker
nodes there will be the case where
sometimes worker nodes certain worker
nodes shouldn't be able to see an
external service so they shouldn't get
access to that service in order to
ensure that you do not get access to
that service you just put put the
namespace so the namespace is helping
you to separate the things that you have
inside the cluster and to be able to
ensure that you're not accessing things
that you should not shouldn't see from
from those from those worker nodes so at
the end of running this command I get a
secret as a result of that so community
secrets are used for handshake between
what inside what's inside the cluster
and what is outside the classroom in
that case this is a kind of secret that
it's going to help me to handshake with
the database and say from the
application yeah I want to use that
application Bey
on the information I have in this secret
I am allowed to use this application
because I know the contents of this
cigarette and for the actual community
deployment I need to change a few things
in the application code so one thing is
to set the mounts to set the path to the
mounted secret so you would need to
specify where is the path to the mounted
secret and how do you know where is the
past and mounted secret well I was
already sure I will show you shortly
you deployment that well how it's done
and I will show you where I'm mounting
my secret so you could see where the
path exactly is but inside my Springwood
application I just really wanted to make
sure that I am seeing the secret then I
simply inject our config it depends on
how you're using the configuration in
Springwood I just simply inject it with
that value and I specified that path
there then for reading the secret it's
easy you're just doing a file reading
from from somewhere that we were
specified the path so this is pretty
much it from for reading the secret
however if the secret is encrypted you
wouldn't need a few more things to be
added and when the Java side yet it's
nothing extraordinary or difficult to be
done from a developer's perspective in
order to get handshake with the service
outside the cluster then the deployment
yml it needs two parts the application
description and the service description
and this we got here to the deployment
time and let's see how it looks like
okay hope I can see this delete it I
don't like it okay so it has two parts
one of those is the description of the
application so this part here is what
application I'm going to deploy in my
case is called boot PMX timo and I'm
gonna use just more replicas for this
application I don't need more and the
kind of deployment that I'm doing its
deployment so you can the kind of
specify to the deployment of the
application can be either deployment or
can be stateful if you have a stateful
application so deployment is equal State
applications I'm gonna deploy a
stateless application in this case and
then the name of the application is
pretty standard book vmx demo and then I
have a service part because after I will
deploy this application I want to be
sure that I can go ahead and access it
somehow and I need to define a service
on on which I would know that if I'm
accessing that service in my cluster
from from another application that's in
the same worker I would be able to go to
that and and see how it behaves so in my
case I just said it the same name as my
for my application so I said okay I want
a service bubi MX demo - to be given to
to expose - to the rest of the of the
neighbors from the cluster so these two
here and then I need to specify what are
the contents of my deployment so I need
to specify the containers for that and
then specify the containers I would need
to specify a name for that
I said the same as in my application an
image so I'm saying directly where is my
image located so in the private image
registry in my registry namespace and I
have the image name with book mia max
Nemo and the tag 37 it can be any number
here and the image pool policy is always
so this means that all the time that I'm
going to do a deployment I want for the
image from the registry to be retrieved
from from that area however the
application deployed there as mentioned
earlier is using a secret specified
somewhere with a path so enable - in
order to be able to use the path you
have to mount some volumes and to mount
some volumes one second yeah you would
use this volume mounts and and you can
use this it belongs to the city
container and you're saying the mount
pot and you can say here whatever you
like so it can be
Jiji if you like or whatever the idea is
that the mouth path is going to be used
by your applications so you can set
there whatever you like but be sure that
your application is going to read it
from there so this is like you are
creating your folder your virtual folder
where you are going to read this used by
only by your application because the
secret is there in the cluster there is
no need to preoccupy yourself of
thinking mate how we're did in the
cluster it was put there I wonder what
folder what whatever no this is
something a to manage so for the sake of
it I'm just gonna go back to to what I
said here and then I said service name
service buying volume then I have to
specify these volumes here and for
volumes
sorry for that it's difficult to see
after this battle okay some bound volume
yeah that's what I was looking for so
make yourself green ah perfect so the
volume mounts and the volumes I have the
same name as specified in the volume US
service bond volume again this is
something you can also define as you
please so it can be gg again if you like
it but all the time make sure that's
gonna be gg here too so they would know
about each other so this one volume
mount is referencing what is actually
containing in the volumes so please be
aware to have the same name like
otherwise you won't be able to see the
volume itself and then you were just
reading the secret name in my case I
know that the secret name is this one
and because I when I have created the
bxe s service bind to the cluster then I
have obtained as a result this name here
binding book - DB - class and this is
how I just managed to do you know which
secret is what's name of my secret and
later use it in my application and then
this is pretty much it though so this is
like this is a pretty valid deployment
at yml it's important to be green all
the time I always have this issue here
with the X but it works trust me and I
can show you the application up and
running
however the hard part of doing these
things is because you always have to
make sure that you are indenting
correctly and for a developer that comes
from the object-oriented world it might
be a little bit difficult and since I
don't like to do this often and
repeatedly again I have created a script
for that so in order to deploy that
manifest I don't want to do all the time
- brynee Keys command
from the command line so what I would
what I did was to make my own script of
cutie pie that's eh and I simply apply
repeatedly my configuration from deploy
that why am L using the Q basically
apply command so apply what it does it
takes the configuration specified in
deployment that why Mel and it's
applying it repeatedly to what I already
had if I already had something or is
installing using that configuration then
I'm checking how my deploy service is
doing because you always have to check
if things are working for you so one way
to check how your deploy services were
is doing is to describe the services
called boot BMX demo hopefully is going
to be only one book vmx demo and this is
how you get what happened with the
service that you deploy so if you were
going to get some details on how things
are working after your deployment and if
you can't access that service that you
described in deployment that yml then
you can't see how your pods are doing
and how is the application inside your
pod doing by simply issuing to basically
describe pods - - selector so you can
use the label given to the application
boot PMX demo in the deployment dot yml
in orbit in order to be able to get how
your deploy pod containing that
application are doing so this is how you
can see if they're healthy or not if
they're struggling to scale or not and
this is a good way to ensure that
everything worked as intended when you
did the deploy so this this way of doing
things with key BCD apply - eff
deployment at yml is called the
declarative app management of doing
things with kubernetes there is another
way of doing things the imperative way
of managing your application with other
primitives with other commands actually
and it takes more commands from your
side as a developer for me personally it
was easier to use just the cube ECT
reapply - if deployment at yml and
simply all the time I'm just updating
the contents of my pod with what I
already developed in my application
this is pretty much it so I am done
everything is okay
the dew point one works the application
one moment it's some round here wait a
minute
yeah it's kind of working so it would be
pretty much done however on the
long-term this is not okay for a
developer because you need some kind of
flexibility in order to maintain the
deployment at yml you don't want to go
all the time and update the values of
the tag of the registry that you're
using of the image name and all these
things and this is where helm comes to
your rescue so home charts represent
packages of pre-configured communities
resources you could say there are a
bundle of communities resources in or
they're existing there to help you to
describe better what you can't what
you're trying to do in terms of
deployment with kubernetes so how charts
representative weekly anarchy of files
containing a package description the
chart dot yml file here and templates
for doing the deployments so this is
what we refer in general to help charts
and these templates are very useful
because they can receive multiple values
and you can change the values much more
easier just using templates you're
applying patterns again to the way you
are doing the deployment this time and
this the developer perspective however
from the operational perspective helm
means to have two parts it means to have
a client helm clients every time from
your command line which when you're
issuing a home command like helming it
can create or any other command you are
being the client and has a server called
tyler and the server typically is where
your car is in your cluster you can have
a cluster you can try to do it on
localhost also and you have the tyler on
localhost however it's not very
productive yet in the cloud what you
would have you would have a client
install also and the tyler
installed so the client is gonna issue
commands helm commands able to be
interpreted by the Tyler and later on
being able to work with deployment dot
yml manifests of cuban it is much easier
and do the actual deployment so these
two components are actually helping you
in terms of deployment and to speed up
your things I am important from the
development part of Part A of looking at
the things and also to help you have a
more organized structure of your project
so let's see some help concepts that I'm
gonna use further in the presentation so
I'm gonna hit run I'm gonna use the term
of release so in terms of release for
your helm it means the chart instance
loaded into kubernetes each time that
you're doing communities deployment or
actually each time that you're
installing or upgrading the charts you
are doing a release repository means
repository of published charts and there
are various published charts by vendors
describing their products so vendors
that make the databases or for example
open Liberty server is being available
on them it has published charts in order
to be able to use that in your home
cards later on and to be able to build
that more easier so you cannot you would
not require to do that from from zero
then the templates as explained earlier
they're CUNY's configuration file values
so values is going to be the place the
file where you're going to store those
hard coded things that you saw earlier
in deployment at yml and it's it's more
simpler to maintain only only one file
for every temp for all the templates and
optionally you can use requirements the
requirements i'm going to show an
example later is an optional yml file
where you can list dependencies for the
chart so for example if you'd have an
application that requires a server
that's also described by a chart you can
list the dependency to those
charts in the requirements of yml file
then for the hell transcreation it's
easy you can just do he'll create and
the name whatever you want and you just
get this therapy here with all given
containing values at yml your values
your this package description as
mentioned earlier and now what you saw
earlier in deployment at yml yet
splitted in two parts
so the deployment itself concerning the
application where you specify the
containers and the volumes and so on
it's going to get here in the planet yml
the service to which you can access that
deployment at yml it's going to be in
the service that yml file so you're
splitting the concerns and the ingress
of my mail because now we haven't talked
about ingress so far well the ingress at
yml file is a file to configure the
routes of your application because
you're not gonna in a typical
application you're not going to access
an IP with a port
you're gonna access a route a hostname
you're gonna have something more
beautiful they're not just a simple IP
with the port and get into that
application so this is pretty much it
for the templates and also charts folder
if you want to reference other charts in
your project or just to put additional
charts in your project and then you can
check those charts by using the helm
lint command this is a pretty simple
command it's not doing a lot of check up
sometimes you can get some errors when
you're using it so it's not always
validating a hundred percent which you
have wrote in your helm tarts in terms
of templates because as you will see in
a moment I will alter my health charts
to fit my deployment and this is pretty
much it let's go to the command line for
a moment okay
hope everyone can see ya
so help create
manus acquisition and that's seem very
well
oh good yeah and going back to here I
should be able to see a new folder
yeah the book acquisition folder so here
I have values this is auto-generated
when you're executing help create and
you got all the templates here and
everything that you need also
description of the chart some being
pretty pretty easy and you can put here
also maintainer who maintains the charts
and so on if you want to be able to
track who did that chart then for the
values part I'm not using this then I'm
just gonna go and delete these things
values yeah so these are corresponding
values to my deployments so again I'm
just stating here about my red image so
my image is being obtained from this
phone from the following repository of
unfollowing repository what tag I need
to use what pull policy I'll use so as
in deployment at my email I'm gonna use
always if I need any secret to be pulled
the secret name for my database and
again some description for my service so
what's the name of my service what type
of service I need external or internal
port and then this is pretty much it
from the values perspective however
later on when I'm going to go and
interact with these for example in
service dot yml here I'm not changing
anything because I already have
everything up to date so it's already
done for me by going changing it's just
taking the values from what is already
pre-configured
in the values that why am L so going
with this forward is pretty pretty okay
for me now the deployment that's why I'm
out file
this is good also however if I look to
the specifications for the containers
well I do not see the volume amount so
it's here here is where I have to add a
few things so first of all I need to add
a few something regarding theme okay to
the volumes and I'm using again values
specified in the values that yml file so
I'm just putting a template here okay
just go in values at yml and read the
property secret name
or that you can see under image so it's
pretty nested the way you can read into
the values of values that yml file and
then I'm gonna use this volume from here
going under the image pool policy okay
come while I'm out here and this is
pretty much it I haven't had time to
change these mouth paths and the name
here because I could have provided this
also in the values at yml and not her
coded here so this is a bad thing for me
and this is enough to describe my
application deployment later on the
helpers dot TPL are actually the
templates there that but I would need
later on to replace things in the charts
and the ingress as I mentioned earlier
is the way of defining the routes on how
to access your application so this would
be just enough to describe the book VI
maximum application however I also
mentioned to you about the requirements
file so you can think of the book VI
maximum application as an application as
that's being it's exists just as a
back-end however if the front-end would
be in another place in order to do that
I would need the requirements that why
am L so
okay and the requirements yml for me it
looks would be like this first of all I
want the backend to be first and I'm
gonna go here come on and represent for
here so I'm saying that okay for the
backend of this application that I'm
trying to deploy with helm what I'm
doing is I'm taking a description of
this from from the from charts and I can
just go and copy paste the contents of
charts or had the values previously used
so what i'm doing here i'm just using
charts with different values and also
with values dot local if i want to use
the certain values only for local and
then I have the front end at another
github repository and say that I was
also described by helm charts and I can
put these these two together when I'm
doing the actual release and they're
binded together only by these
requirements at why a male and in the
templates I would not require any more
this deployment and service because now
I'm just splitting the concerns to the
to the requirements at yml and I'm
taking those based on requirements at
yml yep and this would pretty much be it
and of course of the values here I don't
need those values here because what I'm
going to do is I'm gonna say okay make
me two deployments one with the front
end and one with the back end and then
one with the front end and bring them
together so these values here I don't
need it anymore to exist only if I would
need something globally declared now use
it here and this is pretty much it on
how you would do to bind things together
and use the requirements just yml to
gather your things or also as I listed
here you can use a repository already
published charts as dependency and add
those to you to how you're building
things then you would require this one
first if you need the Liberty server
first and then put the back end and then
the front end
this is it with the with the helm from
from more elaborate scenario of doing
things let's go back a little bit yet
okay we know how to make charts now we
have to deploy them so first of all you
need to configure the tiler for
configuring the tiler the commands would
be first - helm in it so helming it
without - - upgrade would actually do
would actually stop both the client and
the tiler for you so it put the client
and the tiler in the in the cloud
however - - upgrade i'm just force i'm
just upgrading if Tyler is already
present I just want to make sure that
things are there and then I'm gonna
check how is my Tyler doing my Tyler is
a new pod and what I'm going to see I'm
gonna check in the status of that pot by
using QB CTD rollout status and check
wow house this house this Pilar doing in
the name space queue system and I should
see this up and running is okay then I
just make some additional checks on how
version or home listing the name or held
listing previous releases in my in my
name space and the cluster and this is
pretty much it so this is what this
would mean configuring the Tyler then
also deploying it would mean issue just
a part of this command held upgrade - -
install a release name whatever you want
and you just say hereby book DMX demo
this is the chart name or book
acquisition in our case whatever just
Drew cynically created however because I
sourced I have this script source in the
pipeline I have just I'm just putting
these as variables and I'm replacing
them at the runtime so and also I have a
deck here - - said in order to show you
that if you have values already
specified in your deployment however
you've changed your mind about certain
values from your file and I don't know
maybe it's too late on what you have
done you can just go ahead and do again
some setups on your values so you can
just go and say - - set the image
repository the tag or whatever
L
you have any values that yml and
overwrite the contents of your values at
yml while you're issuing the helm of
great command so this is a way of
escaping from deploying something that's
I don't know maybe stole may be the
values that yml contains values that are
not already up to date and you just want
to overwrite those and in the instance
without going and modifying the values
of my round so this would help you in
some difficult moments so this is about
deploying
and all the time I'm saying he'll
upgrade because I want to be able to
upgrade my application I'm doing changes
to the same application I'm not doing a
new installation of my application so
this is why I'm using helm of grade and
then check my deployment with helm
status and I'm just checking the release
name and then I'm just checking the
classroom availability and getting the
address and how the application is doing
to specify the IP address and port so
this is the basic then you can just go
further more and inspect which hostname
I can find the application and how I can
get access to it based on what I have
defined in the egress ingress that yml
some respective and some key takeaways
so the journey with respective consists
of well you have a code repository you
check that you have darker installed and
then you build your docker image and
then you are storing it in a darker
image registry I have also some built up
properties stored in my environment in
archive there and then i validated my
image in the pipeline using a
vulnerability advisor service this is
particularly - my scenario into the
cloud so this is why I haven't described
it so far this is just a way to ensure
that I am securely having that image
there and some checking against security
some security better is that the image
is OK and then I'm doing the helm deploy
with a pretty boy job where I'm just
configuring the tiler and then I am
deploying the job I'm deploying
everything using
the helmet braids - - installed command
so this is pretty much it and some key
takeaways that I wanna share with you is
that from my perspective as a developer
it helps a lot to have kubernetes
deployment to be so agnostic of the
programming language that you're using
so my front end and maybe I don't know
maybe it's a no GS application so I have
the front and running separately with
node.js application yet it doesn't care
the the deployment at why Mel does not
care on the underlying language that I'm
developing the application so this is a
helper for me as a developer however
navigating to generic primitives in
order to make a deployment with a little
bit difficult time at the beginning then
there's a good process relation in terms
of what you're doing in the deployment
so you have everything in pods
everything very structured now helm is
helping as developers to maintain more
easily the kubernetes deployments by
using templates to those deployments and
you can extend the multi environment
capabilities of kubernetes by using
multiple values at yml charts so if you
want you can apply instead of helm
upgrade - install you can say - F and
you supply a different values local dot
yml if you are working on local and you
would use those those just for yourself
on local and any other type of values at
yml can be applied based on the
environment that you are trying to
deploy as a personal recommendation is
not to track secrets in your revision
control so one thing is that in order to
be able to replicate how your
application would interact with the
secrets you'd have to somehow mock that
secret for you to be able to test with
it so if you're making like a file with
secret that would simulate the secret do
not put that into the revision control
then plan a good distribution of
services across cluster so plan very
well what you're gonna put in which part
and which worker based on who should see
what from your artifacts
create a dedicated chart per code
artifact produced so this especially for
those of you that are making
micro-services so make a chart because I
did with help create for each artifacts
of Java or other language that you
produce and you can as again manage
environments with multiple values with
my amount files and this is pretty much
what I wanted to share with you today I
want to thank you for listening to me
and if you have any questions not sure
if we have time right now but you can
find me here today and tomorrow and we
can debate more thank you very much for
listening to me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>