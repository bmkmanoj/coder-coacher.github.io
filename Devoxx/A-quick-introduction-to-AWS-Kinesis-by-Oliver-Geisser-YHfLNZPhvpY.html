<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A quick introduction to AWS Kinesis by Oliver Geisser | Coder Coacher - Coaching Coders</title><meta content="A quick introduction to AWS Kinesis by Oliver Geisser - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A quick introduction to AWS Kinesis by Oliver Geisser</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YHfLNZPhvpY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome welcome to my talk a quick
introduction to AWS kinases streams and
I will give you a high-level overview
and then show a little demo and kinases
kinetise is part of so called Genesis
platform family and they have three
members of this family and the first one
is Knesset streams this is a original
Kinesis before there was a family the
family is rather new so this is
available since last year a new member
of the family can SS firehose was just
announced and is available since october
of this year and there's also an
announcement for a third member of the
family
the analytics in this talk i will focus
on kinases streams which is kind of
think about this as a managed calf calf
cluster and it's not really cup Kafka
it's an popular tarry build system from
Amazon itself and but it's very similar
from the from the point of view of a
high-level concept okay what do you use
this kind of software for and to
implement what kind of use cases and for
example you have web server click stream
source you need to collect a lot of
events that events record in the in the
Apache log file and you need to store
this somewhere before you start to
analyze it and then to present
personalized subject just suggestions
out of these analysis but all similar
use cases can use kinases so the overall
high level architecture is that you have
an ec2 instance or a client application
or mobile application all these kinds of
sources can write into the MS and
condesa stream this stream is subdivided
into so-called charts the shards
partition the data and then on the other
hand you have consumers which read from
these stream read from these charts and
process the the records inside of the
shot so let's get into the details into
the concept a little bit more detailed
the stream stream is a named event of
data records the data record is usually
stored for up to 24 hours but you can
increase this retention time up to 7
days and the data inside of the stream
is partitioned and the data record is a
unit of data stored inside of the stream
and data record contains of 3 parts and
the data blob this is binary blob you
can put anything you want into it and a
petition key and a sequence number so
what is a partition key a petition key
is assigned to the data records by Z
data producer so by your application and
it's used for partitioning the data
across shots how does it work it's
defined an md5 hash this is calculated
by kinases and then this dear Gemini's
chart and the data record goes into a
sequence number is assigned by kinases
on right so if you need your business
application level identifier you can put
this into the binary blob but in
addition to this Genesis also creates an
eunuch unique identifier for every data
record you write into the stream and the
chart mentioned before it's a group of
data records inside the stream its
stream is composed of multiple charts
it's partitioning the stream and you
scale the stream by adding or removing
charts
what does it mean it means each chart is
not only a logic construct but also a
unit of capacity so if you do not have
enough capacity read or write you
increase the number of charts inside the
stream and one chart is ingest as able
to ingest what up to one megabyte per
second or data divided in up to 100 1000
records per second on the read side you
get for every chart 2 megabyte of second
2 megabyte per second of in egress
traffic so you can read doubles the
amount of data volume as you can write
so a little demo because I think this
would make
sings more clear
that's a problem
that starts again okay so um to thank
you and to help me put two executors a
demo I've prepared some small shell
scripts which guide me through this demo
so first step is to list available
streams and as expected it's empty you
see I'm using the AWS command line two
forces but of course there are multiple
options and ATI is higher level api's
and so on but this shows the command
line tool has the ability to show the
actual data on the HTTP level so this is
a json encoded data and like you see
this JSON array is empty so there are no
streams available and so we create a
stream the stream gets a name this kind
in this case de box and a number of
shards and I can increase or decrease
the number of shots
afterwards but at the beginning and
needs to decide what should be the
initial capacity so I can then sorry
okay sorry for this by saying this okay
thank you and what you then can test is
to describe the details about a stream
and you can see that it's already active
the stream name de box like we ordered
it and one
with an average chart itself gets a shot
a deep inside of the stream and you can
see this H key and I explained before
it's a hash key of your partition key
determines the chart so we have only one
one chart so this goes from zero to the
maximum hashing hash value so all
partition Keys goes into this only one
chart and what you also can see it's a
starting sequence number so this would
be the sequence numbers first data
record will be signed and as you can see
this is not starting with 1 or 0 but
it's a very big number and it's not
considered two consecutive about it
increasing so then let's put some data
into this stream and of course you can
put much bigger sizes I think it's one
megabyte or something processor for the
binary blob in this case I'll just put
the string hello and the current time
and let's put a second data record
inside of this hello dear Vox kinases so
we have now put 3a data records into our
stream and the response is always the
shot where the record is put into and
the sequence number and you can see the
sequence number is has increased but
it's not 86 87 88 but there are some
gaps in between them and now let's get
the data back and to get to get a data
back we need to
have one and step to do which is to get
an iterator the iterator is like a
cursor and it's about its temporary so
it's only its lifetime is limited to
five minutes so you need to always get
new iterators and you can see the houses
works and if I proceed because I need
this iterator I will save it in a temp
file and then get the data based on this
currently saved iterator and it is you
can see I have now written read from the
from the stream three data records and
as you can see the data it's a little
bit cryptic because the the data is
base64 encoded and the library the API
library will do it for you but in this
case I need to decode this and if I put
this here then you can see this is my
original data record if I and what you
also can see is that I always receive
the next chart iterator so if I now put
this into my temp file and I get data
again then I start at this position and
the response is an empty record set
because after these last iterator I have
not put new data records into the into
the stream last step would be to
Oh delete the stream and then you can
see that now the status is deleting and
after a little bit of time 1 or 2
minutes then the stream is gone
ok so some closing remarks if you want
to use
kanessa streams understand consequences
of the limit The Shard is a unit of
capacity this means there's a
dependencies between the number of shots
the number of consumers the latency for
every consumer and you need to calculate
and to balance depending on your use
case and also decide if kinetise is a
right tool for you you have to trade off
you have a lender login because this is
proprietary cloud service and and but
it's managed it's completely managed and
alternative would be to host and operate
and manage your own Kafka classical
cluster but if you have done this in
production to manage your own Kafka
clusters is a lot of work so this is an
easy scaleable 24/7 always on service
but you need to get to keep this in your
back of your mind
it's a vendor looking and as last point
is choose the right access library I've
used a command line this is just for
demonstration purposes there are
different levels of clients you have a
direct api's low level you have higher
level api's which supports multi stretch
reading coordination in a distributed
cluster scenario and multiple party
connectors for example to
Oh to help or to acha streams and stuff
like this okay I'm done
thank you and if you have any questions
I will begin</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>