<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>In-memory data revenge: the substrate between apps and data - an Infinispan case by Emmanuel Bernard | Coder Coacher - Coaching Coders</title><meta content="In-memory data revenge: the substrate between apps and data - an Infinispan case by Emmanuel Bernard - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>In-memory data revenge: the substrate between apps and data - an Infinispan case by Emmanuel Bernard</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-ZxorLP_ROQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone thank you for being here
today we're going to talk about Hin
memory data the systems in memory data
systems and what I'm going to try to
convey to you is two things first is the
whole idea of data at rest and your
application accessing that data and then
you know just when it needs it and then
leaving it there it's kind of gone and I
really think that data is going to be
flowing from one system to another being
transformed and so on
ok so first one is data is flowing and
you need to get used to that and the
second one is I'm trying to I'll try to
show you where in an architecture you
can make use of those in memory systems
and you know benefit from them and some
use case much surprised you some use
case you might be you know very familiar
with them so my name is Emanuel Bell now
I work at reddit historically in the
Infinite Earths or a hibernate team for
pretty much forever so working on the
JPA implementation historically and
founded hibernate search hibernate or GM
- invalidate - and so on and right now
I'm kind of looking at all of the data
projects we have inside Reddit and JBoss
middleware and make a common vision on
what we think you know things should be
going and I'm extremely interested in
the feedback for that presentation like
why did you like what did you not like
what you wish you had in that
presentation that is missing so use the
my devoxx application to give me
feedback on that of course the notes is
always interesting but the details is
actually what I'm you know also super
interesting about so we all know that
with getting we're getting more and more
data and we have to deal with that and
the traditional way to do that at the
moment is to say hey let's create this
big data lake and pour everything into
it and we'll see about that later
my I think the way things will go and
should go is a bit more
and more like my data is flowing from
one system to another and is transformed
and I sure I love to keep track of this
what we can what is called data line
edge the evolution of that data and it
won't really be addressed in that talk
and the other stuff is since this data
is flowing I can use many systems to
play with it so let's try and use the
best one for for the job and here is an
example it's not the only one of course
but it's something we call insightful
application at Red Hat and it's really
about the interaction between analyze
analytic story and application so in a
traditional universe you say ok I'm
pouring some data to the data scientist
is going to do a job and then say ok
here is my prediction model and talk to
the app developer and good luck right
the model where we want to go at the and
the end of the day is more like let's
try and make a continuous improvement
loop let's just make one application
that happens to have analytics and not
not be two separate things and to do
that you first both at runtime to
actually run once the stuff has been
designed both both at one time but also
at design time design time being the
data scientist exploring stuff you need
to be able to access the data so that's
already political fighting there's
nothing really I can do for you on that
but you will access value status
services the relational database you
know in memory system and you know no
SQL store whatever but also streams of
events happening your logs click streams
maybe you store stuff in Kafka in a you
know application-specific fashion and
you want to then extra gain so all of
that needs to be accessed and available
to the data scientist and then the
system when it's flowing and then the
data census needs to clean the data and
make sense of it so it needs to explore
things and that's where a system like
apache spark is quite interesting and
quite powerful i'll discuss that a bit
later but also you need some kind of
temporary storage of sort to say okay
i'm computing
stuff let me stall that temporarily
because then I will build another stage
of my computation on top of that and
once you think you figure out some kind
of interesting patterns in your data you
need to basically go into prediction
mode know that I think I understand the
past let's do let's do and make some
prediction so for example categorization
of users this guy is you know once the
latest stuff is ready to pay a lot of
money this guy is very you know price
sensitive and so on so you categorize
people so again you you take the well
first of all you build that model and
then you take the data you have and you
apply the model on the data you have and
then you've got the categorization the
prediction on the data and that
information needs to be extremely needs
to be accessible to your actual
application which is the last bit here
the act and do it in a very fast fashion
the application displaying a webpage
doesn't have to I mean shouldn't have to
do a lot of computation to say ok give
me the most interesting articles for
this person versus that person it should
be already pre-calculated and access in
our you know millisecond fashion and of
course an application doing things and
and getting input from user will
generate more data and that's the
feedback group that we're talking about
okay today there's a lot of manual step
going on and you will see it right at
and it's a bit beyond that the scope of
that presentation but we're trying to
squeeze that into a shorter cycle and
make that easier for you so from now on
I'm going to focus on what in-memory
systems can do for you and are using
finish span as an example well a because
that's the one I know and be I think
it's awesome I'm totally objective on
that I'll I'll give you a kind of a
definition of sort so in memory system
are usually key value store very
important point is that they are
distributed so the data well you got a
virtual memory of many servers spread
and the data is replicated enough so
that if you lose some nodes life still
goes on and it's elastic meaning I can
add more
service is if I'm getting filled either
by computation or by too much memory
being used but of course also going down
like scaling down because I don't need
that anymore and we'll talk about
Skinner a bit later and the other one
which is represented by the pictogram
here is that it's primarily primarily in
memory so it doesn't have to be a
hundred percent in memory you actually
do have overflow on these core another
back-end system or whatnot but a lot of
the benefits relies on the fact that the
data you need to query and access is in
memory so that the access is that worse
one network trip away from from you so
the next steps in in the in the
presentation are going to be I'm going
to describe a little bit the use case
you know a system can address then I'll
dive into the search capabilities which
I hopefully if you're not super familiar
with in memory system will surprise you
by what they can do and then we'll go
back into the use case but from an
architectural point of view and say ok
if I've got that system what can I do -
you know accelerate things and how can I
use the in-memory system in that so use
cases so of course the the one you
probably are aware of is caching so
you've got data your back-end is a bit
slow so you want to temporarily store
the data it could be actually next to
the app like within the same memory
literally a hashmap you stop the data
here and then you've got you know faster
access and you release the pressure on
your back-end as well so that's kind of
interesting it could also be not that
the data is slow but it's kind of hard
to compute on you don't want to have to
compute too often so you keep it in
memory as much or as long as you have
you know free memory that's easy the one
that is a bit less known is there is a
lot of data well assuming you have such
a system there is a lot of data you
could consider a temporary it's not that
the data is not important and having the
failover capability is actually a useful
pattern but it's the data is either
changing quite
turn or is useful only for a small
amount of time and you want to access it
in a fast and efficient fashion so
storing it into a relational database
with a very complex model etc might not
be the best fit so here are pictograms
you know httpsession
or a shopping cart we've got actually
one customer using that I mentioned that
in a architectural diagram a bit later
but you remember the arc the analytics
approach where you need to take the
model take your data apply the model on
the data and then the output of the
categorization needs to be accessed
extremely fast that's another kind of
temporary data and the data needs to be
shared probably across different servers
potentially even different apps at least
several versions of the world several
income instances of that app and of
course the final one is the grid so here
you actually treat data grid as your
primary store and you can go quite
deeper in the way you do queries and
because it's everything in is in-memory
you've got pretty a pretty efficient way
to are at least a fast way to do that
you can literally do distributed
computation next to the query and do
interesting things and that leads to the
analytics but I'll go into that detail a
bit of it after so far so good ok
silence is constant I guess let me dive
into search capabilities so I'll explain
of course while in Finnish band does but
a lot of the other products in that
category do some of it maybe there is
something that I've missed that is not
there but generally speaking you know
pretty much all of them do that and the
first one is kind of the elephant in the
room is if you don't have to do a query
don't do it right if you know exactly
that the kind of operation you want you
want to retrieve that data based on that
information prepare it that's gonna be
the fastest right and that's gonna be
the easiest it costs you at the right
time but then it's done it's very it's
very common pattern in secure
I swear the response side of things is
actually ideally precomputed to that and
it's super easy so the way it's used
here is Mikey represent the parameters
of my query so give me all the
interesting no give me all the books
that I need to push to this person based
on the user ID so that's the the key and
the value is the list of the books okay
but of course you need to know in
advance the kind of query you want to do
and it requires a bunch of preparation
and more memory and so on so that's
where more Adventist queries actually
come into play but a pure key value
store is kind of dump it knows the keys
that are chosen once and it knows values
that are zeros and ones and you cannot
do a lot of interesting queries of that
to understand to do more you need to
understand the scheme at you need to
understand them the meaning of it a bit
like in the matrix where the zeros and
one becomes something actually feasible
so in infinite span case we've got two
ways to extract Express the schema a one
is the actual Java object which after
all as a type and properties and sub-sub
objects and whatnot but we also have the
protobuf approach which has the big
advantage of being client independent so
if some of your apps are in Python and
other are in you know dotnet and whatnot
you have this abstraction and once
and since we have schemas we can do a
lot of interesting things
indexing stuff ahead of time like
looking into the value inside the value
to find the information and so on and so
on so let me dive a bit into some of
those options so I say simple queries
I'm not sure why I say simples but it's
more like other queries that you want to
apply on a given entry and think of the
entry very much as a document so key
value stole nowadays a lot of them
actually ever when they have a schema
it's really pretty much a document to a
nested you know structure of thought so
I can query on the type and on so
the property and apply restrictions here
I happen to also show that you can do
aggregation of those on those entities
you cannot do joins between the elements
at least not natively but and depending
on the system in Finn's panel others
it's either a query language as in
string or an actual funeral API it
doesn't really matter what the query
here expresses give me all that give me
the average yeah the size and the
average size and the biggest transaction
that have been processed in in my system
in family we indexed stuff when you tell
us to index the specific properties and
we'll use those index if you don't have
an index or only partially indexed we
first look at the index and then we do
the rest in memory by kind of doing a
full scan which looks horrible from a
relational database point of view but
remember everything is in memory so of
course we try to favor the ET index but
if it's not there you can still do other
queries and SQL remember is all about
other queries I've got a model but I ten
years down the road and can invent a
query that I didn't thought about when I
was writing that model right
that's why SQL is so powerful in
relational database have been King for
for so long
and still are to be honest what's
interesting with in finish man and I
think that's kind of a kind of a
specificity of that project is that the
indexing system is actually Apache
Lusine which out of the box offers us
pretty advanced full-text scale I mean
probably the most advanced open-source
full-text capabilities that you have out
there so all of the you know let's
Google my data you you can really
Express first the approximation phonetic
approximation restriction of the query
by by your location finding the most
interesting document the most relevant
document first and so on faceting which
is the idea of doing a query but then
having on the side kind of a navigation
health saying hey here is this query
here is the result here are the results
in in for that query but within those
results they are those
five brands and people write those
products from one star to five stars and
I don't know maybe the price there is
like kind of a scale of prices and so on
so faceting is really a way to refine a
query giving hints to the user so he can
go and refine the query what else should
I say about that so yeah you can really
you don't have to use those advanced
grace but they can be handy like oh you
already have a tool that can do advanced
you know full-text query my data is
already there let's make use of that so
that's one of my favorite continuous
queries there's quite a bunch of system
which really say hey I need to know the
top end elements of you know in that -
to build a dashboard or I need to get
the number the people that are you know
of the right credential and because I
need to be fresh are going to query I'm
going to run the query every one minute
or every 30 seconds and whatnot and that
looks pretty inefficient so conscience
query is solving that by saying okay let
me tell you let me give you the query
I'm interested in so I'm interested in
all of the elements that are actually
matching that query and every time there
is a new element or a change of an
element that means it's no part of the
query or it's no longer part of the
query please notify me so I can update
my list on the fly and be reactive
instead of proactive and kind of system
consuming so here I've got three
continuous query probably from three
different clients that are interesting
interested in a user the user type and
some of them have the same predicate
some of them have different predicates
and the cool stuff about this one is
that instead of indexing the data which
is hey this new data just come in or
this new data just got updated having an
index of that one element wouldn't make
any anything useful so instead of
indexing the data we actually index the
queries so we say hey this query is
looking for age so let's let's index by
age so so I can do a query of queries
that says give me all of the continuous
queries that are looking for age because
I know the a just changed from 17
2:19 maybe a user that wasn't into the
matching choruses no into the matching
corpus and that's true of I guess the
first queries the first and the second
okay and once the system detects that
and actually it's also executed one
predicates only one so when predicates
are sure between continuous queries it's
it's not actually executing all of the
continuous grades so it's really quite
efficient in that way and we push the
event to the client that was listening
to this continuous query by the way
continuous queries are actually built on
top of what we call the cluster listener
so it's essentially an event model that
says hey new entry coming in updates
deletes topology change and so on and
you can react to that you can read a lot
of things on top of that and it's plain
Java so you can write anything but
continuous query is kind of a
higher-level thing that makes it a bit
easier for you to write and use
distributed stream is also build on top
of something that I will talk about a
bit later so who uses Java 8 okay
streams okay you sure that's very
interesting because instead of saying
okay I'm looping over my data and
applying those transformation and then
filter and then you know correlation
yeah callate correlation of the of the
data what you're saying is okay here is
what I want how I want to filter here is
how I want to transform the data here is
how I want to collect in D in the end
and please system do it for me on the
data and Java can do it in sequentially
or in parallel it can optimize a lot of
things and do it lazily and so on so
that looks interesting because instead
of doing that on a map in memory how
about a distributed map which is
essentially what infinite span and
in-memory distributed system are well
you can actually do that with infinite
span so we really embraced the stream
API and you literally use the stream API
and pass the lambdas to express the kind
of operations you want to apply on your
on your data and it's smart about it so
it tries to push down the operation next
to the data and do as much of the
operation including you know I don't
know if it's an average operation
happening we try to do the average
locally then we bring back the
information and finish the the
aggregation at the node that requested
the query right of course some stuff
cannot be done there is there a specific
operation that requires to go back to
the motherboard to do the final
collation but it's we try to be as you
know as local to the data as possible so
how does that look like from a court
point of view well it's just like Java
stream so in Finnish pal and actually
any kind of Jay cash compliance product
exposes the cache API which is a
subclass of map so you just say cache
that values and you got the stream API
and from there the fun begins so here
I'm saying okay I'm going to filter by
transaction that have been processed
only and then I'm going to do the an
average of the price of the transaction
and I'm going to return that so I
cheated a little bit because lambdas are
not serializable they actually are but
they are not marked as sterilizable
because well I guess the JDK team has
good reason for that that means that I
need to do either some kind of down
casting for those operations or I need
to use some of the helper class that
infinite span is offering you so the
core is a little bit more verbose but
essentially that's what the code is
doing and distributed stream is actually
built on top of something called the
distributed execution framework which is
a generic way to say please go and
execute this stuff on all the nodes of
my grid or some of the nodes all of my
grades so you can do anything any random
tasks that you can write in in Java or
some other language of course you can
access the local data of each node and
do some computation locally that's where
it really shines and then you've got
lots of flexibility that's
you know what's the timeout what's my
retry and fade over strategy should I
try and say I'm going to execute this
task only on a given rock and to not try
and do network too much network
navigations by the way in finished panel
and some other products have this which
is kind of stamping the nodes by rock
name machine name and site and they try
to put the data so that the data is
properly distributed so if you lose one
machine and an entire machine having
several nodes of infinite span that's
okay because the data has been put on
another machine or even a different rack
so you can customize that it's quite
interesting
okay so Apache spark is so who knows
about Apache spark okay so it's it's
part of the whole Hadoop ecosystem and
it really bring kind of a mini
revolution into this one because they
try to do stuff in memory and as lazily
as possible and instead of exposing the
god-awful MapReduce model they have a
much nicer and higher level API and in
practice it's it's work so it's kind of
a distributed framework so they don't
have the data side of things that other
data grid has but it's essentially of an
efficient distributed framework that do
computations lazily and a lot of systems
share those kind of approaches so you
can access you can get the data from a
relational database of course the whole
you know I do Pico system HDFS and
whatnot but a lot of other stores are
actually started to integrate with spark
so in finished bindings is one but
there's a lot no lot Moss Cassandra and
whatnot and the real key value of spark
besides being a fast is that there is a
massive ecosystem on top of both an API
on SPI so SPI is how do I get the data
to spark so it can do computation and
that's the value sources that I was
describing and the higher-level API is
let me build higher level things on top
of this distributed framework API so
mashing you know the machine learning
libraries and so on that are built on
top of SPARC makes it a super useful
ecosystem so for example because
infinite span is the spark source
compliant or expose its its data to
spark you can actually do some machine
learning computation on top of hinge
infinite spans data
let me explain are some of the cool
things we've done when we walked into
integrating properly spark and infini
span so for people not familiar with
spark there are three concepts the first
one is the spark master which is really
the resource manager that says ok I'm
going to I know this worker is kind of
busy so I'm going to push the task to
this worker on that worker right then
you've got the workers that are actually
doing the task and then you've got
another not quite sure but I think I
know the master of the driver are the
one that says from this big job let's
shrink it into those tasks and then
split them into the values workers and
the driver is really the application as
you think of it the spark job as you
think of it because it's got the card
and will delegate that work to the
workers and then aggregate that back
because spark has two strengths the work
into small bits to be able to do the
distribution wouldn't it be nice to for
infinite span to push that information
to spark saying hey I already have my
data Trent already distributed the data
so in practice I've got subset of the
data replicated in two different nodes
and I can say please split it into those
buckets because I already have those
buckets that would be more efficient so
that's what we do so with the the client
side of Infini span is really sorry the
spark client that is the infinite span
integration is actually saying hey I
know the topology and the splitting of
infinite span please use that so spark
is like okay cool and the other stuff we
do is we say to that that bits of code
by the way if you find a spark worker on
next to an infinite span node the same
machine for example
try and get that Walker run on that
bucket that this node host so that
instead of doing a remote connection and
move the data from infinite span to the
spark worker to do things the the
connection is a essentially a loop back
and you avoid the network connection so
it's much faster and doesn't kill your
bandwidth the other stuff we do is
because infinite span has a pretty
decent search capabilities that I was
describing we can push down some of the
filtering and work that spark would have
to do in memory so we don't even move
the data to the spark node if we can
okay I think that was pretty
comprehensive okay so that's the well at
least to me the most interesting part of
the talk is I gave you the ground like
higher level use cases what in memory
systems are to my definitions what kind
of queries they can do and now let's go
and dive into the architecture on how
you guys could apply those and use those
systems into your the problems you have
to solve solve so of course the first
one is plain good old caching you've got
two models one would be hey let's go and
embed the distributed in memory system
inside my application and the memories
is shared actually between my
application and in in memory system so
that's called embedded or library mode
depending on the systems it's good it's
good because well it's just a library to
add and the different nodes actually
join together but because you share the
the same memory it could be a bit harder
to tune your JVM because your app kind
of object creation workflow might be
vastly different than the the data grid
you know access and garbage garbage Inge
of the of the data so the alternative is
to say okay no I'm going to dedicate
nodes to the actual data grid and I'm
going to use going to use a
client-server approach to go on access
the data and from there I can really
tune and scale up and down the value
things in differently so it's it's very
common in containers these days to
really do
one stuff for one job so separating is
kind of the trend these days so of
course you can cash at the service layer
or cash at the between the your slow
data access and your your system kind of
the hibernate second level cache thing
any question well this one was easy I
guess you didn't come for this one this
one is more interesting so let's go a
bit more ballsy and say no I'm going to
use this in memory system for my primary
store and I will use that maybe for
temporary data maybe for not so
temporary data and there is a lot of
interesting things that you can do first
of all into this container and stateless
apps it's it's nice to have stateless
applications but States is kind of a
fact of life so where do you put the
state so you can put the state into your
relational database but memory there is
a lot of temporary state that you want
to share between your stateless nodes
and the data grid is actually very
interesting for that to be able to say
hey let's pretend this is a hashmap it's
actually happens to be a remote ash map
but that's okay I'm just pushing
everything outside who tries or does
microservices okay some of you so I
don't know about you but so
microservices this can bring quite a
bunch of challenges on the app side of
things but it's even works on the data
side of things because in a perfect
universe one micro service should be
totally independent from another micro
service in its way to run including how
to access the data and use the data so
what about my legacy database that had
all of the data and those values micro
services that actually need to use the
same data it's not it's not the
different data it may be it's
transformed a bit it's not exposed the
same but it shares the same data so how
do you do that well that's a pretty hard
problem and it's beyond the scope of
that presentations we that's something
we're gonna try to work on especially
hot right out and I'll mention something
that can help you in in one of the next
slides but each micro services in a pure
universe has to have its local
that he plays with and he's not sharing
with the rest right
you could use an in-memory system for
that especially if you know that you can
repeat rebuild that data from another
data potentially exchanging data between
apps is an interesting one so here
you've got up 1 and up 2 so maybe they
want to share exactly the same data so
when up one updates something up to can
you know read it and access it and maybe
you have a continuous query on it or
whatnot but you can even also say you
know up to or this other part which is a
you know micro-service number 2 has a
slightly different view of things it
doesn't need all of those entries but
just this column and that column so you
could you could say when I store data
into the grid I've got a listener that
listens to that and say whoo data coming
in or a change coming in let me
transform that data into something else
and put that into a different - shell
into the same grid and up to is actually
accessing that cache so that's a way to
do kind of a poor way to do not an ETL
but transforming data from one 12 to
another of course there is the search
capabilities we've described full-text
search or or not and even if you don't
go all the way down to spark you can do
distributed executions and redo some
very interesting things on the on your
full data set in a distributed fashion
in a pretty fast fashion this one is
also I mean in so my heretic but mixing
stories might not be that bad so maybe
some of the data you want either a fast
access or it's really temporary data but
the rest of my application is to store
things in a different fashion or in a
more classical fashion well don't worry
you can mix stuff here I added a twist
to it because I use hibernate OGM which
is accessing data from no SQL stores as
well as hibernate Oran and all of them
actually exposed to Dharavi at JPA so
it's really the same kind of programming
model that I use for both you don't have
to do that of course
another point while I'm on that subject
of oh by the way I didn't say that but
data grids and infinite span in
particular can be transactional if you
want to and it's an Excel resource so
you could actually put those two dates
from two different store into one you
know two-phase commit transaction if you
want to don't have to of course but you
can and while I'm on the subject of
mixing stalls
I just want to plug something a kind of
a general category that is called data
virtualization and the idea is to say
I've got lots of different sources of
data with their physical again table and
mapping okay I want I want to abstract
my applications from this black massive
differences and all of those different
sources so I create a logical model that
will say okay
that physical table I'll expose it as a
logical table with slightly different
names and that physical table from this
other system I will actually expose it
into the logical you know schema of as
if it was a single database so they
literally virtualized the value sources
you have into a as if it was a physical
single data store without copying the
data so that's another one interesting
and in our system while we work on a
project called read which is doing that
a virtualization and we have a what
should say let's do a materialized view
so let's the system's let's say the
under underlying data source is pretty
slow so let's load the data into the
data grid maybe because I've done some
John and some transformation and store
that and physically represent that in
the in the data grid and instead of
accessing the underlying system actually
access the data grid to do to do my
query or twitch with my operation and
are rich on that fast to the user so it
it's faster I also lowers the weight of
potentially an or legacy data store
system and going back to the micro
service it's also interesting big you
know I was mentioning that data
partitioning is hard for micro services
like
do i flow the data from one
micro-service to another and so on while
keeping them independent from one
another so a first step to make life a
little bit easier is to use to start a
virtualization layer and say ok in the
end that's a big old legacy database and
I'm not changing that right now I'm
starting to micro service stuff but I'll
use the logical mapping to say no no no
I'm only accessing this table in that
view and only those columns and that
other tables so that's a subset of the
mapping so I'm already decorate yeah
removing the correlation of my big data
and the risk of trying to grab a bit too
much and only focus on that micro
service with this contract and maybe
later on you'll be able to split the
data ok that was a pretty long diversion
sorry analytics and and and and spark
it's a very evolving universe and people
are kind of figuring out the useful
patterns so let me give you some things
that I find interesting so let's assume
I've loaded some data into spark I some
smart guy as found some kind of
predictive model and I want to apply
this predictive model to my actual data
to actually to categorize people so then
I can sell them stuff you know in a in a
better way so the output of applying the
model on the data it would be nice to
put it into the infinite span data grid
in this case so that my application
doesn't have to think about all of the
smartness and so on it just go buy a key
say ok for that user is their model is
it is it part of a category yes if it's
part of that category I do this if it's
part of that other category I'd do that
right so the app is kind of unaware the
contract is really he receives cash with
those entry and that format and it
doesn't care about all the smartness
that has been done inside spark and I
can get this the spark jobs running
several you know every day every two
days every three days and update that
information and the app doesn't really
doesn't really care
by the way Amazon's the Amazon they do
update their model like several times a
day
and the reason is like we don't know
people are weird and they change their
pattern and so on so which we literally
retrain our model every several times a
day and we update that model to just
stay at the top so that's that's kind of
this continuous model that we're trying
to help you achieve here so that's the
computation out of it you could store
the actual model so there are some seer
ization form for you know predictive
model I'm not super familiar with that
so I won't go into the details but
that's usually a small structure you
could store in the in the grid that's
kind of a small use case another one
which is an elephant in the SPARC room
is that spark is lazy it tries not to do
operation until the last minute and when
it does then it posts the data and do
some operation and we cache those stop
array operations but if the cache goes
down it expects to be able to go back to
the source and rerun those operations to
achieve to the to reach the same result
if your source underneath is actually a
moving target like you update the data
you add things remove things then the
computation that was supposed to return
exactly the same result actually might
not so it could be okay for a predictive
model maybe it's okay if this guy is
predicted to be in this category where
is that other category that's not too
bad
but for for other computation that might
be quite problematic so a way to walk
around that is to say well let's go and
copy my data into another system that is
literally a snapshot and bring my spot
computation on top of that snapshot and
you could store the snapshot into in
memory and thanks to the you know push
down and the locality effort that
infringement in this case has done you
you know gets the faster the other stuff
is the spark job is usually done in
several big tasks it's really several
spark operations and storing this
intermediate data actually makes sense
not always because if the computation is
cheap you don't necessarily want to
store it but if the computation is a bit
heavy you want to store it temporarily
so you could use in memory for that and
of course if you
store your data inside Infini span
either other copy was primarily store it
is a source of data that you can access
spark with but it's also a source of
even so in spark there is two model you
can say here is my data let's go and run
some computation on it or ear is the
stream of change events and let's do
some computation on it and the model is
the same so that's quite interesting so
we can do real-time stuff with the same
model and same API as the let's do the
work on this big data set when you put
something into infinite fan then we can
transform it into a district party
stream and then trigger of spark and say
hey reset change please you know do
whatever computation you are used to do
so you could have real-time analytics
based on the infinite span data that you
store for example okay I think I've
covered pretty much what I wanted here
so reactive is a fairly overloaded term
so I won't i won't go into that wall but
think about this model where i've got an
application i'm storing data into my
data grid and i want this other
application to only do things if some of
the data change changes and that's the
continuous query model or even at the
lower level the cluster listener model
please wake me up when this data goes
from x to y or there is a new user into
the system because i need to do things
okay so need to do things might be just
hey let's put a message to mq or and or
any other you know messaging system or
it could literally be a my up is is
ready let's go and run this stuff okay
so it helps a little bit into this
I wouldn't say microservices ation but
at least simplification of the app where
the plundering of react e reacting to
something is actually done by the
framework and your app it just has to
react and do things and look at division
io so let me do my my second part is
this it's a pretty it's a new project
but it's very cool it's doing what is
called change data capture and the
objective of change data capture is to
tech data sources so MySQL you know
MongoDB and you know Oracle whatever
look at the literally the bin lock the
the transaction log and transform this
data at rest into the list of change
that have happened to the system so you
go from the system where you have to
create the data to see if something has
changed into a system while you got the
list of changes into a queue let's Kafka
queue to be able to replay it and you
can react to it you can add a new macro
service that will consume that kind of
information and you know maybe he store
its own internal model which is a slight
copy of what legacy system is doing so I
really see it as the way to transform
your data at rest into a dalit flow and
help you go from your legacy system
expose that as events and then build
your new interesting architecture that
will explore the data experiment do some
micro service and whatnot if it fails
that's okay you just experiment and get
started with that without really
affecting too much the rest of your
application ecosystem which is whittler
say legacy so have a look at DBM dot IO
it's pinned on top of Kafka essentially
and the objective is to say for all of
the data the main data sources will
create a common model that says here is
a new entry here is a change between + 3
n + 3 B here is a deletion and so on and
so on
here is a schema change and so on the
final one it relies on the fact that
that accretes well first of all day you
know you can put add nodes and remove
nodes from a data grid but you can also
have two grids that are actually in sync
with each other that's kind of useful if
you do a follow the Sun and have a low
latency concerns so one of our customer
is doing exactly that so they're so in
their shopping cart and during this no
sensitive time of year where you don't
want to lose customers also called Santa
Claus and they really literally have two
data centers
the big I run load balancer in front and
they put half of the people on data
center one half of the people on data
center to the shopping cart is actually
not stalled locally on the app server
but actually deported into the data grid
and the data grid is in active active
replication mode if one of the data
center goes down the load balancer will
detect that and move everybody to the
second data center and we haven't lost
any of the data it's live it's super
fast from an access point of view and I
can keep going so that's for like big
iron people but even us even well let's
say even me because you know maybe I
don't have two data center of that home
I can use that to say look I've got a
grid that is really OLTP and I don't
really want to mess my and mix my
analytics and my OLTP into the same grid
because I want to keep low
I don't know low latency and
predictability and whatnot so I'll
create another grid sync it get all the
updates on my analytical grid and then
do my computation on this analytical
grid which I can scale up and down
depending on my needs so that's an
interesting usage here so that kind of
concludes what I really wanted to say
think of data as gremlins after midnight
when you pour water on them remember
they you know they pop the transforms
they multiply so it's fine embrace that
so I don't have an address the data line
edge kind of problem it's kind of an
open problem to be honest but hopefully
you will see stuff coming into into that
universe but the other part is because
I'm transforming the data it you
shouldn't be afraid to store transform
data somewhere else and experiment with
it and keep going so the continues you
know inside food analytics is one
example but you could imagine many
examples of of the data being
transformed and having to be a reuse
from one system to another
thank you very much
experiment take calculated risk maybe
copy some of the data experiment with it
from an elite analytical point of view
see how the data grid is reacting to you
with for you and then you know keep
adding stuff slowly because you can even
start with not storing your primary data
into the grid but just copying or
copying in the flow and experiment with
it it's kind of safe for you you're not
actually using your data which you know
will I guess see or might be pissed off
otherwise okay let's let's throw down
that and let's open up for questions
hopefully you have you know lots of them
and let's do ideal even an open
discussion any question
okay so I guess I love to ask you
questions do you guys use any kind of a
memory system right now in your in your
environment yes no yeah what kind which
one okay so did you thought about all of
those use cases when you have it in mind
not all of them okay so maybe new new
usages down the road okay anyone that
wasn't really here that you want to
describe or anything okay so you use it
for caching yeah which I sort of
described okay
okay so that the remark that this
gentleman was saying is that they they
use infringement for caching but the
main problem they have is they are dev
and they are ops and they are not the
same world so essentially they the ops
are kind of concerned about you know
adding those distributed system and and
putting them in in place so that's
something we're working but within the
universe of openshift so that that's all
way to solve that problem to really say
try and bring that DevOps culture but
also because of this kind of abstraction
through the platform as a service we can
actually automate a lot of the use cases
so at some point I was this frame that
developed realization can use a data
grid to cache stuff we would have a way
to say oh you use that Aviat and you
want caching let's actually pop up the
nodes for you
ideally with some kind of affinity and
you or the apps don't have to do it
that's kind of a template it could use
to execute on that so we're not there
yet but that's the kind of directions we
we want to go to any questions on you
know caching distributed great search
queries anything okay so I guess I'll
release
you if you have any question private
question you can come down and and talk
to me thank you oh and give me feedbacks
please like you know hey I expected that
or I liked it I didn't like it
whatever</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>