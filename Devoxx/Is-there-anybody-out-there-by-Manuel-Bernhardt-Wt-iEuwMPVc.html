<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Is there anybody out there? by Manuel Bernhardt | Coder Coacher - Coaching Coders</title><meta content="Is there anybody out there? by Manuel Bernhardt - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Is there anybody out there? by Manuel Bernhardt</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Wt-iEuwMPVc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hi everyone thank you all for
coming I was hoping to play some Pink
Floyd on the audio here but that didn't
quite pan out but anyway you will notice
probably if you know them that this is a
Pink Floyd themed talk and not so much
an x-files theme talk as the title might
have suggested so during this talk what
I would like to talk about is
distributed systems and more
specifically the viewpoint of a single
cluster node like this in a cluster and
so let's start right away with a little
bit of a disclaimer because you know
what you're doing as the system
distributed systems engineer you read a
lot of papers and papers have this
property of being recursive so you start
reading a paper and then in the paper
there are some references and you start
you like some of the references and and
so you you also get these papers and you
print them and read them and sure enough
you end up with a big stack of papers
after a while and then at the same time
you have to make some money because you
don't make money reading papers so you
eventually you're building also a
distributed system like like that one
but you know that you know take some
time and maybe works and maybe it
doesn't and then as the year go by your
sanity slowly leaves you and I just
mentioned that because I'm I'm fine with
this constant feeling of doom and exists
essential crisis that comes with getting
a bit too deep in in the world and
realm' of distributed systems but you
may not so please don't come to me in
five years saying that I ruined your
life because you got interested in the
distributed systems and and complained
because I will have warned you a bit of
about myself my name is Manuel Bernhard
I'm an independent consultant I
specialize in reactive systems which is
to say systems that are built mostly
with acha
scowler java language the play framework
or the legume framework and what I do is
either my clients fall into categories
either they come to me before or while
they're building the system and that's
the good case and then the other cases
they come to me after they launch it
resistant to production and then they
come to me with a certain sense of
urgency because something doesn't work
as they would have expected I also train
I'm a live band partner and I train on
the topics of acha acha cluster active
streams so that's what I do for a living
and you know other other than that I
also like to scuba dive and this is not
the Danube
I wish yeah so let's start with some
motivation here life is a single-player
game you're born alone you're going to
die alone all of your interpretations
are alone all of your memories are alone
you're gone in three generations and no
one cares before you showed up nobody
cared it's all single-player so that's a
very nice quote by naval ravikant the
CEO of AngelList are you all motivated
now yeah okay great no I didn't actually
try to motivate you here I'm trying to
motivate the talk because you can take
this this quote of a single player game
and apply that one-on-one to the life of
a single cluster node you know because
that's what a node in the cluster is
doing is looking at these other
computers and when we think about a
cluster we like to think of them as
being is well-defined we know there is
these three little raspberry pies here
and they all are part of our cluster
this is what I would call the God view
because we know it all but the reality
of building distributed systems is that
you don't reason in terms of all the
whole of what is there is you reason in
terms of one machine that sees other
machines through a network
and that may network is gonna be
unreliable we haven't managed to build
an and non unreliable networks yet so
you know it may just happen that your
network is government and you don't see
the other node and that is something
that you have to think about when you're
building distributed systems and this is
what I want to focus on during this talk
so really when you want to build a
cluster there's three things you need to
be doing first one you need to have a
way to tell who is there with you in the
cluster who is joining with leaving who
is there with part of the group then you
know nodes can crash either the process
crashes or the hardware fails or the
network fails they're gone so you need
to figure out who is in trouble it's the
failure detection part and then finally
in some cases at least maybe in most
cases you using a cluster because you
can't fit it all on one machine you may
use a cluster for fault tolerance so
that you can still continue working when
one of the nodes fails but more often
than not you also want to use a cluster
because you want to scale out or machine
it many machines because you have too
much load and then the question is how
do you balance the load across all of
these nodes who can take up new work etc
that's the load balancing and it turns
out there is one abstraction that you
can use and that makes building and
reasoning about all of this a lot easier
and that is the abstraction of group
membership who is there with me if I
have a service that runs on every node
in the cluster and that tells the
application who is part of the cluster
that makes building distributed systems
a lot easier you basically put this
abstraction in there and it's a it's a
nice abstraction that you can use on an
application layer then to to build
things and and then so let's look at
what you need in order to implement
group membership such a service you need
three things you need two failure
detection figure out when someone is
crashing you need a way to tell other
nodes that something has happened
for example that you have been contacted
by a new note that would like to join
the cluster or that you neighbor just
told you they want to leave the cluster
or that you just figured out that
another node you were monitoring has
gone away you need to disseminate this
knowledge that's the second thing and
then the third thing is that you need to
reach some kind of consensus as to who
is there and who isn't in the cluster so
you need to reach consensus as to the
the group the list of nodes that are
there with you in the cluster this is
the third thing it's pretty much all so
hard or in in some ways it's also quite
impossible as we'll see but but we'll
get there in time so I will start with
the failure detection part which is I
don't know why I developed this
irrational irrational obsession with
failure detectors some time ago so what
our failure detectors and there's let's
let's let's look at the definition here
there's two key properties of failure
detectors first one is the one of
completeness and that means that if a
node crashes all the other healthy nodes
in the cluster that haven't crashed
should know about the fact that that
node is gone so that there is no node
that will still try to talk with that
one all the way we know it's gone away
that's the first one the second one is
accuracy which is to say that if there
is a healthy there is no healthy node
that should be suspected by another
healthy node of having crashed or in
other words there are no false positives
you don't want to you don't want your
healthy nodes in the in the cluster to
start suspecting one another even though
they're perfectly fine and then you know
there is reality kicks in and you have
these pesky little things like speed and
network message load that you also have
to think about when you want to
implement a failure detector in practice
because if you don't think about them
your failure detector is going to be
pretty much unusable in a real world
system but we're not going to use these
two properties to classify failure
detectors further on now as it turns out
when you do distributed systems
engineering
this category of proofs called
impossibility results so these are very
smart people that proof things that are
impossible to achieve and it turns out
that it's impossible for a failure
detector algorithm to deterministic
Aliyah chief both completeness and
accuracy over an asynchronous and
reliable network so an asynchronous and
reliable network is the kind of networks
we work with so in other words we're
doomed we can't have both of them so
what do we do well we make trade-offs
that's one thing we do in in many many
parts of computer science and in in in
the realm of distributed systems we make
a lot of trade-offs so we talked about
strong to weak completeness where either
all or some of the non faulty members
detect a crash or we talk about and we
talk about strong to weak accuracy where
there are no or some false positives we
can't have a bit both but we can make
some trade-offs and in practice what
happens is most applications they want
strong completeness they want everyone
to know about nodes that have failed and
they choose a weaker form of accuracy so
there's two ways in which you can
implement failure detection the one is
to use a heartbeat so I know that's
being monitored well ping will send a
heartbeat out to the monitoring node
every now and then or every second let's
say and when they stop heart beating we
know the node is gone away there is a
problem there is another approach used
by for example a cluster where you have
a ping pong so a monitoring node will
ping a node and it needs to be a reply
that so that we know that the other node
is healthy so these are the two sort of
strategies that we can employ here ok so
now that we looked at some definition
says look let's look at some failure
detectors first one I want to talk about
is the fee adaptive accrual failure
detector so this first property of this
failure detector is that it has a very
cool name it's also an adaptive failure
detector which is to say that
it tries to adapt to changing network
conditions so when the network
conditions changes failure detector also
sort of figures that out and that's
quite important because in real life
networks also tend to change especially
if you're in the cloud and if you look
at how network load and so on in the
cloud evolves it's unshared shared
medium it's it's quite interesting and
what feed the fear accrual failure
detector what it does it introduces this
notion of accrual failure detection so
accrual means that instead of having a
failure detector which that is boolean
which is to say either not-it's trusted
or it's suspected here you have a
spectrum okay so the failure detector
gives you a suspicion value and you make
or you or the application then makes a
decision as to what it does with that
value when it starts to take certain
actions and we can see here that lower
the value the faster detection time okay
that's that's nice but then on the other
hand the lower the threshold value the
higher the mistake rate the mistake rate
is this accuracy that we were talking
about before so you start suspecting
processes that are perfectly healthy of
not being healthy and so that the way
it's described in the in the paper is
that for example the example they give
is a master and some worker processes so
the master distributes the work and what
you could say is well if my suspicion
level really grows over eight
I'll stop sending new work to that node
if it grows over ten I'm going to send
the work that that guy was supposed to
do to some other nodes and then over
twelve it kicked them out of the cluster
I removed them entirely that's the idea
of this accrual value that sort of
accrues and changes in time so I would
recommend if you want to learn more
about failure detection this the paper
is it's quite interesting because it
sort of gives you a broader idea of what
failure detection is about it's a very
nice introductory paper if you want to
get into this stuff another failure
detector not you know the name
is not that cool but it's a new adaptive
accrual failure detector so it's new
it's it's a but it's a bit new word and
fee and mostly this one is about is it's
much simpler to implement than fee it's
it's using it still gives you a
probability a suspicion value but the
most interesting thing so I meant it
this one for akka cluster and what I
found is that it's more adaptive it's
better when it comes to changing network
conditions or to put it bluntly when you
run this thing on a longer time frame on
AWS which goes low you like on the
network this one is better off it will
have a better easier time there than fee
but that sort of is still the same idea
of a cruel failure detection it doesn't
really innovate and in the way in which
it suspects things so there is one
that's very interesting called the swim
failure detector as you swim lazily
through the milieu the secrets of the
world will infect you the idea is this
failure detector and in fact it's not
only a failure detector it's also a
dissemination component is it's built on
on by a lot actually what sites and by a
biologic paper it's about epidemics and
how how infections spread in a network
so we'll get to that when we get to the
Malaysian port button so it uses this
analogy of of an infection and the idea
here is to say okay it's one thing to
build a cluster that has you know 10 or
20 nodes you know is another one to
build a system that has 200 200 nodes
how do I build that how do i scale so
the idea of swim was to have a scalable
membership protocol that could give back
the same at the same speed and same
network load or like not exponential
growth as the number of nodes increases
and and one technique that they use is
to say ok if someone fails we're not
gonna remove them right away and there's
two mechanism the first one is that if a
member is suspected to failure and it
receives a gossip message saying that it
has failed it has a chance to say hey
I'm still
why do you say that I'm gone so that's
one technique the other technique is
indirect probing the way that works is
one node will ping at random another
node in the cluster and then for some
reason you know the acknowledgment may
god make it just get lost or then that
you know something is wrong with the
network or more realistically speaking
the node being pinged runs Java and
there is a garbage collection and then
you know there is no answer for one or
two seconds which is enough to sort of
trigger false positive here so what did
this node then does it will issue ping
requests messages to other nodes also
chosen at random which in turn will try
to ping that node that is being
suspected and then eventually one
acknowledgment may make it make it back
and it is being forwarded to the
initially requesting node and then we
know okay this guy is still there I just
couldn't reach them directly for
whatever reason so the whole point of
this is to reduce the amount of false
positives to get a stronger level of
accuracy now the problem is that in
practice it doesn't swim it sinks you
know then in practice it still - there's
still too many false positives with this
approach and so I don't know if anyone
knows the company called Hoshi Corp they
do things like console terraform vault
vagrant and so they have distributed
system I mean console is the distributed
system service discovery and and so what
they did they try to implement swim
that's l0 up here and they got a lot of
false positive events here you can see
it's it's quite a bit so what they did
they implemented thing called lifeguard
and they added a few extensions to the
to the swim protocol and they managed
when you are when you activate all of
them they manage to get to less than 2%
in terms of amount of false positives on
the as compared to swim so it's really
this is really quite something they've
done here and the implementation of
their membership layer is open source
it's called member list it's
and go you can go fetch it on github and
I haven't found anything more recent
than that in terms of failure detector
mechanism approaches and this is from
2017 is fairly recent so if you want to
go and build a membership service don't
start with swim I mean built on top of
lifeguard this is this is what I would
say okay so that was the port on on
failure detection now let's take a bit
of time and talk about dissemination and
anyone knows the name of this album yeah
it's being Floyd but no no division bell
so I'm sorry I'm probably the Pink Floyd
Week in the room here so and the
question here is how do we make how do
we talk about things how do we
disseminate things in the cluster how do
we spread knowledge in the cluster how
do we talk about members joining and
what members leaving about numbers
having crashed and so there is two
strategies here the first one is
multicast so that could be hardware
multicast that could be IP multicast
that could be UDP multicast but it turns
out that there is two reasons why this
isn't that popular the first one is that
if you go to your sister admins in a
data center and you say hey I want
multicast they will look at you the same
No
well maybe not all of them maybe not all
data centers but many data centers and
I've seen that at clients sites where
they they say yeah but we can't do this
multicast UDP discovery of you know
members joining or because because or
admins won't let us do that so that's
one real-world restriction that's there
but even if that wasn't the case even if
you had very nicely said means that
we're going to go and configure hardware
multicast for you because they love you
so much even if you had that it turns
out that
building distributed algorithms that use
multicast is surprisingly hard because
you don't again you don't have this
guarantee that everybody gets the
message what do you do if a few members
don't get the message if they have
failed while they were the message well
underway
so multicast happens you know it there
is like a big very long compilation of
algorithms in the talk only about
multicast that that shows that this is
extremely hard to prove and to make work
to work in practice so what happened is
instead people started looking into
another way of doing things and this one
was implemented in or researched
actively in the days of pair two pair
who remembers Napster Kazaa yeah yeah
all very legal so so what we what
happened there there was a lot of
research done there and that's when
gossip protocols because the question
there is like I'm searching for this mp3
file how do I span across the pair two
pair no how do I again how do I search
how do i disseminate things in my in my
paratrooper cluster an internet how do I
make that efficient etc many nice things
that came out of that word amongst
others many papers to read there and so
let's talk about gossip a little bit the
idea with gossip is that a node will
gossip with or the idea of random gossip
is that a node will talk with one other
node in the cluster so at every tick of
the clock so let's wait here at t0 I
talked with one and then at t2 these two
they talk to other ones at random and
then t3 and then you see that it takes
only three ticks of the clock let's say
three seconds to get the the word across
a five node cluster and it turns out
that this scales very well only if you
only take one as you only talk to one
node at random in a cluster so this was
proven in 98 by fan Rena CL is this sort
of random gossip style then in 2000 we
have
of a paper that talks about a few more
something that's a bit more
deterministic okay round-robin gossiping
because random gossip is not quite
deterministic you don't know what's
gonna happen so round-robin is one
option binary round-robin is something
that performs a little better and then
round-robin you a sequence check is
where you actually use the fact that
it's round-robin and so you should be
able to tell who is supposed to have
received a message by what round and
based on that you can fail them faster
slower but for some reason this hasn't
been very popular I haven't seen this
picked up in in more recent research and
it turns out that people always go back
to random gossip I mean dynamo or other
they they all they all went back to
random gossip actually dynamic does
local gossip so that's even another
thing so a third type of gossiping here
is this say okay well I'm gonna
piggyback on something else so for
example that swim does that the swim has
this failure detection protocol where
anyway they're going to probe another
node at random every protocol period
every second or every two seconds
they're going to talk to someone else
and so instead of just sending a ping
here up there or an ACK actually
piggyback information on top of these
things so I sent your ping but I'm also
telling you that I know about these five
other members and then in my ACK I also
said tell you that I saw this other node
three seconds ago so it uses another
protocol so they call this infection
start gossip and and that is also a very
interesting approach because you have
less messages in there you send less
stuff around it turns out that life
guard so that's the swim paper but life
guard said that well after all we need
gossip to be something periodical we
want to do that faster so life guard
went back to gossiping every few seconds
anyway or whatever you configure it to
and so these are like sort of three
approaches and then the most successful
is the
and gossip but then what do you go see
about what what what you know what's the
point so let's say we have a node a and
a maintains a list of all the other
nodes it knows about a B and C and then
a has seen a zero seconds ago a has seen
B 1 second ago and a has seen C 3
seconds ago that's my list viewpoint of
a B I've seen a three seconds ago has
seen B zero seconds ago because I'm B so
I know that I'm alive and then C has BS
in C one second ago and now if a gossips
to B if they both exchange their gossips
when you sum these things what you end
up with this a and B have seen each
other zero seconds ago and have seen C
one second ago because B so C one second
ago and this is how you spread so this
is sort of a way to do failure detection
using gossiping and the idea here is to
say that if see if sort of my if my what
I see here is if this value gets more
than say five seconds eight seconds
whatever you configure it to you
consider that you consider the notes
faulty this is how you can say okay
nobody heard about this guy for five
seconds I'm gonna say there they're dead
we can't trust them anymore
so that's that's one one thing you guys
have about so there is a few ways in
which gossip can be optimized
acha cluster for example what they do is
like a cluster maintains a cluster
maintains in the gossip message a list
of who has seen this gossip as a certain
version so what they can do with this
information is that they can gossip with
a higher probability two nodes that have
not yet seen that gossip message and
that way they spread things faster so
they they gossip with an 80% probability
to notes that may not have
seen the latest version with high with
larger clusters like 400 above 400 nodes
you want to reduce that probability
because it starts to get a bit messy but
that is one optimization there also I
got cluster what they do is one less
than half of the nodes have seen the
latest version so that happens for
example when you bootstrap a cluster or
where there has been a network partition
and you have to reach cousin you have to
repair that then the node when they
notice that less than half of the nodes
have seen the latest version of a gossip
they all start to gossip faster they
speed up the gossip because if 3 times
as fast and then when the node comes
when the cluster converges again when
everybody has seen the latest version of
a gossip they slow down Lifeguard what
they do is they have this anti entropy
mechanism where they will take an oath
at random and do a full sync by a tcp/ip
ok because in a life guard what they
uses UDP but UDP might get lost so what
they do and especially after network
partition when when the network you know
when both parties part see each other
again you're in them in this sort of
unstable state and you want a Reaper
things fasters what you do you use
convergence you use full full sync to to
make things well again and and now now
let's talk about one of the hardest
things in distributed systems which is
consensus or really in the context of
the stock consensus means how do I make
it so that everybody every node knows
about all the other nodes everybody has
the same idea of what the cluster is
this is God view like when you when you
have three of them counts hold them up
but here you see three so everybody
should see the three nodes okay how do i
how to reach consensus so let's start
with impossibility again the
impossibility of group membership and
the idea here is that the primary
partition membership problem cannot be
solved in a synchronous systems with
crash
even if one allows the removal or
killing of non faulty processes that are
erroneously suspected to have crashed so
what what that means primary partition
means primary partition is like we have
this main partition or ideally we only
want one partition okay if we have two
or three partitions it's not so much
interesting how do we build an
application on top of that and therefore
some for a long time in the research
community there was this belief that if
you let nodes sort of when they suspect
one if you let them remove that one
suspected node and say okay they're gone
then you could still have this primary
partition this nice world where
everybody is there but it turns out that
this was proven to be impossible in the
in 96 by Chandra Al and the
impossibility of group membership that
result itself is is based on another
result which is very well-known in in in
distributed system the FLP impossibility
results so the point here is the moment
that you have one it's it's enough for
them to have one proceeded that has
crashed in an asynchronous network so
that you can't reach consensus anymore
so again we're doomed here we can't
reach consensus how do we even build
things that work and so what we do and
or what we what we take of did what this
means for us is that if we build a
cluster based system it would be it
wouldn't be very smart of us to make
membership related decisions while the
cluster is unstable while there are some
nodes that are suspected to be failed
because the moment that I have someone
that is suspected of having failed I
don't know if my messages reach them so
that means we need some coordination or
we need to make something there and so
the rest of this is about how do we how
do we work around again it's how do we
work around the fact that we can't
really reach consensus but we need it in
some way or another and so the first
thing when it comes to reaching
consensus that's really sort of vital
and annoying and
it come always comes back biting us is
how do we consensus about the time so if
and is a very nice paper by Leslie
Lamport time clocks and ordering of
events and distributed systems this
paper is one Turing Award it's one of
the most cited or maybe Mandi at some
point it was the most cited paper in in
in computer science and what what this
paper introduces is between reduce many
things but amongst others it introduces
the notion of logical clocks also called
Lampert clocks how do I order events how
do I tell that an event that took place
on one note happened before or after
another event that happened on another
note how do i how do I create this order
and there is a notion of a logical clock
which is a counter basically it's not
any more physical clock because not
everyone is Google not everybody has GPS
and atomic clocks in their data centers
like spanner has where you can then
create a nice order and even spanner I
don't know if how many people of you
have heard about spanner even spanner
has a seven milliseconds span where
there is no total order so even spanner
is not absolute and in that in that way
so this is this is using the logical
causal sort of view to order and now it
turns out that Lamport clocks are not
enough if you want to tell that two
events happened at the same time it can
tell you that they happen after or
before but it can't tell you that they
happened at the exact same time so if
you want to do that you need vector
clocks and vector clocks are vectors of
Lamport clocks so each node in the
system knows what other what other node
which version sorry of the clock the
other nodes in the system know about
currently and that's how you can detect
conflicts so to make things a little bit
more confusing for you and there is
another sort of parody that was
developed independently but it's pretty
similar it's the one of version vectors
and these two guys vector clocks and
version vectors are very often confused
that vector clocks are interested in the
semantics of how do you order events in
a distributed systems and version
vectors are interested in the semantics
of when you replicate data in a
distributed system on various nodes and
to know this data is being modified on
two nodes concurrently how do you detect
conflicts there that's version vectors
and there is this blog post here 15
which is version vectors are not vector
clocks which sort of disambiguates these
two things they're extremely similar but
slightly different and it turns out that
version vectors are not enough because
they're they're false positives so if
you really want to build on version
vectors you don't use version vectors
any more you use dotted version vectors
which prevent false conflicts that may
like it we'll say that two things are in
conflict when they're not in conflict
that's dotted version vectors and that
is only that is basically research on
finding conflicts in in distributed data
which brings us to an to the to the
question you know is this stuff actually
relevant in practice or is it just is he
just talking now about theoretical stuff
that sounds fancy but that's it and it
turns out yes you can use that in
practice finally some code so here what
we have is a gossip message and the
gossip message has members who is in the
cluster we have an overview and we have
a version and the version here that's a
vector clock and why is that a vector
clock well because you don't you have to
tell and if if nodes that I've seen
things happen independently of one
another if they have all seen the same
events because they may have seen them
happen in the maybe there is an you know
that join a cluster and some nodes
learned about it before others because
remember we have this random
gossip so the information may spread in
different in a different order but
overall we want to tell do does
everybody know about everyone else
and that's where you use a version a
vector clock in a cluster to tell ok if
if this node has seen this version you
can tell that they've seen the latest
version what you can do with the vector
clock is is simple you know if I'm a
node here and I'm receiving a gossip
naca gossip with a version vector I can
say ok this new gossip that I just
received it's newer so I'm going to
update my information or this gossip
that I received from this node is older
so I'm going to reply to the guy and
tell him look there's a newer version
here grab it use it or maybe the two
versions conflict conflict detection we
talked about and when they conflict what
you have to do you have to merge both
worlds maybe one gossip talks about a
new node having joined and the other
gossip talks about another node having
left so you merge these two things and
then you create a new version that's the
new version of reality and you've got
about that one and so these things can
be used in practice and are very useful
in practice so while we're talking about
conflicts there is an awesome thing
called conflict free replicated data
types so the idea of CRD T's is that and
we talked about strong eventual
consistency is to say ok I have my data
structures and my network may be
partitioned for some time there is data
being written in two parts of the
network and after the partition is
resolved I will I will not have a
conflict because what happens is if I'm
talking here and I'm talking there
independently of one another
usually I'm screwed right I don't how do
I merge and we see our duties these are
data types that always resolved
something there's two families there's
the ones that use
commutativity of operations where you
say the operation plus one plus two
equals two plus one that's the property
that CMR DT is use and then there is
another one the CDR DT is their use
conversion of state there is a function
and merge function that it has to be
monotonic which is to say it only grows
in one direction and in this example
this is a state-based ERD T it's using
max so let's walk through this I have
three of my three nodes here like I have
one two and three and we start here and
node zero sets the value of the sixth
value to one no two sets the value to 4
and node 3 doesn't set any value so I'm
first gonna talk to this third node here
about 4 and in order to merge I'm using
the maximum between 4 and 0 so that's
gonna be four then here I'm going to
talk about my new value to that other
node up here and it's gonna be the
maximum between 4 and 1 so it's gonna be
four again and then here I set my value
to four so I'm talking to this node here
it's a maximum of 4 and 4 so it's gonna
be 4 at the end of the day everybody
sees the same value it's 4 everywhere
and this is this may sound a bit
esoteric and it is but there's more and
more seer duties being discovered sets
Maps things like this so it's getting
easier and easier to build things on top
of that they're extremely powerful and
it's a very active field of research
there um so that now let's step back a
little bit and talk about mechanisms
that are used when you want to reach
consensus in a more general way and that
the idea here is to use replicated state
machines everyone knows what a state
machine is so any sufficiently
complicated model class contains an ad
hoc informally specified bug-ridden slow
implementation of health a state machine
so very often we don't like to build
state machines it's like we prefer to
build our own stuff and then when it
grows way to complex and we have
solved 252 JIRA tickets then do we
realize that maybe we should have built
a state machine to start with and that
again is an a concept that has been
expressed the first time in this in
Leslie Lamport time clocks and ordering
of events in a distributed system and
it's the idea of using a replicate state
machine so what do we have we have three
nodes and we have a client down there
this orange thing is a client and the
orange thing wants to send a value say a
bank transaction for example what's to
make sure that you know that it's not
going to get lost so we want to make
sure that in our distributed systems we
all agree on this value okay it cannot
be a different value in any of the other
nodes so what we do is we may send a few
operations or events up here like plus
minus plus whatever and then each node
has a state machine and we know how to
preserve the order between these events
so we're going to apply the same event
to all the other nodes to all the state
machines and all the nodes so what
happens then if since they all have
started with the same state and they all
get to see the same events in the same
order then we will end up with the same
state in our distributed system and then
we are safe we know that this value is
now being replicated and we we know that
if one of the node dies well we still
have two other nodes to keep the same
state and that is so it's this a pretty
fundamental idea to build these
replicated state machines turns out that
that is the theory and in practice there
it needs a little bit more to get there
okay so to get there what we need is
something called consensus protocols and
how do we have how do we make it so that
multiple servers agree on the same value
you may have heard of things like taxes
taxes yeah raft caspak sus nobody that's
normal because it just came out two
weeks ago and if yeah so and so what's
PACs is so taxes is sort of this
original consensus protocol
and it turns out that this was hard very
difficult to understand so the paper
came out in 98 and in 2001
Lanford was at the conference got fed up
with people telling him that land taxes
was too hard to understand rounded up a
few people and explained them taxes
again and then he took these notes and
published his paper taxes made simple
and then 13 years later there was
another paper published like or another
protocol called draft and the paper is
called in the search of an
understandable consensus algorithm so
turn in taxes made simple was not simple
enough yet so raft was there to make it
simpler and it did basically the idea of
the whole the whole idea of the paper
was to make a thing that was easy to
teach and understand but it's still
complex ok so you've got the notion of a
commit log if you look on the right-hand
side here you have this commit log you
have a leader and your followers and
every period you you elect a new leader
and there's a whole set of thing so that
all everything is there and there's a
notion of committed entries and you want
to make sure that the committed entries
are going to be the same everywhere
there is no way to rollback or anything
like that okay so it's it's hard to
understand ok raft is easier but it's
still difficult and then fairly recently
CASP axis came out which is very
interesting because it basically removes
the need for log and the need for
leadership election so it makes things a
lot easier it uses functions and and you
don't talk about values anymore you
don't transfer values you transfer
functions and this is I think this is
gonna make waves I came out like I read
about it on Twitter two weeks ago very
interesting paper a bit raw but but yeah
I think this this we're going to hear
about it a little bit more but still you
know consensus protocols require a lot
of work and of course you need them if
you want to make something that's really
safe but it's hard and if you heard
about maybe the Jepsen series by by on
the blog the website is called F fear
I also it still many implementations of
wrong so my preferred way of reaching
consensus is by convention and that is
to say you don't transmit any
information at all so for example a very
elegant way of doing this is by the
means of leader the election actually
leader designation in a cluster what you
have is in fact what you say is I am
going to order all the nodes in my
cluster by IP address and I'm going to
put the ones that are not fit for
leadership for example the nodes that
are leaving or exiting or something I'm
going to put them at the end so I have
my order and I pick the first one so I
just locally order all my nodes and so
by convention I know who is gonna be the
leader I don't need to talk about it I
just know everybody knows and that's
very elegant I find it also means that
it's the leader that has to make some
membership decisions of like who is up
and who is down etc but it's it's quite
nice this way of doing things so um I
think we have a few minutes left maybe
three minutes left so let's talk about
akka cluster very quickly akka cluster
the architecture is simple you have a
membership part on the bottom that's
what actually is the cluster module is
this membership where you have a failure
detector so akka class to use the fee
adaptive actual failure detector it uses
this ping pong strategy for for doing
failure detection every node monitors
five other nodes with ping pong by
default if for the dissemination it uses
random gossip but it's biased towards
note that haven't seen a latest version
of a class of a gossip yet and then for
consensus there is this convention of
who is the leader and then it's also the
leader of the cluster is making the
decisions who is deciding who is up now
is part with leaving with left etc it
also means that while there is some
nodes being suspected of failure the
leader cannot do anything so we have
this this bottom layer here and out of
this we have a few models distributed
data these are CD
he's pretty neat singleton means you
only have one thing in a cluster okay
singleton actor then your sharding where
you can charge your actors and I can
okay it's all about accuracy so you
shard them across many machines and then
on top of that final you build your
application so let's look at an example
in practice how does it look like when
you build a when you are a member node
in a cluster okay in the nice cases I'm
joining I'm up because the leader said
I'm up because everybody else knows that
I'm there and I'm leaving and then the
leader says that I'm good to leave and
I'm exiting and then I'm removed that's
my happy case my not so appliques or you
know in other words reality you know I'm
joining up and then something happens
the failure detectors see is that I'm
gone I'm in this pewter state which
doesn't really exist in a state machine
it's just I'm not reachable and then I'm
down and I'm removed and then of course
the question is you know how do I get
from being unreachable to being down who
is making this decision and it turns out
that this is not it's not the failure
detector it's not the leader but either
I have to do it my own my own so I have
ops team that's there three 24-hour
seven you know that does it makes its
decision of downing of removing
explicitly someone from the cluster
remember nobody can join or leave while
there is a suspected node because the
leader cannot because of all the things
we talked about before we we need this
to be explicit somehow and so this is
the category of protocols called or
known as split-brain resolvers
split-brain resolution how do make sure
today so how does that work well it
turns out that we are out of time now
and split-brain resolution protocols
would be another talk on it their own so
what i invite you to do is go and check
my blog from time to time because I
intend to talk about to write about this
in more detail there and yes I cannot
explain the magic
or you know the algorithms there it
turns out that this is not too difficult
to implement but it's hard enough so
that all the open source implementations
of split-brain resolvers for a cadet
I've seen so far they're all wrong
they're all buggy so uh don't use them
and so or use them but fix them while
you are using them or write your own
that is correct
um yeah so if you want to learn more
here at the end of the slides which I
will put online you have all the papers
that have been harmed while reading this
while creating this slide deck so you
can go and read these all and more
there's more but I didn't put them all
here so yeah that's pretty much it
I hope you enjoyed this I hope you
learned a little bit and yeah thank you
and I don't know if there is still time
for questions or if you if anyone has
questions yeah there is a question back
there it's all pretty scary it is is it
scary did I scare did I yeah I probably
did scare you
sorry about that um what I would
recommend is pick a few papers that are
introductory material so fee the fee
failure actual failure detector the swim
paper these are papers that do a really
good job at explaining these concepts
and then yeah the hard thing about it
okay so there is one very nice thing
that's more akka specific maybe but
there is one project by Eric I will
tweet about it all I'll send you the
link it takes a rack of raspberry PI's
puts LEDs on them okay and then you see
the LEDs with the different colors
depending on the state
of the cluster node it's very like
educational you really get to understand
what happens you see the leader jumping
from one node to another when you
replugged plug in the IP address changes
and so on
and there is gonna be a few videos of
that online I or that's what Eric
promised me anyway so um that's one one
of the resources that I would point to
you know out of the top of my head
because the reality is there is it's so
deep that it's like a rabbit hole and
there is so many different strategies
you've seen it I just showed a few
failure detectors but they're all very
different so yeah there is and it still
keeps evolving every day and so there is
many different direction but these two
papers that I just mentioned and the
Lamport paper please weed you know
everybody should have read that paper
because it's so fundamental that or in
general the the papers that when the
that windy Dijkstra a word-a-day
deterring a word sorry they these are
important things I would say yeah
should you invert and should you buy
Bitcoin no well and the blockchain
itself is extremely simple that's not
innovation but there are a few things
so Adam coiler has this thing called the
morning paper every morning he blogs
about a new paper and there is one paper
that he talked about like three weeks
ago that seems to be doing some
interesting advances there in
blockchains in distributed production
because there again you have to reach
consensus except that you don't trust
anyone you're out in the wild you have
the problem of having a ton of garbage
or like a ton of history to maintain so
there is definitely some innovation in
some things but don't trust I mean at
the end of the day for fast systems
blockchain is the wrong technology okay
don't go there for real-time systems
this is like blockchain is for slow
thing you know and I would say in a few
like dog training is the first
generation if you look at more
interesting things like the direct
acyclic graphs being used da geez that's
that's what I would look into more the
blockchain okay well if you have more
questions just you know feel free to
come to me grab me there's gonna be
beers in one or two hours I think
and also ping me on Twitter or via my
website and yeah thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>