<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Exposed: Introduction to Machine Learning by James Weaver and Katharine Beaumont | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Exposed: Introduction to Machine Learning by James Weaver and Katharine Beaumont - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Exposed: Introduction to Machine Learning by James Weaver and Katharine Beaumont</b></h2><h5 class="post__date">2017-05-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v7xyxDcRM_M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">buongiorno welcome
I'm very loud is that okay
I just don't want to defin you so early
on a Saturday well welcome to machine
learning exposed I'm Katherine Beaumont
this is James Weaver we'll introduce
ourselves
so I'm sure all of you know machine
learning is trying to teach a computer
to find patterns without being
explicitly programmed and hopefully
we're going to shed some light on what
that means and how you can get started
with it so I'm Katherine I work for
foxed I speak at conferences and I
interview people you can check out the
de vacas YouTube channel and season with
those I have a background in a lot of
different disciplines as you can see and
this is where I am now so I hope you
dress I'm James Weaver and I work for
pivotal pivotal is a query software like
spring and cloud foundry and provide
services on that in around software I've
written several books the latest one
being Raspberry Pi and Java and I'm
really happy to be here to kind of get
started what I'd like to do is share
with you a course that both Katherine
and I took from Coursera this
gentleman's name is Andrew Inge and he's
kind of a luminary in in machine
learning and he has a great free
Coursera course and I just want to play
the introductory video from it to kind
of introduce the session why this
machine learning you probably use it
dozens of times a day without you even
knowing it each time you do a web search
on Google or Bing that works so well
because they're machine learning
software has figured out how to rank web
pages when Facebook or Apple's photo
application recognizes your friends and
your pictures that's also machine
learning each time you read your email
and a spam filter saves you from having
a way through tons of spam again that
because your computer has learned to
distinguish spam from non-spam email so
that's a sheen learning the science that
getting computers to learn without being
explicitly programmed one of the
research projects that I'm working on is
getting robots to tidy the house how do
you go about doing that well what you
can do is have the robot wash you
demonstrate the task and learn from that
the robot can then watch what objects
you pick up I'm going to put them and
try to do the same thing even when you
aren't there for me one of the reasons
I'm excited about this is the AI or
artificial intelligence problem building
truly intelligent machines we can do
just about anything that you are you can
do when you scientists think the best
way to make progress on this is through
learning algorithms called neural
network which mimic how the human brain
works and I'll teach you about that too
in this class you learn about machine
learning and get to implement them
yourself I hope you sign up on our
website and join us so you've seen
machine learning in lots of different
areas of course self-driving cars the
ability to generate images or generate
descriptions of images so if you think
about what that takes
you'd need to be able to first of all
read the image and pick out what's in
the image but then use natural language
processing or other techniques to be
able to describe what's happening in the
image so there's several layers of
machine learning that's required there
so if you're a top-down thinker like I
am you want to have a framework to
understand some subject so in this
subject with machine learning there are
three categories that I'd like you to
remember there are just three categories
of machine learning one is called
supervised learning another is called
unsupervised learning and another one is
called reinforced not learning so I'm
going to go through quickly through all
three and then we'll we'll hit during
this talk we're going to go into detail
on supervised and on
supervised learning so supervised
learning is characterized by teaching
the right answers to the machine
learning facility or the model in this
case we have a very simple model it's
got one feature which is the size in
square meters of a house and the thing
that we're trying to predict is the
price in Swiss francs of the house so
here we see data points on this XY chart
the X's are the data points that we know
about so we teach the machine learning
algorithm those about those data points
and then the model needs to then try to
fit that those points to predict what a
new data point might be like a new size
and square meters how much would that be
so supervised learning you're giving it
the right answers and those things that
you feed in our features like the size
and square feet we have one feature and
then we're trying to predict something
in this case we're trying to predict a
continuous output so it's a regression
problem if we are trying to figure out
what classification someone something is
in it would be a classification problem
but that is supervised learning then
there is unsupervised learning now super
unsupervised learning you don't give it
the right answers you give it a bunch of
data and then you ask the model to make
sense of the data for example making
sense of the data might be trying to
find where the data's clusters may be
market segments you want to find out
where the market segments are and then
there's reinforcement learning
you see reinforcement learning in games
and simulations and other areas where
you have an agent a software agent that
is trying to do some tasks and then
you're giving it
you're giving it some reward for doing a
task correctly and then it learns to do
their task correctly more quickly with
less steps one of the success stories
with reinforcement learning recently has
been alphago where Google brain created
a primarily reinforcement learning
system it also used by the way
supervised learning and some
unsupervised learning but primarily
reinforcement learning to beat the world
champion go player so we'd like to dive
in now to supervised learning and
Katherine is going to give you some
intuition now on the maths behind
supervised learning this dog is your key
that we're going to talk that math it's
just a heads up we call that math dog I
don't know why it's happened one day and
so first what we're going to talk about
supervised learning and James brought up
the example of the house price and
that's an example of linear regression
so we're trying to find a linear
correlation between data
so obviously different machine learning
models two different problems this one
suits things like predicting house
prices or stock prices really well so
here's some example data back when I
lived in an affordable area snowing in
Switzerland by the way and what we're
looking at is really here the size of
square feet of a house because we're
trying to see there's a relationship
between the size of a house and how much
it'll sell for in reality you'd have
many more features like location it is a
garage or a garden but just to keep it
simple we're going to look at this so
because we only have two dimensions here
we've got the output which is the price
and the input feature which is the size
we can visualize it in a nice
two-dimensional way and so what we
really want to do in linear regression
is we want to find a line through those
data points that describes the
relationship and the idea is that using
this example data which is the right
answers we can find a relationship that
helps predict or
known so someone might come along and
say I have this house it's spoken 800
square feet how much can I sell it for
and we can provide some insight so the
nice thing about linear regression is it
starts off with the equation of a
straight line so for anyone who has
needs a refresher on that what we're
looking at is this is really simple
equation the output here Y can be
described as a transformation of the
input X so the y-intercept is where
we're going to cut through and n here is
just a gradient so it's the rate of
change how steep the slope is now when
you look at machine learning if you go
into it in any detail you won't come
across y equals MX plus C you're kind of
cross notation that looks like this and
what that just is is that Greek letter
there is theta and instead of writing
like C we're going to call it beta 0 and
theta 1 for M and this here means the
hypothesis so some kind of
transformation happening on X according
to these data values and the reason we
look at theta 0 theta 1 is in reality
when you're doing with your learning you
might have hundreds of features so
you'll have theta 0 to 100 and you can
start looking at how to vectorize things
but we're just yeah I just want to so
who is scared of data okay there's a few
people so I just wanted to say cut it in
and say that that um so this the title
of this cost
class or course is machine learning
exposed and so what we're doing is we're
pulling back the covers and showing you
all of the details underneath all of the
math but it's not necessary to know all
this stuff in order to use machine
learning there are api's out there but
just like when you learn to program you
can program for years and not really
understand the internals of CPUs and and
RAM and those kinds of things but it's
nice to have an intuition about that and
that's what we're trying to do is give
you an intuition about that
you don't have to know calculus you
don't have to know you know linear
algebra things like that but so but but
we are getting into some detail here
theta is just a replacement for that
very simple slope formula so so don't
don't be put off by the math it's okay
it'll be okay
totally right please pursue I should
have been a pre-warning which is I'm
going to talk about this but you have to
know it it's just it's good to know
something of what's happening and that
when you have your equation in a
straight line and X plus C just when we
say theta just think we're replacing
those variables so we could just start
off with a guess so we could guess we're
going to say okay that's the gradient
nerve and that's the y-intercept so
we've got the theta values there and
then what we need now is we need a way
to see how well that gets performs so we
want a way to say this is a good guess
or this is a bad guess let me compare it
to other guesses so one thing we can do
we can really obviously see that for one
of the data points it's way out what
we've predicted is vastly different from
what the data is told us so what we do
in machine learning is improve linear
regression is we look at the error so we
could look at the absolute error so just
the scalar value but we tend to instead
look at the squared error and the reason
for that is for data points that are
really far out like that that on there
it heavily penalizes them so you can see
okay
it fits generally well but it works
really badly on these examples so that's
why we square it there so what we do
it's quite simple really we just look at
the difference between the hypothesis
line that we've guessed and each stage
point we add them all up and square them
so it's quite simple and we take an
average is and we divide it by the
number and then 2 and really want to
know why the two trust me later okay so
we can go through here look at all the
different square errors and then add
them up and we can see wow this performs
really badly
so but now we know we can make other
guesses as he how they compare so the
bit where machine learning comes in is
of use data values because what we're
really trying to do is find these data
values that give a really good
transformation so let's just look at
changing one of them and see how it
changes the cost so here I just took
different values between two points and
I measured the cost for each one and we
can see that there's a point where we've
got an optimum value where the cost is
the lowest okay there's two points that
you mentioned on the x-axis what are
those what are the what's on the x-axis
oh sorry whatever done here is I've just
plotted to base it against the cost
function and what is Theta so what we
were changing is the gradient of the
line so if we go back and imagine the
original data points we're just going to
change the way the slope nevermind where
it intersects the y-axis is going to
change whether how deep it is and
there's a point where it fits the data
best and that's what we're looking for
but we're going to look for it
programmatically so there's something
called gradient descent and this is
where it does get a bit complicated but
if you go back and refer to this and if
you can make yourself quite familiar
with gradients then it's really good
because then it's really helpful when
you look at something like new your
networks so this starts off quite simply
gradient descent isn't simple it's a bit
tricky but if you can get that it'll
make things like your networks much
easier to grasp and that's why we're
going to concentrate on it so what we're
basically doing is going back to that
curve if you look at a point here you
can see that if you differentiate it to
get the slope of the line you can see if
the cost function is increasing or
decreasing at that point and then what
we do with gradient descent
just pick any points and then look at
the gradient of the cost function
against that value of theta and then
what we can do is we say okay the
gradient is going down we know that if
we increase theta we'll get a lower cost
function
but if the gradients increasing we need
to decrease Sato we need to come back
this way so it's a ways of the algorithm
to work out do I need to go up or down
to increase the cost function and when
we get to the minimum the bottom of the
ball the gradient is zero so we stop
changing and I've just drawn two
tangents line there so you can see that
gradient descent effectively does that
at each point you get your theta value
and it takes the tangent it works out
how we do wonderful
I'll just get skip ahead you want
whatever you want to do I'm just going
to put these here and these totes are
recorded so if you want you can refer
back to them if you're a mathematician
you can beat me up to writing B instead
of a partial differential but later I'll
just leave that there as well also
others briefly mentioned alphas a way to
control how big a step you take each
time you go down or up so there's a
slide here about the learning rate and
again you can refer back to that when
you've got multiple values of theta
suddenly you don't have this nice bow
shape then you have a three-dimensional
four dimensional five dimensional and
that's where it gets more complicated
and that really comes into play with
neural networks weight as well as using
gradient descent you have other
algorithms that help you not get too
stuck in like little local minimums as
opposed to the global the biggest
minimum you can find and as well as
linear regression you can describe it in
a polynomial fashion it's not just
straight lines but obviously the limits
are what happens when you don't have a
linear relationship for a polynomial
relationship and that's where other
models come into play and relax
wonderful Thank You Katherine Katherine
mentioned that the talk is being
recorded it's also the slides are also
on the web so if you look at my latest
tweet at job F expert then you can get
your own copy of the slides as a matter
of fact it's web-based so you can just
follow along with this if you'd like
there are lots of links on these slides
so it's it's good to have them and then
you can link to some of the demos that
that we
show you so there's a very classic data
set in machine learning and that's
called the iris data set their iris
flowers and in that data set there are
four features the petal length and width
and the sepal length and width and then
there's one label one output and that is
the species of the iris flower there are
three species so the data set then has
those four columns for the features and
then one column for the label and you
train the model then with that data
there are a hundred and fifty rows in
that data set and you typically train
the model with maybe a hundred of those
rows and then you use 50 of those rows
perhaps to test to make sure that the
training went well now if you think
about what the machine has to do then as
its learning to distinguish between the
features so that it can tell you what is
oh I can learn what iris species go with
what features it has to be able to find
clear boundaries called decision
boundaries between the different
features so here we have I'm walking
slowly so that the camera person who's
not there can follow me I will walk back
do the moonwalk here so we have four
dimensions there we have a it's okay
you're fine we have four dimensions the
sepal length width petal length petal
width if you think in terms of a you
know one dimensional array there would
be four elements in the pin and array in
math speak that would be a vector with
four elements or four dimensions there's
four degrees of freedom and so you can
think of this as a four dimensional
problem well it's hard for us to think
about more than three since we live in a
three-dimensional world but here we try
to think about it
to be run over nice very sharp so so now
we have compared sepal length and width
and we can see that view and we can see
a clear boundary between those features
in that dimension and then we see all
the other ones so that's the challenge
and and there are tools there are
facilities to be able to visualize more
than one dimension or more than three
dimensions in a three or two dimensional
space for example with tensor flow which
I'm sure you've heard of a machine
learning application or facility there's
something called the projector projector
tensorflow org and so I'm going to I'm
going to show you how we can visualize
the iris dataset with this embedding
projector so here we have four
dimensional data but we're showing it in
three dimensions and you can see where
there's a clear boundaries between the
different species that's iris atossa
there's a iris versicolor and there is a
iris virginica so that's a tool that you
have you at your expose ofor visualizing
higher dimensional data in two or three
dimensions as you know with machine
learning with artificial neural networks
we modeled the brain we not we just
telling about it but whoever made up
this great stuff you know decided to
model the brain back in the 1980s and
1990s but then there was this AI winter
for 10 or 15 years because even though
the idea of modeling the brain is a very
good one
the Moore's law you know the wear
transistors are small enough and we have
enough computing power and speed and
networking power and cloud storage it
hadn't caught up yet so just recently
maybe in the last two or three years we
have this perfect
situation where the technology of
modeling the brain matches the
technology that we have in order to be
able to realize that so on the left is a
brain like neurons and synapses in
dendrites which are inputs axons which
are outputs and on the right we're
modeling that using a neural network so
if we zoom into that we see over on the
left we have an input layer which are
like the the dendrites in the brain we
have neurons which are the nodes in this
network and then we have synapses which
communicates between the neurons in the
brain likewise the edges or the lines in
this graph are the synapses and then the
output layer we have the outputs the
axons so if this were modeling our iris
flower data set the inputs would be the
four features and the outputs would be
the three tasks a cathegories categories
it's time to think of species and I
couldn't think of that word did you ever
like try to think of something you could
remember the word so when I was trying
to learn machine learning I created an
application being a programmer I created
an application that would help me really
understand the internals and I called it
machine learning visualization
application and it's available to you
it's a patchy - licensed and you can get
it from github I wrote it in angular 2
and Java with spring so here's the
application I can click on iris flower
the iris fire problem and you can see
this neural network you can see the
numbers changing as it's learning I'll
go ahead and do that again
and what's happening is the the weights
and the biases these are the weights on
each of the synapses and the biases on
each of the neurons are being updated as
it's learning to be able to as it's
being trained to be able to
make the correct predictions or
classifications given given an amount of
data so if I wanted to create if I saw a
new flower and I measured the length and
width of each of the components of the
flower I could go ahead and make punch
them in here maybe five millimeter
sequel length maybe two millimeter sepal
with two millimeter petal length and and
let's say two millimeter petal width
I'll say predict and the prediction is
with 0.99 probability that that is an
iris versicolor and so when training the
neural network we came up with all these
numbers and I'm going to explain those
and then Katherine will take a deep dive
and explain those but I want to show you
how we come up with those numbers so
there's something called forward
propagation and the idea with forward
propagation is that is when I put when I
input information in on the left like
the features the network it's going to
forward propagate using some math
through the network to give a prediction
just like you saw me do so I'm using I
want to use a simpler case this thing
called X or this this data set that I've
created called X or which as you know is
a very simple data set has maybe four
rows in it and first of all I'll show
you the example here in in the machine
learning visualization so if I say true
X or true what is that going to be
anyone I'm in a room full of programmers
somebody's got to know through X or
true/false yes zero right okay so false
so there's one you know 1.0 probability
that that is false so here's the math
you know here's how it goes through the
neural network so you enter some
information good job
blow no multi bidding is that right
multi pin okay anyway we enter the
information here and then it's going to
forward propagate with this some math
through the network and then here's the
calculations it's actually quite simple
it's just high school math where for any
given node for example this node right
here we take in each layer we multiply
the inputs by the weights so there's an
input 1 x 8.5 4 plus 0 times 8 point
five five equals eight point five four
and then we add this bias which is
subtracting three point nine nine so we
come up with four point five five and
then we input that four point five five
into a function it's called an
activation function in this case we're
using a sigmoid activation function and
the activation function is associated
with a neuron and it's it's a
characteristic that allows it to fire
and so doing that we come up with 0.99
and then we do that for all of the nodes
in that layer and then we do it for all
of the other nodes in that layer and we
propagate it through the network so
that's easy because all we're doing is
doing simple math to propagate through
the network the hard part is coming up
with all the weights and biases in the
model and so I'm going to wave my hands
a little bit in it and explain it at a
very high level and then Catherine might
shed some light on it but
when you start out when you're training
a neural network you you see that you
give it lots of very low random numbers
for all the weights and biases and then
you take the data set and you take the
the features coming in and you say
what's the answer what's the what will
this produce when I forward propagate it
through the network well because they're
very random low numbers the nut the
answer is always going to be wrong just
like when Catherine talked about finding
that the squared error it's the same
thing only with a neural network you're
going to have an error between what you
came up with and what your data set said
for the answers and so then there's this
there's this facility that uses gradient
descent like Catherine talked about that
then updates not just the slope and the
y-intercept like like in a very simple
example but it updates the weights and
the biases throughout the network
iteratively so over the course of over
the course of several iterations it the
cost lowers the the error lowers because
we're updating those weights and biases
and then doing forward propagation to
see where we're at
and then as we do that we incrementally
going through the data set maybe
hundreds of times we incrementally get
those weights and biases exactly where
we want them that's called back
propagation just just as a sign aside
this application that I just showed you
it's a html5 client single page
application and we use Java and spring
for rest services and and WebSockets
with Java --nz bring it it makes it
incredibly easy I'm also using deep
learning for Java it's a deep it's a
machine learning library for Java
and if you go to start that spring that
IO it makes it really easy to create a
Java spring application so there are
several types of networks we showed you
I just showed you the the underlying
neural networks but then we have
different flavors for example if we
wanted to recognize images there's
something called a convolutional neural
network and I think I'm out of time
so I can cook accommodation in it let's
mention combo okay so the convolutional
neural network has the same kind of
architecture where we have a fully
connected layers but then we have some
things up front where we do some some
filtering and pooling and I'm going to
show you graphically the way that is so
there's a demonstration on this slide
and I'm going to show you that here and
for example let's say I wanted to
recognize handwritten digits so I'm
going to draw a 7 and many people draw
them like this so this is a in your
browser convolutional neural network
where we draw this and then what's
happening in the layers of the
convolutional network is some filtering
which filters different things like like
you might have a Photoshop filter right
and it's looking for edges things like
that and also because there can be lots
of pixels in in images we have to down
sample or pool them and then here's
another layer and here's another down
sample layer and then finally we have a
fully connected layer and then and then
so this ends up being a 100 vector or
100 dimensional vector that represents
that image and then if i zoom in here we
see that it's predicting that that is a
7 dot the strongest prediction is that
it's a 7 and then opt as you can guess
optimizing your neural network is is
more of an art than a science where you
try to figure out how many
hidden layers who I need how many nose
do I need and each hidden layer and then
there's a lab exercise that we do when
we do do labs and so now we're going to
dive into unsupervised learning very
quickly and here is a an art google arts
and culture experiment so unsupervised
learning you remember we don't give it
the right answers so there's this Google
arts and experiment Google AI experiment
that they created that takes artworks
from hundreds of galleries so thousands
of artworks and they're represented then
in this map but with what artworks or
ebooks typically you give them tags or
metadata with this
there's no tagging or meta dating it's
all finding structure in unstructured
data by seeing by doing visual
similarities for example if we said I
want to see ballet dancers in the
artworks so we could go right to this
area where those were clustered together
because they are visually similar and it
uses a unsupervised learning technique
called T Snee which takes the last layer
of the image in the convolutional neural
network the last layer that hundred
dimensional vector or however many
dimensions and it hangs those in vector
space and then where were we're reducing
the dimensionality to see it in two in
two dimensions here the same thing with
words you can translate words into
vectors and visualize them and now
Catherine so the interesting thing about
neural networks and linear regression is
that we're looking at when we have
examples of the right answer especially
the convolutional neural nets that's
interesting because the neural networks
are trying to look at how the neurons in
the brain might work and find connection
but also if you look at a chair you
don't have something in your brain
saying that is a chair that is the right
answer
so unsupervised learning has a big part
to play in like clustering images but
how do we find these clusters so I'm
going to just talk to a really really
simple example of clustering and the
k-means algorithm so we can see visually
that there are two groups here they're
unlabeled so we don't know what they are
we don't know that one is type A and one
is type B and we want to
programmatically find these groups so
strong circles and and so you can see
where I think the groups may be what we
do with the k-means algorithm is the K
refers to the number of clusters so
we're going to go okay I think there are
two clusters so k here with two and what
we do is we randomly initialize what's
called centroid so the center of the
clusters and the reason we randomly
initialize them is we don't want to add
our own bias to the data it might be
impossible to tell and also you might
get different answers depending where
you initialize them so typically you'd
run through this model several times but
I'm going to do it twice first of all we
look at each of the data points service
called One X and we measure the distance
between it and the centroids that we've
initialized so we're going to measure
the difference between new one and we
just use by convention we use mu to
denote the centrosome u14 centroid one
new to century two and we just look at
the Euclidean distance and then we find
which fencerows it's closest to you can
see it's closest to new one the
Euclidean distance would be it's the
distance in Euclidean space but you can
just think of it as like the Pythagorean
theorem maybe yeah so if I want to
measure the distance to myself in the
corner of this stage I could take how
far I would walk this way and then how
far I would walk that way and take the
hypotenuse okay so in two dimensions and
three dimensions you do the same thing
yeah four dimensions whatever yay yeah
the square root of the sum of the
squares in however many dimensions you
have right yeah Wow yeah I thought
you've taught me well by the way I'm not
I'm not a mathematician like Katherine
is
it rubs off I have any math tutors when
I jump in with Euclidean space feel free
and so what we do is is we then for each
individual data point we find the
centroids that it's closest to and that
is how we can then group them so all of
these data points here are closest to
that centroid in the blue own perimeter
and then so Mew one the red now what we
do and if you're doing this if you it's
very easy actually to write this
yourself so what you can do is you keep
track of the index for each of the data
points so does it belong to that Center
or does it belong to another century and
then for each of them that belong to the
first event road you work out what the
average is in between all of the points
so you look at all of those red data
points we're just going to take where
they are and then just average the
location and then what you do is with
that average you move the centroid so
you move the centroid to the center of
those points so that we've moved to
Center it now we can see that that blue
data point of there might not be closest
to view two anymore so you repeat the
first step we work out which centroid
each data point is closest to you and
there we go so in two easy three easy
steps
we found the clusters and now I've just
written this up so if you want to watch
this back and pause and read over the
slides such a summary of what I've said
again we need a way of working out what
the cost is because we need a way to see
how go one solution is versus another
and the cost function here actually just
takes the distance I'll let you go over
that in your own time because I'm afraid
we're running out of time and that
actually you made up some time yeah yes
I didn't thank you okay so so another
type of neural network is is a recurrent
neural network and it uses an artificial
neural network as well but it has to do
with time steps so all the neural
networks we've talked about so far
use give static data so static features
maybe this plant that I just picked or
some image but sometimes you have time
series data like a video where it
matters the order matters so for example
in this picture if the computer saw this
it would be able to say you know there's
two cats there there's a box but the
computer wouldn't be able to tell you
what happens next because it has nothing
to go on so what do you think happens
next
does somebody shout out what is likely
to happen next okay the the top cat is
going to close down the lid now why do
you say that okay now you happen to be
right but it's possible that the the cat
below in the box is stronger than the
cat and it's pushing its way out of the
box but you are right the cat closes the
box so now if the computer saw this if
the machine learning model saw this it
could then you know or maybe even three
or four frames of that it could then
infer based upon its training what what
direction what's going to happen next
and so that's the idea behind recurrent
neural networks in a given layer of a
recurrent neural network it loops back
to itself and then holds timestep data
when it's being trained and that way at
at time prediction time when we're
actually using it then we can infer
what's going to happen next so we could
use that in in art or art composition or
music composition so I'd like to do a
demonstration of another AI experiments
from from Google
it's called project magenta and there's
a there's some links to that in this in
this but the idea with this recurrent
neural network is that we
they did is they extracted melodies from
songs so they took everything from
Radiohead to Bach and took the melodies
as well as the rhythms from those songs
and put those and trained a recurrent
neural network with that and so where'd
I do I disappeared I disappeared so for
example here if we're starting with a
blank page then then the neural network
might say oh you know an empty
composition should start with with see
the probability based upon the corpus or
the the songs that I've trained on maybe
the first notes of C well if I have a C
then maybe the next note is a C and
maybe if I have 2 C's then maybe the
next note the probability based upon the
model is the next note would be a G and
an A and so then you can compose music
that way so this project magenta there's
a website and it's you can link to it
from here
it pulls up this application which is an
AI duet in which you can you know it's
been trained with with the music that I
just talked about thousands of songs the
melodies and the rhythms and then you
can hook up a MIDI keyboard I'm using
something called al instrument it's it's
kind of a fun keyboard but you could
hook up a regular keyboard or you could
use your mouse and play the keyboard
that's on-screen or you can even play
the keyboard on your computer just go
into the Chrome browser which supports
web MIDI and so will go ahead and play
this here here's the keyboard here and
so I'm going to play just a couple of
things first of all play a standard 3 3
chord progression and as I play
it's going to take that as input and
play something back that maybe sounds
good with that so it'll kind of jam with
me I killed it mm-hmm the sounds of
silence yes sounds of Simon and
Garfunkel I call this Simon and
Garfunkel very good and so that's the
idea yes I won I won the game and so um
so the last thing that I'd like to kind
of leave you with and also if you have
any questions the last thing we'd like
to leave you with is a video from Tesla
that kind of shows what a self-driving
car sees so it uses of course neural
networks and machine learning and the
inputs are over on the right there it's
a little bit sped up by the way so it's
seeing it's it's recognizing through
image and other detection other cars
stoplights pedestrians and then if
you're a Tesla owner by the way you you
probably know this but as you are
driving you are training than the neural
network in the sky in the cloud because
of the way that you're acting to your to
the inputs that it's seeing and so it's
learning how to drive so if you drive a
Tesla please drive carefully
okay and I think we have a few minutes
for questions so so please if you have
any questions we would love to hear them
any questions
opinion of uber did you say Uber's okay
okay um with the alphago and so did you
say uber though yeah
Oh Google yes so what's my opinion of
that okay so my opinion of deepmind
and helpful go and other things that
they're doing the a deep mind is is is
just one of the most advanced I think
areas or companies that's contributing
to machine learning they're very
regarded in this space I should say that
is a motivation for kind of getting into
machine learning to you that that the
the researchers like at google deepmind
and in academic researchers are now
making the kind of dollars that that
rockstars and professional sports
players are making there it's just
incredible how in high demand they are
and so but yeah I have a high regard for
Google brain and google deepmind time is
over
thank you very much good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>