<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Stream Processing   Concepts and Implementations by Matthias Niehoff | Coder Coacher - Coaching Coders</title><meta content="Data Stream Processing   Concepts and Implementations by Matthias Niehoff - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Stream Processing   Concepts and Implementations by Matthias Niehoff</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5inVCagXc2A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah welcome everybody thanks for
showing up this early in the morning
first talk of the day my name is
Matthias Nia I work for code centric
which is an IT company from the lighting
company in Germany and I will talk about
data stream processing about concepts
and a few words on implementation and
frameworks of those concepts so just a
short overview about what I'm going to
talk about some basic ideas by stream
processing important what is data stream
processing then some typical problems
you encounter when you're doing stream
processing going over to streaming
frameworks and then taking a look at
current innovations what is going on in
the field of stream processing and at
the end giving some hints
recommendations if you think about using
some of the frameworks which might help
you which might not best fit yeah
so the basic ideas of stream processing
so a lot of times if you do stream
processing and not in the maybe in more
older business like insurance or finance
its basic here some dump of data some
database with data you do some batch
processing on it maybe on a nightly
basis crunch we got our waiting the
results somewhere else reading the
results again doing some analysis and at
the end you have in serving layer giving
you the maybe in web UI on West API or
whatever giving you the results you
calculated earlier in the night so
basically it's a nightly or even maybe
hourly or six hourly to trigger
batch processing and after certain time
they say okay that's not all we can do
maybe just process it process the data
wide when a device also stored in the
database and the serving layer make sure
it actually always gets the newest data
the data from the speed layer above
might not be the correct one might be a
little bit and might be an estimation
and so and so on so the layer down here
was called the batch layer and the bare
layer up here is the speed layer and
most of you may be
no this se lambda architecture so having
an bench layer having in speak layer for
processing this was basically because it
was like if you do streaming these
streaming results won't be correct at
all that was the main reason why we have
this this differentiation of two layers
so but which device of streaming data
and we have a lot of sources for
streaming data if you think about where
streaming data actually every data is
kind of streaming data changing entities
you can make it as a stream because you
have an creation of an entity like a
customer you have an update on the
custom because the address changes then
you have an creation of in conflict
whatever so this could all be done as a
stream but they're more natural things
like streaming data one of the most
popular cases is of course IOT data so
all these stuff coming from temperatures
immunity from from machines in
industrial use cases but there's also
like click streams if you want to track
your user on the website maybe having an
online Shore shop or whatever you do
monitoring of monitoring data of offices
it's more like technical data having
monitoring your systems online gaming is
pretty famous with streaming data you
may be no king which is an company
producing games for Facebook I'm not
sure I might think that the candy crush
saga is from funking they are actually
using stream processing to fake user
interactions in their games and to to
improve the user experience then so
subsidy we may be a fire teach enter
data its automotive data with all the
vehicle tracking stuff with predictive
maintenance also in in the in automotive
with routing information and the score
at least financial transaction are also
are quite interesting for streaming data
because you have a lot of incoming
strands action which is nothing as then
a stream and with current technologies
you can even do like have the exactly
one semantics and all the requirements
you need for
transactional processing so all these
sources for streaming data and not so
well fitted model often lambda
architecture leads us to the distributed
stream processing and if we talk about
distributed stream processing we
basically talk about an endless and
continuous stream of data so we don't
have any defined and we don't even
define beginning like if you have bet
for selling at night you can have like I
start here arm stop here this is the
amount of data I have to process if
anything fades I can just tweet by this
part of data I know all the data I need
is inside this batch so that makes it
easier to reason about processing but
here we deal with endless data we don't
know if there's already an end there
might always be coming data furthermore
we want to you have to reside earlier we
don't want to wait until the next day
all the time you see in if you and
custom and using an internet application
and see like your case thanks for your
situation you will get your details
tomorrow because we're doing nightly
benches in my opinion everything like
why should I wait for a one for maybe 12
hours just to get some information which
can be calculated right now so we have
an need for speed and real time real
time can be defined as a few seconds but
we attempt can only be can also be like
milliseconds which is not the best case
for all these three frameworks or could
be a few minutes but it's not ours and
because the amount of data is too
increasing furthermore we need to have
the ability to scale and we do this by
our horizontal scaling so by
distributing the processing over
multiple nodes not having to scale a
scale up one single node but to
distribute it over multiple nodes so the
first step when we come to stream
processing there was a friend record
spark you may have heard of it I will
talk about it later and there was this
this was actually a batch processing
framework and they think about hey we
need to do stream processing and what
they do is actually they call it micro
batching they said okay just have very
small batches like
half a second or a little bit longer and
just process one of these small batches
after another and this is streaming so
this was the first thing they did with
the streaming this was microbe etching
why was it good thing in the first step
because you still have batch semantics
if you want to be one or if some of the
micro batch has failed
you could easily rerun this it was just
like batching you could always think
like batching all the data you had in
this batch was the complete data so this
was the first step but it further work
so we came back to native streaming so
every event is processed at the moment
it revives at the system we don't batch
it anymore so if you do matching and
have batches of 10 seconds you will
actually have a latency of 10 seconds
because if the day event arise
10 seconds later the batch is completed
it will be processed this one actually
makes sure you don't actually have
latency but you can process us on the
ongoing as the data comes in
but coming to native streaming there are
a few very typical problems you will
encounter if you do stream processing
and the most of the problems are related
to either time or when does an event
occur and when they set ups when is it
observed by the system and order so if a
event comes in rather events before that
did not have them which have not been
seen by the system or is it or is there
are some other events missing or
whatever and this is basically the
problem of event time versus processing
time the event time is the time when an
event actually occurs so if I am on the
mobile phone and playing some some
mobile game that candy crush and I did I
push the button and make some things I
don't know about candy crush is all
about I just know the name so I cannot
guarantee DC so if you do something on
candy crush um this is the time when the
event occurs which is actually the event
have the perfect thing would be the
- cursing at the same time it's seen by
the backend power system this would be
like this case event occurs a few SEC in
the same moment it is seen by the system
most of the time this is not possible so
most of the time it's a little bit at
least a little bit delayed or a little
bit further delayed then the next one is
again a little bit later on but it could
also be like there's an event and
another event is seen and for some
reason the event is really really late
and also out of order one good example
is for instance if you take an
intercontinental flight 1012 hours
everybody is switching on their mobile
phone everybody was playing candy crush
during the flight
and 12 hours later a lot of events
coming back to the candy crush Stella
arm so this is actually a thing you
might have to consider if you do extreme
precision so one thing if you do stream
processing is you slice the data into
chunks into chunks you can actually
reason about it's a little bit like
batching but it's not on the are
strictly processing time if you do micro
batching it's like 10 seconds passed in
my system so I slice here it's more
about slicing on the event time for
instance it could not also not only be
made on the event um it can also be made
on the amount of events occurred this is
a count or it couldn't be based on the
content for instance if I have a few
events and then there's a lockout event
if a tracker user in a web store then
there's a logout event which might
actually end the window because I want
all the events which happen during his
session when he was logged in and there
are a few kinds of Windows there's
tumbling windows sliding window session
window I would just go over them in a
few words so this is the tumbling window
which is actually the easiest window
above there are the stream for 5 3 6 and
so on we can assume there's one event
every second and we have a window size
of 4 seconds and we just split them into
windows of length of four seconds pretty
easy and it's tunneling because um
it just folds over its tumbles over its
no it's not overlapping nothing and on
the window we do earn a pretty easy
function in this case just doing in some
aggregation and that's it so and the
sliding window is a little bit different
not that much we still have an windows
size of four seconds but we have now in
sliding interval of two seconds so we
moving the window not every four seconds
for four seconds but we're moving the
four seconds window every two seconds
and so we have overlapping windows that
this was the four seconds here and two
seconds later we start another four
second window and we have an overlapping
here so this is if you do in a sliding
ever which of our stream this is what
you want to do with this lining window
and they are see a session window so
this is key so you take the stream say
okay I take every element by some key
for instance and user in this case and
if a certain amount of time like here
has passed I would say okay all the
events before this one window and then I
would start another window but there's
also some very small here so in this
case the window was not triggered by the
amount of time between some events but
it was triggered by and logout event
over here position actually and not in
time to bigger phone window but in
content figure but what happens now we
have we have all this window and then we
have some delayed event coming in
because of some network partition or
whatever so we have a net a delayed
event coming in here and now the or we
have to reason about this what does the
event mean for us because in this reason
it means in this case it means we have
to merge those windows the time span is
not long enough or we have actually
treated as one window so this is also
something we have to think about good
news the streaming frameworks that we'll
talk about later take our take care of
the serious things
so in recap then time processing times
pretty important if it comes to stream
processing and
this combined with Windows we saw with
triggers is actually one of the most
important challenges we have in stream
processing and one of the solutions of
the for this change is the data flow
model the data flow model was originated
at Google there was tile Aikido is the
one of the main authors there are a few
more they actually bought a bit a system
which is Google Cloud dataflow no it's
an on-premise it's another nun
parameters in cloud solution for doing
data stream processing and the main
thing is what he said is we have to stop
trying to groom unbelted in data set
into finit Pruitt's which is exactly
what micro batching does that eventually
become a complete we don't know if they
come complete but we instead to live and
breathe under the assumption that we'll
never know if or when we have seen all
of our data we don't know if there's
some innate data the only thing we can
do is we have to balance between
correctness if we want we can actually
wait for all data for all time it's just
the meaning of resources we have a lot
of memory we have with a lot of storage
to cache all the data so it's a balance
between correctness between latency when
do I get my wizard how long do i wait
and cost for resources and they made
some pretty good
I have some pretty good ideas how to
handle this and the first thing is the
watermark the watermark describes um
when will I we calculate my result so
when do I assume I have seen all my data
and so if you have if this dream which
the watermark of 10 10 o'clock then I
that means I assume all the data until
10 o'clock has been seen by our system
just to make it a little bit easier to
understand to smoke graph so this would
be the ileal way one second pass in
event time one second pass in processing
time we see all the events right
Pond when it occurs in event time
normally it's more like this there's a
little bit of skew in between our events
will be delayed the easiest way to say
okay we have a fixed watermark of maybe
we wait three seconds and after this are
we assume everything is there this will
be like this one here we see our we see
all the events in between we have seen
all the events or we have in delay of
three seconds everything is fine but
there are so some gaps in between where
the delay is actually one seconds would
be enough this is more like an heuristic
watermark this is the yellow line now so
um you see in the in the beginning
everything is fine and then the
heuristic watermark is a little bit too
slow the events coming data so we
collect the watermark increasing it a
little bit and adapting through the
actually incoming curve so coming back
to this one here we have a now at the
event time of suis we see the data at
the processing time of six so BFS we
seconds delay but we see all the data
and no data is missing with the
heuristic watermark we see we have an
delay of one and a half seconds only but
we might miss some events so this is the
first thing of wonder actually calculate
my results which is the watermark the
second part is a trigger besides having
the watermark we can have other ideas
when to or other things when we want to
figure the result already talked about
the event time already talked about
count the content for instance if the
lockout event has occurred you can
combine those so this is this could
actually be like every 10 minute in
processing time so if I do an one hour
window I can calculate after 10 after 20
30 40 50 60 minutes after the watermark
then when the watermark is reached with
each delayed event and also but also for
another 15 minutes after 15 minutes I
will just break up and say okay those
events are too late they don't I don't
have any benefit if from processing
those events
but if you do a few more triggers you
will get an individual result every time
you get a new reside after 10 minutes
you get a new result after 20 or 30 and
so on and now it's the question what are
we doing with the separate the results
we have to combine them in some way and
this is what accumulators are for so
there are three types of accumulators
defined the one is discarding so I just
care about for every partial result I
don't care for the whole wizard then
there's accumulating taking the formal
result into account adding s adding this
to the new result and an accumulating
retracting saying ok this is a new one
it's accumulating but this one was the
previous young so if you want to correct
it in your downstream system this is the
old one just to make it a little bit
easier to understand we have a stream
again our very smallest part of the
stream right now the first is the first
three guys here after the two on the
result is 777 in our cases um then the
next event are 8 and 3 so 11 would be
added discarding just gives us a 11
because there's this this is the result
of the last computation accumulating
uselessly
the 18 because 7 and 11 is 18 and
accumulating rejecting says ok the new
result is 18 but the form of his I plus
7 and so on so this is actually
important depending on your downstream
system if you have in system downstream
with just over White's the result may be
because of an idempotent operation the
accumulating accumulation is just fine
if you need all the difference with that
because you need to make different
calculations with it or the system are
underneath cannot be easily updated then
maybe discarding is an important one so
this is a theory we have watermarks and
triggers to describe when we want to
calculate the without and we have the
accumulators to handle the different
results
putting this all together the cloud a
Google Cloud dataflow guys actually
build a pretty good API our to express
this so if you take this the input is a
stream the first thing they describe is
okay we want to do in window of a fixed
size nothing else is given so this is
actually a tumbling window where the
size for 60 minutes um twittering
describes when we want to trigger the
result at watermark and also with the
early firings early meaning before the
watermark every 10 minutes and was late
firings after the watermark every time a
new event comes in but I will only wait
for 15 minutes which is the allowed
lateness after this odd events I will
just drop in ignore and I have been
discarding accumulator so I just get the
partial results of a VI calculation and
I think this is a pretty good way to
express this this way of how to handle
the stream in regards of late data so
this was actually a pretty huge one
there's another option this can actually
be presented in one slide so the data
flow model is one thing and then there's
a model of streams and tables so this is
not that complex model is pretty easy
model
it's comprar Piquet tada a confluent the
company behind Kafka right now a little
bit more so it basically says okay you
can tweet every stream also as a table
and the other way around
so if you just take this one this is
stream coming in two values coming in
and this could also be Mendez some table
with a key in the value another thing
coming in from the stream arm just add
it to the table and another value on the
stream this one again was key one just
updates the table in this case you can
think of it
are every relational database uses this
technique because it's a change log in
the database which is basically the same
as this one here so and they just say ok
if they do stream processing I would
just treat every incoming event
updating my reside my reside table doing
this you can actually improve it a
little bit not for every event because
it would be too much and I have a
treasured just a normal a short after 15
minutes I forget everything so in a lot
of use cases this is totally fine to use
but it's basically a subset of
functionality of the data flow model so
if you're planning about in stream
stream processing application you have
to think about whatever week my week
variants when do I need results how
often do I need need to need to
calculate my resides do I really need
the flexibility of the data flow model
which is really really good or is it
fine to just go with the stream and
table model so this is all the thing
about order and time which is maybe the
most important thing when doing stream
processing but there are a few other
things to think about if it comes to
stream processing one of the things is
stateful processing stateful meaning F
start something for an immediate time
and in temporary storage for instance if
I do a window window calculation and F
in one hour window I have to store this
data and catch this data for one hour oh
I do n counting or whatever so you if
you do a non-trivial situation on
trivial application you will have some
state this state is actually mostly hate
and memory because of performance
reasons it's backed up on disk
it can also be only on disk depending on
what sizes but the interesting things
are actually um I'm in a distributed
word what am I doing with my state is
why we partition my stream and I don't
want to have to stayed on all my notes
if I haven't stayed maybe key-based per
user ID I don't want to have all the
user IDs stall on every note but but I
want to make sure that the stream is
actually partitioned and divided and put
on the right note we also also the
stream resides
but then my data increases my
my so I have to rescale I have to add
notes I have to whiskey in my state as
well but this is all the interesting
stuff right now most of the frameworks
saw fit pretty good but they's there are
differences so this is the the main
challenges its partitioned it must be
for tolerant what if a state what if
some note goes down I don't want to lose
to state it must the axis must be fast
so they are different tolerance back
ends back end used for state spark
streaming for instance users and native
own build our state there was with an
off heat storage mainly then Kafka
streams and fling post use box to be
risk together with Kafka topics as a
backup for instance blinker some
snapshotting mechanism so and there's
one pretty good paper discussing all the
challenges like partitioning spreading
out we scaling and it's linked about
down there so if you're an interested in
state implementation and state managing
in streams firmly this is actually
pretty good
so another thing next to state status
mainly built from the data which comes
from the stream but most of the time
this the data inside the stream is not
enough basic maybe you only have an ID
from some IOT device but you actually
don't know what IA device it is and you
have to read some thresholds when I want
to all on something which can be chained
the threshold can be changed in a web
application and the threshold is then
stored in a database so you have to make
lookups on to the database so we have a
queue data coming in we per set it
process the data we have to read
metadata and store the results some ways
one way to do this is to read the
metadata from one note and did our DD
data is written to note for processing
and the note dozen dozen remote call
network are we
the metadata the same for every event
it occurs and which has to be will be
processed so every time the data just go
here again every time I have to read
data additional data I will make a
network request to the to the database
ask the database for the data maybe the
database is fast in processing and
giving me the data maybe the database
takes some time but I will for sure
add some latency to this one this is
actually what SPARC and fling to fling
works on these or worked on this tries
to improve this going through the local
read so we have the data coming into the
nodes again the meta data is thought
into the in the database and what we are
doing right now is at the beginning when
we start up the application we move the
data to the nodes and we also make sure
that all changes in the database in the
metadata database will be propagated to
the notes again and if you think now
about the stream and table model this is
basically just retreating the table we
have as a stream moving the stream over
to the nodes again and basically do a
stream join over here one stream is
actually a stream with pretty fast
changing data or the OEE data whatever
and one stream isn't has data which
doesn't change that much like metadata
more or less static metadata and in this
case we don't have any latency because
we do a local join on the database
important thing both streams have to be
partitioned in the same way so we have
user IDs for instances petitions both
must be petitioned in same way to make
sure all the data resides on the correct
node and then match on the same node so
this is actually a table streams model
are propagated by Kafka so this is what
Kafka streams actually tries to do so
now we have an application we can manage
time we can make lookups we can store
stores data and between first a
but now we have to run it somewhere it
works fine on our notebook works on my
machine pretty good but running some way
it's also pretty important so they're
two things this is the one time
environment above left from flink
and the right side is from spark we
don't guarantee details right now and
this is actually what you can do with
kafka screens and the difference is if
you use flink or spark or whatever you
need a definite separate cluster minitor
you need a master you'd need worker
nodes where the extra processing is done
you need some resource manager the
resource manager can be on or Maysles in
most cases um but we haven't cluster
infrastructure it's in specialized Kafka
infrastructure because the worker is an
actual flink worker or spark worker may
be done with yarn but it's an extra
hardware for its extra software for this
one and cuff christine's is basically
just a library it's a Java file you
added as a maven dependency greater
dependency whatever to your to your
application and you produce a young Java
and this Java you can run in whatever
you want you can provision it with
interpreter chef it's somewhere in some
location you can put it into docker
containers one little Q&amp;amp;A it is you can
run it on yarn you can run it and broken
containers on measles whatever you want
or wherever you want to run it or if you
if you're using a cloud you can run it
in a cloud we're using the using some
platform services you're completely free
what you want to do is we have a lot of
flexibility and if you just want it
because the RP is cruel but just not
maybe the only reason you should for
this to go for distributed stream
processing but you could go for it
because it once fine if you just have
one jar run it somewhere and you're good
to go and it automatically skates up if
you just another at another instance it
just uses Casper consumer mechanisms to
rescale another important thing if we
run it we have to monitor it um make
sure there is no all-in-one SuperCrew
solution for this one all the frame the
all the frameworks have different sway
different ways to monitor the
some have been pretty good UI maybe all
of them happens west api sometimes of
worth a lot of more with more
information information sometimes with
less information um whether metrics is
often used if you want metrics then
there the java classics on the other
side
J mix everybody they are based on Java
so they all offer some gem its
capabilities are using a profiler you
have to monitor your scheduler if you're
using for instance Kafka screens so just
move the complexity out of the stream
processing but tweezers scheduler you
have to do your own logging having
distributed logging and you will login
and distributed environments so it's
it's not easy distributed environment
makes a little bit more complex so after
talking about running this solution we
will talk about delivery guarantees
which was actually quite interesting at
some point because cough cross streams
and together risk Africa only supported
at least once guarantees so at most once
means I will make sure you get it at
least at most once either a zero or one
time the event you will see it at least
once means I will make sure I will
process the events but it might happen
that I'm person at multiple times not
only one time in exactly once means um I
will make sure I will process this event
want and only once not zero ma not zero
not two times I will force it at once
there are few different techniques to
acknowledges attitude to achieve this um
Victor clung once had this tweet which I
actually find pretty good um effectively
once meaning that you have at least once
guarantees and doing idempotent
operations so regardless how of me you
execute this operation the result will
always be the same after the first
execution why is this is actually a
pretty good idea exactly once is quite
challenging challenging in a distributed
environment
ASEC T once needs a lot of interaction
between nodes between between systems
and processes there's a lot of
coordination going on and at least once
much easier if it comes to resources if
it comes to latency and performance so
at least once it's most of the time
pretty good if you can go with at least
once and idempotent operations I will
always go for at least once and not to
try for exactly once but that to
understand me one they are definitely
exactly once used cases where this one
is pretty important and there's a good
reason
Kafka now added exactly ones
capabilities to their solution because
in finance and insurance companies and
so on executive ones it's always pretty
good important so I already talked to
mentioned a few streaming frameworks a
few times so spinning frameworks is
nothing but an execution engine designed
for unbounded datasets which is Palakkad
also there's no thing what in streaming
framework should do besides sending
unbounded datasets and we see we've seen
a lot of things now which is which which
are important if you want to handle
unbounded data sets and the first thing
I want to talk about is a Patrice Park
it might be one of the most popular Big
Data projects right now it was basically
batch programming first then doing
microbe etching adding this a spark
streaming to spark this is an park
streaming which is was only the engine
taking this team putting this in two
batches and then taking the cost Park
enter to process the data
sparks filming is actually pretty
widespread spark is distributed with
every Hadoop distribution you can get it
everywhere but spark streaming itself
does not support event time window
operations are quite limited to the
because you always have to stick with
micro batch size you cannot have you
always have to be a multiple of the
micro batch size but they work on this
right now is Park structured swimming
one thing that they unified the API
further we see batch processing which is
pretty good so if you read a file from
Jason this looks like this and if you
read and stream from Kafka it's just
weed stream instead of wheat so this is
quite a not quite the same pretty easy
API
they were working on the event time
operations the last time I checked the
event mo probation with late data was
just like you okay you get your results
later if you have wait for ten minutes
you get your results ten minutes later
so there was nothing like triggers or
intermediate result or anything like
this but they're still working on this
one they get rid of micro batching
underneath so pretty good then we have a
Petri flink and European German project
in Berlin right now so they started also
with so okay we do stream processing and
vegetable settling coming from batch
processing it's just a special case of
screen for settings on the other way
around
we have been streamed with a limit with
an boundary they focused much on stream
processing which was which is quite good
in my opinion the best part of link is
not that important but the stream for
singing is pretty good they do all the
event tending stuff they have watermarks
they have triggered if accumulators
there's really low latency so this is
actually pretty good if you want to go
for event time handling and then there's
a petrikov cross streams whoever you
know scoff come maybe most of you
because Kafka is the most important
message book a fig leaves a message
brokers it's a lock
it's a lock only but if it comes to
stream processing this is most of the
time to sort and then at some point it
was like hey we are the source of
everything why don't we process the data
as well then there was two Kafka streams
API which is basically um a better not
so simple consumer anymore like the
normal Kafka consumer but has some
streaming things with it like the table
streams concept for handling light data
it can only read from Kafka it can only
store to Kafka so if you want to store
in a database you need something else
like Kafka Connect which takes the
stream of data and stores in the table
streams and table again you see the
pattern and it heavenly realized on
Kafka so all the hard stuff of stream
processing like Arabic like partitioning
like scaling they all do it with Kafka
they all just give it to customer and
that's that the reason what they have
only a single library instead of a whole
cluster so if they say okay where's
Kafka streams you only need a library
that's only half the truth the other
truth is because you have Kafka assert
cluster handing the hard stuff what's
going on and stream processing right now
a few interesting things you remember
this this view this is the state we have
some data coming in doing something with
the dates
states throwing something in the date
some aggregation some wind or whatever
and processing processing again um
normally you would Vidal in the database
in the downstream system but why not
create the state we RF or the data in
the state it's distributed yeah but
that's one thing you can do so this is
known as query every state which is a
little bit better names and interactive
tree what Kafka streams calls that so
and the it's still pretty low-level in
the framework so there's nothing about
data lifecycle what happens when the
node goes down the state is lost where i
want to i want to reload the state i
want to clear the state whatever this
realization is an issue for instance if
you query the state using flink you
still get bytes you have to deserialize
since you're - you're safe and Kafka
streams for instance you have the
partitions to a state and you have to
figure out for yourself which node
actually I have to create for the state
so this is still work in progress but
could be at some point really
interesting but I would never never say
that the database would be a would be
not needed anymore if this is production
already so they still a need for the
database
another thing is writing you're curious
actually as a sequel Kafka was pretty
big and marketing the last months
promoting their case equal actually
fling had sequel for a long long time
ago but not doing so much marketing
about this so this is nearly Aldi secret
a bit different if you see here we
create a table s and then this is this
three
doing a group by and having this are
pretty normal to sequel but we also add
a tumbling window of five seconds so in
this case we were just counting the
number of transactions with one credit
card in five seconds if there are
multiple transactions that might might
be an indication for a fought pretty
easy an example but also pretty easy to
write for a lot of people because secret
is pretty widespread so this was a lot
of stuff right now we talked about the
problems we talked about a little bit
about frameworks are doing stream
processing I want to give at least some
recommendations some hints when you
might think of stream presenting the
first recommendation is if you have
small just a small data set don't use it
that's the first thing although we are
all technologies and like really new
technology if you only have like 500
events per second but something with
Java or whatever was Carla or occur or
whatever and use this one or cut them
this is a new one sorry forget this so
but if we go for this review then sparks
beam might be a new option if you
already use Hadoop if you're familiar
with or are we already used spark in
batch processing and if you don't have
event I'm hidden link event amending is
maybe dumb thing if you really have
heart recurrence movement and handling I
would really think about now maybe Spock
is not the best option my change in
structured streaming so have an eye on
structured streaming this they actually
working pretty good on this one
and the good thing on SPARC as the
community is really really good it's
this pretty huge really have for then
fling if you need all these event time
stuff like watermarks figures and some
take a look at fling this might be an
option
and cough cross streams if you just want
if you already use Kafka if you just
want to start with maybe small footprint
the event I'm handling risk Africa has
if it's okay for you if you don't leave
all the trigger stuff and so on might be
good then Kafka streams might actually
be a good fit I try to do
short comparison are especially the
differences are mainly between hardware
deploy this stuff and how good is the
event I'm handling or the other stuff is
they learning from each other and
inspiring each other which is pretty
good then just a few words on if you
ever income this so Apache beam is what
Google Cloud dataflow they they built
all the platform-as-a-service stuff then
they made a Pepsi beam out of it Apache
beam is the API having different one us
so you have can specify the API and beam
we see small Scott snippet we've seen
earlier and have different one us and
the needs for instance filling for
instance Google Cloud dataflow it's a
kind of abstraction then Google Cloud
dataflow the Google product and there's
apex which is an only yarn based stream
processor then there's flume a lot of
people say hey flume is also stream
processing in my opinions only locked
file shipping from getting data log
files into HDFS and then the storm and
Haren and actually storm is pretty right
foot because tom was the first one I
dismissed it here because if you start
right now I in my opinion a lot of
people tend to use the other suite
frameworks but there's a lot of storm
deployments over there the first stream
processing framework so you might
encounter a little bit of storm as well
take away so swimming is not easy we
have event time we have state EF
deployment we have lookup we have the
correctness which deliver we guarantees
we have the distributed system there are
basically two concepts which are a
little bit different at some point they
match at some points they differ a
little bit be aware of the monitoring
it's a distributed system and don't use
it if you don't have an
distributed problem if you have only a
small problem if you don't want to skate
if you don't have the need to scale in
my opinion don't use these frameworks
play with them but most of the time a
normal Java process will do the same
that's it for me thanks for your 10
and if you have any questions please
so I don't see any if you have any
questions just come up front to me other
otherwise or is there some question
spring cloud dataflow is more like if
you don't have them really scalable for
escape problem yeah it can be scared but
it doesn't have all the event time
handling stuff sprinkler data flows it's
more like in Java library we always are
the distributed time managing stuff then
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>