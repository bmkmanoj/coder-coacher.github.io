<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Docker Container Orchestration Platforms on Amazon by Arun Gupta | Coder Coacher - Coaching Coders</title><meta content="Docker Container Orchestration Platforms on Amazon by Arun Gupta - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Docker Container Orchestration Platforms on Amazon by Arun Gupta</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/woq063HeQTs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and today i'll talk about container
orchestration on Amazon Web Services
Platform just like everybody I have a
Twitter handle as well so first of all
this is not a dock or 101 talk so I
presume you have basic understanding of
what docker is but if we look at a very
high level what is it that the docker
container is really offered to us or
what is that the container workflow
looks like typically if you think about
this there are three components to do
occur there is a client which is the
darker CLI which is how you instruct to
build an image or run a container then
there is a docker host typically
multiple hosts set up in a clustered
environment where you say this is where
my containers are going to run and then
there is a registry you know it could be
docker hub could be Amazon ECR or some
third party registry where your images
are living so your typical flow is the
client would say the client would be
pointing to a docker host and say ok now
go run my container the host says the
image does not exist so it goes and
talks to the configured registry
downloads the images and then run the
container the important part here to
understand is the client is completely
stateless client gives a convenience
command say docker run it goes as a REST
API under the hood the darker server of
the docker engine on the host is
listening to that command it receives
that REST API does all the things that
need to be done on the host because the
client is stateless and then returns the
response back to the client of whatever
you expected it to be now typically when
you're building your application that
application will have multiple
containers let's say you're running a
few nginx web servers you're running a
few walls like containers and you're
running a few MySQL containers so very
easily you know when you start with an
application you typically have multiple
containers running and then
have these hosts soccer host so to say
one people hosts of those running so
your application very quickly and very
easily becomes a multi host multi
container application and that's exactly
the role that this container
orchestration frameworks play they
essentially say okay you know you can
have a cluster of docker host somebody
needs to manage that if a host goes down
what happens in that sort of behavior
and then when I am placing a container
on a host there needs to be a scheduling
algorithm somehow I need to figure out
that your container needs such-and-such
memory and CPU and I need to figure out
which of the hosts has that capability
and I should be able to match that
request and if it doesn't happen to be
so then return the response
appropriately so that's exactly the role
for container orchestration frameworks
so what I'm going to do today is in the
next leg say about 45 minutes or so give
you a quick overview of for container
orchestration framework we'll look at
the darker swarm of the docker engine
itself which gives you the orchestration
framework we look at kubernetes which is
open source framework originally created
by Google and then donated to CNCs then
we'll look at Amazon
ECS which is ec2 container service and
then we'll also look at mesosphere so
let's get started first with docker now
how do you do your development with
docker well essentially if you think
about this there is darker Community
Edition and Enterprise Edition you can
go to docker comm it will tell you on
which version you want to start with
Community Edition is all out in the open
source and anybody can download the
tools over there Enterprise Edition is
when you're ready to go into production
and that's where it provides some
additional value-added features over
there now when you are doing development
then you have the option to say I want
docker for Mac Windows or Linux because
Mac is something that you do development
on that's not something that you go
production with essentially Windows and
Linux you can go production with and
then of course if you could do your
development on AWS and Azure as well so
in that sense if you can download a
darker Community Edition for Mac Windows
Linux
and AWS or as your we'll talk about the
AWS part in a second now if you download
the darker seee
or the Community Edition the releases
are done on a monthly basis
those are bleeding edge releases anytime
the code is put in there you know they
will build it out and then the end of
the month pretty much they release
something new so that's your community
docker seee the author for Mac for
example releases and then they also have
stable releases which are every quarter
but this is purely for development dr. e
on the other side is also release
quarterly and it has a support lifecycle
for up to a year but our focus will be
primarily on dock of a Mac so this is a
macbook that i'm using so all my
development in that sense can be very
easily done on a single box very easily
in a single docker engine i can enable
the orchestration and say okay now run
my multi container application so how do
i do you know i have a single macbook
here in that docker you know where I've
installed docker for Mac and I could
just simply say docker swarm in it and
it is an optional mode but it enables
orchestration in your single docker
engine now now if your macbook has
multiple Network addresses or network
interfaces and you can say I'm going to
listen on this particular host or this
particular IP address and then the port
on which cannot listen so this is
basically a single host darker engine
running in a swamp mode now you could
also run multiple VirtualBox on your Mac
and then each of those could be
configured as a docker host and then you
can say my first node is always a
manager but now I want to add a worker
as well because I want to create a multi
host cluster essentially the difference
between manager and worker is manager is
responsible for the actual orchestration
of the containers and the cluster itself
worker is only where the containers are
running you could easily run up to you
know a multi host cluster now I mean on
this Mac
for example I've been able to easily run
up to six VMs in a VirtualBox VM and set
them up as a docker swarm cluster okay
now you need to specify the token and
depending upon the kind of token that
you specify whether it is a worker token
or a manager token you could join the
node to the cluster so essentially you
know if you want you can have a multi
master docker cluster with multiple
worker nodes around it and this gives
you the ability on how you can easily
set up your cluster very easily for your
local development now that's good for
local development because then all of
that development is happening on your
local machine but you really want to
simulate this on the cloud and that's
where you're going to Amazon Web
Services
so darker ink the company and Amazon Web
Services have partnered together to
create what they call is CloudFormation
template so this is the Amazon language
so essentially the CloudFormation
template now you take that
CloudFormation template you're going to
say I'm going to run this CloudFormation
template in such-and-such region so
let's say you pick up US East or us West
what about region you pick up and then
it's going to create all the resources
that are required for you to spin up
your darker cluster so it spins up your
ec2 instances you know it configures the
master it configures the worker all of
that configuration is automatically done
for you it also configures your workers
and masters into auto scaling group and
what that means is if a node goes down
because of being in auto scaling group
the node will automatically come back up
it also configures ELB an elastic load
balancer so any services that you create
in that darker cluster can be front
ended with a ELB it also has connection
with EBS so now you are running say
stateful containers so your state could
be stayed on those EBS stores as well so
once again you can download docker
Community Edition for AWS or you can
download docker
Enterprise Edition for AWS so if I can
switch to a slide for switch to browser
for a second here so here I am at docker
comm essentially and if I look at the
very four
slang here is say get darker community
edition okay so I'm going to go here I'm
going to say get darker see II from
darker store now here is showing darker
Community Edition for Mac that is good
for my development here these are all
the windows and the Linux and then this
is where I'm looking at darker Community
Edition for AWS so if I go here it can
say get darker for me here and if I
click on this link this will essentially
bring up my AWS console and in the AWS
console it is actually pointing to a
CloudFormation template and then in the
template the link is automatically
loaded for me that where the template
URL is but now when I click on next it's
going to ask me okay what is your
CloudFormation stack name how many
number of swamp managers how many number
of worker nodes you want then you can
start assigning certain swamp properties
you can start defining your ec2
instances here so let's say I want a m3
extra-large here then you can say how
much storage volume you want similarly I
can define for the worker typically I
would like my worker to be a little bit
larger size because that's essentially
where my containers are going to be
running so maybe I can go here and I can
say m4 extra-large here and once you
have defined your instances then you
click on next here take everything
default here it just confirms everything
looks good actually if we can go back
here we can also choose SSH key which
allows us to interact with the cluster
later on because these are all
essentially ec2 instances and this will
allow us to SSH into the instance as
well so now I go back here next again it
confirms that okay can I create the I am
resources for you on your behalf and
then you click on create and there it
goes so we'll come back to this a little
bit later now towards the end of the
talk but you will see that how your
cluster is automatically created now
your dog or swamp cluster is created in
AWS all you need to do is use your
docker client on your local machine
to that cluster you can literally set up
a tunnel and then instead of doing the
development on your local machine the
same darker compost file or the darker
images that were running on your local
machine are now running on AWS it's very
seamless migration from your local
machine to remote one so we will wait
for this to kind of spin up my cluster
but essentially if i refresh it here now
you can see my CloudFormation template
is in action is creating the resources
is going to create the subnet the VP
sees the storage volumes all of that
will be created for me all right let's
switch gears let's talk a little bit
about kubernetes then now we understand
that the occur is the base you know
where your image is stored that's sort
of the most prominent format although
that is not the only format so
kubernetes
is a open source orchestration framework
that was originally created by Google
based upon their multi-layered
experience of running borg
so essentially inside Google any app
that you think of and that is back ended
by a container and that's their own
format of container and that system they
call is Borg and in that Borg you know
about a couple of years ago the numbers
were quoted that there used to run two
billion containers every week that's a
lot of containers running every week so
they had do have experience in terms of
Google scale on how to run that many
containers and what could potentially go
wrong with that so Google essentially
leverage that experience and created
kubernetes is a brand-new codebase but
the experience is there now the basic
terminology for kubernetes kubernetes is
pods pods is a co-located group of
containers docker containers that are
shared together don't think of it as oh
my my model my view my controller or my
wall fly my database and my nginx they
are co-located no you want them to scale
independently think about your
separation of concerns and that's how
you package your pods so pretty much you
know think of it from practical purposes
one-to-one relationship between pod and
container but that sort of the unit they
have identified as a basic unit
and this the containers within the pot
they can talk to each other using
localhost they share the namespace
filesystem all that now pods by the
nature or ephemeral and what that means
is a pod is running today on one host if
the pod dies for some reason it will
come up on a different host it will be
given a different IP address now that's
not an IP address that you can rely upon
though because IP address can keep
changing or for some reason the node
goes down then the pod comes up on a
different host now think about your java
application talking to a database
application and if your database is
running as a part over there you can't
rely upon that IP address and that's
exactly where and how the concept of
service comes in service is a single
stable name that you can give to a set
of pods all together so essentially your
pod has a label and that label is what
is being observed by the service and
then service can say anytime any part
comes up with that label that part
becomes part of my service and in this
case for example I am running to nginx
pods they have a certain label and
service is sort of now the front-end for
it
now my other pods can talk to the nginx
parts not directly but using service and
so this is how they really talk to each
other by using labels now typically you
will not run a part by itself you would
typically wrap it up in a concept called
as replica sex and what that means is
that lets say i'm running my nginx
server or my wildfly servers then i want
the ability where those servers can
replicate themselves that means i can
run multiple instances of the wildfly
servers depending upon the demands or
the needs of an application I should be
able to scale the number of replicas up
or down and that's where replica set
comes in so all you do is you define
your configuration in animal file in the
Yambol file you say here is my service
here are my pods here is my replica set
and then you pass that to kubernetes and
say ok now go ahead create these
resources and then at the runtime you
can say
scaled my replica set up and down and
that's the way your application looks
like so pretty much you have to take
your application break it down into the
concepts of port service labels and
replica set and those are sort of the
main concepts that you work with now on
the off side of it just like docker
there is a concept of a node here as
well which is basically a machine or a
worker machine or a VM in the clock and
the in the cloud now of course there's a
master master has some key components
just like darker there is an API server
which is listening for requests coming
from a client docker has a docker CLI
kubernetes Ice Cube cutter or cube CTL
oh lovely call as a cube color so the
cube curdle is where you take the
configuration file and you give it to
cube corals say go ahead create these
resources and it will create the
resources for you then behind the scenes
in the master there is also a scheduler
because the scheduler is responsible
you've got a five node cluster running
in the back where should I schedule a
particular part there is also a
controller manager so for example
replica set which says okay you said
create five replicas of this part and
I'm responsible for those five replicas
but what that also means is if a
particular replica goes down then I'm
going to bring that replica or that
particular pod up not that particular
pod up but at a given point of time I
will make sure that there are five
replicas because that's what he said
I'll make sure that that's running annex
II D is essentially is where all the
labels are stored and that's a
distributed watchable registry on the
worker side of course there is a docker
host running on it even though
kubernetes supports multiple container
formats as well well rocket being the
other one and they're also looking to
support OC i compliant container format
as well but essentially you know you
have your docker host running over there
that's where your pods and containers
are running and the docker host or the
cubelet itself is monitored by your
operating system specific processes so
that in case it goes down that process
can come back up again so let's take a
look at our overall architecture of how
governess would look like well here we
got our master we have our cube cuddle
which is basically my client where I'm
going to give my configuration file that
okay cue cut we'll go create these
resources those goes to my API server
behind the scene again is a REST API I
got my workers lined up over here I'm
just showing a three node cluster one
master and two workers essentially API
server talks to the scheduler and the
controller manager accordingly and then
he says okay now I need to go ahead and
create these resources instead of
creating the resources directly it talks
to cube let and cube let's say okay you
know what I'm going to create there is
parts on this particular machine there's
a proxy which is which is what acts as a
proxy to your pods now on the right side
what I'm showing you is where my client
is setting the client is talking to a
load balancer could be like a f5 or an e
lb load balancer which then talks to the
proxy to get the application being
served so this is essentially if you
think about this in the middle is what
is my kubernetes cluster on the top left
is my client that is creating these
resources and on the right is where my
client is actually accessing the
application okay so cube Caudill as I
said is my entry point here which is
what controls my kubernetes cluster
manager once my cluster is created I can
say cue card will get pods which is
basically what my pods are running know
how many pods are running or I can say
get nodes and that terminology used to
be minions earlier so you can say you
know show me the nodes that are running
in there you can create any resource
cube cut' will create dash F and the
resource configuration file you can
update or delete any resource now let's
say a replica set is one of the resource
that you have created and you want to
change the number of replicas for that
resource so then you can easily say
resize - - replicas
how many replicas and replicas set and
then the replica set name now there are
multiple ways by which you can run
kubernetes cluster on AWS what I'm
showing you here is well let's go to the
link actually so the easy link for you
to remember is
you can just remember the link
kubernetes - AWS dot io that is
automatically redirected to you for now
kubernetes is running on AWS so there
are literally what 15 odd ways by which
you can run kubernetes on AWS there are
a couple of my favorites you know I like
to use cops a lot now cop just the way
cube Caudill manages resources for you
on the kubernetes cluster think of cops
as the cube cut' rule for clusters
itself you know it kind of helps you
manage the clusters you know kubernetes
clusters so cops is one of my favorites
I've played with that quite a bit
actually then there is tecktonik
installer that is pretty good as well so
let me show you a little bit about cops
here so if I go to cops essentially now
this is the github repo now you can see
is github.com slash kubernetes slash
cops I won't dig into the details of it
but essentially the key part here is
with cops all you can say is well it
first of all it allows you to create a
multi master cluster very easily let me
show you one of the command-line
configurations here there's a full
detailed tutorial over here essentially
you need to create a I am user where it
needs specific access here so you need
to give those access to your account up
until one six one you have to configure
a DNS server because that's where all
the information is stored about it
that's how where your API server is
hosted so that's something that
additional work required on your side of
it but one six two was literally
released this morning and it has some
gossip based protocol where you don't
have to set up the DNS server
so things are already improving a lot in
terms of one six two
you also need a s3 bucket where your
cluster state is going to be stored so
let's say you know should happen the
whole cluster comes down but then your
cluster state is stored in an s3 bucket
the whole thing can come back up again
because of auto-scaling groups and
things like that what version of
kubernetes is being used so this can all
automatically all come back for you so
really what you need to do is you're
going to say what is my cluster name
what is my cause state store
all right now it's only possible to
persist your state store in s3 bucket
but there are other persistence
mechanisms are already being discussed
but you just export these two variables
and then you can say cops create cluster
and I'm saying zones uswest to a so
essentially what I could do is I could
spread my masters across multiple
availability zones in a region you can
only create a cluster in a region not a
cross region for that there is federated
clusters but if you are creating your
cluster in a zone you can easily have
your masters across multiple
availability zones you can have your
workers spread across multiple
availability zones so if I go to my
shell here if I show you cops help here
it can show you the different commands
that are available so you can create a
cluster delete a cluster update a
cluster but then if I go further here
when I say create cluster it shows me
different options so here I am saying
the zones in which to run the cluster so
you can say ok this is where my worker
nodes need to be split across and then
if I know that in u.s. West two there
are three zones so 2 a 2 B 2 C I can set
up those different zones and then my
worker nodes are spread across those
availability zones for higher
availability similarly there's a master
zones the spring as well so you can say
now go ahead and spread my master across
multiple availability zones so all sorts
of fancy combinations are available it
also provides support for rolling update
of a cluster so for example let's say
you
running on kubernetes version one five
seven and now your one six six is
release which is what is being supported
by or one six two rather which is what
is supported by cops today then you can
say cops do the rolling update so it
will update all the masters and then it
will update all the workers and without
you doing anything cops will take care
of it now the important part to
understand here is this is not a managed
service so this is still on you to
manage the cluster essentially alright
so that's my second continuing
orchestration framework now let's take a
look at the third container
orchestration framework which is Amazon
ec2 container service now Amazon ec2
container service is a high performance
highly reliable fully functional fully
managed service by Amazon that's one of
the biggest advantages of it so from
your side there is no management that is
required just like any AWS service you
go to aws.amazon.com slash ECS and you
say i want to deploy a container and
that container just like there is in
kubernetes there is a concept of a pod
in this case you create attack okay so
let's talk about the concept first in
terms of ECS it provides you full
cluster management all of that cluster
orchestration the management you know
where the instance is running that is
completely behind the scenes handled by
AWS it provides you container
orchestration so it has the bin pack
scheduling mechanism by which it
accordingly figures out where your
container needs to be placed and it
looks for you know the same availability
zones and all those concepts that you
are used to particularly working with
the AWS and the cool thing over here is
because it is a product by AWS it has a
full deep AWS integration and I'll touch
upon some of those points is a bit later
so how does it work well first of all we
have to have easy to instances that are
becoming part of our cluster so when I
go to ECS and I'll show that to you in a
second
I need to create a cluster and as part
of that cluster I need to define I need
a six node cluster so those are call as
container instances
then I need to have services because I'm
running essentially services over here
so if I think about my typical
application where I have a front end and
a middle tier in a back end those would
be my three different services and each
of the services will have tax now
Services is a way by which you scale the
task
so essentially front-end service will
have our tasks but it could have
multiple instances of the task similarly
my back-end service will have a task and
could have multiple instances of that
task now this is a very classical
example here let me see if I can walk
you through this one here okay so on the
very outer edge what you see is the AWS
cloud then what I'm doing here is I have
my ECS service sort of the second big
square over there now when I'm creating
my ECS service I my service is
distributed across two availability
zones identified by a Z 1 and a Z 2 and
then in the AZ 1 and AZ 2 is where my
container instances are running and in
those container instances essentially is
where my tasks are running so if I want
to scale the number of contain the
number of instances of a task all I say
is service scale-up if I want to scale
it down of course I say sorry scale down
so what is sort of the deep integration
with the AWS essentially over here now
here I can specify I am role to be used
by the containers in a task so right
away you can start identifying that ok
this service should be running in this
particular I am role and that
integration is possible the service can
deploy and scale very easily and very
quickly and it has all that integration
built-in so for example AWS has code
pipeline and code build tools so you can
trigger from there anytime there is a
change over there and I'll go ahead and
scale my service or deploy my service
over here you can also set up your
service to scale up or down based upon
cloud watch columns cloud watch is a
monitoring system within AWS
so anytime you know you can set up that
such-and-such alarm happens that ok my
CPU has increased to 60% automatically
scaled my service up or you can set up
your alarms comple--
specific to your application you can
audit through cloud trail so it provides
you full non-repudiation so you have a
full trail of events that have happened
on how your services have scaled or how
your service to task as a task control
association looks like another cool
thing that is with ECS is lb or
application load balancer now you can
expose each service as a ELB a very
classic ELB but then for each service
you will have a separate eld essentially
what we recommend to our customers or
developers is have an application load
balancer in which case the request comes
to alb and then based upon your header
or your path you can redirect to the
appropriate service so that's where
you're creating a single ELB instance as
your front-end to your application and
then you can route it to the right
service which then goes to the right
task based upon your application needs
so what you do is you create a simple
task definition in this case I'm saying
I'm going to use the image of J bar
slash wildfly the image name is
certainly not correct here I can define
the soft limit and the hard limit for
the app or for the container for the
task in this case for example I can do
my port mappings are shown towards the
end here and then I can configure my
service well you know what let me show
you in the console itself so here I am
in my let's get into the console first
so I'm in my console here and I'm going
to pick up ec2 well let's search it the
way you would do this I want to do
something with containers so I get my
ec2 container service here and I can say
get started well as part of the
container service of course I can run
the containers or you can say I just
want to host a registry where I'm going
to be hosting my docker images so you
can do either of those but for now I
just care about spinning up my
containers so then I can say continue
there are some pre-built samples
available for us but what I can do is I
can define the task definition I can
give it the container name and here I
can say I want the wildfly container to
be running here now this is a wildfly
container so I want to give it at least
a megabyte of memory and then I want to
define my port mapping here I can click
to next step here then is saying okay
this is your sample web app how many
number of tasks how many instances of
the task you want to run here I can
click on to for example I don't want any
load balancing at this point of time it
can always be configured later on and
then I can click on next by default it
says ok I'm going to use a t2 micro but
here I can change that ok I am looking
for m2 extra large and because you have
said 2 tasks I would like to make your
task being distributed or highly
district available so I can set up for
instances in this case and once again I
can set up the key that I care about I
can define the ingress and with my
firewall ports etc I can define the I am
role it says review and launch and at
this point if I click on launch instance
and run service this is basically going
to go behind the scenes run a cloud
formation script and set up a four node
cluster for me create those task
definitions all of that now this is good
from the console perspective as a first
experience but very quickly you want to
automate this so essentially you are
task definition all of your commands etc
can be configured as a JSON file and
then of course AWS CLI that's
thing that I've been playing with so you
can literally create your cluster create
your task definition deploy your
services scale your services do all the
cloud watch alarming all of that using
the CLI there is no need to go to the
console essentially as a matter of fact
so all of that can be very easily
automated so in that sense it is very up
friend Lee as well in terms of other
services yes there is a LV that we
talked about where it can be configured
as a classic ELB or application load
balancer which does the load balancing
based upon your host header or your pack
it integrates with EBS V PC now it
automatically creates a V PC or you
could say no I have my own V PC run this
ECS container or ACS cluster in my own V
PC as well we talked about cloud watch
how you can actually set up a cloud
watch alarms and then on based upon that
you can trigger your service to scale up
and down you can define the IAM roles
you can define the identity and access
management roles say this particular
task is going to run in this particular
I am role so that's a if you have
multiple services you can start defining
multiple I am roles in terms of the
capabilities that those services are
capable of it of course also everything
is trackable using cloud trail and it is
very well integrated with code build and
code pipeline so think of code pipeline
as you know a container or a code
pipeline is where you will have your
build system all set up and oh think of
this Jenkins actually you know is on AWS
alright and then the last orchestration
framework that I want to talk about is
mesosphere so mesosphere is a company
well medicine is not our Christian
framework mesosphere is a company behind
the continual orchestration framework
essentially and the container
orchestration framework itself is DC us
so and this us is essentially based upon
Apache mezzos which is sort of all the
big
for behind it so what is Apache measles
Apache measles were started as a
research project in 2009 in UC Berkeley
and it was an open source cluster
manager so this is actually predates
before the containers were even
introduced to the industry so all it
does it it provides the resource
isolation and sharing across distributed
applications and what does that mean
okay well it says you have a distributed
system now we have a bunch of machines
and each machine then has a resource
like a CPU resource of Emory resource of
processing power and all those things
independent of where those resources are
existing you want to provide a unified
interface to those resources and then on
those resources you can say I want to
run certain applications ok so let's
take a look a little bit better so you
can say I want to now have a unified
reset of resources on that I may want to
run Hadoop or spark or Jenkins whatever
you going to run it exposes that
interface it also provides support for
cluster monitoring and behind the scenes
everything is run using Linux containers
that gives that task isolation so let's
say how does the Maysles architecture
really look like the concept of mezzos
slave is basically those individual
nodes where all the work is going to
happen now you could have multiple
measure slaves with all different
capabilities and that's where measures
master provides a unified interface to
what what resources are available to run
a particular application and on the top
what you have is different scheduler so
for example I may have a Hadoop
scheduler the Hadoop scheduler will
actually schedule the tasks using
measures master of course multiple
masters can be done and those masters
are configured using a zookeeper quorum
so let's look at a little bit more
detail here in terms of how would you
really set up the cluster so first of
all you set up a zookeeper quorum
because that's where you know all your
masters are going to be registered you
set up your first master and then you
can set up your standby masters as well
so at a give
one point of time there is a single
master then you can have your slaves so
multiple slaves I know this could be VM
this could be nodes this could be
wherever and all if we are running in
AWS these are AWS ec2 instances and then
on the top what we have is different
frameworks that are supported on top of
Apache measures there is spark Hadoop
these were supported no far before but
marathon is the framework that we care
about because marathon framework on top
of Apache measures is what supports your
docker containers essentially okay now
each of these frameworks have an
executor and those executors are the
ones that are running in the underlying
slaves essentially so for example I
could have a coup Burnett is executor
now it goes beyond me why would I run
VCOs and have a kubernetes executor on
it on top of AWS I would rather just run
kubernetes Raw on AWS that's sort of my
preferred approach but let's say if you
have bought into the design of Maysles
where you want that you know cloud
agnosticism and you want to be able to
run your nodes across multiple clouds
then you can look at that approach where
okay no but give me those resources that
are available to me consolidate them and
then I can run multiple frameworks or if
you are thinking that hey I want to be
able to run my Hadoop and my spark and
my Kafka and my docker containers all
together as part of a single framework
then you can look at mesas as one of the
things over there as far so in terms of
frameworks frameworks are targeted at a
use case and very domain-specific and
that's the ability which really allowed
measures to evolve and say okay we're
going to provide a marathon framework
which can now run docker containers so
essentially the way it works as I said
and a master node offers resources to
each framework the framework accepts
that okay you know there's a concept of
offer and accept so the master makes an
offer that I have such-and-such CPU and
such-and-such memory available and then
the frameworks would say oh I will
accept that offer and
is a task that you run using that offer
now the framework itself has scheduler
and executors based upon what offers
have been made it will actually send an
executor executor using the scheduler
the scheduler basically registers with
the master for the offer not ok this is
where I'm going to execute a task an
executor basically launches the task on
the slave nodes so that's sort of your
Maysles terminology to understand this
better and it also passes a description
of the task to run so let's try to
understand it slightly better now I have
my four slaves down at the bottom I have
my master and the master there is an
allocation module and then on the top I
got my different frameworks so in this
case I'm just showing a marathon
framework and a kubernetes framework and
each of them have a job that needs to be
executed let's say they both have two
jobs and they also have a scheduler as
well okay so let's say the two slaves
make a raker offer to the master that
okay I got 4gb you 4gb and for CPU and
similarly the slave 2 says I have 4gb
and for CPU so to the master
that's a consolidated set of resources
that are available the master the
allocation module within the master will
make a request to the framework that
okay hey I got these two offers and a
notice is not making the full offer as
is it's not just a pass-through master
may decide to offer it now only whatever
is available or whatever it wants to
offer and all it could do the time
sharing between different frameworks so
for example in this case is just passing
2gb and 4gb from two different offers
essentially now based upon what did the
scheduler from that particular framework
accepts it registers with the master and
then the tasks are actually placed onto
the slave nodes by itself and then
whatever is remaining the allocation
module can actually offer to the next
available framework okay so how do you
and that's far too complex in or to
understand and to run it by yourself but
how do you do this on AWS well
essentially bcos
again has
the CloudFormation templates they are
very easy to get started one of the
disadvantages of bcos that i don't
particularly like is there is no local
development environment they claim there
is a vagrant box but there is there is
none essentially I think about running
vagrant on your local mac and then
running you know your measures then
running master then running your slave
nodes then running your framework and in
that framework is where you are
somewhere running your valve like
container there's too much stuff
happening on my laptop it will come to a
crawl now you will not be able to see
the performance that you really care
about so there is really very little on
the local development side in DCOs
particularly that's available so pretty
much every time I have end up developing
with DCOs
I've always gone to the cloud and the
cloud formation templates are super easy
it is highly customizable and I think
that's a good side and the bad side as
well because there are too many
configuration points there are too many
choices as a customer or as a developer
that I may have to make I'm happy if you
give me less choices and make a little
bit opinionated to me so let's take a
look at DCOs here so what I'm going to
do is I'm going to go to DCOs
this is basically I go to DCOs dot IO
this is the main website I can go to
install here they have they claim
install locally but essentially it
points you to this vagrant box I would
not recommend going that route at all
that's at least my personal experience
you know if you have a big honking
machine with 64 gigabyte memory stuff
like that sure go crazy with from DCOs
install locally but here I can say
install in the cloud I have two options
here so let's look at the W s options
here and here it shows me all my cloud
formation templates so I can run with a
single master or three masters I can
choose a region so here I'm going with
say Northern California well I'm in
Poland so maybe I should have chosen a
Frankfurt or some other region here but
essentially you know it is very much a
similar process here so I
say okay create my stack here and I'm
going to call it as d cos I'm going to
choose a key name here in DC where there
is the concept of a public slave or a
private slave private slave is where
where nothing is ever exposed to the
public and in a public slave is where
typically you could run your nginx
server now these guys can the containers
running on these slaves can talk to each
other but the idea is that the public
slave is the one only that's going to be
exposed to the public so I can define
say let's say one public slave and let's
say 3 slave instances privates in that
case I can click on next and here we are
so somebody should be asking me the size
for my instances it didn't okay well all
I do is I click click create here and
that's going to create it so now well
this is my region here so this template
is actually working on but if we go back
here so here is my well let's look at
that look at the darker part first okay
so now I'm in this is US West - you can
see Oregon region shown over here so
this is the darker stack here
essentially so in the outputs it shows
me sort of part of my default DNS target
is the region has 3 availability zone so
it talks about how you can have a highly
available cluster running over there it
gives you the addresses of the managers
and if I look for more details here I
can actually go to ec2 I've got a bunch
of instances running over here
but now if I sort them here it shows me
that okay I got a darker manager three
darker managers and four docker workers
running for me so that's pretty cool now
similarly you know I have same thing
running for ECS as well so it kind of
labels them accordingly and this is a
kubernetes cluster that I started with
cops earlier this morning so that is
already running so that's why you can
see you can run those multiple clusters
very easily over here now for the DCOs
part of it that stack is still being
created but once it's been created then
you will see something similar for DCOs
as well and for each of them essentially
whatever your native experiences you can
take the client you can configure the
client to not point to your local
environment but point to your
environment in the AWS cloud and boom
here it goes so we'll just fire it up
for you accordingly and the last thing
that I want to leave you with is where
the slides are available so I'm going to
go to my github repo and I'm going to
say docker
- Java and if you look at slides here
container orchestration Amazon and
that's sort of where the slides are
available the source and the PDF are
available so you're definitely welcome
to use the slides any way you like
so I think I have just about three more
minutes and I'm happy to take questions
and if there are no questions I'll be
out in the hallway happy to talk to you
oh there is one question
so the question is can I use different
docker images for ecs well it depends
the fun because essentially when you
define the task the docker image is big
into the task so what you can do is you
can have different tasks and each could
have a different docker image that is
entirely possible
this question up there yes the question
is can easier scale the machines once
the cluster has been originally created
yes that capability is there as well
where you can say add more instances to
my cluster that's a fantastic question
which one of you these would you use in
production see I think the way I mean I
played with all four of them so to me
really it depends upon how much time do
you have to manage the cluster so let's
go one by one
so docker is the easiest one for me to
get started with because that's a very
seamless native experience I build my
courage my I build my container I do
that on my local development desktop
exact same experience on the cloud so
that is a very seamless migration in
that sense from your desktop to your
cloud environment kubernetes has a bit
of a learning curve wherein you need to
understand what is the pod what is the
service how do I tie these things
together when do i scale it in both
darker and kubernetes you have to manage
the cluster by yourself though because
yes you're running on AWS but those ec2
instances are your problem and now you
can set up your auto scaling groups and
stuff like that but if things go down
you are still responsible DCOs to me has
a very steep learning curve because of
marathon and containers and all those
things the debugging part of it what
I've heard from customers is a little
bit harder on the DCOs side of it
although it works very well at a huge
scale for example
you're running thousands of containers
so that's where it's good at
ECS independent of whether I work for
AWS or not to me what matters is the
easiest abstractions are not very clean
in my opinion but once I get those
abstractions right somebody else is
managing the cluster for me that's why I
can focus on things that I care about so
once I have my images already then I'm
going to say okay know what go ahead and
deploy this and scale this and all that
stuff so I think it there are multiple
ways to skin the cat so you really have
to look at your specific opinion on what
matters to you you know are you bought
into AWS tack do you have the I am roles
predefined are you familiar with cloud
watch cloud trail and all those tools if
yes now I think ETS makes us a very
logical choice because it kind of fully
managed in a very classical Amazon way
but if docker is what your dream is that
ok you know what I'm going to stick with
the whatever is offered by darker you
can certainly look at docker Enterprise
Edition in which case docker takes care
of you know managing your cluster so
there are and if you look at kubernetes
for example kubernetes is offered as a
managed service on a different cloud as
well so I think there are options that
you can look at it there is no standard
boilerplate answer to this I think my
time is up so I'll be hanging out in the
hallway if you have more questions and
if you like the talk I would really
appreciate a good rating on the app
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>