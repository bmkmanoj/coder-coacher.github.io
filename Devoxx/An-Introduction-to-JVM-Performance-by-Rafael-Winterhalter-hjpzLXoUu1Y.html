<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Introduction to JVM Performance by Rafael Winterhalter | Coder Coacher - Coaching Coders</title><meta content="An Introduction to JVM Performance by Rafael Winterhalter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Introduction to JVM Performance by Rafael Winterhalter</b></h2><h5 class="post__date">2017-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hjpzLXoUu1Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes although music's out that means I'm
starting hi i'm rafael i'm a consultant
from oslo and an open source developer
of a library called pipe body which
brought me very close to JVM internals
over the years so I'm giving this talk
here as an introduction to Tavian
performance because this is something a
lot of us treat as a bad black box you
think of JVMs is a magical machine where
we put in our code and it somehow makes
the code faster and we rely on that and
we're happy about that but today I'm
trying to demystify a bit around how did
JVM works internally and I'm trying to
avoid any complex concepts I'm not
showing you machine code I'm not even
showing you byte code I'm trying to
build this on a level that is
approachable to anybody because as a
matter of fact it's not that difficult
on this overview level and I hope that
yeah this helps you in the future to
better understand how your programs are
actually run but in order to give this
talk I have to have just this climb the
disclaimer slide here because if you're
watching this as a video now for example
and it it's far in the future and this
might not even be that far let's say two
three years and whatever I say today can
be very wrong and given two and what the
JVM implements because we're not talking
about a specification today the JVM
doesn't say how it optimizes code that's
not written anywhere and this might just
change from one update released to
another and there's of course also
different JVMs out there to do things
differently so one has to be very
careful about saying what to do to write
perform a Java programs and actually
this is a problem in practice if you
look things up on Stack Overflow for
example and the answer to some problem
or the general question is from 2007
eight or something then these answers
are often very much outdated you find a
lot about reflection how it is slow and
about certain patterns that are very
slow like volatile reads or volatile
writes and a lot of has improved over
the years so always take these things
with a grain of salt and also of course
I'm speaking about hotspot today the JVM
that's included in the open JDK in an
Oracle JDK if you use another JVM
like j9 from IBM things often work quite
similarly but there are of course
differences between these slides before
we dive into anything around
compilation or the compiler of the the
JVM let's just review the model of how a
Java program comes into existence
because all of us we write a program in
Java or another language you might use :
Scala that's plenty plenty languages out
there that can be run on the JVM and the
reason for this is that all of these
programs compile to Java code right we
take source code and use Java C or Scala
C or the copying compiler any of these
tools to generate class files which
contain bytecode and bytecode doesn't
really do anything informants context
Java C is a very simple compiler to some
extent it's it's not simple as and
simple but it is not trying to optimize
your code like a C compiler for example
you compile a C program the the binaries
that you generate are already including
all or most of the optimization work
that is applicable to these programs
with Java it's very different and it's
also a reason I think that a lot of
developers don't know much about and JVM
performance because we can take this
five code and only at runtime the JVM
parses and processes bytecode and
applies the optimizations there and in
this state and stage it generates
machine code that is then run on the
processor this will then be Hardware
dependent and will be operating system
dependent and this was also the reason
that Java is only needed to be you only
need to compile a Java program once and
you can then run it on any hardware that
supports the Java Virtual Machine right
so and today I'm trying to look into
this last step where we get from
basically a Java program to machine code
and how the JVM looks into your code how
it learns about your code because that
much I can say already the JVM is an
adaptive optimizer it looks into your
program at runtime it profiles your
program at runtime and the same program
can be run twice and result in very
different states because
JVM translates things as they run and
how it observes your code being executed
in practice right so hotspot has a
system which is called tiered
combination any Java program starts out
in an interpreted stage and then based
on the interpreted stage programs can
evolve to be compiled by two different
compilers in the past and all JVMs you
could either choose to use the client
compiler which was basically the
compiler used for desktop applications
where a short startup time was important
but then the efficiency of the program
was not that much in focus and for
server applications where you would
rather spend a bit of time starting your
application where things take longer to
compile but then you would yield a more
more efficient program this would then
be the server compilers and server
applications normally run for a long
time compared to a desktop application
for example things have over changed a
lot it's not really working out that
well anymore to say okay you have a
client compiled on the server compiler
applications are more polyglot so today
we have tiered compilation where any
program runs to a from through a
promotion stage starting out with the
least efficient way of running code
promoting things up to the most
important a most efficient way of
running code and this is done basically
by the JVM
by looking into your programs and
finding so-called hotspots and this is
where the name comes from this is why
the driver to machine is called hotspot
it is because hotspots are basically
programming parts of your application
which are executed a lot if you think of
your average application you use maybe
something like spring and you have all
these configuration classes these are
just executed a single time and then you
have this boot strapped right these
configuration classes wouldn't be
hotspots well if you have this REST API
where you have this handler which is
called a million times a minute then
this obviously would be a hotspot the
JVM looks into your code and identifies
where these hotspots are and compiles
them accordingly and this is what makes
Java so much different through platforms
like C and where this is not possible a
C compiler does not know what parts of
the application are the most important
parts because it doesn't know how your
application is used at run time that
makes optimization work in C or C++ very
tedious you have to identify these parts
manually and you have to optimize your
compiler setup manually and compile
accordingly and if this use case changes
or if it might be changed even violet
violet program is run you don't have a
chance to change this behavior without
restarting the JVM is very different but
we looked more into more detail there
pretty soon
so these compilers are not normally
referred to by the JVM itself the JVM
knows even more fine grade level the
interpreter is called level zero and
this is just machine code templating so
every instruction in the JVM like adding
two numbers or calling a method has a
template in the hardware's machine code
and every time you hit an instruction
you execute just this template with the
values that you get so you want to add
two and three you will take this
template to add the numbers by two and
three on your Intel processor if you run
one and then you execute that and
instruction by instruction like it is in
your Java program your code will be run
right and after that the JVM but at some
point is side ok this is not very
efficient I cannot just run the program
step by step I have to look into the
program itself and I might find
optimizations like let's say this
template as the numbers one and two a
compiler even a very simple compiler
will be able to say ok the sum of one
and two is always three so I don't need
to do that anymore and this is what a
compiler does or one of the things that
compiler does and the c1 compiler is of
course applying these optimizations
there however three levels on the c1
compiler because as I said the JVM looks
into your program it finds so-called hot
spots or important part in your program
in order to do so it needs to profile
the application it needs to collect data
like how often is a method invoked or
how often is the field read how often is
an if statement true or is it always
true all of these things and based on
this data code can evolve into level for
compilation
which is then based on this profile that
was collected in this in the and c1
compiler where these optimizations are
applied for example a JVM might be able
to prove by profiling that an if
statement is always false right it might
be just a constant value where it says
if something which is not which is
impossible to ever be true and then the
c2 compiler might take this information
and say ok since this if statement it's
never true I can just throw this code
out I don't even have to check the
condition anymore and basically based on
this profile do something that you
otherwise without profiling you couldn't
do and that's also a reason why Java
sometimes or actually many times can be
faster than a C program because in the C
program at compile time and without
having collected this runtime
information you cannot prove these
things and then not apply these
optimizations right and that's a big
strength of Java and the JVM that's why
it's so popular as well for other
languages not only the Java language
because anybody can write a programming
language not care about performance much
give this program to the JVM and then
let the JVM do its magic right so the
compilation a unit of hotspot is a java
method so any method can be compiled in
one of these stages so you have a method
like some some business logic method the
JVM will start always in level 0 by
walking it step by step through all
instructions and then the most typical
pattern is to push this method right to
level 3 after it is executed a certain
amount of time since often 10,000 times
once a method is executed 10,000 times
it gets compiled for C 1 it's often even
lower and then after having run this
method sufficiently often the JVM will
at some point decide ok I know now how
this method is is used in in the JVM or
in the program I'm currently executing
based on this information
I will now compile it in the server
compiler in situ at level 4 however C 2
compilation is rather expensive and this
is a trade-off you have to make and
doesn't something JVM has to deal with
compiling code
takes resources right and these
resources cannot be used to run your
program so the JVM internal runtime is
competing with your business logic about
processing resources and memory and if a
lot of methods are compiled and this can
be quite a heavy operation so if you see
two compiler is very busy because also
profiling data to collect all those
profiling data this has to be stored in
your memory right in your RAM and this
Ram is there not available to your heap
for example so if the code cache is
already very very full then the JVM will
put it into a simple profiling mode
where it collects very little data but
enough you know and let to at least
decide if this modern method is worth
bothering at all and then once the C 2
compiler gets available it goes to level
3 and then probably to level 4 another
typical pattern is that something starts
out in level 3 as we seen before but the
method is trivial and for trivial
methods like a kettle or a setter
there's not really much to optimize
right what can you do better an together
you can read a field and return it
there's not much a strong compiler even
can do with that so if the c21 compiler
already realizes that this method is too
trivial it just puts it basically it
keeps it but takes all the profiling
logic out because profiling is overhead
obviously which you want to avoid in
case that you don't need anything and
this is something that works very well
on the JVM I can say that much but it
also makes the JVM very hard to observe
because of course you might measure
something you might look into your
application you just start the server
and you think this is slow what is
taking so long this is like an
unacceptable margin I promised my
customer a latency of that in that many
milliseconds but in the beginning of JVM
basically you have to reach the steady
state there's a lot of up and downs in
performance using profiling a program
can actually be slower for a short while
before it gets faster and this is of
course a problem especially in finance
people try to find their ways around it
a typical way of doing that is for
example you have a trading system and
you turn on the JVM with the program and
then you shoot fake data at the training
system
right and once the JVM has basically is
trained for this sort of usage pattern
only then the JVM is put online and that
way you can avoid do you things but but
in the average program it is sometimes a
problem and especially when you try to
observe things that often as a problem
because you think you can already
observe the JVM in its natural state how
your program will be executed at some
point but you are just experiencing
these level shifts the compiler does for
you right okay so and this is basically
the general overview how does the JVM
well it's the compiler walk through your
code and today here to talk about what
the C 2 compiler does because if you
have a performance problem typically the
performance problem happens in code that
is used a lot right and these hotspots
as the JVM name implies are of course
compiled by the C 2 compiler eventually
in the normal case this is also an
introduction you can of course talk
about the C 1 compiler you can also talk
about the interpreter and their
performance implications and even though
it's the last slot I could just go on
forever probably but you guys want to go
home at some point so I'll focus on C 2
and what it does and the most important
part to understand about a JVM and how
it how it works and what assumption it
makes is understanding call sides and
the difference between mono more fizzle
and mega morphism and if this is all you
take out of this talk then I'm already
very happy and because this is actually
something that you can influence with
your programming style and I've seen
this many times I'm like if you have a
very data-intensive application that
executes a lot of code which is
basically very bound to how strong its
CPU is then this matters this is also
something I should mention it's great to
know these things
but typically performance problems of
course happen at the database level or
some HTTP level or you do micro services
so you have to marshal a lot of data and
send it back and forth with HTTP this
typically consumes much more time and
resources as than the extra like
execution of your program but but this
is something that is really worth
knowing math
so what's a call side a call side is
basically when a method gets called so
where we have well bar this is an
execution of a method right and and
where this method is invoked in
particular this is the call side we'll
be mentioning here so we're calling the
method bar on the class foo on an
instance of the class true and who
obviously has a simple simple method
implementation for bar and in a trivial
manner you could just say right this is
a simple method call what do i do I go
to this method and I invoke it but it's
not at easy in Java in Java any method
unless it's secured final is invoked
virtually so any method can be
overridden there might be a subclass of
foo that overrides the implementation of
bar right and you always have to check
for that and typically especially in C++
this means that these dispatching this
this dispatching of virtual method calls
is much more expensive because it
doesn't sound like much right you only
go to an instance you check what the
classes and you check what the Methodist
this is like three things but think
about what your Java program practically
is it's it's a combination of a lot of
invocations of different methods so if
you can just scrap off a bit of
performance from a method call this has
a huge impact right and of course the
JVM s main goal or one of the main goal
of the C 2 compiler is to make these
method invocations fast because they are
so common in the Java program so
typically we have indirection and we
have to find out what method to call
actually and the way this is done in
both in many cases when a methodist
virtual is by doing a call on a virtual
method table or vtable so a virtual
method table is basically yeah a table
you can say that where any method is
listed by its signature and it also
contains like an address of the code
that is representing this methods
implementation and this is for example
the method table for the classroom where
the bar method at index 8 in this case
you just assume it's index 8 points to
system out rental hello right but as we
mentioned there might be a subclass of
foo which overrides this method
and then this sub class sub has its own
virtual method table where index eight
for method bar points at a different
address right so any class in your JVM
will get to the table where method
signatures point to addresses and it's a
nice implication of single inheritance
that Java has the nice implication is
that you know that any class can
basically inherit it it's super classes
method table so index eight will always
point to the same method so the JVM when
meeting a call side where Boris invoked
already knows what index of the table
has to read it just has to determine
what table it needs to read and this is
basically the job that's left right so
every time the JVM invokes a method that
is dispatched virtually it has to first
check what type the class has at the
instance has then it has to find the
table of this type the virtual method
table and then it has to go to the
machine code address that is
representing this methods implementation
and execute it and in practice this
means basically three natural operations
I can say for any virtual method call
and this doesn't again it doesn't sound
so bad this is pretty straightforward it
doesn't take a few nanoseconds maybe but
in some since all of your Java programs
are basically an assembly of many many
method calls these three in directions
are a lot and would matter a lot because
just being able to let's just assume for
the sake of the argument that any of
these steps takes equally long time then
just reducing this three-step approach
by one step would meet a 33% performance
increase for your application and that
is of course something that you will
notice so one way to do this obviously
it would be to implement the cash at the
call site so most programs don't have
that much inheritance as a matter of
fact let's think of a getter method get
them Gators are rarely overridden in an
actual program so you might just
remember last time I encountered
the sparkle it was on an instance of
actually foo and then I'll just remember
the machine code address of the
implementation of this this bar method
right so you do it once and the next
time you you find that you would have a
cache and other virtual machines like
small talk for example which was a big
influence to Java they use this cache
these caches intensively and basically a
small talk performance debugging small
talk often ended up by looking into
where the caches would be too small to
contain all this Patterson and you would
try to optimize it there so now you
would only check the class of the
instance and then look up the address
from the cache which is much faster
because it's local and you would already
improve the performance of your program
the Java Virtual Machine is not having
and not implying such caches the cogito
machine does something that is in
practice much much more performant and
it also leads to other performance
optimizations being a possible due to
this optimization and you might have
heard heard this word monomorphic sounds
very academic and something that only
people that are deep in the matter
understand but monomorphic basically
just means a method only knows one
implementation or rather a class only or
a call side only ever meets a single
class of an instance so let's say foo
doesn't even have a subclass then it is
trivial to assume that this dispatch
will also always invoke the same method
even though the method call by the
conventions of the java language is
virtual in practices is not it is not
virtual because it's never over it so in
the in the end what JVM can do it can
put an assertion there to make sure that
this really is an assumption that is
true and this sounds expensive but these
assertions are very cheap and that that
is related to how hardware works and
then it can't just go to the address
that represents the implementation of
foo which is as simple as it can get
right yeah and this is done for most
call sites and your Java programs as a
matter of fact so most most methods are
monomorphic
and other languages
we enforce this feature by not having
virtual this method calls by default
like c-sharp for example doesn't have
virtual calls by default which is
sensible because for the compiler does
matters a lot for the JIT compiler but
this also enables a lot of things like
for example mojito which is the library
I work on and we what we do is we mock
classes by overriding them dynamically
and this will only happen on your test
classes but if you run something in
production then these methods will not
be virtual and practice but for your
mocks we will override all methods
because a mock isn't supposed to do
anything so we basically override all
methods by an empty implementation and
this is how we can make this work
without breaking performance in your
production applications right yes so as
I mentioned before the JVM starts with
interpreting your code then it profiles
your code in a simple compiled n't and
it pushes you over to a more performant
compiler and you qualify it's can
basically be monomorphic and mega
morphic at different times in the
application say most code sets are
monomorphic about 90% on average right
90% of all your methods are never
overridden so these are just always in
line JVM are not in line linked they are
basically just replaced all these method
calls are basically just replaced by an
invocation of the target address then
there's also biomorphic method calls and
this is typically true for collections
you might have a call site that is
calling list size or something and you
either use an aerialist
and you should never do that or a linked
list so never use a linked list it's
also something you can take out of this
talk but there's biomorphic because you
have two implementations of this list
size at this list size method and then
there's two call so it's called
biomorphic and they are the JVM does
something similar to to a direct link it
implements a conditional direct link
everything above that is treated as mega
morphic by the call side so you have an
actual heuristic --all cutoff value
which is that if you have three
implementations of a class or three
three three
three instances of a class at a certain
call site then these method calls will
be much more expensive than if you had
two classes at the call site right
there's a fallback for so called bombing
and cold sites if if a call site is mega
morphic so there's many many classes
seen but one class in particular is met
very very often right you have an
ArrayList and 99% of all cases but a
linked list very very seldomly and that
this dominant target is still treated as
if it wasn't one more monomorphic but
all other types are treated with mega
morphic and the JVM basically probably
might start out with a ritual dispatch
and then after while it sees okay this
is actually always the same type i now
recompile this method in in situ to
imply a direct link and it optimizes it
and this might not even be true because
at some point you change your program
like let's say you have this trading app
and the behavior of this application is
very dependent on what markets are open
at that point right and then the
Japanese market opens and your app
changes it's it's invocation pattern so
suddenly you meet other classes in your
program which might even be loaded
dynamically at that point for the first
time so the JVM sees ok this is no
longer no longer an assumption I can
prove to be true so I have to do my's
your program I will probably fall back
to meromorphic dispatcher right and this
is why also some times and again in
trading this is common right I mentioned
this story where you would train an
application with fake data before you
put it actually online just that the JIT
compiler is ready you might have
applications that are trained ready for
all sorts of markets so you have a nap
that's that's ready for the Japanese
market and a half that's ready for the
European market and once the Japanese
market opens you switch basically your
load balancer over to this other fleet
that you have trained for this other use
pattern that's something you can do
because every time this happens this of
course eats resources that otherwise
would be available to execute your
program but since now that
compilers using that and it's not
available to you and you don't you have
very little possibility to influence
what C 2 does we'll see one also and
this is for good reason
they shouldn't configure too much
because it's very difficult and very
easy to to get wrong there's of course
flags for things but generally unless
you really know what you're doing and I
include myself and in the group of
people who often doesn't know what I'm
doing in this context at least so I
wouldn't recommend it generally often
the JVM is actually quite smart figuring
out what's right all right all right
and besides the reduced overhead of
invoking a method monomorphic all sites
have a cascading effect by allowing
so-called inlining and inlining is
basically the so called uber
optimization in the JVM inlining means
basically what it says inlining means
that the JVM can take a method and it
can basically copy paste it to where the
go-to statement was because if you just
can go to some fixed address you can
just take the code add this address and
copy it into the target and space we're
ready to go to statement would normally
be and this is something you wouldn't do
as a developer of course you want to
have your programs nicely normalized
we're basically logic it's only
implemented once that is the same logic
right but for a JVM or a machine it just
reads instructions and it does what it
says if there's an indirection that's an
extra CPU cycle that it's wasted on
walking to some address if this address
is very remote to where you currently
have flouted stuff into your processor
you probably have to read it from from
some cache or some even the disk in the
worst case and this all takes time
so by inlining basically we have now
reduced this this indirection to not be
indirection anymore we now just have
basically implemented what we would have
implemented and if if this yeah
fubar method was be obsolete and just be
called at this one place and yeah this
is basically a big improvement because
it's called an uber optimization and you
it's quite intuitive if you think about
it the more code you have in a method
the better possibilities you will have
to optimize this group of code right if
you have 20 lines of code then there's
probably some redundancy that you can
remove something that you can reorder
nor
in order to get a positive effect well
if you only have a getter method as I
mentioned there's not much you can do I
get us just returning a value well if
you have 100 lines of code that's
probably something you can rearrange and
buy in lining of course the the mass of
instructions that you can optimize the
bond grows and allows you to do things
like like removing stuff right and that
saves you run time again because you do
not waste cycles on these things em and
I'll show you an example quite in a
while which is actually something that
that happens in practice and you can
look at it with a few tricks right ok so
and this is something I can tell you by
heart this is something I have debarked
actually in a production system which
behaved weirdly and there was a it said
for it's mostly applications that crunch
a lot of data that use a lot of CPU
already which will benefit of that and
this was a case like that
so you have maybe seen double braced
initialization this thing right if you
do that this this inner brace is a
constructor and by this constructor you
can basically define code and people
this looks more functional somewhat
because you can write the instructions
that belong to the list initialization
within these prices instead of having to
define a variable and you have to call
variable add and blah blah blah blah
right the problem is that in this case
you are creating a subclass of aerialist
and this subclass of ArrayList is its
own type and after what we have just
learned about monomorphic and
metamorphic call sites if you have a lot
of these then any such double brace
initialization class will be its own
type and suddenly instead of having one
type of error list which would yield
them monomorphic call side you might
have twenty subclasses of error list
which will use a mega morphic call side
and if this isn't a tight loop somewhere
this can have a severe effect on a
runtime in this particular case we
removed this double brace initialization
arraylists and gained about 90 percent
performance which surprises a lot of
people but it really basically hit
something in the JVM optimizer that
destructed its inner-workings because
the JVM the hotspot compiler what it
does it it does heuristics there's no
good reason to say
- is the cutoff we have by morphic but
no other polymorphism right it just the
way the hotspot compiler works is they
look at a lot of Java programs they
think how do most people implement
something and then based on these
typical patterns we will optimize a Java
program you have to make assumptions
otherwise you couldn't start anywhere
right so this is why it is important to
write sane and clean programs and most
of the time it is enough to write
basically a well expressed java
application to hit all before almost
performance benefits of the JVM and then
that comes a cliff where you have to do
very weird things in order to trick the
JVM to do something else and where your
code gets very ugly again if you just
have to squeeze out the last nanoseconds
of something right so besides that this
is not a good style of doing things
because your program also grows by
adding classes this is another reason
why you should never use double brace
initialization even though unfortunately
you still see it a lot right so how
could we micro optimize something like
that and this is something where we what
I mentioned your code can get uglier by
if you want to do very specific
performance optimizations sometimes your
code needs to get uglier so here we have
an interface foo with three
implementations so this will be mega
morphic right how can we force the JVM
to have this dispatched monomorphic well
here's the trick we implement a class
foo and instead of having the interface
and then we implement three static
methods and whenever we want to invoke
the method or one of the methods we do
this we switch on the ID and then we go
through the cases and then we invoke the
sub method that we actually want to
invoke this time the JVM looks into your
program and sees ok this is three method
calls they're all static static methods
cannot be virtual so they are
monomorphic I can inline things and I
can dispatch this non virtually now
these sort of tricks you sometimes see
especially if you click through
frameworks that you use and they have
some some logic like this sometimes you
actually see this pattern even they do
that to avoid a monomorphic Amega
morphic whole and you should be very
careful about when uses pattern but
sometimes it can
to help you but then you should profile
things and look into things and be very
sure that you're doing something that
you want to do that right but this is
something that it's a possibility and
it's good to understand why people do
that and then how this effects the your
program it's nothing I recommend though
so let's look at something that is a bit
more real-world let's say we have this
log method and this log method is static
and it takes a var arc of arguments and
then it prints all these arguments to
the console e so we have a call site at
system.out.print on the print will call
is a call site but we also have arc to
string and arc to string you can think
of in a typical logger framework you
will get all kinds of instances right of
all kinds of classes this arc to string
call this call site will very likely be
mega morphic right because you will
encounter all kinds of instances so
let's say we have this method very log a
string and integer and an object then we
will have three types already three
types and hotspot means
mega morphism this means that logging
becomes very very slow and this is bad
because logging is often sometimes we do
a lot because we want to be able to
debug and all these things right however
as I mentioned before thanks to inlining
this megaman monomorphic log call can be
copied into to do something method and
this is something hotspot will probably
do will copy the code out of log into
you do something method and now we have
basically added more data to chew on as
I mentioned before and thanks to this we
can for example see okay do the array
arcs always has three arguments this is
a fixed amount of arguments so we can
apply another optimization technique
which is called loop unrolling so we
just copy the loop body three times and
put the arguments directly in there and
now we have replaced one loop with a
single call side by three statements
with a call side for each statement
meaning that this this previously
metamorphic call side for logging turned
into a monomorphic call side and this
happens with logging frame works a lot
like simple owing for say say you hand
out all these arguments and if it's a
process synchronously and
and like freely to console and and the
cheat compiler will typically pick up
the little code it is in this logging
method and copied in that's why if you
have these very generic methods like
typically logging frameworks have but
also other frameworks like think of arca
archa the actors that take an object as
a payload right this can be anything and
these if you write frameworks like that
and maybe some of you dude you have to
be very careful about the inline ability
of these dispatches because if the
logging if you had a hundred
implementations of the log method which
would be applied all around then this
initial inlining of the loc method
couldn't happen anymore and then
breaking all subsequent optimizations
and that sometimes you make a little
change with then prevents a whole
waterfall of future optimizations which
then can have a big impact on your
program yes so and this brings me to
another thing I'm a proponent of static
typing I'm I like statically typed
programs but I of course I'm consultant
to some extent I have to do where the
money what the money where the money is
and that brings me sometimes to using
javascript
unfortunately mm-hmm and I'm not I'm not
having problems with JavaScript in
general but there's interesting
performance implications in in
JavaScript runtimes like we ate or also
announced word because all the
performance optimizations we looked into
so far they were based on typing types
right so how does a compiler work on non
typed languages does it work very
differently and the answer's no inlining
monomorphic megohm office and all these
concepts that's nothing that's exclusive
to the JVM you find us in basically any
virtual machine and of course also in
for example v8 so let's say we have a
class and this javascript doesn't have
class but let's call it classes an
object rather than foo which defines two
properties we ate in order to optimize
this program has a concept called hidden
classes so basically when you define the
few object the empty object we define
our first class which we denote here by
a star saying this doesn't have any
members and then we add
so v8 will create a subclass for this
instance so we'll change the class
dynamically where we say X which
inherits from star then we define Y
where X Y and Herot from X right and now
we define another object but we change
the order in which we define the fields
right so now we define Y first so first
we have the same class act their star
but now we get Y and Y X so we have two
different types so JavaScript or v8
would have now failed to recognize the
similarity of these types and might
escalate the same thing on your
JavaScript program by using a mega
morphic dispatching method for something
that otherwise would be monomorphic
that's why I think from because I'm
doing a fair amount of performance
sensitive work that's why I like static
languages so much it's partly that
compilers rely a lot of on structure and
on typing and recognizing similarities
and by putting these similarities
explicitly into your programs you help
the compiler to do a lot of optimization
so this is generally the best
performance advice you can give just
right clearly structured programs avoid
too much virtualization unless you
really have to put it in somewhere and
the compiler will be just fine and this
is of course true for dynamically typed
programs as well so in JavaScript
instead of having this random object
definitions rather I have some factory
or something this will result in similar
hidden types and then you will get the
performance gains out of that right so
this is just a side note why types don't
only metaphor programming structure for
you as a programmer but also for JIT
compilers and like all the form and
relevant tooling and that can then take
the structure you put accessibly into
your application in order to find out
how it can restructure your application
to be more performant yes okay so now we
have gone through a general overview of
how hotspot on the JVM or most jaebeum's
worked with the Chiat compilation we
have looked into call sides and mega
morphism on morphism this is the second
important concept I want to get across
and then basically all beyond that is
bonus lights
I have 11 minutes left but it's
important to understand all
it's basically 2cy program sometimes
behaved
yeah differently so what we have here is
a program where we create 20,000 random
values and then we will basically sum up
these values a thousand times just to
see basically this is of course not oh
you're useful program but just to
demonstrate something and in if the
number is bigger than 50 we will add it
and if it is slower than 50 we will
subtract it this is important so this
here is a branch and the if statement
any switch statement is a branch and
very similarly to mono morph is amande
mega morphism it is of a big advantage
if you know what you are going to do in
the future and where a branch can be
true or false if you can predict if the
branch will always be true this is very
similar to knowing that you will always
dispatch a certain method of a type
hierarchy right and ironically there's a
way to make this program much much
faster because here we have random
values they might be with by a 50%
chance there might be bigger than 50 or
lower than 50 so you don't have very
good predictability here in order to
make it faster you can add the sort
method and this program will basically
become more performant and why is that
because you do something extra right you
sort the array but now by having sorted
it the JVM and also the processor they
can predict in the beginning that the
branch will always be false so they can
optimize the program to basically prefer
the second branch the else the else
route yeah right so for the first 500
values in average 10000 values on
average we will always hit false and
since we know that and since we can now
suddenly predict that your program runs
faster and then for the last 10,000
elements we will suddenly realize okay
now we are always hitting the true
branch and the if branch I mean and then
we go down right so just as with mega
morphism and and call sides it is always
good if your program doesn't surprise
you and I mean that makes sense right so
but in general if you if you structure
your programs in a way that allow
prediction both for the reader of your
program but also for the church compiler
is always an advantage so you cannot
substitute virtual calls with branches
because the same program applies if you
don't know where you'll end up your your
JVM will have problems and your
processor will have problems alright so
I have seven and a half minutes left
maybe one birthday mentioned it's escape
analysis this is something a lot of
people aren't aware of and as you all
know allocating object costs time and
collecting garbage that these objects
are in the end takes even more time so
what the JVM can do is when you allocate
an object and this is basically a list
iteration which is a compiled down to
this you allocate an iterator here and
this iterator is invoked as long as it
has new elements and then the element is
print right this is an object allocation
and you would like to avoid that so I've
seen this and I've argued with people
about that who don't like the this
syntax here because it compiles not yet
and rather iterate over integers and
they reference the index exit Li this
way you save yourself from the
allocation right
however the JVM is fortunately smart
enough to realize that this allocation
is very local and it can then fully
delete your allocation from your code
and basically take it out for you and it
only can do this in very tiny scopes so
you shouldn't just assume that the JVM
can can find out that this object that
you allocate somewhere is still in scope
a few methods downstream and the further
you escalate the reach ability of an
object the less likely it becomes that
this allocation is deleted but for very
local allocations it typically doesn't
really matter so keep your keep your
objects always in the smallest scope and
this way you increase chances a lot by
allowing the JVM to delete this
allocation altogether and some people
like to example they have a long loop
and then they allocate a lot of
containers outside of the loop because
they argue it will be much cheaper to
reuse this object throughout the entire
loop but the opposite is true because if
they varied like deep down in the loop
and in just in a small segment of code
JVM might realize aha I can just
allocate these values on the stack and I
don't have to actually generate an
object yes interesting in this context
is also dead code elimination which is
something many people get tricked by
what we do here is that we write a tiny
benchmark where we want to calculate how
much time it takes to sum up 20,000
random values right we generate an area
with 20,000 values then we take the time
then we iterate over all values and sum
them up and then we take the time again
and then we print out how long time this
take the JVM will however look at this
code and say wait a minute
this segment here this is not really
necessary so let me delete that and this
is the code you'll end up with right so
this this is a reason by benchmarking on
a JVM is extremely tricky and done wrong
a lot and you find this in open source
projects even they add a trace
themselves look how little time this
token and look at the benchmark and you
say wait a minute you know of course
this takes zero milliseconds it's not
because your program is so good it's
just because it's not getting executed
so you have to be very careful here as a
matter of fact system occurrence time
Elise is a cached value in the JVM and
it is synchronized with the processors
clock every now and then so if a
synchronization happens between these
two indications the time you get might
even be negative because the clock
basically went too fast and software
instant resync but the hardware so and
it says then took minus 4 milliseconds
and of course that's not true in order
to do these things properly you would
have to be a bit more efficient about
how you run your benchmark and first of
all you have to say before train your
JVM to run C to compilation and C to
compilation is only done after 10,000
runs on average and so most of the times
actually so that's this rule sometimes
that you cannot even rely on that number
so you have to at least 10,000 times run
your benchmark before you can run your
benchmark because you want to run it
while it is compiled right then also we
use nano time which doesn't suck
for the problem that million
milliseconds time has and then finally
we have to use the value somehow and we
have to use it in a manner that the JVM
cannot prove us that we're not actually
using it so you cannot just store it
somewhere and assume that that way it is
it's basically safe to run the code if
you print it out to the console II the
JVM cannot do an optimization where it
says you know what I know you're not
actually reading that I'm not gonna
print this value right it's that's
impossible so we know for sure now that
this is not happening of course this
makes it very verbose and there are
better techniques last last thing I'll
gonna give you on the way is there's a
great framework which is called JM h
which is bundled with the open JDK and
it is a way to before use and there's a
great tutorial around it which allows
you to make these tests much easier to
implement right it is still easy to get
right but jmh takes care full of a lot
of things like warming up your code it
does its properly and it does it really
in a way that you can be sure it's
warmed up and it allows you to do this
escaping of the result value very easily
such that that code elimination doesn't
happen you just have to return the value
from your benchmarking method and then
jmh will not print it to the console it
has a much more efficient way of doing
that yeah and that that way you can spin
up benchmarks quite easily which can be
dangerous if you if you do it wrong but
typically what you do is you write a
benchmark you post the benchmark
somewhere if you want to prove a point
and then people will try to prove you
wrong and over time you you will get
edited and learn a lot about the Trivium
so I learned a lot by benchmarking and
of course if you have performance
sensitive code it's very important to
know what you're doing there right so as
I said all of this is already bonus
slides and I always make too much slides
so let's me let me just click through
this and I have one 30 minutes left so
I'll just end it here thank you so much
I'm Raphael you can find me on Twitter
you can find my library bite buddy
which I appreciate if you check out on
github and on the internet and also
documents which is not so scripted run
if you have questions I'm still here all
day thank you so much for coming here
and enjoy dress
today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>