<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Event-driven task automation of Java and Spark jobs with Mesos by Florian Froese | Coder Coacher - Coaching Coders</title><meta content="Event-driven task automation of Java and Spark jobs with Mesos by Florian Froese - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Event-driven task automation of Java and Spark jobs with Mesos by Florian Froese</b></h2><h5 class="post__date">2017-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RA5YjfiCyzw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so first of all thank you very much for
further to all of you for coming to my
talk although it's in the lunch break I
hope I can make it as interesting
interesting as possible possible for you
to tell you a bit about myself my name
is Florian i graduated from eth in 2014
i'm a software engineer paralytics we
are a little big data start up here in
Zurich I've been working with Big Data
technologies and the last the last six
years and to tell you a bit about the
data that paralytics is working with
we're working with telecommunications
data so but the data that everybody of
you generates every day every time you
make a call every time you get an SMS
every time you travel around through
switzerland through any other country
you're generating events within the
telecommunication networks and these
events we receive from the
telecommunication provider and try to
analyze them further so if data is
pretty sensitive of course we don't get
this data just with a phone number or
with a name but we get it of course
anonymized in a way that you cannot
track a user for example longer than 24
hours so what we're getting in the
events is we're getting a user we're
getting a timestamp and we're getting a
base station ID so can you see this ok
so mobile network is based on multiple
base stations and every way station has
an ID so we're getting basically two
sources of data in paralytics one of the
events and the other ones are the
so-called seropositive which link to
rework location so we get an ID and
there's ID leads to a location for
example the antennas on the roof of the
cinema complex or for example at help
our house the business model of turtle
it exists as we are not a big circle
like Swiss commercial but a little
startup we're working with multiple
providers in several different countries
like Germany us singapore hong kong and
try to scale our business model
in that direction that we can just go
take the data from one provider as its
similar data and we are everywhere
people make phone calls people get SMS
and so on the data is really similar
from country to country so we get this
different data we have to unify them a
unified data model and then we can run
some algorithms on it we can estimate
the location better we can try to find
out what's the mode of transport that
he's using we can try to run a slight
detector was this person flying for
example how did all of you come to this
location for this morning did you take
the tram or did you check the the train
and of course we have an infrastructure
forever every provider there they are so
sensitive that it cannot leave the
premises of the providers so we have to
maintain this whole infrastructure for
every provider in every country that we
want to run our programs in so of course
the question is now what mode what do we
do here we have to do some kind of task
automation because it's just not
scalable that we have to do ingestion
and we have to run statistic
applications we have to run location
estimation in every cluster for every
day and the data gets in sometimes every
day sometimes every five days and so on
so as this doesn't scale and really try
it's almost one employee to serve all
the clusters and all the providers
checking from your data is that and the
problem is that ok did Florian already
run program P encrusted that what
happens when Florian is sick is sick who
takes over this job and also the
disadvantages are that you have human
error did you configure your program you
run correctly did you take the real
correct version all these are the
questions that we want to solve with the
task automation and of course with the
Vela present we're lazy and we don't
want to do manual repetitive tasks all
the time so what should our task
automation do normally we receive the
data in a landing zone somewhere in an
ftp server for a bio City and so on so I
want to transfer this data to an HDFS
which is a distributed file system
which it has hired a very highly
available and hazard replication
built-in then once we have the data in
the HDFS it's safe to be safe in a
distributed environment so we can run
the jobs and distributed way we can run
injection of events in the street away
because we receive multiple gigabytes
terabytes per day we want to run
statistic applications every day is the
data correct the provider device
delivers the correct data or is it
somehow flawed a fraudulent or is it not
available at all we've entered on the
other jobs like location estimation mode
of transport detection all these kind of
jobs and of course as we're working in
distributed environment with hundreds of
service we had it has to be fault
tolerant the server might easily crash
and we don't want our job or whole
automation program to fail just because
of this reason so we came up with a
program and we call it Tara scout the
basis of this is also what the first
part of my job description is the
event-driven task automation so we
separate two concepts we have a patrol
which is there to the event generator
and we have a handler which is the event
consumer so patrol in our example for
continuously checks for new path in a in
a directory so whenever a new path like
for example whenever new file pops up we
can say okay this is an event we have to
we can give it to the hangar so the
first question is which would patrol us
to the handle do you accept this event
can you work with this event is it the
correct hazardous correct the file name
on file format or maybe well maybe we
only want to work with files and not
with folders so we can actually directly
ignore the folders the next question is
are you ready to actually handle the
event maybe you're right we have to wait
for some other files to come in maybe
maybe we're waiting again every time we
receive in the file the file we have to
wait for the next trial as well maybe
the file is filled
edition at we have to wait until it's
stable on HDFS maybe we're still writing
to the file and we'd one we want to
handler only to handle and stop start an
action one to fight off stable so then
the next aggression a thing is once
we're ready to handle we say okay please
handler handle the task running the run
a job maybe try to copy the file to HDFS
etc in order to achieve that Tara's
guard leverages measures not sure how
many people of you know members okay a
couple so metal describe themselves as a
distributed system kernel but you can
think of it's in it's a it's an
abstraction of all you distributed
environment you have several notes and
denotes has a certain amount of course
with a lot of ram and these notes then
can register with with methods and say I
have this much ram intake etc its fault
tolerant as the for to the level
scheduling mechanism I will tell you
what is it more about this later it has
native support for containers so for
example internet exchanges and
artifactory and all our development
tools in there there's research
isolation using see groups and another
nice feature is it has resource
reservation using roles so you can
separate the exhaust eventually into for
example production and the staging
environment so to talk a bit about the
nether architecture we're here we have
the methods masters we have multiple
masters because it has to be faltering
one where we have about 23 masters the
legal action is handled by zookeeper and
on the right side here we can see these
are our slaves so these are might be our
100 machines that we have that whether
we put in our cluster then the other
notion is actually a framework a
framework you can think of it just a
program that you want to run a
distributed program so the program a it
might have a task on slave 1 it might
also have another task on safety etc so
how the two-level scheduling networks
the slave register send an offer to the
Master they say okay I'm slave one I
have two cores and ten TP a team two
cores and 10 gigabytes of memory just to
make it simple then Methos ghost and
tess s okay framework a can you work
with this we could you can start any
task you want with it and this is where
the second key level sickening comes in
because the application can itself
decides what which resources should I
accept so the framework a let's say they
accept 14 and two gigabytes of memory
because it just wants to run run a
little java application then the rest of
the resources goes back to the master
and the master can of other the other
resources to infer mercy so how to
implement actually your own metal
scheduler main method metals actually
makes it quite easy there's a maven
artifact that you can download and it is
a Java interface which has a couple of
methods which can implement graduate
Sturridge you get the message when you
registered with it megasoma for the
first time you have a reregistered when
you actually had a master change the
resource offers are coming in as you can
see here as a list of offers then you're
the status update their offense where
when you whenever your class received in
their status update I'm important part
of those for the scheduler driver here
the schedule is rather is basically the
object with which you can communicate
communicate back to the measures master
to describe more about the term
terascale components because i'm going
to show you a diet and diagram later
later in a minute we have to tear us a
mezzo scheduler which which is the metal
scheduled early implementation of what
what are the interface that i just
showed you it's scheduled task and
character arc out right now it's just
really first-come first-serve q which
takes task spent can just pawn them in
mezzo then we have a massive task which
is all abstract a section of the task
that you can run with in mezzo so
this is the diagram so far we have a
massive master up here and all the other
components that you that you have here I
actually be components that are running
in terror Scout we have the handlers
that I showed you earlier this is
actually ok when you have to handle the
method you accepted it you have your
ready to handle it I want to handle it
now okay i want to create a task i want
to spawn a spot report shuttle so the
handler creates the measure star kingdom
this message asking since then gets
submitted to the scheduler the schedule
are always receives the offer a resource
office from the meadows master the mega
faster the determine whether scheduler
then groups these resource offers first
life because every slave might send
might send multiple offers because
Meadows kind of cups the office down in
to get a little office so with the terra
mater scheduler does for you at groups
the groups that already it merges the
offers so you know okay for this place
in total i could get these in these many
resources if i combine all the resources
of this slave so the first thing is what
the terminal scheduler the calls denies
of tasks do you accept these resources
can you work with a 22 course in the 10
gigabytes of ram if the terror in mezzo
scarcely megastars can work with it it
can try to launch a job so in order to
do this then mature Amazo scheduler asks
them Tomatoes task please give me your
task description give me your lunch
could launch a month that should be
executed on the excited executors and
give me all the you is that you might do
download in your to your executor in
order that you need for execution for
example from some jobs the next thing
the term metal schedule sense to task to
the metal scheduler which then forwards
it to the message master there's the
creation of the top so what happens when
we see some updates the mezzos master
sense an update will sense an update to
the term meadow scheduler we see fit to
the interface that i showed before then
this actually gets forwarded in the
Kovac method that every measures task
can implement so the rather stars gets
an update okay this is just another
update your app is now in the state
running it's not studying anymore it's
not not starting it
you running for example then another
thing that we won't want might receive
as a task update which means okay your
task is finish it might be a car that
got killed it really finished or
finished with an arrow we have another
method for the callback method for this
if you want to have some tidying up to
do with your task or something else
that's it yeah then of course because
all the jobs that re running in mostly
Java in spark jobs we can make some
abstractions on top of it because the
matter of tasks you have to always say
okay this is what the task that you want
to create but connection Simon second I
somehow abstract these tasks further so
we have a medal of JVM task which is
calling an application using the Java
command and where the sparks middle
sparked off which is calling the
application using spark submitter talks
and the goal of this is actually to just
be able to define an application by a
configuration which should we show you
in a second so this message a vm talk
what should it do it should abstract the
gym let's check the JVM talk started
with Java it should know how to accept
resources should know how to create an
arch command it should upload local
sites that you might have on your local
system or just generated by chair asked
out on to HDFS so they are reachable by
any other note on which the job might be
sport so it might be their own place 92
now this job will be mayor will be
spawned and after the job which should
automatically tidy up all the files that
you uploaded to the temporary folder so
this is an example of a configuration we
have a JVM JVM we have an epileptic
executable we have a main class we have
driver course that we want to register
this is a multi certifications that we
might need more than one task we have
drive and memory with extra mac section
s the normal parameters that you know
there might have some extra class path
application arguments and system
properties that we want to set so this
epic configuration has been passed and
get generated into this command
you see the normal star command for java
your application here in the over memory
that you want to use systems and
properties and with one Tarasco uuid
that regenerate all the time for each
measures task instance we generate this
ID so you can later identified when you
want to use it for logging or other
purposes that you might find you the
main class and the parameters so what is
what for the spark tasks as you can see
it's very very similar to the java task
except that it should start with a spark
submit to a launch command it accepts
you learn also takes of resources launch
command etc of the temporary folder get
created famous for java so this looks a
bit different because we have an
distributed job that we want to start
with it Ja as well but we also have
driver memory and executive memories so
if spark launches an application
multiple excited excited executives gets
boned as well and how much RAM we do
want to give them how many cause we want
to use in total here we use 500 course
you can we have some these students with
some configuration properties that we
can search for Spock as well and this is
actually the command that gets created
then afterwards so we have to spark
submit command we have to measure small
so that we know anyways because it's
mega framework that we start to start
from where the executive memory total
number of course whereas with dash dash
calls we have the spark parameters that
we want to set and also we have in our
driver and in our executives as well we
can access this terror Scout ID so what
other features do we have in tariff
count because we have integer days we
have a notion of staging and production
we actually want to run this these tasks
in a specific Meadows role so we can do
this we have reconciliation of nettles
talks to check ok is the job still
running and so on with ilysm always
updating or scheduler while we were
running we want to start the
a task only in a specific the time of
the day so that our data scientists can
use a cluster while during the day and
doing tonight telesco then takes over
the resources and can't on the jobs that
it needs to run until the next day it
can deal with mothers master changes it
has a REST API to query status and
corrosion and trigger a shutdown
remotely and terrace code is pushing
matrix into a time-series data base so
all the jobs that are started and so on
we push push the number of jobs the
number of handlers that got executed
into in fact DB in our example that's it
so far for me thank you very much for
your time and and go to yep if anybody
has a question
okay I guess then thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>