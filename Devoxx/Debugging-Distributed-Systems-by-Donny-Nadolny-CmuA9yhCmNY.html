<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Debugging Distributed Systems by Donny Nadolny | Coder Coacher - Coaching Coders</title><meta content="Debugging Distributed Systems by Donny Nadolny - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Debugging Distributed Systems by Donny Nadolny</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CmuA9yhCmNY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone I'm Donnie I'm a developer
at peh-chu duty and I'm one of the
people who investigate outages to figure
out what went wrong and what we need to
do to fix it did I am going to tell you
about one outage that we had or one
problem that we had with zookeeper that
caused several different outages for us
can I get a show of hands how many
people have heard of pager duty okay
some not bad our core tool is basically
an alerting tool so usually you have a
lot of different monitoring systems what
they do is they send an event to us and
then our software will call or text or
email your developers or your ops people
who are on call to let them know that
you had a problem so that you can fix it
and we also have a bunch of other tools
for doing incident management to help
you organize large-scale responses
around when you have these kinds of
outages now because we're the ones that
you have to rely on when your software
has problems we care about high
availability a lot so one of the tools
that we use for that is zookeeper how
many of you know about zookeeper have
used it or heard of it okay a decent
number good
zookeeper is a distributed system which
is useful for building other distributed
systems so what it gives you is you can
think of it as a small in-memory file
system or you can think of it as just
kind of a hierarchical key value store
and the idea is that you don't store
regular files in it you store little
bits of configuration information the
API that it gives to you it gives you
lots of different things like you can
create a directory you can create a file
you can atomically update a file you can
watch a file for changes you can create
an ephemeral file so that's a file that
will go away whenever the client
disconnects from zookeeper or you can
create a sequential file this is one
where if you have multiple clients that
try to create the same file name at the
same time it will add an ID to it and
ordered ID and the idea is that you take
this and you build other higher level
primitives on top of it so you can use
it for leader election you can have
multiple machines try to create the same
file in whichever one gets it that one
is the leader you could use it for
service discovery by creating one of
these ephemeral files where the value is
your IP address and port and then
whenever you're offline that's going to
go away and the one that we mainly use
it for is this last one the sequential
file you can use that to build up
different concurrency primitives like
some of fours and locks and it's
distributed locking that we mainly use
it for at peh-chu duty
one of the tools that we use we have a
lot of different services that use
Cassandra for a database and Cassandra
is pretty nice overall it's highly
available it's scalable but one of the
things that it doesn't give you is any
real transactions meaning like an asset
transaction that you might be used to if
you use a regular sequel database
instead you just give it rights and it
will do them kind of at the same time
you can overwrite data and it doesn't
really give you a lot of easy ways of
dealing with that if you're interested
in some of those issues I gave a talk
earlier this year at Cassandra summit
and if you think about Cassandra it's
not a lot of people think about kind of
eventual consistency you do everything
and it will eventually happen but
there's a lot of kind of weirder
anomalies that can happen if you really
dig deep and really kind of look at
what's going on so if you're interested
in that you can take a look at that talk
but the the benefit of using zookeeper
with Cassandra for us at least is that
we can take a distributed lock and when
I say a distributed lock I mean this is
a lock that is pager Duty wide and we
would take it on some enemy or maybe on
a user's data and that will guarantee us
that none of the other machines that we
have well or only one of the machines
that we have will try to operate on that
data at one time and it's not as good as
having actual transactions but it does
help quite a bit you get at least some
level of isolation
and zookeeper is pretty useful for doing
that because it and what its weight we
like using so you could go for that
because it's highly available one weird
or uncommon setup that we have at
peh-chu duty is that we actually run our
cluster over a wide area network now
this is for our Cassandra cluster as
well as long as you keep your cluster we
just have one big cluster that we run in
multiple data centers and in fact even
over multiple providers and I should
it's one cluster per per service that we
have so not just one but that's of one
but all of them run across multiple data
centers with a decent amount of latency
between them about 24 milliseconds in
our furthest data centers and the
problem that I'm going to talk about
today was kind of because of running it
over a wide area network but in general
it does work and Cassandra as well does
work like this a little bit about how a
zookeeper works you run a zookeeper
service as a cluster so you have
multiple servers they're all running the
zookeeper software they all have an
entire copy of the database and the
database in this case is going to be
usually pretty small because you're not
storing real data in it you're just
storing kind of small amounts of
configuration or coordination
information so every zookeeper node has
a full copy of the database and they
elect one server to be the leader as a
client you can talk to any replica and
zookeeper and perform reads and you can
perform writes as well but those rights
are just going to be forward to
whichever note is the leader
that's how zookeeper gives you the
that's how it gives you the ordering
guarantees that it has and the
consistency guarantees now this
generally works but in our case we had a
series of failures that happened they
they followed a couple of different
patterns so the main pattern what
happened was we had some kind of network
meaning packet loss or latency and then
the zookeeper cluster would get stuck so
at point number one this is a graph by
the way of the the size of the database
of the number of items in it and regular
jagged lines are good and flat line is
bad what would happen is one of the
followers would fall behind the leader
so it's supposed to have a full copy of
the database but instead it didn't have
all the replicas or all all the items in
the database and then a little while
after that the entire cluster would lock
up that's that point to where everything
is everything has flatlined the other
pattern that this followed was pretty
similar at point one you have a follower
that fell behind that point to the
entire cluster fell behind but at point
one point five that's where the leader
actually fell behind the rest of the
cluster and this might seem really weird
if you've worked with any kind of leader
based systems usually the leader is
supposed to be up to date but in this
case that's actually okay anytime you
have one of these leader based systems
where you're replicating data to all
these other nodes if you don't
explicitly check that the leader has the
data that you're trying to replicate
then you could end up with a situation
where the followers have more up-to-date
information that the leader hasn't yet
applied because it replicated the data
out but it didn't actually apply it to
its own in memory copy but in in the
case of zookeeper this is actually okay
and that isn't the real problem the
problem is number two where we have this
flat line now to to recover from this
situation initially what we tried was
just restarting all the nodes in the
cluster and then when it came back out
they was fine so that that actually did
recover but later on when this outage
had happened later times or that this
similar pattern we tried restarting just
the leader and we found that that
actually worked so we didn't need to
restart all the nodes we just needed to
restart the the leader to get it to
gather the cluster to recover
now we'd spent quite a lot of time
investigating this we were not really
making much progress and the first hint
that we had to try to figure out what
was going on here was this log line on
the leader too busy to snap skipping now
what this is is zookeeper replicates all
the data from the leader to all the
followers and it keeps a transaction log
of all the operations that have happened
and it also keeps a snapshot every once
in a while it will just write out the
entire state of the database to disk
that it will be able to recover faster
and that this is an you know that this
takes not very much time because the
entire database is in our case less than
a megabyte but if it's already in the
process of writing out one of these
snapshots when it goes to write it out a
second time it won't write them in
parallel instead it will just skip the
second one and it will wait for the
first one to finish and it logs this
line that it was too busy to take a
snapshot so we decided to try out some
fault injection trying to replicate this
there's a tool that we used which is
pretty useful called sshfs you can use
this to mount a remote directory of any
server that you have SSH access to you
can mount it locally then you can point
zoo keepers data directory at that
folder or that directory and then you
can use all the tools that you use for
messing with the network like IP tables
or TC which I'll talk about a little bit
later and you can use those and even
though you're messing with the network
those show up as disk problems so you
can make the disk appear to to be slow
or to hang completely and when you try
that out we found that that gave a
similar failure profile to what we had
seen before sometimes when you do it
even get the entire cluster to lock up
and other times you get first to the
followers sorry first the leader to lag
behind and then for the entire cluster
to lock up but after we had looked at
that a little bit more we took a look at
some of our metrics for disk latency and
we saw some logs that were being written
at the same
time and we figured that this probably
wasn't what happened what was happening
because for this to happen it would have
to take more than two minutes for us to
write out this one Meg file and from
from the metrics that we had seen it
just didn't seem like that would write
like that was happening one of the
annoying things during this outage was
that the first warning that we had had
that this problem was happening was
general application level monitoring so
we all of our applications have loss of
monitoring them in them and they had
some high level monitoring that would
tell us just kind of things are slow or
things are queuing up and we're not
really making progress and we didn't get
a nice alert saying zookeeper is having
a problem it was just things are bad and
then we would have to go and figure out
that zookeeper was the problem and then
take this action to fix it now
these high-level application checks can
be useful because they can catch lots of
different problems so when you have this
kind of alert that we did on things just
you know queuing up and not really
making much progress that can catch
problems if your database is down if
zookeeper is down if too many machines
are down any kind of problem like that
but it doesn't really give you a lot of
information into what the actual problem
is and for that we actually did have a
little bit of monitoring on zookeeper
which which we thought would have told
us what was going on this is a command
that you can send to zookeeper
you can tell that to a certain port and
send that the this four-letter command
are you ok and if it's working then it
sends back the string I'm ok and we
actually ran this but zookeeper was
actually replying I'm ok on all these
nodes and by the way the the leader
during all of these outages was still up
the the process was still running it was
just that the cluster wasn't making any
kind of progress so what we ended up
doing was we added a deeper health check
to zookeeper instead of just doing that
simple check we did a still simple but a
little bit better check of just writing
to one zookeeper key and then reading
from that zookeeper key that we just
wrote and if we could do that then
had this monitoring on the side saying
that yes zookeeper was okay and this
actually ended up giving us quite a bit
earlier warning than our regular
application monitoring when this
happened again and sadly this average
did happen again our monitoring on
zookeeper told us pretty much right away
that there was a problem that the
cluster was freezing up and then we had
a little bit more time to investigate to
figure out what was going on while the
outage was happening and when we did
that one of the pieces of information
that we captured was a stack trace from
the leader this ended up telling us
quite a bit about what was going on in
the cluster so this is on the leader at
point 1 we have ZK database serialize
snapshot this is the code where we are
trying to send a snapshot from the
leader to the follower so in the case
where the follower falls behind it can't
just catch up from the transaction logs
from the leader it just gets an entire
snapshot of the database to then rejoin
the cluster and be useful again so on
the leader at point one we're sending
this snapshot to the follower at point
two we are serializing one individual
data node this is just an item in the
database and then at point three and we
we have to take a we have to enter a
synchronized block on that so we're
taking a lock on that item in the
database while we're serializing it and
then at point three we're doing a socket
right so we're writing over the network
and what we saw was that this thread was
hung at this point for quite a while I
think it was a few minutes that we had
waited to get another stack trace and it
was stuck at this point the entire time
now this actually tells us quite a bit
about what was going on if you dig into
the other threads that are on the leader
for zookeeper so the way the zookeeper
works is to process requests it has kind
of a chain of thread so you may have
heard it called Sita before staged event
or if in architecture you have a chain
of threads that will process the
requests that are coming into zookeeper
and we also have one thread per
follower these are called the the
learner handlers and those are the ones
that handle requests from other a
zookeeper instances now when we need to
send a snapshot to the follower we go
and we have to iterate over every item
in the database serialize it and send it
over the network so we're entering
synchronized blocks and then leaving
synchronized blocks for every item in
the database but then at one point for
whatever reason we don't know yet it
hung and it would keep that lock on that
individual item that this synchronized
block now the problem is that these
other threads that process requests they
also need that lock if you're doing an
operation that that operates on that
item so if a request comes in then our
our request processor threads need to
take that lock as well and by the way
these requests process or threads do
things like so first of all receiving
the the data they they write it out to
disk to the transaction log and later on
it will replicate it out to the
followers and then the last step is
applying it to your own in memory copy
of the data so now we have one of these
threads with the the Java synchronized
blocks of the lock on this one item and
then we have another thread in our
request processor that is trying to take
that lock as well so when this happens
we just have a deadlock and that's why
the the leader can't make any progress
but this also explains why we had these
two different manifestations of this
failure because depending on what kind
of operation you're performing you might
need to take a lock at the beginning of
the request processor chain or at the or
you might not need it till the very end
so if it was the kind of operation where
you don't need its at the end then you
can still replicate that data out to all
the followers you just can't update your
own in-memory copy so that's why we had
the two different ways that this failed
this is the snapshot code that zookeeper
uses to serialize that data what we have
here is we have we enter the
synchronized block for the node
we call output right string and output
right record those are the ones that
could that are writing over the network
and could potentially block and then we
get a copy of the children remember this
is kind of a file system or a hierarchy
and then outside of that lock that Java
synchronize block we go and we iterate
over the children and then we we just
recurse and call it again and it's at
that point one of those output dot right
string or right records that were
hanging now we know most about what was
going on why this failure or kind of
what was happening with these two
different failure cases but there's
still one other part that we don't know
yet which is why didn't one of the
followers take over zookeeper is a
replicated system so you have the leader
and you have all these followers and the
followers were supposed to be able to
take over if the leader isn't doing its
job we had done that manually by killing
the leader and found that the the
cluster recovered but we're not supposed
to have to do it manually zookeeper
supposed to do it automatically for us
so though the way that zookeeper handles
this is by having a heartbeat from the
leader to the followers when the
followers get the heartbeat then they
know that everything is fine and they
don't have to do anything but if the
followers don't get a heartbeat within a
certain time out then they start an
election and then one of them should
take over from the leader and now
actually at this point if we look more
at zookeeper we can figure out why this
wasn't happening because going back to
all these threads on the leader we have
to request processor chain which is hung
we have the learner handler which is
blocked but the thread that sends all
these heartbeats is off on the side so
even though basically everything in the
leader is stuck we still have this one
thread which is healthy enough to send
out heartbeats to all the followers
saying you know everything is fine don't
take over and because of that the leader
process can still be up but nothing
actually happens the the leader isn't
making progress so we know most of all
was going on why we had these two
different failure cases why the follower
by one of the followers within take a
note taking over but we still don't know
why that Network right was hung for so
long so to figure out that we need to
dig into TCP behavior a little bit here
we have connection from between the
follower and the leader and assume that
we've already done we've already
established the connection we've done
the three-way handshake and then the
follower has to requested a snapshot
from the leader starting from that point
the leader would send a bit of data to
the follower and then the followers
colonel will reply with an act to let it
know that I had it that I got it but
let's look at what happens if you have
some sort of network problem if you do
the leader sends the packet to the
follower it doesn't get back in
acknowledgment and then what happens is
it will wait for a little while and then
it tries again so this is the q-sphere
retransmission timeout and it is at
least 200 milliseconds but if you have a
good connection it'll be pretty much 200
milliseconds now if you still don't get
the ACK you try again using still 200
milliseconds and then if you still don't
get it
you start doubling the timeout so now
you wait 400 seconds and then you try
again and then you wait 800 and you try
again and you go up to a maximum of 120
seconds and 15 different retries and if
you put all that together these are the
the values from the from Linux defaults
we have the retransmission timeout
offset and the reach rise when you add
all that up you end up getting fifteen
and a half minutes so that's how long a
socket right can block for if you have a
network partition in between them and
that's actually a lower bound it could
go higher if your TCP retransmission
timeout happened to be higher maybe you
had like a little bit of network trouble
before and so it would have bumped it up
so during this whole time fifteen and a
half minutes if you're doing the socket
right it's going to be blocked for that
and
haier time now this is useful to know
and it was really surprising to me but
it's not quite what was happening
because for us we had a few minutes
where the network was really bad but
then after that it was pretty much ok so
let's look over the the timeline of what
we saw happening at 0.1 we had some sort
of network problem we had packet loss
and latency number two one of the
followers would fall behind and then it
would restart and it would request this
snapshot from the leader number three
the leader begins to send the snapshot
to the follower number four the snapshot
transfer Falls this is where we have
that really bad packet loss and the the
socket right call is just hung number
five the followers who people will
restart because it couldn't get in time
and because of that the kernel will
close the connection that it had open or
at least it will try to number six this
is just a few minutes later the network
heals but at number seven we found that
the leader was still stuck and the
cluster was still in a bad state so to
dig into why that was happening we need
to look at what happens when we close
the connection and what when the network
heals so this is the regular TV close
connection starting from an established
connection when one side wants to close
the connection they send a fin packet to
the other side the other side replies
back with fin ack and then once you get
that you said you reply back with an ACK
and you set a timer for 60 seconds to
wait for more packets to come through
and then after that 60 seconds you
consider the connection closed and when
the act gets to the other side the other
side will consider that connection
closed as well
that's what normally happens but let's
look at what happens when you have a
network partition in between we have
similar behavior to just sending regular
packets except it's a different
configuration parameter for the number
of retries so when you're sending the
fin packet first you do retry again it's
a linear retry and then you have a tree
tries but then after that
you still haven't heard anything back
you just marked the connection closed
and all that if you work out the math on
that one it ends up being just a minute
in 40 seconds
so if we combine those two together you
can see that if you tried to close the
connection at the same time as another
the other side who's trying to send you
data but you had a network partition the
one side will consider it closed after a
minute in 40 seconds but the other one
takes 15 and a half minutes before it
considers it closed so during this time
if you were to run netstat on these two
machines maybe a couple minutes in you
would see that the follower had no idea
about what was going on in this
connection but the leader still thinks
that it's established it doesn't even
have any kind of warning or error State
it's just retrying packets but from its
perspective the connection is still
established but we'd said before that
the network had healed after a few
minutes and TCP actually has a way of
recovering from this if you get a packet
for a TCP connection that you don't know
anything about you're supposed to reply
with a reset packet this tells the other
side that you don't know about the
connection and they should stop trying
once they get that reset packet they'll
mark their connection as closed and we
were only seeing half of this we weren't
seeing the reset packet going back but
we were seeing that packet coming
through on the follower but the reason
why we were seeing that packet come
through is because it was being logged
in syslog by IP tables this entry shows
this is on the follower
we received a packet from the leader and
we dropped it because of that the the
leader will keep on retrying we keep on
dropping that packet and we don't send
back any any reset so they still think
that it's established until they go
through that really long 15 and a half
or more minute timeout now the reason
that we were dropping these packets well
to find that reason let's look at our
firewall rules this is a pretty typical
iptables firewall first you say that you
want to accept any connections for an
established oh sorry any packets
for an established connection or for
related meaning if you're in the process
of opening it or or closing it then you
allow whatever ports you want and then
at the end you drop every other packet
so the reason that we were dropping this
packet and not sending that the the
reset was because our IP tables firewall
didn't know about that connection
anymore but one thing to keep in mind is
that IP tables has its own way of
keeping track of the state of the
connection so when you have that that
rule at the beginning about established
and related IP tables starts tracking
the state of the connection but it's
completely separate from the regular
kernel view where you use netstat so you
can run netstat and you can see one
state of the connection but when you run
a contract that's a tool to to list the
other ones it will show it or it could
show a different state of the connection
because the IP tables view is done based
on just a simple timeout so in this case
when you send a Finn packet it sets a
timer for 30 seconds and then after that
it would consider the connection or it
will forget about the connection and
then drop any packets but if you keep on
sending Finn packets it'll keep on
bumping up that 30-second timeout so
what you can end up with is on the
follower when you send the Finn packet
to close it the kernel view will take
about a minute 40 or 100 102 seconds for
it to market is closed and then we'll be
in Finn wait that whole time but
iptables can have this different view
where it's setting these 30-second
timeouts and it can actually forget
about the connection after 80 seconds so
with this case if you ran netstat at the
90 second mark you would see that you
still had a connection it would be
infinite but if you received a packet
iptables would drop it saying that it
wasn't with it wasn't related to any
established connection so now we know
quite a bit more about what was
happening starting from we have some
packet loss one of the followers will
fall behind
and it requests a snapshot the packet
loss continues and then the follower
closes the the TCP connection after a
little while the followers connection
tracking module will forget about the
connection and now at this point the
leader is stuck because that or the
leaders connection is stuck in the
milliliter zookeeper as well
if stuck for about 15 minutes even if
the network heals after that point so
now because we know about this but we
know more about what happened we can
reproduce it so the the way that we can
reproduce it first we want to make the
follower fall behind so we can use this
command tcq disk
this will add 500 milliseconds of
latency plus or minus a random amount
from 0 to 100 milliseconds as well as
35% packet loss so you can add that run
that on the follower wait for a few
minutes and that'll call us it to fall
behind actually an easier way of doing
that is just killing the follower
process but this gives a nice excuse for
showing a useful command and there's
nowhere else that I can really put it so
I used it here so we actually uh so we
we have the follower they've fallen
behind the rest of the cluster next what
we can do is we can remove the that
latency and packet loss from the
follower and instead add a bandwidth
restriction so those are the commands in
the middle there the the queue disk add
commands next we can start the follower
as you keeper process again and then
it's going to request a snapshot from
the whoops it's gonna request a snapshot
from the leader yeah but yeah so what we
want to do though is now we're in the
we're in the middle of sending that
snapshot so what we want to do is we
want to pause it right then so we can
block the traffic to the leader and then
remove that band with the restriction
that we added so now at this point the
the transfer has been started it's right
in the middle you still have an
established connection on
sides so we can kill the follower and
then the follower will try to close that
TCP connection and we can just wait for
a little while and we've blocked traffic
so you can monitor this at this point
using this contract - l you can see what
the state of the iptables connection
view is and so after about 80 seconds
you'll see that go away and at that
point you can allow traffic to the
leader again if you ran that's dead at
this point for a short period of time
you would still see that connection on
the follower but the iptables has
already forgotten about it so any
packets that are coming through will be
dropped and it won't send that reset
connection back so now at the end of
this we allow traffic to the leader and
we have a completely healthy Network but
you have a TCP connection that is
completely stuck so this this works to
reproduce it now everything up until
this point has been pretty general it
applies to just kind of TCP behavior on
linux and and what happens in zookeeper
but there is one other part of our setup
that's a little bit a little bit rare
which made this slightly more likely to
occur it would still occur without this
but this made it just a little bit more
likely and that's with IPSec I'm curious
how many people are using IPSec in
production show of hands like five five
and a half IPSec encrypts all the
traffic between your machines so it can
be pretty useful because instead of
having to configure each individual
application to enable encryption or that
kind of stuff for each one or if it
didn't support it then having to set up
s tunnel or something like that instead
you can just enable IPSec and all the
traffic between the machines will be
encrypted the way that this works is
anytime you want to send data TCP or UDP
data whatever it will first be encrypted
by the IPSec code and then it will be
sent as this ESP packet encapsulating
security payload and this is you
and send those kinds of packets over TCP
but you it's much more common to send
them over UDP and it may seem weird to
take TCP data and then send it over UDP
data or and send it over UDP but really
it's okay because we've already seen
that TCP handles that kind of packet
loss so if it has encapsulated that TCP
data and it gets dropped well at the TCP
level it'll retry it and it'll just get
sent again now I won't go into detail
about this handshake but just enough to
show that before you can talk to another
machine with IPSec you have to do this
this these two different handshakes to
negotiate an encryption key and that
ends up being for round trips to do it
now while this is going on so that this
happens right when you first try to send
key speed data or just establish a TCP
connection to a machine that you haven't
talked to in a while now while this is
going on while it's trying to establish
the connection we still have that TCP
data that we wanted to send and you
might think initially that like you know
you might automatically assume that we
would just buffer that TCP data up and
then send it all right at the end but we
don't need to do that because TCP
handles tackle loss it is correct just
to drop that packet on our side before
we even attempted to send it and let TCP
retry while we're trying to establish
that connection
so that's what IPSec does it it will
just drop that data until it has a
connection now the other part to know is
that IPSec also has a heartbeat that it
uses so with this is different from TCP
with TCP you don't have any heartbeat if
you establish a connection with another
machine you could leave it open for days
not send any data back and forth and it
would still be considered established on
the other end and it would only be kind
of routers that are in the middle that
would really interfere with you being
able to do that but I P SEC has a
heartbeat that it sends fairly
frequently and if it doesn't get that
heartbeat at least with enough enough
times it will tear down the entire
connection and then you have to
re-establish the connection from scratch
and again during all that time any TCP
data that you're trying to send is going
to be dropped on on your side before you
even try to send it across the network
so the end effect of this is that if you
have high packet loss not a hundred
percent packet loss but just high packet
loss it can be very difficult to keep up
an IPSec connection to the other side
and so this will turn high tackle loss
into effectively a hundred percent
packet loss because you can't establish
an IPSec connection so you won't try to
send any at all and by the way this this
kind of messes with the the retries of
TCP as well because TCP has the the back
off that we've seen of if you haven't if
you haven't got acknowledgment of what
you've sent you retry it with some
increasing back off but if you don't
have that IPSec connection established
at all then it means that all the quick
retries that you've done are dropped
before they've even been sent over the
wire and it's only later on when you
have these rare ones that you might be
trying a bit more and actually sending
them if you could even get that IPSec
connection going in the first place now
there are I think a lot of lessons that
you can take away from this one of them
is that this stuff is really complex and
you have to really keep on digging and
be really persistent to figure out
what's going on but I also have a few
probably more practical lessons the
first one is that you shouldn't take a
lock and perform a potentially blocking
operation this was kind of the the the
key part that caused su keeper to stall
it's that we were take we were entering
a Java synchronized block and then we
were performing in that Network right
and that right can block for a really
long time fifteen and a half minutes or
even longer now one interesting thing
that I noticed while I was kind of doing
this analysis is that interfaces and
abstract methods can make this harder to
reason about in the zookeeper code
that output stream that we were writing
to or it's a not an output stream but
it's a their own output class that was
actually an interface that you that you
use where you call write record or write
string to perform the network operation
and it just happened to be that the
implementation writes over the network
and that that can block for a really
long time now I don't think it made a
difference in this instance but it's
very easy to imagine a situation where
you write some code you enter a Java
synchronized block and then you call an
interface or another abstract method and
at the time you write that the only
implementations of it are non-blocking
they just do something in memory but
then later on someone could add a new
implementation that could block and then
all of a sudden your original code is
incorrect the other lesson for this is
that it would have been helpful to have
automatic debugging info debugging info
collection so things like stack traces
keep dumps for databases transaction
logs or other things like that we were
only able to figure out what was going
on after we had this stack trace of a
hung java process for the zookeeper
leader now before this had happened we
didn't have any automated collection of
this and we we didn't get it at all we
were stuck kind of looking through logs
which were done at the info level and
then you have to do that annoying
reasoning of okay this method couldn't
have been hid because it logs at the
info level and I didn't see that and
it's just really annoying to try to have
to use that limited information to
figure out what was going on it would be
much better if we had these that this
extra information stack traces and
things like that and in an outage
situation it can be hard to remember to
collect them so it's useful to make that
just happen kind of automatically the
other one is that the health checks that
you're using should be at least a little
bit deeper than probably what you are
using of just the basic you know is this
port open or that kind of thing
anytime your application has a
dependency on other things like a
database or a third party service if you
want to make sure that that service is
working is
useful to perform more of a real request
to it rather than just the most basic
one so in the case of zookeeper we did
an actual right and then a read I know
for a lot of databases people use kind
of just a you know a select one query
but that one doesn't even hit the disk
so it doesn't go through a lot of the
normal stuff that a database would have
to use it's possible that your database
can respond to that query but can't
respond to real queries so anytime you
have these dependencies first of all
it's useful to have something on the
side separate from your application
monitoring weathering it's whether it's
healthy and have it just be you know a
little bit deeper than you might
otherwise think to do now the the other
thing which is applicable more to just
kind of distributed systems in general
which i think is a pretty big problem is
that anytime you have this kind of
leader follower based system if you're
using heartbeats or some other health
check like that to prevent followers
from taking over or from trying to take
over and become leader if on the leader
you do anything less than check to make
sure that you would actually be able to
process requests you know hitting disk
doing all the things that you need to do
then you can end up in a situation like
this where the only thing that's healthy
is a thread telling the other the other
followers that you're still healthy even
though you can't actually do anything
and that's pretty much the worst thing
that you can be doing in a distributed
system is not doing your job
telling all the other people who could
do your job not to do it either
I forgot if I mention this but that this
is fixed in the latest version of
zookeeper the fix that we applied that
was where we picked a pretty simple one
back in that in that synchronized block
instead of writing over the network
while we had while we were in that Java
synchronized block we instead took an
in-memory copy of the data and then
outside of the block we still write to
the network so that's a very simple fix
which means that then you're not
blocking or you're you're not taking a
lock that other threads need so you
wouldn't end up causing it to deadlock
and there are other things that you can
do as well that that would make this a
lot less likely to happen
so thank you all for listening one quick
thing I'll leave up here these are all
the commands that I showed in this
presentation as well as a bunch of other
ones that are pretty useful yeah you can
take pictures if you really want them
right now also post the slides a little
bit later so does anyone have any
questions
yep
yep so the the question is that the the
health check in zookeeper that had it
that's that's not very good and has that
been fixed so yep I I agree that it's
not really good I think that a lot of
other distributed systems have a similar
thing like that where they don't do a
real health check before sending out
that heartbeat to prevent other people
from doing it so I think a lot of
distributed systems to change do that I
don't think that has been fixed but I do
have plans to fix it I should probably
just open a ticket cuz I've been
planning for a long time and I've never
done it any other questions yep how do
we automate the debug info collecting so
for zookeeper at least we've done most
of the work of automating it where we
just have a single a single script that
you can run which will take a stack
trace it'll take a heap dump so if they
get a stack trace you can just run in a
stack or do kill - three and it'll send
it to a standard out keep dump as well
you can just run a command to get a heap
dump we also copy over in the case of
zookeeper it has transaction logs that
it writes I think they call it a commit
log there we take a copy of that because
that can also be used to determine when
it was doing things and what it was
doing so in our case we haven't gone or
we haven't been able to just completely
automate it of any time there's an
outage take it but we have it at least
at the step of a single command
we'll grab all the useful information
that we need more questions yep yep
okay so elaborate on the use of locks
when we're interacting with Kassandra so
we we do that to deal with some of the
consistency issues of Cassandra what we
do is typically we lock at either a
pretty fine grain level so usually it's
either all the data related to a user or
separate from that just one individual
database entity and so we we take a lock
and then we operate on it and then we
give up that lock now the the use case
is there is to prevent other machines in
our system from trying to concurrently
operate on that because Cassandra
doesn't have real transactions so if you
did have two things that we're trying to
operate on that that database entity you
could end up with you know you you wrote
five different columns and three of the
columns from this right are the one that
one and two of the columns from this
right are the one that one they're also
clock skew issues in Cassandra if you do
have clocks you well you you always have
clocks you but if you have clock skew
that is bad enough compared to network
latency then you can end up with a lot
of weird things like a write that
claimed it was successful but really it
didn't go through because it was ordered
before another write at least with the
timestamp even though it happened later
in real time and Cassandra will resolve
it just based on the timestamp that it
has so it is possible to read an item in
Cassandra modify it write it out and
then have that write be completely lost
even though it tells you it got it so a
simple trick that we use with with our
locking is because you have to take this
lock first we know that no two people
are concurrently updating it but also
after you're done doing whatever
modifications you have and you've sent
them to Cassandra we actually have a
thread dot sleep before we give up that
lock that means that we can wait out
this clocks you so we can have what I
think where our default is waiting 50 or
100 milliseconds afterwards so we can
have clock skew
of half that amount because one could be
ahead one could be behind does that
answer your question
cool any other questions yep oh did we
consider using network connection
read/write timeouts I should have
mentioned this in the presentation there
is a socket read timeout so when you're
you're reading from a socket on a per
socket basis you can set a timeout there
is no socket right timeout for a regular
blocking i/o you just can't set one it's
just done indirectly by these kernel
parameters of the number of retries that
you have when you don't get
acknowledgment for the data that you've
write you can't set any socket options
saying I only want to wait this amount
of time for a right to happen and yet
would there be another way to impose
those kind of timeouts yeah so for
example if you used n io
you can use non-blocking i/o and then
your MIT you can you can do whatever
check you want about how long you're
willing to wait for a read or how long
you're willing to wait for a right in
zookeeper it's a really old project they
were using regular blocking i/o and on
top of that regular blocking I know can
be a lot easier to read so you don't
know if you know if they reimplemented
it with n io maybe they would have other
bugs of like error handling or things
like that so it's a trade-off there but
with regular blocking i/o you can't
specify any any write timeout any other
questions
yep
if the leader is down for 15 minutes
when it's writing something shouldn't
the health threads so do you mean the
heartbeat on the heartbeat thread on the
leader that's writing yep so that would
be ideal if it did some sort of
monitoring of the health of things and
knew that it wasn't wasn't doing it but
it doesn't but that that is another
thing that ought to be fixed as well
yeah there's a lot of things that should
be fixed so like what we did the the
simplest fix of don't do a blocking
right in the in the synchronized block
there should also be some some work done
on the heartbeat mechanism there and on
top of that it would be nicer if we
tuned some of these parameters or made
some of these parameters tunable so that
you didn't have to have this 15 minute
wait for for doing a network right it
would be nicer if we could well what I
would like to do is keep the number of
retries that you do the same or even
bump it up but lower that 120 second
maximum retransmission time out have
that be configurable so you can lower it
and then you can you can it would still
be indirect but you would be able to
tune with that and say that you know we
only want to keep this connection open
for 30 seconds worth of time rather than
15 minutes or even higher any other
questions
Oh
did we change something in iptables what
do you mean oh did we change something
to have the with the the contract module
not being aligned with the regular
kernel view I think that would be a
really involved change I think it's
mostly for historical reasons that I
think iptables used to live outside of
the kernel but now a lot more of it
lives in the kernel so maybe you didn't
have access to that before it would be
nicer if it did that because as it is
right now IP tables just has simple
timeouts it doesn't know about the real
TCP state machine so it would be nice if
it knew about the realities the state
machine but that would be a huge amount
of work to do it'd be nice if someone
did it though any other questions
yep would it be better to use UDP
instead of TCP I mean it would prevent
this problem but then you have to deal
with ordering yourself yeah so it I mean
UDP versus TCP it's it's a trade-off you
wouldn't just get something for free by
doing that but it would be an option yep
so the question is about IPSec how it
open and what was the other one so I
versus OpenVPN yeah I I don't know yes
any other questions yep how long did it
take to figure all this out something
like a few months worth of man time like
two or three ish worth of time not just
my time there was other people that paid
you to be is polliver investigating this
yeah
but by I learned a lot so I'm very happy
that I did this investigation I didn't
know nearly as much about TCP before yep
so where do most of the time go to
investigating java code versus tcp the
vast majority of time was spent not
making much progress so before we had
this stack trace so once we had the
stack trace it was pretty quick to
figure everything else out because we
had spent so much time going over the
zookeeper codebase we even were kind of
grasping at straws we started reading
the there are a couple of papers on the
consensus algorithms that zookeeper uses
we were going through trying to figure
out flaws in that yeah poring over the
the codebase poring over logs trying to
figure out what was going on trying to
like reason about what what might have
happened so the vast majority of time
was just kind of investigating that
stuff and learning the zookeeper
codebase and all of that and then once
we had had the stack trace because we
already knew about like we already had
these diagrams that we drew on the
whiteboard all the time about the
threads on the leader and the threads on
the follower we already knew that so
then it was quite quick to go from that
to figuring out what was going on there
and then just a bit more time of
experimenting with TCP to figure out
exactly what these timings were and I
did that mostly out of curiosity like we
pretty pretty early or after we had done
that a lot of that learning stuff we got
the stack trace and then pretty early on
after that we knew what we could do to
fix it but I dug a little bit more to
figure out kind of what exactly was
going on
any other questions nope okay thank you
all very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>