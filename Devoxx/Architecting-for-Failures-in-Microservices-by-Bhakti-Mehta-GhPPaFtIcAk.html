<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Architecting for Failures in Microservices by Bhakti Mehta | Coder Coacher - Coaching Coders</title><meta content="Architecting for Failures in Microservices by Bhakti Mehta - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Architecting for Failures in Microservices by Bhakti Mehta</b></h2><h5 class="post__date">2017-04-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GhPPaFtIcAk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone welcome to my talk
architecting for failures and micro
services my name is bhakti Mehta
and before we get started let me just
ask you a question how many of you
deploy code to production please can I
have a show of hands
perfect please can you keep your hands
up if you've never had any issues in
production okay then your line in the
right room and this is the right talk
for you
so the reason the motivation for this
talk is distributed systems fail in the
most spectacular ways there are times
when it's it's a component which you
would have never imagined for which
could become the root cause for one of
the biggest incidents you would have so
with this talk I would like to give you
all some examples of real life outage
situations I have been in and how we
went about proactively mitigating some
of these issues which come up with
incidents a little bit about my
background I am in the platform team of
Atlassian I have a few folks from
Atlassian in the audience right here
cheering for us so we support all the
various of Atlassian products we provide
services which would be used by the
different products so you can imagine at
the scale of Atlassian and if we are the
platform layer for this broad all the
products it is extremely huge crucial
for us to be available to be reliable to
take care of our SL A's to take care of
our throughput so latencies so there is
a lot which we have to worry about
rather than just delivering or service
and we shall go over these I will give
you examples of what we do how we are
doing things in Atlassian and we could
follow up later with questions if you
have any questions we could take it at
the end of the talk in the past I was
the platform lead at blue jeans Network
it is not in the garment business
and I don't know how many of you have
used bluejeans network it is
videoconferencing in the cloud so I was
one of the first few employees at
bluejeans
and we scaled to millions of users and
dealing with billions of minutes so you
can see that there would be issues which
come up at scale and there would be
outages which would happen and I will
talk about some of these as we go
through the presentation before that I
have worked at Sun Microsystems for ten
years and after the acquisition of
Oracle for three years one of the open
source projects which I have worked on
is the GlassFish I'm not sure if you are
familiar with the GlassFish application
server it was a Java EE 7 based
application server it's in maintenance
mode now I have authored two books this
is my most recent book this is on
restful Java patterns and best practices
some of the topics which we might cover
later on in the talk would be based on
this book before that I have another
book which is on developing restful
services with jax-rs
WebSockets and Jason honestly I think I
have a short-term memory loss
because I forgot how hard it was writing
the first book and I still went ahead
and wrote the second one so a little bit
about Atlassian you could see the
various products and I'm sure most of
you are familiar with at least one if
not more of the atlast in suite of
products so like you most familiar would
be like the JIRA the issue management
software there is confluence for
knowledgebase bitbucket I'm sure you've
used this for your code and pull
requests there is bamboo for the CI
HipChat
is another product which we use for team
collaboration the newest additions would
be the Status page team and Trello
currently which has joined the Atlas in
family so why this talk
and what are we covering in this talk it
is important you could always learn from
your failures but it is more easier to
learn from the failures of others so in
this talk I will talk about how we go
about micro services what is the path to
micro services and then how would we go
to building resilient systems so most
companies would start off with a
monolith app you know it's it's got some
uisce and control the backend and then
there is the whole layer which they
would bundle it deploy it and that would
be the evolution of most initial
products for a system but then as as
time evolves you realize that you know
ok micro services it is done you hot
buzzword but there are a lot of
advantages with micro services there is
simplicity component would just do the
part which is assigned to do say they
could be a user service they could be a
billing service you don't need to couple
it together in a monolith act like a
monolith app you could just separate it
into various services there is isolation
of problems each service say if one
service goes down it doesn't impact the
whole application at least that's what
your goal should be that even if there
are transient problems in intermediate
services it should not impact your whole
application if this was a monolith
application the problems could not be
isolated
similarly scale up and scale down
it is very easy when these are decoupled
microservices you can scale one based on
how many instances you want the other
service may not need or so many
instances deployments become easier each
team can take care of the deployments
and manage it rather than going through
the whole process of being bundled in a
big monolith application there is also
advantage with polyglot ism and
heterogeneity each team can choose the
language the bills the libraries the
frameworks of their choice and they are
not bound by whatever the monolith
component dictates so all the sounds
fantastic
right in reality
so with microservices this is what
happens when there are outages it seems
more like a group debugging of
murder-mystery the since we had if it
was a monolith application there was
just inter process dependencies now we
have changed it with network latencies
now there is a user service component
there is a billing service component
there could be a category service now
all of these have to work together and
any of them could fail at any point and
that is where it becomes extremely
important to handle for failures so now
we move on to what is a resilient system
a resilient system is a system which
could keep working even though there are
these small roadblocks on the way they
could be transient impulses there could
be some stresses to the system but it
should just keep working you have to
accept when you're building micro
services you have to accept that
failures are going to happen and you
have to design for crumple zones just
like a car manufacturer is not going to
say - the best car in the world and not
design and dooster the crash testing
similarly you have to expect the worst
you have to expect the unexpected and
design for failures
now resilient system in terms of how a
customer should perceive you so I give
an analogy of a duck so be the duck so
your customers should think that
everything is behaving perfectly and
that's the way you want to portray
even though underlying there are going
to be issues but you want to make sure
that you build a system which at least
your customers perceive you to be
straight stable and available because
that's where that builds your confidence
for your customers internally this is
what you need to do you need to heal
quickly before your customers notice
there are error budgets for outages you
need to make sure you don't cross you
have SL s which your customers are
expecting from you
you need to make sure that your system
is available and that is where you have
to design for all of these failures so a
resilient system is one where the
customers could perceive that everything
is behaving normally even though it
might not and the sky on the other hand
on your side you should try to heal as
fast as possible now let's talk about
what would be the failures when you are
building resilient systems there are
challenges at skin I was at the meet up
with Randy issue from Google and he said
please think of your software as
artifacts you're going to throw away in
two years because every there is always
going to be challenges at scale and what
works now may not work in the next few
years because you know you have to
anticipate that there is going to be
growth and you may have to react attack
to your solutions there are always
integration point failures there are
network errors there are semantic errors
we all make that there are I will give
you an example of an outage where a slow
response brought down a whole system
there are Hanks and there are so many
times where all of us have run into GC
issues your API scan take the throughput
can take a nosedive because you know
it's doing a garbage collection so these
are the kinds of failures you have to
anticipate and figure out what would be
the best ways you would handle them
this is there was from Amazon the first
two lines there this is the new way of
life you build your services you run
them I have added two more to it this is
how we do it at Atlassian we own it and
we plan for it so we are the dev team we
are the ops team we are the QA team so
we not only build it we not only run it
we are responsible we get paged in
production so we have make sure that our
service is strong enough because wither
when you get paged at 3:00 in the night
that is not when you will be sitting and
debugging these issues so this is the
new way of life where you build it you
run it you own it we also plan the
capacity for it right you know you need
to estimate what other sizes what are
the instances say we are building or we
how many charge you need you have to
reprovision them so we make sure
of all of this so that is where it
becomes extremely important for us to
figure out what other things which would
go wrong this is one of my favorite
quotes long gone other days where people
could say oh my feature is deaf complete
no whenever somebody says the feature is
deaf complete it is definitely not ready
for production there is a lot which goes
to make a feature ready for production
there is a lot of operational expenses
which you have to account for so you
never consider that your feature is just
deaf complete and you are done
so now let's let me tell you share a
story about one of the outages where I
was involved in so this was one around
some time last year I get a message on
HipChat in the morning around 8:30 from
a colleague hey there are you there and
I'm like okay and he's definitely not
calling me to ask about my commute on
101 because most of you who are local
know how pleasant that could be in the
morning at 8:30 so I'm like okay we are
reaching in office and we had started
seeing a surge of 500 responses so there
are errors had gone up and so the next
thing what we do is like okay let's
figure out what's going on and there
were multiple things which had happened
overnight there was a new code which was
deployed there was notes which were
taken out by the ops team because it was
of some issues so and there was over the
pit was a peak of the our traffic so we
saw that there were a lot of errors and
that was causing the customers to have
pain points to integrate with our API is
and get into the meetings and all so the
first thing you do is as we talk be the
duct roll back the release got the nodes
up so the system went back to its stable
state now it's the painful process of
identifying what went wrong and what
could we have done better these are some
of the things which went wrong in that
outage so when you are in the war room
believe me it's not a
feeling I mean you are sitting and
cranking what are the various
possibilities get go wrong and when
there are so many things going wrong at
the same time you're really in a bad
shape because you have to come back to
your normalcy and you don't know when
the next peak load hits and you might go
into an outage again so we had bad nodes
in a load balancer group so there before
they were they were by the time they
were investing so we were serving at
fewer nodes than our usual planned and
we were handling the peak load there was
a new code which was deployed now there
was one of the api's was doing a very
complex calculation on the back end this
was an API which would figure out okay
which features whether you as an user
will you have this feature or know it
which figures out how to roll out
features to use other enterprises and it
does a lot of complex calculation in the
back
now this API was gradually increasing in
latency so over time it's like the
global warming it had over time it had
become slower and slower now there were
a lot of clients mobile clients web
clients who are depending on this API
and as everybody has their timeout set
if this API starts taking longer and
longer
they will timeout and eventually they
are calling the clients will timeout and
then you start seeing all these
connection failures on the other side
this new deployment had brought in an
abuse from the client side these api's
could be called where you know you could
get all the features in one call they
would call multiple times for all of
these for every feature there were cases
where even if you have to display a
button whether it's orange or purple you
know the UI team always keeps on
figuring out and trying these a/b tests
they started calling this API now
eventually when the choices made they
should really be removing these calls
but all this debris had accumulated over
time so it was being abused on the
client side so there are so many random
Network calls made which were really not
needed this is a problem which most of
us Playa face you know you can test
things in staging but there is no data
like prod data so how much ever you try
there are sometimes some of these
failures will never happen in a staging
environment another issue we had is say
these features were there there should
be a way to trigger a lenient fallback
if I'm calling an API call in it's
taking way longer than its supposed to
maybe it should just return back a
boolean value like a you know a true or
false and continue and rather than
timing out so we learnt this over time
and we saw how many things went wrong
to create that perfect storm and that
outage and we had alerts but I think
they should have been more alerts they
should have been more metrics on the
latencies and how things were getting
bad with time this is a lesson which we
have learned that errors can be frequent
but the latencies are consequential so
there is a saying that if you want to
know the value of a millisecond you
should ask an Olympic person who's
caught in a silver if you want to know
the value of a second you can ask a
developer who's gone through an outage
because of that one second or api's were
giving out a latency of one second and
that caused this huge outage so don't
underestimate your latencies and ensure
that you have some checks and balances
in place these were some of our action
plans following that outage we decided
to have circuit breakers we shall cover
that in the next few slides we decided
to have fall backs where it would be
lenient values which you could if
there's API is not returning what could
be a fallback we could do predictive
caching say you are asking me for a B
and C I can anticipate that the next few
features you would ask would be maybe de
and F and I could just cache all of this
proactively and provide it to the client
so the client doesn't have to keep
asking me I would just provide it
predictively we also ask the clients to
reduce the surface area and whatever
features were already done role already
rolled out maybe that core should be
removed so there is some cleanup which
needs to be done load tests load tests
extremely crucial and we made sure that
you try to get the same data as prod and
more
on your staging environments failure
injection testing there was a talk
before we're even Kathy Katie had
emphasized on failure injection testing
you have to test for failures bring down
nodes bring down your all your
dependencies bring down a component make
sure you ensure that you know there are
where your API is become take longer
than expected see how your systems
function in those cases and finally work
on alerts there is a lot of emphasis on
good alerting and that will help you
minimize this so I have another caption
which is the more you sweat on the field
the less you bleed in war the more
rigorous testing you do in these
environments the less the you'll have
issues in your production there is no
way that you're not never going to have
issues in production but you can
mitigate these risks by doing a lot more
in your staging and dev environments so
with this I will go into three stages of
resiliency planning the first stage is
what you would do when developing code
what are the patterns you would use the
second stage is what would you have to
deal with if you are before you deploy
to production and this is the checklist
and I would go in details in the next
few slides and the third stage is what
would you do when you are watching out
for failures after your deployment after
a big deployment what is it you would
look for we also Atlassian run crisis
drills so we plan for a day where okay
we might take down some systems in an
availability zone and we see how the
other downstream services are reacting
to it so we do plan for crisis drills
and this is definitely something which
you should take into account that run
your crisis drills because a real
calamity is not a time for training so
ensure that you do failure ingestion
testing so what are cascading failures
cascading failures are failures which
are caused by
reactions say in my previous example
there was one node which was not doing
well there was a high Lord and then
there was api's which was returning
slowly and eventually that adds up those
pylon and finally your whole system can
degrade I'm sure most of you must be
familiar with history and circuit
breakers basically it works the same way
like a circuit your current passes
through it at some point to circus trips
and it doesn't allow any more current to
pass through so the way hystrix works is
if some command has failed in the last
10 seconds it is highly unlikely that it
will pass now so it basically opens up
the circuit it won't allow the command
to be executed and then periodically
will sample to see maybe this mooo node
was just coming up so initially first
you might have failures but then if
there is a success rate it will allow
more traffic to it so I highly encourage
you to use history history as very good
metrics which you could monitor there is
a fallback option so if you know you can
always specify return from cache or
return some stale data in your fallback
option timeouts all api's should always
have a timeout associated with it we
typically forget request would have like
a five-second timeout and timeouts
usually go with retries so there if you
would call an api and you could wait for
a while and then it times out you can do
another retry retries help in cases of
transient errors but retries should
always go with a policy of say
exponential back-off where you don't
want to say a node is coming up you
don't want everybody to be flooding it
with requests so you would always have a
policy of say exponentially trying after
a fixed delay so add some jitter in your
retries you should also specify what is
your max number of retries there are
libraries which will help you do the
retries failsafe is a good library so
you should just use these third-party
libraries which would take care of the
retry logic for you
bulkhead so just as there is a ship and
the ship has these various sections so
you can see each section is separated
from the other how does this help is if
there is a flooding in one section it
tries to avoid the water from spilling
over to the next section this is another
pattern which we can apply in a
micro-services based architecture the
way we do it is if we have a micro
service which is using a resource say
our database we try not to share it
between multiple services say you have a
user service and you have a billing
service
try not to share the same database
because then you are introducing a
single point of failure if this database
goes down you are impacting two services
versus if you have these isolated
databases it is going to just impact
that one service so that is a similar
analogy to this bulkheads where you have
this section and if that is having some
trouble the problem doesn't creep
through the next few sections rate
limiting this is another aspect which
you should be aware of
there will always be malicious clients
intentionally or unintentionally who
will be flooding you with requests your
server can only handle a fraction of the
requests so at some point you have to if
there is you're seeing too many requests
from a certain IP from a certain client
you have to tell them to slow down you
this is extremely simple again there are
rate limiting libraries this is as good
as just adding a filter and you can
identify your clients by their access
tokens by their IPs
and you can figure out how many requests
they are making if the requests are
exceeding or threshold then you should
either send them an error say retry
after X number of seconds so this helps
you to evenly span out and avoid
malicious clients from taking down your
service caching most of the latency
problems can be solved by having a good
good caching story in place so I have
some analogies like if you have some
data which you can fetch from your first
level cache
like getting a beer which is right there
in your hand if you are getting it from
a second level cache it is still not bad
you might just have to walk till the
fridge and get the beer but if you have
to fetch something from the database it
is like driving cross-country to get a
glass of beer so cache often cache well
I have another story I would like to
share this is about the cache
invalidation whenever you use caching it
is extremely important to figure out
what is your time these cache entries
would stay in the cache so we had we had
this API which would take in so most of
you are aware you will get like an
alphabetic code sometimes which you have
to enter to verify and that API would
take in given five letters without the
vowels so we thought there was ample
space for you know and these were stored
in the cache so every time a code is
generated for you it should be a new
code which should not be there in the
cache we noticed this with this didn't
take us an outage this was found out in
a load test where we noticed that at
some point with load and over a period
of time since each of these entries were
valid for 24 hours we actually ran out
of letters and validation codes which we
would give the people so we of course we
made the immediate fix was yo you could
increase the number of letters but then
that involves changing all the clients
we could reduce the time to live in the
cache so we since the 24 hours was too
long we made it to 12 hours so if I have
given you a validation code which you
need to enter and verify yourself maybe
after 12 hours I think that was
realistic where you know the code is not
valid anymore so this was a case where
you know initially the plan was okay we
have a lot of you know this the load is
never going to be this much let's keep
the time to live as 24 hours in the
cache and we saw with a load that that
didn't work for us
so make sure you invalidate your caches
well and you have proper TTLs set in for
your cache entries
logging so whenever there is a
production issue logging and your
metrics they are gonna be your best
friends because there is nowhere else
you're going to identify what went wrong
in production so there are best
practices for login logging would be
have a consistent pattern which you can
identify across services we use MDC's a
lot which identifies what is an event
type what is a source we put all of that
information in our MDC's we use plunk
for logging so then we can do a stats
account by groups so logging is
extremely crucial and make sure you have
a consistent pattern you have libraries
which you use across your services to
ensure you get the proper logs in
production
another thing is most of these are you
office create the sensitive data you
don't want your client related data to
be shown in your logging especially when
you are doing stock to compliance and
all because there would be PII is in the
data so never lock payloads by default
trace your requests across services so
there is a call which is going from user
service to billing service you could
there are libraries like Zipkin which
can allow you to trace your calls it can
help you figure out how much time each
call took so you can see which are the
bottlenecks in your system so ensure
that you have requests tracing across
all your services one more point I had
over here was to identify the initiator
as part of your logs there were many
times when an API is called but when it
is logged if you don't know whether it
was the web client calling or whether it
was from the mobile if you could add in
extra headers and that is passed on to
the logs MDC that helps a lot in
debugging where you know that this
sample of failures were occurring only
on the mobile clients so identify your
initiator as part of this your logging
so we covered a lot of resiliency
planning practices when you write code
we talked about caching logging we
talked about timeouts retry circuit
breakers now we move on to the next
stage which would be before you do a
deploy so it is extreme
important before you do a deploy to do a
load testing a lounge ability testing
and also to plan your capacity
accordingly the way we do it is we have
a staging environment and at during
running these loads and longest
longevity tests we are ensure that a
staging environment is similar to the
prod with the same number of instances
and we'll try to pass in the load which
we anticipate which would be in over the
next six months and see how our system
behaves in terms of load we also have a
micro service which takes in gatling
simulations and all the data from other
services and that runs it so it's like a
load test as a service so it would take
in the micro service endpoint it needs
to hit the environment what is the load
you want to give what is the duration
and then it would plot in graphs so that
is how we do load testing in our various
environments we capacity planning is
another area which you need to invest in
you need to anticipate growth you need
to see if the instances you are
provisioned initially are sufficient
enough to handle this volume of requests
if not maybe you have to scale out you
have to upgrade your instances change
your descriptors provision your
resources accordingly so capacity
planning is also something which is very
important before you deploy your code to
production now we move into the third
stage of resiliency planning this is
when you've deployed your code to
production at that point you need to
worry about health checks monitoring
your system and phased rollout of
features so what is health check the way
health checks would work is say you've
deployed your system to these
environments each service could be
checked for some basic basic attributes
like memory CPU threads error rates you
monitor your system for this if any of
these checks are going above a threshold
you send an alert so this is known as
shallow hell
checks where you know you're monitoring
your system and seeing if it is healthy
if not then you remove this service from
the load balancer group there could be
deeper health checks where you could
look for your downstream dependencies
say I have a service which needs a zip
it's a prerequisite that I have to fetch
a zip from another service if that zip
is not available in my micro service it
doesn't make sense for me to operate so
we do these kinds of deep checks where
we check if this is not that I declare
myself unhealthy so I could be removed
from the AWS cluster and a new instance
could be spawned up which would make
these requests so these are the kinds of
deep health checks also we do so there
are shallow health checks and then there
are deep health checks this ensures that
the health of your services is
acceptable when it is deployed the next
thing which you would watch four-poster
deploy would be metrics and monitoring
so metrics again I said whenever there
is an incident the logging and metrics
are going to be your best friends you
can always go back in time and see how
are the patterns at that point so we
have metrics for everything we identify
metrics for response times for
throughput we see how long it takes to
execute queries and we'll have metrics
for that and we see if there is
something which is taking longer than
expected maybe send an alert so at least
you can identify and see what's going
wrong there are metrics for GC rate
there basically there are metrics for
any unusual activity there are a lots
which are created whenever a threshold
is exceeded now whenever an alert is
created and a person is paged we try to
have these run books these aren't run
books for actions to be taken so now
saying you get an alert say your node is
run out of memory what are the next
steps which the person could take so
what we do is we try to do these
operational run books which anybody new
person in the team would also be if this
is a failure these are the steps he
needs to take this helps them identify
the system and help to another system to
recover quickly so we invest some time
in
Gudrun books so this is just a light
note so this is what happens when you
are paged at 3:00 and you are debugging
somebody's good and you definitely don't
want somebody to look into your code and
be figuring out what to do at that point
when there is a production incident so I
thought I just put in some light humor
when we are talking about so many
serious things so this is what a
monitoring server does it looks at the
environment and it checks if everything
is healthy if not if there are some
alerts which if there are some values
which have gone all over the threshold
it sends alerts and the alerts could be
in the form of emails we use pager duty
so we can get paged and you can set your
preferences so you can get paged and
then you respond to the alert I have
another story to share and this was an
example of where we got saved by metrics
and alerts I was on call one of the days
and we started seeing these alerts
coming where the database there was a
max DB connection alert so we have a max
number of DB connections and we were
going way over that and then that caused
the CPU to spike up so the first
response would be is okay you go and
look at whatever the slower running
queries we found out some of the
culprits and then you identify that you
know okay these queries were taking on
an average 718 milliseconds and then the
95 percentile was taking two seconds so
you know that there is some problem in
this query the next step was to identify
what caused this and this was a code
push which was done for a bug fix
earlier and all it did was it added
pagination and there was an order by
clause but the order by clause a cause
did not take a function based in index
and that caused a whole table scan scan
with millions of rows and that caused
all of this to spike up and then since
it was being loaded in memory on startup
we started saying the CPU utilization
spiking up so these are
these are small cases where we were
saved by the alerts this was still we
have alerts in a staging and abroad
environment so this was something in the
staging environments so we didn't have
to take it to prod and see an outage we
identified it way earlier and we could
rectify the situation we fixed put in
another patch and we deployed the fix
for it so this is another case where
metrics and alerts can be extremely
useful rolling out of new features so
whenever you roll out new features the
way typically it is done is you want to
launch them dark and do a phased rollout
you should roll out maybe to 5% of your
customers see the feedback then move on
so always ensure that there is a phased
rollout so you can see if there are
issues you can catch them earlier rather
than just doing 100% rollout so we
typically do a phased rollouts of all
our features and again whenever you
rollout have metrics have alerts so you
know if something is wrong you could
just turn off the feature and work on
the fix so this was another incident I'm
sure you must be aware AWS had an s3
outage this was around two weeks ago did
any one of you get impacted by the s3
outage okay I see a few hands so you
know what I'm talking we did not survive
this I cannot say that we are the proud
owners of this t-shirt so we also saw
and I will give you some context on what
happened that day
so it was an AWS s3 authorized team
member who was supposed to remove a few
billing servers and he had a run book he
had a script but there were some
parameters which were entered wrong so
whenever he rerun that it took out a lot
of nodes on the s3 instances and it also
took down another system which was a
metadata system which held information
about all the other s3 nodes there was a
placement system which would help you
create update more s3 nodes so all of
them went down with the running of one
script the impact was heavy on most of
the service
not only okay we got impacted but the
services we used to monitor like pager
duty New Relic metrics
none of those day themselves were
affected so that is where you see like a
badly run command and you could see the
repercussions so some of our services
could easily recover and went to another
provider CDN some of our services which
were not in u.s. East were fine but we
had a lot of key takeaways from this
outage this is just just shows the
fragility of the system it was simple
mistake on the s3 on the AWS end but the
repercussions were wide across a lot of
teams and we were one of them so yeah we
learned and we do test for availability
zone failures but testing for multi
region and failures and the easy
failover
it is hard also in this times when the
AWS s3 is not working as well in Us East
if you try to spawn off resources in
other regions it doesn't really work
well because AWS will start limiting you
so because they don't want a thundering
heard so you know at these points you if
you try to create new resources it is
not going to work so either you have to
plan for in pre provision resources in
all regions or you have to handle but
this was like a example of like a small
error on the Amazon site and all the
cascading failures which happened to
downstream services so I have this cheat
sheet
I've studied in New York and the way we
used to do it before I exams our
professor used to give us one sheet of
paper and saying ramp as much as you can
in this sheet of paper and this is what
you can bring for your exams so old
habits die hard and I thought you know
if there is something which you could
take away from this talk I would think
it would be this cheat sheet we've
covered a lot of these topics and I
think this is something which would help
you architect and build resilient
systems in the future I ran out of space
to complete the alphabet so maybe take
that as a homework assignment but I hope
this is useful to you I'll leave it on
for sometime if you need to
so with that the key takeaway is yes
systems will fail expect systems to fail
plan for failures it is not a question
of if but is the question of when so
make sure you take control of your
systems before they on overpower you
with incidents and outages automate as
much as you can plan for automation
allow its metrics all of these are
extremely crucial to keep your systems
up and running and finally keep count
and cloud on so that's all I have for
today
if you have any questions you can ask me
now and we also have a booth and the
exhibit hall so we would be waiting
there afterwards so if you want to
continue we can have conversations over
there too so any questions for me okay
any questions
okay thank you for attending this talk
and thank you there walks for giving me
this chance</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>