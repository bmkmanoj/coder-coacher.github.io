<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Need for Async: In pursuit of internet-scale apps by Konrad Malawski | Coder Coacher - Coaching Coders</title><meta content="Need for Async: In pursuit of internet-scale apps by Konrad Malawski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Need for Async: In pursuit of internet-scale apps by Konrad Malawski</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/91jSc8tFTzA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so hello and welcome to the first
talk of today it will be about in
general a little bit of a synchronous
programming and a little bit of not and
its a mix of various topics so I hope
you'll enjoy most of them if not all so
we talk is it facing hot pursuit for
scalable apps so even vova topics are
very you know weird and not very similar
to one another the similarity lies
within or being something that you can
use and should use if you need a
scalable application and maybe you don't
use it directly but maybe you should at
least know that it exists so my name is
Conrad minovsky awake at typesafe on
vacati and also did a little bit of
reactive streams work recently random
stuff yeah what I do otherwise so I run
a geek on conference it's in Poland in
may if you want the visit cracker
wonderful place and also was involved in
a bunch of other community things so
including the Polish Java user group
software craftsmanship and will and
allowed and for java one program
committee for the emerging languages
track so i tried to smuggle in scala
whichever one that's basically my rule
so yeah I credentials see I did a little
bit of a TCK but enough about me and
more about you guys so I need to know a
little bit if you at least know what
reactive programming is ike hands up
again who knows what acha is okay great
who's actually using acha Oh less people
okay but you're aware of it that's fine
if I won't be too much occur in this
talk but there will be a lot of talk
that occur is using so maybe you'll find
it enjoyable this way so the way I got
to do this talk is I figured where we
have this great stuff but let's actually
explain what's underneath all this stuff
so we can appreciate it and a great
quote from Martin Thompson that explains
these concepts for me was fat
high performance is not really about bit
twiddling and you know changing one line
or two lines to do you know a bit
operation instead of adding a one
something like that it's more about the
core and fundamental design principles
but something is built with right so if
we are given is crap you can do all the
kinds of bit fiddling you want it still
be and will be slow with the high level
algorithm this you know N squared or
something so Atlanta for today loads of
topics and was even a bonus section and
I will see how far we get and feel free
to interrupt me if you want to but yeah
let's go so why this talk because the
free lunch is over so this quote is
pretty old by now but i'm still using it
so the free lunch is referring to if you
and now you add more cpus to a program
or well you add a better cpu to your
machine the program goes fast right but
that's not what's happening in recent
years in recent years we see you no more
cpus and in order to take you know
advantage of more cpus we need to make a
little bit of work ourselves or at least
use tools which are ready to leverage
multi-core and distributed systems
versus how we come to distributed
computing an asynchronous and
synchronous processing in general right
because we want to leverage more course
because we're getting more curves when
we get just normal speed up of a single
core nothing new right we've been
talking about this for years and what i
want you guys from to take from this
talk is to not be buzz words driven but
you know be well informed when you make
decisions both about frameworks and in
general what to use in your projects so
basics what synchronous wats
asynchronous simplest thing on the
planet is synchronous communication so i
sent you a message and I wait until I
get the reply back this is basically a
method call right I call the method and
I wait until it returns and when I call
another method I wait until it returns
synchronous programming right simple so
how does the synchronous change with
where a synchronous means I send a
message I don't wait for a reply I send
another message and now first thing that
can happen here the ordering if we're
fully a synchronous is not really
guaranteed all right I can get a reply
for the second message before I get a
reply for the first message why is that
nice that's nice because maybe the
second message is you know invalid or
very simple to fulfill but the first one
maybe takes a longer time so instead of
waiting for a long message to be
computed I can already start processing
other stuff that maybe is way faster so
here third message I sent the third
message and get a third room where a
second reply back for the third message
and only after all this stuff has
happened I finally get a reply back for
the first thing right so this already
changes quite a lot in terms of how we
think about communication and you know
ordering guarantees in such a system so
let's talk about a sinker it matters in
this example event loops and active
systems and anything that's basically
trying to put many executors on a
smaller number of resources the same
could be said about Fred's when you have
more threads when you have CPUs exactly
same thing at least concept twice so
conception and conceptually let's talk
about free actors they have in mailboxes
and the way an actor works is it takes
messages from the mailbox and it's you
know gets cpu and works on the message
so and the lines here signify time right
and i will be using yellow and blue to
signify which actor is using which cpu
at this point in time and like i said
you if you don't want to think actors
you can think of Fred's basically the
same thing in this example not in
general but in this example yes so we
have two guys each one of them is
processing a message so we used both of
the CPUs and at the end of each process
of a message it can't decide where do I
want to keep processing other messages
or do I give the threat to someone else
the middle guy decided yes still have
messages to work on so we'll continue
using the yellow CPU but the bottom guy
it's not really blocked but it's you
know doesn't get resources so it may be
starving but let's go further so at the
end of each processing of a message
first this point in time were canva can
be this kind of where the bottom guy can
get cpu and can start processing a
decent theory if he's lucky his messages
so okay after the second message of the
middle actor was processed the botton
actor gets for cpu and now can do his
job and on the top we see the guy keeps
working on his mailbox right he keeps
pulling in more messages so the middle
guy now is a little bit starving and we
call this a starvation right it's not a
new concept but what I want to highlight
here is the gray zone right so the gray
zone is when i use a blocking I oh I use
a blocking database operation a file
operation a network operation that I you
know I send for data and I wait alright
so now the bottom guy from my
perspective at least from the program's
perspective is wasting resources because
it has sent some data over to a database
and instead of giving me the middle guy
some cpu to work on it he's just idly
waiting right so this is a little bit of
wasted time so what can we do about it
so this is synchronous i/o and in
general any kind of asynchronous i/o I
asynchronous database driver would work
like that but I signal some message and
some data to the database right bottom
guy is signaling some stuff to the
database thing and when I get a call
back once the data has been written over
the thing has been processed right
and it goes into the same mailbox so
we're using reusing the same mechanism
and the difference is well now we don't
have a gray area that is wasted but in
userland we can just keep using this
thread for my middle actor right so
we're in improving utilization like
actual utilization and not utilization
on waiting okay green sound good lessons
learned that blocking is actually
indistinguishable from being very slow
right so if you look at it if if the
bottom guy wouldn't be actually blocking
here but only processing this message
for five minutes the effect is kind of
the same right the other guys are not
getting the resources they want so this
is why we really don't like blocking and
but i guess that being very slow kind of
gives the same effects so you want to
chunk up your pieces of works into very
small pieces so you can you know be fair
to your other peers so no parking at any
time but now let's jump topics of it
let's talk about latency and this is a
quiz I require some attention and maybe
hand waiting from you guys so latency
quiz which one is latency yellow or blue
line ends up with yellow hands up with
blue more people think it's the blue
well actually sorry yellow is at least
when we look at for queuing theory
definitions of what latency is latency
is the time from you know I come to the
queue and you know I get serviced right
if I'm at the border the guy is looking
at my passport when I'm at least being
serviced and it may still take some time
but at least you know the process of
handling my thing has started and the
bottom thing is more usually defined as
response time and now many people are
scratching their heads isn't it the same
thing well let's fix leave a problem but
these terms are very often used
interchangeably and in you know collect
your beard discussions fits fine maybe
but when you're discussing maybe an SLA
and people you know talk about latency
and you talk about response time make
sure that you understand it's the same
thing it's fine if you define okay when
we say latency we actually mean response
time but it's way better to actually
spell it out during a discussion went to
assume that other guy understands also
the yellow or blue as the same thing you
understand it too it's just a language
thing I would say so what is the green
thing any ideas so it's referred to a
service time so service time plus
latency is for entire response time at
least in these form and definitions so
if you want to be really precise with
words or wants to use if you want to be
somewhat precise then at least define
with your discussion partner what you're
talking about so for the rest of this we
stock I will define response time as
being latency example right it's good
enough if we understand what we talked
about so another quiz so this 10 seconds
latency acceptable in your applications
maybe hands up 10 seconds right it's
usually may be a bad thing or something
slow 200 milliseconds right yeah people
are starting to raise your hands fists
sounds already pretty good and now we
start getting a little bit more fuzzy
how about most responses within 200
milliseconds and who starts to think i'm
getting somewhere like this is a trick
question yes this is a trick question
okay because the following questions are
like so mostly within 20 but some within
one minute right so you see we're
starting to go into the actual behavior
distribution of the latency not only
saying in an SLA it has to be 200 buts
that's a very hard SLA but do people die
if we go beyond 200 million
if they do then we better try to stay
beaten below 200 pride but if I don't
and I don't know someone will be a
little bit frustrated because the kitten
picture is still loading then maybe it's
fine and we don't need to spend four
weeks trying to optimize the kitten
service right so it's a trade-off we can
always go faster we can always slept
more caches or more machines or better
algorithms and stuff but do we need to
it's a good question to ask yourself
sometimes so and this is actually a gun
from guiltiness talk these questions are
actually stolen from how to not measure
latency talk highly recommended so the
thing that we actually want our as
allies to look like is okay so ninety
percent of the requests should be below
299 below a second and we really
shouldn't go beyond two seconds for a
response right and sometimes people
instead of talking about this
distribution so like a displayed here
with fees and percentiles we talk about
our response time is 200 milliseconds on
average and then they remember a little
bit of math from University and see okay
standard deviation is 60 so let's derive
you know present types from it so
because you know bell curve and I move a
little bit to the right and this is my
ladies turns out that's not how reality
works so the mathematical model works
really fine at least four simple one if
it is a normal distribution but turns
out the profile of latency and
applications is very rarely normal
normally distributed it's very messy and
chaotic so remember that and don't
directly derive percentile values from
you know mean and average and standard
deviation right instead measure
everything that you can and verse tools
that help you do that for example it
your histogram a
it's a histogram implementation by guilt
Anna and what it allows you the
screenshot is from the Jacobs library it
allows you to record basically late
pauses in your jvm and when you can draw
it out like on the bottom picture and
when you compare these pictures it's the
same system but on the button top system
you only see on the top picture you only
see ok if there was faced hiccup and it
took how much is it here ok 100
milliseconds and it's sometimes
happening ok but on the bottom picture
you see the distribution of how often
it's happening right you can see ok this
system is exhibiting certain modes of
operation write this number of requests
is having this behavior and then its
slowly getting worse and then suddenly
is going worse really fast so what we
have to do then is to investigate and
look at this graph maybe with a little
bit of imagination and try to figure out
why this could be why the Swiss system
have these three modes of operation and
then you need to pattern match with
something that you remember maybe from
your project or childhood and this case
looks a little bit like a snake but
actually when you remember it's actually
an elephant so what I mean with elephant
is you remember in your system we have
this queue right but this queue it has
faced buffer sizes all right so maybe
maybe the queue here it's growing and
still coping but maybe here it's
completely full and starts rejecting
work and we need to resubmit or
something like that you can track this
behavior kind of hints to actual you
know code an internal knowledge that you
have about the system so that's the
elephant you looked at it from the
outside it's a snake doesn't really make
sense but the elephant is the thing that
you can connect to what actually makes
the graph make sense it's for thing
inside if it causes with ya so the
right hand side of such graphs we
usually refer to a state latency so it's
the higher percentiles right and v's are
usually the ones we want to optimize
because no one really cares about for
fifty percent case usually we talk about
the 99 or 99 comma 9 99 percenters right
so that's Forte latency something you
can google for afterwards so lesson
learned in this section use precise
language or at least define what you
mean by the words you say especially
when you talk about as allies and try to
get to an agreement with someone and
yeah verify the results and try to think
about the graphs don't only draw graphs
but also try to understand them okay
topic change again so like I said we're
going to jump in between topics quite a
lot so concurrent and lock free and
weight free algorithms so maybe going
from from the beginning you know what a
concurrent data structure is right
everybody should know that a lock free
data structure and the weight free yeah
the user were slowly going down with
hands which is fine so congruent data
structures are it most easily defined as
if multiple threads access for data
structure it doesn't blow up but a non
formal definition but very intuitive but
let's look at what can happen in such a
data structure so you have to fret or
multiple fat and they try to write a
value into some container so in this
guy's a tries to write B tries to write
B wins and again we have this loop of
people trying to write the value and
somehow somehow always were the other
guy wins so what does that mean so I
keeps retrying right these other guys
also write the value but they always win
so moral from the story is a is a
complete loser but more formally
speaking it may never make progress by
making
here we would define it as actually
setting the value or stopping to even
try write some form of progress another
thing that can happen in a control and
data structure which is also perfectly
fine is a choice to take a look the
other guys right for values and is still
waiting because it's waiting on the log
and no one is unlocking it so again same
result but slightly different actual
behavior because here we have the first
guy still waiting in the previous thing
he would be trying again and trying
again still concurrent data structures
still a valid concurrent data structure
but is still a complete loser and he
still may not make progress so this from
this definition will be interesting for
following algorithms but let's look at
which AP is actually exposed such
behavior so this is from the congruent Q
implementation in the JDK and it has a
bunch of methods which you can use to
put values into the q1 is offer and it
returns true or false and well it
returns true if it was able to put the
thing into the queue and it returns
force if it wasn't and when you can keep
retrying right that's the first case
when we have ad with frozen exception if
it failed to put stuff in vacuo again we
could do the first implementation based
on that a little bit less nice because
exceptions for control flow put okay and
then the third one which is no fun
because put a value and it would really
return a boolean which says through a
force depending on if it was able to put
but it actually blocks until it is able
to put in rephrasing that this method
will never return false because it will
keep waiting until it can return true
and in this method we don't have a
timeout so it will wait forever not a
really reactive thing to do because I
would maybe try to put into the queue at
most within a hundred milliseconds and
verse api's for that as well
but this one waits forever so no fun so
concurrency does not mean parallelism
and pluralism means when multiple people
keep you know working at the same time
and concurrency means we're very
structure that multiple people can try
to go to it but it will actually execute
sequentially step so slightly different
terms a concurrent data structure can be
a parallel data structure this way
around not V ever so lock free also now
also sometimes referred to lock lock
less right so here's a picture to
explain of what i mean by loculus and it
should play right so low class means
that you never really wait so you see
where our traffic signs traffic lights
but we don't really use them for
synchronization we go as fast as we can
through the traffic and we don't really
crash so that's still fine even the
people don't crash so great this is a
lock free data structure a better
example but I don't didn't have a nice
gift for it is if they would actually
crash but when they back out a little
bit and try to go through again and they
crash again so they keep trying and
crashing until they finally don't crash
and go through the traffic so formal
definition wise lock 3 means that if
runs efficiently long at least one of
the threads will make progress so if you
imagine that a traffic there invert
crossing it would guarantee that at
least one car at the given point in time
is able to cross and the other ones all
crash into each other so everybody backs
out and tries to cross again but at
least one will make the crossing
properly and the rest crashes and tries
again how does this look in code and
very simple so we have a for example
atomic reference and Here I am
implementing a cue that is implemented
on top of this atomic reference so I
just wants to add a value to the queue
here so what I do is
I take the frozen value of the queue
here i append to it and then this
operation took you compare and set
previously observed Q value and updated
Q value and the semantics are I compare
whatever is inside this atomic reference
with previously observed value and I
insert the new value inside what this
means now maybe i'll explain in bed with
a picture first this is actually
something that cpus implement so this is
actually an atomic operation on entered
ships and it goes all the way down to
the hardware but when we retry so if i
wasn't lucky and i wasn't able to set
the value but i wanted to i will retry
so I call myself recursively here and
yet randoms Carla information in Java
you would get a stack overflow exception
if you do that a lot of times but in
Scala you don't because this is
optimized away and actually compile it
down as a while loop so because Carl is
a functional language wears a lot of
recursive calls so this is an
optimization of the compiler does for
you so it's very nice to write these
things even if you're potentially
infinite but let's explain what happens
if multiple people write to the same
black box okay we have these two frets
and both at the same time observe the
value of the observable gray q they put
things out of and take things out of it
try to append to it and issue of a
compare and swap the other guy does the
same but because he was the letter and
to issue the comprehend that well he
doesn't make it but he can retry and
after foreach why he's successful and
everybody is happy and the state of the
queue is here on the right hand side one
right good it's a wire cast so wait
three data structures and so what's for
guaranty here the guarantee is that an
algorithm a wait for your girl written
guarantees fit after a bound number of
steps we will make progress
right so in the previous one we still
can get this infinitely retrying case
here we don't have infinitely retrying
case it may retry up to a given number
of times specifics wise the concurrent
links cue from the jdk is a very nice
implementation of that but as you can
see not that simple and basically if you
want to understand that one best read a
paper and i'm not doing a talk
specifically about that one and it would
be a topic about specifically that
algorithm for a one hour or even more
talk so it's more for your information
but lesson learned is first different
ways to implement it data structure and
you know sometimes you need to implement
one yourself but sometimes you're simply
picking one picking the right to right
and it's good to know that verse with
different ways to implement a data
structure and if you actually know that
you need a concurrent one well how
parallel should it be how how should it
be spinning and trying to do the right
or should it you know bail out as fast
as it can so good to know nice talk
about IO now so different topic again so
I o stands for not the google conference
that's also fun but not that one its
input output and then we have a i/o
which is the asynchronous input output
at least on linux and when we have an i
oh that's in java right we also have 0
so 0 is a little bit tricky here but
it's part of a I oh and but you'll see
next so that's a fun quote from my
colleague type-safe he was working with
us for quite some time havoc so he was
working on a lot of genome and desktop
stuff right and what people do every
have an even loop right so when he came
into Java and he was really surprised
like where the hell is my event loop
let's talk about it and actually
interruption so interruptions are
something that drives most of Cisco's
and
basically how computers work underneath
so this is a good example good place in
time to talk about them so we have these
two modes in the CPU do people know
about user space and kernel space right
everyone knows so I'm going to go fast
through that so the only thing you
should remember or maybe remind yourself
is that this is an actual cpu mode it
changes how the CPU is addressing memory
and going through what from one mode to
the other actually takes time I don't
have a very recent data but it's at
least hundreds of cycles even on new
CPUs and on a painting for it was
thousands which was in almost a
millisecond right well half a newer ones
I'm sure were someone who could give me
a number when I can update the slide but
it's still pretty painful why is it
painful so because when we try to read
something from a file what we actually
do is we tell the corner hey I want to
read something from a file so we do the
mode switch the syscall is done in the
corner space and then we go back into
users place my application gets for data
and then I can issue a right of the same
data this is if I would implement a very
naive copying algorithm a copy a file
through gone by going through the JVM
not optimal but let's think about this
here so we have four mode switches that
already accumulates to some wasted time
on remote switches but yeah it only gets
worse when you start thinking about the
buffer spaces so what what i mean by
buffer space is here is when the colonel
reads data it reads into a kernel buffer
and when it hands it out to the user to
my application it needs to cooperate and
copy it into a user space buffer so we
have one well one copy will useless copy
kind of already and then we have another
copy from user space to kernel space
because we are writing this maybe to TCP
or somewhere so we're using a lot of
copying here
and also we are blocking all these
operations while the colonel is working
in the colonel here in the corner space
here userland is doing nothing right we
call this I await so one improvement to
that is very synchronous i/o and yeah in
Java it's an i/o which no one calls new
anymore because it's in since 2004 but
if it was originally in a name but what
it exposes is a api's like that I want
to perform the read and I stay in my
user space Fred I can keep doing other
stuff and I will get notified once the
reed has been performed and i will get
this data synchronously so we're not
wasting time and use a note here just
once it's done I've got a notification
saying over rights but we're not really
changing the number of copying that is
happening in the background right
there's still the same number of buffers
in which the data goes through so let's
try to do something smarter instead of
working harder and this is what the
linux and corner people in general
figured as well and this is what we call
zero copy even though technically it's
not zero but you'll see that in a moment
and it's a kernel function called send
file because this is a virtual most
popular use case of this api so imagine
i am a web server and i'm a web server
of a disturbing static content cat
images right so i want to be really fast
at serving the cat images so i want to
avoid copying the data around because
it's wasteful and takes time so what
sent fire does well yeah it is a
blocking operation as you can see here
but it does not do any copying instead
of well it reads from the disk for
example and then it pipes directly to
the target without going through user
space right so this is really fast for
stuff where we don't need to inspect the
data right because user in space never
really gets the data
so it's fast because we avoid the
copying cool that was my first reaction
when I saw it this is amazing and this
is basically what all the superfast
static content file servers use right
basically everything is built around
sent file now you know so that's an
interesting topic to talk about as well
so we'll we'll a synchronous i/o beat
synchronous i/o because sometimes people
refer to a synchronous being infinitely
faster and I'm certainly one of those
guys who should be saying that but I'm
not because that's not always the case
and even rarely the case what a
synchronicity gives you is better
scalability and also sometimes
performance but that's not really the
focus and also disks are really good as
sequential read a disk is really good to
go to a sector and read the entire
sector right so synchronous i/o maybe
you actually want a synchronous i/o
because it's good at reading
asynchronous synchronously a sector and
just keep spinning verb ok another topic
so vassi 10k problem and now we are
jumping a little bit from the from files
and disks to network so who is familiar
with the seating hey problem not that
many ok so it's not that new actually
and sittin k stands for 10,000
concurrent connections and by not that
new I mean yeah 12 maybe 13 years ago or
something like that and the problem
initially was stated because of our
party httpd and problems with scaling it
to a very high number of concurrent
connections the problem still exists
with any kind of web server that is
using thread per request right because
where we are limited by the sheer amount
of how many threads we need to keep
running and then for scheduling overhead
and you know the multiplexing of these
fred just starts to be such a baggage
that we cannot scale to so many
concurrent connect
so like I said the problem was
originally stated because httpd using
the same model as a tomcat and servlet
containers which is thread per request
and having trouble to scale it to more
connections so important thing I I said
it a little bit I signal to the problem
a little bit already but this is
orthogonal to performance the problem is
strictly about being able to scale out
on with single notes to make it take
more connections it's not really talking
about any kind of performance here they
are related somewhat but you know we can
optimize for one or the other okay so
how do how were service implemented
mostly back in the day and maybe still
some implementations using that so verse
for select and poor methods and here
we're talking about selecting I have a
number of sockets so I have a number of
clients using my API for example my
server and they keep your connections
open and I need to figure out okay who
has data to read or write right so the
clients are sending me data but only
five of five thousand ones are sending
the data so i need to select which one i
will read from so this is this array and
then the colonel exposes methods like
okay poor and tell me which of these are
ready to be read from it's basically
what meaning here and it has a timeout
so if no one is ready to read within 100
milliseconds when we basically return
with an empty array here so we pour and
then use the problem in the scalability
of this part here so because the return
type is an array that we need to scan
and ask every of these connections well
do you have any events for me right so
we're doing the scanning manually here
the solution to that part of the city NK
problem was to instead and create a new
cisco when user school is called a poor
and sadly it's not a cross-platform so
Linux has a poor Windows has something
that I forgot and Max and have KQ KQ and
belief but they all do the same thing
the only problem being it's different
API so once you go direct to a pole and
you're not on the JVM you would need to
implement it three times so what it do
what it does is I create a flag which i
will use for reading in basically I
register that I'm watching for the
sockets and then the corner knows okay
you're interested in V socket and when I
do a similar thing as previously but I
pass in an array a pre-allocated awry
with a sealant right so remember good
old see days of passing in an array and
then the colonel what it does instead of
giving me back a array of all sockets
all the time it populates the one I gave
it with starting from exactly verso cuts
that have data available so I'm not
needlessly scanning for over sockets
which don't have data available right
with less scanning involved so that way
faster to select the socket and then we
do it again and again so what what this
teaches us is that Owen is a no-go for
epic scalability and what do I mean by
that I mean by radford things that are
really at the bottom of our above
operating systems and your random
application like any application if
something is really at the bottom of all
the course it should be fast because
it's worth to optimize for things that
everybody is using well everybody is
using scheduling because we use Fred
everybody is using socket selection
because we write HTTP servers so these
things in read
you need to be 001 to be really scalable
so it is a go for epic scalability and
the same lesson can be for our
applications right so let's talk about
distributed systems random topic change
again but what is a distributed system
actually my favorite definition by
Leslie lamp toad is it's a system in
which failure of a computer you didn't
even know existed can render your own
computer unusable and when you think
about it it sounds as a as a joke but
it's actually a wonderful former
definition because I can check that for
example I have a database server and
they have a web server and some customer
is coming to my web server to order a
thing database goes down those four
years and over they have a database yeah
and the ski Karen up but it rendered his
system unusable because he can't do
anything with the system now and also
the big of a system the more weird
behavior it exposes right I'm sure
you've seen that once you get more
machines at some point one of the
machines will get a bad disk or start
behaving weirdly so instead of hiding
this problem we need to embrace it so
let's talk about optimizing the long
tail agencies we know what for long
tailors and we'll be doing that by
issuing duplicated work so yeah I will
increase the load on my cluster trading
it off for better latency but response
times okay the technique goes like this
it's called backup requests and here i
have an actor that has an SLA are 300
milliseconds to give a reply to some
question it gets the question and it
knows verse free actors in the system
may be on different on different nodes
right which I can ask this question and
they should know the answer to it you
can imagine this being a distributed
cache something like that so and at the
bottom here is the timer of SLA and my
time spent doing stuff already so first
i sent the question to this one
guy because I know okay he has the data
and I'm asking for it and at the same
time I start the timer for a fraction of
my SLA so my soda is 300 so I start the
time before 100 milliseconds and once
the timer runs out and I didn't get a
reply yet I start panicking and running
around for office asking everyone else
to give me the answer right so we do
exactly the same in distributed systems
and we start panicking and asking for
other replicas to give me the data and
then turns out well actually the second
node was faster to give me the reply
than the first one why well maybe the
first one is being hammered by all the
other servers because everybody has
their it on well first try this note if
if it's a knave implementation and when
it's under heavy load all the time but
the second note is not really under vet
heavy load so it's faster to give us a
reply so and yeah in a naive
implementation we would get duplicated
replies back right I asked to people so
two people give me a reply independently
first ways to optimize right away so the
replicas would tell one another that
they have already replied but let's say
optimization but the interesting thing
here is we have both optimized stay
latency and we have optin were fixed for
the case when the up no upper node would
be actually down if it crashed right
because maybe we didn't yet realized
that the node is down so by using this
backup request we are actually also safe
against failure one more resilient so
where does first technique come from
it's actually coming from Jeff Dean's
blog and talk about the big table system
so also not very new but popular and
used in loads of database systems for so
Cassandra does first off by default
forgot the name how they call it but
it's owned by default and any kind of
cash very easy to implement
so do we have it Anaka here of course
it's a world we in our words you can
give a root or a number of actors and it
will do exactly that protocol so if it
doesn't get a reply very fast it starts
asking other people and yeah the thing I
wanted to highlight is for change in
actual latency distribution so the 99th
come on nine percent I without backup
request with the second with backup
request after 10 milliseconds we're down
to 250 already and you can see even if
we back out and make it after 50 we
still have improved to the 99th
percentile a lot right this is nothing
this is not a second this is still
hundreds of milliseconds range right so
is it worth it definitely so lessons
learned we increase load but get better
Layton sees okay but I had an example of
a preview of opposite so opposite is the
opposite is that i have a1 downstream
and in and i get a lot of questions and
instead of sending all questions through
one by one i keep accumulating into a
bigger question and I send one bigger
question downstream when i get 1 reply
back and from that one reply I can you
know fan out for response to the clients
so this is increasing latency because
verse first time where I collect the
data but it's decreasing load on the
downstream because it's getting less
questions per second and if it's
something like a search engine and maybe
the searchers are the same then it
doesn't really need to do anything more
because it's the same question right if
five people search for cats only need to
search for cats once and then reply to
all of them with the same answer sorry I
missed the slide for it so the next
topic further ilysm and pipelining
so this is something I must explained
using pancakes let me explain how I mean
that so when you make pancakes turns out
different people make pancakes very
differently and we had this discussion
in the team who does pancakes in what
style and I think was it Patrick I think
Patrick uses two frying pans no vets
Roland's technique he uses to point and
two frying pans but sequentially in the
manner of I take the scoop of butter I
put it on the first frying pan and once
one side of pancake is ready he flips it
over to the second frying pan right this
week or pipelining okay so this means
that i have this buffer of scoops of
butter i put them onto the frying pan as
fast as this frying pan allows me to and
I from it I have a separate buffer here
to make it even more detached rate
detached so if the second frying pan is
full with still a pancake where I can
give almost done cunt cake onto a plate
and it's waiting there and if the frying
pan is empty I take from the buffer from
the from the plate onto the frying pan
again what what this allows us to do is
basically detach the speeds of
processing on each frying pan and you
can imagine this being a stream
processing pipeline and one of the
stages being slow one of the stages
being fast by introducing buffers
between them they can and go at
independent speed because even if a
frying pan is for I can put on the
buffer and it will consume from the
buffer at its own speed okay and when we
get pancakes out which I have
wonderfully illustrated as these things
yeah so API wise and very simple
basically visitors arca streams but you
don't really need to know anything about
arca streams to understand this so it's
a flow from scope of x
to half-cooked pancake and that's how it
looks it maps better to a pancake then
we have a second frying pan which from
half-cooked pancake goes to real pancake
and then how do we connect this to HF so
it scoop of butter to a pancake is to
connect these two right so a flow of
scoop of butter and via first frying pan
via second frying pan which gives me
what I want and these work exactly as I
explained those buffers between them and
they can go at detached speed rating the
touch speeds but let's talk about
Patrick's method for doing pancakes so
turns out Patrick was really confused
why Roland is doing pancakes in this way
it's sub-optimal when Roland said well
because I have different temperatures on
the different frying pans and I liked it
better ok but Patrick's method is
optimized for speed because he has a
more than one child as a two or three
and he needs to really get these
pancakes outfit or really fast because
we're kids are waiting so the API is the
same it's a flow of scoop of butter to a
pancake but the implementation is a fan
out fan in operation so he takes a scoop
of butter puts it on the first available
frying pan and then bakes until
completion right on this one having to
turn the pancake once right but you can
see the two frying pans are in parallel
working on a complete process of making
the pancake so API wise I just want to
show but this is how it looks here so we
have a dispatch better it goes for a
frying pan and it merges the pancakes
because we have one outgoing stream of
pancake and not to outgoing streams of
pancakes yeah or simply you can use a
built-in operate at which does fat for
us anarcho streams that called map a
sink and if you have a frying pan that
returns a future of pancake you can use
that
but so we do have 11 minutes ten minutes
and I do have another example which I
can show you guys if you want to see how
buffering works end to end so from an
application layer to the actual TCP
buffers and that's a demo do you want to
see a demo yes I thought so so we're not
wrapping up but bonus streaming API so
let's imagine I have this stream
processing a framework toolkit which is
ARCA streams and acha streams is the
foundation for our HTTP it's our HTTP
both so over so you could think of it
you know it's a jetty it's a actual
implementation of HTTP server but at the
same time it's also the routing layer so
it's the way you root for requests to
whoever replies to them the interesting
bit about okay gdp and streams is that
it has back pressure built in and you
will see what back pressure is in in a
second because we focused on the
reactive streams protocol which we have
we as type safe and redhead and pivotal
we have created over the last year and
now it's up for inclusion in jdk nine so
you can check directive streams protocol
for that and the thing it does it allows
us to have as a synchronous system but
if the downstream is slower than the
upstream we are able to communicate for
that can't keep up and slow down
appropriately this is a non-trivial
thing to do in an asynchronous system
you need to get some information back
from the downstream to the upstream
right it kind of just guess how fast the
guy down stream is so now this is how it
looks underneath the reactive strange
photo course this is not really
something you were meant to implement
it's a very low level thing but a
library is like Eric's Java acha and
reactor they all sit on top of these
api's so we can interoperate with
different implementations using these
and this is jeff 266 which is up for
inclusion in jdk 9
soon I hope so you did see a little bit
of a cough streams already but let's
talk about buffers total buffers so we
have this source of tweets let's imagine
on Twitter on Twitter has with nice API
where you can stream all the tweets for
a given hashtag or all the tweets on the
planet and i am the server which does
wet and i'm streaming out for data and
eventually it lands on tcp because it's
using HTTP connection right and then the
operating system needs to put it on the
wire right so once we put it on the wire
it's out there in the wild and yeah tcp
takes care of retransmission and stuff
so the way this works the downstream is
signaling demand until it's while it's
still able to process data and we send
the data so we generate new tweet while
we can write in my example I have a
random tweet generator so I just invent
tweets as fast as I can and what I want
to show you guys here is that if the
client is using a cell phone and he goes
into a tunnel so he stops reading from
the connection right because he now he's
in a tunnel he doesn't have reception he
can't read from the TCP connection then
first what will happen is our send
buffer on the operating system level
will fill up then the intermediate mrs.
urn so this this line here means whether
JVM starts and this is a TCP
implementation that we did so when our
buffer will start getting full on the
TCP side and then the intermediate tweet
buffers will get full and then the sauce
will notice well okay if everybody has
full buffers there's no reason for me to
keep generating tweets and it will stop
doing that so using this information we
could can do very interesting stuff for
example started dropping data start
sampling data and you know suddenly back
pressure is a first class citizen in an
API it's that good enough I hope so
so without talking too much about the
API you can just read the docs let's
talk about this case so the way up HTTP
works here I start for HTTP server 19
and I give it roots and the roots are
tweet roots and tweet roots means i will
match anything that's on the path tweet
that is a get request and i will
complete this request using a source of
tweet and because we have the spray
jason support it handles the marshaling
of the tweet into a jason automatically
and it also in hand with stuff like if
you want to return it as an array we can
do that if you want to return the stream
of tweets new line separated so every
tweet JSON object is in a new line this
is how many api's work nowadays we can
do that and you just configure it so we
return this stream of tweets and that's
all we need to actually see for the demo
to be interesting enough I think okay so
on the right-hand side here I have Nets
that running using its that yeah so nuts
that is a tool that allows you to
inspect you know all kinds of
information about the networking
infrastructure in the operating system
and I'm going to run this over okay
server is online at localhost 18 okay
and first I'm going to explain what I'm
about to do so you can notice it because
some of the things will happen pretty
fast so I will curve a tweet API as will
start sending me tweets then I will
simulate going into a tunnel and we do
that by control seeing the curl app
right which means putting to sleep not
killing it because killing it what
curved connection
right I'm just putting it to sleep and
then on the right-hand side nets that
will see two connections open because
both the server and the client are on
the same host here so there will be two
connections right and we will see
buffers for both of them one for the i
can start that already so we see if in
tweets are streaming and the button here
this is always get folks wrong I'm not
sure which one is which I think that's
the receive buffer that's first send
buffer alright so here we have the send
buffer with colin and here we have the
receive buffer and once i stop this guy
first the send buffer of the server will
get full ok boom ok also first the
receive buffer of operating system of
the client running curl would start the
buffer which was this guy then the
buffer is for so the server gets
communicated with and he knows ok i
cannot send more data now so i will
start buffering so that's for send
buffer and then once that is for our
pipelines get back rashad and the server
stops generating data why is very
interesting because we don't do useless
work right why would I grind my cpu if a
guy can't read the data right so if I
resume it now you can see first it
drains its own buffer and then emitting
the tweet starts again ok so this is
pretty interesting for any kind of
streaming API or any kind of the
opposite works as well right if you have
clients uploading very big files PDFs
movies same thing we can back pressure
them to not go faster than we can read
the data so this is exciting because it
means bounded stream process pounded
memory stream processing for me that's
exciting because I can actually look at
the code and know how many clients I can
survive
will not be buffering until out of
memory right in some servers in this
situation the server would keep
buffering in memory and eventually blow
up here we have this information and we
don't even generate for data so we stay
within a known size of heap so that's
all I got last minute so i will show
some links if you want to look at some
links wrapping up the talk was mostly
about various techniques but you don't
often need to implement them yourself
it's very often to pick the right tool
so now you know how you can find the
right to a lot of links i want to
highlight two of them specifically from
c++ conf help sort of lock free
programming if you want to learn more
about it it's free or four hours of very
nicely explained lock free our
algorithms and then have a seat an m
problem which is interesting but not
very applicative for our work because
it's ten million connections on a single
host the talk is mostly about how one
has to not use the corner f one has to
go so fast I random thanks for many
people who helped me prepare his
presentation other talks around our
stack back to basics by victor in room
eight and without resilience nothing
else matters by yiannis as he t 0 and
founder at room five thank you very much
and i grabbed me for questions maybe in
just here thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>