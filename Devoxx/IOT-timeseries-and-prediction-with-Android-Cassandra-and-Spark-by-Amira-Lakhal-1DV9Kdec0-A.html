<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IOT, timeseries and prediction with Android, Cassandra and Spark by Amira Lakhal | Coder Coacher - Coaching Coders</title><meta content="IOT, timeseries and prediction with Android, Cassandra and Spark by Amira Lakhal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IOT, timeseries and prediction with Android, Cassandra and Spark by Amira Lakhal</b></h2><h5 class="post__date">2015-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1DV9Kdec0-A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well that's perfect thank you okay
today's talk is about IT time series and
prediction with on the read spark and
Cassandra but before starting I just
want to warn you you may hear some
awkward word during the presentation
don't worry it's only my friends that
came up sometimes so my name is Amira I
am in agile Java developer at vault-tec
I code in Scala in my free time too and
in spite of coding I'm also a runner
addict I started two years ago and my
challenge for this year is to do my
first marathon race at Paris so wish me
luck you can follow me on Twitter if
you'd like to hear some tech news and
all the code of this presentation will
be available on my github I'm also one
of the dishes uses false leaders uses
France is an association aiming to
promote female profile technical profile
to give them more visibility and also
create technical vocation among girls so
we always need support from the IT
community follow us on Twitter and have
a look on our blog IOT the Internet of
Things when I started doing my research
is about IOT I was wondering when all
this has begun and why when while doing
my researches I found an interesting
report of Cisco the report shows the
increasing number of population and the
meantime the increasing number of
connected devices and according to this
report IOT was born between 2008 and
2009 when the number of the connected
device exceeded the number of person on
earth that's amazing
today we are about 7.2 billion people on
earth and each one of you had at least
three connected devices and this number
is going to increase incredibly the next
years we may reach 50 billions of
connected devices in 2020 amazing how
so imagine this network of connected
device from wireless connection to the
internet from embedded system to the
micro electromechanical systems all
these contribute to the Internet of
Things and all this color system are
capturing a huge amount of data
worldwide social media users totals into
billions and each one are accumulating a
thousand even ten of thousands of
connection to content the way we can
even cite Facebook who has 20 terabytes
of photos loaded each month and the
bigger one is Google of course which
processing one petabytes of data it's 72
minutes so the need of speed of
analyzing all this huge amount of data
when the most def data are even
unstructured text based so it's really
really hard to analyze all this data
there's the challenge of doing it
we have exabytes many data bytes data
available now and for analysts an
estimate for 2020 we will have 40 data
bytes of data on the web
so what we're going to do to analyze the
data is first to extract the information
from the data then we will use the
extracted information to gain the
knowledge and after gaining this
knowledge we're going to enlarge our
wisdom so we're going to have billions
of machines products and things that
will merge from the physical world into
the digital world around really a neuro
time connectivity and analysis machine
and products in the future will have the
intelligence to deliver us the right
information and the right time to any
devices and when smart machine and
products start communicate they can help
us person and machine understand the
information so we can better make better
decision
react act very fast save time and many
money and improve product and services
yeah that's a good dream but we we are
really far far away from this there is a
longer road to take imagine if people
pepper instead of doing his welcoming
pitch at the keynote gives you your
favorite beer as a welcoming gift or
even better it can order your favorite
meal and stand off the crab sandwiches
at lunch time Stefan this message is for
you therefore I'm not going to go far
away in the future I'm just going to
share my favorite connected devices as I
stated in the beginning I'm addicted
Frank so when I started running I used a
lot of devices
first I used my smartphone as each one
of you I uploaded RunKeeper application
which allows me to measure a lot of
metrics my acceleration the calorie I
consumed the distance elevation etc but
it was really hard to do my running with
an armband the the phone keep moving
sometimes it fell down so last Christmas
Santa gave me a very great grateful very
precious gift
it was my Garmin watch my first Garmin
watch I use it since and the garmin
watch really helps me have my instantly
acceleration the time to rest after a
physical activity and a lot of vomit of
other metrics and even better I can
synchronize all the data from my watch
into my phone or the website directly
and recently I bought the willings
as little think little small devices
that you put in your pocket and these
things help you to calculate your to
compute your footsteps daily so to see
if you have spent a healthy day or not
in the mean time it it can compute all
the things like a sleeping time and so
on so all these connected devices has
encouraged me
to study the sensors are used in these
devices and therefore it inspired me to
the goal of today so today purpose will
try to predict physical activity using
measurements came in from a sensor in
our case it will be it will be the
accelerometers activity to predict our
walking jogging or even sitting this
case of user's case can be used by
doctors for example to do health care to
monitor their patients health status
their suffer from obesity or other
problems physical problems so the
accelerometers it's mentioned sensor it
helped us compute their proper
accelerations it's used or reduce many
of phone application to estimate your
arrival time at the destination and an
acceleration measurement contains four
information the time stamp and the three
forces according to the three axis the x
axis the y axis and the z axis the unit
is meter per second per second and the
accelerometer are making a lot of
measurements during time and all these
successive measurements collected
successfully and an interval of time are
called time series so what we're going
to do is to collect all these time
series and to save it somewhere so I've
implemented an Android application that
captures the the sensors information
posted into a rest server and then we
store it into a database the screen are
much simple so we are look to from the
right to the left the first welcoming
screen you have to put your URL your
server URL where you are going to post
your data then when you push the start
button to to collect the data you'll see
you'll have another push button to start
composing the data and then you can stop
at any time and you can see the data
updating in real-time so now as we know
how to collect our data from our
smartphone from
a sensor yes I didn't say it but all the
smartphone contains a lot of sensors and
the accelerometer is one basic sensor
that you can find in every smartphone so
now we need to store all the data and to
store the data I choose Cassandra as a
solution did anyone did anyone knows
Cassandra here oh great
anyone used Cassandra nice few people
not you care so let's have a quick
reminder cassandra has crew has been
created by facebook it's an open source
project since 2008 and the recent
version of Cassandra was released the
last Monday the 3.0 version Cassandra is
no SQL database Cullen oriented which
tends more towards distributed table
abstraction type Cassandra is scalable
masterless really really simple to
operate to get start with we have we can
have multi data centers with Cassandra
which contribute to have a continuous
availability so to write in Cassandra
Cassandra uses the last write win
principle so here what you are doing we
are what you are seeing is the secure
the Cassandra query language and here
with like only two inserts user data
login name and the age so what happens
that we create the SS table under our
disk we have a row with our Lardon and
two columns with the values and each
column will have an auto-generated
timestamp and if we'd like to update
this data for example we'd like to
update the age what happens that we are
we are going to have a new assistant
with the new value and the current will
have a new timestamp and then when we
try to delete this information like to
delete the age we will also have a new
in us s table with a gemstone which is
the mark that's indicated as the value
does not does no more exists so as you
all have seen if we'd like to select the
age what happens what we're going to do
is to get all this assessable to our
memory with all of them and get the
column with the latest timestamp in our
case it will be the last one which
indicates that the value does no more
exist as you have seen each time we do
an insert an update or delete we create
a SAS table on our disk so even a row in
Cassandra could contain two billions of
gallons we can reach this limitation
therefore there is an automatic
mechanism called compaction which runs
automatically reads all the assess table
create a new one copy the value of the
Cullens in this newest table and remove
the older one that's great but remember
we're trying to save our accelerometer
data which are produced each time and if
the compaction occurs we're going to
lose most of our information therefore
we're going to choose another another
way to utilize our data we're going to
use historical data so we're going to
use also time there is time series data
model so the first step is to identify
the properties that are not going to
change the user ID for example and the
time stamp then we create our table our
table here will be the accelerometers
which contains the accelerometers values
we'll have our user ID as a partition
key and the timestamp as a clustering
column so as you see we have a compound
primary key and then when we store the
data our logical view will be like this
we'll have for each line the user ID is
a timestamp and the different values of
our forces according to the three axes
and the only disk will have a row
identified by the user ID we have a cone
containing the timestamp values and sub
columns with the three axis values
that's nice but since I'm going to
capture all these accelerometers values
and these values are a curing H second
it's milli second hundred milliseconds
I'm posting my data so if we do some
computation I think we'll reach this
limitation of colors in in basically one
month and avoid reaching this limitation
of saving data in Cassandra we can add
another information for example we can
save data data daily so we are capturing
data each day each second etc so here
what I do is my partition key who will
contains two information instead of one
I'll have the user ID and the date and
then my logical view will be like this I
have my user ID the date of the saving
and then my timestamp and the values and
the most interesting thing of course is
the disk view disk layout I have a row
for each day so I'm sure I'm not going
to reach this limitation and I'm going
to store all my data as wanted then with
this type of modernisations we can do
range queries so each time I'd like to
get all the information to do the
analysis of the computations I'm going
to specify to specify my partition key
which is the user ID and the dates in my
case and then precise the interval of
getting to get all the columns
containing in this slice there is
another way to do it also if we like for
example we are capturing data from our
accelerometers and would like to read
the latest data first it's more
interesting if you would like to do
prediction so we can add a clustering
or the to our table and it's pretty much
important in my case so here I added a
class drink order to trust to order my
clustering : which is timestamp at the
decent order I'll get the first the
latest acceleration first and if you
don't need to save your data all the
time and you need to use it only for for
during a little time you can use the
time to leave option so you only have
the your accelerometer data to analyze
it in the few five or twenty second
comment seconds and then you don't need
it anymore so to free your database you
can use the time to leave option to do
it for example it can be used to detect
payment fraud you don't need to store
all the payment events you sort you
store to the data for a temporary time
you analyze it you didn't detect
anything then given the data will be
removed okay now we have seen how to
store a date on Cassandra and the next
step will be to analyze this data and to
analyze the data
I used spark so who knows spark
I think everyone know ok whoever used
spark nice so quick reminder to spark
was created by amp lab it's an open
source project since 2010 the current
version was released also last Monday
through 1.5.2 version and the spark is
totally written in Scala spark is a big
data processing framework it's a fast
cluster computing so it's built around
the speed the ease of use and the
sophisticated analytics it's supported
by data breaks if we have a look to the
spark ecosystem the main interesting
thing in the spark ecosystem is a spark
or a spark or is a
projects it provides in-memory computing
capabilities to deliver speed and
general generalized execution model to
support and also sparker can integral
can contain a lot of following language
you can find it API in Java and Scala in
Python and even in air and another other
of this than the spark core API there
are additional libraries that came above
spark with additional compute
capabilities there is a spark SQL which
compute structured data using data
frames spark streaming to stream data
and use historical data emily library to
do some machine learning algorithm and
graphics memory to build and transformer
and reason sorry about the graph
structured data at scale and spark
integrates it works with a lot of data
sources there is a lot of data sources
that came prepackaged it's prepackaged
with the spark such as hostage FS - Swan
and some other sources are for are
provided with the data source API so
developers can implement their own
solution to connect the data sources to
spark and we see one with Cassandra
SPARC is based on one famous thing the
oddity the resilient distributed
datasets RDD are a collection of object
that can be stored on a disk across a
cluster of course or in memory RTD can
be operated in parallel and they are
fault tolerant so what we do with our DD
is we get our data sources we do a lot
of transformation as needed we compute
all the transformations that we need and
then we can apply an action and have the
final result
so we'll have a little simple how to
work it spark it's the famous spark
simple how to do work count here I would
like to read data from a text file to
see how many times proven the word
occurs and then keep only words that
occurs more than three times the first
step is to initialize our sprite
configuration we specify our application
name and then the masters of our spark
here with it will run and cloacal with a
lot of executors as needed and then we
initiate our spark Java spark in text
I'm using I'm using the Java API and
after all we can apply our
transformation here we will it first the
file our text file we're going to split
all the text into words so we get an RDD
of words and then we apply a lot of
transformation to our words RDD first we
can we create a couple of words for each
words we can we're going to give one as
the first value then we reduce all the
value to compute the sum for if every
occurrence of words and after all we
apply filter to keep only typos
containing occurrence that is more than
three that's what we need at the end
after doing all the transformation we
can apply our action which is to collect
our data here i apply it collection
collecting which gives us a list of
tuples at the end so so sparks
runs on a cluster it runs as an
independent set of processes on a
cluster coordinated of course by the
spark context object and the spark
context can connect a lot various
cluster managers either in standalone
mode loco or missus or ER and then the
cluster manager will allocate needed
resources and we require executor node
to do two
all the computation defined in our
application code so spark is fast
flexible and easy to use I saw that a
few hands raised that of people that use
the spark
I'm a spark bedouins and I really
enjoyed starting with spark and - it
wasn't hard at all now what we need to
do is to do real-time analysis we're
going we are going to have time series
in real time so it will be interesting
to have communicated communication
between our spark in Cassandra to do a
real-time analysis so we're going to use
a specific custom connector to connect
spark to Cassandra
it's an open source project implemented
by data stacks the current version is
1.5 it's totally written in Scala and
what the connector does it loads data
from Cassandra into spark and it can
reach write the result from spark into
Cassandra so to review the connector
expose
Cassandra tables as RDD or de streams or
Sparky streams the spark configuration
won't really change we're going only to
add true properties to the spark
configuration in our case it will be our
central URL and the port to use and then
if you'd like to read the table from
Cassandra we're going to call our
connector API here we indicate our space
name key space name and our table which
we which are training and then after
getting our Cassandra ROS RDD we don't
apply queries to get very very specific
information here I'm going to have my
time stamp for each user and activity
album I'm going after all to map all the
rows I will get to Michael from
Cassandra into a map
to have all the values and then I will
only get the values of the timestamp and
catch it
it's the action but it's my cache but to
stream it I'm going to need spark
streaming which is another module of
spark spark streaming if you one of you
has followed the conference about spark
streaming yesterday I think it's a high
scalable throughput fault tolerant
stream processing it allows you to
create batches of streams to compute it
and then you have your result your
result process and here how we do it in
Java so instead of calling this java
spark context we're going to initiate a
java streaming context instead and we
are going to hit to give him also the
duration of our batches so here it's two
seconds it's two second we going to
stream data to compute or what we're
going to need and then to have the
result for each to second stream what
I'm going to need is to read the data
from Cassandra instantly in real time
and then compute my results so there is
no Cassandra receiver yet so implemented
my own Cassandra receiver which allows
me to get my results after computations
and after raw we have to apply our
action in my case it's pretty easy I
have only to print my my computation and
after all you're going to indicate to
our Java streaming context starts and
the termination condition which is here
when the computation are are finished if
we have a look to our central receiver
what I needed is quality to extend the
receiver spark class and to override on
start methods I have to implement my own
receive medals with all the computation
I have to do and all the instructions
and that's all pretty easy huh
so we see we have seen how to store it
on Cassandra how to stream it with spark
I have my custom Cassandra receiver what
we're going to do now is how to predict
our results remember we're going to
recognize our physical activity and the
possible physical activity our walking
jogging standing or sitting I'm going to
use the measurement came in from our
accelerometer our time series so here we
are facing a classification problem we
try to classify our time series into
physical activity classes we're going to
use machine learning problem is multi
classic multi-class classification we go
into level and unknown patterns using
known patterns there is a lot of argue
argue rhythm that exists the first one
logistic regression okay nice let's have
a look the logistic regressions do
binary solution success fear fail and
then we cannot apply this solution to
our case we're going to classify into
multiple classes working jogging so
there is another version of that of
logistic regression gold multi nominal
logistic regression which taken account
a lot of solutions and classes to
classify that's the problem with
logistic regression that's it's using it
doesn't support independence between
variables in our case all the values
that we're going to capture from our
sensor are independent we don't know
there is no link between the values so
then we have another solution which
called naive Bayes basis now you've paid
this classifier based on the three RM do
the use the independence assumption of
features so for example to give you just
a small example
you have a tiny object with a spheric
shape with a red color you can say okay
oh it's a nipple but naive basis is
outperformed by other approaches such as
random forest and that's why I'm going
to use a random forest random forest
operate by constructing a multitude of
decision trees at training time of
course so we have to build our decision
tree with the different features value
and then we have to use it to predict
our data while I'll use it the random
forests I test it I use the random
forest and a decision tree in the
meantime I computed my accuracy and then
I found that with random forest I had a
higher accuracy so we're going to use
Park em live to do all the computation
all the I do is move machine learning
here we are facing a supervised learning
we're going to train a model and then we
give having new features and we're going
to ask him to predict the act physical
activity so training means collecting
training data then we have to collect
elaborate data we have to under Phi the
feature the characteristic of our
information extract the information you
know gain the knowledge we have to
compute all these features and then we
have to create and train our model which
will be our random forest model and
after all we can do at our prediction
I'm just going to do special thanks to
all my friend and colleague that who
helped me collected my training data it
wasn't pretty easy to find volunteers
running during break times with the
phone in the pockets and so on so guys
thanks to collect the data
I created my training table containing
user ID
the activity because we know which
activity and we are going to do the
timestamp and the different forces
values and as you see here my partition
key is composed by the user ID and
activity so then I have my android phone
to I added a new activity to my android
phone to you have you see there is
another button with collect data on it
and then when you push the button you
click on the button you'll have a new
menu where you can write your name you
have drop lists select the activity to
do then you push the start button and
when you click on the start button the
sensor you will hear a sound you can try
it on all the sources are available a
meter you will hear sounds little bit
sounds that announces the posting of the
data and just it stay it's the last 20
seconds duration of the posting data and
then you hear another sound you know you
have to stop and after collecting all
the training data on my Cassandra
database I have to to compute the
features but before showing you the
feature I just want to be very very
clear there is no big data here I only
get a few data for my training model of
course we can do it but I have to found
a billion of volunteers to do it you can
do it guys huh okay
so I had my data I had my time series
saved on my Cassandra table the first
thing to do with my training data is to
prepare it and how to prepare it I have
to slice into Windows you know when you
save your the audio the mic the your
voice or singing the one when you see
the the shape of the of the graph it's
really tense so it's the same with the
time series you have to zoom in to see
the good shape of your of your graph and
therefore we're going to slice into a
tiny time interval I choose two sec
they're wrong and for its interval we're
going to identify the features extract
the information from this tiny interval
and then compute it here for example we
have two time series of two physical
actions you see one is sitting and the
other one is standing and just by
looking to the two graph you can
immediately identify some features look
at the blue line when you're sitting the
blue line is below the green and the red
one and when you're standing it's a blue
line comes above the the two other
graphs so here we have one feature one
simple feature okay I can compute the
difference when I'm sitting I know that
see Y graph will be below and when I'm
standing it's the the it will be above
okay that's the first feature very easy
huh more complicated one if you are
walking or jogging here we see the
frequency of our measurement when you
are walking you have an amplitude very
slow amplitude and when you're drawing
you have a lot of bits so here you can
say okay you can we can measure our
amplitude peak to peak so we can see the
difference if I am walking or jogging
that's a feature well that's not enough
we have to guess a lot of other features
don't worry everything exists on the
Internet all the studies have been done
or the statistical statistical studies
so just by using Google you can find a
lot of other features and that what are
used we can measure the average
acceleration the difference as I show
you for sitting and standing the
variance the standard deviation the
average absolute difference the
resultant acceleration etc and for of
course the big topic of amplitude for
the y axis for example
so how it works and with the ml deep
library first what I have to do is to
get all my acceleration data here I get
an RDD of an array double array
containing the x y&amp;amp;z values I have to
transform it into a vector because the
ML deep library works with vectors
statistic yeah and then you have to call
the multivariate statistical summary
which contains a lot of computation
already defined like mean variance you
don't have to do anything a lot of
methods already exists and if you like
to do some specific computation for your
feature you can do it by reusing some
methods from this class okay we compute
all our features and then we go to
insert all the future into a vector and
create a labelled point the label point
is the is the main important thing about
decision tree model or random forest
model all these training model are based
on labelled point to train them liable
more appointed means that we're going to
insert value with a lab a list with the
precisely a label and a vector with a
lot of features already computed then
we're going to create our decision tree
for example to compare between decision
tree and renew forest so the first thing
is to initialize all the parameters you
can you're not it's not necessary to
need to modify all the default values
here I used almost all default values
only for the max death I choose 20 depth
to navigate on my decision tree and then
we have only ticket to call the decision
tree class with the training methods and
the training methods will initiate
a decision tree model the training data
so I didn't precise this to train our
model we have to get all the training
data that's what it suggests to split
the data into training data and test
data so I would take 60% of the data to
the training data to the training set
and for testing will keep the 40% here I
created my model and then I saved it to
reuse it for prediction of course and
after all I did some computation to see
the accuracy of my model by testing my
using my test data of course
okay and for the rest random forest
there is no difference no big difference
we have only some parameters more some
parameters for example the number of
three we're going to use here I'm going
to use 10 trees in my random forest and
the subset strategy - I keep that I kept
the automatic strategy and you have to
choose a value to do random seed such I
put it a default value without without
really thinking about it and then after
all we computed all the values with the
testing data so to remember I had 30,000
of 13,000 of training acceleration 50
features at all we count all the feature
among the three axes I get 26 labelled
point to my mother and then I computed
the accuracy so for the decision tree I
currency I got 69% but for the random
forest a currency I had 92.3% and that's
why I used the random forest okay so now
let's see all these words
I'm going to share my screen yep
oops here we go
so first of all I'll show you my time
it's really tight my REST API iSpring
boot just to initialize a small server
to post-dated on it from directly from
my android phone
it's really really really basic so I
have to start it added the login
information here
I don't know if you've seen everything
and then I'll show you my under which
one too so just going to get my network
so the camera has to zoom in two seconds
I'll show you
yep
okay here we go so here as you see I
started my accelerometer lab I only set
my URL to my server and then after I'm
going to click on the start button
but before clicking to the start button
I have to start my my spark so the first
thing to do I have to start the
application that I'm going to show you
the results that'll be the latest result
yes that was for yesterday prediction
game tweet be refreshed every two
seconds over three seconds I think so
you have to move to the spark part up
and the here it is so I have my
prediction service when I initiate my
spark configuration here I I indicated
my Cassandra URL which is local to all
of the things through a random on logo
and then I'm going to call the
protection with Ray Alta real time which
is the era and then you'll see I I
called my Cassandra receiver and then
I'm going to print the result after the
streaming if we go to CIA Cassandra
receiver here I just initiate initiate
my random forests I load my on time for
a random forest model I get all the data
from Cassandra I do the computation and
then I try to save to store my
prediction result I could show you
little small of the computation I did
mmm
there's a lot of computation to do so
here you see all my features I'm going
to zoom a little bit yep the mean the
variance the standard deviation the
average absolute difference etcetera
etcetera and then I get my array of
features for each axis is for the x y&amp;amp;z
that's a 1 and I
I created my victors which is used to
create my labeled point and even to
predict so here we go we're going to
start our prediction service yep I put
two screen together so we see as you see
here streaming has been started show so
it's it's already been streaming the
latest data from Cassandra and now I'm
going to push the start button yeah yep
and now here you start seeing the
accelerometers real-time data and then
we're going to have our prediction of
course yeah I'll be standing and I'm not
joking standing yeah great and now if
I'm sitting so the phone will be like
here if I'm sitting walking no sitting
great and I'll be walking walking how
nice and if I do some jogging some
morning jogging here we go no no jogging
no it doesn't mean us yeah
okay that's what's fun huh of all these
stuffs okay so in conclusion what we
have seen is how to collect data from a
device connected device which is a
smartphone here and now it gates and
analyze it we have a lot of
possibilities I use only the
accelerometers but we can combine a lot
of sensors there is a gyroscopes and
sorceries counts which captures the
rotation movement so for example if you
like to develop an application for start
dancing for example and then you can
have the great effective steps of
dancing for salsa tango and so and so on
by combining acceleration sensor and the
gyroscope so there is a lot of
possibilities with IO T's and this
presentation it's only small simple to
show you how to do it and to show you
the easiest way to do it is of course
and to inspire you do your own IOT
connected devices so thank you
I think we have a lot of time for
questions we have 13 minutes left so any
questions or if you'd like to see the
demo running after all I can I can show
you two after the presentation of course
yeah yes so the question is did you paid
with time to get the batch like batch
interval seconds yes I did it it's a
tricky way with prediction it's a very
very sensitive I don't know how to say
it it's very very sensitive parameters
so I tried with one second with two
second with five second and doing all
these testers I tested test I found that
two seconds is the appropriate duration
to do in my prediction yes it's a very
personal choice it depends on your data
so you have to try various values other
questions I think no question this
morning
yes I was waiting for this question
thank you so the question was I'm will I
was doing all this stuff as sinkers Lee
I collect the data store to Cassandra
and then I'm going to do a start my
streaming after all and yes of course
the best way to do it is to do it in a
synchronous time so I started at that
will be my next presentation maybe I
talked about using Kafka so we can post
data directly into Kafka and inspire
instead of storing the data into
Cassandra and then we can analyze the
data directly and and compute all their
features yeah
so the window the question was if and
there's all of it I choose to think ins
as a period to slice my training data
and to create my windows to compute all
the features after all and then how to
use all this information first for this
part for the training data it's really
personal choice you can choose the
window time you like and it's only used
for training step to create the truth of
the labelled point for our training
model after all for the prediction I
didn't use the same thing for the
prediction I get I didn't show you maybe
I get only the latest rose store saved
in Cassandra mmm let's see here I put
the query so honest I don't know if you
can see it here but I get the the latest
latest data acceleration total here I
get only this one hundred of latest
acceleration saved into Cassandra to do
my prediction so I didn't use the window
saying L so on I only get the last saved
data and analyzed for the protection
yep other questions
so I think I stop here thank you thank
you for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>