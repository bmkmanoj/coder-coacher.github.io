<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What can we learn from 750 billion GitHub events and 42 TB of code by Felipe Hoffa | Coder Coacher - Coaching Coders</title><meta content="What can we learn from 750 billion GitHub events and 42 TB of code by Felipe Hoffa - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What can we learn from 750 billion GitHub events and 42 TB of code by Felipe Hoffa</b></h2><h5 class="post__date">2017-06-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-VLlMl5zK3k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you congratulations to you for
laughing until the last talk of the day
there will be drinks after this so let's
get started let me introduce myself
first my name is Teddy B Hoffa I work at
Google I've been working at for at
Google for six years this week so happy
Anniversary to me thank you I started as
a software engineer six years ago and
then four years ago I changed my role to
be a developer advocate a developer
advocate is mainly a software engineer
that speaks so now I travel the world I
go to conferences I'm on videos on
social media follow me on twitter
twitter etcetera etcetera and I like
sharing the technology that we have and
the things that you can do and today I'm
going to talk about how to analyze
github how to analyze billions of events
terabytes of code there's a lot of
interesting things you can do with this
any open source man maintainer here yes
you have your own project that's awesome
what kind of project Cassandra schema
migration that sounds pretty cool I'm
going to find it maybe later but if
anyone has any question and wants to
interact and wants to run any separate
analyses please raise your hand or just
shout it and we can do any everything
interactively even analyze your own
projects or your favorite project and so
when I show you this what do you see
here get happy okay what else do you see
hmm
code yes we understand code what else a
license yes so yes so there's a lot of
things we can start finding here it
first it's code but if we zoom in we can
find a license we can find the modules
that have been imported things from the
future
when was this file copyrighted etc etc
and then if we start looking around we
also have the number of stars the number
of Forks how many people have
contributed to this code what kind of
things have they done so that they
retreated so what we have here is a beta
and it's in big letters because it's big
data there's a lot of data that we can
analyze so who wants to analyze github
hopefully everyone here but you have to
give some examples project maintainers
if you have your own project you want to
know how popular your project is you
want to measure your popularity compared
to everything else but it's not only
about how popular it is but who is
following it how they will become became
aware of your project and you want to
mine exchange if you want to offer a new
API if you want to change things you
might want to know who are you helping
or what are you breaking or if your
project is Kelsey at the issues close of
time it's a community participating at
iterative method and we can measure
those things that's for a prime
maintainer but then also if you are a
project user you want to know that kind
of stuff and you want to if you want to
ask for new features if you want to ask
for changes you want to add data to back
whatever you are asking and sometimes
you're looking for related projects we
are going to look at the kind of thing
too and then even more important when
you want to choose a project and if you
want to choose one of the thousand
JavaScript libraries that align around
you may want to also ask these kinds of
questions
is it very popular it's a project Kelsey
is a popularity growing
how is adoption going what related
projects I should be looking at etcetera
etcetera
and if you love data this is a perfect
data set to come explore understand
what's happening around compile this
data for other people you could there's
a there's a lot of stuff that
that Texas eita run their own analysis
rankings etc you're a security
researcher if you want to see what
things people are doing right or wrong
you can do this at large-scale
investigate any language and if you just
love data like me you can go and play
with this we're going to look at three
main data sets today that all of you
have access to right now
one is the github archive this is a
collection of all events on github or
all of the public events of github for
the last few years so far or the last
time I counted it was April 7 billion
events that updates every hour every
hour we can see at everything that gives
me happening github even prevent a star
a fork a commit etcetera and and that's
give have archived and recently became
aware and started using this other
project the edge Turin the edge torrent
allows you it takes basically the same
data but goes further and annotated and
add more tables with more metadata so
you can traverse the graph of metadata
each project its user offers and then we
also have a full copy of most files the
open source files that live in github so
we can also analyze it and I will show
you how here
but let's go back in time how all of
these would have started there was a
novel developer advocate back in 2012
mr. ilya grigorik that started
downloading all of this event from
github and he would give he was offering
this event he started collecting them
and then he wanted to share what all
this data that he had collected with the
world and to start off what will how
would anyone do this you would leave the
hourly files around
a activity for each hour it's pretty
easy to get if you has a gzip file and
one hour of data compressed data is
approximately five megabytes you can
download that is less than a second you
can uncompress it you can see that each
hour has around 40 megabytes of real
data when you uncompress it 18,000 line
16,000 events per hour and that's pretty
manageable until you want to analyze for
month a whole year seven years of data
if we multiply this file by 365 days ot
for hours and put everything together
now we're talking about 2.30 nine
terabytes or 1.1 billion events so when
you want to share data type scale you
have to look at a different way of doing
things and that's was back in 2012 when
we were also opening up Google bigquery
who knows the query here perfect so this
side knows the query this side doesn't
hopefully at the end of the stock
everyone knows I will be query works and
how you can use it so quickly you have
to put everything on the same page
everyone on the same page a the queries
are cloud analytical database it's fast
it analyzes terabytes in seconds you
just need to know sequel to use it it
scales from bikes to petabyte it doesn't
matter how much data you have you can
just put it there and you can analyze it
and it's always on there's nothing to
turn on there are no server hours
there's no RAM is just bigquery there
where you can put your data and analyze
it you can integrate it with any other
tools or tableau are Python wherever you
want lucas cetera and something that is
really nice is that you can share data
so you can put all of your private data
here
but if you want to share it with someone
else you can and everyone has a free
monthly quota a three terabyte every
month to query either your own data or
whether other people have shared with
you so what I'm going to do right now
you can repeat it on your own computers
you just need to create an account with
Google you don't with Google cloud you
don't need a credit card you will have
access to all this data that is shared
so if you don't believe what I'm going
to show you you can do it on your own
and just test it out and find me get
your own findings so to start with this
let's look at the Stars and github stars
I'm sure you've seen rankings of
popularity which projects have more
staff maybe you have star projects
yourself so if we go to github to any
project let's say something with
Cassandra just because that was the
first example we got and if I'm login
yes I can start this project now
Cassandra has one more stuff and I can
remove this star and I can start it
again and I can remove and proper pop up
and every time I start a project this
creates an event and I have a table with
all of these shares this is the query
the web UI it also has a REST API but
the web UI helps a lot with to rank
these fast so get have archived I have
my face I have 2000 days of date I also
have the data by mass from 2011 to 2013
May June etc so we want to see the
number of stars what are the projects
that got the most stars let me switch
microphones if you want to see the
projects that got the most stars we just
go to my work table mmm this table has
this columns the type of event is it's a
palette
project payload the rapidly Papa Papa
Papa so in May we this table is got 82
gigabytes of data 33 million events and
I know that the stars are the ones that
are tied watch event and I can get the
repo name here I can count the number of
stars and I can group by the first
column order by the second one in
descending order so what are the top ten
projects that got the most number of
stars in a boom so the first thing that
you can notice here is that it analyzed
one month of data really fast 3.8
seconds
to analyze 1.2 gigabytes and the project
that got the most tasks was free code
camp face followed by facial prepass jet
plane scotland it's pretty famous now
that andrew is accepted as one of its
official languages so it makes sense
that this very got a lot of stars
last man 2 to 2 and you can continue
looking at this and now we're counting
stars I told you that to analyze all of
this project was pretty fast but let's
say you want to analyze all of 2007
let's say all of 2016 you have to make
things more interesting if you took two
seconds to do one month how long will it
take to do twelve months nine seconds
not bad now we process 11 gigabytes and
in all of 2016 these were the most
interesting projects and now we're
counting now the first thing you have
when you want to do this type of thing
you have to be aware of is if you come
back here to see Apache Cassandra and
stuff I get it every time you do this
I'm create generating a new user
so if people start counting stars like I
just did
there's a lot of fake stars there so we
need to do things in a more interesting
way we need to count distinct actor ID
and now we're counting the number of
stars by but only by the number of
unique users let's count these are the
real stars and things start looking
different in this case so these are
things you have to be aware of when you
are analyzing a lot of data and still
things should work pretty fast if the
queries capable of doing the
deduplication first so yes so free code
card that had received a hundred eighty
five thousand stars last year really got
a hundred seventy four thousand and some
projects go up down a lot like this one
got two thousand less stars saturated if
you see has to get you started here any
questions so far before we go deeper
into the stands okay so this as we saw
this at the top projects a disciplic a
to the stars but things get more
interesting when you start thinking that
not all stars are equal it's not only
about how many stars
everyone gets because each star is given
by someone and whoever gives you a star
has a different background sometimes you
may be getting stars by a new these
people that are new to program sometimes
you are getting starts by people that
have been programming for many years and
some people are experts in Java some
people are experts in Big Data so if you
just if you're just counting stars you
are losing a lot of additional data that
might be interesting you may to give you
an example here it's last year free code
camp what a hundred
ninety-two thousand stars and it got
more stuff than any other project like
it's an order of magnitude more and then
in terms of flow what twenty four
thousand stars and there were three
thousand people that start both projects
but then I'm I want to see deeper things
like let me see if I have my quote here
I can start asking people questions like
so he the query had to compare any two
projects and in this case I'm not only
country the number starts but the age of
each user how I don't know the news of
late but I know how long they've been
around each hub how many repositories
each one has words how many comments
they have left to papoom company pull
request to sister cetera I can start
looking at all of these dimensions and I
can compare this project not only by the
number of stars that a turns out free
code camp the age in github is how long
they had been on github is one year
versus the people starting tensorflow
had been around for at least two in
average people that start terms of flow
has start AP price instead of twelve and
they have laced 2.4 comments in
proposing news verses in Philippine five
for the other play so you can see that
the nature of stars is assured
so the next question I wanted to ask
here was what are the top projects by
stars from people that have experience
in github so the question I have here is
okay so what happens if I look for
people that have left comments so now if
I look for the stars only by people that
have left more than 20 comments on
github a free code camp
numbers go way down we go down from a
hand
70,000 starts - only 3000 which is much
closer to the number of stars that
tensorflow got and now with your
decision depending on what you are
looking for if you want to see users
that do you care about users that are
new to give that or do you care about
users with experience in this case it's
pretty good for both terms of flow it's
a nice place for people with a lot of
experience and then freaking camp is a
project for people that want to learn
how to code so if we compare the
rankings the top star prior to last year
versus the top style of year only by
people with a lot of experience the top
price changes with for people with
experience the top price was young and
followed by would have University and
Apollo 11 which goes a lot but I get a
lot of physicians ranking and terms of
flow which we are very proud of drops
from the temple because they are private
what more stars my bit with experience
so the message here is always look at
those dimensions look at what kind of
data you care of and now if you when a
price has stars let's say Cassandra or
whatever other player you can start
looking at okay people that start my
project what else did they start so in
this case are we doing on time and in
this case we disclaim looking at okay
what to find people that style texture
flow and I want to see what other prize
they start and what's fascinating here
is that it quickly gives me all of the
other machine learning projects on
github get models from terms of flow I
get Cafe Kara's University secret lair
Microsoft here TK so just I look it and
related projects but just by looking at
willing to start and start getting a lot
more dimensions and interesting things I
could be looking at and then when people
start looking at stuff it turns out you
don't get a lot the same number of stars
every day
since lactate is part one day you place
has a hundred stars on the next day has
two thousand and it basically discovers
because people get very interested about
your prey because they read it on Hacker
News or any other social website or
there were news so for example I made
this discharge of the number of stars of
this case replay it you can see that
they get a lot of stars on one day or in
three different days but the rate is
very sporadic and the annotations the
little notation that you can see there
is every time the project was featured
on the hacker news front page so
basically you get on the hacker news
front page you will get a lot of
attention and a lot of stuff that they
now the most interesting for me here is
that I didn't create these annotations
manually but I was able to do this with
the query it doesn't require I not only
have all of these data github data set I
also have all of the hacker news
comments I have I stole a lot of read 3
billion reddit comments to that you have
access so with a simple joint like this
if this query basically looks at all of
the hacker news stories which ones are
pointing to a github project and then
looking at the now what are the papers
got the most stars and I can create a
chart that shows me yes B no hacker news
and you know how can use these days
gives you way more attention that you
were expect and you are you will be able
to run this analysis cross-site cross
with Dallas's let's talk a little bit
about right health repair with most
issues people feeling issued cigarette
data and this way the price has got the
most issues a comments on issues early
2016 on top Cuban Aries with 17,000
issues Mauritius than any other price
followed by spark followed by openshift
followed by Sauron demo I don't know
what that project is you might know the
other ones but and this again is when
you need to start looking comforting not
just doing an 8-count
but looking at what's happening here so
what happens with Sauron daemon is that
it's a test project where one person is
creating all of these comments so I want
to see things go deeper and with the
query that I put here I'm not only
looking at the number of comments I'm
also looking at how many different
people commented on each project so this
instead of just looking at the number of
comments I'm also very interested in
having a large community with a lot of
people commenting and then I'm also
doing and I'm dividing those the number
of comments divided by the number of
people and that gives me the number of
comments per author because as we can
see this ranking kubernetes had 500
results where each one left
about 18 comments that's a pretty
healthy community this is a lot of
people coming back and a lot of people
writing and collaborating where I'll add
a product that get a lot of comments ooh
like font awesome GfK less than two
comments per person this is yes a lot of
people coming writing something leading
and not coming back and Google we are
really proud to have a the top two
projects it is ranking in community
participation so that shows you the
communities are alive the community are
working and you should you can expect a
lot from the top right here and just to
make sure in this query also removed M
everyone that is looks like a robot like
people that last more than a thousand
comments during a month those are
removed so my averages don't go up just
because someone is writing less the
robot
a lot of comments of the average would
go up and we can even start doing text
analysis we can go beyond numbers so for
example in this case I'm looking at how
people start an issue what at the top
ways to start an issue how do you make a
request on github so and it turns out
most people the the most popular way to
start an issue is it would be nice
followed by is it possible to or I am
trying to and those are not only the
most popular ways things to do you can
start looking deeper and you can start
looking at things like it is working so
in the third column there I'm looking at
how many issues got close and when you
start an issue with it would be nice you
get 56 percent closure and but instead
of writing if it possible to get your
way better percentage we were it's a
much better way to ask things and the
best way I have here two violent issues
to start with I get the following being
precise I get the following and
expecting something else you have tell
all the very maintenance what you want
and you will get closer
well things the window has released
closer is saying something like so that
the humans which again is another test
rate that does that but and you would
need to count the number of different
user the number of different products
that are doing this and this is a lot of
people writing comments on the same
project that creates the third click but
try to be very precise try to define
what you want and use the best language
to get what you want and this is a very
interesting thing when people hear stars
when people do things on github many
times you put in your profile
what's your own country where do you
live and that allows us to
to investigate how every country is
different so in this case I'm taking the
data share by DS torrent the data set
that looks at the metadata along github
this is so they are looking at every
country that HP to put and I counted the
number of countries and of course the
United States is a top country they're
followed by India followed by China
followed by the UK Deutschland Alama
head there are many the field so this is
interesting and a little bit expected
those are the larger countries only cap
but then you can start looking by
activity number of pushes which is also
gives you a similar number but way more
interesting is to look at divided by
population what are the top countries
where do you get the biggest
concentration of developers so you can
see that the north of Europe has a lot
more if is what is on top of the ranking
of having colors by per capita and this
is the query that I wrote let me see if
I have the query here - basically what
I'm doing here is and with these queries
this is not standard sequel and looking
at push events people are pushing code
and I'm joining with this table off as
all of the countries so I have the
population for this country with a
simple join I can see this Iceland 159
users which divided by population given
the most name the biggest number of
coders per capita followed by New
Zealand's will fit Sal and Norway etc
let's see if we can find Singapore but I
started doing this chart this morning
improving them and I notice that I
didn't have New Zealand in my country
stable so I have to add it down so you
will see the
that no one else has seen at least by my
query what we can see here is that and
if we look at this ranking is that the
coldest countries are the ones that get
the most developers or capita so you
might want to measure that where do cows
earth go do they prefer cold places or
hot places what do you think call it
well so that's what we just saw now we
all have you happen to be colors and you
happen to be in a very hot place so
let's try to quantify that what if we
got all of the weather for the world in
one place and we have it
so I ran this query this morning this is
here I'm looking at the average
temperature for each station for each
country in the world so the size of the
bar basically shows you all of the
variety of weather that each country
have and I'm sorting it by the average
weather and it turns out that Singapore
is different it's a really hot place
there are places I get hotter weather
than Singapore but in average the three
weather stations I have for Singapore
show that this place is really really
hot and you have no not much variety of
places to to go and then if I take this
data if I take the average weather for
each country so each country is pretty
big each country has a variety of
weather but still they average seems if
I were average all of their stations at
least it shows me something that makes
sense I can get a chart like this let me
show you the point because there's
nothing better than showing you real
queries running this is real - this is
an open source project that is also
really good to save your queries
visualize them etc
and so what I did here I got all of my
stays on my weather for 2015 my stations
and looking for my countries my
countries I'm grouping the average
weather pair per country and I want all
the countries that have more than ten
stations or I did this manually
Singapore Singapore has only three by
still wanted to produce it on this chart
and I'm joining with with my number of
users per capita and once I want this
join four countries with a population of
more than 300 of the people I get this
chart and what you can see here this is
to the left things are here things are
colder since I hotels there and you can
see a line that shows you the average
weather for hotter weather you get less
developers but what's also very
interesting here is to see what are the
appliers so we have here Finland Canada
this has the top countries in number of
coders and this number of colors we have
here is mm-hmm Senegal needle malli but
then you have the higher places that are
really cold like Pakistan North Korea
catechist and Russia and we have our
advice from the tops like if you like
hot places and you want to find a lot of
programmers you can go to Australia you
can go to Brazil here and here on the
right top right the hottest place with a
huge amount of developers capitals
Singapore so yes if you like hot weather
and you want to find a lot of
programmers you are in the right place
due to the very interesting thing I
found here is that if you color these
things by continent this is basically
you
roots on top with the best cold the
weather and then you have Africa very
hard with less number of coders and then
you look only at Asia turns out you have
the an inverse relationship like the
heart of things get the more colors you
get followed Singapore is followed by
the United Arab Emirates which is also
pretty hot and also has a huge
concentration of colors and then you
have South Korea and Japan so hopefully
you can tell me more about this if you
know more I love running this kind of
analysis and then you can also find the
top projects and what each country is
starving each other but I want to show
you got the other part of systems where
we look at the code how we can analyze a
lot of code with sequence so the first
thing we got here is that we announced
last year that we were able to set up a
pipeline that every week is copying all
of github contents at least the open
source code into bigquery
so you can go and analyze this and let
me show you the real table we have our
open data pros data set here bigquery
public data keep repositories content so
in this table today we have one point
ninety four terabytes of code 232
million files the schema basically the
file with this ID has this size these
are its contents and the number of
copies so in this table what we did
instead of having duplicate files which
there are a lot of duplicate files on
github we store it each one only once
and then you can do a join with the
files table
to find out what are the names what are
extensions of its file so to see exactly
how much data we have you can do a
select the size of each file by the
number of copies and this tells us
exactly how much data I might call this
represents I want the thumb how much
data we have stole here a pop up on this
matrix so since by it's a terabyte
Google can compute faster than me this
is basically we have 49 terabytes of
code that you can start analyzing at any
time and doing interesting things with
it like putting things you have to
remember when you are analyzing this out
of it before doing the demos you will
find only the contents for text files or
it because it will make sense to copy
binary files each file we have one
unique copy as I showed you you can join
it with the repository with the files
table to see all the path but try not to
do I joined to just to get everything
because you will end up with 49
terabytes of code and that's not what
you want you want to analyze you don't
want 39 teletypes code like there so if
you are going to run an analysis the
first thing you should do is just
extract from this table all of the files
you want extract all of the Java files
extract all of the PHP files
I left you some of these tables but
start by extracting and then running the
analysis world for example if a table
with 10% of contents of the top project
with one stop two parts of five to get
you started so you don't run out of this
one three terabyte you have every month
it's only open source audience you if
you want your product to be here make
sure it has a license that it have knows
that it's open source
and sometimes I'm missing because github
cannot tell that the license is one of
the approved licenses so things that
we've done with this item for example if
you want to analyze Java which is a in
what I did here is with this query and
just looking at every line of code
within each other
so I started all of the code from Java
from 2013 I can split each line now I'm
analyzing code itself and then for each
line I'm only keeping the lines that
start with report and now I'm counting
the number and getting the percentage of
five that style with this import the
percentage of imported pallidus and
where they can see here is not only what
were the top imports for each year I can
see what were the top growth
what has biggg between 2013 and 2015 and
South the project most the imported most
had the most grocery representation was
injection and the Google common in the
released some of our classes nullable so
you can see how the language evolves I
have 50 minutes left so we are doing
well and also if you can analyze code
you can extract for example all of the
URLs that are pointing to Stack Overflow
and we also have Stack Overflow on the
query so that means that you can see
what are the top questions
tiddler pointing to with these queries
I'm doing a regular expression extract
of anything that points to Stack
Overflow questions from in this case all
of the JavaScript files and that shows
me okay this has the top questions that
people are linking from github code to
Stack Overflow and what's a question
when we're talking about your sleep the
top question asks what is if there is a
regular expression escape response in
JavaScript how to encode decode basics
before deep you want to detect number as
a decimal X decimal to densely tapa tapa
top
so for any language you can go and see
okay this is what the black coffee and
the very reductionist en boxes released
two weeks ago is he was clueless about
how many people go to stack overflow
look at the problem they have copied
according to the base because that's
basically what we are all doing when we
work but yes but that has some legal
problems because this code has special
or specific license you need to lice
encircles a certain way so it's not good
but we can measure how people do it so
for example if you went found one of the
most popular general questions which is
how to convert by size into human
readable format in Java some one of
these in 2010 the answer from 2010 has
almost a thousand up volt it's a pretty
beautiful answer to to translate numbers
to human readable format and the
question is how many people have copied
this answer and put it on their own code
so the first thing he did was he took
the answer and he tried to create a
regular expression that looks like that
answer which looks complicated but it's
basically the structure of the answer
that could fit in any format that people
ended up putting it and yes with this
query that with this regular expression
he found this answer in 448 different
data files and only 20 cent 27% of these
files at least gave Prairie to Stack
Overflow and all of the matches with the
regulars
raishin look like the coffee
stackoverflow so that's an alarm for
anyone that is just copying calls from
Stack Overflow people can sign that and
this is some cool stuff from the gold
world like when you're asking for things
like Sam came and wanted a better way to
express time until he wanted to have a
time until instead of writing after time
until he wanted to do the gold language
to offer the facilities you have to make
almost - and my teammates asked brought
data to this feature request so first to
produce query and he found out there are
2,000 repositories on github that can
benefit from this stitch so this is
pretty cool because if you want to ask
for something new from the go language
you can tell them this is how many
projects would benefit and even if you
don't answer anything yes by open
sourcing your code and leaving it'll hit
her
your code is voting for me for the
development of the language because
people can analyze your code and people
can see oh you would benefit and since
this time now for a couple months and go
now offers this new API same as someone
else came asking to have to fix tlf
config because it used one spelling one
package and a different spelling on and
a different go package and it seems like
a good idea to be consistent the
emphasis on a different query and she
found out that if they go if they fix
that there will be at least six and
rectified repositories that break and so
far they have not chosen to fix this
because they don't want to break these
reporting yes your vote your code comes
up votes when you let other people
analyze it
we can go beyond regular expressions and
regular expressions are cool but they
have a limit anyone that might have
written a regular expression and you
want something more complicated you know
the limit of it so you can deliver a in
the query you can do more than sequel
code inside your sequel queries you can
write JavaScript code and with that you
are free to do a lot more stuff in this
case I took a get a product that's
called JJ skin that the static code
analysis so with a query like this
basically I'm telling the query to
breathe the JavaScript code that I
downloaded from the internet take the
hint
run this query look at all my JavaScript
files but between this as on this size
and in the middle of a query I'm able to
add this JavaScript code which calls J's
hint with the code create some - static
code analysis and I collect all of the
warnings and now I'm able to analyze
anything I really want to analyze I just
will need to write a parser that
analyzes code with all those dream the
world and yes I found results that the
most frequent warning that yes it gives
is missing semicolons our but then you
also have warnings that look at
variables without that I cater detecting
that something is a variable but now the
name doesn't matter
it just knows because it's able to
analyze code in real time and my love
the love them I have here is pretty
popular question is spaces or tabs who
is for spaces we for TAS who likes go on
we don't have goal yes well so what I
did here again I love show you queries
first I define my rules how I'm going to
measure space let's stop
first I'm going to look at everything I
have in the query but not all of the
repositories will come in my rank I want
to look only at the top four
hundred-plus repositories and I choose
the top four countries also repositories
by the number of staff I need to see
stacks those are the place at which
which votes I will be counting here and
looking for files that have more than
two lines only one vote during UNIX file
if a file size of spaces untapped I will
count and see if it has more special
tasks and give the board from that file
based on that and I will only look at
these top languages first step extract
from my sample code I will only get the
fight within stations and then I can
write a query like this boot let me
switch microphones so my query here
let's run it while I talk looks these
exact rules I'm looking at if it has
tabs or spaces I'm doing the split but
to look at its line look at that lines
that start with spaces all tab that at
least have ten of these lines and these
were my results which look better
Disha life and I can copy this data
quickly to a spreadsheet and yes because
it looks more beautiful I want I'm going
to do a copy of my table here so
what I want to do with look at taps in a
negative space so basically I'm going to
do the opposite v2 yes everyone is
pretty curious about what has a result
so these are my results I will do a
chart third chart boom let's make the
more beautiful top versus FS stacking
standard here you have the results as
you can see a Java everyone prefers most
people prefer spaces but this is not by
only by language every language has
different preferences in C is 50/50 and
languages like Go Go fan what everyone
is doing
taps that's the language you should go
to if you are a space a fan while we
movie everyone does spaces for one
reason that I don't know yes this is how
fast you can run any analysis you want
over a lot of code and it's also pretty
- pretty easy to add visualizations and
write get this result so nurse a lot
more hey I think we yes I only have nine
minutes left I'm going to leave some
time for questions I really like word
some Googlers did hear there was a big
security back found last year and the
schools and out that there were a lot of
unprotected projects and 50 Google
employees went out to fix all of them so
they started by using these cold days to
find every place that
was open to this lag and they fixed all
that and that was really cool so yes
even github uses bigquery to analyze
themselves I made this video with
Allison wants the data scientists check
that video if you want to see more of
how they do it I hope everyone wants to
analyze kids have there's a lot more
that people have published I'm
collecting everything is fine with this
data set so you can go deeper with this
you can also see on what he has done
within groovy because as a groovy
maintainer owner he's willing to say
that Acadia and if you have more
questions find me already find me on
Twitter Stack Overflow is a great place
to ask questions if you have feedback
for me I love feedback and we can go
drink now unless you have questions
before we end this thank you let me take
a picture of you you are the brave one
brave ones that stayed until the end any
questions
if you want if you want to do this yes
we could you could do this immediately
from your own notebook if you create a
Google account you go to cloud a
bigquery cloud Google Chrome you create
an account you can run any of these
queries right now yes all of this is
open shared and ready to be queried you
you don't need to start by downloading a
terabyte of data the data Terr with
queries on you can copy-paste any query
or regular it will be work yes
that's an excellent question how is it
possible that we were able to do things
these quickly when the queries magic no
it really is like that the would a lot
of people say I may be saving through
the comments because it really changes
how people are working like we have
customers like Spotify that know a lot
about Big Data but when they they start
using the queries like whoa their
problems are gone because they have to
they can forget about all the maintainer
in fact they can just load the data and
work maybe I can load a diagram of how
the query works but just the secret here
the secret of this query Alto one is the
capital storage from computing so
everything that you storm the query it
doesn't leave in servers because what
you need to do in every big data system
what people do for performance is they
put the data on each computer so in most
big data systems that you might be using
the first question is when you load data
is how do you want to this DV date how
many servers you want how do you
distribute it between servers and
because that's the only way to get fast
results because if you put the data in a
different place across the network
networks are slow so analyzing data
arbitrarily and not isolating it will be
slow unless you have a very very very
very insanely fast Network and that's
what we have inside the Google that
wasn't one pipette elite bisection and
their world so we don't need to think
about data locality anymore they take a
leave in a separate place so I'm that
allows us to not think about the
solution we just put it there and we're
able to read it in parallel the other
secret of the query is the ability to
just have
a thousand servers analyzing things so
if I come to my query analyzing here we
can see the explanation the first step
in this query like the Scout explosive
versus tab to thirteen million files
which were the beginning of this and the
output was just seven thousand rows
basically you could think that there
were around seven thousand servers nodes
the slots as we call them I'm reading
this data parallel and if you have a lot
of computers being able to run in
parallel that means that your
performance can be almost linear you
have more data we have more slots more
computers reading it in parallel and
then the next stages just need to
analyze the data that these are leading
but basically you have access to a lot
of computers in real time and you're
only paying for queries they are not
paying for the thousand servers you are
only paying for how much data your query
scans
yeah if you servers with fiscal service
or container these are containers
so we really call them slots that's a
and yeah this doesn't mean that they
were saying social service it means that
there were seven thousand rows of output
but sometimes let me run a simple query
that just counts things so let me look
at very comments from September 2016
and I want to what I want to do here is
to prove with standard sequel so week we
used to have its own sequel but now we
can do it under standard which a lot of
tea to prefer and when I want to count
the distinct number of users the authors
so count distinct usually an expensive
operation to run and what I'm doing here
is getting it pretty fast
e getting that there were three million
different users already in September
like here and the explanation shows me
that I went from 67,000 65 million rows
to 52 million and then this there was a
shuffle here to be able to run the to be
able to County parallel so each server
that was counting in the next layer were
sure that there was no other server
counting ladies
yes yes we have a magic shuffle system
it well I mean things are easily fast
like a lot of people ask me hey can I
run require my own data center the
answer is no because things here go
beyond software this is really making
the most out of our custom hardware and
networking that is able to run shuffling
at any same speed yes it in a normal
database across giant kills your
database in this case I can run pretty
insane shuffles like what I just did
here and here we have a better look at
how many servers for about 50 at least
on the after the shuffle there were 50
different counts and what stage three
does is it just adds the number the
unique number that each server gave and
then if I want to just to all of 2015
instead of only one month I can
understand and things again are insane
because it's fast and now instead of 50
we had 300 servers
not floods involved yeah
yes please
yeah the 7,000 favor for a little bit of
an exact acceleration because each one
of the slots involves a gave more than
one row and what we are camping here is
Ross the so that's why I run this
different query where I'm sure that each
node only outputs one row after the
shaft
so these 577 a 500 million half a
billion rows yes we're countered by 320
different slots
yes so where's the data and the first
step when you're running for to run all
of these queries is to load the data in
bigquery who the queries able to read
data from external systems if you leave
it on Google storage with some BigTable
the query can even read data from
spreadsheets from Google strategy if you
want to do joins with data that form
analysis doing and but what you really
really want with the queries to load
your data so would you do something like
this you if you're using the web UI you
basically tell me this is my file that I
will upload don't upload the terabytes
I'd put it on cloud storage web and you
only need to tell the query this is the
table where I want to leave things and
this is my schema and it can
automatically detect the scheme this
difficulty will take your huge file and
store it in a columnar storage format
optimizing a small optimizing it to be
analyzed later but the number of
decisions you need to make is almost
zero you basically tell it this is my
file and these are my columns that can
be also detected and actually before we
can do the rest of the TV station itself
yes so the first step would be having
your data in a CSV or a JSON file I'm
telling the query this is my CSV this is
my Jason yes yes so if you want to do
useful for reporting a great place to
store all this data inside the query
because it will be so you don't have
access to the background storage after
it's told in the query you usually query
API to access it oh so the question is
if we're using big table or spanner or
I'm trying to think what's our latest
official answer bad it's one of we could
say its victims like a lot of Google is
based on the table and if it's not a big
table to spanner
it's one of our two main storage yes/no
questions I will be linking with
everyone else it's not finally they're
sending us finding reddit okay thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>