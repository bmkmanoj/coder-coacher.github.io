<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The JVM and Docker. A good idea? by Christopher Batey | Coder Coacher - Coaching Coders</title><meta content="The JVM and Docker. A good idea? by Christopher Batey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The JVM and Docker. A good idea? by Christopher Batey</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6ePUiQuaUos" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome thank you all for
coming to the the session so doc is not
pry this post pot popular as it used to
be but still still pretty popular so
what I'd like to talk today is not gonna
be it's not gonna be very introductory
it's gonna be lessons learned and our
kind of war stories from running doctor
in production with JVMs inside now for
around around a year and a half so why I
care about this and why I'm talking
about this is for most of my life I've
been a Java developer or at least in my
adult life and over the last year and a
half I've done something very different
I'd be know what I'd classify as like a
platform engineer and what I've been
doing is building a platform for people
to deploy docker containers which
normally contain JVMs
across a large set of a large set of
hosts so I'm working in a very small
team now which manages hundreds and
hundreds and hundreds of dollars--
contain there's most of which a JVM have
JVM is inside them and it's using a
custom build of kubernetes what I want
to talk to you about today though is a
very short introduction for motivations
for moving to docker in production and
you could replace the word docker with
Rocker with all sorts of container and
it doesn't really matter what you're at
what your container engine it doesn't
really matter what your schedule is
because I think you have to have a
pretty good reason to want to do this
because the rest of the presentation is
all the problems which I've had since
doing that which might be worth it might
be not so I'll let you decide based on
that and how I'm going to talk about
that and present it is I'm going to show
you a bunch of examples we'll talk a bit
about how dog food containers work so my
assumption is that you've used it
probably on your laptop maybe in
production you understand the CLI but
you don't necessarily understand how it
works under the covers you don't
understand and say that well you don't
understand too much about cgroups and
names faces etc so we'll go through that
then I'm going to show you a bunch of
tools which I now use on a day-to-day
basis for diagnosing problems with JVM
running inside inside containers and I'm
going to do that using two examples
one's going to be a drop as an example
one's going to be a wrap
pack example is not too relevant I just
wanted a thread per request application
and a thread for core application so if
you want to do what I'm doing on screen
or what I'm showing you on screen then
you can do that I kind of designed this
talk as if it could be a workshop so
there's a Gradle project on github that
will build you three different docker
images and then a set of docker compose
files which can then launch some with
different configuration options so you
can experiment and see how JVM behave
differently depending on whether using
CPU set or CPU quotas or CPU periods if
you don't know what docker composes do
not worry I barely know what dock who
composes just know that it's a Yama file
which can then launch a set of
pre-configured our docker images as
containers are with-with-with
pre-configured parameters I have no
production experience of that so don't
see there's like go and use docker
compose as I said I use Cuban edits so
the example that I'm going to do is this
so we like to build microservices these
days and what that tends to mean is we
build a lot of services which make lots
of call-out to other services be it HTTP
queuing technology databases whatever so
what I wanted was a JVM running inside a
docker container which did something
realistic so each of them do a little
bit of computation so the rat pack and
the drop wizard application are exactly
the same if you've not come across drop
wizard do not worry it's virtually the
same as spring boot it has an embedded
jetty inside it rather than embedded Tom
cuts but just think it as any kind of
you know app server which is going to
have a thread per request what they're
gonna do then is call out to another
HTTP service and for this I'm going to
use wire mock which is a really cool
HTTP stubbing library but again you
don't need to know anything about that
to understand this talk what we're gonna
do then is hit it with some load I'm
just going to use a bug I've just used
Apache benchmark to generate the load
and this I recreated all of these things
on my desktop which has six cores
house 12 Hardware threads and 64
gigabytes of RAM you might think to
yourself why does that matter
and one of the reasons for moving to
docker is that you abstract yourself
away from the underlying machine you
build the docker image it goes to
production and you're happy you don't
need to worry about that but one thing I
very quick
when they started trying to run Java
applications inside docker containers is
you really do need to care about what
hardware is under the covers it does
make a big difference so Before we jump
into the technical stuff let's have a
quick review of the motivation for doing
so or certainly this is my motivation it
might not be yours hopefully by the end
of this section you'll have a good idea
whether your current problem docker
might be a good fit so have a think
about your deployment model how do you
deploy your JVM to production right now
who here still runs an app server who
builds an executable jar would say a
jetty or a Tomcat inside ok so it was
still more the app server side now some
people don't like them some people do
not like running them but they give you
one major benefit or certainly a benefit
from my point of view it abstracts the
hardware away so that developers can
build their ear the war files their
application and they don't need to worry
about what hardware it's running on you
know that can be deployed now anyone
with any experience will probably know
that you do kind of care which app
server is et cetera and it will maybe
work on your laptop and not work inside
the app inside the app container but you
know it does attempt to give you that
benefit over the last five years I've
built a lot more executable jars
completely independent jars that only
depend on a JRE
which are then deployed to single
purpose Linux virtual machines or Linux
like actual bare metal hardware and
that's the only thing running on the box
and they might have something like
system D managing it to make sure it's
running and restarting etc and you have
to ask yourself the question if you're
doing the latter so you're doing your
executable jar what benefits are you
getting from putting it inside a docker
image cuz you've only got you only
really got one dependency you've got a
JRE and probably potentially you needed
a port open on that box and that's about
it so what benefits are we getting there
well one of the benefits that I
certainly saw was I worked at a company
that had hundreds and hundreds and
hundreds of applications deployed and
there were single purpose VMs and what
happens is the traffic to those virtual
machines so those those Java
applications it's very spiky sometimes
it's really busy I've worked for an
online television provider in the UK
so everyone like logs on to watch Game
of Thrones or Manchester United or
something like that and then there's
only a small subset of the population
that continue to watch you know daytime
television um watch BBC news all day
which means that on the whole the actual
hosts are running out Java applications
on a typically extremely underutilized
who here runs an AWS I've got many a
good chunk all right AWS have a really
cool tool it's called trusted advisor
and it's quite a funny thing I don't
quite know why they give it to us but
it's aid in the u.s. telling us how to
give AWS less money now I can't think
what the business motivation behind that
is but essentially it looks at how
you're using AWS and it tells you
whether you're doing potentially stupid
things so this is a real account that I
worked on for a couple of years and what
it's saying there is we could save a
hundred and six thousand dollars per
month I can only Berger's you could get
for that you could hire between 10 and
20 engineers on your team like that
isn't an insane amount of wasted money
and that was because we had a large
estate we had a lot of applications and
they were all running on their own
virtual machine and they're all sized
for the peak part of the day and then
what would happen is the rest of the day
nothing would be running or very little
would be running so on average they're
very underutilized and you can take
another few strategies to try and fix
this you could turn them off you could
have smaller deployables and
horizontally scale them and scale up and
down and we definitely did some of that
and we've got this number down quite a
lot but in reality my opinion the
smallest virtual machine inside a device
or the smallest thing you can do say in
your actual physical height you say
you're using cloud stock or something or
maybe you've got feel real hardware
virtualization isn't even an option for
you well if that's the case and you
kinda need to start running multiple
applications on the same host you need
to bin pack them you need to get as many
on as you can and then you need to move
them around dynamically depending on how
busy those applications are now who is
tried to run multiple JVM not your
laptop in production on the same machine
all right it can be easy it can be hard
I used to do a lot of a work with Apache
Cassandra and it was extremely common at
a JVM based database to run Apache spark
and Apache Cassandra on the same host
because you're typically doing data
processing with Apache spark with the
data inside Apache Cassandra so these
were two very beefy but JVMs all trying
to alter it to run on the same box so
they'd all want all the CPU that all
want all the memory and they'd blow up
in a heap and there's a reason for why
that for that why that is so the JVM is
extremely good at utilizing the entire
box you know if you actually just run a
single JVM on a host and you not say in
a really latency sensitive environment
you might pass a zero flags to it you
might not configure anything and that's
because of JVM ergonomics so the JVM
does not pick its heap size based off
our some hard-coded value it bases it
from the amount of memory on the machine
how about garbage collection say you're
running a concurrent garbage collector
how many garbage collection threads do
you think they are well that's based off
the number of cores on the machine and
the JVM is really good at that it means
that people coming into the JVM
ecosystem I think get a really good view
of the JVM they've run on the box and
all kinds of works but when we suddenly
shove 10 J VMs on the same machine then
they start to step over each other and
that's kind of the problem which I
generally try and fix by using
containers but to effectively utilize
them we need to know what they are and
how they work you know I think that
containers and then it's containers
regardless of whether using docker
Rocker whatever it is a leaky
abstraction you need to understand how
they work and what underlying hardware
is there for you to use them effectively
for that we need to understand a couple
of features inside Linux so we're going
to have a look at namespaces very
briefly because I don't think there's
much specific to Java about namespaces
but we're going to spend quite a bit of
time looking at how JVMs behave when
they're inside control groups so names
races are a way of Linux to keep things
separate so let's say that you're
running a process and you don't want it
to be able to interact with
the processes on that machine or see
them or perhaps you want to divvy up
your file system so you can only see a
small amount of it these are the
features which docker use under the
covers but then you've also got the
resource utilization so how do we divvy
up this cpu how do we divvy up the
memory that's where control groups come
in that's what Stoker is using under the
covers and rather than just go through a
lecture on what they are let's have a
look at some actual tools that you can
use I use a lot more tools than I used
to
or there's a whole new set of tools
which I use since I started using them
the styling using docker and anger and
you've probably used quite a few of them
in your past I imagine a lot of people
here have used PS and H top and top and
the great thing about these tools is
virtually all of them are namespace
aware if you jump inside a docker
container and run top it's only going to
see the processes of that of that
container because it's in that process
namespace but virtually no tools
including the JVM RC grouper were so if
you jump inside a container and run free
then you get to see all of the memory on
the host which means that JVM does so
start to think about the implications of
that we'll also look at D start we'll
look at C advisor if you've not UC
advisor with with with the docker then I
will by the end of this talk hopefully
and a few others so to give you some
context this is what the this is the
base image which all three of the images
I'm going to play with over the next say
half an hour they all extend from this
one so they're all based on the fusion
base image as anyone looked at the
fusion base image right I I would go and
read about it this google fusion base
image pit one problem it solves a lot of
problems which happen when you say run
Java is the pit wan inside you have
container you know zombie processes etc
so just go and have a look at that but
the only other thing that this does is
it downloads an Oracle Java it copies in
my favorite start javascript which we're
going to have a look at and then sets
the entry point to something in the
fusion base image the respective other
applications the other images so a rat
pack I dropped wizard and ry mock they
all just stick in an executable jar and
then start Java with a bunch of options
right so let's start C name a namespace
as an action so who here has jumped
inside a container using docker exec
feels a bit like your SSH into it this
is really dangerous never do this in
production what we're actually doing
here is we're running the bus process
inside the same namespaces and the same
control groups as our production
application so what are the implications
of that well first let's see the
benefits of process namespaces so we've
jumped in and I've run PS and I only get
to see a small set of processes if I run
this on if I run PS on the host
operating system I would see hundreds
there are many hundreds of processes
running most of these are started by the
fusion base image so we can ignore
nearly all of them but we can see our
Java process sticking there it's about
third from the bottom does Java native
memory tracking unlike diagnostic
options but the problem is if we then go
and do something there let's say that we
grab some logs or we sort some logs what
we're actually doing there is using up
the resources of the control group that
our docker that are our production
application is running him so you could
jump into here you could run a command
and you could kill your container
because you would hit the memory limit
that you've allocated in say or maybe
vomit allocated that manually or via
kubernetes or marathon and me sauce
doesn't it doesn't really matter but
just be where you are using your
production applications resources when
you jump in and do this if we want to
see those processes we can do it on the
host operating system right Linux
containers they are not a virtualization
technology they're an IDE more of an
isolation technology so we can find the
same Java process on the host it's going
to have a different pid' though we're
going to see the host pit so this same
process which is running as process
number 28 on the host operating system
we can see it is 2 9 8 9 5 so try and do
as much as you can on the host operating
system so that's the first tip which
means that we don't need to run things
like top and H top and all those tools
that were used to using inside the
container we can run them on the host
operating system so here I've started to
hit this drop visit application this is
this JVM running inside a container with
Apache benchmark and right at the top
there it's using you know 133 percent CP
you and as many threads running a chop
shows the different threads and of
course if you just don't like colorful
things like hate top you can continue to
reuse top if you like and that doesn't
give you the per thread view and we can
see that the Java process 2 9 8 9 5 is
right at the top there now when we get
onto looking how see group CPU
allocation works there is a few
differences but this is how I would look
at the CPU usage I'd be doing it from
the host but that can become quite
tiresome you see this little PS at the
ball at the bottom here now imagine if
you're on a host with a hundred
containers all running JVM is inside how
do you find out which processes are for
which and there's quite a few ways of
doing that as long as you understand see
groups under the covers and there are
also quite a few good tools which are
control group aware so systemd
integrates with control groups
extensively this is like the mo I'm
guessing most people running Linux now I
use a using system D or quite an old
district and it has a set of tools that
work with control groups it doesn't know
anything about docker doesn't know the
fact that docker exists but because
docker is putting things inside control
groups see group LS are listing the C
groups it's going to be able to see
things inside docker so when you do
docker PS and you get hold of your
container ID that container ID becomes
your control group name so if I do C
group LS I actually get to see my docker
see my docker C group and then I get to
see the C group specifically because the
hierarchical for that container and then
all of the processes running under it so
I get to see my Java process running
under there and again I haven't had to
jump inside the container I don't even
care what the pit inside the container
is now you might not have that tool
installed so like most things in Linux
everything is in the filesystem so there
is a C Group file system that's going to
describe everything that you've said
docker is set up for you so if you go
into certainly on RedHat distributions
if you go into sis FS c group you can
then find your containers so inside the
docker directory the docker the ID that
container ID has become a directory and
you can go in there you can see how much
memory it's using you can see how often
it has been throttled is
in CPU throttles you can see if it can
be in kill and things like that and this
led me to understand an error message
that came out of docker so a lot of the
time when you start things in docker
what you're really doing is all it's
doing is passing options down through to
Linux and sometimes their neck says no
that is a stupid stupid option so look
at this one I tried to start myself
eyesight to start I tried to start a
container and I said it got a system
error couldn't write to a file and
that's because the configuration
parameter I was passing to docker was
just being passed through and then I can
know and you can go and look at actually
see group documentation to see why that
was an invalid value and even if you're
not using Red Hat so using a Bunter it's
probably the most popular one for
following for running docker go and read
their documentation take half a day and
just go through all of the C group and
namespace documentation because when you
come to be done when you come to
diagnose problems diagnosing pauses in
your java applications having that
knowledge is going to be invaluable so
let's have a look at how memory behaves
for our applications and insert inside
containers so if you're not setting
memory limits when you're using docker
in production there's not much point in
using it all right if you're just
kicking off twelve docket containers
running on the same host and they've got
unlimited resources why aren't you just
running them as processes there's no
point so what you need typically want to
do is be fair you want to give this
application needs 500 megabytes this
needs a couple of gigabytes this needs
you know 100 megabytes and normally
you've got a something doing that
scheduling at a higher level like
marathon or kubernetes but let's say
you're doing it yourself then you
compassion - - memory 128 megabytes if
we docker exact into that container and
I told you not to do that but I'm not
following my own advice and run
something like free then we get to see
the whole 64 gigabytes of memory even
though this container supposedly only
has 128 and that's because it's just
proc /proc all the information in there
isn't virtualized it's just getting the
the information from the host and that's
information the JVM Singh so he's going
to be a quiz
part of my start javascript sets the min
and the max heap to 256 megabytes so
it's going to pre allocate all of its
heap it's going to be nice and eager
there we're gonna give the docker
containing 128 megabytes of run what's
gonna happen when I start this container
shout out someone have a guess mmm
anyone crashes is pretty accurate who
thinks it's going to be a happy JVM
tootling along in its container no all
right so the whole point of control
groups the whole point of docker is to
be able to allocate certain resources to
certain sets of processes and if you
suddenly try and use 256 megabytes ram
and you're only said 128 it's going to
kill it
so one tool that you should just get
used to using if you're going to go down
the the docker route is D message so D
message or if say if you're using system
D you can see this in Journal control as
well it'll actually tell you when it
does that and if you read all of this
the read doesn't show up very well sorry
about that
but it's simply saying I killed your JVM
it's using too much memory and it'll
give you a breakdown of how many pages
each of these processes are using in
memory which is also useful so you can
diagnose why so a common problem I often
have is that a certain process will only
use say 100 megabytes but it'll spawn
child processes and then they'll use
memory and child processes are also
allocated in the same C group so then
you'll the inside the d message message
you'll see what's being killed what
about this so remember we had 256
megabytes of pre-allocated heap you know
min equals max heap and we're now we're
going to give the docker container 300
megabytes who thinks this container will
survive not survive doesn't care on one
person doesn't care about our pod drop
visit application well this will depend
on the configuration of your operating
system but this survived on mine it
wouldn't survive on some of the server's
I work on there's a little caveat though
it's very slow orders of magnitude
slower so what's going on what's up poor
little JVM up to this is something which
is more applicable to JVM sand is to
other types of applications that run
inside containers
so I went for a cup of tea and five
minutes later it was still running
installed behaving very slowly we can
see there from the dr. PS output they've
been running for five minutes another
tool I like to use when trying to work
out why acquired I could my applications
running slow inside of a container is C
group top so this is a top which is not
going to give you a process breakdown of
CPU usage it's going to give you a pure
control group so if you were to start 6j
vm's inside your container for some
bizarre reason then it would aggregate
the cpu percentages for you our little
container there which I can tell which
one it is which C group it is based on
the the docker ID has 12 tasks it's
running at 356 percent CPU and it's
using two hundred ninety eight point
nine megabytes of memory which is it's
what we asked it to use right we said
don't use more than three hundred so
it's almost like a JVM is behaving
d-star is one of my favorite tools in
the world if you've not used d start
just dive in there forget about all
other Linux tools or certainly ones rich
you can do it D start so rather than
looking at IO start and VM start and all
that starts separately D start can do it
all and it's really colorful so the
stuff I would look at if a JVM se was
not behaving well is I might just see
what the host is host operating system
is up to so we get load average on the
very far side there so I started this
this is refreshing every 10 seconds just
before I started the the container and
just before I started the load on that
container and if you look at the one
minute load average is below one so this
is a completely empty quiet box we don't
need to worry about it by the time we
get to it our application acting very
slowly the load average is up to 6 just
at the bottom there we've got all sorts
of useful information here as well that
you can just have a scan over there the
next column is the run queue so how many
how many Pro how many threads actually
trying to run on the CPU then we've got
memory so what I'm looking at here is a
lot of free memory there's 45 gigabytes
of free memory so we do not need to
worry about memory saturation on this
box
we've got system we've got context
switches so this is the drop visit
application so we can see here on the
system you've got SCW it stands for
context for
this is how often threads have been
taken off and onto the CPU and we can
see because drop misses a thread per
request application we're getting a lot
of context riches but that probably
isn't a problem that's pretty normal
our actual CPU usage is quite low we're
only using 20% of the CPU as user time
and around 13% the system but the scary
thing something you never ever want to
see indie start when running it in
production is this column which is
paging so this process is actually
swapping even though we have 45
gigabytes of free RAM and if we look
closely at our see group top we'd have
actually seen that this application does
not do any disk work so see group top
gives you CPU memory and i/o and that IO
actually shows here is it's doing quite
a lot so because we've only allocated
300 megabytes of memory and then I JVM
has tried to use more this process is
actually swapping and we can go into our
other favorite art tools and we can have
a look at H top and we can see look it's
using 174 megabytes of swap even though
there are tens of gigabytes of free
memory so this is the slow dying Jaipur
JVM one of the reasons this is more
likely to happen is if you do a full GC
for instance suddenly the garbage
collection threads are going to access
most of your heap so a lot of the time
you can get away with paging if you
don't actually access all the memory
that you're up the processors tried to
use but in JVM is it pretty much always
tries to access all the memory so we can
disable swapping and be honest we do
this mainly at the host level on most of
our things but what we do is we pass in
a memory - swap and this is memory and
swap so if we set it to the same as
memory then we don't allow the the
containers to use any swap whatsoever
oh I've gonna do another quiz what do
you think happens now it's back to dead
so we're back to looking in D message as
soon as we disable swap will allocate
the heap up front the JVM will use
things for thread stacks and native
memory and it will get killed right away
right so we need to have a look at
memory utilization and we need to look
beyond the heap when we're doing this
stuff now if your JVM is the only thing
running on a Linux box
I could imagine never looking at this
stuff unless I had some kind of memory
leak or I'd something funny with the rat
byte buffers and caching a thing
normally you wouldn't look at this too
often but when you start to run them in
JVMs inside containers I find you just
have to do this for every single
application so we need to look at the
amount of memory we're using on the heap
we need to look at threads we need to
worry about meta space because by
default unlimited we have to look at the
compiled code inside the JVM so when
your JVM takes your byte code and and
compiles it that uses up memory that's
stored you've got things like GC uses
memory and then you've got any native
memory you could have a library which is
using unsafe you could be using direct
byte buffers and one with the tools I
always used to have on now inside my JVM
that running inside docker containers is
the native memory tracking on but only
to summery so not to fall because that
is a too much of a too much of a cost so
J command is a useful tool if you're not
using it on a day-to-day basis it comes
with your JVM now and everything is
slowly moving into it and other tools
have been deprecated so I would
definitely suggest having a having a
play with it now I've broken my rule
again now by docker exacting into the
container to see what's going on to see
what's using this memory now the reason
I have to do this is I very I don't have
J command installed on my boxes I don't
actually have a I don't actually have a
JVM there anything running on the host
is docker itself and if you just run j
command without any options that lists
out the JVM s for you so you can find
your JVM then you can run J commands
with the pig and then help and it nicely
gives you some help and the one we were
actually want to do is do VM dot native
memory tracking native tracking
depending on which version of Java
whether it's an Oracle versus an open
JDK and which version you'll get a
different set of options but via far as
I'm aware VM native memories is on all
of them now what this'll actually do is
give us a breakdown of all of the
different memory regions inside the JVM
which is really useful so because this
was the drop visit application it and I
hit it with quite a lot of load using
apache benchmark it spawned a lot of
threads and actually I only used 12
concurrent requests for Apache benchmark
but somehow our JVM had created 102
threads which is quite a lot to service
such a small amount of load and of
course the stack size on a 64-bit JVM is
a default of megabytes
that's actually requiring quite a lot of
memory just to keep all of those threads
about other things we've got on there we
can see the compiler that's not using
much internal so internal is internal to
the JVM but if you start allocating
direct byte but for certainly the
version the JVM which I'm using at the
moment it would show up there as you
need to worry about your libraries doing
that the rat-pack application for
instance there's a lot more of that
while the drop visit one doesn't
virtually none but the rat-pack while
that number would be a lot bigger and
then we can use jaikant card again to
get a thread dump but actually have a
look inside our jir JVM to see what all
of the threads are and then we can
hopefully try and try and reduce it now
I know you can signal a JVM to get a
thread dump but I find this to be useful
because it comes to the standard out of
my console whereas if you signal on JVM
it goes to standard out of the JVM and
then you have to go hunting to where on
earth that might be and if it's big
enough which it probably just is on this
screen most of the threads are drop
wizard so jetty requests threads how
many do we think so all of the drop
wizard requests threads have DW in them
which is very nice you know means I can
quickly just grap and see that there are
65 so even though we're only handling 12
concurrent requests we still had 60 odd
threads lying or it lying around being
used by the JVM for that now jetty also
has quite a few other thread pools so
selectors what they are is those are the
threads which are interacting with n io
taking the data off the socket and then
giving it to your request thread inside
jetty something very similar inside
Tomcat and whatever kind of like web
framework or web web library you're
using now by default we appear to have
24 now 24 is quite a suspicious number
because we were running on a box with 12
virtual calls so because what jetty has
done is it selected the number of
selectors based off the number of cores
which is a really good default if you're
the only thing running on that host if
you have a hundred JVM is running on
that host
it's a terrible default you'll have
hundreds of threads that are just
selecting the same for accepting so
these are things which accept new
collections this is again based off the
number of COSM and we've got 12 of them
now you might think that the worst thing
that could happen when I'm running my
jetty on my Tomcat inside a container
would be that I've got arm wasted
resources I've got too many threads
well that isn't true jetty is kind of
clever what it does is it looks at the
numbers you've selected and then tells
you whether you're being stupid or not
so on some of our applications we might
say we only need to handle five
concurrent requests so we would limit
our web server to only create five
threads for handling requests you know
why would we want to create more but if
we don't if we aren't aware that our JVM
inside a container is going to base
things off all of the cores on the host
then we can get into some pretty sticky
situations so this was an interesting
one
so I've recreated this so this isn't the
actual stack trace from from production
but what this essentially is is I change
the AWS instance sizes under the covers
I decided to use more like fewer bigger
machines more cores more memories I
wasn't managing too many computers and
we'd hard-coded the number or it's fixed
it was configurable how many requests
threads were going to have so that was
five but we hadn't overridden the number
of asset acceptors and selectors so the
same docket image that have been running
fine for many many weeks suddenly would
not start anywhere and that's because
jetty was like why on earth would you
have five request threads when you've
got twelve acceptors and 24 acceptors so
you actually do need to worry about this
under the covers and of course now
everything is configurable so remember
when I talked in the motivation about
JVM economics and JVM economics for
about the JVM making decisions that
suited the machine it was running on
well we need to do that for all of our
libraries as well so jetty is just one
example if you look inside your database
drivers or any kind of like parallel
processing then it's gonna have thread
pools and it's probably going to base
those off the number of cores and even
if we set all of those it doesn't stop
us from creating many threads and one of
the differences I found when running JVM
is inside containers is I suddenly got
very tight and by tight I mean I didn't
want to spend any money and that's
because I was like oh I can fit all
these JVMs on that server let's try and
make the JVM as small as possible let's
try and use as few threads as possible
that trying to use as little memory as
possible
but most frameworks and web frameworks
etc insight Java our thread per request
so we actually we end up with a thousand
threads if we need five that if we need
a thousand concurrent requests so there
is a solution though it's to use nodejs
I mean we should stop using the JVM I
mean we're in 2016 now I'm joking so
there are lots of frameworks inside Java
that do not tie a user request to a
particular thread you know using
operating system threads as are almost
like abstraction around concurrency is
very expensive you know because they use
a memory the use of operating system
resources so for certain select
applications we moved away from things
like jetty and Tom Kha and we decided to
use rat Park that's just one example
where you have a number of threads based
on number of cores evolve most of those
systems are built on top of a nettie of
that if they're a network based but then
you've also got a system like akka not
at my current place but in my last
company we use a lot of akka and it's
not as bad for those type of frameworks
because they're not going to create a
thousand threads inside your container
that's only been given 128 megabytes of
memory they're probably only going to
create based off the number of cores and
Rat Pack defaulted to the CPUs time to
so does occur and so do a lot of other
things so we still need to set it but
it's not quite as catastrophic if we do
it if we if we don't but we don't need
to stop with our web framework I
mentioned ergonomics before so GC
threads for instance GC parallel GC is a
good thing it stops us from having to do
full GCS what it means it's got lots of
threads running in the background with
our application trying to clean things
up but if we've got 100 des VMs running
inside the same host and they've all got
GC threads based off the number of cores
then we end up with these hundreds and
hundreds of extra threads which bring
our systems to a halt same for compiler
threads so that's not as bad because
that isn't linear it's a very slow
progression with the number of cores for
our compiler threads and then we've got
things like parallel streams or anything
which is using the default fork/join
pool that again is defaulted to the
number of cores on the machine and you
need to override it if you're gonna if
you're going to run your JVM inside a
container what I end up basically having
to do is recreate all of Java
jvm economics I make all my
configuration dynamic based on the
allocation to that container so so much
speaking so much stuff about threads
it's time to actually look at how we
allocate CPU for our containers who here
likes GC pauses know I love GC pauses GC
pauses are great because the JVM tells
me when they're happening and I know
exactly what they are you know you can
print time to save point you can print
total time stop do you can do GC pauses
this is great it's quite visible what's
going on but I really don't like a
pauses in my application for which there
is no data about why that pause has
happened and this happens a lot more
when you run JVM is inside docker so
let's talk about that now so using C
group top and if we run sour drop visit
application and we hammer it with some
load it'll use loads of calls it's on
700% cpu there that's absolutely fine
which again if that was the only thing
running on the host fantastic but that's
not the point we're trying to run lots
of things on the same host and get some
fairness share out the the CPU resources
fairly we can see that inside top as
well now this is what I'm going to use
some docker compose files that are
checked into the github repository and
I'm gonna go through the three different
ways in which you can give CPU to a
docker container and theorize about how
the JVM might behave differently in
those those scenarios so the most the
brute-force approach is to use something
called CPU sets what you're actually
doing here is saying this container can
only run on CPU 1 2 &amp;amp; 3 so in this
example here I've said that my Y MOC
application the one which the other two
were calling out to it can use 4 cause
it can use core 8 9 10 11 whereas my rat
pack on my drop is an application they
can only use one core each you know one
of them gets called one and one of them
gets core to this works and your
application will behave very much like
it would on a normal host it'll get to
go on that CPU this free time on that
CPU your application will proceed but of
course if you run this on if I run this
exact same docker compose file on my
laptop rather than my desktop I don't
have 12 cores so it would
start it would say that CPU doesn't
exist so it's kind of brittle another
disadvantage to it is if we're trying to
increase utilization then this doesn't
help unless we were to put two
containers on the same you know core
which kind of then just makes it
complicated to reason about but if we
were to try a new CPU sets and then we
were to hammer our drop as an
application with some load even though
it would love to use a lot more CPU it
is completely pinned at a hundred
percent it can't you can't go above that
however none of the container engines
these days really support CPU sets I
know could be that is doesn't I marathon
when I looked at it did not but might do
now so we need to look at the other two
so the next is CPU shares so rather than
saying you can run on CPU one you can
run on CPU what CPU shares are is a
saying you know given if everyone wants
CPU I give quarter of the CPU bandwidth
to this process quarter to this and a
half to that so what that looks like
whether you're using control groups
directly or whether you're using docker
is setting CPU shares to some number now
the default for some number is 1024 and
then you can you can make other
containers relative to that so what I've
done here is I've set the Y mark
application to get four times the amount
of CPU as either the rat-pack
or the drop is an application now the
big difference between that and say
using CPU sets is if the rat-pack
applications not running and the Y mark
application doesn't really need all of
that CPU usage then you can burst above
you can use as much CPU as you like if
the other containers aren't using it so
looking at C group top with this
configuration my drop wizard application
and the heavy load is back to using you
know many hundred of percent CPU a
disadvantage to this though is who here
does capacity planning its capacity
planning easy crusty bunny hard not
imagine if you're doing capacity
planning with CPU shares so you deploy
your containers to whatever hosts you're
doing and they're all done with CPU
shares you hit them with some load how
much CPU they get is going to depend on
what else is on the machine and whether
that other thing on the machine is busy
so it can be quite hard to reason about
like how much how many real how much
resources you need for you to each of
your processes so here's a couple of
examples about when things can go wrong
so imagine we have three containers
running on the same host imagine one of
them to be a Jenkins slave one could be
a production application and imagine it
ended up having two cores it was quite a
small box
you cheap dowel on Amazon you went for
their t2 small ah
if we've got three containers I'm
running with 1024 512 512 then the
yellow container will probably get a
whole CPU whereas the green containers
would probably only get half a CPU so
what if we hit all of these applications
with a lot of load what do you think
happens to the green applications well
they get starved of CPU your application
will pause and but it would be quite a
steady pause you know green container
will get some and then the other green
container will get someone or flip-flop
between all we could depend upon a fork
or box and everything would be fine
you know with the one with 1024 should
hopefully get access to at least two
cores and then the 512 ones would get
access to to a single core but sometimes
that's not good enough sometimes you
want to guarantee a minimum amount of
CPU time so the next thing that comes in
is something called CPU quota so this is
another number but this number means a
bit more so the accounting period for
control groups is every hundred thousand
microseconds so 10 times a second and
what you basically ask for CPU quota is
how much CPU time am I allowed per
accounting period so I would say I want
a hundred thousands microseconds of CPU
per a hundred thousand microseconds
which could could being the important
word kind of equate to having one CPU
whereas if we looked at our wire mark
application there it's asked for four
hundred thousand microseconds per per
100 1000 microseconds for accounting
period now unless you invent some
magical computer the only way to achieve
that to get more time than the
accounting period is to run on multiple
CPUs however you're not limited to a
particular CPU here
so imagine a
GC gets keeps going on in the background
with a java application what it can
actually do is use all the cause on the
machine and it can wipe out its cpu
quota in a fraction of the accounting
period so you'd get say 10% the way
through the accounting period and your
GC threads or perhaps just your drop
wizards you know your actual application
threads will have used up entirety of
your cpu quota what do you think that
happens to your application for the
other you know 90,000 microseconds it
just stops nothing happens to it
whatsoever right so you'll be running
haffley with your user requests coming
in and then you'll just have a blip
there's no nice logs to tell you that's
going to happen it's not going to be
something like printed out like a GC was
the JVM has no idea that this has
happened to it so if you generally run
this and then you have lots and lots of
threads inside your application you'll
use up your quota very quickly so I
don't use this a huge amount also your
notes you'll be able to see this by the
CPU percentage that your process uses
that your JVM uses if you remember back
to CPU sets it was pinned at a hundred
right so is it could not use more it was
only allowed on that particular core
whereas when we come to CPU quota if
there are calls available it can use up
its quota as quick as it's like so in
this example we're are over a hundred
percent CPU for the drop as an
application even though we'd only kind
of given it one core by some definition
of one core all of this data about CPU
shares CPU sets our quota and actually
there's a really cool metric if you go
and look in that sis Affairs see group
file system which actually isn't an SI
adviser called CPU throttle time so if
you're doing your capacity planning and
you're using docker and whatever
scheduling it under the covers is using
CPU quota and you're curious about you
know what's going on to the covers why
is my application pausing why does some
of my requests take much longer than
others then you have to go and look at
this the secret file system but for
every other metric then see advisor can
get it and this is actually a GUI tool
it's a tool provided by Google you can
actually run it inside a container on
that host itself to mount that
filesystem into the into the
Tayna and it will allow you to graph it
and for me this is really useful I don't
need to go and look at my settings etc
this will tell you how many CPU shares
it's got there cause it's allowed to run
on its memory limits and it actually
exposes all the data over HTTP so if
you've got like an external metrics
database where you're aggregating like
application metrics we use Prometheus to
get all the data from see advisor for a
lot of our docket containers and then
take it off and then aggregate it so
that's it for information if you have
fallen asleep and you you weren't
interested in any of it but you're still
thinking about using docker and
production I'll just remind you of a
couple little takeaways so if you're
gonna run JVMs inside there you need to
size your container based on threads you
need to have an appropriate number of
threads you need to reinvent all of JVM
economics you need to be constantly
evaluating which libraries you're
bringing in which thread pools they have
and you need to be setting those based
off the there's like the CPU memory that
you're actually allowing your container
to use and for memory it really is about
testing like yeah I can't give you an
answer for its your dis JVM is going to
use this much memory it really depends
on the application it depends how much
native memory it's using it depends
about your live objects the kind of
things you can do and will do for a JVM
running on single hosts it's just so
much more important on a on a container
where you're trying to like cram it down
into the as small as possible so on that
note thanks very much for listening and
I'll stick around for any questions
Russians as anyone anyone got any
questions yeah yeah so the question was
what are we using for pid' one it's
really important what you use for pid'
one inside the container it's going to
reap it's going to reap somebody
processes etc it's something that's
built in to the fusion base image they
spent a long time building a really
small like inert system specifically
inside docker containers any more
questions
no thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>