<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Interview with Robert Kubis at Voxxed Days Ticino | Coder Coacher - Coaching Coders</title><meta content="Interview with Robert Kubis at Voxxed Days Ticino - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Interview with Robert Kubis at Voxxed Days Ticino</b></h2><h5 class="post__date">2017-05-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/S1L01o6zbLU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">buongiorno whereabouts space - Tino I'm
here with Robert from Google he's
talking about app UX so would you talk
about today primarily I talk about our
programming model that we donated to the
Apache foundation which allows you to
create data pipelines for batch and
stream processing so that was initially
created within Google as so called
dataflow and we donated our dataflow SDK
to Apache beam and it became a top-level
top-level project in the beginning of
this year cool so when you say
passionate stream processing what do you
mean so if you look at big data
processing like a little bit at the
history it all started with MapReduce so
the data amounts became so big that a
single machine couldn't Pro set anymore
in a reasonable time so some smart
Google engineers Jeff Dean and some ago
he they came up with a model how to
distribute and chop off the data and
basically paralyze the processing and
also create a framework to have that
resilient in a way that if any machine
fails that someone another machine takes
over that work so that started MapReduce
the issue with matter produced is that
you basically take a batch of data and
you have to wait till you have that
better of data and you process it and
then you get a result so you have a
delay to the point that you get insights
of that data now if you compare that
through stream processing is what you
want is you get a stream of data like
events from mobile devices or if you
look at IOT for instance like sensor
data and things like that and if you
want to get inside of that data in near
real-time
you basically have to process that
stream of data continuously and that's a
different way of programming and if you
look at like how that was solved in
recent years was that on one hand you
wanted to have correct results where you
used batch processing and then you
wanted to have insights
like estimate of your real-time stream
and you used stream processing like
store or like storm or Apache fling for
instance but it's two different
programming models so you have to learn
two different systems on one you use
spark for batch processing and use maybe
flink or storm for for stream processing
what a petty beam tries to achieve is to
combine these two tasks of batch
processing and stream processing so
getting correct result in one program
model yeah and we have done that at
Google for many many years we donated it
to the Apache foundation and today I
going to talk about what does it need
like what kind of semantics does it need
to be able to do stream and batch
processing in one token model
oh yes sounds interesting so and when
when you're using an app what are you
typically trying to analyze at the
stream level compared to batch
processing and what was an example of
the use case so for instance what you
could analyze in an app is if you
realize that a user doesn't know like
where to go and app you see like a delay
between user interactions and your
mobile app you might be able to guide
him to something you might be basically
highlighting areas where you think this
user should go so you basically analyze
what most of the users do how they use
your app and if you analyze the stream
from a particular user you could see
like okay this user seems to be stuck in
my application maybe I have to highlight
certain areas to give them guidance
where to go okay so how do you I suppose
I'm discussing here the biggest
challenge then things like how do you
get the data out that is relevant like
how do you categorize your users how can
you tell the users the first-time user
versus an advanced user where do you
start within the matter yeah so usually
like if you look at these things so you
have for instance signup data in a
database and you can take this data and
read this data into your streaming
pipeline so you have this basically as
an S insight input and
then once your event data comes in you
check basically against that data that
you have in a database and see like okay
what's data set user and have I shown
him that already or like things like
that and what do you find the biggest
challenge when you're implementing
implementing it so the I think the
biggest challenge with with Apache beam
is to really understand that new
programming model which like the
semantics of differentiating between
event time and processing time so this
is like one of the core there are many
really nice things about Apache beam but
that's one of the core different shaders
to other libraries that are out there
and so if you talk about processing
versus event time is for assessing time
means when an event reaches your
pipeline and it's ready for processing
but that can be rail ATAR than when the
actual actual event happened so we
differentially differentially
differentiate these two times in a way
that our event an event happened versus
when an event is processed and that way
we can get calculate correct results
even so we do a stream processing</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>