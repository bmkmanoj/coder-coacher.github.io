<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Continuous Optimization of MIcroservices using Machine Learning by Ramki Ramakrishna | Coder Coacher - Coaching Coders</title><meta content="Continuous Optimization of MIcroservices using Machine Learning by Ramki Ramakrishna - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Continuous Optimization of MIcroservices using Machine Learning by Ramki Ramakrishna</b></h2><h5 class="post__date">2017-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zhjrfBemz8w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I assume many of you have already attend
a bunch of dachshund the micro-services
track of which this is a part and what
I'll be talking about today is some of
the problems with respect to performance
that arise when you have a microservices
architecture and how you might leverage
modern machine learning approaches
towards getting a handle on the only
performance optimization problem
my name is romkey ramakrishna I work
with Twitter I work with the platform
engineering team at Twitter and I'm part
of the VM team did Java Virtual Machine
team they're the work that I'm going to
be describing today is actually joint
with a with colleagues with a bunch of
colleagues at Twitter Joshua Cohen
Joanne Xiao Lu was actually a summer
intern who spent last summer working
with us he's a grad student at Purdue
Joshua Cohen is a team member in the
maces Aurora team at Twitter Chris
regard o is an SRE with the acai team at
Twitter and Alex will Chico was with the
Advanced Technology Group which is part
of the Twitter machine learning and
cortex teams and this work wouldn't be
possible without the collaboration with
colleagues in a bunch of teams at
Twitter with that preamble let's dive
into the problem here's a picture of the
wheel that you see over here with with
those little you know annotations which
you can't read along the circumference
are all of the micro services that make
up Twitter so when you send send out a
tweet and it lands first with the to the
front
and then gets routed to a bunch of
services which will do various things to
either persist that tweet help it
populate timelines of people who are
read where your followers and so forth
so the the effect of of that of that
single tweet is in fact to orchestrate a
whole bunch of other actions which are
performed by other micro services that
this first service will call upon and as
a result of that lots of things might
happen and before before your your tweet
actually appears on someone's timeline
they might be a bunch of other services
that might need to to do their job for
example the database of the storage
layer might need to persist your tweet
the social graph will need to find out
who your followers are so that their
timeline can be updated with your tweet
maybe if you have if you being a twitter
user have a lot of follow a lot of
people it might have to do some
filtering in order to figure out whether
this tweet should appear in your
timeline and so forth suffice it to say
that there are a lot of you know serv
micro services that together make up
their Twitter experience and to give you
some numbers there's roughly on the
order of a thousand or so service micro
services running within Twitter in terms
of the number of service instances so
every service might itself be built out
built of a number of instances usually
horizontally scaled which will which
will which will process your request and
so if you look at all of the service
instances for all the services that are
running there they might be close to a
million or so service instances running
and you can think of a service instance
as a process or a maybe a small set of
processes these all run and Twitter's
data centers and the data centers
themselves have Hardware varying kinds
we might be running Intel processors of
a certain generation and then as we
upgrade our data center they might be
racks full of new hardware and so at any
one time there might be instances of a
specific service that might be running
either on new hardware or older hardware
in addition there might be software
upgrade going on when you have a you
know or close to million instances of a
thousand services and these are being
upgraded
you know weekly or quarterly or whatever
at any time there might be some
instances that are running an older
version and some which are trying a
newer version of the software as as
upgrades are slowly rolled out so there
might also be situations where your
hardware is if your OS is being updated
so for example I might be I might have
half of my data center running a version
of Linux and then half of it is being
upgraded with a newer version of Linux
in other words there's lots of change
that's always happening in the data
center moreover each of these services
that are running in the data center
might require might have different
performance characteristics so some
might be might require if for example
they were running Java some might
require a smaller heat others might
require a larger heat some might make
use of native memory others might not
and then the services themselves will
make use of for example Java platform
and different things and so there is no
one-size-fits-all solution that can be
rolled out in a data center of this kind
which is built out of Micra services
here's an here's a picture of a of a
typical or you know a performance stack
at Twitter in this case this is a
microcircuits written in Java and so it
runs up on the JVM the the hardware it's
let's say x86 hardware
running a Linux kernel and on top of
that we might be running Mason's
containers so each of the services might
be running in its own container and they
might be some amount of resource and
cross container isolation provided by
that container framework within each
container there might be one or several
JVMs in this case I show two different
containers running on the same Hardware
each of each of which is running a JVM
now if you look at the stack the the
performance of the micro Service some
performance metrics some figure of Merit
that we can't care about is shown by the
by the function f but F itself is a is a
is parametrized by various tunable
switch are provided in the layers
beneath many of you have used the JVM
you know for example that the JVM offers
lots of tunable knobs those here are
shown by the by the parameters j1 j2 and
so on the microservice itself might be
might provide certain configurable
parameters s1 to s2 and so on and the
settings of these parameters might
affect the performance F which we care
about in addition the lower layers might
have other tunable parameters and so
what I'm trying to show over here is
that in order to optimize the
performance of the service there might
be a lot of tunable you might be able to
play with and that's the problem that we
are we are trying to solve here let's
take the example of JVM ok the hotspot
JVM has you know close to here I show
something like with this version of Java
that I was running there close to seven
hundred and fifty seven or eight close
to eight hundred knobs
now not all these knobs actually affect
performance some of them are are to
adjust you know various aspects of
behavior and they may or may not affect
the performance of your of your micro
service but the large fraction of them
probably do so
and then depending on on the
microcircuits that you're running some
microservices might be sensitive to the
performance to the setting settings of
certain parameters others might not be
some of these parameters might be
dependent might be sensitive to Hardware
so they might work well a certain
setting might might be preferable on
certain hardware and new hardware a
certain other setting might be optimal
in addition the setting of one parameter
might have affect the optimal setting of
a different parameter in other words if
I if I tuned one log to get optimal
performance and I then I tune another
knob the combination of the two might
affect performance and so there's a lot
of mutual in the interdependency between
the parameters there are some of them
which are completely independent some
which are mutually interdependent with
something like you know 700 parameters
of which let's say 400 are tunable and
can affect performance trying to find an
optimal performance point is is a is
very difficult what do we typically do
well typically we we've looked at blogs
we've looked at postings we've looked at
Oracle release or Oracle release notes
people have performance blogs and so on
and we - we know some some parameters
are supposed to affect performance quite
a bit and so we can tune them maybe we
tried to tune a few parameters we run
some test loads and we find out that
okay with the setting of parameters I'm
getting good performance and so I think
I've tuned those parameters - and you
know push them out into into the data
center first of all doing this kind of
hand tuning is itself time-consuming
it's labor intensive and it's
error-prone so it might be the case that
I have tested my a my microservice with
a certain load and and in production
it's receiving a different load and so
the parameters that I set
in my hand tuning may or may not be
optimal in introduction with thousands
of microservices what often happens is
use you look at one you know you're
writing up a new microservice and then
you look at the parameters that I use
bias certain other micro-service that
kind of looks like your microservice so
what do you do you say hey this set of
parameters is working well for them my
service is somewhat similar in in its
shape as the other service so why don't
I just use those parameters and then
I'll run some tests loads maybe and and
it'll turn out to be a fairly good
setting and so I I start using that and
so a lot of parameters that we have that
that are being used by Microsoft are go
counted from you know settings that were
used in some legacy Micra services that
exist in my data center so at the time
at which with which I do my performance
tuning as a writer of the micro says the
developer of micro service the settings
that I find might be optimal but as I
upgrade my if I as I make changes to my
micro service I fix bugs and so on those
parameters may not no longer remain
optimal because of the frequent upgrades
of all of the layers of the stack and
especially the application that the
Micra service layer as well as the JVM
layer which are you know probably
changed much more frequently as well as
the OS layer and the network stack and
so on all of these upgrades make the of
any optimality that we achieve be very
transient and fleeting you know within a
couple of months of having tuned
something it'll probably be obsolete and
no longer be optimal and so our
hypothesis is that many of the micro
services that are running in our data
center are actually operating below
optimality and optimality here can be
defined in different ways you can think
of you know power consumption resource
usage you can think of the the
throughput did get the microscope is
giving the number of requests per second
it's serving and so on you can define
your your criterion for optimality and
with respect to any of those
at any point in time a large number of
mike services are probably running below
below optimal setting so uh here I'm
going to be talking about our attempt to
to leverage machine learning to try and
solve this problem with thousands of
micro services and lots of upgrades
going on all the time and heterogeneity
in the hardware and in the software
stack it's very difficult for anyone for
any team of people to be able to to
really make a dent in this problem right
and so how do we leverage machine
learning for this let's look at
performance optimization as a formal
optimization problem okay so we are
looking at a function f which is a which
is different whose domain is an n-tuple
and a vector of n variables over over a
domain X so X is the the the Cartesian
product of wherever x1 through xn range
so there could be real real values they
could be integer values or they could be
enumerate in terms so each of these
parameters x1 to xn takes the value from
its domain so for example I could have
the first one might be a heap setting
and I might have heat settings that that
go up in in units of megabytes or units
of 256 megabytes or so on I might have a
garbage collector choice which might be
an enumerated setting which by the
picture GC or cm or a concurrent
mark-sweep GC or a garbage one GC and so
on and then there might be other
parameters that that will that will
control the behavior of whatever GC that
I think they could also be things like
what what compiler JIT do I use and what
are the performance parameters of that
compiler of that jet and the the
optimization problem is to find a
configuration which is the setting of
these parameters such that the objective
function f is maximized
so it's a purely you know mathematical
optimization problem and it turns out
that most of when we when we talk about
performance they're often a whole bunch
of constraints that we place on
performance we can't you know we can't
say that in other words we can't say for
example that oh you can use a heap of
any size and that's kind of make GC go
away forever because you're using an
internet or a huge heap so they might be
constrained for example there might be a
constraint which says that the heap size
must be below NGB okay that might be a
constraint that I place and I place
other constraints for example x1 and x2
here might represent the size of the
young generation and the heat size and I
there's a constraint that the heap size
the Nugent the young generation size
must be less than or equal to the heap
size in fact it should be strictly less
than the heap size for any reasonable
JVM setting or I might have a constraint
that says that the tenuring threshold
that I use must be must must take a
value between 0 and 15 because that's
what the JVM gives you so a setting of
30 for example it doesn't make sense I
might have more complex constraints for
example I might say something like
setting a constraint on behavior and
this here might be a W here in the third
bullet over here W X is less than K W
might be defined as some sensor sensor
reading that comes out of my of my
application so just like F is a
performance metric W is a is another
performance metric and I want that I
want to optimize F subject to W staying
the below a certain threshold so for
example I want to maximize throughput
subject to my to the 99th percentile of
my responses being below 5 milliseconds
I might say that every request gets a
response so Rx equals 2x over the over
the time domain over which I'm carrying
out my Miron so these are
kinds of constraints that we might we
might ask for in addition to these
constraints which at which we can
actually define in terms of sensor
readings that we can take they might be
hidden and uncontrollable variables in
the system so for example I can't
control the in a production setting I
can't control the load that's coming on
on the system right it might be that the
that the load varies by day by time of
day or by the season or there might be
certain spikes in the load my my
function might execute on different
kinds of hardware right and I don't as
as someone who's deploying the Micra
service I might not be able to control
which hard hardware platform it runs on
so this might introduce noise into the
function so if I'm looking at the
performance of the micro-service
depending on which hardware platform
it's running on it might produce one
value for F or or a different value for
F even with the same settings of the JDM
parameters right and if I if I'm if I
don't know how to control for hardware
the the value of the function f looks
like noise to me because or it is F
looks to be a noisy function to me they
might also be in inter container
crosstalk in the picture that I put up
there were two containers that were
running on the same piece of hardware
and because of the shared hardware let's
say because of shared network you know
metal buffers or network bandwidth they
might be crosstalk between the
containers and that's something I might
I might not be able to completely
control it could be that the paging the
the paging cache for the two containers
is common in which case you know if one
container hogs the page in cache then
the other might others performance might
be affected similarly for this kayo for
example so there inter container
crosstalk that might be present which I
might not be able to control for either
and so all of these will make the
optimization problem one of optimizing a
noisy objective function f
because hidden variables hidden
parameters appear to be noise so the
performance tuning problem comes down to
be due to the following steps right
first we have to pick up a performance
metric that we are going to optimize we
decide on the knobs that we are going to
be tweaking and then we use an iterative
strategy to tune these knobs that's how
a human would typically do it right this
is what the performance engineer might
do he says okay you know what these are
the most important parameters that I
have seen affect the performance of my
life service I'm going to pick that
those set of parameters to test
I'll pick values for each of those
parameters and I'm going to run my
system and I'll make some measurements
of its performance and then based upon
what I saw I might make a few make some
changes to the parameters and look at
the at the performance once again and
depending on how whether this you know
changing one variable or the other is
changing the direction in which I'm
moving about am i improving performance
or am i degrading performance I might
learn something about the shape of the
optimization surface if you will and
based upon that I might try other
parameters and as a result of that
through some process of iteration I
might actually arrive at what looks like
a reasonable reasonable set of
parameters that optimizes my performance
now with an automation assistant what we
are going to do is just basically
replace the performance engineer making
those changes and instead have a black
box tuning assistant so the black box
tuning assistant will be told what set
of parameters it can tune so it knows
which knobs to turn and it knows what
the constraints on those knobs are and
based upon those it will pick a
suggestion run an experiment which will
call an evaluation get back a value F so
I change X 1 through X X X n run the
system measure F check whether the
constraints hold or not and based upon
that I'll say ok now I need to look at
at another performance point so all of
this we want to be done by this black
box tuning assistant without human
intervention so how do we do this
using machine learning so the in the in
the industrial risk of operations
research and industrial engineering
literature global optimization of
functions as has been started for a long
time ever since the 60s because
optimization problems regularly arise in
in engineering problems and one of them
is called Bayesian optimization and
basically what it is is a method to
learn potentially noisy cost functions
iteratively and efficiently so basically
the idea is that I should be able to run
to search a huge search space so let's
say let's take an example let's say I
have 30 parameters that I'm tuning each
of them can take 30 different values so
the entire search search space can be 30
times 30 times 30 30 times right so 30
to the power of 30 that's a huge search
space we can't do a exhaustive search of
such a space because our experiments
themselves are expensive and they take
time to run and so I need to be able to
through only a small number of probe
points be able to learn the surface end
and navigate towards an optimum and
that's what Bayesian optimization gives
me the ability to do it can find optimal
very quickly on a wide variety of
responses they don't need to be you know
monotonic unimodal single global optimum
they could be highly nonlinear surfaces
and still it's able to nonlinear
multimodal and high dimensional
functions and it's able to find the
optimum fairly easily so here's an
example let's say we've done three
experiments okay which are shown there
by dots and let's say for simplicity we
are using a function f which which is
can whose values controlled by one
parameter so this is a function on a
single variable and the variable values
on the x-axis and the performance on the
y-axis there
and higher performance is better so
we've got these three values and what
might we learn from here well we might
think that okay maybe if I reduce the
value of the parameter maybe I pick
minus 4 it might give me a higher
performance because I think probably a
pair of parabolic trajectory fits it um
but let's see the true function is
actually something like this right we
don't know this if true function shown
here by the dashed line might be the
actual performance want a performance
surface and what we've taken is readings
of this kind so how is Bazin
optimisation going to work on this what
it's going to do is bayesian
optimization
essentially models models the function
as a as what's called a Gaussian process
so at any point for any setting of the
variable the value of the function is
supposed to be a probability
distribution over with some mean and
some standard deviation and the mu and
the Sigma are at that point is what the
Gaussian process is going to learn as
you give it more values so we start off
with this with this probability
distribution whose shape we don't know
it's kind of a very wide probability
distribution it's all we know is that
it's normal and then as we learn each
point we we refined our knowledge of the
MU and the Sigma for that for the
probability distribution at that point
okay so here's our here's our original
function and this is what after having
gotten three points this is what the
machine learn this is what the basin
optimization system thinks the shape is
like so the blue shape defines what the
basin optimization system has fitted to
the values that you've given it so
having gotten these three values how is
it going to pick the next value to to
evaluate because the basin optimization
system is actually going to do you know
to help you search for the optimum so
what
the next point to pick it doesn't do a
random picking of the point what it does
is it'll it'll look at the optimum the
best possible value that we've gotten so
far
and then it will basically take the
slice of the probability distribution
that lies above it because those are the
things which will improve performance
and what it will do is that every point
it'll take an integration basically it
will look at the expected improvement so
the improvement times the probability of
that improvement and it'll take an
integration across the vertical slice
over there and so for every point I'll
I'll get what's called an expected
improvement and what the next point the
probe will be one where I expect the
greatest improvement so I in this case I
will pick the point that is a little bit
to the right of minus two probably
around minus one minus one or so because
that's where the expected improvement is
maximized so really what we've done is
we've we've taken an unknown function
and we have we've reduced it to
optimization of of an expected
improvement based upon the you know the
the model that we have the Gaussian
process model that we have and so the
next point that we will take is around
minus one
oops let's see so some reason that
disappeared but anyway so we picked that
point the point to the right of minus
two and we find that we find what the
correct value is and then we repeat that
step we look at what the expected
improvement having having gotten to
third the fourth value basically all of
the probability along that along that
point collapses down to the reading that
we just did that we just took and so
that reshapes our model and based upon
that we can then look at the next
optimal point which turns out to be
between those two so we read that and
having taken that the expected
improvement over there has vanished and
we the search turns to looking at the
point to the left at around minus three
and so we take a reading at that point
and it turns out that we will have to
take something in between because the
model thinks that that's where the
optimum is and so having
found that that the reading at that
point the next iteration of of the
machine learning algorithm says that
okay you should look at at the point
that's between zero and two because
that's where an expected improvement is
maximized and so we take that and having
taken that having read that point we
find that in fact it turns out that it's
actually reduced performance so one of
the things is that since we don't know
anything about the about the shape of
the function and we're learning it as we
go there might be evaluations we will do
which will look at highly suboptimal
points of the performance space so at
this point basically I'll kind of go
through these quickly but basically we
prove another point we find that this
over there and then finally it turns out
that we have a one-point between 2 &amp;amp; 4
which seems to maximize the expected
improvement we take that and then
through another couple of iterations can
you hear me yeah right yeah so through a
series of such evaluations of the
projective function we will find the
optimum so that's how it works in a
single dimension and obviously as we as
we increase the number of dimensions the
problem becomes more and more difficult
and in practice for a human to do
because a human could have possibly done
this let's look at what the Bayesian
optimization as a service looks like at
Twitter so what we really want is we
build on we have a Bayesian optimization
service running as a service and our
performance optimization or you know JVM
optimization system will make calls to
this base in optimization service in the
manner in which I I described so this is
actually a when building this Bayesian
optimization system which was done by
the by the cortex team at Twitter what
they wanted was something that would be
easy to use it would require minimal
coding by by the user in this case the
user being the
tuning service it should support
multiple languages so that depending on
which language you're running your micro
service or your your tuning service in
you could call into the API is offered
by this Basin optimization service and
it should be able to run concurrent
experiments so you could run multiple
experiments at the same time so here's
what the API looks like so the scientist
is really the one that's running the
experiment scientist is the experiment
set that we create and we have a name
for the service for whichever service we
are tuning and then we have a way of
specifying the set of parameters that we
want to tune and for each of the
parameters we can specify a range for
example value a may be a integer type
that goes from a minimum of zero to the
maximum of two value C might be
so value C might be an integer goes from
zero to ten value D might be an
enumerated type that takes values red
blue and green for example and then
having set up the experiment you also
find us to make a suggestion which is
the evaluation that we're going to do
and we take that set of parameters that
the job here is the set of parameters
parameter settings for value a through
value D and with those parameters having
been set I test my service and when I
test my service it gives me back a
figure of Merit which is the performance
objective that we are trying to optimize
and having gotten back that value and
having run that experiment I feed that
value back into the into the machine
learning system so I ask it to update
that job that that experiment I'm
running with the parameter settings and
the value and the throughput that I
obtained and having updated that I can
go back and ask him to provide another
suggestion and so we kind of go through
this last three lines iteratively until
we find and finds up find a setting that
optimizes my system and this is really
how it was built there's basically an
auto scaling group over there so that
multiple calls into into the basin
optimization service can run efficiently
and scale according to the load on the
raishin service itself there are lots of
other approaches towards global
optimization and here we list a few
random searches an obvious one
instead of having any intelligence in
this in this in the having almost no
intelligence in the system we can still
do some kind of spraying you know random
search in our high dimensional data you
know space and be able to fire perhaps
look at and find something that that
that's at least better than what I am
running right now there are various
other approaches which divides up the
parsing trees one divides up the entire
space into into into basically it does
you know the equivalent of a walk down a
hierarchical tree where settings of
certain parameters will divide up the
space and then you'll search within that
sub space and there are lots of other
ways of doing this reinforcement
learning has been used especially
workers especially so by Google in their
data center cooling for example and
Bayesian optimization is preferred in
our Twitter for many of our machine
learning and optimization work because
it's we found it to be robust extensible
and it's been tested in the large number
of applications and it provides a fairly
you know narrow and elegant API to which
we can we can do the optimization and
here's some of the ways in which it's
been used at Twitter it's been used for
spam detection abuse detection the
various deep learning applications where
we run neural networks but the neural
networks themselves are tuned using the
optimization service so for example you
might pick some parameters in the
architecture the number of layers for
example the interconnections between the
layers what's the you know what what's
the aggregation function that you're
using at each layer of the neural
network and so on and then things like
Hadoop cost reduction reduction has used
has used based on optimization - good
good - good effect and I'm going to talk
about JVM performance input
in the stock and we these are some of
the something of JDM parameters that we
that we used last summer we did a
internship and summer intern did a
project in which he picked a handful of
of the parameters that we could change
so for example in this case I left you
know a few of them there were some 30
parameters and all that we changed at
one time and just to do a quick recap we
have the service with the JVM that there
the kernel and each of these settings
can be changed in our case we are
changing J 1 through J N and measuring
the effect on on the condo service
through measuring of some performance
metric F so as I as I mentioned there
could be other parameters that we could
tune but in the in the result in the you
know next few slides what I'm talking
about is just the J 1 through J n being
changed but there's no reason why s 1 to
s n or KK 1 through K or etcetera
couldn't themselves be subjected to the
same kind of the approach that we are
taking could be used at any layer of
boost of the stack so basically what we
did was we took a micro service which is
it's a large production service and it's
used to access user objects at Twitter
and this was picked because it was a
mature service that didn't undergo
frequent deploys which made some of our
prototype and proof-of-concept
experiments easier it ran a large number
of service instances and we'll see the
moment why that is important and the
setup for the proof-of-concept was
basically a Microsoft staging
environment so we didn't run the
experiments directly in production even
though I have been telling you that oh
in production things are different from
him from staging
we did use real production traffic which
was kind of tapped into the staging
cluster the reason we did this was
because we didn't want the staging
cluster to be redeployed very frequently
while we were doing our science
experiments if you will and the
performance metric that we optimized was
requests per second per unit GC cost so
in some sense if I increase the requests
per second which increases the
throughput that's a good thing if I
reduce the GC cost that's a good thing
so in
in each case basically the performance
would be would would increase if I the
direction in which both the performance
metric increases is where the value
where the system performs better
basically and this is how it was set up
basically we had the JVM tuning service
which is the one that is going to be
doing in orchestrating the system and
there's a certain number of instances of
the micro service running in this kind
of controlled environment and what we
did was we asked the Bayes observations
the little little pink rectangle at the
top right and we asked it for a
suggestion okay and having gotten a
suggestion we generate a JVM
configuration based upon that and we
upload the configuration to into a you
know file store service and having done
that we look at all of the baseline
platforms running in the system in you
know in my cluster and and and basically
we want to so we want to bootstrap our
test system the evaluation which is
running on shard number zero using the
settings that we've just gotten from the
bayesian optimization service and so we
restart that shard and then start
measuring performance but before we do
that we want to make sure that there's a
baseline against which we can can
compare and that baseline should be
running under similar conditions to this
one so that we are actually able to do
apples to apples comparison so having
done that we run the experiment which is
the evaluation for a fixed duration and
each of these these instances actually
is is has a lot of sensors which are
logging into the observable
observability and metrics service and so
all of as the experiment is running
these metrics are being collected in the
metric service and having run the
experiment for the for a reasonable
duration we then collect look at the
performance metrics that came out
the of the metric service and compare it
with that of the baseline and having
done that comparison we by taking the
ratio one to the other we know by how
much we improve performance and that's
that's a suggestion that we then send
back that's the that's the score that we
then send back to the base and
optimization service that's the value F
and then having gotten that we then go
back and ask the optimization service to
send us the next suggestion and so these
are this here is the trace of the set of
of the number of iterations we did this
was 30 parameters and one over here is
the baseline where we performed as well
as the original set of settings and so
you can see over here that there's a
large number of probes that we do during
the performance during the while we are
trying to learn the shape of the
function which turn out to be suboptimal
but there's a large number of them which
actually go and go and improve the
performance quite a bit and so towards
the right over there
around iteration 75 or so we've actually
found a configuration that's about 2x
better so over here but if you look at
the way things kind of the values of the
performance metric are distributed you
can see that there's a good you know
balance between exploration which is
somewhat like gradient descent credit
gradient descent methods use we're
having found the value and another value
which is better we try to kind of go up
a gradient versus exploration which is
where we go further out from where we
are and try to probe good performance
points so based upon that we've we
finally found that you know after about
80 iterations we found something that we
are satisfied with and we go with that
and here's basically what it looks like
in terms of the GC the performance ratio
plotted over the duration when we when
we ran the experiment if we look at the
request if you remember the performance
metrics for metric was requests per
second of a GC cost if you look at the
requests per second the the baseline as
well as
the evaluation gave you exactly the same
number of requests per second but in
terms of the GC cost itself the GC cost
of the optimized instance was much lower
than that of the of the baseline
instance and that's that's how we ended
up improving performance here's a quick
look at the changes that that had seemed
to help these were based basically found
by the optimization service without any
help from us and basically turns out
that the new generation size was
identical to what had been tuned what
had been hand tuned in the baseline
system it however decided that smaller
tenuring threshold and the smaller
survival space sizes were more optimal
it found that you wanted so in the JVM
there's there's some pre fetches of
objects that are done during GC and it
found that the interval for that
prefetch had to be larger than than the
default setting it also we also gave it
some old generation allocation filter
profile allocation filter parameters to
tune and it tuned them slightly
differently and it turned out that that
sped up promotions quite a bit because
you were allocating much more
efficiently out to the old generation as
a result it said that I should use more
GC threads than than we then we had
actually been using by default and then
it also said that the compilations the
method sizes which are subject to
compilation should be much larger so
actually this is not compilation size
threshold but inlining size threshold
should be larger so I should be able to
inline larger methods into into other
methods and the performance gains were
of course in the GC overhead and these
translated into a tail response latency
that was much smaller and in turn this
could be this could translate into lower
data center footprint because for the
for the same load you might be able to
run fewer instances and still be able to
serve the the same load into the system
so for a smaller data center footprint
you are able to serve
the load that you that you normally get
so the takeaways from the
proof-of-concept was that because of
hardware heterogeneity seasonality in
the ambient load load spikes things
could could kind of go awry okay what we
did was we took the optimal setting that
we had gotten in in the staging
environment and ran them in production
and we actually found that they didn't
run very well we also saw and I'll come
back to this in a little while we also
found that you know occasionally we'd
have very suboptimal suggestions from
the Bayesian optimization service so the
Micra service that you are tuning should
be able to or at least the architecture
should be such that one or two instances
if they perform sub-optimally or and our
service shouldn't get taken down and we
also found that in order to converge
rapidly to a to an optimum you need to
reduce the noise in the measurements so
there could be noise because of changes
in the environment platform or Hardware
heterogeneity could also introduce noise
in the manner in which I first described
the sensor then mice might themselves
inject some noise into the system and
the less the amount if we reduce that
noise we can get convergence faster and
there's a trade-off between the
evaluation lengths so it typically when
you have noisy measurements what you do
is you run the evaluation for a longer
period of time but of course if you run
the evaluation for a longer period of
time your convergence is reduced but
what we found that you could do parallel
evaluations so that you could run
multiple suggestions at the same time
and thus be able to to converge faster
even though you ran evaluations for a
longer period of time and all of these
can of course affect your convergence
speed as well as the quality of the
optimum I think I've probably talked
about a bunch of these what you have to
do is you know it should what affects
things is the choice of performance
objective the choice of parameters to
tune the duration of the evaluation runs
so if you make very short evaluations
they turned out to be noisy you have to
sufficiently long evaluation runs you
have to factor out hardware effects
because hardware can itself inject noise
into this into the into the experiment
so you might want to run experiments on
different hardware and keep them
separate you have to protect against
environmental and sensor noise you need
to use baseline configurations for
normalizing evaluation so if your
ambient load is changing and the
baseline and it and the evaluation are
subject to the same changes then by
taking the ratio of the two you can
normalize out that noise long-range
effects are important in other words if
you are experimental short you might not
be able to see things that happen
develop after a day for example and
there are various ways of looking at
this and one way is to actually open up
the black box and expose new sensors
which will look at the direction in
which this long range effect is adding
so that you actually have a read of that
and finally as I said staging is not
production because staging loads are
different from production loads
so in order to fix those problems we ran
evaluations on fixed hardware so each of
those was a separate experiment we ran
concurrent evaluations and then we used
production loads basically a third thing
of another thing that we did was we used
red line load so we don't want to be
able to be just testing our system with
a fixed load we want to be able to vary
the load and so we have a setup through
which we can do ramping red line loads
and then look at the cumulative
performance over that ramp up period and
that turns out to be a good way of
actually finding an optimum we want to
terminate obviously poor suggestions
early and because of the fact that we
have a good metrics collection framework
which can be fed back into the
experiment we can monitor how the
experiment is going and if something is
suboptimal we can terminate it quickly
and finally experiment sets is something
that we used in which what we do is if
we have done we has a history of some
experiments and we are going and we've
and we are changing our microservice in
some ways and want to find a new optimum
what we can do is
we use make use of the history and the
new experiments will follow the shape
that is somewhat similar to the previous
shape but different in some details and
so we can leverage the fact that we know
the history in order to make the
optimization process converge much
faster and the way we've actually in the
implementation that we are working on
for this auto-tuned service micro
services which are running within the
Twitter data center can opt into the
optic auto tuning so they can say that
ok
I want my service to be subjected to
this Auto tuning they can place
evaluation instances behind a load
distributor so that we can we can be
protected against load spikes which will
inject noise and slow down convergence
and then we can control the load profile
and the shape of the load which which
will we talked tapped in from production
into our evaluation instances we we we
specify the objective function itself as
a as a query on the metrics so that we
you know as we run the experiment
metrics are being collected and we can
run a query which takes a composite of a
number of metrics and maps that you know
which about the the query then evaluates
to a actual scalar real valued number
which is our performance function so you
could take a composite of a number of
metrics and try and optimize that and
these are all picked up picked by the
service which is which you are trying to
optimize
you can also set performance constraint
constraints which will take a sequel
query on the performance metrics and
yield a boolean which is whether or not
that constraint was satisfied or not and
so basically you're optimizing the
objective function for the first sequel
query subject to the performance
constraint which is a set of other
sequel queries and then we specify the
parameters to tune and their ranges and
the duration of evaluation runs this is
what the implementation actually looks
like in practice and this is basically
if you you know the the prototype that
we ran was written in Python this is
being developed in Scala but basically
the shape is somewhat identical the auto
tune services how to the right over
there it has its own little storage
which which stores histories of
experiments
experiments that are in progress
remember that the experiment itself
might be long running and experiment
sets need to be stored and so all of
that is done in that storage service
over there once you have reached an
optimum and are and you're happy with
the performance you push the
configuration to configuration service
so that next time the services service
starts up it can pick up these optimal
configurations and run with them and we
expect that when the thing is is running
in production that services will
automatically you know the services that
opt into the service will automatically
find their optimum and kind of redeploy
at the opportune time with those
settings so the conclusion here is
basically that the you know continuous
automated optimization is we knew that
it was is something that that we needed
and it is it seems as though it is
possible using Bayesian optimization to
actually make a dent in that problem and
mainly because with a very few very
small number of probes so in that
experiment with 80 probes 8 evaluations
we found an up a near optimum in a space
that was huge I mean it was probably
equal to the number of 30 to the power
of 30 which is as many as the number of
atoms in the universe or something like
that anyway a very large number of
values we are we are able to actually
find the shape find the optimum fairly
easily
and we hope that this automatic tuning
will actually push the efficiency on
envelope in our data centers there's
some challenge the challenges that
remain one is the dimensionality in
practice there are you know thousands of
parameters we put 30 of them tools today
are you know can can deal with sub with
some number of parameters but if you
increase the number too much then the
efficiency then basically we have a
slowdown it turns out that some of the
algorithms used in Bayesian optimization
have
Kubik complexity which means that beyond
about 50 or 60 you cannot actually use
you can't simultaneously tune so many of
them and so you need to have some
strategy for tuning say a thousand
parameters through some kind of
iterative in some iterative way you have
to of course deal with noise and non
stationarity which which are things that
we are working on in terms of
dimensionality I should also state that
more recently linear methods have been
found that seem to work really well and
so that's something that that the that
the cortex team is working on to try and
get these experiments to to be more
efficient and that's it if you have any
questions please yes
No yes so let me repeat the question for
the for the recording the question is if
you have 30 parameters are you tuning
them one at a time basically pick a
parameter find an optimum setting for it
then go to the next parameter final
optimum setting for it and so on
stepping through the parameter space and
that is like you know going in you know
along the coordinates and tuning them
one at a time no so basically what it
does because there what would what it
would depend upon is the pass through
the sequence in which you followed those
parameters would determine the value the
optimal value that you see which may or
may not be a global optimum so what it
actually does is it changes all of them
at the same time and some might change
others might not change and basically
based upon you would expect that it
would you know with 30 parameters you
might expect about 60 different you know
at least two different values for every
parameter might need to be checked and
so you would expect that you know you
you would have 80 60 different points
but in practice know in practice it
changes all of the parameters at the
same time and it goes all over the place
so they're all being optimized at the
same time if you will
yes yeah yeah good question so the
question is how do we know that we have
converged to what may be an optimum
obviously we don't know the shape of the
performance surface it's huge and so the
the value that I've shown over there
where the performance is two points um
two point one times more than the
baseline how do we know that's an
optimum we don't really know it might be
for X right as you said and so basically
what we do is we we use a convergence
criterion that depends upon basically
how many iterations have we gone without
improving performance and so you'll have
to use heuristics of that kind to figure
out that oh you've done a long enough
experiment and then there is not much
improvement that you see now we are
trying to expose API is from from the
machine learning system which will allow
us to if you remember we looked at the
expected improvement graph you can look
at the mass within that graph and if
that has shrunk below a certain level
then the tool might be able to tell us
that look I don't think there's much
improvement to be had here because from
what I learned on the shape of the
function there is very little you know
the expected improvement is almost
non-existent but obviously you know
tools can it so it all depends upon the
way in which we are regressing we are
regressing against the Gaussian process
model and your optimization surface
might have sudden dips which you'll not
be able to see there are parameters we
can tune of the Gaussian process in
particular and what's called a
covariance matrix which which basically
tells us how quickly the house moves or
how sharp the changes in the in the
system are and as you change values and
those could be exploited but right now
there's not an API through which we
could actually change that runs with a
fixed set which it learns over time
yeah so the question is will might
humans make better decisions than
machines I think with a small number of
parameters humans might be able to make
make decisions that are better than
machines it but it's kind of it's a
difficult question to answer because
there's a small number of parameters the
machine can actually run almost an
exhaustive search and find an optimum so
it can do as as well as a human with a
large number of parameters unless the
human has deep insights into into the
shape of the performance surface the
human may not be able to do very well if
the human does have those insights then
they can be used as constraints on the
optimization function and so you expect
that at every level as you're tuning
these these parameters you might call
upon experts to specify how you know
what ranges to use with these parameters
and so on and I expect in fact that the
micro-services the owner of the mic the
service owners will actually our each of
the stack owners will actually specify
what parameters to tune what ranges to
set but because the actual settings
which are optimal for some specific
application or service might vary you
don't want a human spending that time
you want the machine to automate to
automate that search and that's really
what this gives you because with a
thousand you know micro services there's
no very human a human a team of humans
can take care of this Thanks any other
questions
is this open is this available as an
open source library so the machine
learning system is not currently open
source it's derived from an open source
project that came out of Harvard and you
know to Toronto called spearmint and
spearmint is available as an open source
package and there's actually lots of
Bayes and optimization packages that are
available in the open source sig oft is
one and you can yeah so all of them
offer interfaces that are somewhat
similar to this so there are black box
optimization packages out there which
are open source in terms of the
auto-tuned service that we are building
we hope my hope is that we will build it
in a sufficiently abstract manner that
we can open source at least some part of
it so that you can adapt it to your own
setting but obviously it will require
some amount of specific configuration so
that that is indeed our goal any other
questions all right well thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>