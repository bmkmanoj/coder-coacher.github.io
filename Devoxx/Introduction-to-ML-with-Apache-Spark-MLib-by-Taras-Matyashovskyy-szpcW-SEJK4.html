<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction to ML with Apache Spark MLib by Taras Matyashovskyy | Coder Coacher - Coaching Coders</title><meta content="Introduction to ML with Apache Spark MLib by Taras Matyashovskyy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction to ML with Apache Spark MLib by Taras Matyashovskyy</b></h2><h5 class="post__date">2017-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/szpcW-SEJK4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah thank you it's time so let's start
thanks for coming this is a talk called
introduction to machine learning to the
Apache spark and a lip so as you may
have noticed the agenda consists of two
parts the first one is just an
introduction to machine learning for
everybody's curious about that there
were some decent talks in these days
about the machine learning the AI the
data science the deep learning so this
talk is more of introductory for you to
understand what's happening behind the
scenes and what actually learning means
and the other part will be related to
the Apache spark and I will try to
convince you the spark might be a good
fit for you to try it out as it's very
simple and it's very useful for machine
learning stuff so briefly about me I
come from Ukraine from the nice city of
Louisville's western part of the Ukraine
and I work for company called Logica
recently we were bought by a bigger
Ultron group when I caught a code
primarily in Java and I truly believe in
a technical communities that's why I
lead one in in live cold money at huaca
and I am proud to be a program committee
member of two great conferences in
Ukraine called G conf and XP days if you
are nearby if you want to speak just let
us know just make it clear right from
the beginning I am NOT a data science
engineer
I'm Java engineer as most of you are I
don't have any degree in that field of
knowledge so you might be curious why I
talk about machine learning I truly
believe that a modern software engineer
is far beyond just one programming
language a tool and machine learning is
currently very overhyped and we need to
pay attention to this to learn from this
and extend our outlook broaden our
horizons and try something new and my
motivation is the following there is a
strong belief that machine learning
career the data science is just
exclusively for data scientists who have
really deep mathematical background or
for Python engineers they have
very nice ecosystem for for engineers
who code in our in our studio use
specific tools like MATLAB and there is
no room for Java in this world so when I
show you it's my path when I switch to
machine learning give it a try and it's
a smooth path and I hope that if you are
not too worried about machine learning
it just curious and your Java engineers
and want to try out this can be a good
motivation talk for you so let's do
machine learning in 20 minutes
usually the slides that are going to be
next they are presented like in have
different courses like Coursera but
great course by Angie Angie or other
stuff so it takes two three months to
learn it in depth we will cover a little
bit briefly to brush up the terminology
that is going to be used
afterwards and that will help you to
like spark ml Lib more because it hides
complexity then I want to show you
beforehand so what's actually a machine
learning machine learning is a study of
computer algorithm that improve
automatically through experience what
that means it means that the more
experience we get from the data we want
our algorithm to learn from the data and
become a better one usually the machine
learning is a very very broad concept
and it's divided into three major
categories or problems that we want to
resolve first one is called the
supervised learning so in this case we
assume that there is a supervisor that
helped us to label out our data set so
we actually know the the right answers
for the problem and we want to learn and
find a mapping between the input data to
known output data and then generalize
our knowledge our mapping to unseen
examples the other broader category is
called unsupervised learning as you
probably guess there is no supervisor in
this case there are no correct answers
given and algorithm it's on its own to
find some hidden structure in this data
and the third one is called
reinforcement learning in this case
algorithm behaves in a dynamic
environment and wants to achieve some
goal for instance drive a car or play a
game and it is given a
reward we are punished it's given
feedback we are punished
we are rewards and punishments I don't
have time to cover all of this so it's
going to be a first brief introduction
into supervised learning it's very nice
for us as a human being to have this
analogy from our real life so in this
case we all are we all were children at
some point in time and we all have a
supervisor our parents who helped us to
learn this world so for instance we seen
one example and correct answer was given
the next one the next one the next one
and we were able to learn for instance
to classify cats from dogs numbers car
makers whatever so this is all about
supervised learning
usually when supervised learning is
being explained it's all about hi see a
housing price prediction it's very
overused and corny example so let's
imagine we are on the conference so
let's let's imagine we have a speaker's
feedback data set this like imaginary
data set above the speakers and
feedbacks about their talks and they
have given for like last couple of years
so in this case we have many attributes
like a date and time conference named
speaker some final impression was it
good or bad some rating etc so using
this data set let's brush object the
glossary is being used by machine
learners so first of all it comes to
features and a target variable so in
this particular example those liked
attributes of talks they're called
features and usually represented by X
wise depending on the business problem
it's very important to understand that
we want to actually have a business
answer so for instance we want to
predict impression after my talk so it's
going to be either one was it good or
zero was it negative so in this case the
Y is called target variable having
enough data we can form a training
example taking into account one record
about one talk and one impression
combined together having enough examples
we can have a big training set then
knowing this having this training set
and
learned about machine learning
discovered some algorithms we might make
a prediction what's the hypothesis
between the input data and the output
might be what make you feel good about
this stock the way I look some I don't
know jokes I used the slides decoding
whatever and then we want to actually
have a referee to say how good our
hypothesis is how good is our fit to a
data so in this case this referees just
called a cost function this is a
terminology may be a little bit boring
but bear with me is going to be
interesting later on so let's learn the
basic supervised machine learning
pipeline usually it consists of the two
major parts the first part is called
training so we take our input this all
records with feedbacks above their
speakers then we extract what's what we
think is important for our prediction
let's this usually is this stage is
called the feature extraction and we
have eventually the features then
knowing the label for instance was my
goal was the stock like good or bad zero
or one we combine this into a big
training set then we make him hypothesis
and select a particular machine learning
algorithm we want to train so we fit in
the training set into that machine
learning algorithm it's being if it's
good if our result was successful is if
training error is low if the test error
is low then we have our model then it
all comes to the prediction part for
instance I want to predict result of my
talk today so I take the same input I
extract the features in the same manner
I have feature extracted I put them into
machine learning model and hopefully I'd
might get a prediction about my target
variable for instance zero my talk will
be good it will be bad or one my talk
will be good I hope this is all
straightforward there are two major
major problems that relates to the
supervised learning the first one is
called regression let's imagine we have
number of jokes that are used during the
talk versus the speaker's rating this
is called regression because we want to
predict the real-valued output as rating
might be a real value for instance as in
divorce application it starts from one
to five so let's imagine I have this
data plotted on this scatter plot so
what's there what's the mapping might be
I'd might use some quadratic function
and say it's like this I might think
that it's a linear dependency between
the number of jokes used and the
speaker's rating so in this case when we
have such sort of this is the easiest
example the linear regression that
calculus behind the scenes is like this
so you might be a little bit concerned
right now I think oh my god is this Java
conference yeah you are on the Java
conference still where where's me I will
explain you this a little bit briefly
but you will actually understand what's
learning means so in the linear
regression we our hypothesis as a linear
function from our input features
represented by ex-wives and the unknown
here is our parameters data so using
different parameters data our linear
hypothesis might change so as a result
we need to referee so for the linear
hyper linear regression the cost
function looks like this this is a
well-known cold squared error function
so using that our actually learning
problem is being transformed how it is
transformed instance of the actual task
of the learning is transformed into a
finding of the minimum of the cost
function having those unknown parameters
theta when we will solve this problem
the minimization problem the parameters
theta will be known and as a result when
we will take unknown the features of the
unknown speaker and just use those
parameters theta that we calculated
before then we will have our hypothesis
what I want to say here about this
briefly a little bit of mathematics on a
Java conference sorry for that there is
no magic is just a math the machine
learning is actually transformed in this
particular case in finding a minimum of
cost function so the learning process is
actually process of finding a minimum of
a particular function and there are a
lot of decent algorithm that can be used
to solve this problem the gradient
descent is the most popular one the
other the other problem we want to solve
is as I mentioned before it's called the
classification in this case the
differences is that we want to predict
the the discrete value to output that
impression might be a negative or
positive based on the number of gjokaj
used so let's imagine if I will joke few
times then it's going to be negative but
if I'm just long urban cut then after
some quantity of jokes the impression
about my talk will be positive so in
this case we want to solve this classic
versification problem if you will try if
you will try to use a linear function
like this you'll notice that this is a
bad fit for our data so we need a
function that looks like this and our
hypothesis needs to be like this so
thankfully there is a function that is
between 0 &amp;amp; 1 and has a similar shape
it's called the sigmoid or logistic
function and as a result we have the
most commonly used the classification
algorithm it's called the logistic
regression the naming is a little bit
bad here because we are actually it's
not a regression it's a classification
problem but due to historical reasons
it's like that so in this case the
situation is very similar our hypothesis
is a little bit different but still the
concept remains the same it depends on
an unknown parameters data and the cost
function looks a little bit more complex
but it's actually not like that and
behind the scenes once again in order to
learn the algorithm to find those
parameters data we just need to solve
the minimization problem of a different
cost function find those parameters data
then take the example that we want to
classify use the same formula and have a
probability between 0 &amp;amp; 1 is this is
this talk for instance going to be
positive or negative
usually the tragic hold is 0.5 but
we can play around with that so once
again it's no magic
it's just calculus it's just math behind
the scenes and there are different
different algorithm that were developed
by engineers around the world I just
mentioned the two simplest ones for you
to have at least a feeling what I am
talking about now let's go to an
unsupervised learning as I mentioned
before unsupervised learning in this
case there are no correct answers given
and algorithm is on its own to find some
structure in the data the most common
example used to explain unsupervised
learning is clustering let's imagine I
have that data I've used before and I
will plot time spent on life coding
versus number of jokes used by speakers
and I want to find some coherent groups
of speakers that behave in a similar way
if I use the number of clusters for
instance five my algorithm might find
this sort of structure in this case like
top on right there can be speakers who
code a lot and make a lot of jokes that
might be a Josh long this one can be the
speakers that code last but still make
wrote a lot of joke like one card those
speakers that just have slides without
life coding and are very serious geeks
may be like me and etc but if I change
the number of clusters I'm sorry the
clicker stopped working sorry for that
if I change the number of clusters to be
two then the structure can be a little
bit different so in this case we might
have coders and me I have all the
presenters that's like this and the most
famous and popular algorithm used for
clustering is called k-means so usually
it's like that we introduce some k
cluster centroids then we assign each
example to the closer centroid and then
we move centroids as a mean or average
of the examples assigned to that cluster
and the steps second and the third they
are repeated until clusters and choice
not
no not longer move so as a result we
call this that the algorithm converged
so but what I want to say one more time
even for this unsupervised learning
example there is an objective function
that we want to minimize one more time
so once again the the task of learning
is actually transformed into tasks of
the finding of the minimum of the
particular objective function and there
are different topics that are covered by
this area there another one is animal
did animal detection but I am not
spending much time on this so a little
bit of theory bear with me it's worth
spending staying right now and learn why
I like spark for that reason knowing
that theory another topic I want to
cover and I hope it was not covered on
other by other speakers it's called the
model selection and a hyper parameter
tuning so it looks very easy so we need
just to train model and we'll be fine
and a lot of like newbies like I was a
couple of months ago was like hey I like
the scenario when I have no training
error so my model is perfect and as a
result it it's actually not zero error
to distinguish one class from another
one so usually this problem is called
overfitting or we say that algorithm has
a high variance and this is very wrong
is the same as you want to pass the test
your driver's test and you just learn by
heart all the examples but when it comes
to a real driving you are stuck because
you just learned the correct answers
that's why usually such sort of the
algorithms they are very bad on
generalizing the examples they have
never seen on the opposite side there
can be an algorithm that has a high
training error and this problem is
called under fading or we say that
algorithm has a high bias so this
obviously is not the way we want to go
so what we want to go is go like the
appropriate fitting somewhere in between
that usually the algorithm can has some
decent
training error that we can couple with
but it still is able to generalize on
the unseen examples and in order to
achieve this there are a lot of
methodologies like regularization hyper
parameter tuning but for the sake of the
demo I just want to talk about one it's
called the k-fold cross-validation so
what's happening having all this like
data set we split it into a particular
set of folds that and for instance taken
as an example k equals two free we might
have a three different combinations we
take that two-third of the data as a
training set and we learn and then take
one third as a test set and we evaluate
our algorithm then repeat this procedure
three times and have an average metric
of our algorithm and that helped us
displ it helped us to actually avoid
overfitting and another good thing about
this if you have some hyper parameters
in our module using these different
combinations we might try out different
hyper parameters and choose the best
ones i know that you are tired so it's
enough of theory and let's go to their
more applied stuff i hope that all of
you are aware about the apache spark
it's another hot topic it's like the the
most the most active big data project in
the world starting from the middle 2014
there are a lot of talks about it and it
was mentioned few times on the
conference it consists of the many
components and I will talk right now
only about AMOLED that's obviously
related to the machine learning
algorithms so what's cool about that
it's a part of the SPARC ecosystem it's
a library of machine learning algorithms
and utilities designed to run in
parallel on cluster so the another
advantage is that it's library out of
the box helped us to run those algorithm
a different note I don't know how you
but I'm a Java guy I mostly applied Java
guy and I like abstractions I don't want
to deal with those like hypothesis I
don't want to deal with those cost
functions I don't want to implement the
gradient descent on my own so that was
the intent of the creation of the ML lib
so they created the new data types to
represent the training examples and
unless the vectors dancer sparse and the
label points that unites their vector
with the particular label and those all
algorithms that I mentioned before they
are implemented using spark abstractions
the most commonly used spark
abstractions are resilient distributed
data sets and just a new one data says
that came as a replacement for for the
data frames so what's inside spark ml
lab it consists of two major packages
the first one it's a little bit outdated
and it's currently cold in a maintenance
won't
it's called spark ml lip and it's built
on top of our DDS and the second one is
more more than one is called spark ml
and builds on top of their data sets
what's features are there the utility
the linear algebra the statistics the
the media and the mean calculation of
the vector it's all implemented for us
we just call it as a as we usually do
just call to their code the call to the
method and it calculates date for us it
has a lot of nice feature extractions
and feature transforming for instance we
want to transform text to to to the
vectors it's all in there
we wanna yeah that's the the most
commonly used example we want to
denormalize factor or normalize it so
it's all there there are regression
classification and clustering algorithm
implemented for us we just make a call
and use them from the applied
perspective and many many more like
collaborative filtering and
dimensionality reduction but the thing I
like the most is SPARC ml because what
we usually do it like history repeat
itself someone invented cool feature in
a c-sharp then Java decided to implement
it then it was about functional
programming so of the scholar appeared
most and took them some part of the
market dance then Java engineers noticed
cool features in Scala so currently we
are reimplemented some functional
paradigms in Java so what happened like
the
the data science word and the machine
learning world it's currently all about
Python so that's why we borrowed as if I
may say so the concept of the pipeline's
from the library called scikit-learn
it's very popular mount Python engineers
and we can represent our thinking logic
using those pipelines and the cool thing
is that we can actually persist it so
instead of performing calculations all
the time learning and actually calling
we may learn then save the module and
then just read when needed and all this
complex stuff like the model selection
and tuning for instance we are k-fold
cross-validation it's all implemented
for us too so we don't need to manually
split the data set into a training and
test calculate the matrix and etc it's
all implemented for us so of this look
like we have some raw data for instance
about the speakers then we'll transform
that using spark abstractions so we can
drop a column at the column do some
simple feature engineering etc then
there can be an estimator in between
some intermediate model in between that
can be trained
it's called estimator in terms of spark
ml then we can do a different sort of
transformation once again come to our
final model that makes our business
value it's once again will be estimator
and then all ends up with a cross
validator that tooks our pipeline our
evaluator and our parameters and adults
all that magic and learning stuff behind
the scenes for us so I think that's
pretty cool and the I showed you theory
just because for you to appreciate that
and enjoy that's all hidden behind the
scenes and we as a Java engineer can
really easily start out and try this out
so what I did I was just wanted to play
around I have a lot of simple examples
in my git repository for you to give a
try after all but I have just this idea
I really like heavy-metal music I've
been to a lot of nice concerts in US and
Europe mostly and I wanted to think hey
is there any difference
- in the heavy metal in pop in terms of
lyrics like in terms of music and sound
that's really obvious but in terms of
lyrics so my idea was like this like
giving some a random verse from lyrics
with the simplest algorithm be able to
recognize genre so taking into account
this particular lyrics would somewhat
gasp is this Pope or heavy metal well
you do think that certain if you will
look for a particular words it's not
because I put a Michael Jackson lyrics
into my training set so and Michael
Jackson used like such an thrilling etc
so if you will focus on a particular
words it's not as easy as it seems so
I'm rolling thunder appearing crane and
coming on on a hairy gain so it's
actually ac/dc hells bells so for some
of you so what was my thinking pipelines
so I collected a raw data set of lyrics
they're available on the web and I
haven't spent much time on that just a
65k of sentences as this is a supervised
machine learning example I split those
into two different sub categories
I took not what I call sorry for being
subjective nice pop music into one
category and the second is like nice
heavy metal music into another category
so I created this big training set and I
can show you on my laptop in a couple of
minutes as I know the correct answer as
I am the supervisor I'm mark the heavy
metal to be zero and pop to be one and
then I've trained to logistic regression
there are a lot of algorithms that can
be used for that but as I explained you
logistic regression before and this is
the simplest algorithm for everybody to
understand that's why I use that so the
curiosity might be the spark machine
learning pipeline so I took a road data
set then I've added a cleanser so I
removed the punctuation I remove the
empty lines etc so I am currently
designing the pipeline as I think as a
data science engineer then I added to
numerate
to numerate the lines of lyrics you may
be curious why and why I'm doing that I
will explain it a little bit later then
I split like sentences into tokens into
the particular words then I remove
stopped words because I don't want to be
influenced by the words like I me your s
etcetera so I remove that from my
training then in order to do stemming a
stomach and this is a process of
shrinking the words in order to have the
same root there is a like not the best
stammer used in spark right now so I
needed to add some mechanics to explode
my dataset into a particular rose and
then I used stammer so as a result all
all similar words have the same root
then I United the sentences once again
and then is the biggest problem what
would be the feature what would be the
feature that I might train on so we all
know this it needs to be a numeric
vector so I need to transform a text
into multi-dimensional vector but the
complexity is what's the the minimum
unit that I need to use it can be one
line it can be two line of lyrics it can
before it can be eight it can be sixteen
it can be the whole song that's why I
introduced a bursar that has a hyper
parameter called sentences inverse and I
play around with that so it's able to
combine two lines into one feature four
lines
eight lines 16 depending on my wish so
then it all comes to the feature vector
is Asian there are a lot of different
algorithm B
to transform text into a feature vector
on the talk from from my colleagues from
Harvard I think it was on the first day
of the conference they mentioned the
count vectorizer that that can be used
they used the back of words to do this
there is another approach in SPARC aft
of the box it's worth two back it's the
small neural network from Google and
it's able to transform words into a
multi dimensional vectors and
as a result they have another parameter
called vector size for that to play
around 200 300 100 and the cool thing
about this that's the words that are
similar in sense they recite very close
in this multi-dimensional space then I
pass this data set to the logistic
regression and you as you might guess I
have a cross validator model at the end
so this is the the simplest demo and I
will show it you right now but you might
be curious
it looks very complex I have data set it
transforms transforms transforms what
actually was was my work and and it's
really nice to say that all that in
black it comes in spark out of the box
so we as a java engineers it's just
building blocks for us to create the
pipeline and all that's in orange that's
I just United together to to make this
like a clue to my pipeline and it really
reminds me there slice from the
beginning of my talk so it actually
represents the supervised machine
learning pipeline so enough of talking
let's let's have a quick demo the source
code is pushed and there will be a link
at the at the bottom of each slide so
you can just check out and play around
so I have my model trained it took some
time to train it I tried different
combinations of the hyper parameters so
on my laptop
it took about I don't know 30 minutes 40
minutes to do that so it's actually
trained and persisted what I'm going to
do I will just call it and what I like
it the most is just the way it does it
just for me a simple call to the method
I have a yeah I am showing something I
don't know why it's not showing
they have split this place
it was turned as a mirror I know that
I'll try one more time yeah sorry for
that this is a spring boot application
and it has no UI it just has the exposed
HTTP endpoints and I've what I did I
just have prepared some lyrics for you
to make a call what I hope is going to
be a live demo so you may also tried
different lyrics so what I have here is
the same 8 lyrics from the ACDC hells
bells and just I am making the call to
this to this API so what's happening
behind the scenes I am loading this this
model it's stored on a file system
pre-trained and i providing the output
what it's like so it says that I've
loaded my cross validation model these
are my hyper parameters that provided
for me the best results and I can see
here that my accuracy it's about 95
percent and I the best results were
achieved using the sixteenth lines of
lyrics and there are the words used in
my work to wack vocabulary and the
vector size was 300 so as a result it
makes predictions it was different
probability so currently it's 73% for
heavy metal 26% for pop having Trisha
Kult of 0.5 it makes a prediction that's
a matter that's why it says here like a
matter if I'll take the same helble's
but provided more stuff because the best
results were achieved with the sixteenth
lines of lyrics and I will send it one
more time let's wait I am reloading the
model so my metal probability increased
and currently it's like 86 percent so
I've tried different once if you have
any like your favorite song you may say
to me and I can give a track
[Laughter]
baru one of the string class okay let's
try let's try some song conventual a dog
because I think Java dog is not fun yeah
how it's called beetles
okay I'm googling for that I am not
prepared so I am googling it's a live
demo I'm copy pasting as this oh like
it's 16 it's perfect so what do you
think
so it's like says pop 99% anyone else
like it's not a perfect model it's like
I want you to to motivate you to check
out the code to play around and it's
very simple one yeah Bruce Bruce
Springsteen how it's called borned okay
okay I see sorry
let's take less for it to fail like 9:00
what do you think it's like it says
metal but it's like only 65% if felt if
I take more I'll try to take more I've
used just a binary classification I can
add more I can add country I can add
blues it just takes more time so it it
decreases so like having more lyrics
means that it looks more like pork so
I've tried like a Justin Bieber and Ozzy
Osbourne and the the weird thing is
about rap like if rap is great it's
about all social Marik's and kind of
stuff it's a split of the model it
usually says like sixty to forty or
forty-five to fifty-five but back to the
code wanted what I wanted to show you
can you see my screen this is the
pipeline if you if you paid attention to
my slides my slides actually represent
my coat and my coat represent my slides
so I have a pipeline that actually
starts with a cleanser numerator stop
words remover and ends up with a
logistic regression and it's Allspark
abstraction the pipeline and all those
transformers and estimators at the end I
have a cross validator as I mentioned
before I have a number of K faults
equals to 10 and I have my parameters my
evaluator and my pipeline and what's
great about that is this I put a
parameter grid and I don't know what
might model
we'll look like was the having different
set of the hyper fur amateurs so I'm
trying to guess I'm saying
please spar give it a try for a vector
size forward to back being 100 200 300
try making the versus from 4 8 or 16
lines try maximum iterations for the
logistic regression 100 or 200 and etc
yes when I am training so this is all 1
and training and it Eddie tries out all
different combinations of those
splitting each time into tan folds so it
actually does a lot of job for you and
this is the thing I like the most so
it's all hidden behind the scenes all
those hypotheses all those minimization
problems collecting those metrics etc so
that's why I like ml lab for a free Java
engineers to give a try it's really easy
and straightforward but the other thing
I like it's a prediction path so when
the prediction is happening my my lyrics
that you've seen in a postman they
follow the same path it's all once again
started from numerator cleanser etc and
and it's up so their pipeline use for
training and the pipeline used for the
prediction it's actually the same the
only difference is that it's stored on
the hard drive and being grad all the
times
in order in order to wrap up with just I
currently show to you so this was the
initial lyrics the first step that the
eight lines then I did a cleaning I
removed the punctuation then I removed
stop words as you may see there are no
articles no prepositions like that then
it's all about stemming so rolling it's
currently roll the pouring crane is
currently just pour etc then the the the
most complex on the curious part is
actually the vectorization so and as a
result ending up of the of the word to
wack I have a multi-dimensional vector
that represents a chunk of lyrics then I
train their logistic regression well the
logistic regression provides some
probability and based on the probability
and the and the threshold I I can I make
a decision so summary about that the
machine learning the start is really
quick everything is out of the box what
you need the the basic algorithms the
feature extraction for tax the the
statistics the evaluation the stuff like
that the scalability and performance is
out of the box so you get all those
algorithms run in parallel on your spark
cluster currently you haven't seen that
but it can be done in cluster in
parallel and simple machine learning
algorithms they work really great the
other thing if you are in a big data
world if you have a big data application
you might have you are actually using
spark and you have like a streaming
application or you're just using the
spark seeker etc and you want to add a
flavor of machine learning of the AI
into applications for instance the user
entering their website you want to allow
put it to a particular market segment
and depending on then bar
it segments provide a different add or
provide a different content and provide
a different layout so you might train
your module in background and use this
as a part of your streaming application
or another part of your application and
integration is very very easy of course
there are some some concerns about that
some negative parts there are limited
features if you had visited talks of my
colleagues about that it's all about the
neural networks the deep learning that
the neural networks that have a lot of
like hidden layers and are trained
usually on GPU so SPARC after the box
it's not about that it has just one
neural network implementation it's
called like multi-layer perceptron
classifier but it has really tiny
customization it has just a few
activation functions and and it said it
has like limited data types so currently
if you had visited once again those
talks it's all about my
multi-dimensional vector is like and
they are called tensors so out of the
box SPARC doesn't provide that so in
order to do that there are some
different additional projects
integration with standard flow or
integration with the deep learning for J
and deep learning for J can be executed
on a spark cluster but after the box it
doesn't provide that and another thing
that annoys me Lord there is no feature
parity between the spark and a leap
package and spark ml and sometimes you
switch between those one think is
implemented there and other thing is not
but it
they say it's at least for now the
current version is 2.1 and in version
2.2 they are promising to have a feature
parity and as a result the ml lip is
going to be deprecated currently it's
just in a maintenance mode and we will
use the ml I am I wouldn't sell you
SPARC as this and say hey use that for
machine learning etc there are any
different nice tools and libraries that
you might use and there are a lot of
alternatives I just want to mention just
four of those there is an old-school one
called Weka that came from University I
cannot recall which one
and Kok there is a library called
aerosol from Airbnb and there is a big
competitor of spark currently especially
in a streaming application is called
flank
so flank introduced its own machine
learning library that's called flank ml
so my motivation and my main purpose of
the stock was not about that so I will
just wrap up for a second machine
learning is overhyped you might want to
switch to Python to give a try or you
want to install the art and our studio
and give a try but we are all a Java
engineers so machine learning it's
usually it's not above some magic is
just above calculus is just about
formulas the matrixes their products and
etc of those matrix and vectors so you
need to understand what's happening
behind the scenes and when you
understand what's happened behind the
scenes you can really appreciate the
work of the fling kemal or spark ml lib
or vodka or an cog that does all that
calculus for you behind the scenes and I
just wanted to show that if you have
knowledge of spark or spark is already
in your in your prep in your project you
might give a try and create a very super
fancy spark ml pipeline that would work
out of the box pretty well give a try
for a machine learning as a Java
engineers we have enough of tools and
libraries for that we are not in a
Python world but maybe in future thanks
to other speakers for instance they
offer on and the main contributor to the
deep learning for J we might have our
own robust ecosystem too so thank you
for coming to my talk I hope it was
useful and interesting</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>