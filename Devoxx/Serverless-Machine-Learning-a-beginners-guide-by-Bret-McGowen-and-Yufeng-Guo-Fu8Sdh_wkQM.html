<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Serverless Machine Learning: a beginner's guide by Bret McGowen and Yufeng Guo | Coder Coacher - Coaching Coders</title><meta content="Serverless Machine Learning: a beginner's guide by Bret McGowen and Yufeng Guo - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Serverless Machine Learning: a beginner's guide by Bret McGowen and Yufeng Guo</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Fu8Sdh_wkQM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right good afternoon everybody
welcome my name is Yu Feng Guo and I am
a developer advocate at Google focusing
on machine learning in the cloud can I
get a show to show of hands here who
does like machine learning of some sort
either at work or on their nights and
weekends for some the definition of
machine learning awesome thanks cool and
I'm Bret McGowen
I'm also a developer advocate at Google
I'm in my focus area is server lists so
similarly is anyone doing server list in
production right now ok not too many um
is anyone kind of in the research phase
what what what is server lose how can it
help me cool is anyone hoping just to
learn a little bit about what does
serverless mean in this session ok
excellent so cool so we make kind of no
assumptions about knowing anything about
server list or about machine learning
there'll be a gentle introduction to
both so since these are our two sort of
areas of focus we decided to do a talk
on how we can make machine learning go
serverless you have no idea how many
times we practice that so if the rest of
the session is terrible this is why cool
so let's get started so I am NOT the
machine learning expert so I will
endeavor to teach you sort of the
layman's guide to what does it mean to
do machine learning and how is it
different from maybe traditional
programming models so briefly machine
learning is using examples and
experiences to train a system to answer
questions rather than explicit or rigid
rules so you might normally write a
system that knows in this situation 2x
in this other situation do Y and machine
learning turns out on Tom's head a
little bit and say here's a bunch of
examples of X or a bunch of examples of
Z and then it can learn on its own how
to make those decisions so by way of
illustration let's try how we might
program a traditional system to do some
image detection so we're gonna do some
human-powered image detection so say
we're writing software we're building a
system that we want to use to tell the
difference between an apple and in
orange so if I'm writing some code what
are some things we could maybe look
and these two images to tell them apart
what's someone having ideas color I
heard anything else shape okay so these
are some pretty good examples of maybe
some things you could look for so color
is a common one so maybe we count all
the pixels or we look at all the pixels
in the image we count the number of red
ones and we count the number of orange
ones and if there's more red than orange
it's probably an apple but right away
we'll run into a problem because what if
we're given these images well now our
system no longer can tell the difference
between an apple and an orange so then
we might fall back to another rule that
we've come up with like shape so they're
both roughly round so maybe that can't
it can't tell the difference in this
example so maybe we'll do something like
texture so if it's bumpy then maybe it's
an orange if it's smooth it's an apple
but then there's all kinds of other
situations we can get into where our
rules start to break down so then you
get into a situation where every time
you come up with an example that doesn't
fall into your rules you can't have to
rewrite your system or add another role
and you're kind of continually chasing
this goal of telling difference between
an apple and an orange and at the end of
the day you just have a finely-tuned
system that can tell the difference
between an apple and orange and not much
else so if I now an add a pair to this
mix I got to start all over because it
shares a lot of the characteristics of
those other things and it's green and
also sometimes apples can be green so
you can see how these rules are pretty
rigid sorry how these rules are rigid
and can be broken as we need a more
flexible system so sometimes I get asked
well I kind of want to do these these
rules but machine learning seems really
complicated there's kind of some
statistics and there's math involved so
do I have to do this in every situation
what if counting up the red pixels and
counting up the orange pixels actually
works in my use case so let's take
another example let's say you have
something like like a dog and like a
cupcake or a muffin and I want to write
a system to be able to tell the
difference between these these so let's
play that game again what are some
things that can look forward to tell the
difference between these yeah that took
a long time I don't does this looks do
these look similar to you guys this is a
maybe it's too early in the morning so
shape is one they have two different
shapes eyes hair
some things you can look forward to kind
of tell the difference so you might not
need to use machine learning because
it's really different or it's really
easy to always be able to tell the
difference between a dog and a muffin
right or maybe you want to tell the
difference between a dog and a mop hmm
or between a croissant and a sloth maybe
so as you can see doing doing image
detection it very quickly becomes a lot
trickier than you might expect
so luckily we have machine learning to
the rescue and we use machine learning a
lot at Google to not only tell the
difference between an apple and orange a
pear a dog or mop but also to do to
using things like self-driving cars so
it's really important to know is that a
stop sign is that a pedestrian is that a
traffic cone because your car's gonna
behave very differently depending on
that so we use machine learning for all
these things so for today's talk we
actually put together a machine learning
demo so we can walk through the whole
process end to end of developing machine
learning system and then using it server
lessly to train and so since I am NOT
the serverless expert here I will
endeavor to show you guys the service
demo so what we have is a system where
you can drop images in this case
handwritten digits into a storage bucket
in cloud platform and it will try to
figure out what number it is
and we also wrote up this handy little
visualizer on the Left which will
hopefully show the prediction so two
works five should work zero should work
right zero well I'm actually just
curious you fan can you show us the
algorithm like what is it actually doing
- oh that's right what are the secret
sauce in this server the forint we
actually added a button here for show
algorithm because we figured this would
be good to show so machine learning
magic magic cool that's a oh are you are
you about to drop that seven in there
mm-hmm
yeah we trained our digits on everything
from zero through nine all the digits
yeah so we remember how we split up
words
you were gonna teach it to recognize
zero through six and I was gonna teach
it to recognize seven through nine yep
that's how we split up the work right
you know how like I was supposed to
train it to recognize digits seven
through nine do you see where I'm going
with this you didn't do it I forgot to
do this come on man
I mean maybe it will work anyway there's
no way if we didn't show it examples of
sevens it's gonna think the sevens come
other number ah so thinks it's a two
this is really embarrassing
I apologize everyone it's a huge event
DevOps Belgium I love coming to this
event it's humiliating if we could all
just pretend that that is a seven maybe
nobody tweet out epic Google demo fail
so I don't get fired because I really
don't want to get fired because I'd love
to come back you know what's that since
we built the serverless pipeline to
update our training with new training
data let's just pull in all our 7s from
from my desk here and upload it and run
the training and then we'll just like
talk about how this works for the rest
of the session by the end of it
hopefully it'll be updated and this will
work wait so are you saying we could use
the server list training pipeline that
we built to live train this in front of
everyone to teach it to recognize sevens
to fix my mistake yeah something like
that what a really convenient mistake I
have made in this demo that works out
really well maybe you should get
promoted instead of fired yeah clearly I
shouldn't be hired as an actor because
that was not that great but you know we
tried so we just saw that we could train
our model with 5s and it works for
example and 7s not so much right
sometimes it's a - sometimes it's a four
sometimes it's zero yeah it's kind of
weird that we already had a slide that
illustrate the mistake that I just made
huh yeah somebody doing some really good
slide editing in the background if only
it said two instead of zero be more
convincing so training new models today
can often be very complicated and
importantly manual updating your
training data and then splicing a name
with your old stuff making sure it's the
right format then you got to upload it
to someplace to store the data then
batch out just the new data that you
have right and put that somewhere and
making sure that everything's at the
right scale and you can scale it up as
needed right there's no bottlenecks in
your entire
haven't even started training yet so
then you train and update your model
which is no easy task you might have to
start over you load in the old
checkpoints and then once you're done
doing that you need to export that new
model then deploy it at scale oh that's
assuming that model is of the right size
and that you're serving system can do
the switchover smoothly without losing
traffic and then finally you have
something that's standing but then you
haven't tested it yeah it's okay I'll be
honest I kind of lost track that seems
well complicated in manual I stuffed a
couple manual steps in there a lot of
drag-and-drop
yes you will so what we want to do is be
able to just add some files and then
have new models out the other end right
not just new models but new models that
are deployed to production now we saw
that not many people were using
serverless in production so maybe we can
we can change that today so let's do
that I got some 7s here are some
examples there's just all sorts of ways
to write sevens it turns out some people
write a 7 kind of just like a little
curve on a road maybe this this kind of
looks more like a 9 maybe or a 4 I don't
know who okay this person is mark hurts
we're going to pick used crayons yeah
come on that's like the Loch Ness
monster yeah really dragon over here yes
you people aren't even trying with these
sevens yeah and then let's see what else
do we got I got some really into this
one this is not a seven lightning bolt
mini lightning bolt yes very good this
one lost itself
we got a boomerang here it's gonna come
back and hit us later so yeah there's
some crazy sevens so let me go ahead and
upload our our sevens and we have eights
and nines as well kick this off and
that's gonna trigger a whole slew of
usually live training oh yes of course
we should see what that looks like
so eventually our files will upload
because uploading things over Wi-Fi is
always fast all the time and never slow
so we uploaded data and we'll look at
exactly how all of this is happening so
we're doing some training now so getting
started getting going
and so there's our seven eight and nine
s we already had zero through six which
we can see on the bottom there and yeah
I guess for the rest of this talk we'll
kind of go over what's really happening
under the hood so with our machine
learning workflow of how we go through
this kind of manual process rep you want
to walk us through kind of how don't get
this done sure yeah so just at a high
level so first we need to sort of
process the data we need to clean it up
so if you have data coming in from all
different kinds of places maybe it's on
different formats you want to kind of
standardized format so that you train
consistently and then also so that it
represents data that it's gonna see in
the real world next we need to store
that data somewhere so we need to save
it participer sister to disk and then
what we need to do is take sort of the
new the new data that is training on and
then use that to update our existing
models so that it can now either
recognize new digits in the example of
our handwritten handwritten numbers or
maybe just do prediction better because
it now just has a wider amount of data
to choose from and then finally once
it's finished training we want to take
it and then deploy it so we can actually
use that model to make predictions so it
can answer questions in the future like
is this a seven or is this a boomerang
or or two or whatever so and one of the
things I want to add is here is that
like a lot of times you know people talk
about all these wonderful workflows for
doing machine learning and you know
we're talking about one today but often
times especially with these kind of
production workflows and pipelines in
reality you end up doing a lot of this
stuff manually first right in a small
instance a test case and you do this
more or less locally and then you start
scaling things up and moving things into
the cloud running things in a serverless
way until your entire pipeline is
serverless
so we're not saying by any means that
you should start your very first step by
just immediately scaling everything
right there is a way to over engineer
all of this but once you have this kind
of reproducible workflow
why not automate it might be a good
software engineer and automate all the
things and so that's how we can kind of
do this serverless pipeline to do
you do it locally and then you do it in
scale so to do all of this we are
powering all this machine learning using
Google's open source machine learning
library called tensorflow
yeah out of curiosity can I get a show
of hands who it has let's do the most
versatile tensorflow a couple of people
here who has used tensorflow
in any way including just a hello world
simple experiment okay so we need to get
more of those hands right a lot of
people have heard of it but haven't had
a chance to play around with it
so let's let's take a look at a brief
intro of just like what tensorflow code
looks like to help you guys who raise
your hand as having heard of tense flow
but hadn't had a chance to play with it
yet right test flow is both good for
research and for production so it's
great for this prototyping work that
local step and it's great for running in
production so this works nicely so you
have to be converting your code or
writing a different language when you
try to move something to scale so I
think I have oh yes there's one more
thing before we go to look at the code
tensorflow for the few hands who didn't
come up for what is tensor flow tensor
is a multi-dimensional array just like a
matrix an array of numbers lots of maybe
you have nested right right and then the
flow part means that these arrays of
numbers are flowing through a
computational grasp what is the
computational graph something like this
you have a two and a three that flow
together into an addition operator and
yield a five and so tensor flow does
everything through this notion of graphs
so by way of a quick example we have
this is you know not a tensor flow
tutorial or tensor flow talk writes a
serverless machine learning talk and so
this is just like a way for you to see
you know tensor flow code does not take
a gajillion lines just to do something
so we have we import our tensor flow
code and set some random seeds I like to
print out the version just to make sure
you know I'm running the version I think
I am and then in this example of course
we're pulling on those handwritten
digits so we'll read in the data set
and then tensorflow utilizes the idea of
input functions to pull their data in so
if you have your data and there's some
files somewhere and then you have your
model that you're trying to train and do
stuff with you need a function in
between that pulls in this data and
actually injects it into your model so
that's what the input function does and
in our particular case for those of you
guys who work with Python and use numpy
there's actually a handy utility
function that just takes numpy arrays
numpy is a library that helps give good
tight representations of arrays in
Python so that it's memory efficient and
you can just pass numpy rays directly in
and tensorflow will take care of
batching that up for you and it comes
with all these handy arguments to let
you specify batch size so how many times
we want to go through a data or a number
of epochs and whether why don't you want
to shuffle now you could write all this
code yourself right like yes we can all
write code to shuffle data around but
it's nice to have it in the in
spunctional-- it with a flag and move on
with your life so the next thing we want
to do is create our feature columns and
our model so in this case the first
thing I made was just a simple linear
classifier this is marginally more
impressive as a result than just fitting
a line to some data a straight line and
feature columns are just a way of
telling the model our our line fitting
how many kind of sets of data we have so
in this case it's a 28 by 28 image that
has been flattened to be 784 pixels wide
and so we'll load that in and attach it
to our model and then we're ready to run
the training and you'll notice that I
you know haven't written thousands of
lines of code here I'm not flying
through tons and tons of code to get
this done so training is two calls right
we have a train and we have a evaluate
so we train on our training data set and
then we will evaluate how we did using
the evaluate call with our test data set
so we want to have this data that's not
part of the training to see how we did
our accuracy so our linear model yields
92% it's not the most complicated data
set in the world even with all those
Loch Ness monsters boomerangs and weird
shapes you can still do
well there's lots of examples apparently
many people write their sevens like a
dragon so it recognizes that but we can
do better right 92 percent we can we can
get from an a-minus maybe to a solid a
or a plus and so you can use a deep
neural network classifier and that's
also just the one line piece so we'll
use and will create that model and we'll
run the training you'll notice the code
looks eerily similar to what we saw
before because a lot of this syntax is
very consistent across the API which I
really like and so we'll run that
training and that's going and going and
going and we can see that with the deep
network and only you know training this
brief period of time we've already
achieved almost 96% accuracy so call
that a a-plus ish and then once we have
both models trained let's do a
comparison right how did the two models
do so our linear model for the random
kind of set of prediction values it was
supposed to get 3 9 9 8 4 and it
predicted 3 4 9 8 4 so it missed this
second one so now the question is can
our deep Network do any better well with
an extra couple percentage points of
accuracy we see we do indeed have 3 9 9
8 4 and so you know this is one proof
that yes I did train two models and
they're different and to that I was able
to do this kind of in parallel without a
ton of code and if you for those of you
who are curious this is what the numbers
looked like 3 9 9 8 and 4 what's kind of
funny to me about this is like this 9
was one that was incorrect right it saw
this one in thought was a 4 and this 9
it got correct but I would argue that
this is a worse written 9 than the first
one so I don't know what the deal is
there machine learning is still
sometimes mysterious or magic yes thank
you
magic
and so that's my way of kind of brief
overview of tensorflow code how
straightforward it can be and how how
little code sometimes that that will
give you some real results that you can
actually see and then do meaningful
stuff with and Tessa phones not only on
my laptop here it also runs on all the
other chips and hardware that you would
expect including Google's kind of ASIC
specialized trip called a TPU or a
tensor processing unit and that's
specialized to hardware to run your
machine learning operations and this
yields great speed ups compared to GPU
now this is only relevant if you have
like huge data sets or really
complicated models but it's it's a nice
thing to to have available it also runs
on Android iOS and Raspberry Pi so for
those of you who are tinkerers or like
to make apps there's kind of something
for everybody with tensorflow and that's
kind of what's really also exciting
about all this machine learning magic
and I think sorry I think when one thing
I want to point out is we're gonna talk
about in the rest of this talk like how
to do your training in the cloud but
then also how to do your prediction in
the cloud so training on all those
digits prediction being answering the
question is as a seven or an eight but
in the mobile use case or the PI use
case what you can actually do is do all
of your training in in the cloud so
you've got all these thousands of images
so you want cloud scale to be able to
scale up automatically to handle all
that data but then when it's done it
produces this model and the model files
you can actually take and put on your
Android device on your hand a bio s
advice or on your Raspberry Pi to to do
on device prediction so your device
could actually be accessing the camera
for example and saying what is this a
picture of what is this picture of all
on device without having to go to the
cloud or use any network at all so even
if you're not actually doing your
prediction in the cloud this I think
serverless workflow we're about to walk
through can also be useful for you to
create a version of your model that runs
on an Android or iOS or raspberry pi etc
device it's great elaboration like it
alright so now that we've talked about
kind of the platforms that it runs on
what is test for what does machine
and how to run it on mobile random
little side bonus material for the
training portion of things we will run
our machine learning in the cloud right
and in our particular pipeline we'll ask
you to run predictions in the cloud as
well we didn't bring any devices with us
here unfortunately and the particular
name the platform we used is called
cloud machine learning engine and that's
Google's kind of hosted tensorflow
offering so if you have tensorflow code
you push it up to machine learning
engine and then you can grab a coffee
have a sandwich and let it rain so it
runs on both the training side and the
prediction side when we're done doing
our training we can take our exported
model files and give it to cloud machine
learning engine again kind of the other
side of it and say make me a
auto-scaling prediction service that has
a REST API security global the globally
accessible access I guess and have it
all work kind of end-to-end which is
really nice the question though is how
do we hook it all up and that's where we
get into kind of how we put it all
together so let's think about the pieces
of our application that we need to make
all of this happen so we need for
example a file system because we
literally just have a bunch of images
they need to live somewhere so when we
do our training in the cloud it needs to
be able to pull that now we need a
database so that we can take our data
the raw data we need to munge it we need
to clean it up we need to get it into a
good format so it's consistent for
training so for that we need some kind
of database and then also we need a
machine learning system of some kind and
traditionally these are all done sort of
manually like if you have storage you
need to go go run to some network attack
store attached storage you need to do if
you need to do like geo redundancy it's
a huge pain you need a handle networking
same thing with database you need to
create a database you need to work and
worry about sharding and clustering and
handling indexes and all that especially
when you talk about machine learning in
general the more data you train on the
better your predictions will be so you
want to get huge huge data sets but that
means from like a database management
and maintenance perspective that can be
a huge pain in the butt because you
really
just run it necessarily on one machine
that you that you keep up you may have
to have a cluster of machines and that
just brings with it a whole bunch of
issues luckily this is where server list
comes in because we need to take all of
these systems and we need to we want to
just run them on their own without
actually having to manage the underlying
system so when I talk about server list
it means a lot of different things to a
lot of different people
but for the purposes of this talk it's
gonna mainly be the first three so one
is you don't manage any servers that
means you're not saying hey create three
virtual machines that are running you
know bun two 16:04 that have this
version of these libraries that have
this version of my application do load
balancing between them as we scale it up
you're just sort of not aware of its
underlying servers as a concept at all
you could just continue and how it runs
the other thing is that it auto scales
so as you put more load either you're
training on bigger and bigger data sets
or when you talk about prediction maybe
you actually have a trained model that
you are exposing externally as an API or
internally to as an API to your other
systems so you need to actually scale up
because maybe you have an e-commerce
website then it needs to do some kind of
pricing prediction or something so you
may have to get a lot of load or you may
have a very variable load and you don't
want to be in charge of actually scaling
up those systems and scaling them down
you want that just to happen
transparently let the cloud provider
worry about that
and then finally sort of only pay for
what you use and what that means is if
your code isn't literally running at
this moment you don't pay anything and
this changes a lot of the economics for
a lot of systems because tech traffic's
tends to be really spiky so if you have
an enterprise application that you're
running at work for example your
overnight usage may be zero so in a
server list world you actually don't pay
anything all those systems scale down to
nothing so it's a really really nice
sort of advantage and we think about is
this machine learning pipeline we're
building you actually have those spikes
where you need to train on some huge set
of data but when you're done you're
probably not training for a while maybe
not to the next day or the next month or
whatever so you don't actually pay to
have all this infrastructure up and
running you want it to just
automatically and transparently scale
down to zero and I
using it and scale up when you need it
so the the fourth one event-driven is
going to be specific to kind of one type
of server list which we're going to talk
about which is functions to the service
and this is how we can connect all of
these pieces as we're about to shift
from sort of like the high-level
concepts of all these things into what
are the actual pieces and how do they
actually talk to each other and work to
each other work with each other so the
three sort of server list products that
represent what I was just talking about
file system and we need to get bogged
down per se these are just sort of the
the versions on Google cloud that you
can use that work service Lee for our
needs so the first one is cloud storage
so this is file storage system so this
is sort of this this general place you
can store blob data we can automatically
have it be geo redundant if you want to
automatically h-how'd available in
different geographic regions
it just kind of happens all behind the
scenes the second one we'll talk about
is bigquery so we saw it surveil is big
data analytics warehouse tool so this
can let you just jump data in you don't
have to worry about indexing shardene
clustering backups it's just sort of
this big black box of database but
what's cool is not only does it handle
and ingest all that data server lessly
but to query you just write your
standard sequel queries and it'll split
up your data and process it in parallel
and combine the results and this has the
effect of letting you be able to query
gigabytes terabytes and even petabytes
of data all just using normal sequel all
without managing any infrastructure and
you can get back results in seconds or
in minutes things that might
traditionally take an entire day in fact
quarry actually came out of an internal
system we have a Google called Dremel
where it was taking longer than a day to
process a day's worth of ads data which
means you're basically slowly and slowly
getting farther and farther behind so we
created the system called bigquery to
let you query just using normal sequel
massive amounts of data so anyway not to
get too deep into it but just so you
know sort of what's happening there and
then the third one which you think
talked about is cloud machine learning
engine so this lets you run your tensor
flowed code that you're writing on your
workstation and it scales it up for
training and scales it up for prediction
automatically and then to connect all of
these we have another serverless
technology called google cloud functions
so these are little lightweight
functions that run the
cloud typically typically you think of
them as one function like it takes some
set of inputs and returns some output
and these also scale these also scale up
and down as needed and I talked about
event-driven earlier so these are sort
of the key because cloud functions can
automatically run when certain things
happen in the cloud so for example a
file gets uploaded to cloud storage
let's say a handwritten 802 a cloud
function can automatically run in
response to that and do some kind of
coordinating coordinating or processing
so this is we're going to use to tie
these all together so let's talk about
training so this is the part where we
take all of our sample data and we train
on it so this is what our pipeline looks
like so the first thing we need to do is
process the data which it kind of talked
about clean it up we need to save it to
the database in this case bigquery we
need to now take the new data so we have
an existing model it already can do
predictions on for example 0 to 6 and
now I want to train it to teach 7 so we
need to batch those new 7s to Train on
and then once that's done training then
we need to deploy it so we can do
predictions on our new universe of zeros
all the way through sevens so let's talk
about kind of the literal action that
happens here so I mentioned cloud
functions can automatically run in
response to to cloud events so here
we've got file storage list events so
you can write this little function and
say hey automatically run this function
whenever a file is dropped in this
particular storage bucket in the cloud
and cloud Google cloud platform will
just take care of coordinating all that
for you so we dump let's say three
thousand two's up into cloud storage
well it will spin up three thousand
cloud functions each running on its own
Verta file that is processing all in
parallel transparently and once although
that's done it scales all back down to
zero so again sort of on demand
execution and pricing is very very cool
and very powerful so then the next step
is we need to process or clean up our
data so admittedly what we're doing here
is ridiculous so this is let me explain
why so we're actually taking an image so
there's handwritten - and we are turning
it into columns so the way we do that is
we take the grayscale value and that's
sort of this integer value and we do it
pixel by pixel
so the first
twenty-eight pixels at the top of our
image we're gonna turn into the first 28
columns of our dataset and then the
second row of twenty eight pixels we're
gonna turn into the second row of twenty
eight columns and the value there is
just the grayscale so zero being black
255 being white and the reason so this
is kind of the silly thing that we're
doing because since our flow actually
understands images on its own like we
don't you don't need to do this per se
for images but the reason we do it is
because a lot of people have used cases
that aren't just image based so they're
doing predictions on sort of analytics
data in your system and so so you have
an e-commerce website and you want to be
able to make predictions about
recommendation example engine is a very
common example this user based on his or
her browsing history or purchase history
you want to recommend certain products
for them to buy so you would train it on
your existing corpus of data you know
what your users have looked at before
what they purchased before and what they
purchase in the future and so you can
train on that well that data is
typically columnar it's in some kind of
sequel type database with explicit
columns that you want to train on so
where is the user region they're their
age you know whatever sort of their
their demographics are its its explicit
columns so we're just sort of mimicking
that so that hopefully you can be a
little bit more relatable when you think
about an on image use case so we're just
turning it into columns for that purpose
if that makes sense and if you guys have
more than seven hundred eighty-four
columns maybe you should consider having
fewer columns it's a lot of columns
already and so if we have all these
steps right how can we organize them now
into a sequence of serverless steps now
my research has shown that audiences
enjoy looking at colorful rectangles so
that's why we chose to kind of describe
it in this way it's very pleasing to the
eye so as Brett talked about the first
step we do is that will convert our
pixels into these columns and the way
that will happen concretely is we'll put
these images into cloud storage and that
will cause a trigger to run these cloud
functions cloud functions always get run
via a trigger of some sort in this case
it's going to be kind of this on file
add event
and once the cloud function runs we'll
have this data saved to our database in
this case bigquery into those columns
and then the question now becomes how do
we get the next batch of data out right
we want to bash the new data into some
file format in this case we'll put it
into a CSV file just all the new data
that has come in since the last time the
training ran but there's no way to
trigger this right there's no no event
really happening it's just files are
coming in and then at some point maybe
once on the hour once a day you want it
to go and so what we did was we use
Google Apps Script to fire an HTTP
request to trigger a cloud function so
that's the second way that you can
trigger a cloud function is via an HTTP
request of some form and so this cloud
function will run a sequel query to
export out a new set of CSVs
conveniently when this CSV gets export
what happens it drops into cloud storage
which we can now use to trigger another
cloud function so that's what we're
going to do to train the model we're
gonna say well this fellow dropped in so
we're going to trigger a cloud function
and execute our tensorflow code in the
cloud so that model already knows look
in that bucket and say hey there's some
new file here that has been dropped in
here for me let me update my model when
tensorflow finishes training it exports
a new model and that's gonna be a file
called saved model TB where's it gonna
get saved to cloud storage and so when
the file lands in cloud storage what's
gonna happen it's gonna trigger a new
cloud function thank you and that's
going to we're gonna use that to push
our trainer model on to the prediction
service and so then we'll have a fully
scaling automatic auto scaling
prediction service and so what's really
cool about this is that we dropped files
in right all the way up there note per
left we drop some files in and then it
did its thing it did its thing we got a
sandwich we enjoyed the sandwich and
then now we have a fully auto scaling
prediction service that's been updated
to work with our new data and this just
happens by itself once you have this set
up this process that nor
we would take by hand a lot of work to
make sure everything's the right way and
things fit together we redid the spiel
so I won't do it again for you but you
can imagine how if you have a
reproducible workflow that you know
needs to happen over and over again you
would naturally want to automate it in
some way and to do it in this serverless
manner means that it can handle much
greater load especially when it comes to
burst load maybe you have more traffic
one hour than the other so the exporter
file is sometimes 10x what it is in
other hours to build your own system
that's coupled together and potentially
brittle can lead to some interesting
kind of bottlenecks and having things
fall over but by doing this way you are
scaling horizontally and preventing
bottlenecks as you work through the flow
which is really nice and again if
there's too many hexagons on this screen
you can just remember colorful
rectangles and so this is really just
like the the true work flow right it's
like these steps no pre-processing
saving that data batching it out running
a training and then deploying that model
the hexagons just have to be our
implementation of it so once the
training is done we're gonna want to do
some prediction or we have this model at
Auto scales it's awesome but how does
the prediction look well thankfully it's
a lot simpler much fewer hexagons we
have our seven this seven looks okay
let's say and we drop that file into
cloud storage like we saw at the
beginning right so now you guys know
what happens when we drop a file into
cloud storage it triggers a cloud
function and that cloud function is
going to call our prediction service
with a REST API giving it the values
from that image and hopefully with any
luck yield a seven instead of two and
voila
right and I want to point out here that
we can also call our cloud function from
another mechanism right we don't have to
keep dropping flowers into cloud storage
every time we want to make a prediction
until the end of time we can also make a
web call right we can make an HTTP
request to trigger that cloud function
so this means that you can call it from
a browser a browser on your phone and
on your phone an IOT device your little
satellite in space know if you'd launch
satellites to space your underground
submarine no no submarines okay I guess
now we have come to it at last
the perhaps great question of our time
really is Brett's job safe am I gonna
get fired were we able to retrain this
model to recognize a seven if not and
you're hiring please see me in the back
of the auditorium you may not want to
hire me after this demo fail but we'll
see all right all right so here's our
seven again doing a show to see visually
to see if it finished training just to
make sure yes
let's let's complete our visualizer look
at that it's new model deployed and
ready yay
I'll be honest I just search for this
animated gif for a long time so I just
wanted to make sure it got on screen oh
yes yeah savor the shape of the gist
okay so I'm gonna go ahead and delete or
actually I can just overwrite at
previous seven so it's gonna prompt me
on this a go ahead and replace and we're
gonna let that go
muted applause so that's kind of our
server lists workflow and how we can
update our training with with new data
as it comes in so if you're interested
in learning more about machine learning
generally you thing actually has a
really excellent series on YouTube you
can check it out there bitly /ai -
adventures so this is a series of videos
they're usually they're pretty
digestible anywhere between like five
and ten minutes per episode that cover
machine learning from the very beginning
so conceptually what are all the things
to think about and then it starts to dig
deeper and deeper into sort of the
mechanics of how you actually do it it's
a great series like I said they're very
bite-sized videos is how I actually
learned a lot of what I know about
machine learning is from his youtube
series yeah and I think that's about it
that's it what's up good yeah I was
gonna say you know and if
you guys have no other resources we can
point you toward is machine learning
engines website tensorflow org of course
as well as the functions functions
website if you have other questions that
you want to ask us but you don't get a
chance to find us at in the conference
the easiest way perhaps is to hit us up
on Twitter so at U Fang G and at bread
MC G over there on the left and one
other thing I would say so we haven't
actually put our code on github yet it's
not because it is poorly written by by
me but once we get it cleaned up and
ready we'll put on open source so
probably the easiest way if you're
curious is just follow us on Twitter and
we will make an announcement so that is
all we have to thank you very much thank
you and I think we have a few minutes if
anyone has any questions about ten
minutes no questions at all yes so he
said what's the underlying algorithm
this this particular model is just the
the deep neural network that we saw in
the tense Rafael walk through I could
certainly do something more complicated
something more advanced that would
actually recognize sevens better
probably but we wanted to do something
that was like very understandable and
like straightforward to code and and
look at so because we did this with the
intention of open sourcing of the code I
didn't want to say like and here we have
this giant model that I've coated up
with like a you know 200 lines of code
separate from everything else so we
wanted to really make it simple and
approachable and you know it still
achieves pretty good accuracy so I was
happy to see that other questions yes
sir
yes so the question is when we uploaded
the data where did we say like for the
training data that it's a seven we used
a very complicated algorithm is that it
just looks at the file name so I mean
and so it's actually it's a larger point
I guess which is when you do training
like you train it on data that you know
the answers to so in this case we knew
we had a bunch of sevens so when we
uploaded it it actually just looked at
the or look at the file path basically
so we uploaded a bunch of sevens into a
directory that's like seven slash and
then all of our images and so that when
the function trigger just said hey what
directory is this image in so in your
system you would probably be you would
actually have like maybe like a JSON
wrapper or something that has metadata
maybe about your data but it just
depends on how you're ingesting it you
may not actually be getting your data
you probably are getting your data from
a database where you already had the
data whereas now we just have to trigger
a file upload event to turn it into data
but yours probably already is is in data
format and for those of you guys who are
walking out right this minute before you
do if you want or are interested in
tensor flow or cloud function stickers
we forgot to mention that we have a pile
right here and we'll also have some at
the Google cloud platform booth which is
like just out on the corner I'm will
dump a bunch couple of minutes any other
questions yep is an API for what now
other languages yes so the question is
whether or not there are other languages
to use tensorflow
new the new someone's gonna ask about
Java ah-ha-ha-ha is that what you're
asking about I'm curious okay so there
is a job API now it's still like being
worked on actively so as the tension
flow team likes to say for requests
welcome it is open source if you guys
are working a lot in our there's a
pretty complete at this point set of our
bindings so that's kind of like the
other side of data science the other
people the Python users on the our users
but now they can both be happy together
and use sensor flow as one Java there's
a go
API somebody put together is putting a
ruby one there's a Haskell one if you
have languages you like you can make
tensorflow bindings there's that on the
website there's actually an entire guide
to what you need to do to make Kaiser
Flo work for your language of choice
even if you made up your own language so
like what to hook into and things like
that because we didn't get too much into
it here tensorflow runs under the hood
in C++ and so basically Python the even
the Python API I'm using is not like
that truly native API right it's just a
wrapper around the C++ that's a lot more
readable and maintainable sorry to any
C++ developers out there I used to do
this too I used to suffer along with you
know I mean it's Python now yes any
questions all right
heard a cricket bassist yeah that's
funny also I didn't mention it earlier
if you want to learn a little bit more
deep dive into a sort of this serverless
technology I'm doing a session at two
o'clock in one of these rooms that talks
a little bit more in like some use cases
and examples and demos of not just
clawed functions but some of the other
server list things you talked about like
what we talked about like bigquery and
how you might use them outside of a
survivalist machine learning pipeline so
all right thank you thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>