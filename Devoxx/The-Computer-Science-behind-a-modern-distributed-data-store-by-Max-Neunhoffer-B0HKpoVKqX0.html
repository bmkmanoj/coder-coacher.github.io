<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Computer Science behind a modern distributed data store by Max Neunhöffer | Coder Coacher - Coaching Coders</title><meta content="The Computer Science behind a modern distributed data store by Max Neunhöffer - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>The Computer Science behind a modern distributed data store by Max Neunhöffer</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/B0HKpoVKqX0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good morning and welcome this talk
is about as the title says the computer
science behind a modern distributed data
store obviously that is a vast topic and
I cannot even hope to cover anything
much of the whole topic so therefore
I've just chosen a few topics and these
are these so--that's resilience and
consensus sorting lock structured merge
trees hybrid logical clocks and
distributed asset transactions and I
have to admit the last one I don't even
fully cover so if you don't know what
these are never never mind it's not a
problem because all I can do if I
succeed is to tell you about each of
these topics essentially what is the
problem and how is it solved in an
overview manner so that's all I'm trying
and since I am I have 50 minutes and
these are five topics so I have ten
minutes each and so that will be a bit
of a rush so let's directly go into the
middle of things basically the bottom
line of the code of this talk is you
need computer science and modern results
to actually come up with a decent
implementation of a modern data stone so
let's talk about resilience and
consensus modern data stores are
distributed why is that there's
essentially two reasons for that one is
you want to scale out you want to be
able to use a lot of computers at the
same time and you want them to appear to
be a single database but there's another
reason you want to have resilience even
if the capacity of one machine would be
good enough you don't want to run the
risk that it breaks so therefore you
want to have a distributed system just
to be resilient so if one node fails or
maybe even to conserve it simply
continues now as soon as you have
different parts on different servers
then these essentially have to agree on
things whatever they are you
data configuration data or whatever the
time say and so you know kind of semi
definition I would say consensus is the
art to achieve this as well as possible
and I call it an art because there's no
end to the problem essentially you can't
really solve it you can just approximate
it in a sense as we will see in a second
if things are well well then this is
relatively simple your computers are
networked they can exchange messages and
so it's relatively easy to make them
agree on things but it is very hard
provided your network has outages some
machine fails some disk fails and maybe
comes back or a complete machine fails
or even a complete rack fails and then
comes back with say different data or
something like that
and these things here they say is it's
not only they can happen they do happen
in particular in distributed systems
where you have lots of machines then
essentially every night something will
happen and you don't want to wake up in
the middle of the night and have to fix
it you want the system to to overcome
this problem automatically and we have
not yet talked about malicious attacks
or enemy interaction or whatever so this
is just plain failures that happen now
in that context is a lot more difficult
and so therefore people have spent a lot
of time thinking about these and
traditionally there are essentially two
protocols to achieve consensus between
different machines one is called pax vez
it's the older one and one is called
raft
traditionally Paxos was being used it
was published in 1998 and to say the
least it is a challenge to understand
and then to implement correctly it's a
lot of fun I do recommend reading the
papers about it they are really nicely
written so
by Leslie Lamport who invented that and
it's it's an ingenious paper but it's
hard to understand and if you have read
it five times you still wonder did I
really understand all the subtleties of
it or not and therefore the newer raft
which only appeared in 2013 has actually
been designed to be understandable
actually to be more understandable
therefore my advice obviously nobody
wants advice but here it is anyway if
you are interested in consensus try to
understand paxos first the papers are
very nice if only hard do this for some
time but do not implement taxes not even
as a toy project after you have looked
into praxis then you can enjoy the
beauty of raft read the raft paper it's
a beauty it's really nice it's easier to
understand if not completely easy but
it's easier but then don't implement
this either use some battle-tested
implementation which you trust I can
tell you that this advice is good advice
because if I have not listened to it to
every in every Engel single instead okay
I didn't implement try to implement
Paxos and I have suffered from it so but
but the really most important hint I can
give is don't try to invent your own
consensus algorithm unless you will have
some kind of time place where you can
spend two years on your own without a
time passing outside it can be a lot of
fun but it's hugely difficult is very
hard to get right and even then if you
get it right it's hard to implement
correctly right so now I'm trying to do
something which is impossible I try to
explain to you what draft is in one
slide here it is imagine you have an odd
number of servers we come to the
question why it's odd in a second and
your target is that they keep a
persisted lock of events say
together everybody should have the same
lock so everything is replicated to
every other machine what I do in raft
they democratically elect a leader and
that's why you need the odd number you
want to have an absolute majority to
elect leaders so if you have three which
is a typical installment then two would
have to agree on a leader or three it's
only the leader at any given time which
may append something to the replicated
lock and the append only counts if not
only the leader has appended it but if
it has replicated everything to at least
a majority including the leader and
obviously this majority has to have
appended it and confirmed it to the
leader that this is now appended now
there is very smart logic in the proper
column essentially issue ensuring that
there's a unique leader that's a tiny
lie what actually happens is that there
is a term number and in every term that
is a unique leader and provided the
leader changes the term number is
increased and then there's a new leader
for the new term there might be a slight
moment in time when both believed to be
the leader but the one with the higher
term is always valid and then of course
we need automatic recovery from failure
in particular we of course need if the
leader fails then the others after a
short amount of time
notice that they don't get any
heartbeats from it anymore
and they say ok we are still to let's
elect another leader the other leader
takes over and continuous service where
the previous one left off and that's
about all I can say because I have two
minutes more for this topic and I just
want to mention the paper is there it's
proven to work to to work this protocol
but then to implement it correctly is
still quite hard so we have actually
done it for a wrangle DB I can tell you
a bit about this later because what you
are dealing with is a real-time system
the real-time comes in when the
followers Willie lose patience with
their leader and then things have to
happen in real time and we have to make
sure that regardless of the load and
regardless of what data is stored in
there the leader always sends out heart
beats heart with hard real-time limits
and then you have to persist everything
and not fail and use fell over and so
that that's why it becomes difficult how
do we put this to use essentially these
changes in the persisted log of events
are the right transactions of a key
value store which is son sitting on top
of that so you basically can talk to the
raft leader and can ask for right
transactions or can read this key value
store and this then makes a resilient
key value store which can be used as a
centerpiece of a distributed system to
keep configuration data to organize
failover and stuff like that now this is
a link and I will make the slides public
and this is all clickable and shows you
a nice illustration I just want to fire
it up briefly this is by the author of
raft not by me and you see here five
service one of them loses patience and
says I should be the leader it asks the
others elect me they come back and say
well yes ok you are the leader the
leader censor this is now s5 in this
example it sends out regular heartbeats
such that the others don't lose patience
with him here we have the replicated
locks there's nothing in there so far
and I believe I can play with this and
tell I can't I can do different things
for example I can stop it now obviously
I know stopping was not the right one
resume I want to kill the leader or time
out or whatever oh no it stopped never
mind I'm over time with this topic so
therefore I suggest you play with this
yourself that's a great way to see what
raft actually
and it's a great illustration done by
the author of raft or one of the authors
Diego Ungaro and I highly recommend to
read the paper which is also available
on that web page here and onto the next
topic
you see we jump from topic to topic
because it's a vast area and just cover
a few interesting ideas now a datastore
usually needs some kind of indexing and
for that in practice we usually sort
things now sorting is a very traditional
computer science topic so you look at
the art of computer programming you find
ten different sort algorithms and then
on modern hardware they are all rubbish
essentially all except maybe one so the
published algorithms what you know
compute quick sort of it whatever just
doesn't work well on modern hardware and
why is that it is because traditionally
we have just compared different
algorithms by Counting how many
comparisons they have to do to sort say
but comparing is no longer the problem
the problem is data movement such a
little electro can easily compute a lot
of stuff but it can't get the data fast
enough and in in sorting you usually
want to sort vast amounts of data and
you just cannot get it into the CPU
quickly enough why is that when I was a
kid and played with my Apple 2e then
there was one megahertz processor in
there a 6502 and since then comparison
to this laptop here the compute power in
one core obviously at the time it was
only one core has increased by a factor
of 20,000 approximately now single
memory access has only increased by a
factor of 40 and furthermore these days
you have sometimes 32 cores on him in a
CPU so that's another factor and this
just means that computation has outpaced
we access by a huge factor of over a
thousand so any kind of sorting
algorithms which concentrated in the
analysis on on the comparisons this
analysis is outdated we have to analyze
how fast you can get stuff into the
processor to actually compare it so
that's a huge problem and what I want to
do is I want to give you just a glance
or two an idea how to overcome that how
to make a sorting algorithm which can
actually make use of the computing power
of 32 cores in a single machine say the
one algorithm which actually can be
salvaged is merge sort let me quickly
give you idea what you do is you have to
if you want to murder to sort a large
array say you cut it into pieces you
somehow merge the pieces first that can
be distributed in a sense and then you
merge these segments to make one that is
you have a lot of sequence which are
already sorted and you want to merge
them into one large sequence sequence
that's a very very important operation
as we will see later on in this talk and
it's in general very important now if
you naively just compare the lowest
element of each with each other may say
you say or with one of these you just
look for the minimum then you have an
algorithm which always has to go to a
main memory to fetch stuff and you are
running into the memory wall head-on and
you cannot never make 32 cos busy
therefore you have to use something like
this idea you use an intermediate data
structure called a minimal heap or min
heap well that's a balanced binary tree
balanced means every path is nearly the
same size obviously some might be a bit
shorter on the right-hand side here and
the single condition in this heap is
that whenever you have a node the
element stored at that node is smaller
than the two elements below it that's a
local condition which which must be
fulfilled everywhere
and I can't cover the details but it's
relatively easy to insert something into
such a heap and to remove the smallest
which is on the top here item and then
fix the stuff all of that has complexity
which is proportional to the depth of
the tree which is okay because that's
not growth making the total number so
what you can do is you put the
beginnings of these guys here into the
heap that's a quick operation they
somehow sink down here and are stored in
the heap and once you have convinced
yourself that the guy here is actually
smaller than all of these here then you
can take it and put it here as the next
element of the sorted thing so why is
that good the idea is that the heap by a
clever management stays small enough to
be completely in second level cache or
third level cache and if you think about
what happens if you take one element and
insert it here
the sinking procedure which I haven't
told you the details of has to compare
the one thing which is new well you have
to fetch that from main memory some
stage at some stage but it has to
compare it only to stuff in the heap and
since that fits in second level cache
essentially all comparisons 95% of
comparisons compared to things which are
in the cache and then of course you have
to write it out but well that's a cost
you have to pay anyway so that means
with this kind of organization you can
organize a sort on a huge array in
memory say such that all memory accesses
for all the 32 cores running at full
speed come from their cache nearly all
of them and that is enough to make a
sorting algorithm which has if you do
top 32 cores running 100% in top that is
what you want to achieve because you get
perfect parallelism for your sorting now
this is of course now a description of
stuff in memory
with second level cache but these days
we have a whole hierarchy you have the
processor registers first level cache
second level cache third level cache
memory main memory on the same package
main memory on a different package
different processor then you have SSD
then you have spinning disk then you
have Amazon s3 then you have Amazon
classy or whatever so it's not the only
second level cache this idea can
actually be applied across the spectrum
and you always have to manage the data
locality these days to get things fast
and if we slightly abstract from this
the idea of sorting here that is an idea
which is pervasive into all of computer
science these days you have to look for
data locality on each level of the
spectrum good and on to the next there
will be some time to ask questions at
the end and obviously I will hang around
the rest of the day to answer questions
the next topic is so-called lock
structured merge trees I'm going to
quickly say what the problem is if you
have a data store and again let's talk
for this time on on a single machine
with an SSD say if you have a box with
64 gigabytes and the two terabyte fast
SSD and you run a taylor store on that
well you expect that you can store about
two terabyte of data you expect that you
can store more amount of more data than
fits in RAM but at the same time you
expect that you can write to it in a
bulk fashion essentially as fast as you
can write to ram because you say oh well
the data store sort it out afterwards
and stores it to disk and you also
expect that if you only use in a reading
way a part of your data which does fit
in memory then you expect the data store
to cache things such that accesses are
essentially as fast as from RAM that's
essentially the same data locality
argument yet again
so therefore data stores need to have a
kind of data setup data structures which
fulfill these properties and if you look
what traditional b-tree based structures
or fun then they usually fail to deliver
in at least one of these two you can
tune them to have good provocation
everything but then writing is awkward
because you have to go all over the
place and or you can configure them that
writing is fast they buffer whatever in
a writer headlock or whatever but then
read excesses are a bit slow and so the
idea of lock structured merge trees is
to well you can't really solve this
problem in a perfect way probably but
the idea of these log structured merge
trees is that they offer a good
compromise between bulk writing speed
and hot set fast reads and now I'll tell
you what the idea behind this is I
shamelessly stole this picture from a
web page which is cited down here but
it's a good picture a log structured
merge stream organizes the data in
levels level zero being the most recent
most recently inserted data and
essentially you have to imagine that the
data flows in up here and over time ends
up further down in the levels each
levels contains files which are sorted
within the file again we need sorting
and you can roughly imagine that each
layer contains say ten times as much as
the previous one so the basic idea is
that you first store new stuff into
level zero in memory buffers which are
then obviously also persisted to disk
for persistence and these are inserted
in memory typically with some kind of
tree structure there and once such a
file is somehow sealed
there is enough of them and this here
can cover arbitrary regions of the key
space aim then a compaction process in
the background goes goes comes along and
takes a bunch of these and merge sorts
them into larger ones one level below
and here we make it so that everything
in here is smaller than everything in
here and so on and we do this in all
further levels such that if you need to
read something what you do is you start
in level zero trying to find it in one
of these guys and since they are sorted
in any memory you can quickly do binary
search and find find something you can
even do range scans and once you do not
find it upstairs you just go one level
down go to the right
sorted file and read from there if you
find it fine if you not find do don't
find it go further below so that's the
algorithm for reading just go from level
to level and the algorithm for writing
is essentially just right on the top
level now why does this give you a good
compromise for bulk writes and hot set
reads mm-hmm well I haven't told you the
whole story so here's the rest of the
story together with such a sorted file
on disk which is immutable because we
only write completely new files whenever
we do some compaction together with such
a sorted thing you store a so-called
bloom filter it's another very
interesting topic in computer science a
bloom filter is essentially a small
memory structure or small data structure
which allows you to ask is this
particular key in this sorted list or
not and the room filter uses only very
little memory at the cost of it can lie
to you but it can lie to you in a very
friendly way because if the bloom filter
says the key is not in the
set then it's always right it's a proof
that this key is not intercept if the
bloom filter tells you when it does so
and say there's a configurable amount of
of per cases it's not it's a it's maybe
in two percent of all questions it says
yeah there it might be in there and you
can't be sure it is allowed to lie about
the key being in there and then in
actual fact it's not in there so why is
that useful first of all it's much
smaller so the usual buffer cashing
business of data stores will ensure that
the little bloom filter associated with
this larger file probably can be cached
in memory so therefore when it comes to
look for some key you go to level 0
that's in memory anyway you go on level
below you see R if it's there then it's
in this sorted range so you look up the
bloom filter the rooms up to bloom
filter says no then you know the key is
not in here and you can directly go
further down if the room filter says yes
well first of all it might be in here
then you have to look and obviously if
you go out of the hot set you have to go
to storage and you have to pay the price
but you can be very quick in many cases
for example if the key is not in the
database at all you can rank all right
through all the bloom filters and say no
we don't have it and that's quick so
therefore the bloom filter business
together with caching on each layer in
memory achieves that you get good read
performance for your hot set which is
cached in memory the level structure and
the fact that you just write to the top
level in May in memory and then the
right ahead log of course and show us
that you have good bulk right
performance and as I said both can even
be optimized with other techniques but
this technique gives you a good
compromise I will just mention one more
thing if you analyze this carefully and
you can't because I didn't give you the
details but if you look at the detail
and there's a lot of more information in
this webpage you look at the details you
notice that all I owe is sequential at
least for the writing and even modern
SSDs although you would expect that they
are random access fast due to the block
structure and there are possibilities
for erasing and so on they are much
faster if you can write six render it
sequentially so therefore this technique
also makes much better use of modern
hardware than other techniques which
have to do random accesses all the time
so this works for SSDs and it works
equally for speeding rust so let's wrap
this up I just give you an overview
I said writes go to a mem tables in the
level 0 all files are sorted and once
they are closed they are immutable it's
only the compaction which happens in the
background which takes a bunch of these
merges them to something further down
you can use merge that so that's fine
and all rights use sequential i/o and
you need some kind of bloom filters or
the more modern cuckoo filters for fast
read and therefore you you get a good
compromise between write throughput and
read performance so because these are
nice they're used all over the place we
use them in Arango DB but also BigTable
Cassandra HBase influx TV level DB level
EB was maybe the first one implementing
that and Rox TV is a fork of that on my
Facebook but also nowadays Wyatt Tiger
SQLite for and MongoDB because they use
the Vario tag or engine and they all use
this technology and so it's fundamental
for modern data stores coming to the
next topic which is hybrid logical clock
now the two previous topics were
essentially about a single node in a
distributed system this is now really
distributed as was the consensus in the
beginning the trouble is if you have a
distributed system then the clocks in
your different nodes well they are not
in sync
there's no way to achieve that well okay
Google uses atomic clocks and GPS clocks
and everything that gets it down to a
very small clock skew maybe 20
milliseconds or even better with atomic
clocks but it's a fact of life in a
distributed system clocks are off of
each other so there's a clock skew so
why is that well there is general
relativity so there's fundamental
reasons in physics why you can't get
them
but this doesn't play a big role in
practice but in practice it just happens
clocks are not perfect they run slightly
faster slower with very large amounts of
money you can buy a good clocks and then
it's even it's a bit better but even if
you run the network time protocol you
will have some kind of clock skew you
can usually in practice between modern
machines on a network limited to 20
milliseconds but you see for modern data
store 20 milliseconds are actually quite
long yeah and so therefore we have to
live with that this essentially means
that if you have time stamps in your
system which come from different
machines
you must not compare them because the
clock might be off well if the if the
difference is large and you run
something like NTP well you can say okay
certainly this guy happened after this
one but if it's close you don't know and
that's kind of bad because quite often
you would like to to compare different
events happening in different places of
your distributed system and tell which
happened after which other one so West
is used sometimes you want to do
conflict resolution there's two
different writes in two different places
to the same item one way to approach
this is to say the later one wins well
if you can't tell what the later one is
it's a bit of a problem
now obviously if you have locks coming
from different places they have time
stamps
you usually want to sort them in time
order or you want to understand if you
debug something you want to understand
what happened after what is it an effect
or a course anyway you want to detect
that work deal I did delays you want to
implement time to live in a data store
whatever there's a lot of reasons why
you would be interested in comparing
time and the fact that you can't is a
bit over trouble so that's the problem
so what is the idea behind so-called
hybrid logical clocks hybrid means that
it's a hybrid between a normal clock and
the so called logical clock a logical
clock is another idea which I'm not
going to cover because it's somehow
included here a logical clock is
essentially just an arbitrary number
which grows and which you say well this
is called my clock it has nothing to do
with real time but it's just considered
to be a clock now what you do is that
you put a local clock in each computer
this is not no problem this is the
real-time clock and you use a network
time protocol to synchronize them so you
can say well the delay but the
difference between the clocks is not too
huge you might even be able to bound it
by a fixed value now what you actually
want is and that's the crucial thing is
that if different events on different
machines are linked by causality I'm
coming to that in a second if they are
somehow linked by that then you want
that the course has a smaller timestamp
than the effect it causes that's
essentially what you want and you see if
you analyze this clip more closely
causality between different nodes can
only happen if there is some kind of
message so what do you do in a
theoretical way you equate causality
with a message is being sent and then
you do this do the following which is
actually quite a simple and neat idea
this hybrid logical clock which which
can be queried on each node
each node has a local hybrid logical
clock uses the fact that every message
going around in your system has a
timestamp which was taken on the
starting side and was which was sent
along with a message to the receiving
side and the receiving side retains the
largest clock value it has ever seen in
any message and if if you query the
hybrid logical clock on a note it simply
does the following it considers the
local clock that is the hardware clock
and it considers the largest timestamp
it has ever seen in any message
including its own and it just gives you
the maximum so it simply lies to you it
just takes the largest value ever seen
and well if it has taken it has given
out that value already it adds one in
some little field and makes it even
larger so basically the secret is lying
so why is this good and why is it not
bad it's good because it's exactly this
condition with causality if there is a
measures message going from A to B and
some time stamp is taken before the
message is being sent then the time
stamp send along with the message is
larger it is retained on the note B and
if then you ask the hybrid logical clock
for time stand after the message was
received then surely this must be larger
than the one sent in the message which
is in turn larger than the one taken
over there so if a message has been
passed from A to B then a timestamp
taken before it was sent is always
smaller than our flight was received in
genius with a very nice little trick but
there's another advantage obviously
lying is not very good but fortunately
time flows so time passes and therefore
eventually assume there's no more
messages
eventually the actual time catches up
with a lie
and if there's no more messages then
later on it can pick up this maximum
here and the local clock is again what
counts so essentially these lies they
repair themselves automatically and
that's the whole idea so it's not
perfect but it ensures that causality is
preserved and it ensures that real time
can catch up with a logical time after
some time now so that's a good
compromise so notice that it's not
perfect in the for all the purposes I
mentioned because if there's no message
being sent between two nodes then you
can't expect that the timestamps can be
compared yeah so it's not usable for all
these part asks for example it's not so
good if you want to compare time stems
of log entries because there is usually
no interaction but usually you want to
stare at these locks to analyze some
problem and usually the problem is
around messages being exchanged so
therefore it's already then a huge
advantage to know that once a message
has passed the time streams actually
compare favorably again there's a link
which has a very nice description with a
lot more detail and which is actually
detailed enough such that you can type
it in and implement it in a hundred
lines of code so contrary to raft this
is actually easy to get right once you
have a mechanism to fetch the clock and
send timestamps along with the time and
on to the next so this is the biggest
one it somehow tries to bracket
everything together which we talked
about and a lot more and there's no
chance whatsoever that I can even
explain the details of such a thing like
asset distributed transactions in one
talk even
but let's have a go at it just to tell
you what the problem is and how a path
to a solution could look like what do
you want from an old relational single
server data store you know asset let me
quickly say what I mean by this you have
write transactions which cover lots of
data points you want that changes within
one transaction either happen as a whole
or not happen at all
that's your atomicity a transaction is
atomic you want them to be isolated so
if another transaction looks at the rare
store what we want is that it either
sees the situation before the writing
transaction so that it sees no changes
apply it or if the second transaction
the retranslation starts after the
commit of the first you want that it
sees all the changes and that is your
isolation a transaction can never see
half another transaction you can only
see the whole transaction or not
according to certain rules
you obviously want that your data is
durable you don't want to tell the
client yes it was written and then later
when you ask give me give it back to me
or to them I don't have it actually and
therefore putting all this together
you want to have consistency within your
data store and you want this in
particular in the presence of of a
distributed system where you have
replication going on so obviously in the
distributed system for this to be fault
tolerant you need some kind of
replication and so you want to have
consistency between your replicas so
that's one meaning of the world but also
you want consistency of different data
points in your data store which comes
from this atomicity for example if I
send money to somebody you want the
consistency that it either is still in
my account or in the others account so
the to account numbers should be
consistent retreats are with each other
in comparison to the transactions I do
so that's the meaning of consistency
which is usually meant with acid now
with a relational data store on a single
node you have all these and there is in
the SQL standard there's descriptions at
well as to what isolation levels there
are and so on and it makes actually
programming against such a data store
quite easy because you can relatively
easily reason about what changes you do
and how things look like at what stage
now if you do all your transactions one
after another then this is still
relatively easily doable
well you just finish the first one you
make it so that everything appears at
the same time and becomes committed but
when you want that transactions happen
at the same time in a distributed system
you of course want to hit it with a lot
of transactions which as best possible
happen at the same time because you want
to scale your right capabilities in that
case it's a lot more difficult so why is
that the problem is once you distribute
your data across across nodes then the
transaction will touch multiple nodes
because you change in one transaction
multiple data points so you turn you
have to interact with a lot of nodes and
they have to change data now during this
transaction and even after the commit it
is still possible that that somebody
wants to see the state of things before
that transaction it might be an old
transaction which is still ongoing so
you have to maintain the consistence
Knapp shot during the changing of all
the places in all in all different nodes
therefore you have to hide the ongoing
activities all over the place before
from this from the eyes of other
transactions which are looking
and you have to handle durability in the
presence of lost nodes lost discs and
and the like so therefore in this whole
transaction business in a distributed
system you have to take into account
replication of data you have to take
into account resilience and failover
and you have to think how your
transactional behavior and the
guarantees you want to give can be
upheld in the presence of these problems
that is again what makes it troublesome
like in consensus and that is
essentially a piece of consensus here
the trouble is not if things go well the
trouble is if things go wrong now in a
sense you can see how difficult this
problem is if you look on the market of
databases and see which players give you
distributed transactions and which don't
and the list of modern scalable
distributed data stores which do not
have distributed asset transactions is
very very long and it is really a quite
elusive Club of a few which promise you
to have distributed asset transactions
in whatever sense of isolation level you
mentioned you can notice that I have put
a wrangle DB twice here because in the
current implementation we don't have
distributed transactions so in our
ability we store or JSON documents and
so changing a single JSON document is an
atomic operation and that is also in the
cluster but if you touch multiple
documents we cannot make asset
guarantees as of now but I have put us
here as well because we have a plan to
get there and hopefully sometime we can
see the headline on Hacker News
Arango DB now with distributed
transactions and then I will be a happy
man provided it works anyway it is hard
only very few like maybe span I was on
one first by Google and nowadays
cockroach DB do these promises
and they go to great lengths to actually
implement that so I as I said I cannot
really tell you how this works but let
me just give you the basic ideas behind
that and that will also be my last slide
here I said already you have to do these
places all over the place these changes
all over the place and they must not be
visible so far therefore imagine a store
like a Ranga DB where you have to
exchange the revision of a document you
essentially are forced to keep the old
one for those transactions we don't
which don't see yet the change and you
have to keep the new one and you have
multiple transactions going on you might
have to keep multiple versions so what
do you have to do in every place where
you keep your data you have to do
something like multi-version concurrency
control so you have to keep multiple
revisions of the data points and when a
read comes you have to decide in a
rather complicated way which one to
actually hand out for that transaction
so that's the first thing that's not too
bad now obviously you have replication
and everything happens to centrally
distribute it but the fundamental
problem is that you now have to organize
it such that you do all these changes
they are not yet visible but then there
comes a time when you actually want to
commit this transaction so you have to
be able to in one moment switch it over
and say now this transaction is
committed and the results should be
visible for every transaction which
start after that if I say after you
always of course think about hybrid
logical clocks and the trouble is all
reared with that so that comes into the
picture here so what you need is you
need a single place in the system which
then of course has to be replicated as
well which says well yes this
transaction is committed or not
now these switches they have to be
persisted as well they have to be they
have to be scalable so you can't just
put them on a single machine because
that would not be scalable they have to
be replicated because you can't have a
single point of failure you have to
implement resilience for them and so on
and so on
and yet it has to be a single place now
one answer is that you somehow run
multiple rafts all over the place for
these switches and use the raft protocol
to have consensus about about the
replicas of that switch and then when
the raft thing has committed then then
it's counted as committed but draft is
slow it's secure but it's slow and
scalability is a problem so therefore
you probably want to be doing something
more complicated which I can't explain
here in any case you have to implement
then the transaction logic the question
as to which version you hand out for a
given transaction all over the place
taking into account these switches the
good news is that these switches can
only go from don't know to commit it or
don't know to abort it for that
transaction so therefore they can be
very well cashed and that is essentially
the secret on how things are done in
spanner and cockroach DB and and we are
planning to do something rather similar
in any case my time is up we have two
minutes left I'm happy to answer
questions now
maybe you answer questions later I thank
you for your patience and for anybody
who would like to have a sticker or
t-shirt or angle DBE I have put some
here as well anyway are there questions
Thanks
yes I repeat the question don't you run
into a problem with a cap theorem no the
cap theorem is not a problem because
it's just true and you have to live with
it and so what you have to decide if you
do a distributed data store you have to
decide if a network split happens and
it's going to happen because it always
happens at some stage in that case you
just have to decide what you want to do
do you want to be available then you
keep writing on both sides of the split
but you have to sort out the MIS mess at
some stage either you say customer
sorted out or you use time stamps and do
some conflict resolution or you use only
updates which are good or as in the case
of Amazon you just live with mistakes
now if stock levels of books are out
they just send an apology and say we
don't have the book sorry we got it
wrong that's the cheapest in this case
yeah or you decide I want to be
consistent then you say on the lower and
on the smaller half of your system which
is on the smaller side of the network
split you just don't service no problem
you will choose consistency over
availability fine so it's not it's not
as such a problem it's of course a
challenge but it's only it just means
you have to decide for a wrangle DB we
decided that we want to honor
consistency over availability and if you
lose contact to half of your cluster
then service stops if important data is
over there let's first have this
question here
the question was in raft I take it when
a server leader fails how do the others
elect a server what happens is that once
the leader is elected let's imagine we
three are the rafters I'm the leader I
just send every maybe four times a
second I send I'm still alive I'm still
alive I'm still alive I'm still alive
and you receive these and confirm now
when I fail I drop dead on the floor
you two will observe that there is no
more I'm still alive so you have a
patience so that is a random time out
you two chose them and that's about in
our case between between one and five
second say and once you don't receive
heartbeats during that time you just say
oh that something is wrong with our
leader you increase the term number and
say elect me I'm the new leader and send
out messages to both of us elect me and
one other server is enough to then
answer okay you are the new leader term
is up and you to continue without me
when I come back I see okay I used to be
the leader but now there's a new leader
with a higher term I follow you
that's it time's up thank you very much
I'm happy to answer more questions and
I'm happy to hand our t-shirts over here
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>