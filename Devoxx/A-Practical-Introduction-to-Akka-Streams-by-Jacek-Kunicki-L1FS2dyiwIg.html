<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Practical Introduction to Akka Streams by Jacek Kunicki | Coder Coacher - Coaching Coders</title><meta content="A Practical Introduction to Akka Streams by Jacek Kunicki - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Practical Introduction to Akka Streams by Jacek Kunicki</b></h2><h5 class="post__date">2017-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/L1FS2dyiwIg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I think we can get started welcome
everyone to this session about stream
processing with ARCA streams my name is
Jessica Natsuki I work at software mill
we remote software house based in Poland
so if after the talk you'd like to chat
with me either about vodka streams or
remote work feel free to catch me
sometime during the conference two
things before I start first of all all
the code I will be showing is available
in a github repository there will be a
link at the end of the talk so you don't
need to take notes hello
and secondly the codes I'll be writing
it in Scala but please don't be afraid
in don't be scared even if you haven't
seen Scala before I think that code
would be pretty straightforward
I'll try to explain that maybe not not
so straightforward things so please stay
here even if you don't know Scala okay
so what is three processing actually
well perhaps a lot of you have heard
about the concept the general idea of
stream processing is that you have a
source of some kind of source of your
data we can call it a producer you have
a target place for your data which we
can call a consumer and then you have a
number of processing stages and of
course the data flows from the producer
through the processing stages up to the
consumer so you might think this is
everything but actually in Anaka streams
which is an implementation of reactive
streams we have some additional concert
here and if you've heard about reactive
processing reactive is basically about
about non-blocking so that's that is the
bottom line of reactive processing not
to block when you don't have to so if
you imagine such a stream processing
pipeline you can basically think about
about two kind of scenarios the first
scenario is when the producer is pretty
slow and the consumer or the order
processing stages are fast so if you
have a slow producer and the first
consumer this is actually there is
actually no problem because well the
consumer will never be flooded by data
the producer will produce the data
slowly and the consumer will be capable
of processing everything on time but if
you imagine the situation another way
around when the when the producer is
really fast and the consumer is really
slow
it can happen that the consumer will be
flooded by data I mean it can get too
much data like much more that it's able
to process actually well it had been
perhaps it would somehow fail in this
situation so in the reactive streams
concept we have like an additional
channel for for for the for
communication so the data flows from the
producer through the stages to the
consumer but we also have something
called back pressure and in reactive
stream the back pressure is non-blocking
this is quite important and back
pressure is basically a way for the for
a slower consumer to tell the producer
that well how much data I can accept so
in Arcis trims the back pressure is is
realized as signaling how much how much
data can I get like a producer give me n
items of data because I know I will be
able to process n items for example and
nothing more
and this way the the like the consumer
is able to limit the right of the
producer so that it's it doesn't get
flooded by data and it's always able to
process the data so this is like a
general idea and of course since akka
streams is one of the implementations
they have their own terminology so in
akka streams the producer is called a
source the consumer is called a sync and
all the intermediate steps are our flows
well actually source and the sink are
also a flows in the akka streams word
but let's just think as a source as a
source I think as a sync and all the
intermediate steps as flows if we
combine them together so solid link the
the various stages they are called the
graph so and the graph is what it's like
the processing processing unit innaka
streams so there are this real number of
technologies for for stream processing
this is just a part of it is from my
colleagues talk about small in throttle
big data you can even notice that akka
streams is not here so how is our caste
really different from all the from from
the other solutions that we have well
first of all our custom streams is not
distributed if you heard of spark for
example and spark streaming well sparky
generally distributed processing engine
and sparked streaming is one one of its
power
which is distributed and actually this
might no longer be true because innaka
streams well when you see the code
you'll see that will be will be like the
dart too part well first of all what
will be writing are only the recipes for
data processing but we won't be we won't
be influencing the how the data is
actually processor what what happens
underneath and in Arcis trims you have
something if you if you have your graph
defined and then you have something that
is called the materializer that is
actually responsible for running the
graph somehow and up to recently akka
streams only had its default
materializer which is based on an actor
system but it turns out that there's
going to be another and another
materializer for akka streams based on a
particular pump about a gear pump is one
of the like distributed stream
processing frameworks there it's
actually an abstraction for for other
stream processing frameworks so the
animal materializer based on Apache gear
pump has been announced for ARCA streams
so the the statement that akka streams
is not distributed may no longer be true
when when defining the the data
processing with akka streams we create
some reusable building blocks which we
have a DSL for creating that the steps
for our data processing pipeline it's
important and when you create a graph
like define the graph everything is like
so so until you actually run the graph
no processing happens anywhere so you
just have a DSL for for defining your
building blocks and only when you want
want to run it you need to have a
materializer present and you when you
execute some some kind of run method on
a graph it's only then actually run but
when you define all the steps nothing
actually happens so in our example
project we'll be dealing with some some
data stored in files the files well each
file would connect would would hold some
lines with IDs and values as you can see
on the left forever forever ID there
will be two lines with which either of
them can hold a value or not so so there
will be there will be pairs of lines
with with valid values the ruble lines
where one of them can be invalid or both
can be invalid but there will always be
two lines for a single ID
and to make things well maybe not harder
but complicated the older files are
going to be gzipped then we want to do
some intermediate processing on the
unaware data from the files some here
it's its aggregation and will be
everything the the values by ID
of course when some of the values won't
be valid in India for for for a for a
single ID we'll be using some some dummy
value - to judge - just like mark that
it was invalid and ultimately we'll be
storing our data to Cassandra
well of course in ours in like small
case we perhaps you wouldn't need
Cassandra at all but this is like taken
from a it's a simplified version of a
real project we made and we used
Cassandra ders so so Cassandra is here
as well and actually if you had a
real-world scenario where you you wanted
to do fast inserts to a database
Cassandra may be a good choice well
because one of its features is that it's
really fast at accepting incoming data
and inserting it okay so let's get to
the live coding part I hope you can see
it in the back if not there's plenty of
room here in the front so so please
don't be afraid to come here okay so we
have we have our CSV import our class
which has some configuration parameters
that I will describe them later as SS
I'll be using them so in our in our like
file processing code the first thing we
may need to do is to when we get a
single line we wanted to parse it
somehow to our domain model so we'll
define a parse line method which would
take a line and the line will be a
string and we will be processing the
lines in some a synchronous fashion so
in scala if you want to declare that
you'll you're doing something a
synchronously you use a future as the
return type so future is basically a
value but but like deferred one so you
don't get it you may not get it
immediately but some time you get it and
we'll be returning a future of reading
now what's the reading reading is
basically were the two types of readings
we have can have a valid reading which
has an idea and the value as I said
previously and we can have invalid
reading which was
it is basically a marker class because
it doesn't hold a value well because the
value is invalid but it holds the ID so
that it's easier to do the aggregation
and to process that to average the
readings afterwards okay here we have a
future block which basically means that
this will be executed in another thread
so this this would be an unknown
blocking call and what we want to do
well the first thing we are splitting
the line by semicolon and the values are
going to be our fields then we take the
first field we convert it to an int and
this is going to be our ID secondly we
take the second field convert it to a
double and it's going to be our value
and now well we could just return the
value treating with the idea and the
value well but of course this is a happy
path for further for the lines that are
correct and of course while some of our
lines may not be correct so we need to
handle this as well and actually where
it would fail if a line is incorrect is
there is that too double because there
will be some kind of number format
exception so let's just surround it with
a try cut well we'll be cutting
everything here here we'll do some
logging just to know that there was an
invalid line and we'll be returning here
is an invalid reading with just the ID
so this is like a way to mark that one
of the or both of the readings were
invalid and we are actually missing a
file path here so we'll complicate the
met our method signature a little bit
now this is if you haven't seen Scala
before this this may seem strange
because we have two set of parameters
and but this is basically such this is
something called a partially applied
function and it means that if you call
the parse line method with only the file
path it would return a function that
would that could then accept a line as a
string and this is just like it would
help us to to like put the method into
our into another method call which which
accepts a function so we need the file
path for the debugging purposes and we
need the line to do the actual parsing
okay so this is this is parsing the line
when we have a string but sync well we
don't
to read our entire files into memory
because there might be a lot of them
there may be big so we go not to stream
the files and actually when we are
streaming the fires we're basically
processing as a stream of bytes and we
and then what we want to do is we want
to detect new lines in the stream of
bytes to be able to split it into in
turn actual strings so our first first
building block of our processing
pipeline will be something we'll call it
line delimiter and here we are we well
the method we defined before doesn't
have - doesn't have anything to do with
a CUH stream so far here's the first
first place where we actually using the
akka streams API so as you may remember
from the previous slides we did the
building block of a stream processing
pipeline is a flow and liked we have a
flow has three type parameters the first
parameter is the type of the data that
comes into the flows or the terms into
the block into our black box the second
parameter the out is what gets out and
we also have a bit mysterious one called
mod this is a shortcut for materialized
value and materialized value is
something that can be emitted from the
flow to the outside world well that's at
least you can think of it as as such
because normally when flows so when when
when the building blocks of a stream
communicate with each other they don't
emit any data to the outside world I
just communicate well between each other
but there is a possibility for a flow to
emit something to the outside world and
we call it a materialized value but
really will not be using materialized
values here at all so we'll just be
skipping it
so in our line delimiter mmm the input
type would be a byte string which is
it's it's a type from the from the
accessories DSL and it's basically a
wrapper for well as a string that we
that is that it's built of bytes we also
output a byte string which only after we
would convert an an actual string and
since we won't be using the materialized
value here we would use some a type
called not used and not used is
basically something that the ties
together the this Java's void and the
scholars you need because akka streams
also has a Java API and well in Java we
have void in Scala we have you need they
are a bit similar but
a bit different so the not use type is
something that unifies it's also that
like that so that the Java API is
similar to the Scala one so if your Java
programmer s is basically avoid if your
Scala program is a particular unit and
for for doing like akka streams has some
has some built-in building blocks for
for for common tasks and actually for
for processing well for dividing data
into frames we have a built-in object
called framing and it has a delimiter
method and what it takes well if we want
to to divide our our our bytes by
something then we need to define the
separator so here we'll just take the
newline character then we define the
maximum length of a line where our lines
are short so 128 is a reasonable thing
value here there is one parameter
it's called allowed truncation and it
basically says what what to do with a
line that doesn't contain the limiter at
the all so basically if you if you have
a file that doesn't has doesn't have a
newline character at the end it tells a
cuss we've got to do because by default
if there are lots allocation is set to
false it will file on a line that
doesn't have a new line at the end but
if we say a settlor truncation to true
it will just process that like the last
line without the newline character
normally so it wouldn't fail okay so
this is this is first first building
block and as I said it's lazy so nothing
actually happens here it's just a recipe
what do what do we do with the incoming
data so the next step we're going to
define is a recipe for parsing our files
so this is well we'll be using the flow
type heavily because we'll basically
define defining the flow the building
blocks of our hours processing pipeline
so file parsing flow would be well if
input type will be a file then Java IO
file the output type we'll be reading in
there valid or invalid because well we
we want to produce any type of reading
from the files and we were like ignoring
the materialized value so we say not
used here
so previously we used the framing object
but normally when you build a flow with
with some input type you just say flow
of file and then you have a number of
methods and I'll just add some new lines
so that it's with in the middle of the
screen so for our when we will take our
files we want to stream them and so for
every file actually we want to treat
every file as a source of byte strings
so a source in terms of hakka streams so
a producer of byte strings so in this
case in the parse file state for every
file we want to create we want to
convert every file to a source of bytes
twinks and then we want and then we will
do the actual parsing and then we want
to to flatten all the sources like we
will have multiple sources of files but
we but what we want to omit from the
flow is like a reading by reading so
that that's called flattening so
basically we have will have a number of
sources which will be similar to well to
the parallelism we define and then for
for every source we would like to take
the output value and flatten it so so
the readings come out one by one
so for this way we have a stage which is
called flat napkin cut anybody what it
basically does it well it it it changes
the it takes a number of sources and
just flattens them to so that the values
are emitted one by one so here we are we
going to have access to the files of the
to every single file that gets into our
flow so since the since the files are
gzipped
will create a gzip input stream which
would wrap a file input stream created
from our file and then we have a helper
mat a helper object called stream
converters to it we can pass an input
stream actually a function that returns
an input stream so here we have an
anonymous function that just returns our
our stream that we haven't yet saved
variable so it's going to be a stream we
have the stream here and what it
actually is
when you check the type so it's a source
of byte strings so basically from every
file there will be byte strings coming
but will be like we could we could
flatten them and done so for every for
every byte string that comes comes out
from the files first thing we want to do
is pipe it through the line delimiter
stage so we have a stream of bytes and
we pipe it through the line delimiter
that we defined before so now that the
output of the line delimiter is a byte
string then what what we want to do we
might as you may remember we may have a
header line in the file so the name of
the columns so optional you may want to
drop it from every file and actually we
have a config parameter for that code
lines to skip so for every file will be
will be first will be like passing it
through the line delimiter to get single
lines then we'll be dropping the first
line because well it's a header line and
our our line delimiter output byte
strings but we want normal normal
strings that will that are accepted by
our line parsing method so we call the
map state and map basically is like a
lot like a map on a collection it's in
Scala or Java and actually a byte string
has a handy method
utf-8 string which just convert it to
string so now we have our lines of
strings and what we want to do at the
moment is convert every string light
we're reading using the method we define
at the very beginning and we want to do
it a synchronously because well it's we
want to preserve the order because the
order of the line is important since
since they're grouped by the ID what we
want to do it doesn't Chronos way with
some some level of parallelism so we
have a map async stage and it basically
well it it's also a partially applied
function it takes a the parallelism
level and for our non i/o operations we
have a config parameter called non il
parallelism so so the this is for all
the operations that don't require any
and resources like disk or database or
something and then we we need to pass
the actual function that takes a string
and returns the future or something so
the function we defined the very
beginning the parse line one actually
does does this well actually after
applying the file path to it so we'll
say parse line here we call it with file
get path and what it returns is the
function that takes a string and returns
a future of reading so this is the this
is the processing step once again we're
taking for every file what we're doing
is the first word we will apply in our
line delimiter to extract to split the
byte stream by newline characters then
we drop the configured number of lines
for example the single one which is a
header then we convert every line which
is now a byte string to a normal string
that we want to work with and then in an
async fashion we convert every every
string to a reading so and here we use
the parse line method we defined at the
very beginning so having our readings we
are ready to actually a like a stream of
our readings we are able to group them
by two and then compute the average so
this is this is going to be the next
next next processing stage it's going to
be called compute average L once and
once again is going to be a flow it
would take a reading and the output
value would be a valid reading so so our
our average computation accepts any
readings be the feed valid or an invalid
one but what we want to output is an
isn't is a valid reading which has an ID
and has a value of course in the case
when both the lines were invalid we
would we would omit a dump just a dummy
value but we always want to omit as
something that has an ID and has a value
so we will be a meeting valid reading
and not using the materialized volume so
here once again our entry point is a
flow of reading so we have we have
incoming readings and what we want to do
now is group them by two because every
every two lines Shardas heard the ID and
they may contain a value and then what
we want to do is process them like again
compute the average in an asynchronous
fashion but this time since we off we
already group the lines by to the order
of computation doesn't matter
so previously we used the map async
method but it turns out that there is
also an another variant of it called map
async unordered and the difference is
that map async preserves the like it
executes a synchronously but it
preserves the number the preserves the
order of the results so so even if if
some result is computed faster but it's
it arrived later in the in the incoming
stream it would be there will be output
in order but map I think an order
doesn't care about the above about the
order of completion it omits the value
downstream as soon as it's computed so
of course it's faster but sometimes
sometimes you need to preserve the order
as we did in the in the previous state
here we don't I don't actually care
about the order because well we're just
we're just computing the average values
and we we don't in the order anymore so
here the parallel is moved once again
would be this non IO one and what what
we get here is it's readings because
it's a well actually it's a it's it's a
sequence of or a list of two readings
every time because well there are there
are a pairs of lines driving the same ID
so what we want to do with the readings
well since we're once again we're doing
here a synchronously so we'll wrap
everything in a future and the first
thing we want to do well to compute the
average we we want we want to extract
the valid readings because well the
invalid ones don't have a value so they
won't be useful for a computing average
so we'll take our readings now do a
collect operation and basically for
every reading that it's an instance of a
valid one of the other values we think
we'll return it so this is basically
filtering by like filter with instance
of if you if you wrote it in Java so we
basically field filtering out only the
readings that our instance of the value
trading and we'll call it valid readings
there is there is a filter method of
course but well it's it is basically a
matter of style because in the filter
method you would need to to use the
instance of upper right or to take the
instance of the class and here you well
such some people may call it more
elegant because you're using pattern
matching here to detect the type of the
object well it's basically the same so
you could also we could also say
something like readings filter is
instance of valid reading so this would
this will be the same it's as I said
it's it's is the measure matter only the
matter of style so we now we're ready to
compute our average so if there are any
valid readings we want to extract value
from them the value from them value
mmm actually it let's check the type of
it oh no so the type the type doesn't
matter yeah because so actually there is
a difference because because here the
output type is still still early reading
so we're getting a sequence of readings
and if we just collect we only we will
get the actual value twitting so well
thanks for this question because this
was something new for me so we're going
back to the collect variant and we
return this one and now when we check
the type it's actually a sequence of
value trading so since we only we only
collect it as a single type the compiler
knows that the the type of the output
collection will be a bit narrower so now
we're going to sum the values and divide
them by the size of valid readings of
course this might be a bit of overhead
when there was a single value twitting
only because then we could just return
it but for the sake of like simplicity
we don't want to write another another
like if branch here just let's say that
will be like a bit not not to perform at
when when created computed as you can
average of a single reading and
otherwise if if the if the value twins
were empty so they're well none of them
will just return dummy value of minus
one and what we want to return here is a
valid reading and since since we always
had two two readings as an input we can
just take the first of them and take the
ID from there and for the value we'll be
using the average that we just computed
so this is the average computation step
once again we take we take our reading
stream groupid bye bye bye bye bye in
groups of two so that we have the like
two two lines in a single collection
then we extract then we filter it to
extract only the valid readings and then
we compute the average so the pre-final
step will be to store our readings in
the database
and this is well this is going to be a
flow that accepts a valid reading since
we only want to store value readings in
the database the output value will be a
data type will be the type that is
returned by our Cassandra driver
somewhere under the hood so we don't
really get into details but this is a
result set from the from data stacks
driver and the materialized value is not
used once again so this is also pretty
simple because we have a flow that takes
a valid reading we can store the
readings in also in a nursing fashion
but this time since this is an i/o
operation we'll use a different level of
parallelism and we have a configuration
parameter called concurrent writes so
this so that we can we can adjust how
many rights we do in parallel here we
get a single reading well and we have
something something called reading
repository that has a safe method and
the safe method well it takes a value
reading can return the future of the
result set so that's everything we need
to know about it under the hood it
stores the value to local Cassandra
instance so we're going to save the
reading and then we will just do some
logging just that our logs are not
completely empty the under Dan method is
something it's called a Combinator on
the future and what it basically does
it's well it can it can it can intercept
the future when it is completed do some
stuff and return the original future so
here we are like when the future is
complete we'll do the we will see
whether it completed with a success or a
failure do some logging accordingly and
then we turn the original future so that
that's all and then does well and this
is this is actually the the whole flow
for for storing the readings so now
we're going to combine the building
blocks we defined so far to talk to to
build like a larger flow to process
which will be capable of processing a
file and returning the result set so we
will call it
import single file
this is going to be a float that take
takes a file returns the result set and
we don't care about the materialized
value once again so now this is a this
is flow of a file sorry and now it gets
pretty straightforward because we just
use the via method to define the the
subsequent stages through which we want
to pipe our our data so the first first
date will be parsing the file when we
parse the file we want to compute the
average and then when we have the
average compute that we want to store
the readings and this is it so this is
so you can see that when you when you
define your building blocks as variables
or as valves here in Scala you can you
can you have a pretty simple DSL that
just lets you combine them and it's
important that this is everything here
is typesafe so that the compiler checks
what are the types of the flows are our
file for example if i if i switched it
switch the order the compiler would say
that well the compute average actually
accepts a valid reading and not a
reading which is emitted by and not
files so so so on in compiled time we
have we have we already have some checks
whether it's whether it would work or
not oh actually whether it's legal to do
or not so this is almost our entire
processing pipeline
so let's now define a method which would
do the ACT what actual processing so we
would well we have so we have a
configuration parameter for our import
directory we want to exist files in the
directory and then like put all of them
set send all of them through our
processing pipeline so let's define an
import from files method and what it
will return is a future well because
it's it's everything happens in an
awesome fashion so it's non-blocking etc
and the type of the future would be
another mysterious type from akka called
done and done is a bit similar to not
used because it's basically a
unification of
again avoidant unit and he in and when
not not used indicates that we don't
don't care about the materialized value
of a flow and done indicate that we
don't like don't care about the value
that is emitted by after after running
the graph so it basically says that well
let's run the graph actually our graph
we have our graph when it's run we'll
have a side effect because it's going to
write to the database so we actually
don't care about the about the value
that we that will be returned so we just
care about the fact that it completes
and this is what what what done is for
and technically it's just a unification
of scholars you need in Javas void
so we have our configuration parameter
for for import directory we just list
files convert it to least from an array
because well we have when we'll be
creating a source from this sequence of
files and not custom source and it has a
constructor that takes a list not an
array so that's that's just just for
this we will start in a variable let's
make some more space here now we want to
do some logging will we may want to
measure the time it took to it to import
it so let's let's have let's have a
start time here now we're going to have
multiple files that we want to send to
our pipeline but of course we wouldn't
want to send them one by one so we would
also leverage the power of ARCA streams
and another parallel processing so we
would like to do a kind of load
balancing we would have would like to
have an input to our to our graph that
would load load balance the files across
number of workers so we knew we would
like to define a number of workers which
would process the files in parallel and
and do some kind of load balancing so
there is one final state will need to
flow we need to define we'll call it
balancer and so far we will only stay
with the type so it's it will it's type
it's similar to the import single file
for well because the balancer takes a
file it returns a result set and we skip
the materialized value let's leave the
implementation
unknown for a second but basically what
it will be doing to be taking files load
balancing them between a number of
workers and then then like merging the
the results of the load balancing and
sending the part files one after another
to the to the next steps of the pipeline
so now to run our entire graph we'll we
need to start with a source as this is a
starting point for for for data
processing so we have a source object
and here we pass the files the list we
created and what we want to do is like
pipe it via our balancer because the
balancer will be taking care of actually
executing the import single file file or
not executing but passing data there and
now this is this is the first place when
we actually do something about running
it so so we say run with and well we
define the source we define the
intermediate steps the only thing we
have to define now is the sync and in
our case as I said previously we
actually won't care about what what what
is the value returned by our processing
pipeline because we only want to want to
know that it has finished so there is a
handy string called ignore which
basically when you run the flow with
sync ignore it returns the future of
done so that's exactly the return type
we want now we can we can and it returns
the future so we can once again use the
end then Combinator to do some logging
it will print the elapsed time and and
stuff like that okay so now it's time to
implement the the load balancer itself
and here we'll use a a bit different
approach than we used previously because
we won't be using the the built in the
building stages so that the methods that
we can call in a flow but we will build
a simple processing as a simple graph
from scratch using a graph DSL which is
another way in Arcis trims to define the
processing steps so we'll have a
balancer graph and now we're using the
graph DSL to create it what we get here
as an implicit value is something called
the builder and builder is basically
like our our way to access
the structure of the graph so if if we'd
like to want it if you like to add
something a processing stage to the
graph we'll be using the Builder builder
instance to add the elements will also
use import the DSL for 4/4 well the
methods of the graph DSL
you'll see them in a second and now as I
said we want two steps well first of
first of all we want that balancing step
that would just like take a file and and
choose choose a worker that is available
and then send the file to the worker and
then after after after we do the
balancing we have we have split our
pipeline to like a number of a number of
sub pipelines so and after they are they
are done we want to merge them back so
so we have like a single input and a
single output but we have multiple paths
of pipelines in the middle but we want a
single output so the first stage that we
want to add will be called balance and
now we call the builder add and we have
a built-in stage for load balancing
actually we need to provide a type it's
type would be file and the number of
number of branches it's also it's one of
our configuration parameters it's called
concurrent files so we have the
balancing step and now is which splits
the pipeline then we want to merge the
results back so we'll need another stage
called merge which is also a built-in
one
so we going to add a merge here the type
of the type parameter of the merge will
be the result set because that's the
like final thing that comes from out of
our pipeline and the number of of inputs
to the merge block will be the same as
the number of outputs of the of the
balance block and here of course well
there's no way to verify it at compile
time but this would actually be verified
at run time so I'll implement one more
step like connecting everything together
here and now we'll be using the fancy
arrows from the graph DSL so now what we
need to do is to basically iterate over
our concurrent files and for every like
the number of times as
the fighting the concurrent files we
need to connect we need to add like a
like a connection between the the
balance stage the data from the balance
stage should be should be sent to import
single file and from there it should be
sent to the merged stage and the RS you
see here are from the from the from the
graph DSL so if you had a bigger graph
it see it well your code could look
almost like a drawing so with with all
the arrows connecting all the building
blocks and this defin oh yeah and what
we have to return at the very end
because this is this is a graph and
every every graph I said at the
beginning that actually source and sinks
are also are also shall also flows or
there are graphs actually and every
graph in our customs has a shape so and
a shape basically says how many inputs
and how many outputs a graph has so a
source is a graph which which has a
source shape which means it has only a
single output a sink shape has only a
single input and in our case it would be
a flower shape and a flaw shape has a
single input and single output so just
just as we define the flows before but
here we here we're just losing the DSL
and oh and the input of our flow would
be the input of the balanced state and
the output will be the output of the
emerge state so this is the definition
and here and our actual balancer flow
will be built from the graph we defined
just like that and now the things here
all the connections between the stages
well they are not verified at compile
time but it would be verified in right
at run time so so as when the graph is
actually built and materialized
I cut the materializer is going to check
whether whether actually the number of
inputs and output matches and if not
there will be an exception yes
mm-hmm
yeah
yeah
you're right I haven't noticed that
before so the question was whether we
aren't actually making more concurrent
rights that we will that we configured
and actually we are because now when we
seen store the store readings
part the store readings flow is a part
of the import single file stage which we
are executing multiple times because we
are reusing it within the load balancer
and yes actually the now the number of
inserts would be would be concurrent
files times store times
concurrent writes yeah unfortunately I
don't have any prizes if I had him you
would certainly get what what sorry for
that okay well the last thing we need to
do is to somehow call the method we just
we just wrote and so here we have our
simple application we have the CSV
importer and we say import from files
so let's now jump to our compiler and
see what happens so we're going to run
SBT
and compile now we see there's a number
of problems here well there are actually
two kinds of problems first of all is
that we are doing some manipulations on
futures and to do some manipulations on
the futures for example to use the end
end Combinator we need something that is
called an execution context which
basically is a thread pool that is used
to execute the async operations and in
scala you need to be you need to be
explicit about what kind of thread pool
do you use so it's it's actually a good
idea because every time you're you're
doing some asynchronous processing you
should like think well what what kind of
processing it is what kind of resources
many tend what kind of thread pool do
you want for it actually you can you can
make a workaround which we will do
because there is something called a
global execution context which is
well-defined it's always in the
application and using a global execution
context especially for for i/o
operations may not be the best idea but
for the sake of simplicity in our
application it would be enough and the
second kind of problem is the it's at
the very end is that we want to run our
graph but as I said our graph is only a
recipe for data processing and actually
we don't have we don't we don't know yet
don't know yet how to run it and for
this we need a materializer
and this is also a materialized there is
an implicit parameter to the run method
or run within our case so here here the
compiler says that hey you need a
materializer so you need something that
will actually run run your recipe so
we're going to be using the the the
actor system based materializer so the
default one so let's just use an
implicit actor system let's divide
define an implicit actor system so that
basically an implicit value here says
that well the actor system will come
from somewhere when we went when we when
we create the class and it will be there
it's an actor system and actually an
actor system well it has a it has a
thread pool that we will be able to use
as the execution context so actually I
lied before because we won't be using
the global one
we'll be using the the one from the
Explorer system which is even worse idea
in the real life scenario for especially
for IO operations because the the
execution context from the extractor
system is responsible for all the
communication between actors so if you
execute an i/o operation on the cyst on
the execution context of the actor
system and if you block it by by some
accident then then the entire exit or
system may stop functioning so don't do
it in production this is this is all
only for for simplicity but this this
this is the fastest way to obtain an
execution context here and here since we
we have the run with method which what
was the shortcut yeah so this is this
shortcut that shows the implicit values
sorry that are needed and you can see
that once again that actually the wrong
with method takes an implicit
materializer so that's what the compiler
complained actually so if we need an
implicit materializer let's create one
it's an actor material I sorry and it's
a value and here you can see that actor
materializer takes an implicit actor
system but we've already defined an
implicit actor system and in the class
definition so we'll have it here and
actually where the where director system
comes from is here so in the in our
application we define an actor system
for the application it's an implicit
value so when we create a new instance
of the CSV importer here it is passed
and an implicit value as an implicit
value so it's it's provided there so
let's get back to the compiler
really
okay I'm not sure whether I can do and
think about it okay something is wrong
here
okay this should be fine so I'll just
okay I need to use this one because it's
recorded so let's just not not dig into
what's wrong but but just get the
working version it's the same I mean it
must be some kind of a minor mistake
okay so we'll say kanpai oh now it
compiles so we are ready to run it and I
have a docker with cassandra running
here we can connect the to the cql shell
to see what's happening there
we have anarchist rinsed reading stable
which is now empty so now we can try to
run it and see what happens
well it may take a couple of seconds or
depending of whether it's warmed up or
not and actually if you if you go to the
github repository I'll be linking at the
end oh it was faster than I thought you
also have the random data generator so
if you wanted to play with it yourself
you can you can try to generate a number
of of those random files then perhaps
play with the different configuration
settings to see how the parallelism
levels influence the the velocity and
the processing speed
okay so that was for the live coding
part of course there is more as usual
actually you have already seen how to
implement your own your own graph with a
with the graph DSL so if you don't want
to if the built-in growth stages are not
enough you can build your own you can
also build your own flow so for example
that with with some kind of other DSL it
may also be useful because for example
you may need to do some stateful
operations within the flow it's also
possible i factor it in a blog post
about it and contribute at one of the
custom flows to the accustoms contrib so
so you may go to the block and see if
you're interested so that's all I had
today under the QR code you have a link
to a landing page which has links to
github links to the blogpost and some
contact information and filling tanks
fear of my exam guide and you have any
questions I know we I think we're almost
done with the time so you can well ask
now or catch me later I'm here and until
the end of the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>