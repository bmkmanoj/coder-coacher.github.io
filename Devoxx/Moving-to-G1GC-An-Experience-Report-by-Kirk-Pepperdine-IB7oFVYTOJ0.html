<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Moving to G1GC - An Experience Report  by Kirk Pepperdine | Coder Coacher - Coaching Coders</title><meta content="Moving to G1GC - An Experience Report  by Kirk Pepperdine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Moving to G1GC - An Experience Report  by Kirk Pepperdine</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IB7oFVYTOJ0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great hmm everybody enjoy lunch enjoying
your day awesome okay so you're going to
talk about tuning g1g see is anyone
tried to tune it yet here one how did
that work out just at the start okay
that's that's good no one else has tried
is anyone trying to use G one few people
how's that working out so far no problem
yeah okay well that's that's that's
really good right because you know then
you don't need to be here right you can
often work if it's all working right and
that's and that's basically the message
we have here right it's like you know
how do I tune g1g see well I set the max
heap size to some appropriate value and
if I don't happen to do that then of
course it's going to be automatically
set to be one-quarter physical RAM and I
set the pause time goal and if I don't
set that then that's 200 milliseconds
the default value and that's how you
tune the gene one do you see any
questions you're all laughing that's it
that's the presentation enjoy your day
right right so you know so obviously
we're all here today because sometimes
it's simply not quite that easy right so
we'd say yeah if only it was that easy
then when we'd be okay so okay so
marking slide here so all the techies
can turn off for a second last year my
CEO said you realize you've tuned about
2000 JVMs this year and I said no
couldn't be and where it's can't be that
much so I went at it up and I realized
that he was wrong right but he was wrong
because I tuned approximately 3000 JV
for trying to move them to g1 for what
reason I don't know I have no idea why
but that's where it is and we sort of do
this under the brand of J clarity where
we're building the what I call the next
generation of performance diagnostic
tooling or diagnostic engines and trying
to bring predictability into the
diagnostic process and of course I've
co-founded jaikrit if you don't know
this is the hottest unconference on the
planet yes so anyone ever
we have one person who's attended here
yes okay yeah you did test right it was
hot very hot yes and some other stuff
and everything like that so you can
Google if you wanted that one so as I'm
talking heat to you today I actually
this is a new thing I just wanted to
start talking about it I've been talking
about it implicitly for a number of
years now I just wanted to I think I
should make it explicit um a lot of the
things I do is I just simply build
models and you know why do I build
models well because models helped me
ignore the complexities of real things
so they can have a better understanding
of how they work right and it really
helps in a lot of other places too and
in the case of garbage collection it
helps in the sense that it helps you
develop a cost model so you can see
what's going on so if things aren't
working right you can say okay well why
now I have this model and I can sort of
pump the information into the model and
hopefully it'll provide me some answers
as long as I'm not dealing with the
complexities you know so if we're gonna
build a model then we have to understand
what are the important moving parts or
what are the parts in our system that we
need to talk about and I think you know
when we talk about manage memory in a in
a runtime and this is not just Java but
you know it's obviously it's anyone
seasoned that nice C++ or sorry C CLR
diagram with the garbage collecting
units there you know it applies to that
those guys also write and you and you
can see there was like in that chart
it's surely cleared there
you know hotspot sir the JIT was hot for
changes and the garbage collector was
hot for changes and and for me that
wasn't a surprise because there's a
there's an intricate coupling between
these things because you know the you
there's more than one moving part here
it's not just collecting memory we also
have to consider that we're consuming
memory and if we're trying to move to a
more concurrent model with garbage
collectors which is the trend over time
then we have to look at the the effect
that mutations have on the ability of
concurrent collectors to actually
actually function right so there's three
really big parts here one is allocators
one is mutaters and one is just the act
of collecting so you know what
allocating mutating and collecting those
are the three steps that we have to
consider and of course to keep this all
consistent
we need barriers and if we're getting
our code then of course the JIT comes
into play because it needs to insert the
barriers all in the right place in order
to maintain consistency in our runtime
so that's why I was not surprised to see
the JIT and the garbage collector so
tightly coupled because in reality they
are very tightly coupled right so so we
have this seeming battle between
allocators versus collectors and you
know the nice thing about having a
collector is that is that it takes away
all the responsibility that we have for
managing memory so we just allocate
allocate allocate allocate and somehow
magically in the background we have an
infinite or as I say in the u.s. I guess
we have a bottomless amount of memory we
can just keep consuming and it's really
good and but in order to in order to to
give you this illusion we have to do
something right so that's what the
garbage collector comes into play so we
can balance the work between the
mutaters threads that are doing the
allocation and the garbage collector by
saying okay we won't have a garbage
collector so that puts all the
responsibility for managing memory on
you guys or we can take part of that
responsibility and automate that and
that's what the garbage collector is for
but that point what we want to do is we
want to if we put all of the collection
work onto the garbage collector then
we're gonna end up with really long
pause times while the garbage collector
does his maintenance step we don't like
that okay if we go concurrent then we
got another issue because we're doing or
modifying memory well we're using memory
and of course that you know if we don't
do that right that can lead to
corruption and things like that so we
need to manage that also right which
means that if we're gonna run the
garbage collectors concurrently then we
need to somehow put some of the workload
on to the mutator threads now if we put
workload on to the mutator threads of
course we have overhead on the mutator
threads which means our application runs
slower all right if we put all the work
in the garbage collector of course we
have longer pause times as we try to
maintain this consistent view of memory
so means we have to stop everything what
we're doing things right so those are
the battles that we're trying to fight
and in the beginning we had just a
single threaded stop the world collector
which was basically dump all of the
responsibility onto the garbage
collector and you know the mutaters
would just go as fast as they can of
course we had long pauses because of
that and I said okay I will be shorten
that up by paralyzing things right that
worked for a while and then we said okay
we've so we can start running partially
concurrent and of course you know that
that worked for a while and then we're
saying okay well we need to do actually
do more to actually try to get more
concurrency and and try to get you know
basically more throughput with smaller
pause times so one of the schemes that
came up with was was g1 of course now
we're very shortly going to be looking
at Shenandoah and said GC as being
possible replacements for
one in in the future as I said you know
the battle on the pause time is
continuing and as it continues that
basically we're trying to get these
things to be more and more concurrent
but the cost is our allocations are
basically our application throughput or
mutations allocations and accesses are
going to be slightly slower so it's
going to cause us some drag on that end
so let's see what we can figure out you
know how the G one fits into all of this
we have things we need when we need to
have a Java heap because that's where
we're allocating into in the case of G
one we're going to take that heap we're
gonna divide it up into regions and we
need a collector so we're gonna rely on
our trusty mark-sweep copy collection
algorithm it's gonna be really nice and
we're still going to use this idea
generation so one of the one of the
things we use to speed up collector
collectors in the old world was we said
okay we're gonna use this thing known as
a weak generational hypothesis the weak
generational hypothesis says data
doesn't stay live for a very long period
of time as a matter of fact most of the
data you allocate is no longer useful to
the application before it can leave the
CPU alright it's dead so we can create a
nursery do our allocations there and
we're using some sort of copying
collector now and we use a copper
collector we're going to copy the live
stuff out to some other place so that
gives you the idea that we have a
nursery and we have some other place for
copying and if you think of this as a
cost model so let's go back to cost
models and everything like that so we
think tend to think of garbage
collectors is we're collecting all of
the unused memory now we've just flipped
that over with this particular idea of
using generations and copying because
now instead of our cost model being
dominated by the amount of memory we
have to collect it's not gonna be
dominated by the amount of live data
that we have okay so in this model live
day
is bad dead data is good so as a
developer how can you use that to your
advantage
well think of it this way right if I
have globally scope variables how long
are they gonna live if I have locally
scope variables how long are they gonna
live right so what am I going to prefer
local scope variables yeah so if you
follow that particular idiom which we
know is one of these things that says
you know it leads to other benefits
because how much of a concurrency issue
are you know terms of concurrency bug
issues do you have with locally scope
variables pretty much none close to none
if not none how about with statically
globally scope variables a lot right so
it's so it's so so you get in this idea
that that the garbage collector is still
there it's going to support good coding
practices if you follow good coding
coding practices all of this stuff is
just going to magically work for you if
you don't then of course you end up with
a whole host of problems garbage
collection being one of them okay so we
have some generational things creeping
into this whole notion here and we have
some supporting data structures that we
need we need a this thing called a
collection set and we call it as he said
we need these things called remembered
sets or our sets we have this thing
called an are set refinement queue that
we're actually going to use right and
then once we deal with sorting out what
all of those are for then we can look at
possibly how how we get some data out of
this thing to tune it okay so we're
gonna allocate this large heap space in
1 or reserved a single contiguous chunk
of memory at JVM startup and we're gonna
divide it into approximately 2048
Regent's so we'll take your max heap
size and we'll do some math on it and
then we'll select a region size which is
appropriate to give us approximately
that number of regions so it's could be
1 2 4 8 16 or
two megabyte region sighs okay so we're
gonna end up with something that looks
like this we're gonna put all of these
regions into a free list and then this
wonderful thing technology called organ
ah mcc's is going to come along and it's
gonna say okay so many of these regions
we're gonna say that's our nursery okay
now your application starts and what
does it do it starts well okay I need to
allocate something so I say new foo and
then what I'm gonna do is I'm going to
grab the region from the free region
list and then I'm gonna start allocating
into this as I normally would so I'm
just gonna go fufufufu right so label it
is Eden start allocating all my objects
in it when I fill up that region or I
hit a waist percentage then what I'm
gonna do is I'm gonna grab another
region I'm gonna refill it
grab Phil grab Phil grab Phil when I hit
my allocated number of regions then I'm
gonna start what's known as a young
generational collection now of course
for the young generational collection
we're going to use our mark sweep
algorithm which is our workhorse
algorithm for tracing collectors and
we're gonna take this and we're going to
paralyze as much as we possibly can so
we're gonna break this thing up into
things we have to do serially and things
we can do in parallel so one of the
first things that we have to do is we're
gonna have to decide okay what are we
collecting so I'm going to take all the
regions I'm going to put them into the
sinkhole to see set and then I need to
calculate a route set for the cset so a
route set is going to be all those
objects that were that by definition are
live now they may or not be live but
that's a different issue okay so we're
going to come up with the definition for
what a live object is we're gonna say
okay fine that's external to the cset
it's pointing to something into the cset
we're gonna put that into our live
object set
and then I'm going to mark everything
that's live and while I'm marking it I'm
going to copy or evacuate that live data
to a to space okay and then I'm gonna
put the Eden regions back on the free
list and I'm going to use economics now
to recalculate the number of regions to
allocate to Eden and the game starts
again
so it's wash rinse repeat okay now when
I build the cset for the next collection
of course I'm going to put the two space
regions into the cset and you end up
with something that looks like this okay
so when you see these types of diagrams
out here okay
unlike any other diagram you're gonna
see on g1 they're all wrong this is
correct because this is these diagrams
are actually being drawn from data to
derive from live systems all the other
ones are this is what we could possibly
do what you want so these are sort of
imaginations so you're gonna see that
basically we're allocating Eden from the
lower right going up now what happens is
that after your data survived a certain
number of collections either 6 or 15
depending upon it has two default values
right so and 15 is actually the max
value so if you start tuning trying to
tune the G one collector it'll default
to 15 so we'll say after 15 collections
the data will actually then start being
allocated into ten your space so I'm not
going to evacuate to another two space
I'm actually going to evacuate the data
that passes the tendering threshold up
into a tenured region and it starts
allocating from the upper-left
so that's gonna be from the higher
dresses down okay so the two are going
towards each other like this right so
let's revisit so we do remarks we we did
a mix of parallel cereal phases place
all young gems and see set calculate a
route set for the C set so okay so now
good
since the question are where are the
routes for the cset if I'm doing a young
generational collection in other words
where are the pointers that are external
to young Gen right and of course the
answer that is there in ten years faced
okay cool
that leaves us with a time complexity
issue in order to scan for roots I need
to go through all of tenures face and I
need to look for things that are
pointing into young generational space
all right
and this is the problem with the old
collectors the old collectors actually
had to do this we had to scan all of
tenured space to find the roots that
were pointing into young generation
space before we could do a young gent
collection which means that our if you
look at our cost model scan for roots is
going to dominate and scan the time the
time complexity for scan for roots is
going to be linear to the size of ten
years face which is ik right that didn't
that doesn't scale that gets us to about
eight gigs 12 gigs maybe 16 gigs over
lucky you have tuned up to 24 gigs with
the strategy but after that you know
it's game over your pause times are just
gonna get excessively long right just
simply because scan fruit starts to
dominate the process and it just takes
you long okay so you know where our
roots
well our roots are up there so what are
we gonna do well let's introduce this
thing called a our set or remembered set
and I'm gonna lie to you now so just you
know I'll tell you this up front this is
not how it actually works
the mavs behind this are rather complex
and you don't really need to know it
unless you're interested what we're
gonna do is I'm going to take each young
gen region and I'm going to create a set
of cards for each
region that's pointing into the young
region right so if if tenured region
pointing into young region there's going
to be an associated deck of cards and
what I'm going to do is when I as I
mutate okay so if I have foo and tenured
and I've fought in in young if I say foo
equals four then what I'm going to do is
I'm going to go into the card deck and I
can say okay well there's a bit there
I'm gonna set that bit and that bit maps
back to that region and that address
value so then basically any doing is I
am recording the pointer value in the
bitmap okay now you think of it this way
right every single time your application
has to mutate we're gonna have to update
the are set right every time you access
I'm gonna have to go through some sort
of barrier to make sure all of this is
consistent so you know so this is this
is a problem and it gets even worse
because if I'm basically collecting
concurrently then I certain somehow have
to track all of these mutations in a way
that I don't disrupt the garbage
collector also so there's a lot of extra
work that has to happen in order to
maintain this data structure and we have
another problem right if we have 2048
reach and say we have I don't know like
500 of them are young gen and we could
have like a thousand that are tenured
then we have a space complexity issue
and this is why the data structure gets
quite complex and it's quite expensive
to update so we're not actually going to
have the mutator thread update what
we're going to do is we're going to
queue the information and what's known
as an are set refinement queue and we're
gonna have our set refinement threads
working in the background that are going
basically going to take this data and
update the remembered set right before
the collector can run this data
structure has to be empty so they
designed it to be zonal and what they're
saying is that I can take the pause time
goal remember the pause time goal yeah
more or less yeah well that's 200
milliseconds right so basically what I'm
gonna say is like okay well I know how
long it takes me to update so what I'm
gonna do is I'm going to try to make the
white zone be equivalent to about 10% of
the pause time goal so it's 200
millisecond that means I can spend up to
20 milliseconds
excuse me draining this queue as part of
the as part of the collection phases
after that I'm going to start the
remembered set so there are set
refinement threads and by the time I get
down into the red zone I'm gonna say
okay you're just mutating too fast what
I'm gonna do is I'm gonna start
capturing the allocation threads I'm
gonna slow down the your application
threads by having them participate in
the are set refinement step okay cool so
that covers sort of like the young
generational space so we're gonna do a
mark copy or mark evacuations what like
what I like to call it right and so
eventually we're going to you know
support and so eventually all this data
is gonna end up in tenure at the live
data and eventually 10 you're just gonna
get bigger and bigger and bigger and
bigger and when it hits this thing known
as nigh hop which is an initiating heap
occupancy percent you know if I'm in the
US and I can basically say not the
pancake house if you know what that is
but when you hit the IHOP and that's
about 40% of 45% of total heap then
what's going to happen is that I'm gonna
trigger a concurrent mark of tenured
space there is no sleep only a mark okay
and we're gonna try to do as much of
this work concurrently as we possibly
can so here's the number of the phases
here I'll just leave them for reading
for later but you can see the red ones
are stop the world the green ones are
concurrent and what I'm going to do is
I'm going to mark all of the live data
and in the process I'm going to
calculate what's known as a occupancy
liveliness factor
okay now when I build a cset when the
concurrent when this concurrent mark is
finished I'm going to take all of the
young generational regions and I'm going
to take some of the tenured regions and
put them into the cset so when the when
the young generational collector does
the copying out it's going to copy out
the data out of the tenured regions and
put them into new tenured regions now
how do we size the cset well since we
know the occupancy percent we know we
had we can have a rough estimate as how
long is going to take us to evacuate or
complete the collection process of each
of these regions we can just select
enough of them that are actually going
to fill you know pat out our our time
goal so you know here's the number of
regions they have different levels of
liveliness so we'll just sort you see
the two on the left here are empty they
can go back on the free list immediately
on the right we have things that are
really full so this is going to be
expensive because in this scheme the
cost model is right it's copy cost so
full is expensive empty is cheap
obviously for looking at copy cost if
we're looking at the traditional sense
of we're collecting garbage then of
course empty would be expensive and full
would be cheap but that's not the case
with copying okay which means that we
have a number of regions here that are
eligible collection we're going to use
this G 1 mixed GC live threshold percent
flag just set that default value right
now is currently 85 percent which means
that anything is more than 85 percent
full we're just not going to consider it
for inclusion in the C set okay so
here's the region's that are eligible
now what I'm going to try to do is I'm
going to try to complete all of this
work within eight mixed collections so
I'm going to be on generation collection
with an initial mark that's going to
trigger the concurrent mark cycle when
the concurrent mark cycle finishes the
next young generational collection is
going to be known as a mixed collection
and the mixed collection is going to
take a chunk of these regions and I'm
going to have a maximum by do this is
configurable also but I'm gonna have a
maximum of eight different chunks or
eight different mixed collections
happening consecutively okay and we can
set that now often what you see is that
it can do more work so it's just going
to do more work which means you get
fewer mixed collections I don't really
like that because if you get a poor work
balance in that case and if it looks at
it and says I'm not going to recover
much more memory by you know by
collecting more of these regions then
I'm just not going to bother and just
gonna stop so there's a waste percentage
there when it hits that percentage then
yeah we're basically finished so at the
end of the day that's basically how it
works there's really only one more topic
to talk about and that is if we have a 1
Meg region what happens if we allocate
an object that's 2 Meg's in size right
now it could be a buffer overrun but of
course we've taken care of that and
we're gonna call that a humongous
allocation okay so here's a nice
schematic that shows you pretty much
what a humongous allocation looks like
so we have anything bigger than half a
region size and we're gonna consider
that to be a humongous allocation so if
you look at the coral bits in there
there's two three coral bits in the
middle that are just floating on their
own that's a humongous region start so
that's something that is you know if
this is a one Meg region that's bigger
than half a megabyte if it's bigger than
the size of the region then I'm going to
need humongous regions continuations
and so you can see at the top there is a
coral meaning that's our start and the
rest of the red ones following the coral
that's going to be a continuation so
that's going to be a single allocation
humongous allocation obviously to get
the humongous allocation in memory we
need that many contiguous regions and
traditionally this has been a problem
with humongous allocations is having
enough contiguous regions to actually
make the whole thing functional make it
work right if you don't have enough
contiguous regions you're gonna have to
force a collection traditionally in
previous versions of g1 this has been a
full GC and a full GC is something you
definitely want to avoid here it's very
expensive
they're paralyzing it I think if the
parallel region version is being
released as ten is being released but
even so that's really not gonna not
really going to be all that helpful
much better to avoid the full G C's so
you might ask okay so you know this is
where we can actually start doing some
analysis you can look at it so anybody
have any idea what the top humongous
allocations might be take a guess wild
guess sorry spring string literals yes
could be more generically that would be
a no meta space is something different
we're not talking about meta space today
byte array cache it's a cache in general
something like that yeah string table
would be a cache form of a cache right
byte array could be a form of a cache
right okay so you know this is one of
these things you know like hibernate
cache or whatever like that you created
at the beginning the application it
stays there in memory it's stable it
lives it's it's fine right okay what
about the things at the bottom down here
anybody have any idea what they might be
this might be a little harder to guess
but
could be temporary buffers most likely
in this case of you temporary buffers
from Jason right marshaling on
marshaling things like that probably
that was I think that was actually the
case here right quite often you'll see
transitory humongous allocations being a
result of of what it called marshaling
protocols such as Jason okay so anyways
so if you look at this data you can
sorta get an idea what's going on inside
the application that's as an aside when
do I have to finish this by anyways
about 110 or 210 just just for timing
okay
the nice thing about a humongous
allocation is the definition is a
variable in the sense that it's half the
size of a region so you know how you can
get rid of some of the humongous
allocations make the regions bigger okay
yeah I kid you not
but but it comes as a cost because
updating the remembered sets is
nonlinear and it's nonlinear to the size
of the region so bigger regions mean
more expensive our remembered set
updates costs it's really kind of like a
bit of a problem and really the the
worst case condition is if I just simply
don't have enough memory that I'm just
going to trigger a full GC for that
reason it tends to maintain this
reserved area in the middle that's about
10% of total heap and this here is just
simply to try to mitigate having to
perform a full GC in order to satisfy
humongous allocation okay that said
here's a newer version of our
application and what you can see in this
one is I have dark blue light blue dark
blue is occupied light blue is not
occupied space and you can see that you
know that these are all tenured regions
and you can see I got a few eden regions
down here in the bottom but after that
basically all of the stuff filling up
memory at
bottom here is is tenured regions so
things really can get mixed up really
badly and and and in this case you can
see a blown right through the reserved I
pushed basically survivor is going to be
pushed out to nothing in this case and
if I don't get a collection very shortly
a mixed collection very shortly
basically your this is this JVM is in
serious danger of suffering from a full
collection very shortly okay and if you
don't have enough room for copying then
that's exactly what will happen is that
you'll end up with a full GC and we see
that quite frequently things will just
fill up this way there's there's enough
memory but yeah it it basically it's not
usable and in order to make it usable I
need more memory and and so in this case
here we just need to make the heap
bigger there's other overflow conditions
that you can solve by taking different
techniques but this one here pretty much
is simply make the heap bigger is the is
the answer okay
how to get a GC log I'm sorry I didn't
get the flags that we use for nine in
here
but you know for most people using JDK
nine now in production
one two actually we had to roll back to
eight just recently so we're at nine so
we had to roll to eight which means we
only had like one offending lambda that
prevent us from rolling back it was a
take while right
anyone been using take while because if
you're not in nine you're not using take
while because take Wallace its yeah it's
very useful anyways okay so if you want
to know what the collector is doing you
need to look at a GC lock and so there's
some flags I'll get you some data and
then you can understand what's going on
now the GC log is this big long blobby
chunk of text which is as I'd like to
say neither machine nor human readable
well most logs fall into that category
of being neither machine nor human
readable so which is why I basically
wrote something to actually parse this
stuff deuce
analytics on it and gives us some
visualizations on it so in this case
what I want to do is just say okay let's
look at a GC log using some tooling here
uh lovely I guess I'll just push this
over here cuz I don't know where any of
my things went to okay so here's a GC
log that I preload it's from one of our
client environments and you can see we
have a number of analytics to make sure
first off we're getting enough
information here we have we're looking
for perm space or meta space triggered
collections in this case I don't know
why we have perm space showing up I
guess it determined it was a log from
700 which it shouldn't have been oh yeah
in this case it would be okay
and again if perm space fails or meta
space fails or hits the meta space more
correctly says hits certain thresholds
then that can trigger a collection and
that's something that people often miss
so for the things that people miss or
things that people are not as well known
what we're gonna do is we're gonna throw
an analytic up there and basically say
hey did you realize and there's a number
of other different analytics we have up
there that are give us proxy measures
for things so we'll pick out you know
are there calls to system GCS as an
example
oh here's what's going on with our mixed
collections so in this case it looks
like we could probably do some tuning to
get a better balanced of mixed
collection distribution here we also
look at CPU times and quite often well
often enough we'll get people coming to
just say we have a positron problem with
our application we near a positive get
shortened and what we'll do is we'll
look at these statistics and we'll say
it's not the garbage collector that's
the problem it's your environment or
it's something else going on right so I
can tune your garbage collector for you
it's not going to help you in this case
right so we'll certainly do it if you
want me to tune your garbage collector
give me a call I'm quite happy to
but you know I'm certainly going to tell
you if it's going to help or not you
know so that's some of the things were
look at there's more metrics that we
have that are not visible right here but
for instance we can certainly give you
indicators as to if and why your system
has gone into CPU so there's some keep -
small analytics up there that'll look
for things that basically say this heap
is - your heap is too small or this data
structure is too small you need to set
this flag in order to try to mitigate
this particular problem so here's a view
of heap occupancy after the collection
and and and you can clearly see there's
an issue here with this particular log
file first off the triangle bluish
purplish triangle things that's your
heap occupancy after a young djenka
lection the blue dots are heap occupancy
after mix collections so you can see how
they're clustered all right so that's
hitting 10-years face we have G 1 GC
cleanup which is near report heap sizes
so we're looking at heap sizes after the
g1 GC cleanup phase which is part of the
concurrent mark cycle as a can anyone
see the issue
yeah we're consistently losing memory
over time so this could be you know this
is symptomatic of a memory leak in
inside the application and it's only
four hours of log so it may not be a
memory leak I don't normally if I'm
going to diagnose a memory leak I want
at least a business cycle of data which
is typically 24 hours but can be
different links depending upon the
business you know so this could still be
warm up I'm warming up caches and things
like that but in general I'm going to be
concerned that I've heap instability so
potentially a memory leak in this issue
some of the other useful views here I'm
just gonna look at this one here this
one here is going to be memory pool
percent sizes so there's some other
conditions that we run into that are a
result of
the g1 organ ah mcc's deciding to make
young space too small so by default it
can get to 5% of total heap right in
terms of size and sometimes it's okay
and sometimes it just C small in this
case here it looks like it's 22% in most
cases and well sometimes it dips down
right so sometimes we'll just say don't
shrink that much and that will clean up
a lot of issues right in terms of GC
frequencies and things like that so
sometimes you can look at that I don't
know how much time we have here so I'm
just gonna try to slip through to the
more interesting points for this one
here here's our pause time view so for G
1 actually this isn't too bad I mean for
me it kind of sucks but we're going to
be very difficult to get much better
well the way to get better is to not
have so much live data all right okay we
have failure events here there are none
lovely I'm gonna come down into someone
here we have reference processing times
here right and you can see sometimes the
reference processing times can take up
to most of the posit on budget actually
if you look at
other phases % that's all these green
dots there's reference processing
parallel reference processing is not on
by default if you have something that
looks like this your easy win might be
just to turn on parallel reference for
assessing right as a matter of fact if
you look at this you can see we're
processing mostly soft references at the
rate of about 50,000 per second but we
also have this trace going up here so it
looks like some soft reference cache
thing happening right so for sorry
reference cache happening for weak
references the other reference type seen
me down so anyways you can get some
ideas as to reference processing inside
your application and that might be an
issue also tenuring summary here we look
at premature promotion in other words if
data is going to we definitely don't
want data that's going to not be long
live to end up in ten year space in this
case here our premature promotion rate
is 0% that's because we've already tuned
this thing to get rid of that particular
problem but if we get like you know 40
50 30 40 50 60 or higher percentage of
premature promotion that means our young
generational space is too small we need
to somehow not let it shrink so much so
we can capture all this this
soon-to-be-dead data down here so we do
draw out a tendering distribution curve
so that you can see what's going on here
so you can possibly start a tenuring
threshold from here it looks like here
15 is probably the best value because
we're still getting memory good memory
recovery okay and if you want to look at
the ten during the amount of data and
ten hearing spaces over time then you
can see that's pretty much what it's
actually looking like okay so coming
back up here we can see all of the
parallel phases and percentage and we
can see that
object copy should really be dominating
should be the dominating phase but in
this case it isn't there's something
else going on we have external route
scanning that seems to be causing a lot
of difficulty I'm not sure why but if we
look at the phases overall the parallel
face should be dominant object copy
should be dominant we don't have that
type of dominance mostly because of the
reference processing and that could be
partly due to the external reference
scanning that's happening so so you know
the next friend I would do for this
particular log as I would say okay let's
redo things let's turn on the parallel
proce parallel reference processing
first let's make sure you know
everything else seems to be reasonably
okay for now and then let's see the
effect that the parallel reference
processing has on the whole thing and if
we have a if if that actually gets to
the point where we actually have object
copy dominating the the picture then
that's a completely different issue
that's an that's essentially it's an
unsolvable issue right because if you
talk to the g1 guys here did you know
continually working on g1 they're saying
they want to get rid of all of the other
costs so that the generally the only
cost you're gonna that's going to be
visible to you is object copy cost cool
right so you know so the question is you
know
well if object copy cost is my
dominating cost and I still need to do
something about it
then what can I do well on the surface
it doesn't look like there's anything
you can do okay so the yellow line is
the weak generational hypothesis so it's
basically an indication of how much you
know it's an indication of when data is
going to be is going to die and you can
see as I said before most of it it dies
early on and if it stays around it stays
around for a long time and it only dies
when it gets really really old or when
you shut down the JVM or something
like that right more importantly it
defines a curve okay
and if you have a curve all sudden we
have all kinds of mouths we can apply at
it and apply to it and do things with it
like you know this wonderful calculus
stuff does anyone remember calculus yeah
okay sorry to bring it up again that's
depressing but okay yeah and we can do
things like you know area under the
curve so what this curve is actually
saying is that the amount of data in a
stable heat is constant over time for
some variable definition of constant
okay but basically you can say at that
level which means that if our max-heap
is the white line and our allocation is
relatively constant well it would be
over time then you can see that our
garbage collection behavior is going to
be fairly constant over time right so
going back to the cost model we have a
garbage collector that has a fixed cost
and a variable cost to it
we can't affect the variable cost
because part of the Demont dominating
part of the variable cost is object copy
which we've now determined is fixed for
each application okay but we do have a
fixed cost so I like this I like to say
I'll use this argument on my wife it
doesn't work but it's like the vacuuming
thing right so if I vacuum every day it
takes me like half hour every day so
it's like two and a half hours a week or
three and a half hours a week vacuuming
if I vacuum every second day then that's
a lot better right because that's only
like two two hours and 15 minutes of
vacuuming a week or an hour 45 or
something like that right but it but you
can see if I can get away with vacuuming
once a week there's only a half hour of
vacuuming a week yeah it doesn't work
okay but anyways the point is I can use
the same principle here and in this case
my wife isn't going to argue back with
me all right I can make the heap bigger
if I make the heap bigger and I'm
assuming I get the same allocation rate
what happens is the frequency of the
garbage collection drops
which means I'm effectively going to
save in the fixed cost because I'm not
incurring the fixed cost at a high
frequency okay and and because the same
curve still applies my variable cost is
going to be the same because we
determine that's constant for every
application right and this works it's
the only thing that I know that works to
actually get rid of object copy costs
slow the collector down give it more
memory right yeah so why is it taking so
long look at the log the logs will tell
you what's taking so long
you can make the adjustments sometimes
if you have eye high allocation rates
you can't tune that out you got a tune
your application other times you can
look at it and say ah I can do something
about that I have an overflow event I
can go and see what that buffer is I can
make that buffer bigger I can get rid of
these failure events right I can make
sure things don't shrink so there's all
kinds of different configurations I can
use in order to try to get rid of
whatever problem the logs the GC log is
telling me that I actually have okay so
did you like sensor it's cool cool so my
boss skype slack me this morning and
said okay we need more gc9 logs for
testing if you have a GC 9 log send it
to us we can get you a free license for
sensing okay
so you just go to J clarity calm and
well you send me a GC log and or
whatever at the support at J clarity
calm and and just let me know and you
know we will we will set flip the
license at you for performance tuning
workshops I have it at my code worksite
and if you want to know more about our
tooling that you go to J clarity and now
that's really the end so now we can
really I don't know if we have time for
questions but if I'll take one or two
yes oh no you're saying we don't have
time
oh okay oh sorry yes thank you for
attending this talk and no no really
seriously thank you even being great and
any questions I'll take them yes sir we
had a customer that needed us to be in
Java 8 and it was just easier for us to
roll everything back then just to do a
one-off for him so since they were
giving us enough money to do it we said
okay we'll just roll back right but it
means that we can go to we compiled with
Java Java 9 we'll compile with 10 so
it's like you know a big deal
interesting question
yeah it's money we're not immune to that
effect unfortunately ok any other
questions yes sir
they're automatically put into a
humungous region the humongous regions
as you can see from the views that I had
were we're always going to be allocated
in tenured space if you turn on region
liveliness info information info I guess
is the flag and you can you'll get this
data dumped I have a project on github
and my github account called GC viewer
thingy
anyways that that creates these charts
so you can visualize that data coming
from the GC locks in that case any other
questions yes ah right I didn't mention
see forest collectors who you know the
Azul guys are very very very very very
very clever probably some of the
smartest guys I know they're collector
is good enough that GC pause times
aren't really an issue operating stall
operating system stalls virtualization
stalls and other kernel maintenance
things actually are a bigger issue then
then the pause times you get from the
Azul c4 collector so and in that way I
think it's quite brilliant that said I
think that what we're seeing with
shenandoah and I don't know about Z Zed
GC yet because you know that's been
basically a phantom thing that hit us
from left field we can see that there's
really been a war on pause this pause
time thing and and and the collectors
are running a lot more concurrently than
they have been which means that the
commercial value in c4 has basically
been reduced over time and like I said
the Azul guys are very very very very
clever they planned for this a number of
years ago which means that they've been
investing in
jet technologies and Justin you know
ahead of time in just in time
compilation techniques and compilation
preservation techniques and things like
that that are actually again giving them
really really good value for going if
you want to go towards a really high
quality commercial JVM there certainly
it's certainly the way to go
c4 was the argument yesterday I think
yeah within two to three years c4 won't
be the reason there'll be but there will
be plenty of other reasons to go in that
direction yeah but I think we're about
I'm guessing there's we're about two
three years off of Shenandoah being able
to be used in production I have no idea
but said you see Oracle claims but you
know it's like the perpetual motion
machine only the inventor is doing it
okay yes we we did but our customers
took it as you must and so we've pulled
them out but we've had paused but in
live we actually have a version that'll
do batching so if you have if you want
to look at no no no like two or three
hundred JVM GC logs at a time and we can
manage that quite nicely and we can
monitor live and the live systems we did
have a positive prediction thing which
means we could give you like a thirty
second we can give you up like up to
thirty seconds head heads-up you're
gonna about you're about to have a long
GC pause right and we had that
approximately 90% accurate right which
is pretty accurate but it gives you one
in ten to false positives which people
don't like you know in other words they
want you to be perfect all the time but
you know this is probabilistic stuff so
yeah I mean if you don't mind being us
being wrong I'm quite happy to put them
back in we don't always get it right in
other words right it's I mean tuning a
collector first off we only ever do it
in production environments second you
never get it right the first time
right because these are emergent systems
and there's a feedback loop with your
application between the application and
the garbage collector which means that
you need to do it once figure out what
the feedback loop looks like do it again
you'll probably get a better idea what
the feedback loop looks like and by the
third time you're probably you're gonna
get it right but sometimes I might take
a fourth or a fifth depending if you
hadn't gone far enough right but so this
is why the predictions are wrong because
people is not that they're wrong it's
the first step you need to keep going
but people don't want to keep going they
just want it all in the first step but
it doesn't work that way okay so if you
don't mind just being wrong while happy
to put them back in but people are
complaining about us being wrong so we
had to take them out or say we may be
wrong I guess would have been better
okay any other questions
awesome then I will wrap it up and let
you go to the next talk have a good one
guys
Cheers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>