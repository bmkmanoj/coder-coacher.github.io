<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>(Deep) Learning to Fly by Krzysztof Kudrynski and Blazej Kubiak | Coder Coacher - Coaching Coders</title><meta content="(Deep) Learning to Fly by Krzysztof Kudrynski and Blazej Kubiak - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>(Deep) Learning to Fly by Krzysztof Kudrynski and Blazej Kubiak</b></h2><h5 class="post__date">2017-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K55tlhPjPOc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello everyone this is what a
Kubiak my name is Christophe kudrinsky
and this is a drone it is in the box and
the world consists of two groups of
people people from the first group will
immediately take the drone out and start
to enjoy flying while people from the
second group will immediately download
the API or maybe even make their own
better one now there is absolutely no
shame to be part of the first group I
have real friends that would do it and
they are nice people
myself and Pasha
we are proud to represent the elite
after over two years dealing with the
drone we have absolutely no idea how to
fly that thing
so a few words about us between 9:00 and
5:00 p.m. we work on an very interesting
TomTom project whose main aim is to
enable cars to drive by themselves and
our contribution here is a set of
technologies that will turn the video
stream and the lidar point cloud into a
precise representation of roads which is
an a enabler for self-driving cars after
work we spend our time with our family's
body with washes myself with mine and
when normal people start to think about
going to sleep we dive into the world of
technologies and we have similar
interests the visual computing robotics
virtual reality artificial intelligence
around midnight wash I walks to sleep
while I sleep to walk
the project we are going to tell you
about was born from passion we did it
instead of meeting friends and instead
of sleeping and in this project the
drone will learn to fly and we will
learn that skills based on what it sees
and it will see the world using a camera
so a starting point for our project is a
calibration chess board you can have a
lot of pictures like that of this very
well-structured object and you can use
simple algorithms to find the distortion
of your camera and corrected in fact
calibration chessboard is a starting
point of almost every project in
computer vision then you start with
detecting a chess board in the video in
the image and then tracking the chess
board in the video these are the hello
words of computer vision however if you
are here to see the hello world
application you might be a little bit
disappointed
we cannot disagree that the technique
chess board is so interesting and it has
so many wonderful practical applications
like detecting of freaks still playing
chess on a chess board up to detection
of people walking in the street with a
chess board and the list does not end
here
but our motivation was a bit different
in these fragments of videos you will
see the drone racing it feels so amazing
to to see that speed and if you realize
that these races were done controlled
manually there is one word that is
screaming in your head and it is
irresponsibility
let's analyze one case this is an
extract from the video of a drone racing
and your task is to apply some control
not to get killed and obviously you know
that you need to turn the
it will be left and fly forward there's
a lot of time to do that there's a lot
of frames where you can make that
decision but if you are starting to
worry now you're doing the right thing
because although there is still a lot of
time to make your decision you know
exactly how it's going to end let us
analyze this one case again you can see
there was so much time to react like in
many other cases which ended in a silly
crush while there were tens of frames
where you can apply proper control tens
of frames begging for salvation together
with was a we decided to put an end to
that as a user you should only click
where you want to fly and it is the
machine that should calculate all the
proper low-level controls
analyzing the visual content that it
sees in the image and today we will all
walk through the steps what needs to be
taken to make this happen on the machine
and we will show you the test results
that we performed while implementing
that actually of course we will not be
showing you the test from ruins like
that or forests but we will show you our
tests from our modern TomTom office but
if you look at it carefully and the
track perspective you cannot miss the
similarity in fact we have absolutely no
doubt that this TomTom building was
built and meant to be our training
ground so let's start to talk about our
solution and as inspiration let's come
back to our first example we all have to
agree that we humans are just amazing
everyone here immediately and without
any effort is able to spot and track the
right place
were flanked everyone here immediately
and without any effort is able to apply
proper control to fly them everyone here
immediately and without an effort is
able to spot the emerging collisions and
act accordingly this is already amazing
but what is even more amazing that using
exactly same brain at the exact same
times we human are able to plan our next
meal and perform deep considerations
about life excuse me general
intelligence like that is obviously out
of question for today science so we will
not consider how to make the drone cook
or make life decisions but we will focus
on three these three tasks and when when
we implement these on the machine
because the machine can be absolutely
dedicated to them and not distracted by
anything else like we humans are it will
quickly outperform human in terms of
reaction times so let's start with the
basics you can start with whatever
equipment you want as long as your
software can communicate with it
we used our drone parrot ar.drone -
because there's a lot of open source
software in the web which controls it
and we chose a drone which is a open
source framework which you can download
there's thousands of lines of code but
in fact we use just one the move command
where you apply the tilt to move left or
right to move forward and back the speed
in change of height and the speed of
rotation only these four parameters and
they in our application will be derived
directly from an image which for our
convenience will be
decoded from using the surplus plus
ffmpeg and this image will be analyzed
using Python libraries including open CV
and Moonpie and the control center to
orchestrate all of that in the beginning
was planned to be to be written in Java
and this is a whole nice system which
which you can start thinking about your
logic and of course it will evolve and
change while and other requirements are
appearing as we go with our fourth
process which will happen and we'll come
back to it later but now let's think
about our logic so the first level of
autonomy which we want is just tracking
so when we focus on some object we want
to track them and because we are fans of
the human brain let's think how we
humans do it how we track objects and
for that
let's analyze a nice real-life example
when we go to the club full of people
and we find something catchy something
that gets our attention and we want to
track that for example a hat but in the
beginning for a nice start let's imagine
something more familiar but you must
imagine it really hard imagine that in
this picture instead of a hat we have a
chess board then we would have our
wonderful algorithm to track it and
everything will work fine unfortunately
sometimes it happens that we have to
track something else and you don't know
what it is until you see it in the image
so this is this particular moment when
this region gains some description it
might be the distribution of colors
inside it may be the shape of the heart
itself it may be just maybe just
following the flow of pixels inside all
of these approaches are easily
implemented on the machine and the good
news is that you don't need to do it
yourself because it's already done for
you in the libraries and the best one
from our side is OpenCV it's a C++
library it is
available in Python it has rappers in
Java and in OpenCV we checked these free
methods in the mean shift in the given
region we calculate the distribution of
colors and then the algorithm shifts and
scales it in the next frame so that
distribution stays the same
come shift is very similar but apart
from shifting and scaling the results or
rotation and finally in optical flow we
will measure the velocities of the
pixels themselves using nice
mathematical tricks involving first or
Delta Rho expansion but the
implementation is not very important
here because we have open CV which at
capsulated at so out of these three
methods the best results for us
empirically were using optical flow so
let's see how it works
in action in our case so this is our
graphical user interface you see what
the drone sees and some visual
augmentation of what it does on the left
and right you see arrows which will
appear longer when there is something
detected and to make the tilt for
collision avoidance in the botton
similarly you can see if there is a
obstacle on the ground or near the
ceiling to change the height but of
course at this stage these three arrows
will remain zero because we are not yet
omitting obstacles we are at the level 1
autonomy which is dragging so we will
focus on the red circle which is now in
the center but when we start flying a
user will click on the table on the
left-hand side and this table will be
tracked and then on the top you can see
the arrow which shows the rotation that
needs to be applied so that we are
flying towards the object that we track
so we start flying and you can see that
the object is automatically tracked and
you can see the strong desire of the
drone to rotate so that it flies towards
this tracked object and if we press one
we will start autonomous mode 1 and the
drone will actually
and now float freely keeping track of
our object very easy that's level
autonomy one we have that and in level
autonomic two there is not much to add
because apart from turning towards the
object we want to actually fly towards
that so we just apply flying forward
which doesn't seem very difficult so we
tested that and it turns out that with
collision avoidance it would be probably
better can you have this sound a little
bit more ok so this is how we arrive at
the third level of autonomy which is
collision avoidance and in order to
avoid risk in fact we need to understand
the content of the image which is
two-dimensional image and we want to
understand the spatial content and this
is not easy and not straightforward
so the question we'll ask ourselves
again is how we humans would do it how
do we know the special content that we
have in front of us and the obvious
answer for that is through the pair of
our eyes so let's imagine this is a
simplified diagram of our viewing system
the black rectangle is what the image is
for so for example the retina of our eye
the dashed line is its principal axis
and a single pixel in this image
corresponds to a very little information
in fact about the object and to be more
precise just the Ray on which the object
lies but we don't know how far it is
until we have another eye which is quite
common and if we find the corresponding
pixel in this second eye to this pixel
on the left then our brain will make
some nice geometry and we will know
exactly where we are looking this is
done in our brain and we can also do it
so the question is
is stereovision our answer well it might
be we might change our kimono camera
with a stereo vision system probably
much heavier probably much more
expensive so before you all go out
shopping let's ask ourselves another
question are we humans hopeless if we
have just one eye obviously not because
while we move our brain has memory and
it is fed with multiple images and it
can be mathematically proven that if we
find seven core at least seven
corresponding pixels in the two images
in non-critical configuration we are
able to reconstruct both the movement
that we done and also the underlying 3d
structure up to a scale factor this is
done in our brain and we can also do it
in our algorithm for example this will
be a video of my chair and in each frame
the algorithm will find some
characteristic key points like corners
and so on in the chair and then their
corresponding points in the next frames
and from these it will calculate the
reconstructed 3d structure which will be
shown on the right this approach is
known under many names structure from
motion multiple view geometry visuals
LOM and there is a lot of open source
projects in the web there is open CV
implementation with which you can do
some simple reconstruction and there's a
lot of books which we can read and write
your own better code if you do it right
it works just perfect especially if you
move horizontally versus the object
providing your algorithm with a lot of
additional perspective information the
problem is that if you move forward
towards the object and additional
projective information is very small in
fact and it may be even so small that it
can can be comparable with the
calibration error and some noises and in
fact it may it may turns out that your
reconstructed word the error is so huge
that the reconstructed world will be
flat which is not true some say it is
but these people are crazy it is even
worse if you rotate because then you
provide zero additional projective
information and in fact in our case when
we fly until we actually see an obstacle
we will fly forward and rotate only so
how can we detect it that's that's a
problem
so in fact however wonderful this
metallurgy methodology might seem it is
not the methodology of our dreams
because we are at its worst case similar
methodology might be to use the visual
odometry itself if you focus on the door
handle and the glass during the movement
you will see that on the picture the
door handle moves slower than the glass
because the glass is closer and you can
use this information to sorry to make
some estimation of depth on that but
again your horizontal motion needs to be
dominant in your movement in order to
make this estimation so the obvious
solution would be to deliberately add to
over drawn some motion sliding motion to
the left and to the right and for us
Polish people and for our friends in
Russia adding a little bit of slaloming
path or motion would probably be
perfectly excused but we would not win
the race so are we doomed to fail
absolutely not and again we will draw
inspiration from the power of the human
brain because we humans
do not need two eyes to understand the
space around us we do not even need to
move around we may stand still with only
one eye and have absolutely no doubt
what's around us how do we know that
from experience from millions of images
fed into our brains with our eyes we
learned to understand objects their
perspectives their relative sizes and
their shadows and through our lifetime
experience of looking at the world we
humans need no tricks to understand the
space and as amazing as it may sound
thanks to the recent advancements in
artificial intelligence we can share
this experience with the Machine and we
will be using a deep convolutional
neural network to pass this experience
to the machine which will understand the
space
baaji will give you some more details
about that in a second
but for those of you who are totally
unfamiliar with machine learning
approaches the idea is that instead of
designing a set of precise rules
algorithms and formulas to classify some
objects to describe them we will be
creating a multi-layer model
mathematical model which will accept all
the pixels in the input combine them
together and produce some output label
and to make it the proper label we will
flood it from the backwards with loads
of images with known labels and this
mathematical mathematical model will
tune itself so that the output for each
given image is the same as the label
that we put to it that's how it will
learn to detect things but you can read
a lot about artificial intelligence in
the books but today we have something
better today we will all be part of an
amazing experiment
in which you will learn how you will
have the intuition how artificial
intelligence works and even more
importantly putting some light on few
concerts concerns related to its usage
in this experiment you will be the AI
and I will train you to perform a simple
categorization task and as every network
in the beginning you have absolutely no
idea what you will be learning I will
provide you some training examples one
by one each with a label and during such
training you will slowly shape your
brains so that finally when I give you a
test example without any additional
knowledge hopefully you will know the
label are you ready let's go
I need the music that's a pity maybe at
least a little bit of music also not ok
nevermind example number one and label
example number two label example number
three label example number four label
now it's time for a test ladies and
gentlemen the allowed answers are yes
and no and you need to choose a label
which you learned that best fits this
example based on what on the training
examples so now please raise your hands
who votes for yes thank you very much
now please raise your hands who votes
for no thank you very much
the results are 5045 for yes and one for
no ladies and gentlemen this is a
correct answer
congratulations you have just
successfully detected a sofa
we are the millionaires we just want the
first price thank you very much for your
active participation it was very
important for me
but now there is something disturbing
there is well there's obviously a sofa
detected as we were here but there is
this one guy that was blinded by some
other less important teachers also
available in this image if there were
more animal lovers in this room we would
miserably fail at detecting a lovely
sofa and now this is seriously a concern
in AI because creating a nicely well
shaped and ambiguous and representative
training set for artificial neural
network is is a hard task and it's an
art and usually you will not know what
exactly is going on inside so you
probably need need a lot of training
data a great amount of training data and
as a designer you have to make a choice
if you can afford it because maybe maybe
using just one example you are able to
make your intuitive alkylate algorithms
which will provide a nice satisfactory
answer and that will be cheaper in case
of our application using a handcrafted
algorithm would be a pain because here
what our task is we have a two
dimensional image at what we want we
want to attach a third dimension to it
in fact a depth because if we have it we
can just detect the places which are
closest using just the color and apply
proper control to avoid them if we have
this and we will train a deep
convolutional neural network to perform
this task
for us but to train a deep convolutional
neural network we need training later in
other words if we want the machine to
provide us with a dev map for a given
image first we need to provide it with
100,000 of examples of this job done
right so how do we get this training
data one idea would be to take a ruler
and for every pixel in this image
measure the distance from the camera to
the object shown in this pixel
calculated it will change it repeat it
for every pixel in this image and then
repeat it for over 100,000 of examples
our rough estimate for that was 300
years although due to low intellectual
complexity of these tasks for hardcore
agile scrum masters that would still be
one story point still we decided to use
a different approach in the beginning we
told that the structure from motion has
a problem because we need to move
sideways and we are only moving forward
and rotate but now we are gathering
ground truth we can move however we want
so we can design perfectly our path so
that structures for motion works
perfectly and that's what we did we will
move some sideways and then we will move
exactly between 0 and 4 meters away from
a reference object a pillar here hence
the name D people are learning and we
will run our structure from motion alga
red which will provide us with a
preliminary death map this death map
will be a floating unknown scale because
that's how structure for motion works
but we know how we are moving so we can
take this death map and
histogram of all the depths in time
localize where on this death map we have
a pillar and because we know we are
moving between 0 and 4 meters away we
can normalize it to lie between 0 and 4
meters then we'll have the normalization
which we can reapply back to our death
maps and we have our training examples
so we can train our network and to be
honest we are in love with our own
solution but you have to be aware that
there are at least two solutions that
will give you the results faster and the
first one is to use internet to download
the ground truth for this task which was
prepared by someone else already and
apply it to training our model the
second approach is to use Internet to
download the already trained network to
do that task anyway now watch I will
tell you about deep convolutional neural
network and about our architecture hello
everyone hello so it is great to make
great things from scratch however it's a
bit wiser to use existing solution
especially if they fit really to your
needs in our case we have to dig in a
little bit we found very nice network
that was training for DEP estimation and
what was really nice not only the
network structure but also the training
model was available for to download from
github it was available in two format we
used internal flow and what was really
convenient also routers of the network
also prepared the script too so we can
we could learn how to how to use the
network how to load the model how to
feed our own data so
we did first test so we download the
model load load the model and internal
that it is very huge it is around 200
megabytes and loading the model takes
around five minutes and that estimation
on CPU took around thirty seconds so it
wasn't acceptable for our solution but
fortunately in our office under my desk
there is very powerful machine with two
gtx titan x graphical cards and with
that that machine we could perform
inference in one hundred milliseconds
which gives us real-time performance for
our project so now we could just use
this stuff but we are guys that always
want to know what is under the hood and
how everything works so we explored the
solution and I will try to explain some
details so the solution can be described
it in one sentence deep fully
convolutional neural network without
scaling layers fine unit from resin all
Network 15 and it's sounds a little bit
terrible so I will domesticate step by
step so let's start with simple in your
network so probably everyone has seen
such picture it is classical fully
connected neural network where every
neuron is connected to every neuron in
previous layer connections are weighted
weight weights are updated during
training process called back propagation
and the network is is training it in the
way to to give correct output or
expected output so why we could just use
such such such a network so take the
photo from our drone camera and feed the
network so think how many weights would
we have if we take the picture and done
scale it 200 by 100 pixels and for each
for each neuron we have connections to
each neuron in previous layer some
numbers ways would be really huge like
too much so imagine that you would like
to update almost billion or over billion
weights in every training iteration it
is so many degrees of freedom that
network could never converge to the
correct solution that's why conversion
of network comes and play nice role in
this game so to understand this thing we
need to explain a few concepts first
this convolution maybe some of you
remember this operation from image
processing class so on the left hand
side we have the frame on the right we
have kernels sometimes called filter and
we just some of we are summing products
of corresponding pixel values in the
kernel and the frame and put result in
that and this is a short video that
shows how how does it work so it we are
iterating over pixels andrÃ¡s we just
convolving the the part of the frame
with the with the kernel by having very
nice most like results in this case next
concept that is important in
conventional networks is max pooling
it's very simple operation for down
scaling image but with preserving most
important information and for most cases
we can assume that most important
information is preserved in a maximum
value pixels that's why we are iterating
over pixels and for each four pixels we
just take maximum value and put it in
the output and however stupid it looks
like it works and it outperforms other
pulling pulling layers that that are
sometimes used and another concept is on
pulling it is even more stupid or even
more naive
so for each four pixels in the output we
just take the input pixel and put it in
the left
left top corner so you can ask why it
works because there are just zeros
everywhere else but take the account
that after disappointingly I'm putting
layer we put also another convolution
which makes some interpolation and in
fact spread this preserves information
out to two other pixels so it is quite
quite simple but but but it works and in
convolutional networks we use activation
function called rectified in our unit
which is the basically in our function
but it is saturated for negative values
and it was introduced in 2000 and it is
biologically motivated so scientists
things that our brains works in this way
so not now let's see how conversional
layer works so in the input we have free
the free channel RGB image we have free
kernels we perform convolution with some
results together apply activation
function and we have feature Maps as the
result so there is separate kernel for
each channel it is quite important
because sometimes we want to extract
different features from different
challenge it is not so obvious but if
you think that the next layers there
will be different features on differ
feature maps and we want to use
different kernels to somehow aggregate
these features if you want to have
multiple channels and the night in next
layers we need to use multiple set of
kernels so for for filter for free
feature maps and free input channels we
need nine kernels so let's compare it to
classical fully connected layer so what
is the main difference so according to
the number of whites as I said for free
free kernels free by free we have twenty
seven right if we would like to have for
example hundred
feature maps and that is as I output we
would have around less than three
thousand ways on the other hand in fully
connected layer in that sample we have
just one one output image but we will
need three hundred million weights so if
we think about conversional layers they
are producing feature there are feature
extractor because using not so many
weights we are producing many many
feature maps and fully connected layers
are good in aggregating information so
very used for classification what is
important thing is a attention to the
locality so in fully connected layer
each neuron is connected to every input
neuron from the previous layer so so the
output pixel may think that for example
pixel and on the rubbish bin is somehow
related to the flower somewhere else so
there is no locality at all and in
convulsion a layer locality is important
kernels works locally maybe in some
bigger context or even bigger context
but still it is it is local so now let's
pull things together it is typical
typical structure of convolutional
neural network were a few convolutions
interleaved with max pooling layers and
this part is called feature extraction
part and there is a few fully connected
layers were which is which are meant for
for classification these days for
reducing number of weights it is quite
common to remove fully connected layers
at all so adding new convolutions with
with fully layers and at the end we have
just one pixel image which is the output
of our network so if you use that such
Network for the image we will we would
have just one
so output so we would have just one
depth estimation so it is not sufficient
for us
we need more that's why we need to add
up scaling layers which are basically
unfolding layers which I told forth and
interleaved with convolutions and in
this case as output we have full full
resolution image with Def prediction so
now it's quite difficult to train the
network from scratch because it takes a
long time
networks are very big it may take a week
weeks that's why it's quite common to
use a nice technique called fine-tuning
or
transfer learning and it basically works
in this way that we take pre training
model and we change a little bit the
neural network and retrain it with new
data so this solution was was used by
authors of the the paper so biased Pro
training model for resonable network 15
they cut off fully connected layers
added dis on pulling layers and very
training this network for def prediction
and actually they've got very nice
results in the middle you see the ground
truth the reference frame and on the
right you see that prediction which
looks very nice so we decided to use it
and Krista will continue to tell you how
we deploy okay so let's come back for a
while to our initial system design which
is already a bit complicated with the
components for convenience written in
different languages and now when we add
our deep neural network prediction it
becomes even worse because not only do
we face porting everything to line ups
machine work you dies but supported
we also cannot just call Python scripts
now independently every time a frame
comes because we would need to load the
huge model of the network every time so
every let's say 30 milliseconds it would
kill us so it is obviously solvable and
there are people capable to manage all
these components and make them work as a
perfect unity as a perfect unity for
those who cannot the world comes with
micro-services in our project these are
probably not micro-services according
the definition but breaking our design
into client-server components made our
life easier
now the headquarters is the Python
application which is served an image
from the ffmpeg service and then it asks
for this image the depth to the CUDA
depth server and then it calculates the
necessary controls and serves them to
the client which communicates directly
with the drone so now having this all
explained let's come to our example the
ultimate demonstration we recorded this
some time ago and to be honest we were
very happy to be able to show it to you
because during the day of recording we
had a fire now if you don't know what a
fire is the fire is a set of failures
that every self-respecting developer has
during the day of delivery for us
you will not see this fire in in this
video because this is the best video of
out of 10 or 20 and also we are using
the most secure delivery platform
statistically proven to be PowerPoint
now seriously all the components works
nice and smooth but you
during this day we've had some problems
with communication with vlogging Network
so frankly speaking this is the best out
of a lot of trials but we think it's
cool so let's enjoy it okay you can see
the drone flying but what's interesting
is on the left the automatic controls
are applied based on what the drone
understands using its self learned
convolutional neural network you can see
it is flying slowly but furiously
towards the window which is tracked
automatically on the opposite side of
the room whenever the blue color emerges
immediately the proper control is
applied to avoid an obstacle and we are
flying towards the end now this is as I
told the best yes this was this was the
best out of all the video so as a second
example I will show you the same video
again but now you can watch it in peace
so according to the task defined in the
beginning we have a full system where a
user only shows where they want to fly
and from then on the drone flies on its
own using its own understanding self
learn understanding of the space it
flies on its own which is the conclusion
of what we did but before we we end we
would like to if we offended and one
while we try to be funny
please accept following rectifications
first of all dogs are more lovely than
sofas second if there are just players
here
not all chess players are freaks when
you take your old good traditional chess
but in fact playing chess can still be a
nice game and finally irresponsibility
is fun two rules to remember our first
watch out for the lamps in your office
and finally if you happen to record a
video of your drone crushing at the
workstation of your boss never show it
to the public thank you very much
very little time for for questions but
if there are any you can also come here
please contact us send us your feedback
tweet email voltic application will be
very happy to hear from you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>