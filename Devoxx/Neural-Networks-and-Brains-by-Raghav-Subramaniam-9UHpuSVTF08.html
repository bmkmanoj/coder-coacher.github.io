<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks and Brains by Raghav Subramaniam | Coder Coacher - Coaching Coders</title><meta content="Neural Networks and Brains by Raghav Subramaniam - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks and Brains by Raghav Subramaniam</b></h2><h5 class="post__date">2017-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9UHpuSVTF08" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Raghav I'm a PhD student in olivier
sitting right here in the middle in his
lab at in the department of biomedical
informatics at stanford and today i'm
going to be talking about deep learning
with tensor flow for medical images so
first one we'll start with the slide
about why deep learning is important in
the medical domain so first of all it's
been showing amazing results in the
image classification and segmentation
tests and all these challenges like
imagenet challenges and CFR and things
like that
next automatic algorithms for tedious
radiology tasks like segmenting images
of of brain cancer MRI can help
radiologists focus on more important
tests like actually doing diagnosis and
finally there's a good opportunity to
integrate imaging algorithms with
existing bioinformatics and genomics
algorithms to for example predict the
genomic predict the genomics of a tumor
via a picture in case getting a biopsy
of the tumor is too invasive or actually
impossible because of the situation of
the tumor okay so here's an outline of
my talk I'm going to start with some
practical information from a developer
point of view about medical images I'm
actually going to cover a few
interesting computational tasks that you
can do with medical images next when I
cover some of the differences between
medical images and conventional images
when it comes to doing deep learning I'm
going to do a quick overview of tensor
flow then I'm going to do a case study
of glioblastoma which is a form of great
brain cancer
segmentation okay so there's lots of
different types of medical images we
call these modalities and different
modalities are useful for different
things for example CT images are are
useful for getting very like high
contrast images of the lungs and MRI
images are more useful for brain imaging
and abdominal imaging and
I specifically comes in different
weightings as well which again bring out
different aspects of things so for
example in the brain the t1 waiting of
MRI brings out the white matters which
are like the connections between your
neurons and the t2 brings out your
cerebrospinal fluid and there is a bunch
of other weightings that have different
strengths and weaknesses
contrast-enhanced which brings out on
pooling blood in the brain for example
flare which cancels out some of the
cerebrospinal fluid so you can focus on
fluid that shouldn't be there
stur cancels out fat there's the fusion
and functional there's a book there's a
lot of these so quick peek into how CT
works a CT scanner is as you can see
this very well but there's the x-ray
source here and there's x-ray detector
here so it's just an x-ray but then this
this source moves around the circle and
it does a bunch of x-rays from different
angles and then in a computer puts them
together and to the right we have the
algorithm that kind of puts them
together so if you have like the images
from this angle this angle and this
angle the algorithm is called back
projection you project to those those
images back into the space they came
from and you can see if you do that with
from all the angles you'll get a pretty
good image of the original patient in an
example of an MRI image again you can
see you can see the the lungs fairly
well you can see the blood vessels
within the lungs you can also see over
here you can see a a small cell
carcinoma of lung tumor and again CT is
a really good place to see this because
anything that's got gone wrong in the
lung will show up brightly I don't think
I can explain MRI in one slide but how
it works is it manipulates the the
magnetic moments of the protons in the
water in your body and manipulates them
and measures them and somehow comes out
with an image so this is a t1 contrast
enhanced image so you can see that there
is a brain tumor here in fact it's a
glioblastoma
which is the type of brain tumor I'll be
talking about later you can see that you
the area that shows up and bright in a
consciousness enhanced image is pooling
blood in the brain and and also can
belong to a highly vascular tumors so
this is a pretty pretty big sign that
something is wrong okay so next we'll
talk about how to obtain medical images
for use in your algorithms so here are a
couple of good sources first is T CIA
which is a website called the cancer
image archive it has right now about 70
um cancer image datasets and with ten to
a thousand images per dataset and you
keep in mind that these images are
three-dimensional and very large so and
they're kind of real-world and by that I
mean they're like different MRI machines
have like different settings and so all
these images like the brains aren't all
aligned the image set isn't normalized
very well and then there's also
challenge datasets and a couple of
challenges are brats which is a brain
tumor segmentation challenge which is
put on by the makai conference and
eyal's which is a stroke lesion
segmentation competition and these are a
lot cleaner all of the brains are
aligned on top of one another um damages
have been normalized to some degree okay
so a little more about TCI aid this is
where you can find find their datasets
and that's their logo so here is a
screenshot of like their website you can
see that they have like cancer datasets
blowing to the like breast cancer colon
cancer lung cancer renal cancer and you
can see that they have a couple of quite
large data sets so they have this
National lung screening trial data set
which is about twenty six thousand but I
don't think you can access all of them
and then they have these other data sets
there that are in the high that are in
like the two hundreds to a thousand
range of images which which is a like
that that range of sizes has worked well
for me
and then the Brock's challenge URL logo
I'll talk a little more about the
specifics of the data set a little later
okay so this is a talk about tensorflow
and most of people most people using
sense flow are using Python so medical
images come in these like kind of exotic
file formats
NII NIJ and among others and these
images are often three-dimensional but
and like they're kind of esoteric file
formats but you can just import
something that will take care of all of
that for you so here's a little snippet
of a piece of code to convert a medical
images to a medical image stored at a
certain path to a numpy array
and here is a method to write that numpy
array back into a medical image at a
given path so once you do that you can
just use numpy which is a lot easier to
work with and can do a lot of cool
slicing stuff okay now I'm going to
cover three interesting and important
computational tasks for medical images
so I've been talking about segmentation
in terms with respect to the datasets so
this is what the segmentation task for
brain cancer looks like so you have the
t1 and t2 weighted MRI images on the
left and you can see the big tumor there
and then on the right you have the
segmentation which is classifying in
general as classifying each voxel of the
brain as belonging to a background class
which is non cancerous or belonging to
one of several cancer classes for
example here we have edema which is the
pooling of liquid necrotic which is dead
cells and then the active tumor region
itself and here is a network
architecture for outcome prediction for
lung cancer so alcone prediction is
predicting whether or not patients will
survive past a certain time horizon
given their current state so in this one
we actually take in the 3-dimensional CT
data and we also take in other
complementary data like gender age this
stage of their cancer and we can combine
all this together to to figure out
whether or not a patient will survive
and then finally another task is imaging
genomics which is predicting genomic
data such as DNA methylation or
mutations from medical image data for
example these brain tumor scans and like
I said earlier we can predict the tumors
genetic information
without doing a biopsy by just going via
the image and then doing a prediction
alright I'm going to compare medical
images and conventional images in the
context of deep learning so start with
some bad news so deep learning has
traditionally tried to exploit huge
amounts of data to make low bias machine
learning models and so if we look at
image net which is like a very popular
data set of conventional images of
things like vehicles cats dogs trees so
this this data set that is very
canonically used has over 14 million
images and what's compared to the
brought some high-grade glioblastoma
again brain cancer data set that only
has 220 brains which is which is pretty
small number however there is one silver
lining in in that it's that medical
images are often three-dimensional and
when you're working with conventional
images you can't you can't really take
advantage of that sort of that
additional dimension of structure but
here you can look at three dimensional
orientations of things okay more bad
news
so three-dimensional medical images
often hasn't had millions of ox holes
like that 200 by 200 by 200 image will
have 8 million voxels and you so for
that reason it's hard to load a lot of
images into RAM unless you're working
with like a very big supercomputer and
you can load them but you can't load
many and that that is training with
small batch size so a batch is at each
training step instead of just training
on one piece of data at a time or one
image at a time you can train with a
bunch of them at the at a time and that
has been shown to increase the
efficiency of training so yeah small bad
size can be inefficient and
okay but however instead of like loading
the entire image into memory at once you
can just work with small
three-dimensional patches of image or
two-dimensional slices of the image and
in the latter case you lose some of the
three dimensionality but they do work
well in practice for different tasks
okay so here's an example of a 3d patch
based architecture for a segmentation
task so under a we have our input image
patch which is a patch of one medical
image and then and then we have these B
and C are all of our convolutional
layers in our network and at the end you
get you get like two images and then you
can take the Arg max on between them and
come up with the segmentation mask and
this is for a binary segmentation task
so we're only segmenting two classes and
I'll come back to this in a little bit
and here's an example of a slice space
to Architecture yes
so the question was I'm segmenting two
classes what are they and they actually
are the voxels in the brain that
correspond to the to the tumor or not
the tumor so those are the two classes
there all right so for a splice based
architecture one benefit of using crisis
is that 2d slices are just conventional
images so here's an example of a slice
based architecture for segmenting an
image of a dog from the background so
it's made up of two parts you have a
this convolutional network at the
beginning which converts the image to a
compact representation which is
signified here by this but it's long 1
by 1 1 but this long one by one array
and then we have a deconvolution network
which uses convolutional layers or which
uses deconvolution layers also known as
convolution transpose layers which kind
of do exactly the opposite of
convolutional layers and also uses UNPO
instead of the pooling layers so it
expands it back up into the segmentation
and this was trained end-to-end and and
I think at least for conventional images
like this specific architecture has done
really well and I think this has done
well for certain brain tasks and for
example like in a pediatric brain in a
child's brain trying to determine what
parts of the brain are what is a task
that this exact network has worked well
for yes
um so for this the so for this is for a
segmentation task again so for a
segmentation you want to start with your
entire input image and then end with
your segmentation image which is the
same size so you're classifying every
pixel in the original image so in this
case like your output has to be the same
size as the original image
yes exactly okay so I'm going to do a
quick five slide overview of tensorflow
which is a package for doing deep
learning so tensorflow is based on a
computational graph where variables in
your programs serve as nodes in the
graph and mathematical operations sort
of serve as or opposites they're called
intensive flow kind of serve as edges in
that graph and so basically you build oh
so a quick example like if you have two
variables a plus B and you do C equals a
plus B then you'll have like lines
connect like in the graph you'll have
like lines connecting a and B to C via
addition operation so you build this
graph programmatically in Python you can
do a lot more than addition you can do
things like convolution you can do
softmax you can basically do any common
operation in machine learning as built
in and it's and you can put together
your own so another thing to note about
the graph is so every time you try to
about you do an evaluation on the graph
that's trying to that's putting in input
values and trying to figure out some
output values test flow is lazy and only
evaluates the parts of the graphs you
needs to evaluate so for example at
train time I'll talk about a train up
there's a train operation that runs the
step of gradient descent to help train
the thing so so at validation time when
you don't want to do training you just
don't return the Train operation and it
won't train and I'll give you a concrete
example of that so data are represented
using special nodes in the graph there's
these tensorflow placeholder operations
that create these placeholders and every
time you evaluate the graph you can
assign values to those placeholders so
those are a good place to put your batch
of data and your batch of ground truth
data finally model parameters are also
represented using special nodes and
these are tensorflow variable capital B
objects and so these are they giving use
these as weights or parameters in your
in your network and every time you run a
step of training these are automatically
updated with their new values after the
training so that it makes it really
programmatically easy to implement in
training and I'm going to go through a
couple of code snippets so this is the
creation of a placeholder and so you
feed it the type and its shape and so so
here this is for a three-dimensional
patch based network so you have your 25
by 25 to 25 patch by for MRI modalities
and so this little different than
conventional images because you're in 3d
and also you have more than RGB you have
as many modalities as you have and
finally this the if you put a nun in
your shape it means that the size can
vary so here the nun dimension
represents the batch size so for example
if I have like if I have like 105 data
points and batch size of 10 like some of
the batches will be size 10 some will be
size 5 so we want to make it variable so
this is again adding inputs into the
model okay next here's an example of a
3d convolution op so we have our kernel
which is a tensor flow variable capital
v again so it gets up so it represents
model parameters it represents the the
filter used in this convolutional layer
and it's called weights it has this
shape it's five by five by five filter
for output layers or for input layers
ten output layers and thence initializer
is just how it's initialized in network
and Xavier initialization is a method
that has been shown to work to give you
pretty good results in practice so then
your and then variable cons is the
three-dimensional convolution of your
image placeholder from the previous
slide
and the the colonel and that will give
you the output of the convolution so
quick note on tensorflow scoping
mechanisms it has several but here's one
that I found to be useful so so this
variable scope basically means every
time you you try to access a variable
called weights inside variable scope
conv one you will access this variable
weights or this kernel or whatever this
kernel is storing you will access
whenever you try to get the variable
weights in the scope cons one so this
mostly is a is a mechanism to make your
code cleaner and easier to read so you
don't have to have super-long variable
names you can just put them in scopes
and you can nest scopes as well so this
is how you do training so this this line
of this is a training operation in the
graph so so basically so Adam optimizer
is a subgradient method that is based
it's essentially doing gradient descent
but better you give it a learning rate
and you you tell it what to max is what
to minimize or what to maximize are what
the optimize here we have a loss
function that we want to a loss function
which is like at the it's it's another
it's another node in the graph and we're
trying to minimize this the value of
this loss function using this optimizer
okay so now putting it all together
I am going to do one epoch of training
so trained on all of my data once so so
this data iterator here is just like a
generator that it's not built into
tensile or anything it's just a way to
get to get all of the data points for
example you would open up the medical
images cut it up into patches and feed
out the patches next you have this feed
dictionary which is a mechanism for
assigning values to the placeholders
that
you have declared so for example here
where we're putting in our image data
and then our ground truth label it's
also you can also use it for things that
aren't input so I'm using here for to
just set a hyper parameter here is my
dropout value and then finally we have
one of the most important functions in
tensor flow which is sets that run so
assess is an object of type competence
flow that session which is just it
basically gives you an environment in
which you can evaluate operations so
here we so and the argument to assess
that run is a list of of nodes in the
graph that you want to evaluate so here
you're about you're you're making a
prediction given your input of what the
of the ground truth that supposed to
close to the ground truth the loss
function between your prediction and the
ground truth and also the output of the
training operation so again times flow
is lazy he will only it only do what I
need to do to evaluate the prediction
the loss in the training function so if
you don't include the training function
it won't evaluate the training op and
training will not occur and then finally
you also give it the feed dict and get
the the evaluated values out here
yeah yeah cubes essentially
so at training time oh yes so the
question is so you're using patches what
if you have a feature that kind of spans
the gap between two patches like will
you put will you correctly predict this
and so so at train time I actually get
the patches randomly from the image so
they can overlap so so a training time
you don't you don't fix the condition
that like you don't fix the position of
the patch of the training time so those
are all variable so the hope is if you
train it long enough and you you sample
enough different like parts of features
with your patches that at add validation
time which is when you actually tile the
patches that that error doesn't occur so
that's the way to get around that okay
so finally I'm going to give a study
about glioblastoma segmentation which is
a problem that I've been personally
working on so first I'm going to talk
about segmentation this entire talk I'm
going to explain why it's important so a
segmentation is a very low-level task
and that might ask you a question like
why why are we interested in this is not
very exciting and I guess radiologists
think so too because segmentation is a
very tedious task do manually and so if
you use an algorithm to do this then the
radiologists can focus on higher level
things and like one step higher is like
eckhart tumor volume estimation because
right now they really all just use like
weird bounding box heuristics to
estimate the size of a tumor but if you
have an automatic segmentation algorithm
that can automatically give you tumor
volume estimations which can aid in
diagnosis again and in addition to to
aiding radiologists in their diagnosis
it can also aid other algorithms in
solving higher-level tasks so for
example if we have a task that uses
image data input we can augment the
image data with the segmentation data so
that the network knows where the tumor
is and where it isn't and that can
helped it make its decisions and we've
actually shown in our lab that adding
segmentations to survival prediction in
lungs has Candian can increase the
performance all right so so glioblastoma
it's an aggressive form of brain cancer
it makes up about 15% of all brain
cancer cases and the 5-year survival
rate is under 5% so the data set I use
here is the broad challenge data set
specifically the high-grade glioblastoma
on portion of it and so this is made up
of 220 brains and it comes with MRI
images and ground truth segmentations
and I quote ground truth because it's
actually consensus of 11 experts they
each did their own independent
segmentations of the brain and then a
majority both style algorithm was used
to combine them to make the ground truth
and the data set comes with four MRI
modalities t1 t1 contrast-enhanced t2
and flare and the data set comes in five
classes there's a background and then
for cancer classes and necrosis dead
cells edema fluid enhancing tumor is a
tumor that is that shows up brightly
under contrast so it's like highly batch
graded tumor and it can also end tumor
with the pooling blood and then there's
not enhancing tumor which is kind of a
catch-all for the rest so yeah so
there's a background class and a cancer
process so the metric that's challenged
uses are your prediction are your
performance on several binary
classification tasks so there's the
there's a whole task which is I'm trying
to segment all cancer classes versus the
background class on the core which is
all the cancer classes except for edema
and the active which is just segmenting
out the enhancing tumor from the rest of
the classes and the performance metric
that's used here is the dice score so
here's the formula for the die score um
so for
binary segmentation say your two classes
are zero and one that you tryna segment
so here p1 is a set of pixels or voxels
in your image that you predict to be
class 1 and T 1 is a set of voxels that
are truly on class 1 so and then these
are set operations so this is the size
of the intersection of the set divided
by this half to some of the
cardinalities and for those of you who
are familiar with deep learning is
equivalent to the f1 score which is used
pretty widely in conventional images and
NLP and things like that and so the f1
score is the harmonic mean of your
precision and your recall so essentially
if you have low precision or low recall
it will bring it will bring down the dye
score so you have to have high precision
and high recall to have a high score
okay so first I tried a slice based
architecture so that I could take
advantage of existing for example
imagenet architectures so it did not
work super well and I can I think I can
attribute this to class and balance so
average over the entire data set only
about 2% of the voxels an image belong
to cancer and and similarly um cancer
voxels make up a small proportion of
each two-dimensional slice so the
network in my case just learned never to
predict cancer I tried using different
loss functions that would make up the
the that would try to compensate for
this but even like doing aggressive even
being aggressive with that I wasn't able
to overcome this class and balance issue
but again this has worked for the slice
based architectures have worked well for
medical image tests that don't have this
loss at all so you shouldn't be
discouraged from drawing them because
they still work just not for this task
so I decided to use a 3d patch based
architecture and I was able to alleviate
the class imbalance issue and also take
advantage of this third dimension of
data and so the way I alleviate class
amounts is sample at train time 50% of
my patches from inside the
tomber so the network learns what's
going on inside the tumor the features
of the tumor instead of just being fed a
bunch of background tumors a bunch of
back on top folks and only very few on
foreground voxels okay so a little deep
a bit of a detail about the patch site
patches I use so the network inputs are
these 25 by 25 by 25 by 4 patches and 4
is for the 4 MRI modalities and then we
actually make predictions on the central
9 by 9 by 9 volume of the patch so for
example one thing you could think of
doing is trying to use an entire patch
to just predict the central voxel in the
patch but the problem here is that
there's over 8 million voxels in each of
these brats images and so it's so if you
did that at validation time you would
have to do eight million forward passes
through the network and here you can
drastically reduce that number so again
a train time we sample 50% from the
tumor reagent and a validation time we
tile the image with these patches so you
can see an image here so this is how
this is what I be my tiling the image so
these red squares are the 9 by 9 but
representing nine by nine by nine
regions and the one blue square
represents the 25 by 25 by 25 that it's
bigger than the original patch but you
tile the outputs of the patches and yet
to use these patches I just use a
generator in Python to slice up the
images and do some images with numpy and
just feed them out
okay so I did some pre-processing on the
brains because the breast dataset is
sort of normalized but like not
completely it's mostly aligned in Nice
that way but the values are kind of kind
of vary across the images so if I did I
stuck to some pretty simple
pre-processing and it was per image
so for each image I subtracted the mean
of all the brain voxels in that image
from each brain voxel so I a zero
censored them that way and then divided
the standard deviation so again the
important point here is each brain is is
kind of pre processed in a vacuum
without knowledge of the other brains in
the dataset and so so traditionally for
conventional images you would take the
mean over the entire dataset per pixel
in the image but here because so it
brains are aligned but they're not
completely aligned so it's so I found
better performance just doing this
independently and there's more
sophisticated options for normalization
so for example histogram normalization
looked at the histogram of each of your
images um history of intensities and
tries to map it to a like a canonical or
base histogram and there's also CSF or
cerebrospinal fluid normalization which
essentially looks at the value of the
intensity value of the cerebrospinal
fluid in the brain and tries to make
that the same across all the brains in
the dataset but yeah I stuck with a
simple thing and it worked well so just
a little bit on tuning and
regularization so my network has a
couple of hyper parameters not too many
and as the learning rate how much I
regularize it with LT regular
regularization and the batch size so how
many training samples that use
through each forward pass or through
each training step and the number of
patches I use per epoch so again I'm
randomly sampling patches at train time
so there's no guarantee that I'm going
to get all of the passages so I just um
train on some number of them I think I
use 4000 batches of patches per per epic
or 40,000 I don't remember exactly
anyways so I found that this l2
regularization was not quite enough it
wasn't giving me as much of the
generalization performance as I want
there was kind of a discrepancy between
the training performance and the
validation performance so I decided to
add dropout so dropout is how it works
is you zero out random elements of your
convolutional layer or four we connect
the layer activations during training
time and so basically this intuitively
forces the network to have redundant
representations of these inputs and I
showed that just adding dropout to the
one by one by one convolutional layers
which I will go into more detail about
in the next slide really helps the
generalization performance okay so yes
so the question was how do I figure out
how many layers I wanted to use for the
convolutional network I actually started
out I actually saw I think in a paper
which is how these always began they use
a similar architecture to this with the
similar number of layers I think I ended
up adding adding one convolutional layer
I just played with my model size into it
until it worked well so there's a lot of
iteration there and other like here so
you can use is like the kind of the size
of a tumor so you want to like so for
example tumor features kind of fit
within like a lot of tumors and datasets
a lot of smaller ones fit within this 25
by 25 um box so I guess I guess that's
how I came up with my patch size but
number of layers was more of an
iterative process also the number okay
so um so a you know are the inputs and
so B are the values for convolutional
layers with five by five by five filters
and their job is kind of to capture um
the original imaging that some kind of
compact representation so I chose the
number filters so I made the number
filters smaller to regularize the
network because I was getting a lot of
overfitting when these layers were
larger so I just made them
I made them kind of on the smaller side
so it wouldn't over fit that's also a
function of how small the data set is
and then in see I have one by one by one
convolutional layers and these are kind
of proxies for the fully connected
layers in traditional neural networks
for classification and detection tasks
and so first I use 80 filters and so
these 80 filters are the ones that I
applied dropout on and then and so here
I'm doing the the whole segmentation
task so it's just tumor versus
background so I have two filters at the
end to predict the foreground and
background
and then I do use softmax and Arg max to
come up with this prediction map so the
question is how do I decide what
activation function to use I tried relu
and I tried sigmoid and or not sigmoid
NH and I found that relu just worked
better yeah so a lot of this is very
empirical yeah I think reloj is more
widely used in image tasks at this point
okay so final overview of the network so
we have our inputs enter outputs or
inputs are again the 25 by 25 by 25
image patches the outputs are central
nine by nine by nine predictions times
two because there's two classes here
because of doing the whole task so we
have all our layers are concur or the
convolution layers I talked about before
and I use the relu activation function
and I also use Xavier weights
initialization which is a way to
randomly initialize the weights that has
been shown to converge well obviously
traditional images so I decided to just
use it here I use softmax cross entropy
loss the atom optimizer which is the sub
gradient method I talked about before
and then for regularization I use l2
regularization on on all of my
convolution layers and I use drop out
just on with a keep probability of 50%
on just at first
one by one by one convolution aware
okay so results so my algorithm the
average the average whole tumor dye
score was zero point eight six so that
were just over zero point eight six
again dye scores on a scale from zero to
one this is pretty good and this is
after I did four fold cross validation
just because the datasets pretty small
so so actually my training validation
split was I use three-quarters of the
data for training and 1/4 for validation
and then I cycled out the training and
validation four times and so over all of
my validations my average size square
with zero point eight six and so the
dice paper their original paper that
they put out where they described how
they made the status that the average
inter-rater die square with zero point
eight eight so that means on these
twelve experts who each who did their
segmentations if you took the average of
their pairwise die scores it was zero
point eighty eight so beating the
average interrater die score essentially
means that your classifier is kind of
indistinguishable from a human expert so
I'm almost there
with the simple architecture no
post-processing simple pre-processing so
this is just a histogram of the die
scores over all 220 brains at validation
time so you can see the mean of the
distribution is pretty high oh I also
want to point out that the bottom value
zero point four not zero so it's not as
bad as it looks
all but the variance of distribution is
still a little higher than I'd like
and I I think that I'm doing post
processing specifically on morphological
filtering operations like when we're
taking the largest or only taking like
the largest few tumor connected
components per brain so I won't have
these little outliers floating around
everywhere so a bunch of operations like
that I think will make the distribution
skewed art will make it a narrower
distribution and that's more clinically
useful because you don't want some brain
story and we have really bad performance
all right so here is a visualization of
the results for a brain that did very
well so a general pattern is brains with
like big solid tumors had really good
performance from the algorithms but
brains with Moore's with more um spread
out smaller tumors didn't work as well
so here's an example of a brain that
worked well on so at the top left we
have the the original medical image we
have the t1 contrast enhancement
allottee this is the same image from
before so top right we have our ground
truth segmentation again for the whole
task so it's tumor versus background we
have our predicted segmentation which
looks very close like at a first visual
glance friendly we have a image of the
entropy of our prediction so basically
the entropy will be high
wherever the network was unsure what the
class was so it makes sense that we have
some entropy here at the boundary of the
tumor there's also a couple of larger
regions of entropy right here and right
here and those are places where the the
network was unsure if you look very
closely up here you have like a little
hole in the segmentation there and that
kind of corresponds with the segment
with this entropy so so the network kind
of had knowledge of this whole being
there but I don't think it had it was
overwhelming enough to actually predict
it predict that region to be background
can you guys see this the hole up here
yeah
all right so conclusions of the suck so
deep learning is a very promising to
illness and just from looking at so it's
just it's just starting out and it has
had good performance on things like a
contradiction and segmentation so it is
a promising tool so as deep learning
goes as the volume of data increases the
algorithms will perform better and
that's good news I've shown that it's
fairly easy to put together a neural
network for tumor segmentation and
finally I believe these these
computational and deep learning general
machine learning algorithms in medicine
will lead to fewer invasive procedures
that will result in faster diagnosis of
cancer and other diseases and more lives
saved ultimately Thanks sorry yeah
I'd also like to thank my professor and
advisor Olivier everyone in the lab
Stanford biomedical informatics and
anyone who's doing the dirty work of
putting together publicly available
medical imaging data sets yeah I'll take
questions yes
um so the question was do you think on
results like these will encourage people
to to work more in the field I think so
I there's a graph I should have included
which shows like the number of papers
that have been being put out with like
for example I blastoma segmentation with
deep learning and it's like an
exponential curve so I think like
definitely at least in in academia
people are doing it and and I think it
will spread the industry it's a really
ripe area for for industry so yes
yeah so the question was is there
anything that we are doing to work with
medicine in China than being the most
populous country you want to go ahead
and take it
all right I'm done thanks for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>