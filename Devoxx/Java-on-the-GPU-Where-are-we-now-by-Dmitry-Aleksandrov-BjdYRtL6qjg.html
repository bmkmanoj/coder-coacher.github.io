<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java on the GPU  Where are we now? by Dmitry Aleksandrov | Coder Coacher - Coaching Coders</title><meta content="Java on the GPU  Where are we now? by Dmitry Aleksandrov - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java on the GPU  Where are we now? by Dmitry Aleksandrov</b></h2><h5 class="post__date">2017-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BjdYRtL6qjg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I guess it's a good idea to start
right now it's the last session before
the keynotes so I gotta make a challenge
you not to sleep I'll do my best so
welcome to my talk time on the GPU where
are we now
a few words about me my name is Dimitri
I'm a principal a developer of the two
systems and I also am Bulgarian Java
user co-lead so there's my twittle
handle so we can follow me so how many
of you have ever owned this video card
remember it was ah awesome I mean that
was that was the start of everything yes
in our world then and eventually for me
it was an awesome pleasure when I saw
this screen how many have ever seen this
screen yeah you know what kind of you
know it emotion you have it when you
have it on your screen really you do
okay so that was present for my 13th
birthday for my father I'm so happy
about it yes and so what is the video
card so we're going to talk about GPU so
video card is a separate device usually
all built-in device in the process you
know which is able to get some data from
your memory or its own memory and
visualize it transfer it so transcoding
to analog or digital signal to be
visualized on your screen so it's a part
of every machine almost but as of today
video cards are actually not limited
only by showing the image that you have
they are also able to process and do
additional processing for transformation
mains on the screen and they relieve
this work from the from the processor
itself so mainly what you do so you
submit some kind of a form to a video
card and then it does all of the
processing and printers it to the screen
so video card is kind of an essential
part of our systems and sometimes is a
even the most expensive part of our
systems actually so video cars can be
quite expensive so and then what is a
GPU so you've heard in in the in the
name of the lecture of GPU so GPU stands
for graphic processing unit so the first
time this this abbreviation was used
back in 1999 and by Nvidia and actually
NVIDIA GeForce 256 had the world first
GPU anybody own to 5/6 is so much
awesome so how did n V actually define
GPUs so it's a single-chip processor
able to transform light pink lights
vertex s and so on and so on
with the speed of about 10,000 polygons
per second so that was back in 1999
so eight actually call it VP you but
here in video 1 again about the name
so in principle the idea of a GPU is in
this picture so we got a lot of
so-called shader caustic texture course
we've got rasterized that we've got
video decoding and coding l work
distributors so the nature of the
processing is very very parallel you can
work with a video card mostly in
parallel mode so every every task can be
quite nicely paralyzed so that's why you
have many shader calls texture course
and it actually made for all this nature
of the computations so and maybe
probably you've heard of this
abbreviation GPGPU so it's time for
general-purpose computing and graphing
cars so it's does actually calculation
which are not only for processing the
images so they could do all the tasks
that are sometimes done by the CPU
itself so and as we have this thanks we
want to use them actually but first
let's have a look at the hardware so
this part of the session is
Stanford University's GPU explanations
of what the GPU is so if we go back so
processing we want to process something
in most of the cases CPU as a
general-purpose processing machine has
this kind of structure so we have a
something that to fetch decoding for the
instructions we've got this arithmetic
logical unit ALU now we got execution
context and since the data itself comes
very very much in random way there are
fancy predictors that data caches memory
professors and everything something to
make your processing faster so and since
actually the work with all video cards
is also running a processor runs on a
processor and you actually do the
processing so you actually run small
programs on it so basically for example
you can convert one pixel to one pixel
color to another pixel color so how
actually we can construct GPUs so we
take a CPU and what we do we simplify so
the first idea the nature of the
computation is that we don't we have
everything very much straightforward we
don't have the data itself is very much
aligned so we don't need fancy
predictors data caches and so on we move
them away so we only have this fetch
decode instruction so what we should we
do on this processor now we have this
ALU you did and of course the execution
context all this all this registers and
so on and so on
and since the nature can be parallelized
we actually do paralyze we put more of
this course on our video card so and we
can even put even more of course the
video card here for example we have
simultaneous 16 processor running a very
paralyze about computation but most of
the cases we are doing the same
operational calculation on a different
data set so we do all the same
instruction on different data set so
here we come to this Cindy paradigm so
you probably heard of Cindy yes we won't
get in much in details but then you have
one instruction which is applied on
several well Y values so at this moment
you can actually calculate by a via one
pass all while by one tact you do
several computations it depends on how
much actually it does handle so we do
this we apply this idea directly to the
to the this our processors so and we
have only one fetch decode stream and
sixteen or in my case eight contacts S&amp;amp;L
practical units so just in one tact if
we use this this way we for example can
convert up to eight pixels like in this
picture only for one processing tag we
actually work on eight pixels and if we
take two of this ideas so we can come to
the to the idea of the Molen GPU so we
have a lot of course we have fewer
fetched so instruction streams decoders
veterans decoders and we get a lot of
arithmetic local and logic units so
massive massively parallel power that we
want to use so and of course since we
know how hard work we want to code there
somehow so um
actually can we use it so some cool
video cards at some point started you
know to provide the programmers the way
they can offload or tell the GPU that it
can take some of the tasks from the CPU
and perform it on all of this massively
parallel processing sets that he has on
board and but the most of the algorithms
were actually hard-coded they were
considered
the standard algorithms you all play
counter-strike yes yes in early versions
the water was made with one of this
hard-coded algorithm so programmers just
call it okay make me some fancy water
yes and video card two phones all these
calculations so developers were only
able to call a very small subset of
these instructions for them and of
course well to make games really cool
because the idea of video card games
come from from the gaming industry
mainly it was not enough to have only
hard coded algorithms so that's why some
of the vendors open access let us say it
like open access to be to the developers
to be able to load their own algorithms
their slow own programs and these small
programs will call actually shaders if
you game you definitely know what shader
is so you have the shader modes and so
on and so on so if you game you know
what it is so as small programs to work
on a video card so at this moment so
developers were able to you actually
write their own algorithms for
processing call these small pictures and
make the transformation sliding and so
on first this vectors were with the
shaders for different kinds of vertex
geometry pixel shaders but they'll
actually come to one common architecture
so several shading languages were
actually developed so the first one is
random man you probably heard of him
then we have this DirectX I Sam CG was
you know very much popular GL cell and
so on and so on but mainly they actually
looked like a subset of a C program
which was then compiled so you an
assembly code so if you see from the
from the left part is exactly a shader
written in kind of a C language and
transponder then compiles to an usual
you know assembly
binary code which should just execute on
this small very very unpowered
processors and the main idea is that
this shaders were able really it's it
provides good transformation of the
quality so I'm not sure yes you can see
they even on this screen so the effects
were visible so it was very nice for the
gaming industry but as there's so much
so a low level and it started with a
gaming several abstractions were
actually made so of course you've heard
of this OpenGL which was created back in
1990 and 1992 and this was a very big
set of abstractions which we were with
which we were able to actually code and
have the ability to interact with GPU
underneath so it was we actually did
some for example we want to render a
square not square but cube which itself
can render as a cube and he will do have
a think it will you know by this
abstractions it will interact with the
GPU
if the GPU for example supports some of
the accelerations will definitely call
them so the other thing is of course
DirectX it was made by Microsoft and you
know that if you install a game on a
Windows you always have this DirectX
under at the end so it was also kind of
you know application interface which
microsoft windows were able to
communicate with the video cards to make
the acceleration but since we're talking
mostly about java yeah on this
conference how can we use this in java
so we will start with opengl he will be
surprised but at some point that was
given a GS article 231 which we started
in 2003 and i'll abandon in 2008 in
which we were able to actually use
opengl directly in java so it actually
did support only to the
OpenGL version 2 but then he actually
separated I'm not sure about all the all
the all the story underneath but it
separated in a independent project go
geo GL which is kind of a life and it's
of course a very high level of OpenGL op
c2fo and have you ever coded with opengl
somebody my god why are you here yes a
disc provides support for a glue and
glued so you know what glue and glued is
and it gives the ability to directly
work with gni so if we take a small loop
by the way my career started in sixth
grade with coding and basic and you know
all this small problems like write list
line to this coordinate then another
line to another coordinate and so on and
so on here we are also able to code in
this way with this glue includes so if
we just run our program we will receive
accelerated to excite why did she pee
you some kind of a cube so it's written
purely in Java but underneath it uses
all these bindings to J&amp;amp;I and then goes
underneath to the to the lower levels of
OpenGL but here if we are working we can
use actually glute here so the funny
part about it is you probably all
recognize this game with the guy made
millions with it as of course and it's
written in Java so you can use Java for
this he used another library which was
actually based on on OpenGL and Gio Gio
but at some time about 2005 people
realized that actually you've got a
massive amount of massively parallel
processing units in your in your machine
so the first effort was actually very
interesting so some of the vendors
allowed the programmers to read the back
buffer or the the output buffer of the
video card so you can take an image and
code your information in this image
write a shader submit an image in the
shader to the video card and then see
what is processed from the back well the
other image which was processed yes
people were playing with it a lot but in
the England 2005 came this assumption
that we have to make it civilized you
have to use it so and that was the first
research project mainly Stanford
University called Brook GPU which was
you know what we call gpgpu platform I
mean everybody somebody used this one
I've only played yes so it was a subset
of C and we were able to write our
programs and run him on video cards and
of course this this evolved very much
and then we have received a lot of
platforms for doing this like CUDA like
I'm the flyer stream which was actually
dad again and direct compute and of
course open CL so and the question is
why we should ever use GPU on Java
I'll always ask them well because it's
write once run everywhere here you have
to love because it's used on three
billions of devices here you also can
love but is this so when you install
Java you have this image but the main
idea is that still there's a lot of data
on anything data status mate on the with
Java
and there's a lot of finance and banking
mate with Java so practically GPU is a
good choice to do this and by the way
why I came to this idea because I use it
for some of you and some of the projects
for exactly finances so it was quite
applicable as an idea and how can we do
this with Java so we have two choices
you probably heard of them so Nvidia and
OpenCL and of course it works on JV Java
works in GBM so how can we get to all
this low-level of course you have this J
and I and J and a somebody use GA
directly well cool it's more more easy
to write there I believe but with this
way you can access lower allowance but
you know going so much low it's it's
it's very hard so we want to make some
obstructions and we want to use reuse
someone's code here so and that's why
for OpenCL we have several frameworks
which are available to Java so first one
is GOC l.joe camp and of course java CL
which is unfortunately dead Jam it's
also old I haven't used it I'm not sure
that's a person who has ever used it
so I don't have I'm sorry if somebody
uses uses well not here and of course
for a CUDA we have also a very good
binding so J CUDA which is a set of you
know of main main oh let us save base
core J CUDA library and a lot of subsets
of different stuff provided by NVDA
itself so I said binding to invidious
set of libraries we'll talk about it
later so and unfortunately I should say
about this disclaimer so it's not
running your problem is not GPU it's not
just you know bright something can fire
it will work
you really need to know your hardware
you really need to understand what it's
running because because it's like this
you don't really need to know where
should you run it but let us start so
first of all we have this open CL stuff
and it stands for open computer
languages a language which is actually
now governed by Conners group and it is
created by Apple but it's a concern some
in many groups to create a very abstract
model of computation itself so it should
run almost on everything so on GP on CPU
or like they position their self up to
the DSPs and FPGAs it provides you way
to run on almost almost every
architecture and it provides you a way
to do a heterogeneous computing so to
use it in general it works like this you
still have to think about host and
device you're just not run program
directly there is a host which runs
something there is a device which
executes or it's like like slave doing
the stuff you actually provide this
provide from coast so you submit data
you submit the program itself or kernel
then the magic happens there inside and
you wait for the result so it's a quite
an a synchronous work but a typical life
cycle of an OpenGL application is very
very complex so to make it very general
you really have to do a lot of stuff
before you actually do run your program
so you do create contacts common queues
you fill in the memory buffers you take
the program which runs on that you know
the device you compile it then you
submit you create a kernel so unite all
this data you define an MD range we'll
come to a little bit later you then
execute then you take the data baked
from the memory of the device copied to
your device and then process it so it's
very very hard so we just take a look
it'll be a little bit scary
to do a very simple simple stuff second
I like for example can you see it I
think I'm making a little bit's can you
just read it it's okay good from the
back good so we have a very
parallelizable idea we have to erase we
want to make so-called vector add so we
want to get one component of the array
with another component of the array and
produce a third array which is a result
of adding element from an array to
another array so the operation itself is
it's like here so one ID second ID and
then again and this thing as you see
this is a pure Java code right now you
see we got this string this is a kernel
this is a program that we had to run on
our computer on our video card and then
we have to do a lot a lot of ceremonies
to submit this code to our video card so
like for example we want to make 10,000
ads 10 million ads so we define one area
for one array then another array then we
have to do something very strange like
pointers and so on why we have to do
because this geo CL actually provides us
binding to this C plus place that C
program that that's the form of it C
framework that lies underneath this
thing so we try to switch our minds back
to C and have to express it with terms
of Java so we have pointers to want to
write another array then we take the
device then we take the platform it's
very much complicated then we you know
get all the IDS then we create context
for example we need one device to do
this program with this set of data then
we start coping the memory
so we could allocate the memory for the
video card for one array for another way
for third array well then we create the
program actually we do compile this code
and finally somewhere here there is only
line one line of code which actually
does the work you know very big ceremony
and then we get back to work
and then we read the data and you know
fetch the results back so if we run this
program I'm very blessed of this small
machine this smock has video card we
will see that we just did ten million
calculations on our video card in just
eight hundred eight seven milliseconds
so now from java we are using our video
card very simple example but I'm not
gonna dive into details as I told today
yo nante on the interview every step
away will be a deep dive in another
problem unfortunately so as you see
OpenCL is very very hard so there is a
host code which is on Java there is a
device code which is a specific subset
of C language and all the communication
between the devices is made by a memory
buffers and you'll be surprised that
actually we are not able to transfer
almost the same data as we use in Java
so for for OpenGL you have a special
subset of data actually which is which
is available only to it so here we guess
calories so there's a unique values you
start to think a little bit different
things like for example on science along
floats you have even have something
called 1/2 which is strange to our eyes
moreover you have scalar values then you
have vectors not exactly like this
vector but vector
you know what a vector is okay I won't
explain it I don't need to explain it
but the idea is that you got a tuple of
values which can be submitted at
processor advicing the instructions it
will be calculated in one actually in
one text so you have to start thinking
different types as in Java and you have
to submit there and you will be
surprised that the way they are saved
there is also not exactly the same as we
started to think so as the nature of the
processing itself is mainly for media
for textures for graphical images which
will which are nicely aligned in the
memory you've got several subsets of
memory in your video card you've got a
global memory constant memory private
memory and a local memory so according
to OpenGL model each video card or each
device actually should have this global
memory than private memories and local
memory to some compute units that for
example we can have not only one video
card but it's array of video cards and
everybody has kind of a compute unit but
in common this is a computer device or a
video card with two GPUs available so
it's a one device but with to compute
units so that's why they made this
common model and each memory works on
its own speed and it will be surprised
that's not all so as you remember Cindy
we've got this very hard execution model
so to be able to perform calculations on
video card
OpenCL itself knows that you are working
mostly with erase one dimensional two
dimensional or even three dimensional
erase so this is a nature of the
processing itself and OpenCL provides
you a very good infrastructure how
should you shard your data to spread it
to different course and this thing is
called and arranged so you can work with
one dimensional day
like this erase which were will copying
together not coping but adding erase one
to another or you can have two
dimensional stuff like where you want to
process an image all three dimensional
but not only in video cards still
support them so you can always express
them in terms of coordinates but nd
range here is to help you so if we take
this very easy example of matrix
calculations you know that from leaf
eighth grade there are always three
cycles in sideways cycle one they match
the second direction and dimension and
then we do processing inside so here
video cards are video Carl place he'll
actually provides us a good way to shout
our data and the framework itself
actually handles the charting updated
for you so if we go back to GPU this
matrix multiplication kernel will be
like this we'll only have one actually
cycle here you will have this I call an
arrow so if we say to our device we will
work in two-dimensional with
two-dimensional data please handle it
for us in a different way so it will
provide us all this other coordinates
and we will always need to process them
so if we take it in general and see
what's happening here a typical GPU well
this my GPU directly in my machine right
now you will see provide us a very good
work items where's the mouse very good
working I work items like for example we
can think in dimensions 1034 and 1024 or
even go third direct and dimension up to
64 but we see that the processor itself
is very much slow so it's small
processing with almost no caches and
what we surprised there was no even
Cindy processing here and by the way if
you go to typical GPU you see that we
are
able to process to shut our day to good
so we only have one dimension here but
we have a lot of power in mega heads and
we have a lot of memory by the way we
even have a lot of Cindy processing call
this I like for example we can process
up to 16 charts you want text yes so you
have to know your hardware and you have
to know how you shard your data and is
it suitable to be Charlotte so and if we
take a look about CUDA so we make an
overview here it looks easier especially
for C developers I believe everybody has
well I have so many hands you have tried
to play with CUDA yes
well fewer hands but CUDA you write code
like a C code your program is like a C
code small program you also like this
define a kernel a program which runs on
your video card then you do all this
special functions with memory
allocations with you know pumping the
memory there of the data and it's in
point as you copy it all the data to
your video card you have a strange you
know unreadable for for C developers
code like okay now please execute this
vector not this vector but this program
with these dimensions with this threads
with all this data so for C developers
it looks like this and then we of course
do something potato and frites
but CUDA itself has some different
superpowers which are usable for us
like for example CUDA in self provides
you a very big subset of internal
libraries which are very nicely fitted
CD and video devices so like for example
for Kubus for mattresses or there is
even a new library deep neural network
library which is a kind of a scary one
I've tried it and understood completely
nothing but it allowed me to use my GPU
for kind of
neural network learning so and you are
able to use them from Java as well by
the way as a good example this is from
real world but one of my friends working
gambling they need very good random
numbers and with library called J Ron JQ
r and yes you are able to produce a very
good mathematically correct from the
numbers so this is a small Java code
this is Java code which we actually use
this provided api's this bindings
directly from Java to produce us random
numbers is based on a very deep theory
by this Sobel sequences by Russian
mathematician based in 1967 and if we
take a look you see we just ran a small
program which is Java program you see we
don't have any kernels any crazy stuff
here which were unable to process we run
it and voila we have a big sequence of
random numbers which are really number
that quite close to real randoms
unlike the pseudo random rambles we have
so and of course the memory is a little
bit different there but as we're talking
to the memory model you see the biggest
problem itself is that we've got a lot
of movement of memory of data between
our arm and the device memory so that's
the biggest issue so if you try to code
something you should always be aware so
how much time do you copy
because if we go back to the example
made by the array copying
and we for example do this not on a GPU
device but a CPU device look how easy I
can switch the device here I just run
different code now I'm running the same
stuff on my I seven processor absolutely
the same calculations and you see it
goes 10 times faster just because I
don't need to copy this memory to the to
the device memory process it and give it
back
so actually you the processing keep
using the profile own it takes 27
milliseconds on video card the other
almost second goes for coping back and
forth there's another option if you have
a built in video card which I'm also
blessed to have here if I run the same
program on sometimes have to warm up you
will have almost processor like speed so
just because we're not physically
copying the data it just stays in the
RAM because our HD card HD yes internet
e card just uses the same memory it
actually is not going back and forth via
this PCI links so this is the main
program problem and unfortunately you
always have to do optimizations
especially for the devices like the
small kernel formatted mattresses
multiplications if we know the device
and we're working is okay to process
vectors our optimization will go like
this all on the same on the same program
and you know when I first said ok that's
this crazy but this code runs several
time faster is just because we're doing
this vector processing there but you
know we don't want to have this thing at
all see we don't like to see it's too
much for us we don't want to think about
these horses and devices but we would
like to still somehow use GPU so here I
guess there is a nice way to do this so
it's a project so much
as the research project is still not you
know publicly open I believe Chris Chris
I'm not sure if he's here but he was
working on this research project which
was focused on July 1 and it was focused
on one thing it was focused on streams
and the more precise and lambda stand
for each in the streams so actually it
was Andy's idea as to optimize this
solution as you see we have to do a lot
of memory copies between GPU and CPU
I am D provided another architecture
called HC architecture which actually is
a kind of LLVM you know what is a lot VM
I believe yes for video cards which
spent a lot of unuseful code and unloose
full memories copies and if we talk in
terms of H sale so not in H sale but in
terms of Sumatra so as it detects the
for each block with a growl
you know what growl is yes you've heard
and talks you know this JIT compiler
okay we produced directly H sell caught
with an then submitted by ji to the GPU
and if we look and start you know
working with this code on AMD APU
so-called well you don't have this
memory copying we got one memory pool
and one memories of space if we combine
these two techniques the burst but the
boost of the productivity is a little
bit you know it's hundreds of times so
here you got this gigaflops of
calculations and together which a cell
sumatra project well performed really
great but done ami a little bit screwed
up so they don't have financing this
but we want some more general solution
we actually don't want to be bounced to
DUI MD so IBM created a patched machine
which is now focused on QD but it has
the same idea so it's we have streams
and if we create our own parallel you
know processor in the stream it can be
offloaded to GPU automatically also
detected so for example if we have this
range here and we detect parallel and we
detect this lambda we want to upload a
float this calculation to GPUs it should
be done automatically so that's what
actually IBM did for us so if we have a
big end in our range and we see parallel
after it the next button for each this
lambda here it transpires to two CUDA
actually code and goes directly to the
GPU so it works only with primitives
only with I believe one dimensional race
but still that's quite enough to perform
really great and if we optimize only
read only caches and cooperate a good
success story here is the is exactly
this apache spark plus this JVM as they
claim they received a very good timings
if they do this Mass parallel processing
directly with a spark framework so spark
experience Java and the JVM itself by
offloading this lambdas to the to the
GPU saves a lot of time so and if you
want to play with it there is even a way
to do this there's a github code called
GPU enabler which allows you to if you
have spark in this IBM
machines so please try it you will
receive a very big benefit of it but we
don't want to even bother with all this
thing OpenCL in CUDA how can we actually
do this without thinking about in this
terms is there a way to do this yes you
can and you probably will know there's
an API call a copy somebody heard about
property ok not so much but it stays
short for a parallel API and it works
like a hibernate for databases but
actually a hibernate for video cards so
it dynamically converts JVM bytecode to
this host and device code allowing
allowing you to utilize the full power
of OpenCL on the cover so of course once
again is started by IMG unfortunately
then abandoned and then in five years in
five years after it was a banner it was
outsourced by a Apache TP license and
now it's back to life what is quite
wonderful and once again the beauty of
it take a look at this this is only a
pure Java code of course unfortunately I
was wrong I think we don't have to think
about all these kernels and other stuff
you still have to understand what a
kernel is but here you don't have any
other code as well so you just do the
same thing you just do vectors
calculations but this is now fully Java
code and it compiles and transpires back
to the to the video card so we have a
small look at it one second yes so it's
just a regular GPU a regular Java code
and if we run it they say voila and you
have all the same calculations but
already done on the on the GPU
and the beautiful part is it's very much
smaller anyway you can for example say
with annotations like for example we
want this this part of the of the memory
to go directly to global memory or we
want this part of the memory to go to
local memory so here you've got control
with annotations what you want
please find more information about this
an example so it does work it's very
much restricted and what you process and
unfortunately if it cannot process it on
GPU it fault falls back to CPU and you
won't feel it but if we find some GPU
and the task which is GPU available so
you feel free to use it it's still a
little bit more like a research project
but you have it but what about the
clouds of course we cannot sell our
product is not cloud native and here in
videos our friend
so as they launched this Nvidia and grid
it's already available to us and most
works on most of the hypervisor visors
as well as in the cloud so with this
Nvidia greets stuff we are able to
utilize so-called virtual GPUs so it is
available to you as a full machine as a
full device in your configuration but
actually underneath almost as we see
here in test lines and we got 64 virtual
GPU so 64 virtual machines can use the
same GPU and he's already available in
our Amazon Web Services or we're Melba's
citrix so if I hope my dam is gonna work
I am now connected so please switch off
all of your Wi-Fi devices I need the
full bandwidth
because I'm connected now to this is an
Amazon machine somewhere in islands and
here I'm doing a little bit fancy stuff
so here I have a server which is a usual
Tommy server and I want to calculate
some of the stuff and visualize it first
calculated with a GPU and visualize it
so this machine has Tesla Kaz processor
it's quite cheap I pay only less than a
dollar for an hour and it's not even
spot machine
it's just on-demand machine so it's the
highest price and if everything works
fine which I sincerely hope this is a
pure pure itami application if we go
back and I believe somewhere here I've
got the URL it runs somewhere in the
clouds and I go for it I'd CIT pause and
go leave 1.0 just a second what out so
you see a pure Java EE copulate of
application which actually did a
computation on GPU somewhere in the
clouds renders as a beautiful image and
we got back it as a usual you know web
application so that was the idea of our
application what we did we had to do
some heat maps calculation heat map is a
good example of using a GPU
it's a canonical example so if you're
able to do this I'll float this task so
you see it's possible and what I want to
say this unfortunately AMD oMG it's a
little bit behind this they're still not
able to
do this so they call it empty moon to
use a GPU but it's still not available
to us so anyway it is here just take a
look at the real GPUs for example Nvidia
180s 2560 course which can massively
paralyze work for you
AMD Radeon has 2300 this APU machine or
calling it was you know very good CPU
this big array of Cindy processing of
Cindy processors and even on my machine
this small device almost more one third
of the of the processor is given to the
Intel graphics so I got some chorus and
then I have a big array of of this small
video cards this small course on the
video card you will be surprised but
most of the Tegra by Nvidia they also do
have GPU on boards which allows you to
process a lot of amounts of parallel
stuff and you probably have heard that
yes that Intel is gonna make a big merge
with Vega video cards for my OD so you
have one device which actually combines
two devices together Intel and AMD and
it will be quite cool but first
unfortunately to use there you have to
read books you cannot just use it
directly but if the task is suitable and
it's worth to try it and you just try it
and you will upload a lot of the work
from your processor to the GPU so I
believe that is Satan and Flemish donkey
here
thank you so much for their attention
and that's what this but that was it for
my side I hope you find it useful
as apart from the bulgarian java user
group that we have free t-shirts so who
is closer please take the t-shirt from
us</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>