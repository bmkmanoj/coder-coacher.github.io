<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud Machine Learning APIs for fun and profit with Harry Potter by Yufeng Guo | Coder Coacher - Coaching Coders</title><meta content="Cloud Machine Learning APIs for fun and profit with Harry Potter by Yufeng Guo - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cloud Machine Learning APIs for fun and profit with Harry Potter by Yufeng Guo</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/o0ZL8XGuv90" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Yu Feng well I'm a developer
advocate at Google and I focus on
machine learning and Big Data so this is
kind of a fun skit talk that I put
together and I guess before we start I
have to ask did anybody come here
thinking that my name was Harry Potter
and doesn't know what Harry Potter is
because I can set some context you know
I mean we laugh but it actually happened
where I have to explain the storyline
and set the plot a little bit so don't
be shy if you don't know anything about
Harry Potter anybody all right no takers
this time yes so you guys remember the
in the in the dormitories there was the
the painting with the ladies right and
they have to say a password to get past
this is a interesting example you know
comedy aside of a kind of system that
can you can you can get into the dorms
if you knew the password right but only
if the painting felt like it and and if
you were the right person because it's
kind of a silly system because if it was
just the password then any Draco could
hide around the corner and be like Oh
Fortuna major got it ha ha then two
minutes later or the next night come by
and be like Fortuna major now you have
to let me in huh right so the painting
also knew who should have gone in anyway
so as a plot device I always thought
that was kind of silly but it is an
interesting piece of inspiration for a
real-world version of something like
this so um because everything is better
with the Raspberry Pi
I have no
I'm good friend it is my password naught
indication passed
whoo-chow past the home let's find out
who you really are
prepare yourself it's picture time nice
look analyzing your picture now David
authentication passed welcome the
dramatic finish is that last moment
where the where the lock clicks across
probably should have made it bigger or
something louder but that is my real
world version of this painting may we
have a raspberry pie with speaker with
camera and microphone webcams right and
something about the cloud will fill in
that box later because raspberry pi
couldn't do this on its own and of
course that lovely lock that you saw and
so I painted to put a pub picture of it
on the back side and the lock is indeed
held together with a paper clip and
that's a little servo it goes to the
right and then it pulls the lock across
very secure so any do I have any
Kickstarter backers for this home
security system Kenneth yeah
can you cut yeah maybe I should have it
be able to dial down that one a few you
say the wrong thing and then inside we
want to open the door is there's a
painting of the photo of the Gryffindor
common room so trying to make it kind of
complete now when we think about vision
and speech and being able to recognize
that a lot of times we take that for
granted tons of pictures are shared all
the time every day on all number of
platforms speech is also somewhat taken
for granted we pull out our phones so we
can do voice to text or you don't you
feel like you can't you don't want to
type and you just like you know
on my way home send and so that aspect
of it has also become something that has
become commonplace but it wasn't that
long ago that these kinds of systems
were very hard to come by and they were
serious engineering challenges involved
to getting meaning out of a picture and
getting text out of audio so see moving
through that the systems for this even
though it's become commonplace in kind
of a consumer world building such a
system from scratch is still it's two
o'clock two o'clock it's still not
feasible for a lot of a kind of a
general audience right so that's where
having systems that are packaged
together for you help so I want to
introduce kind of three AP is at least
three api's maybe we'll see if we get to
cover more that wrap up the hard work of
visual recognition audio recognition and
text understanding so I think I see at
least one repeat face my take on machine
learning is that it is using examples to
answer questions that's my super in a
nutshell short boiled down version and
the answering questions is where machine
learning is useful you can collect all
the data in the world but if you can't
use it to answer questions that people
want to have answered then it's useless
so having api's that do the question
answering part is just as important as
building systems for training your own
data so this is my brief cartoon eyes
version of intro to what is machine
learning kind of stuff so this is a
Swiss cheese version of what is a neural
network so you have the layers of cheese
that the data pass through and as it
goes through each layer of a neural
network it gets smaller
and you can think of it as you know from
at the base layers it takes the image
and picks up the minut details learns it
has like detectors for edges and lines
at different angles and shapes and then
you go up a level and you might have
something that resembles recognizing
corners and circles and squares and
things like that and then you continue
up a level and you might get things that
are more resembling recognizers that
pick up their specific features like a
nose or a foot or an ear or a pot and
eventually you pull that all together
and you say oh with these types of
things this is a cat or this is a dog
machine learning has really taken off
right it's it's all the hype now and
it's been driven by three major factors
the first is just datacenters tons of
data centers everywhere and tons of data
secondly is the video gaming industry no
it's the GPUs graphics processing units
which allow you to do massively parallel
computations a lot of machine learning
and in particular visual understanding
is massively parallel in nature and the
reason why graphics cards are so useful
for this is that when they were
initially designed they were just meant
to show frames of images of either from
a video game or a movie on your computer
screen nice and smoothly so to do that
it had to push literally write upwards
of million or more pixels at 30 or 60 or
if you really want to shout out for a
fancy monitor 120 frames per second
that's a lot of pixels and it can be
drawing those pixels one dot at a time
as it goes across the screen right those
of you who remember dot matrix printers
so it has to push all those pixels to
the screen at once 30 60 120 times a
second and so that's what makes it a
really good hardware architecture to run
machine learning on and thirdly and I
think this is a point that is often
under appreciated networking
vast networks have enabled us to harness
the power of multiple machines together
because the the silicon the chips only
could get good so far and we need to be
able to have not just multiple servers
running multiple chips to do the
processing but you also need to be able
to network together all of your data as
our data sets have gotten larger it
becomes impractical to have them on one
disk especially with things like disk
failure and other kind of catastrophic
loss scenarios so thinking about the
cloud as a supercomputer basically
you're you're putting all of these
resources together all the bottlenecks
that have previously existed that
forbode us or cause us to have trouble
hooking together multiple computers now
are those walls are slowly falling down
one by one and so now you have a system
that is truly integrated as one even
though it takes up multiple city blocks
am i coming through okay by the way in
terms of the mic great and much more
lively audience in the afternoon in the
morning at some point in the morning I
asked some question I'm like not for yes
doing like sleep in the morning so deep
learning at Google has seen a well I'd
call that a meteoric rise in usage over
the past say three years these this is
the number of folders directories and
Google containing model files internally
and it's across tons of products
hopefully you recognize some of these
hopefully you don't hate all of them but
maybe you've only just like some of them
and everything from Google photos self
labeling the images too recently there
was a launch of a feature on Drive that
suggests at the top if you go into
Google Drive it'll show you the files it
thinks that you might want to use right
now in this moment which sometimes
misses but when it hits the hits it's
really quite something it's like yes
this is exactly what I want it so that's
a lot of fun
and smart reply and inbox all the
different things going on in naps right
like every other problem that Maps
solves is machine learning of course
YouTube recommendations
another classic use case of machine
learning so back to our Raspberry Pi
door I think we're ready to replace the
our cloud with some blue hexagons so in
Google cloud we all of our products are
in different blue hexagons and they're
all logos that are either drawn by
Egyptians or are just sometimes you
can't tell what they are so the one on
the left is the speech API and the one
on the right is the vision API so the
one on the right kind of looks like a an
eye right but painted maybe by like
hieroglyphs so in short understanding
the real world is hard on your own if
you wanted to create a system that could
recognize what it could see in a picture
in general right that's a huge problem
space so why not just use something
that's already existing that you can
just make a call to get your response
and then move on you can move on with
building the thing you're trying to
build rather than trying to solve the
problem of mmm gosh these sound waves
are really confusing I don't know how
I'm gonna get a word out of this so
again the speech API the vision API and
there's a couple of other api's as well
so let's talk about the speech API
apparently my orange arrows have shifted
over so imagine they're to the left a
little more it supports basically this
very simple operation right of turning
audio into text and wow that's really
easy to understand like a conceptual
level the actual task of doing that can
be hard thankfully using the API does
not have to be hard I think yeah we'll
go to the demo so what I've got for you
guys here is a C first so this is the
JSON that you would send to you know
make a request and there's just a couple
of items that I want to call your
attention to you know one is you can
choose a couple of different
I happen to just use flash you can
choose the language so if we have any
non-english speakers well we will grab
you up here say a few words see if we
can pick it up you can have it tell you
alternatives to to what it thinks it is
it will give you some sometimes funny
alternative translations a profanity
filter and there's a speech context
which we will fill in shortly or in our
site in our second time around so what
I'm going to say is hey would you pinch
me okay this is the old Simpsons joke so
okay so that's it's gonna record about
five seconds of audio hey would you
pinch me
okay so then it's gonna write that file
out and wow that Network you get what do
you think exactly what you think you
would expect to get right now what I'm
gonna do is give it a little nudge and
have it say hey I wanted you to add some
custom words to my dictionary Heywood
and you pinch me and now I'm going to
send the exact same audio file up
because I don't trust myself to
reproduce that amazing vocal performance
and see what comes back with the new
parameters added and you can see that
first result with high confidence coming
back as hey would you pinch me
so that has interesting implications in
terms of being able to customize
something on the fly as well as being
able to put things like company names
person names things like that right like
my name is yu Fang which is not in the
English language so if I said my name it
would probably yield something
interesting funnily enough I've never
tried that and so something to try out
another time so that's the speech API in
a nutshell
did we have any volunteers for other
languages alright
what language would you like to speak
sir Chinese excellent
that's right it is so here we have the
list of supported languages and what
we're gonna do is go down to Chinese
I'll actually would probably just type
in Mandarin I guess spell man
man great so we got a couple of
different dialects here we got Mandarin
traditional Taiwan we got Mandarin
simplified in Hong Kong and Mandarin
simplified China which which version did
you have any preference Hong Kong
Mainland China
okay mainly in China so the ears are
here's a language code and we're gonna
go ahead and go back to our request and
we're gonna replace our language code
here with that value and save that file
out and now think about what you want to
say would you want to say you know what
you're gonna say yeah okay ready go the
transit yeah three times really hard
against it see all right so you can see
there I can you see the characters now
you guys heard it now if you guys have
really good memory and really impressive
confidence value here well thank you so
let's put here oh I copied that instead
of the actual text I'll just do this
whole thing so you could oh right I said
if you guys remember what it said girl
she changed she thcbats in Hoboken Zoo
so so there you go that's the API you
can turn any audio or the language of
your choosing and have it say anything
you want
next up we have the vision API and this
is for visual understanding of the world
and you know as humans we experience so
much of the world around us through
visual understanding our eyes pulling
such rich data and we're able to discern
a lot of details through just pictures
or just the visual component of our five
senses and so too does the vision API
have a lot of sensory capabilities so it
does these six and and more actually now
and so I'll step through a few
highlights so when you send up a picture
here of a hydrangea you can get back
various descriptions with you know
scores just like we saw before and here
we're asking the API to label this
picture and say what do you see in this
picture in general right so if I took a
picture of us here today it might say
conference classroom things like that if
you wanted to say like carpet you might
be disappointed right because this a
picture like this is not a picture of
carpet that's not the primary subject
just because carpet exists in that
picture and and because you can't really
tell it I want carpet here because if
you already knew there was carpet here
why are you asking so if you want a
carpet take a picture you know it'll be
a picture of the carpet proper the next
thing is landmark detection continuing
with our Harry Potter theme here's a
picture from the Wizarding World of
Harry Potter in Orlando and we also get
back a latitude and longitude and it's
detecting this from the picture not from
the Geo tag or geolocation so that has
come fun implications of just being able
to upload your pictures and have it
tagged it automatically you can imagine
a situation where you have a picture of
the Eiffel Tower they have pictures of
some other stuff a few minutes later and
then maybe another picture of the Eiffel
Tower well that's probably all in the
same geolocation then right we can make
some educated guesses about that and so
some interesting kind of applications of
that and got a little sidebar here those
of you who are watching carefully you
might notice there's this mi ID
and what is this garble garble e Guk it
is a ID that tags into the knowledge
graph and so so you can put that value
up here in the ID and search for it in
the knowledge graph and that'll get you
back interesting information like a
Wikipedia link a description and so you
could follow that Wikipedia link and
then scrape that page and the rabbit
hole goes on and on
next we have face detection the face
detection aspect of the vision API is
really rich but what it does not do and
I want to make this clear is it while it
does facial detection it is not facial
recognition it does not recognize your
face as you it recognizes your face as a
face what does it see a better face it
gets things like roll angle pan and tilt
so it's like your angle so it's straight
onto the camera that's 0 0 0 and then
depending on how your face is oriented
it'll give you those numbers so you can
get idea of like where everyone's
looking in the picture you have emotions
joy sorrow anger surprise I believe
positions of the facial features right
eye left eye but also really esoteric
ones like that spot between your nose
and your upper lip that's got some
particular name you see there there's
the midpoint between the eyes so you end
up with a pretty rich facial data and in
general you'll get responses that are
kind of what you expect if the picture
is well framed right like these pictures
will come back with numbers they'll come
back with maybe things like door sign
and things like that but if you and sing
here if you have pictures that are it's
clear what it is a picture of you got
tractors in the bottom row there you got
various types of judo in the middle and
if you have a picture like this and you
say gosh the vision API is giving you
back information that says like
storefront or sidewalk and I was really
hoping for Statue of Liberty it it's not
gonna it's not gonna happen right but it
might actually pick up that it's
Manhattan just because of that that
location
aspect of it also doesn't hurt that it
says and why souvenir right there that's
a separate separate hint so that's the
vision API portion let's look at some
pictures here's me hugging a panda the
Panda looks very pleased to be hugged so
this was at Google i/o in 2014 and
what's interesting here is that the
vision API has changed over time in
terms of what it's told me you'll see
that there's no indication here that it
is a fake panda so for all intents and
purposes I hugged a real panda but this
has changed over time both in terms of
the confidence scores the ordering and
the values have come back and so the
model is updating in the back and
improving that does mean and then that
had various questions about this that
means that you cannot a hundred percent
say that right now what I see coming
back is exactly what will come back a
month from now there's no actual
versioning in the request right now and
I'm not entirely sure where this saft X
came from but that's also something that
previously was not detected when I want
to run this in the past so again you
know that you can see evidence in the
model updating the deck okay the next
one we're gonna do is this witty quote
that I saw at a conference in Serbia
about Tesla and the Edison so this is an
example of OCR vision API also does text
extraction from pictures for you and so
you can say give me some OCR action on
this thing and you get back right you
can see here it it still tries to you
know you can ask it for labels too and
you get back things that are of
reasonably low confidence and just kind
of like generic terms but the one you're
interested in is this text and you also
notice that it has the full text of just
everything gobbled together in one
string and then the actual individual
words broken down one by one little
chunks
let's see what we got next next we have
a statue this is a statue of this is
also at the same conference it happened
it's a statue of some prince in front
and this is in front of their like
National Museum of some sort and you'll
notice there's no signs here and let's
see if i zoom in not really you can't
really make out any sort of signage
about what it is so we got some people
and we got some town City Road landmark
that's all good and all but then you got
this location down here got the statue
of Prince Mihailo with latitude and
longitude in front of the National
Museum of Serbia so this is also kind of
fun little thing you can do if you want
to run through your own datasets Coke
machine logo detection so you can pick
up different logos and for some reason
it shows this thing wishes I still
haven't figured out why it has this
ascii decode issue but aside from that
it picks up coca cola and the last one I
will save for the actual Web API so I've
got here I uploaded this picture of me
holding a giant stuff Pikachu yes well
said
real Pikachu well let's see what the
vision API says about it the labels on
this stuffed toy so no no no luck on
that one couldn't convince it you got
mascot here though that's kind of neat
so the vision API is website has this
little interactive web thing that you
can literally users upload pictures from
your computer and see what would come
back and you can see here faces you have
two bounding boxes that come back for
like the whole head and then just the
fascia portion and then each of these
dots is a labeled point on the face so
it can get super detailed for detecting
different parts of the face and you can
see here we have the emotion detection
and that's yeah that's dry as expected
because I'm happy to hold picture and
then there's a web section now that has
some incredible things such as this
I was amazed by this one of new things
has been added this is where it was this
was in Tokyo and it was at this exact
store and it was in the you know
sunshine like city there's a mall called
sunshine City there and I was just like
blown away by that
yeah sunshine City Tokyo there's an
aquarium in the basement that's true is
it auto geo-tagging so it's using what
it sees in the picture literally the
only data that it has as an input are
the raw pixels that is it so there's no
so it's literally because I guess I'm
not the first person to take a picture
with this background surprise surprise
you can see lots of lots of Pokemon
hanging out back here and so as its I
guess we've gathered more data and
things from the places like Google
Images and such that's gotten better and
there's no text here but somehow it
decided that this little barcode here
had some text you see that gets
highlighted and then document yeah
there's the text and the ghost they'd
also give some dominant colors and
cropping suggestions for like automated
cropping of pictures just like the
subject and then the the only reason I'm
doing this here is for safe search safe
search comes back with four parameters
there's medical spoof adult and violent
now which of those oh let's do a poll
who thinks that this will be categorized
as a violent picture all right two for
violence who thinks it will be
classified as medical how about spoof
maybe a spoof and when I was like five
or six and I assume everyone else thinks
that it's a dolt like I can only assume
that that's the case because exactly
because Pikachu is not wearing any
clothing
now to be fair it is only a rating of
possible alright it's not saying that
it's even likely so there's 5 categories
there's very likely likely possible
unlikely and very unlikely so this is
three out of five so it's somewhere in
the middle but it was just funny that it
even suggests that that's the case still
to this day I have no idea why it's
doing this but it's been very reliably
telling me that it is adult hospitals so
that's the vision API it's it's a lot of
fun to play around with and you know
there's yeah I'll tell you more about
how to how to play around this some more
so vision speech so with speech we took
audio and turn it into text but Texas
just characters right and unless you
want to do raw character substring
matching
you're gonna need a little bit more than
just the raw characters of what was
spoken so it would be nice to get some
entity recognition recognize that which
parts of the sentence might be
particularly important relevant to the
rest of the sentence which ones are
cities which ones are people which ones
are places things events I know this
isn't Wheel of Fortune so then there's
sentiment which gives you the general
positive and negative sentiment of
sentence and then syntax where it gives
you the classic kind of breakdown of the
nouns and verbs and adjectives so it's
into the entities you get something like
this it picks out these entities and for
each one you could it'll show what it's
tied to so this one is a location
Roylott you got a Wikipedia URL so you
can see there it's born and raised in
Joliet Quebec and then you have this
person's name Denis Coderre
who apparently is famous enough to have
a Wikipedia link still working on that
and thirdly the University of Ottawa
categorizes organization so this sort of
thing you could have imagine if you had
tons of pictures and then you did OCR I
got text out of them then you took that
text and then ran entity detection on
them and so you can begin to see how you
can build up a trove of metadata without
having to
open a single image on your computer and
without having to type a bunch of things
into Excel spreadsheets so
sentiment analysis gives you polarity
and magnitude polarity is just positive
or negative and help much
so that's only goes from negative 1 to 1
and then magnitude goes from 0 up to big
numbers but usually it's just within you
know zero to like say 10 or so and
syntax turns a whole sentence into a
whole lot of details the actual response
that for all of this everything we
talked about today is JSON these are
just various visualizations to help you
see it better because if I throw a bunch
of JSON at you that said like this links
to that that links to that it's not
nearly as fun but sentences apparently
get really complicated so here we can
see a little jiff I made of the three
aspects entities with sentiment and
syntax would anyone like to do a
sentence anybody want to come up with a
sentence so I'll type in 3 whoo well
said Pikachu is a child friendly
character let's see if I can where does
the box go box
and we can see what it does in terms of
whether we add a hyphen between child
and friendly later so we'll see how that
goes
so here's Pikachu we got a Pikachu
article right to Wikipedia and we got
character and you see here it also comes
back with salience salience shows this
importance of a word to the rest of the
sentence how relevant is it
I have sentiment generally positive not
super strong up to one but you know
closer to that 0 and then syntax got the
full morphology here anybody at grammar
nerd get in on this so that's by and
large the natural language API and yeah
so with the natural language API the
speech API the vision API we also have
the translate API you guys who has not
used Google Translate ever great so we
have zero hands which means I can
continue without explaining about Harry
Potter or this is gonna make the talk so
short I'll explain all these backstories
and so Google Translate is now also an
API which means that you can send a text
ask it for a language and you get that
the language back so these things can
all begin to chain together you have
vision and speech kind of interacting
with real world closer to real world
type stuff and then once you get it into
some form that's more kind of computer
computer eat you can pass it through
things like speech API translate API and
get even more information and then you
can tie that with the other awesome
things that you guys are all building
and generate more data and perhaps pass
it through other API is right again or
in different combinations so we've seen
a lot of fun combinations built from the
kind of creative combinations of the API
so
I think this is something that will
continue as we go there will be more
things will move in this direction of
api's and of things that look like
customizability we start out with this
peach API right richer API is with lots
of functionality richer machine learning
api's that do more and more things to
the degree that when you think you need
to do something custom when it comes to
machine learning a I consider not if you
don't need it it will save you time
energy potentially money but but mainly
it's you know the frustration and work
of that right you want to build this
interesting thing and you want to get
there as soon as possible
so doing that by leveraging the kind of
LEGO bricks of the various API is out
there can get you there faster easier
and in us with a system that's better
architecture basically right you don't
you don't run the risk of creating
monoliths if you are being forced to
call out to external services think that
was oh yes
I promised free tiers I guess they
remembered of course what I meant was
free tears all the api's we talked about
today have free tiers that means you can
go and use it and play with it hook them
together experiment with it and not need
to worry about putting in a credit card
or dealing with all that mumbo jumbo
it's it's I've had great feedback from
like universities and students and
things like that they can play around
with this stuff and you know as we saw
some of the responses can be well put it
lightly hilarious
the only explanation there I would add
is with the national language API 5,000
units a month is one unit is a thousand
Unicode characters so that's like five
million unicode characters so yeah it's
just so that you don't it's not like per
request because you know then you're
incentivize to make one request massive
and nobody's it so keep it simple on
both ends the other thing I'll point out
is with the visionary pal I say a
thousand requests for month the detail
there is it's a thousand requests per
month her type of request so it's a
thousand labels thousand OCR's a
thousand facial-recognition a thousand
of all of each of these things that's a
lot more than it might seem at first and
so we make that request if you say
here's a Pikachu picture I want you to
detect for the label and the face that's
two that's two detection 'he's right
let's see yes and finally there is the
recently announced vision i mean video
intelligence api some of you may have
heard of it we just publicly announced
it two weeks ago and what it does is it
lets you take a video and it will return
to you in an annotation of the video
with timestamps and what is seen at
those timestamps so imagine having a
video where you're biking through your
neighborhood with your gopro and you
know there's some shots of some puppies
and there's some lawns and houses people
walking around there's some cars right
so you would get an annotation bag that
says like cars
ten seconds to 12 seconds 13 seconds to
14 you know 20 seconds to 23 seconds so
little shots with those things in it and
then it also breaks it down by shot so
maybe you were like holding your camera
over here for a while and then you kind
of like in you know one second sweep you
like go over here and you over here for
a while right you're still pedaling
along that's what this balancing I'm
doing is so it'll break it down by shot
I'll say the first shot was here was
this portion of the video and then the
next shot started and then the next shot
was this long and then we had a short
shot here and then there was a quick cut
to this so what you can you know imagine
doing there is making things like
rich meditated tagging of all your
videos and then being able to search
across an entire library of videos you
say show me all the shots of puppies in
you know these ten videos and then just
being able to be like oh it's this
little bit here this a little bit here
this a little bit here and doing that
sort of thing I let's let's since we
have time let's go ahead and just do it
instead of me trying to act it out
because nobody wants to see that video
nope I think it's like video -
intelligence or something here we go
video - intelligence called it alright
wait okay okay
there's nothing there so it actually
comes with a few sample videos so I
promise bikes so there will be bikes oh
okay so this is uploading this video
right now to the video API and it's
processing it
and well in real time it's actually
faster than real time little process
about four seconds of video per second
so so it's it's turning through this and
I will watch it just just to give you an
idea of what it is I'll keep the sound
down for super exciting so you got a
dinosaur there we got yeah there's some
sports action at some point so this is
like the levels of the entire video
right so what we're really care now
about is the per okay so the shots
and the shot labels
yeah okay so there's your shots there's
your four shots so you had the first
shot does this little bike sweep and
then the second shot might be the bikes
yeah and then this is the second shot
now we're like looking up here right and
so this is kind of long shot as the
camera sweeps along I'm gonna skip ahead
oh here we go okay so still that shot
and then we're wrapping up that shot and
it's segmented the next shot as being on
this side and now we're like over here
so as always the data that comes back is
coming from in the form of JSON right so
the API comes back like this you get a
long response move so all the different
values these are the I'm just gonna
scroll past them because reading it now
is not a good time to read it right it's
not especially not with this blown up
fund so that all got you know turned
into something that you can more easily
visualize so per shot it gives you
information but it also gives you the
data of like for each label where those
things are so you can assemble the data
how you want and when you think about
having when you have more than one
videos worth of data and then things get
interesting and then if you know what's
in there
maybe you can say okay within this time
snip I know there's something here that
I'm care about extract some frames maybe
one every half a second feed that to the
vision API looking for the thing I'm
looking for maybe there's some text I'm
looking for because I'm picking out
signs then I put it to the OCR then they
get the text and I can do national
language API it's a lot of fun
combinations these combinations wit of
which are would be very challenging to
do if you were to just like try to do
this from scratch truly from scratch
right and I guess my point is that I
would encourage you to play with it this
notion of play is really awesome here
because you can tinker with this
easily rearrange and try different
combinations and that's something that
until now but now I mean like within the
past year hasn't really been possible
and we've been stuck kind of close to
seeing just the tree but now we can zoom
out and then work at the forest level
and move things around move bigger
pieces on the board double clicked so
with that I'll let you guys out early
class dismissed now let's open up the
questions yes
yeah so so the question is about my
so-called crazy combination what what
kind of crazy combinations are there
some of them are more along the lines of
practical applications some companies I
think it was realtor.com uses the vision
API it's to pick out you know their
signs from from pictures and to pick out
like the postings right and then extract
the text but in terms of fun
combinations one of my colleagues ran
sentiment analysis on Twitter hashtags
during the presidential debate right
then you can chart that out she loaded
all the data from the Twitter firehose
into bigquery and then turned through it
all went one by one and that took like
took a couple hours to run play down
then again so did the debates right and
let's see what else's vision API the
facial detection on the visual vision
about in terms of emotions is
interesting so you can bucket them into
different like you have different
buckets of pictures and some of the
conferences that I've been to they've
made kind of like we called it a moto
booth so you can take a picture maybe
sometimes they hire a photographer you
take a picture and then they will run
that picture through the vision API and
then get the emotion analysis right and
then based on what emotion it shows that
it frames you in like a certain light
and like if you're like happy it's like
yellow if you're like angry it's like
purple or something red there's also one
that where they'll you know how at
carnivals or even at like amusement
parks there are those pictures where
there's like this big wooden cutout and
there's holes to her face and you know
get behind it and then the rest is all
filled in right so it generated that
because you it knew where your face was
right there was those facial bounding
boxes and so it's based on your emotion
put you in a different face spot right
so so you can really get creative with
it I first blush some of the data that's
coming back
just like yeah well I don't really care
where the faces in the picture or where
the edge of the eyelid is but like well
if you want to draw a pair of sunglasses
on all your friends faces then you kind
of do or a mustache you can put a must
because that spot between the upper lip
and the nose is is also identified you
know some glass glasses there's like the
dots around the I are like Center by and
there's like above below the left or
right and that's just like the inner
circle and then there's like another
loop around that and like where the
eyebrows are it goes crazy I'm making
everybody's eyebrow go up and and while
a lot of these are like toy examples
there are obviously also think more
business see applications to question
the back so the question do it and
switch out what do you mean by oh yeah
you want to see it go right now
okay let's do it so ready three two one
quick well with what with Wi-Fi so let's
try quickly any good experiment needs to
have multiple runs right yeah so so some
part of that is uploading absolutely but
it it takes out it's the the actual time
in the servers like one to two seconds
system including network traffic is what
I've seen yeah
so the question is around facial
recognition versus detection and whether
or not you can supply your own images to
train the model instead of speed yes yes
so that's actually a great point is that
this Harry Potter example it's really
listening for my voice and looking in
this particular case I had to be smiling
so I only wanted happy people to who
knew the password don't want any angry
mobsters and so unfortunately the
visionary plan right now is waste design
is is a read-only basically system so
it's a model deadish everybody uses the
model obviously they upgrade the model
but you all use the same one and so
there's no way to upload your own custom
data for the vision API if you wanted to
do custom things we have the cloud
machine learning engine for doing custom
work yeah just because the problem of
finding where in a picture the face is
at all and what angle right that that is
taking you good ways um like using
libraries like open CV it's easy to do a
perspective transform so then you can
say okay this face is this way in this
part of the picture let me just give it
a turn and now all your pictures are
like perfect centered front facing shots
which makes any sort of machine learning
you want to do after that a lot simpler
yeah so this idea of having certain
things turn into API is in building
blocks like this allow you to chain
machine learning on machine learning you
know you because the vision you know
basically you're running it through a
neural network and you take the result
you run through another neural network
you take that resulting two runs or
something else and you can just begin to
build this entire end-to-end system
there's a question here
so the questions around whether or not
there's data coming back from the speech
API around voice recognition right
getting pitch and tone and things like
that so right now the response from the
speech API is just the text it's its
main purpose is to determine the words
being said and translate that from audio
to text as accurately as possible
and each of these api's are kind of
trying to to whatever degree they can
limit their scope as much as possible so
that it does the one thing and it does
it well and then it allows you that
developer then to easily to put them
together in different configurations
because if it did a whole bunch of
things at once and then you're like well
I only want this part and I wanted this
in this other order it's like I can't do
that so the pitch and tone thing is an
interesting area and I wonder if that's
something that might be better suited to
local detection right doing doing that
sort of analysis question in the back
yes
how many way hmm
question is about how many different
types of Spanish does it recognize let's
find out see because you're not the
first person to ask this not this
question for these questions I always
have this tab open so let's do search
for Spanish the control F says one of 20
I won't read them all out but here's one
page of them I'm going to scroll down to
the others half of them so there's a
couple yeah
so for the Chinese yeah so the questions
around whether you can process large
timescales of audio and turned it into a
large body of text and so the short
answer is yes there is batch mode which
is what I demonstrate here but there's
also a streaming mode where you can
stream audio and just have text coming
back to continuously so if I were to
livestream this talk and I wanted to oh
I don't know caption my video that would
be an application oh yeah the caption
and say the video into text right you
would want to separate those beforehand
but there's the tools that exist today
the separate audio and video tracks from
from a video that those are yeah that's
very easy that's yeah tons of tooling
around there just one quick cool
I will say that there I have I don't
have stickers for these guys but I do
have tensorflow stickers tensorflow is
the machine learning library that Google
has for doing my custom training so if
you guys want tensorflow stickers I have
those or if you has any have any other
questions feel free to come up it looks
like there's enough another session
starting soon question mark I got some
nods see people are awake in the
afternoon that's great yeah thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>