<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to fail at benchmarking? by Pierre Laporte | Coder Coacher - Coaching Coders</title><meta content="How to fail at benchmarking? by Pierre Laporte - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to fail at benchmarking? by Pierre Laporte</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gQ6LEbjhVr4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody thank you for
coming to this talk about about
benchmarking first thing first if you
have any questions feel free to to shout
do not really wave your hands because
those 44 spot slides are right into my
eyes and I don't they see a lot of
things and if you have questions don't
wait until the end of the talk it's it
would not be the most useful thing to do
ask them right away so thank you for
attending this talk about benchmarking
my name is Kayla port I'm performance
engineer at datastax the company behind
the chica Sandra so who who knows
Cassandra here ok quite a lot of hands
you guys are awesome because there is an
amazing piece of technology and that's
that's really really nice to see so many
people investing their time and that
brain on on it for those who don't know
Cassandra this is a no sequel database
we usually represent that using using
rings I don't see my pointer okay using
rings like this and it has a master less
architecture so what does it mean it
means mostly two things first since we
don't have any master since we don't
have any privilege node it means it can
go down if we not involved on time at
the database level and this is this is
great right it means you can lose a
given number of nodes your application
is still up and also due to its
architecture it has some linear
scalability capabilities so if you
double the number of nodes in your
cluster you immediately double the size
of what the amount of data you can store
and the amount of requests you can serve
our goal at datastax is to make some of
the best transactional database
distributed database in the world and on
top of that we develop our commercial
offering where we integrate more apache
foundation project such as solar for
faster queries Hadoop and spark which
are spark is really 20 these days for
analytics another
other features especially for security
and monitoring the mentoring especially
for the operation guys you know the one
who wakes up in the middle of the night
other ones who read natural salutes just
for that they deserve our respect so in
the whole sequence pace we see a lot of
benchmark really a lot and they pretty
much all look the same now let me let me
show you how it looks I'm speaking about
fruit boots right so more is better
usually it's a man thing usually the
benchmark look like this our solutions
their solutions losers and it's I didn't
even tell you which one where cassandra
is is cassandra this bar is cassandra
that bar I didn't tell you that now why
didn't I tell you that well because it
depends on who publishes the benchmark
obviously so what is a benchmark yeah
what is a benchmark well a benchmark is
what you do when you take two pieces of
technologies you put them under the same
condition really the same conditions you
even the same workload on those on those
technologies and then you immediately
declare who is the best or which one is
the best now if you follow that
methodology you could as well prove that
the squid is more intelligent than the
cat when it comes down to sorting
geometrical figures so the lesson here
is if you want to have a pet do not take
a cat take a squid a squid can help you
sort your socks do your laundry it's
obviously more intelligent so that's the
lesson here now I've come to realize
that whatever you do you either succeed
or learn and in the performance world we
learn really really really often so what
are we here for what why why this talk
well we are here to make mistakes if you
recall the keynote from from Wednesday
physicists make mistakes all the time
there are two
our theories most of their theories are
proven wrong every day and this is how
they work you shouldn't be ashamed of
our mistakes as long as we do not blame
each other we're not here to blame
blaming is not part of any scientific
method and plus benchmarking is also a
people exercise you see a benchmark you
think it's obviously wrong now you have
to explain that without hurting the
people the the person's feeling right
because it may be a genuine mistake
maybe no no harm was intended so that's
a people exercise that's a social
exercise we're here to increase our
standards and we're not here to blame so
now let's see how we can fail we will
java conference and the java developer
so i write a lot of a lot of AP is and
especially i have this REST API where I
allow my users to log in on my system
it's easy it's an HTTP request post
request on the / login endpoint now I
want to venture that that endpoint so
what I'm going to do is perform 100,000
logins now I'm a bit late on my schedule
and I'm a bit lazy as well so I could
write this huge test data set but I'm
not going to do this what I'm going to
do is well I'm going to think and say
nothing is different from when logging
into another so one thing leading to the
other I'm going to perform 100,000
logins of one user because that's
obviously the same now I'm going to
measure each and every log in starting
from the first one and i'm going to do
that during one run and then i will have
my my results right so another point is
i'm not the only one light here my IT
team is also a bit late because i asked
them to provide me with for the term
like machines but yeah they are late the
order was
I don't have my machines so what I'm
going to do is run my benchmark on this
which is obviously a pretty good machine
question no okay and so I'm going to run
that on my laptop now my product my
project manager he's not into really
statistics performance he's into Excel
spreadsheets and gun diagram so he asked
me for really simple data right so what
I'm going to do is average all my
latencies and give him the result and as
a bonus I'm I'm going to look at my cpu
utilization and do some capacity
planning on top of that right I told you
we were going to fail I think that's a
good plan do you guys see anything any
point in there that is wrong okay shout
averaging more users every single point
yes yes absolutely there is nothing
correct in that benchmark so let's have
a look at this when i say i'm going to
do 100,000 login of my system well i'm
actually not going to answer the right
question I want to benchmark the
capacity the ability of my system to log
users in but if you recall the cash 11
session from monday we have caches
everywhere on our infrastructure we have
caches in the CPU the l1 l2 l3 TLB we
have caches in our operating system if
you use a database oracle Cassandra it
comes with its own caches as well and if
you use a very famous object relational
mapping framework it comes with three
different caches as well so the question
I'm going to answer is not how many
users can I simultaniously login it's
how cash friendly my test data set is
and in that case it's probably going to
sit nicely in the l1 cache
then I have the clock resolution well
this one is a bit it's not exactly a
mistake it's more of a warning if my
login takes five milliseconds and I
measure it with system.currenttimemillis
obviously one unit of measurement is
going to give me twenty percent variance
so I could use system no no time but
then I must be really careful because I
will get a number of nanoseconds but not
necessarily an increment on a per
nanosecond spaces it could be
incremented every 15 micro seconds and I
have to to know that I can I cannot just
think i will get non 0 seconds i'm good
to go obviously there is the warmth
period so you all know when you when you
start a JVM the code is interpreted at
first the JVM observes the code look at
how it's how it's behaving how it's
called and decide which optimizations to
apply now you might be wrong at some
point it might have to do Q eyes the
code as well if you measurements is
during that period what you are going to
have is a lot of noise in your
measurement so you don't want that you
don't really want to measure anything
during that period now when I said I'm
going to perform one run what if I had
extra background noise like one or
another developer that was on the
machine log in some or zipping some logs
downloading them to analyze my previous
run what if I add I had better noise in
my measurement how can I build
confidence over my results well with 11
I just can't so maybe I want to perform
a lot of runs and do some statistics
over them obviously there was a time i'm
pretty sure it's over now but there was
a time every developer had a windows XP
32-bit machine with a really big and I
cameras on it it's obviously different
from a production machine and finally
there is the average well I'm not going
to talk now about the average we go with
about that later but pretty much the
message here is beverage it should not
be the only thing you're measuring but
it's interesting now what about capacity
planning team is a question for you guys
during my benchmark my cpu utilization
was fifty percent and i could sustain
1000 logins per second so my question to
you is how many logins per second can my
system handle God knows anyone else more
than 1,000 okay no one is going to
suggest two thousand and that is
actually a good thing the funny part is
we are in a conference right now and I'm
there are a lot of traps in this talk so
you obviously know that something is
going on but often our brain when it
sees that think well it must be close to
2000 isn't it actually no there are many
many things in our in our infrastructure
is that can make us not use our CPU to
its full extent and it's completely
wrong to assume that we would have a
linear increase now it's very special
it's very curious because that same
brain that we completely unplugged there
it's at it full power when we drive when
you drive you're in your car you are
computing integrals all the time to
deduce where will be the term where the
objects around you will be in five
seconds so that you don't bump into them
that there is a huge amount of power
there and at some point I don't know we
completely unplug it so that was a
puzzler using only words and images now
let's see what we can measure in our in
our benchmarks I talked about the
average I personally think the average
is an interesting thing to measure but
not the only one so anything about it
the average is the best way to get rid
of the outliers
typically the average of two numbers a
and B would be that third number that is
equally distant from a van from be you
know yeah I think it's an interesting
data point but it's definitely not
enough I think it's interesting because
why do we keep hearing about average
latencies why I haven't answer because
we demand it we demand an answer we talk
with our product managers and we asked
them what should be the latency of that
of that particular endpoint and our
product managers they they have a rough
idea and they know one thing they know
that a production environment is not
like this definitely not so they know
that but yet they don't want to let us
with an unanswered question so what they
tell us is well roughly 200 milliseconds
yeah 200 milliseconds that would be that
would be good so let's say on on average
and this is why we we keep hearing about
the average this is why it's an
interesting data point because this is
the vocabulary of our product managers
and we need to talk we need to speak
that that language now it's not enough
right what we may want to measure or
what we should always measure includes a
lot of latency person dies there so
typically if I have 1 over 10 requests
that is really slower than the rest what
would be an acceptable value if you went
to Conrad session this morning he had
this this nice sentence if I'm not at
the average latency if I'm at 500
milliseconds response time does someone
die if not then maybe that's an
acceptable response time for my 90th
percentile and as well I want to include
a lot more percentiles so that I have
more precise requirements
now how many nice do I do out there you
see some some dots some point how many
9s should I measure in my benchmarks
well I'm going to go for the cheesy
answer it depends it depends because if
i have a 500 if i have 500 users on my
application what does my 99th percentile
represent well it's going to represent
the five unluckiest users of my system
and since it's my system i get to choose
who is the unluckiest users who are the
unluckiest users now if my system has 1
million user that same 99th percentile
is going to represent 10,000 users so
it's going to be a bit different and in
that case I smash as my system gains and
maturity and we want to add some nines
in my requirements I will want to target
less and less unlucky users I we want to
grab them and get them in the nice nice
spot and of course I always want to
measure the maximum response time not
really the standard deviation by the way
I think it's pointless for latencies but
the maximum response time we should
always measure it if you are in a video
game and the message that is going to be
game-changing for one team takes 30
seconds to be delivered because probably
because of network timeout is it okay to
ignore that not really so you always
want to measure that now any benchmark
starts with a question and if you went
to a Kirk and Morris session on I think
it wasn't monday at some point they
started the tuning exercise and Kirk
asked okay what is my latency target
what is my performance target and he
didn't get an answer now would you write
a recursive function without a stop
condition when you do a benchmark
and you don't have precise requirements
that's exactly what you do at any point
in time you will have a component in
your system that is slower than the rest
when do you stop well if you want to
stop you need to have very precise
requirements so typically that would be
a really long sentence which we're going
to see right now this sentence contains
three big big part the first one is the
environment the environment says how
many machines do I have what is the type
of the machine how many CPUs how much
RAM what kind of disk spinning or says
these what kind of network card and what
is the state of my system when I start
my benchmark then i have the load which
is what i'm going to write in my load
testing framework but what i'm going to
do on my on my machines during that
benchmark and finally i have my
requirements now you see here i have an
average latency and that's pretty ok
because that's what I got from my
product manager but I also got the 90s
percentile and the maximum response time
so now I have enough material to work at
least for a couple of iterations and if
a benchmark start with a session it ends
with an answer that might be
satisfactory that might be not and that
that might be not that's the good part
that's the good part because when we do
a benchmark when you do some performance
chinning the interesting part is not the
what it's the why back to this question
ok I'm not meeting my my latency
requirements that's the good news
because now I get to start my profilers
and do some really interesting stuff so
having a not satisfactory answer is
actually quite good but anyway even
though benchmarking is fun please
remember that the results are contextual
I told you environment low
requirements everything defines your
contact you also have some compromises
if some part of your infrastructure is
completely fixed your results will be
specific to that context and we should
never forget that so benchmarking is
about statistics so let's see some some
statistical truth what I like with
statistics is that it's the best way to
be right and to be proven right by
numbers so given a the CTO of little
system has this blog called latency tip
of the day where he has some really nice
articles including this one it says when
I'm on a website I'm going to experience
the 99th percentile latency percentile
most of the time on average now most of
you guys should be like what and it's
it's really simple it's a simple it has
a simple explanation it's based on a
study that is from before HTTP to that
says a typical web application had 100
web object which roughly means 100 HTTP
request images CSS JavaScript files now
what is the probability of me avoiding
the 99s latency percentile it's easy
it's zero point 99 but if I issue 100
requests that probability is 0 point 99
to the power 100 which roughly means a
little bit more than thirty six percent
so on average I'm going to to get the
99th latency percentile now this is
front end performance right so if you
want to dig more into this this topic
there's an open source tool called
yellow lab tools which is really great
which will tell you Oh in that page you
should merge those files merge those
images which will help you improve your
front end performance that's a
completely different feel than back in
once and GC tuning and such now back to
statistics we need to talk about normal
distribution my client these these four
numbers here these are what my client
kept of a previous benchmarking session
that's all there is the average the
standard deviation the maximum and the
median and my question to you guys is
what is the 99th percentile roughly I
see some crack any idea isn't anyone
going to suggest 160 okay so that would
have been the correct answer by the way
in a normal distribution the 99th
percentile is roughly the average plus 3
times the standard deviation so that
answer is correct the problem here is
that my client assumed that latency
follows a normal distribution and that
he could only kept that numbers and
still get all the data points back so
that would be a really efficient
compression method but the latency the
latency is never follow a normal
distribution and it actually makes sense
think about it the normal distribution
is like this what does it mean if my
latencies follow that it means that on
average my my latency is like well it's
okay ish sometimes it's slow easing Lee
fast but most of the time it's okay ish
and sometimes it's really really slow
that doesn't make sense actually if you
don't know the this quartet name that
are unskilled a mathematician this is
composed of four different data sets
that have exactly the same average and
standard deviation on both x and y axis
axis so my client is completely blind
his latencies could be distributed
in any of the three first graphs
probably not the fourth one he's
completely blind because of that
assumption latency never follows a
normal distribution now who uses metrics
here the metrics library okay a couple
of people internally metrics uses a data
structure called a SF well to store the
data points the good point about open
source software is that it's it's not it
about not paying any money is that you
can have a look at the source and when
you look at the source of the reservoir
look at the four last words of the
Javadoc every single implementation of
the reservoirs in in matrix contains
those four worlds so what does this mean
this means that your money your metrics
framework already knows the shape of
your distribution it already knows what
the distribution will look like so when
you will send some points it will not
store them it will keep them based on
some sampling and based on that normal
distribution it will pretty much give up
the really important metrics even though
you trust it so you get wrong
measurements now how can we check that
how can you we fix that there is a
library called HDR histogram which if
you don't know it you should really have
a look at it it's well designed it's
used in the high frequency trading world
and it's regarding latency it's it's
great you pretty much send all the data
points you want it will keep them it
will truncate them after the third most
important digit which means usually a
0.1 percent of error not a big deal and
what you gain from that it it's that it
has a constant memory footprint so
that's pretty cool you can measure all
your data points using the same amount
of memory
us you can also graph it in a very well
interesting way look at the part in this
graph between the first and the 90th
percentile it's complete it's completely
compressed a better at the left most
part of the graph why well because we
already know what it's going to look
like anyway it's not the interesting
part our requirements are in the right
part of the graph this is what we what
we are interested in and this is why we
should zoom in the closer we get to the
maximum response time so we've seen some
we've seen a lot of images but we're in
the Java conference so let's talk about
some code this is a code sample
extracted from a very old egb doesn't
even see a problem with that code
integer comparison is using equal equal
which means the reference comparison
that's an easy one so maybe this one was
was initially using ins and that it was
migrated to use integers so I'm using
reference comparison to compare
something that is well objects not
values now this is it's a bug right or
is it I have this code sample which
create on the release of 50,000 integers
from 1 to 50 thousand and I have my s
consumer method here and what I'm going
to do is iterate from 0 to the number
given as a command line argument to
check if that reference comparison holds
and when I run my test I get this it
seems to work for every number between 0
and 1 27 why is that integer cash is it
so in the JVM we have an integer cash
which catches all the javelin integers
from minus 28 2 + 27 and so when you
call integer that value of using one of
those one value in that range you will
get the same integer instances so that
that was an easy one right now does
anyone know the flag X X colon plus
aggressive opts Kirk obviously ok what
does it do it enables super cool
aggressive optimizations it could be
replaced by X X the dash X X colon plus
do magic yes actually not really so this
flag did unable some really cool
optimization in the early days like Java
5 something but since then all those
optimization have been moved to the
hotspot trunk and are enabled by default
but let's see it still does something so
thing quite interesting I'm going to fix
my code without modifying it I think
that's that JVM option to my command
line I fixed my bug and now I'm good to
go so what's the explanation what is the
possible explanation here well obviously
it much it must increase the integer
cash and that's exactly what it does
pretty much that the only thing that it
does now the question is what is the
maximum java.lang.integer value well I'm
not going to give you that answer
because that's part of the fun max int
no that's not this one no so if you want
to find out you have three possibilities
you have the runner code it's it's on
the slide that's pretty much it you can
look at the print flag final output and
compare that with a regular regular run
or since hot spot is open source you can
look at hot spot source in the argument
dot CPP file that's pretty interesting
now let's increase our standard here is
another pest law so we're in 2015 i have
a class called dog that has a method
first that calls 1200 times the method
second that calls a thousand times the
method third that creates 1,000 object
now my question to you guys is what does
it do but that's going to be a multiple
choice question this is that it does
this code yes does it not do what we
think does it measure hotspot or does it
create roughly 10,000 objects two and
four so let's do this who says one this
code yells no one who says that this
code doesn't do what we think okay i
would say twenty percent of the room who
says this code measures hotspot
two people okay and it does it create
10,000 objects for five people okay the
correct answers I told you the correct
answer is well they were all of them
there were all of them because we are in
2015 we have a class dog that roughly
create one point twenty-one gigawatts so
it yells definitely it yells now moving
on to the second part I'm creating one
point twenty-one gigawatts the
time-travelling issue aside I run that
on my two gigahertz machine two
gigahertz CPU and it took 169
milliseconds to run now let's think
about it two gigahertz means two billion
CPU cycles per second if my benchmark
run or if my test run during 200
milliseconds it means i consumed 400
million cpu cycle to create 1.2 billion
objects that means three objects
creation / cpu cycle that's not possible
the CPU cycle is the smallest unit of
computation that I have that's not
possible so something must be going on
write this code actually doesn't do what
we think that the code that is executed
is not the code that we wrote let's see
why well let's start with the third
method in the third method I am creating
an object now if I'm hotspot and I'm
super smart I can prove that that object
is not used anywhere the block inside
that loop doesn't have any side effect
so it's pretty much dead code so I'm
going to remove it now when I've removed
it I'm I'm left with an empty for loop
which can be eliminated now if I merge
the third function into the second one
well I'm left with an empty and one
thing leading to another I don't have
any more code left so the co biet that
was executed was actually this one now
how much time did it take for hotspot to
optimize my code away well roughly
10,000 iteration that's the default
compile threshold of hotspot and so in
my test here my test did yell it didn't
do what we thought it did measure a
feature of hot spot called dead code
elimination and it measured that during
roughly 10,000 object creation that's
fine isn't it benchmarking is fun so
let's talk about coordinated omission
now i am a Java developer so I have this
fantastic rustic di love rest api is
right i have this fantastic rest api
that has lambdas and method references
and what it does is well when I will
issue a HTTP GET query on my routine
point it will answer hello world now if
we also wait 10 milliseconds here but
there is an explanation for that I think
returning a string hello world is
probably going to have a latency in the
nano seconds maybe microseconds range so
I'm going to dominate that with a threat
but sleep so that I know in advance what
my latency will be pretty much all the
time I know that it will be around 10
milliseconds maybe mmm 11 milliseconds
not more than that and you're going to
understand why that by the way does
anyone see something wrong with that
code I'm going to take that as a no so
I have this client now my client is
quite simple it's going to perform 100
queries at a rate of one query per
second and it's going to store the
latency into an ArrayList then I stole I
sort the array list and I print my
person dies that's that simple right so
do you see any is any problem with that
code I'm going to take that as a no so
now let's do one thing let's run my my
server and let's freeze it during 11
seconds precisely how can I do that well
I either have to really to rely on the
GC which is not an easy thing to do or I
can freeze the server at the operating
system level using a six-top and that's
what I'm going to do at the 52nd of my
run i'm going to send a six-top to my
server sleep 11 second in my bash script
and then send a secon to my to my JVM to
unfreeze it why am i doing that well
because I want to freeze every single
parameter in my benchmark I want to
predict exactly what will happen in my
benchmark and now i can do that because
i know what my timeline looks like i
have a normal state of operation here
and there and here is the part where bad
things happen that's that simple isn't
it so let's do a pre run check I know my
throughput I know my typical response
time I know that my longest or my server
will be freezed during 11 second it will
be completely unable to answer any query
during that period and I know that my
test is going to last 100 seconds as
well so what i can do is well here
is the throughput of my benchmark I'm
going to this is a boring graph I'm
going to perform one request per second
during 100 seconds now if I do the same
for Layton sees i'm going to have this
the request that will be sent at the
50th second will have a latency between
10 and 11 second i don't exactly know
the precise latency because it depends
on when it will be sent but i know it
will be between 10 and 11 seconds the
next request will have to wait between
nine and ten seconds to get its answer
and so on and so on until the last one
everyone is okay with that right away
yeah okay so now the beauty of this this
benchmark is that I can state my
expectation completely I know already I
already know what my 99 s my 98 my 95th
percentile will be why well because i
only have 100 data points so you may say
that benchmark is really well written
you would be correct so i know what my
outliers will be i don't know anything
about my average but we already know
that averages are pointless i might have
a couple of hundred milliseconds errors
in there but but since i'm measuring
seconds it doesn't really matter and
plus it's not a real benchmark you know
that because you were there in this talk
at the third minute of the talk so you
know I've done everything wrong don't
believe it now I run my benchmark and in
reality what i get is this output let's
analyze this so the comments aside i
have my 100 data points I have my
maximum response time which definitely
is between 10 and 11 seconds this is
fantastic I completely agree with that
that's okay but look at my high
percentiles look at my 99 and my 95
tyla what did happen 16 milliseconds for
my 99th percentile this is definitely
not possible I have my 100 data point
and my the reality doesn't my it doesn't
match my assumption but I told you I
froze every single parameter of my
benchmark I did predict everything that
had to happen in my benchmark so I'm I'm
in a situation where either I don't have
enough knowledge about my system or
there is a bug and in that case there is
a bug the system is simple enough so
that I can say yes it is definitely a
bug the reality is yeah I couldn't do a
talk in 2015 without back to the future
reference the reality is really
different from what I wrote once again I
didn't answer the question that I wanted
to answer i did answer another one and
we see that the mistake is in that code
you guys have any any idea where it is
yes I do the request synchronously yes
that's exactly the problem the problem
is here what i'm doing here is i'm
sending my request in a synchronous
fashion using a thread pool of one
thread obviously I could use a thread
pool of a 50 thread that would do the
same it I would have to send more
requests but I would have the same
problem the synchronous fashion here is
the culprit now what this means is that
when my request wilton will take 11
seconds it will push the subsequent
request after it will delay them it will
prevent them from being sent so my
throughput graph actually looks like
this
during that part here I didn't send any
query and yet since my my code was
written this way it was written to send
100 requests so it added 10 more
requests here in that part way after the
102nd mark the problem is when it comes
down to latency the bad I did completely
ignore the bad Layton sees in that part
of the graph and I replace those data
points with this one the ones in the
part where everything is fine everything
is awesome this is exactly the problem
of coordinated emission in statistics we
allow for some omission so if I have a
10 billion data points set I can remove
half of them and still be statistically
relevant as long as I remove them
randomly in that case those data points
were not remove the randomly they were
omitted in a coordinated way with an
even that happened on the server side I
did replace those events because my test
has a bug so what is coordinated
emission exactly well it's when I
execute my request in a synchronous
fashion and usually my latencies are way
under my interval between my threads but
somehow at some point I have outliers
that are above above my my interval and
the effect is that it pushes the
subsequent requests after the bad period
so I told you still I had a 99th
percentile so I must have answered a
question what question did I answer what
does my 99th percentile well it
represents the 99th percentile of the
request that went well not the 99th
percentile of all the requests and
that's my mistake
now is coordinated emission a frequent
problem unfortunately yes and that's the
long version of yes if you use jmeter HP
loadrunner any any spec benchmark or why
CSB you are going to have coordinated
coordinated omission issues now if you
use Gatling who uses getting here not a
lot of people so gattling sends its
request in an asynchronous fashion so it
doesn't suffer from coordinated emission
that's actually a good point so how can
we mitigate that that that problem well
we need to test the tester that's easy
right you unplug the system in the test
and you replace it with a stub that is
going to do nothing now what could
possibly be my latencies if I'm veg
marking a system that doesn't do
anything it must be zero average 90s
99th percentile maximum response time
all of them must be equal to zero if
they are not well I have a problem I
have a problem in my tester and also
once you are beyond that point you can
also do the controls e6 stop test where
you freeze every single perimeter of
your benchmark you state your assumption
before actually running the benchmark
and then you check the results this is
actually an interesting exercise because
you stake your assumption and if you
happen to be wrong well you have made a
mistake but that's not problem because
you gain to understand more the system
you get to understand more the system
now how can we fix it at the test level
well it's actually quite simple we need
to measure the latency but not starting
from the moment the request was sent if
we should measure it starting from the
moment the request should have been sent
so in that case my 50s request should
have been sent at the 52nd and if I use
that as my starting point
I will immediately see that the effect
of coordinated emission in my results I
we know that I have a bug in my tester
there is another fix I can use HDR
histogram provided i know the interval
between my samples and high up haha i
have two two methods here that I can use
so to summarize I hope you enjoyed being
here I hope you enjoyed making mistakes
I hope you learn something please
remember that making mistake is a normal
part of operations it's it's a normal
part of our daily jobs recall that every
single physicist out there keeps on
making mistakes again and again and
again our theories are proven wrong
every day and this is how they make
forward progress why should it be
different for us for yet for us it
shouldn't we shouldn't be ashamed of our
mistakes and also when you publish a
benchmark don't forget that your results
are contextual if you remove the context
out of your benchmark well you might as
well prove that the United States
spending on space and technology
directly correct color correlates with
the number of suicide by strangulations
and you would be correct because
obviously that's true so keep an open
mind enjoy the front of benchmarking and
thank you for your attention I think we
have 10 minutes for questions or 10
minutes of very old music depending on
what you prefer oh and it can accelerate
the sandwich deliveries yes as well
yes do I see any value of using jmh to
the Java Michael benchmark harness to
test higher bigger systems than just
data structures no I don't think so the
thing is jmh has been designed to fool
hot spot so that micro benchmarking
becomes interesting and easy to do and
they have succeeded in that now
benchmarking a bigger system well you
would need to understand the threading
model behind Jay image and eventually
what you will do is I Oh operations we
couldn't be dead code eliminated by hot
spot because we don't know what what
happens right so using jmh for bigger
benchmarks I don't think that would be
useful I don't think that would be a
good idea yes and with larger benchmarks
you don't want to fool hotspot yes
absolutely good point so I didn't touch
J imagine that in that top because
that's a really interesting topic but i
would need one more hour for those who
don't know it it's what they use in the
oracle performance team to to improve
the internal classes design we can use
that in our applications to check if our
algorithm are really well written but it
might be a primitive optimization you
might want to pay attention to this and
using jmh means you are interested in
algorithmic issues but what if you
actually perform a lot of database
queries and it's completely dominating
the cost of your application so you're
not even that
rhythmic level you could spend two weeks
optimizing a data structure and you
would produce something that is
interesting except the results are not
visible because you end up always
calling the database or you end up
dominated by the GC itself for those two
elements they are way before the the
algorithmic issues in terms of cost yes
can you make a comment on getting yes of
course so the coordinated omission is a
result of back pressure from from the
server to yeah ok so when you controls
edge your benchmark which which is a
great way of testing for this um then
what your sect effectively simulating is
back pressure from the server and
gattling is not immune to that effect
also so you can actually get you can
also get the back pressure affecting how
gattling affects it right so there's
really two sources in this case right I
think for this particular example what
you're looking looking at is like a
fire-and-forget scenario if you have a
more complicated scenario that if to
simulate then of course you can have the
intermediate interactions be stateful
which means that gattling has to wait
for response so any any of the load test
harnesses will do that so I think in
Apache we actually have no coordinated
admission such no a plug-in being put in
so that it can detect it and report on
it well that's pretty sad you you you
are really pessimistic well I'm going to
end up like this and the other question
okay so I think it's time to turn the
music on thank you everyone for your
attention and enjoy the rest of the
conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>