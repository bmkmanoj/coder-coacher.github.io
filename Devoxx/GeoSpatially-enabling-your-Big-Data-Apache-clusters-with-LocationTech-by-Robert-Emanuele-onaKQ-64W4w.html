<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GeoSpatially enabling your Big Data Apache clusters with LocationTech by Robert Emanuele | Coder Coacher - Coaching Coders</title><meta content="GeoSpatially enabling your Big Data Apache clusters with LocationTech by Robert Emanuele - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GeoSpatially enabling your Big Data Apache clusters with LocationTech by Robert Emanuele</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/onaKQ-64W4w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name's Rob manual VP research at a
company called xavia based out of Philly
yeah 65 developers or 65 people a bunch
of developers we have some data
analytics people you know project
managers etc all based around doing
geospatial software some products some
professional development included in one
of our efforts is a project called geo
trois which is an open source geospatial
processing library which I'll talk about
later and which I'm the maintainer of so
what we'll cover today go over like what
what is processing geospatial data at
scale mean to talk about like sort of
the geospatial problem talk a little bit
about what location Tech is and then how
some of the projects and locations tech
use space-filling curves to overcome
some geospatial challenges I then also
give an overview of three location X
projects including geo troubles that
geospatially enable Apache projects yeah
so processing geospatial data at scale
like I like to kind of take this phrase
and then chop it up and kind of you know
dive into it a little bit buzz-worthy so
what is this a geospatial data let's
talk about that first geospatial data
consists mainly I mean it'sit's data
that has a location right so if you have
any sort of data it has a location
that's geospatial data we mainly chop it
up into two different categories raster
data or vector data alright so looking
at raster data first what is raster data
it's a you know matrix of cells they can
be any values right here this is like
just yellow and blue and so the
geospatial aspect of it is when we take
this and we put it over a location and
now we can you start to use those values
to say okay well Philadelphia is on the
seat Coast blue and then kind of in it's
mostly yellow everywhere else another
example of raster data is satellite
imagery where satellite imagery can have
like three bands
and that can combine until I can RGB
image here this is a Landsat image that
actually has 11 different sensors that
has 11 different bands for each team so
there's multiband raster data such a
saddle imagery and then something that's
like worldwide but of course a
resolution is you can describe like
temperature data right so you kind of
describing a sort of continuous value
over a surface to some estimation by
like just bending it into some
resolution for vector data vectors
David's like point line polygon geometry
stuff so if I do a google search and say
hey where are bars around here I'm going
to get a bunch of point data so it's not
just the the location like we have a
sort of a lat long corrects y location
but it's also the metadata that's
associated with those points I can also
view lines right so here we have road
network in North America summer red
summer blue I'm not sure what that means
but that's the metadata associated with
those geometries and then polygons right
so just describing regions in Germany
and then so you can combine all types of
vector data into these interesting data
sets that can represent could really map
out the world in a pretty detailed way
so this is it this is a rendering of
openstreetmap openstreetmap spot where
you can go there you can look up your
address find your road and then make
edits to it if you find something off
like it's a Wikipedia of maps and it's a
lot of places in the world it's the most
detailed map better than google maps
it's a really interesting project so I
check it out all right so that's a
little bit about geospatial data right
so what is processing geospatial did it
data mean is it it's just if we just
think it's like a lat long coordinates
we're just processing doubles that's not
you know unique but actually there's a
lot of unique challenges with processing
geospatial data for instance this is a
really easy problem to solve just with
your brain right you're like okay the
triangle contains the point the hexagon
doesn't contain the point but if we take
a minute and just try to think of a
generic
algorithm that does this for any polygon
and point it's actually really
non-trivial right there's like if you
know computational geometry is just like
oh you just do yada yada but it's it's
it's a yes you have to kind of work in a
space that's kind of unique to the
two-dimensional plane and then if you do
things like include multi polygons and
multi points and so okay give me all of
these points that are contained by the
multi polygons that is India you can see
that it quickly gets pretty complicated
raster operations are sort of a little
bit simpler which is great but they have
a lot more a lot more data what is the
what is the term that uses raster is
faster but vector is more corrector I
think it's faster because it's just like
a straight-up operation okay to take the
one so take the other cells combine them
I'm done so here's a local operation was
called a local operation of map algebra
saying to take each cell you know and
add them together and here I'm saying
blue and yellow equals green all right
so I can produce other do
transformations and combine raster data
into other recipe and what you can do
with that where it's useful is doing
things like suitability maps so if I
have a bunch of different layers and
then I wait them according to some model
I can sort of dynamically see on a map
okay I'm going to paint reasons that are
highly suitable based on my model a
certain color and then and then you know
reasons that are not suitable for
whatever my model is modeling another
color and another type of raster
operating as a focal operation this kind
of if we're working with imagery is like
you know convolution with a colonel but
we in geospatial call things like focal
operations we're taking some like for
this for instance this is taking a sum
over a 3 by 3 neighborhood of the pixel
and what you can do with that is things
like hillshade so if i have an elevation
r after that just says for this cell
it's at a specific resolution that i
think this is 30 meter by 30 meter for
this cell this is where the cell is in
elevation and then so if i take the if i
measure the slope
with a focal operation and the aspect
and then I say okay the Sun is like over
here I can actually render a shading of
the terrain that allows you to visualize
it and we'll see a little bit more of
social shade later then you can do
things with like point2 rather or like
vector to raster operations taking a
kernel density to sort of blur the
values of point data into a raster so
you can create things like heat maps I
think we all kind of like seen this
before and kind of into it we know with
this with this means and then like
polygonum summary of suitability maps
right so we have a suitability map it's
a bunch of layers x weights combined and
then we take a polygonum area and then
we do some summary score over it right
so kind of building up these basic
operations into more complex like
geospatial operations and then one
really cool one that we're working on is
a via is doing feature extraction or
image segmentation so taking like
imagery and then running tension flow
models to train on you know classifying
the data into various each pixel in
tavares classes and then what we can do
is just extract vector data out of the
regions and say okay here building
footprint sir here's where tree canopy
exists so that's pressing geospatial
data at scale so what's what's our sorry
Preston to geospatial data so geospatial
data at scale what's the story there you
know what scale we talked about so
vector data gets pretty big so that
openstreetmap XML for the whole entire
world is like seven hundred fifty gigs
mapops did a blog post a while ago about
three years of geotech tweet so it's 3
terabyte raster data gets really big
especially when you talk about satellite
imagery so Landsat 8 is a well the
Landsat program is a program run by NASA
and the USGS it's been running since the
70s the latest iteration is Landsat 8
and it takes 30 meter by 30 meter
resolution images of the world all the
time it's just collecting data up there
and that data is you know free for the
public you use and so like I said
collected 11 bands
each each each scene it's about two gigs
let's say so the back of the back of the
envelope calculations so I want to
specifically talk about Lance out on AWS
so there's an s3 bucket that has Landsat
8 from 2015 and beyond that selects
older scenes and I counted a couple days
ago and it has over a million scenes so
that's a lot of dat about how much how
much data is that it's about two
petabytes of imagery data just sitting
on AWS available to the public for
create which is awesome on on AWS part
so to try to visualize that much data i
like to use an analogy right so cell
phones not everybody has them but
they're very common this particular one
besides catching fire it's not nice to
just kick samsung anymore it's like
they've had it so rough but this one is
that cell phone I holds about 64
gigabytes and if we dedicated solely to
holding land set a TEENs it would hold
roughly 32 lanta 18th right and so I
used to say this is how many people it
would take to hold this this is actually
no longer true because the the amount of
imagery available is just with growing
and ground so when it was about a
petabyte I was like there's many people
which holds 20,000 about 25 people is a
wells fargo center where the
Philadelphia Flyers play right so a
stadium packed full of people with cell
phones that have roughly 32 Landsat 8
scenes on their phone right so that's
the analogy they're going to come back
so keep those flyers fans in mind all
right so that's about the scale that
we're kind of talk about so processing
data at scale is you know a whole topic
that's been that's been researched and a
lot of code or out there so I'll talk a
little bit about like the generic
problem of processing data scale and
then like how we piggyback on that to
make a geospatial specific right so the
apache software foundation has plenty of
really awesome projects about doing
stuff at scale i'm going to talk
about two of them today spark and Hakeem
you lo start with spark so practice
product is a distributed computation
engine it's an API that lets you work
with distributed data as if sort of as
if it were a collection which i think is
one of the really core value things of
spark you kind of treat it as a
collection you can do functional
transformations to that collection and
that builds up a directed acyclic graph
of operations and then you sort of send
that to the or you execute that as with
the distributed computation engine and
it takes care of distributing all the
work out so your cluster and doing all
of that all of that heavy lifting of
distributed computation it's written in
Scala but it has language bindings in a
number of different other languages so
the where I like the you know uses
analogy again is like spark would be if
you if you wanted to count all of the
images that everybody had on their
phones right so a wasn't exactly 32 each
you might like go to the Senate village
and say hey flyers fans how many images
do you have on your phones and some
people might shout back numbers at you
you're just like what are you talking
about and some people might be like
bring us a t-shirt gun but so spark is
sort of the the person at the at the
stadium that works from Stadium says
like okay why don't you tell me what
your problem is how you how you like
what you want done and then I have a
number of like sub workers that are in
each section and I'll tell them what to
do they'll speak to their section will
say hey can you just tell me how many
you know images you have and then i'll
tell you the count and then i'll give it
back to you okay thanks for making my
life easy right so that's that's sort of
what spark does in this in this analogy
all right so cumulus another Apache
project it's a big table clone so it's a
column or database no sequel database
its records are stored in HDFS which is
the Hadoop distributed file system where
data is charted across a cluster
and it stores it in Alexa graphically
sorted table index so sort of like this
one dimensional index sorted index of
values so it can look look up values
very quickly you can imagine it as like
a key value store where the key is
roughly a row and a column and then the
values and here's integers and then if
we wanted to we would have this table
sorted and split up along tablet servers
and then so the master kind of knows
where all the data lives when you make
requests to it you say give me like
Rosie through row f it knows how to
where to where to ask and grab that data
from so if I were to say okay I want
Landsat images from 2014 if if I just if
I just asked that I've spark Tamar could
be like alright that's cool but I'm
gonna have to ask every single person do
you have any 2014 images and it's going
to take a little bit but i'll get back
to you is we fine it's like okay well if
everything's stored in a key below
cumulus sort of the cod Sierra tables
with the seating chart as the flyers
fans come in and say okay let me see
your phone okay you have like these this
set of data once you sit here in this
section in this section now a spark can
talk to me say hey where are the people
that have 2014 like I've already sorted
people by that here just ask these three
sections and you can get the answer
really quick right lovely Nelly alright
so let's make that question geospatial
alright so let's instead of say by year
we say for some country like or North
America right so now I have this like
complex polygon that i want to say give
me all scenes that intersect with that
complex polygon well let's back to
square one it's like okay ask every
single person to do this intersection
operation on their scenes and give you
back data so that's not ideal and that's
where location that comes in projects at
locations back right so first I'm going
to talk a little bit about what location
tech is it's a working group inside of
the Eclipse Foundation
I for geospatial software right so the
clips has the number of working groups
some dedicated Science Center things and
where there's your spatial one to me is
a developer and maintainer one of these
projects it's sort of a list it's a list
of projects right it's a group of
projects that are doing similar work to
me in the open source world that I can
you know participate in their
development we can bounce ideas off of
each other sort of provides us a
community with which to sort of yet try
to try to advance the open source
geospatial in general together and then
so there's a lot of different types of
projects here they're all geospatial but
the ones there's three in particular
that I want to talk about today that
deal with geospatial are big big
geospatial data and that's Gia Charles
gwave and geo Mesa all very cleverly
named trellis wave Mesa so one thing
that sort of binds these three projects
together is that we all use
space-filling curves to to deal with
this specific problem okay how is that
seating chart get a range so that i can
ask geospatial questions and it
accumulate notes with hell I'm talking
about right what are spatial and curves
they are curves like this sort of these
recursively defined curves that fill up
the entire space they exist at different
resolutions so this is a Hilbert curve
right and we can recursively construct
this curve and so as you so mad I like
the space willikers is actually a
mathematical formulation and if you take
the limit of this recursive process to
infinity this curve will actually it's a
continuous curve that hits every real
value point in that that n dimensional
space right but for us we don't want all
of the points we just want to sort of
cut up our image into you know some like
grid of points that's of a fine enough
resolution that it works for us and then
run a elbert curve or
the order curve through it and what this
allows us to oh so yeah this is an
example of like if I cut my you know
world map into tiles you know you can
see that you could refer to say Brazil
as you know column 5 row six but also if
I'd number each one of these sort of
points in a line I could refer to it by
some other index that's actually just
one number so it doesn't have to just be
two-dimensional special encourage can go
to n dimensions this is example of a
three-dimensional space filling curve so
yes sort of in a formulation here right
we have some space this is a
three-dimensional space can be
two-dimensional can be n dimensional if
we partition that space into you know
equal area partitions we can take the
center of those partitions that sort of
call that the grid point and then we
draw a curve through each of those
points and now we have a one-dimensional
indexing of each of those areas that
refer back to you know the space is an s
right so what is this allow us to do it
allows us to do this sort of range
decomposition so here's you know to kind
of tie it back to the example like if
here's the space of the world right and
let's just say I said North America but
let's make it easy and just do a
bounding box and I say okay I want all
images that intersect with this bounding
box I can actually decompose that into a
series of one-dimensional ranges where
I'm actually asking okay i want index 74
75 92 min to 90 99 etc and now a key
mule can understand that I can actually
pose the question of multiple ranges to
Hakeem you know and it can just chunk
through them and give me back all of my
ranges right yeah and that's and it
doesn't just work with a key mule oh you
know any any sort of geospatial or
spatiotemporal right if I said give me
North America in 2014 if I have my space
in dec
by a three-dimensional space filling
curve I can just decompose those ranges
and now pose that question to a number
of different backends that deal deal
much better with the one dimensional
index range then some sort of like two
dimensional thing and it also allows me
to totally genera size the way that I
query back ends so that we can support
many back ends with this with this
method any question on spatial and
curves but don't think we're this is
this is fine to have interaction if if
anybody has questions just problems yeah
yeah so yeah so the question is like so
if we have some set and we want to add
to it yeah we would find we would find
what location it was and then you know
so save that with that index and so
really important part of spatial and
curves indexing versus like something
like our trees or there's there's other
spatial indexes that get built up those
are I don't know the actual dichotomy of
what to call them but like they're like
static indexes or they're like you have
to save an index right so it's like I
have a set of index data and I've built
up this index and then so when I save a
new piece of data I have to modify the
index to account for that space dhawan
curves are a mathematical construct so
they have some configuration right and I
say okay my configuration is that it's
this resolution and it's a Hilbert curve
and now I can mathematically compute any
point on this index so I don't need to
like store an index it just kind of
exists and then I say okay what is what
is you know the coverage here and that's
a variance decomposition is it's also
true for that it's just like okay I have
this bounding box just from the
configuration of the Hilbert curve tell
me what my ranges are and it'll give it
mm-hmm cool so I'm going to talk about
the three projects in a little more
detail so geo Mesa is a project that
it's it's sort of an umbrella for a lot
of really cool geospatial technology but
if I wanted to boil it down to the core
functionality I would say it's
geospatially enabling accumulo and
accessing it through geoserver geoserver
is a open source java server for serving
you know geospatial information on the
web right it's it implements a bunch of
what's called OGC standards for
forgetting for interacting with
geospatial data it's part of OS geo too
well well known well established a
server for the source stuff right and so
it implements the spatial and curve
stuff to actually be able to save off
vector data and query it very fast so
trillions of points etc any scale of
vector data a little video of like a UI
being served by geo mesa right we can
filter points on polygon and then also
by attribute they also have python
bindings so you can interact with your
geospatial data in the troop your
notebook do things like kernel density
sort of heat map generation off your
vector data and then also dealing with
spatiotemporal vector data like here's
point data for like taxi locations that
we can visualize over time and here's
another heat lap example another cool
feature of geo mesa is that it works
with sparks equal Sparsit equal is
sparks engine for allowing you to post
sequel queries to your distributed data
sets it's a really great project and
yeah if if you're dealing with large
data in general and want to pose a
sequel query to a check
out sparks equalist it's a really
advanced like awesome project as first
for a sequel query like okay select the
tweets where the tweet user ID equals a
user user ID smart sequin can already
handle that but where the Geo Mesa comes
in is this bounding box right so now
we're now we're dealing with geospatial
location we're saying okay give me this
thing but inside of some geospatial area
without geo Mesa sparse equal can't
handle it female can't handle that so
this is what its enabling right G wave
is a technocratic I'll talk about so
this is a pretty cool graphic generated
by geo wave it's based off a GPS track
data so this is actual point data that
if you do if you have enough of it and
you do a sort of a heat map on it you
can actually extract the road networks
from the data from just point data right
so this is points over time it's
collapsed into you know where are people
most of the time and you start to see
this emerging of you know where people
actually go this is great and then so
here wave again like sort of in a
nutshell as the core functionality can
be expressed as geospatially enabling q
lo and accessing a crew chief server
sounds very similar to what I just said
right which is true their their core
purpose overlaps pretty significantly
there are differences though they were
sort of they were sort of developed
concurrently one by a company called
CCRI and one by radiant blue and booz
allen hamilton in conjunction with the
NGA one is in scala ones in java right
but they also use two different space
filling curve index types and g wave is
built as an n-dimensional indexing
mechanism so where do Mesa only handles
2d and 3d data which takes care a lot of
your use case especially the geospatial
gos biltmore is like a generic and
dimensional indexing engine for a
cumulus and also there is a no
support for Cassandra HBase and a couple
other backend for geo Mesa and then
Kimya Lopez put a lot of sorry G wave
has been a lot of effort into its HBase
support all right geo trolls now I get
to talk about my favorite one because
that's the one that I that I you know
maintain and lead up development for and
you can tell by the how much I talk
about it okay so Gio Charles is a
scholar library for doing geospatial
operations and working with geospatial
data types so if you're doing anything
Scala and geospatial it probably has
some functionality that you'd be
interested in and then with that
functionality enables spark to work with
geospatial data so working with apache
spark over various backends to do geo
spatial processing with a focus on
raster data so raster we sort of come
from the raster world we do have
capabilities and data types for vector
especially in the core stuff and also in
spark and a couple other geospatial data
types but we mainly focus on unwrapped
ER this is a big ugly slide about like
all the things you kind of can't you
know some up about geo stress right so
we have geospatial plus kala and then so
raster is point cloud vector and vector
tiles which are a couple different
geospatial data types plus spark over we
work with Cassandra HBase s3 honey boo
camilo and then also support vector work
with Jim a stingy with the boil it down
a little bit more to sort of like a
higher level I like to think about it as
like a library that can contribute to
any sort of scala geospatial application
working with raster data and scala and
then also doing large-scale raster
processing in apache spark and this that
dichotomy kind of is a useful breakdown
for sort of a design principle that we
that we adhere to or have kind of come
to and kind of crystallized that I think
might be useful in a more general case
as well
so our systems tend to have two
different processes like when you're
working with big data you have a bunch
of data that you want to do processes to
and then you have like these user
applications that you want to actually
use that data right because having the
data is not enough we need to build
applications that allow users to
interact of it so we find is we end up
writing a batch pre processing pipeline
which sort of put all of that you know
sort of the raw data and does et al or
transformations on it into some
intermediate format and then we build up
web servers that can read that format
and then do on the fly transformations
of it right so if we think about it as a
processing pipeline right where we're
going from a raw data serve data we have
raw data which could be like the Landsat
images or elevation data or some sort of
like geo tip that lives on some you know
ftp server somewhere and we somehow I
have to turn that into information say
like that statistical you know zonal
summary of something that's the pipeline
we can we can choose to build our
application to serve off of that you
know sort of on that spectrum of the
pipeline where if we say okay we want to
do a completely dynamic the application
can live just in total raw data and
that's really good for flexibility i can
write any sort of transformation on that
data but then if i do it at request time
it's going to be slow like i'll have to
like resample the data modify it into
like the web map tile thing painted as a
PNG and send it off and even like figure
out how to how to get at that raw data
so on the other end is if I do a
completely static I could just do all my
pre processing now it's exactly what the
application needs which is great you can
do that you can facet you can serve it
very quickly but there's a lot less
flexibility so for instance if I was
serving Lance that tile on a web map and
i just made a bunch of PNG's i can just
like shitt p + GS and just be like yeah
that's there's no processing there but
then if i said okay i want to color
correct that image a little bit i can't
do that because the PNG's are already
there
right I mean I guess you could do it off
of a PNG that might be a baby what if I
wanted to do like a suitability map like
I no longer can do that I'd have to
extract the values out of PNG it's just
like further that that painting of the
PNG is sorta at the end of our
processing chain so yeah let's less
flexibility so what we find is that we
have this mix of static and dynamic we
have this you know batch pre-processing
step where we take the data we do like
an in jest of it and then we write up
web servers that use sort of that you
know scholar gio cho of stuff to read it
in do perform some computations and then
serve it out right and then so usually
in our scholar applications we have
actually two sub project named ingest or
EPL and then server which represent the
two sides of this this spectrum and then
yeah you can kind of think of the two
sides of geo trails being like for the
dynamic per request processing we're
using sort of geo trellis with scala and
then from the static we're using it with
spark so an example of rendering
elevation in land classification with
geo charles and sparks sort of this
completely you know static batch
processing example this is so lame
classification it's like per pixel is
this road is this water is this you know
tree canopy right and then elevation is
like i said just the elevation of the
train so these numbers are old so taken
with a grain of salt but it's sort of a
measuring stick right basically running
running a hundred spot instances we have
400 cpus a terabyte and half of memory
with spot instances it's kind of nice
because it costs less than or less than
five dollars an hour so running this
process to generate just this nationwide
map of the combination of these two
rashers resident the this is a spark you
I as the job runs so the point of
showing this is the top part is a
representation that spark gives you of
the directed acyclic graph of operations
as they're sort of executing and you can
kind of see how complex it is right
there's a lot going on here with the
programming API I don't have to
right this I just say you know take this
reproject it to this projection kyla to
this layout yada yada yada it's really
kind of hit this complexities really
hidden via the geo trails from the spark
API and then down here this is showing
that there's a shuffle read here that's
like a bit a little bit over a terabyte
so that's like the data size that got to
all it was doing the processing job and
it pyramids it out and saves it all to
s3 and it took about you know one in a
third hour and what we get is this kind
of nice map of the combination of these
two data sets right so this is the
elevation that is uses that hillshade
that we talked about earlier but also
colors it be a color ramp so we can see
that the mountain I mean the color is
kind of bad here but we can see that the
mountain peaks are sort of a darker
color and then down into valleys the
lighter color as well as well as the
nlcd data that's pointing out what the
roads and the water right so we get to
like make this combination and if we
zoom out a little bit we see this is the
Blue Ridge Mountains and you can imagine
the level of detail that we just saw and
that being all over the place and then
you know at a national level there's a
lot of data to chunk through and yeah
that's that's sort of one of these sort
of static batch processing jobs that you
can write with Geo Charles to ignore
these missing tiles that was in the
source data that's not our fault so
examples of the dynamic per request
processing stuff this is a via
applications that use GRL servers so for
example we have like a flood modeling
thing that takes terrain data you know
you set a water depth and it kind of
shows you where the extent of flooding
will be it gives you some statistics
here these are all dynamically
calculated based off of not not the raw
raw elevation data but the ingested
elevation data that's index via is space
filling curve so that we can grab the
raw values but they're already sort of
like in these nice tile format that you
can query very quickly
and then here's an example of like
taking that nlcd data and then computing
some statistics about like you know how
much is this polygon open water versus
how much is a cultivated crops right so
this is the type of information that we
try to extract from raster data and we
can do that with geo trails and then so
this is a was a demo example of taking
climate forecasting data there is
there's these climate forecasting models
this one as TCS m4 and they predict
temperature max temperature min and
precipitation over a hundred year period
so this is saying in 2856 for different
RCP scenarios which are stand for like
different carbon emission scenarios like
RCP 85 is that we continue to emit
carbon you know at the rate that we are
RCP 45 is that we actually curb carbon
and then so there was just a interactive
thing that showed the the state with the
max average difference between those two
scenarios and you can see that it was
Iowa with almost 20 degrees Fahrenheit
right back out so we just covered these
three geospatial projects and at their
core they use some of the same open
source geospatial technology so core
libraries for vector data right jts is
actually a location another location X
project that's if you're doing a sort of
computational geometry stuff with Java
definitely check that out as far as like
that intersects do you know these
polygons intersect to do cropping and
stuff like that it's a great library
it's used by geo tools which is the main
library behind these two and then we use
it directly and wrap it into a scholar
friendly API and then for raster data
again G Masons you have used the
java-based geo tools and we wrote our
own core raster types to be very very
fast some current work that we're doing
there's a collaboration between Giotto
sangiovese on
using python bindings and being able to
work with the stuff through Jupiter
notebook a lot of days data science want
to just like play around with geospatial
data that comes from a large data set
but let me play with the in Python use
my numpy you know sci-fi stuff in
Jupiter so we're working on allowing
that that should come out in the summer
and then geo docker is an effort to dr.
eyes a lot of the components that we use
including all of these projects and then
the constituent project project so if
you're working on docker izing any sort
of apache technology we have some work
there that might be generally useful
even outside of the geospatial room and
then so if you're interested in any of
the stuff please please get involved we
have a mailing list propose them some
projects and with that let's go to lunch
thanks a lot yeah
which what is the function on that oh
yes yes we don't have a specific
function for that but that's that we
have the building blocks where you have
a raster you put a line over it and then
you rescue what you do is you take the
points of the lines or you you make very
easy then depending on the resolution
that you want you make additional points
on the line and then you it's a vector
to raster operation where you just line
it up and say okay what is the elevation
per point in line and then now you just
have that data and you can visualize it
they I'm sure they have that packaged up
nicely it's nice yeah yeah we were at
the library level so it's like you could
build something like that with geo chose
what's up I don't think anybody's done
elevation profile but honestly I don't
know all there's like companies that use
geo show us that are doing stuff that
they don't tell us about potentially
it's some that we talked about to
because there's there's there's a number
of like ways to do that like maps n has
a a pretty great elevation profile tool
that's sort of like fun to look at if
it's something that you wanted to do
like at scale I think that's where you
would probably want Gio Charles to come
in and be able to just you know do it
either a lot of lines all at once in a
batch job but yes it's like it's an
algorithm that I can like kind of right
in my head but it would be like you have
to go on the Gator channel and be like
how to do this
you do a scale we were receiving less
foot agility and we need to plan all
their things are going to be covered by
the accelerate coverage or the maximum
for coverage that crucified so we do
have a lot of stuff related to this
there are some tools and presently tools
that is that our way to Babel I can
write some softwares I broke software
that does rely on some third party yeah
yeah the product ID version no I think
there's a lot of room to grow as far as
like productized versions of jewish
special operations like this like as a
via has a product that we're developing
called raster foundry which is kind of
like that it's more focused on the the
imagery side of things aerial imagery
and satellite imagery and then taking a
lot of functionality that Jeter trials
provides and kind of product izing it
like being you know allowing users to
interact with it without having to code
there's a it's on the roadmap to develop
that for more than imagery so one of the
things that you know is on the list
somewhere that's probably a little
further down is doing few shet
operations on elevation data right so if
you're talking about like where you know
I have a cell tower here where can who
can see it right that yep so
totally so the line of sight like we
have you shed in geo trolls so people
that are developing applications can
utilize that but there's there's not
really like a push-button way and we're
actually currently develop developing a
demo application that will probably like
have up somewhere if that's you know
kind of like has a region of elevation
and you you know drop a point and it
shows you the the line of sight right so
a lot of that time like visualizing quit
it should be I mean it's going to be up
on github but if you give me your card
or something off I'll just notify you
when that's done yeah and then so a lot
of a lot of time it's just like
visualizing it is great and then but you
yeah yeah so actually extracting the
polygon and being like this is the
actual view should is more complicated
so it's like that's that's the sort of
stuff that you can build those pipelines
with geo trellis and if there's like
yeah that's I mean one of the things
that is a via kind of does is a
professional services company for that
side of it is you know when people come
to us with problems they we need to do
XYZ we have geo Charles to lean on to
build those types of applications and
then because it's open source a lot of
other companies also have that tool into
bill to lean on cool thanks any other
questions cool oh you got one yeah
we have tried there was like a research
project that we did maybe a couple years
ago what we found at the time is that
the overhead for pushing data like large
raster data in and out of GPUs was not
really worth it because we can go as
fast as we need to on the CPU that being
said you know GPU is a kind of
advancements then we do use GPUs for
like you know in using like tensorflow
or like other machine learning algorithm
so we kind of just push the data over
there let them deal with the GPUs and
then come back if there was if there was
some like it's still an open question
like are there are their libraries that
will allow us to like really effectively
lean on the GPU to translate our raster
data into a format that could be shipped
to the GPU do some processing and then
and then shipped back yeah cool well
thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>