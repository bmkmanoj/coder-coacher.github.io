<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning in Production by Adam Gibson | Coder Coacher - Coaching Coders</title><meta content="Deep Learning in Production by Adam Gibson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning in Production by Adam Gibson</b></h2><h5 class="post__date">2017-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/olJjvuPqsR4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">sky mine so world we're a little of an
outlier for this conference we're
actually focused on AI I usually
predominantly speak at big data
conferences so this is a little new for
me so if you have so if I if I go too
fast on any of these concepts I'm more
than glad to take questions or point you
at introductory resources one thing I
can't link to right now but I'll just
mention here I'm the co-author of a
Reilly book called deep learning a
practitioners approach which actually
targets Java developers in doing deep
learning so I created the package deep
learning Forge a deep warning for the
JVM and the book is based on that so you
don't need to know big data you don't
need to know Hadoop and spark do you
just want to jump in
we have introductory content for people
like you and so this is this is who this
is so that's that's that's why we called
the book deep learning a practitioners
approach so basically you're an AI you
know maybe maybe you're an m/l
practitioner and you're building
applications and you just you know you
you want to jump into this and you need
to know where to start
that's so that's that's who the book is
for so if you if you want any links to
any of that or whatever let me know
after the talk and so what we're gonna
cover today is GPUs and production so
GPUs and product so GPUs are basically
an alternative kind of chip that you use
for running numerical calculations so
I'll cover that throughout the talk so
starting off this is sky mind I live in
Japan our team is distributed globally
around the world we're 20 people we
co-founded in San Francisco in 2014 I
started deep warning for Jay in San
Francisco in 2013 and I moved abroad it
turns out turns out Asia loves us for
summer so we so most of our traffic
actually comes from China
third so because we have Chinese we have
we have documentation translated to
Mandarin Korean and Japanese and it
turns out it turns out that paid off so
I
focus a lot on our Asian market and
running our engineering team that's
distributed around Asia so so that's
that's this guy mind so what I'm gonna
do first is I'm going to define
production so production is something a
lot of people just kind of they don't
really think about it too much because
you know they just think oh well duh you
know you know production is whatever we
deploy to well it turns out there's
various kinds of deployment depending on
the company size so what I wanted so
there's startup production right so
startup production is where you have
cloud you're probably you're probably on
AWS you're probably not you you might be
you know if you're using any sort of big
data stack you're probably using elastic
MapReduce you're not using you're not
thinking about micro services you're not
thinking about you you might be thinking
about kubernetes or some of these you
know some of these newer Fingal
container things right but at the end of
the day you have very basic scaling
problems a start-up doesn't have a lot
of data that being said the startups
that actually do some sort of deep
learning usually are doing it in very
small data sets and they blog about it
and they say look how awesome we are but
what they what they don't but what they
don't tell you is they don't have any
users they don't have a lot of data and
they're there chances are they're
they're usually just the chances are
even the way they deploy to production
is kind of weird and so what you'll find
then is you know you have you know you
you have this Java conference you have
like pivotal light band and all these
other guys here and they're talking
about all these large-scale enterprise
problems that's who we target as well so
I'm hoping most of you guys are kind of
in the second space which is enterprise
production that's when you're focusing
on micro services the big data stack
continuous deployment with Jenkins good
and in things things involving that
probably having a Hadoop cluster a lot
of you probably use I'm just I'm just
gonna assume you guys use spring because
that's that's that's I saw the room
filled with when I know for the spring
talks I see everybody in the room so
that's probably what most of you use on
production so what I want to do is kind
of show you deep learning in the context
of how do I use it from a spring or even
just a play application right so I'm not
going to go into math there's no linear
algebra in this talk
this is actually an infrastructure
oriented talk so enterprise production
will be the predominant focus but what I
want to just kind of allude to is this
idea of research production so there's
not a big so this is Python land so
there's a huge emphasis on on a stack
called MPI which is kind of multi it's
but it's basically for GPU to GPU
coordination and it's how I distributed
systems we're done in the 80s if you
want if you want look up the field HPC
high-performance computing this is how
physics this is how physics labs are
line run all distributed systems codes
happen in C PO is happens in C++ and
this is this is what the MATLAB and
Python people interact with usually so
it's actually a very different stack
from what you probably run at your
company
um so research production is actually a
very different set of tools so that's so
now you'll kind of see why I want it why
I wanted to kind of define production
for you because it's your policy we run
D pointing in production and no one
defines what it means I hope this kind
of I hope this kind of alludes to a wide
variety of different implementations and
in different ways of doing D pointing in
production and even in the different
even in the content actually I should
mention this also even in the context of
GPUs right like it turns out GPUs are
not commonly deployed in large big data
clusters they can be and that's what I'm
going to show you that's what I'm going
to show you how to do today okay so one
other thing I want to mention is
research so you have various kinds of
research clusters so these are these so
that there's no there's no production
deployment happening here there's no
there's no focus on good software
engineering practices where you know you
know 100 developers can work on the same
product so they don't need Java they
don't need static typing they don't need
a lot of the best practices they just
need they just need something flexible
and quick that allows them to Express
mathematical expressions and so so that
that's that's when you're typically
going to use Python and numpy sometimes
you might use C code like if you have
you know sometimes people are doing a
lot of GPU systems research and it turns
out it turns out those folks usually use
C so there's a dialect of there's a
dialect of C called CUDA see that you
usually use for programming GPUs and and
so there's tons of libraries including
ours that rap these things so you don't
you guys don't need to touch C in order
to use a GPU um but but when you're
doing research if you need to write a
faster convolution algorithm you're
probably gonna end up having to do some
C code and so that's that's what you're
typically using from a higher-level
library but sometimes folks will
something sometimes folks will use C
when when they're when they're when
they're writing new neural networks so
then there's kind of then there's kind
of the the applications research you
know maybe maybe you're like maybe
you're maybe you're like a mid scale
startup like say Pinterest
Airbnb and you're doing some and you're
doing some sort of research and
development and and so you're probably
gonna be using AWS and you may be a
consumer of just say they're they're GPU
they're they're new GPU nodes so if
you're using those then you're spinning
up resources and you're shutting them
down and so that's that's typically
happening on the cloud and then this
should actually I just realized that
that this slide should have said on Prem
research or sorry yeah so on anyways so
just just know that there's another kind
of cluster where you're typically doing
other kinds of production which is like
HPC video transcoding so it turns out if
you're like dailymotion you actually use
GPUs to render to render and stream
video and so that's that's just an
alternative application that that you
might not be thinking of this is this is
also something that Netflix does so a
lot of a lot of companies if they're
doing video streaming infrastructure you
use GPUs to serve everything the reason
for this is because GPUs are really good
at at at processing images they're good
at shrinking and resizing them and
things like that on the fly and so here
that's so you're going to be rendering
things from the GPU
and then finally what we're gonna mainly
talk about today is Big Data hybrid
clusters so this is where you're talking
about resource schedulers meso sand yarn
so if you haven't seen a resource
scheduler before think of think of you
having ten nodes so ten ten ten ten
virtual machines virtual private servers
whatever terminology you may be used to
ten containers you just have a cluster
of ten nodes and you just want to view
that set of nodes as a pool of resources
like I and say so you want to run a
distributed job on ten nodes well how do
you do that and and how do you and how
do you do that how do you do that easily
you use a scheduler so what you do is
you say I want to run this distributed
job on three GPUs and eight CPUs using
256 gigs of ram and then what you do is
you schedule your job on on the cont you
put your job on the scheduler and it
says okay given my current pool of
resources do I have anything available
what it does it'll put the job you know
it'll it'll it'll basically it'll
basically run the job and distribute it
across your cluster and so so you so
this so that that actually is what's
going on underneath when you use when
you use the duper SPARC and you're
running some sort of a MapReduce job or
a count or anything like that what what
what's actually going on is Hadoop and
spark are actually just a client for a
distributed scheduler and so that
distributed scheduler is is what you use
when you're when you want to run large
scale like petabyte level data
processing and so that's so that's
that's what we're gonna see a little
more in depth here so when we're doing
distributed deep learning we're usually
using a scheduler and finally of note
when when you hear about when you hear
about GPUs being used in production one
of the coolest applications this is
actually self-driving cars so Nvidia
actually does so Nvidia is the dominant
GPU chip maker you know so they so they
have literally 90% of the market
AMD sort of competes with them but
there's just no comparison so Nvidia
Nvidia basically monopolizes the German
car makers so they have
I believe Elmo if most if not all the
German carmaker says customers doing
object recognition for things like
stopping your car detecting obstacles
and eventually doing self-driving cars
so this is actually happening actively
today so people are driving on roads
using GPUs and finally the one the one
we all wish we had Google and Facebook
scale clusters we have unlimited
resources and Anna limited budget and
good engine good engineer is teaching us
how to do everything you don't have the
scale and I don't either so we all use
Java here we are today all right so this
is this is this is this is a big this is
what I call Big Data hybrid cluster so
you have your standard JVM stack you
have Hadoop you have and what I mean and
when I say Hadoop I actually mean a
combination of HDFS and probably
zookeeper that I think those are the
only components of the Hadoop ecosystem
people mostly use nowadays so when you
think of Hadoop you're usually thinking
of distributed storage in combination
with distributed node node communication
which in this case is going to be
zookeeper so zookeeper is used for
consensus Hadoop file system is used for
basically storage right like so what you
do is you have Hadoop don't you have
Hadoop Damon's on each node and it it
knows it knows how to use the local disk
and then what you do what what HDFS does
is it allows you to view a whole like a
petabyte of data is just one single
unified filesystem across however many
node you want with built-in fault
tolerance and then you have tools like
like blink and spark and storm they know
how to access HDFS and run distributed
compute across your distributed file
system and so that's that's kind of
where that's kind of where your
large-scale business intelligence
applications happen so you're running so
you're running Hadoop as a service
you're running MapReduce as a service
and you're right you're running it
across however many nodes you need and
then underneath that is meso sir yarn so
what I'm showing here is meso s-- meso
s-- is kind of in the lead for GPU
provisioning so with meso s-- you can
actually use
be use as a resource so if you have a
maysa agent on each node it will
actually auto discover if you have the
kuda drivers installed properly it will
actually Auto discover where the GPUs
are and allow you to schedule it as a
resource and then spark
well then spark will then be able to use
GPUs as part of the job and so this is
active now
so spark 2.0 supports GPUs now with meso
s-- and so that's actually how that's
actually a lot of how we leverage and
train distributed deep warning on spark
is we typically typically use mesas for
that so what happens is is you define a
spark job you have you know you create
your uber jar so I'm assuming I'm
assuming you guys know maven so with
maven you use the maven shade plug-in
you create an uber jar and you create a
main class and that main class is what
controls what spark dies you send that
you then send that jar via spark submit
to your cluster and you say I want to
use a GPUs and 256 gigs of ram spark
then knows how to talk to my house and
get and run the compute on the allocated
resources from asos and so this is so
the this this is basically how you do
training right so so there's when you're
in production there's two kinds there's
two kinds of usage modes or of of GPUs
there's training which is building a
model so that's that's when you're built
that's when you're building your model
that's when you're training your model
that's when you're doing neural net
training and then there's inference so
inference is actually where things get
interesting for you guys because this is
probably where a spring or what a spring
or play or web developer is going to
touch D that's what that's where you're
going to touch machine intelligence
this is probably what name nine percent
of you are you are gonna interact with
somebody else
typically trains everything on your
cluster I'm aware that big data is kind
of an alternative set of even though
it's the same programming language it's
probably a different set of skills then
maybe you guys are used to and so what
so one thing I want to one thing so one
thing I want you two guys to realize is
you guys will probably be interacting
with a model using a model like
consuming a model as a POJO a plain old
Java
object so you're gonna embed it so
you're gonna declare it in your spring
configuration right so you're gonna put
an app beam you're going to instantiate
the model and then you're going to use
the model as an API and so we call that
inference and so when you deploy so when
you deploy a model what you're gonna do
then is you're gonna you're basically
gonna you're actually going to deploy
micro services and those micro services
are going to serve asically going to
take in requests via arrest and return
results like it's this is a dog this is
a cat the the housing the housing price
is going to be $32,000 whatever whatever
whatever decision you're trying to get
it to make and so when you're so when
you're so when you're thinking about
GPUs you can also use GPUs for inference
here so NVIDIA actually has a neat
little they have a neat little piece of
technology called tensor R T so tensor R
T is basically a way of running compute
like compute on GPUs it's with without
the overhead of of sending data sending
sending data back and forth between
between the CPU and the GPU and so why
do you need this special piece of
technology it's because the GPU itself
is actually a separate server it's
actually a separate server it has its
own memory space and it's meant for a
specialized set of applications it's
actually not a general it's it's kind of
a general-purpose computer but in
reality it's only meant for certain
kinds of applications so GPUs are
specialized for matrix computations and
this is why you need a special this is
actually why you need a special
programming language to run anything on
the GPU but but so in so anyways what
happens is is you send data from your
local CPU Ram to the GPU via PCIe bus
and so you have all you have all these
extra connectors on the GPU itself
that's connecting your kind of your CPU
to your GPU and when you and when you're
sending data back and forth about that
obviously encourage some overhead so
what you have to do if you want to take
advantage of the GPU
not to mention all this other stuff but
when you want to take advantage of the
GPU you actually have to know something
you have to actually know something
called how did you wait and see hiding
which means sending data to the GPU in
the back
ground while running compute in parallel
so what you do is you send what you're
doing is you asynchronously so
asynchronous there's that keyword again
you reactively send data to the GPU in
the background and you're you want to
run compute at the same time and the
goal is to always have ETL extract
transform load loading the data into the
GPU and having compute run in parallel
that's how you get the throughput of the
GPU where it has thousands of cores but
this oh but this but this bandwidth this
overhead of transfer data transfer so
again that's called latency hiding so
these frameworks take care of that for
you including including what including
what we do so when you're doing deep
learning you have a pipeline where
you're running data you're running
you're running you're running compute
from disk you're loading data into
memory you're loading main data into a
matrix you're sending it to the GPU and
running all your computations and you're
getting some sort of result out and then
so so what we do when we do training we
you we actually typically use something
called parallel wrapper parallel wrapper
is a piece of software that allows me to
use every single core in every single
GPU on my box on my node and in and what
we do in parallel wrapper is we actually
handle asynchronously loading the data
and running and running the neural
network in parallel using all your cores
and so that so that's that's so what we
do is we load data in the background we
send to the GPU and we just keep
everything running so one way so one
thing we can do is we can specify
something called a prefetch buffer that
will say given this much given this much
ram I'm gonna preload these many things
and into the GPU memory before my
computer actually runs and when you do
that you can get you can actually get
the throughput and so this is actually
what we do on SPARC it's the exact same
idea so when you run when you run a
MapReduce job or when you're running map
in reduce or even okay so think about
flux so flux is flux this a spring thing
Lagaan in scala also have this
functional paradigm of full like you
know like fold reduce map when you're
running all those computations that's
that's basically what you're doing on
SPARC so what we do is
we we we run map and when we run a map
operation
we're asynchronously loading all that
data into memory and running and running
all the training like that so we run so
we break up our data set into partitions
we run map and and map loads each load
loads a neural net load some data runs
you know runs runs runs an update like
when you're doing great you know you're
doing your gradient descent which I'll
show you in a bit
I actually have a live demo for you um
anyways just this just say for now that
your your neural that has an error and
it learns and when it learns what
happens is your your your pairing
basically a data partition with a set of
weights and you're running a matrix
multiply operation and that makes it
Ramon supply operation runs on the GPU
and so when you're doing training or
even when you're doing neural networks
at all you're loading data from RAM
you're running it through the GPU and
you're getting result out and then
you're using that result in your
application and so that's so that's what
happened that's what happens all the
time is when you're when you're when
you're thinking about latency or
training time you have a combination of
ETL and a combination of RAM in and all
sorts of crazy hardware configurations
that you need to think about when you're
when you're doing all this training so
the core of it is train the core the
core of any neural any deep learning
time that you're doing is going to be
doing is a summation of the training
time or sorry of you decompose the
training time down to ETL time and
matrix compute time and so so meso s--
and Mesa send all this up meso since
SPARC and all that or allows you to do
this on an arbitrary scale so that's
that's the whole point of using using
the software is you can say I want to
use a thousand GPUs for this job and
train and train a big neural net and so
why do you need to do this so a big
reason for this is if you if you have a
GPU up for a week you're spending thirty
can Amazon this is this this is to say
this is actually to save money so so for
example so what so why would you need to
wait a week that's because that's how
long it takes the Train imagenet on a
single node you have you have over a
million examples a million images it
takes it to train and early
from scratch and get good results on in
this case I thought more than a thousand
classes of objects various kinds of
animal species fruit you name it right
large data set it that takes more than a
week on a single node and so when you're
renting a GPU that gets expensive very
quickly this is what this is why people
tend to take pre train models and use
them instead of trying to train their
own because not only is it difficult to
train the model but it's also you know
you can also get expensive very quickly
and this is also why you need a
specialized chip for actually running
the compute running these neural net
operations and doing this training so
this is how we do it in D pointing for J
so this is kind of an overview of the
ecosystem itself so on the left there
you see you seen raw data and then you
see kind of you see what you see
basically what we call our distribution
our set of software for running running
and deploying neural networks so we have
D pointing for Jay but D pointing for J
itself is actually a set of a set of a
set of libraries for building D pointing
applications we have an ETL library
extract transform load library called
data vac that's when your that's that's
that's when your pre-processing data
that's when you're cleaning up your data
normalizing it and turning it into
vectors this is a process called
vectorization nd for J I actually ported
numpy to Java so numpy is a de facto
matrix package it looks kind of like
MATLAB if you've ever used MATLAB in
school or something it looks kind of
like MATLAB they have this concept of an
indie array and it turns out you use and
D arrays when you're doing neural when
you're using when you're when you're
building neural networks you you're
using you're basically using a bath
library a matrix library um but but why
do you need an ND array neural nets
themselves can actually be made up of
something called a tensor there's
actually you can actually have greater
than two dimensions on a matrix when you
have a greater than two dimensional
matrix you have something called a
tensor and so when you have it what
tensors are by default computationally
expensive so I'm not going to get too
much into the rabbit hole there if you
have any questions about tensors I can
talk about that later but you
been going we also have something called
arbiter arbiter is for basically
automatically tuning neural networks so
you define a rate you define a hyper
paren or space to search you run you run
the hyper parameter space and you say
this is get this this is the best
configuration so you can actually just
automatically generate a configuration
so this so again this this this is what
I want to say this is a set of software
for building D pointing applications we
also have things supporting deploying
deporting as micro services so you can
you can run everything as a play
application or spring application and
you can explode your model as a REST API
so when you it so when your so every so
if you if you if you haven't deployed a
machine learning model before think
about every time you hit Amazon or do a
google search and you hit submit you're
doing your you're sending a rest call to
some sort of an AI model that AI model
is running some sort of a matrix
computation and giving back the
likelihood that you that you like cats
or the likelihood that you want a
certain webpage so every time so every
time every time you're using some sort
of a webpage you're probably using an AI
based micro service so an example of
this is Amazon research is a big user of
D pointing 4j so they actually use us
for a lot of their product
recommendations so they they're
predicting consumer demand and so when
they do that they're using Java they're
deploying Java based micro services and
they're there they're figuring out what
you want to buy and that's in components
of that are using D pointing for J so
just just just as an example of if
you're a Java based machine learning
shop which there's a lot out there
you're probably my software is pellet my
software is probably powering some your
clicks and this all happens as
microservices written for the JVM and
then finally finally finally aside from
that is integrations with various kinds
of pieces of the JVM ecosystems such as
flink and spark and Kafka so we actually
so we actually have a library called
deal Forge a streaming and dillford a
streaming allows you to connect to Kafka
and and use cough go to do to to
basically serve data to your neural
network
so that so that's that's all in here I
could spend all day on this I'm only
going to mention one more so I'm only
going to mention one more thing is which
is at the bottom where you have where
you have cpus and gpus so so I I am poor
I also implement it so I also I also
write C code it turns out I also write
CUDA C code and so I wrote something
called Lib and D for J which basically
automatically connects to hardware
accelerated libraries such as Intel MKL
and NVIDIA SKU DNN so we connect to
hardware accelerated libraries and you
you can then use those from Java you
never see this we wrote all of it for
you and so this is so that here's an
example of our training UI so in this
case I don't know if you guys know the
emne Stata set at all but the end this
data set is basically 60,000 hand hand
written characters written from 0 to 9
and this is a neural net called called
the one at architecture Billiken net so
Lacan Yann Lacan invented this a neural
network he's the head of Facebook
research so he invented this a long time
ago anyways so this is this is his
convolutional net or architecture this
is a configuration and this is a data
scientist actually monitoring error
going down over time and so if you're
curious this is actually a play-based
web application so all the alt
everything I'm mentioning here is Apache
licensed by the way so you can use this
for whatever kinds of applications you
want ok and then this is this is this is
an example inference pipeline so in this
case we have the web application at the
top this is your REST API exposing your
model you have in the middle you have
you kind of have your train model that
that train model is connecting to a CPU
and a GPU and that CPU and GPU is is
running computations that a lot of the
neural nets to make decisions and then
you're connecting to a data source such
as such as logs or IOT and it's I mean
it and then data is being sent in to
Kafka you run a vectorization pipeline
so a vectorization pipeline is datavac
this is Rho this is where you're running
like an extract transform load job this
is where you're cleaning up the cleaning
up the raw data come on you know
normalizing it in a way and neuron that
can understand and then you're returning
a result and the
serving that to your web application so
one of the things I want - one of the
things I want to be clear about here is
you know you know ml and machine
learning like the math is actually
fairly learn about it takes about high
school math to know to understand the
basics of how to do this um so so a
great guy I want to call out in the
front row is James Weaver who who
actually learned so works at pivotal
talk to him about deep pointing for Jay
who actually learned machine learning
from scratch using using just using just
our content actually and so he learned
just enough and he he actually presented
he actually presented some very
impressive applications of neural
networks just using spring but-- so it
turns out this stuff is were noble so
talk to him about how he did it
I'm a library author so I have no clue
what you guys do just that being said
I'm happy to link you to resources
though so when you sow that application
I just mentioned is going to you is
gonna look something like this
so if you know how to do software
engineering that's actually in in
systems that's actually harder to do
than the math part you can actually
build a fairly basic application pretty
quickly deployment an infrastructure is
actually the hard part you guys actually
know how to do this better than the
people who do deep learning research so
Phil so I hope you'll feel good about
yourselves because that's like Java and
sanic typing is actually is actually a
big problem for these people then the
number one the number one cause them of
support on my we have a getter channel
has over 4,000 people in it it's a
getters a live chat the number one
question I get is what's maven you guys
know what maven is right so here's the
thing like most people don't even know
the people know the math don't know
build systems so I welcome anyone who
knows maven into my channel because I
want more math questions please alright
so so some things to think about when
you're running when you're running jobs
on a GPU cluster memory management so
GPUs themselves don't have very much RAM
they have on average 12 to 24 gigs of
RAM that's it
so we are running computationally
intensive jobs you
can't load very much into memory at once
this is why I'm saying you need 12 yeah
this is what I'm seeing you need
multiple GPUs because what you're gonna
do is you're gonna take your terabyte of
data or whatever you have on your
cluster and you're gonna shard the
problem into into multiple GPUs and then
you're going to run as many of your
partitions of your data in parallel as
possible and you're gonna maximize
whatever Ram you have in your GPU
throughput so throughput includes the
overall training pipeline itself what
time do I what time do I go to you by
the way 5 ok so oh um sorry oh so you
know what I'm gonna I'm gonna I'm gonna
take questions later this this went
longer and I thought it did so all I'm
gonna point out one thing um again we do
all this for you you guys cut you guys
kind of know how Java works and all that
we wrote just of note we wrote our own
GPU garbage collector so you guys don't
even know how you guys don't even have
to know how GPU memory management works
we actually handle that we actually
can't we actually think about these
things for you oh ok got it
ok ok cool ok well then I'll just I'll
just keep going then
ok so I'll try to reserve like at least
15 minutes for questions so the only
other thing I'll mention is from so from
so ink from CUDA from Java you can
actually manage your GPU and so what we
allow you to do is we actually allow you
to configure things like do you want
deep learning to use all your GPUs do
you like how much memory do you want it
to use it turns out GPUs also have a
neat l1 cache and you can configure how
much that cache to use and so we we
actually so one thing when unique thing
we did was we actually implemented
memory management for the 4ku DNN and
videos library and our own matrix
libraries and so so so that's that's
special you you normally how you got you
what you usually have to know how to
compile CUDA C or you normally have to
do do your own memory management with
with other with other libraries with us
you don't have to and so so this being
controllable from Java is novel because
all of a sudden this allows web
developers
to treat a CUDA environment as another
POJO as I know as another as another
thing you inject via dependency
injection so there's so there's this
there's some neat things you can do with
this you can actually use you could
expose this and say a rest api and
implement and actually instrument your
GPUs like this so you can actually
capture whatever the environment is and
then expose expose all your GPUs and
chips as just a simple JSON a simple
piece of JSON for you to use monitoring
software on so this actually allows you
to do some neat applications it's just
this is something to keep in mind we do
all this via something called Java CPP
Java CPP does memory management further
for Java we actually do are all we
actually do all of our memory management
off heap so we don't use the JVM heap at
all we do everything off heap for for
throughput purposes among other things
and so Java CPP also does a neat thing
of automatically generating Java native
interface code we never have to write
J&amp;amp;I bindings we just we just generate it
so we actually have a Java to C++
transpiler they then generates the Jay
and I headed and so oh this is this all
actually can happen as part of an en
clean package they will automatically it
will actually automatically generate the
native code necessary so this is also
how we package things like open CV so we
actually have something called Java CV
that allows you to run everything from
open CV in Java and we automatically
generate the j'ni bindings for that and
so what you get is hardware acceleration
on maven central which again is is kind
of a novel concept like a lot of people
haven't thought about how to do this so
for this is for data scientists if if
anybody's like ever played around to
Kharis it's kind of it's it's what a lot
of folks on Kaggle use for from running
deporting applications we actually
import models from the Python folks and
so what that allows us to do is that
allows you guys to just use our importer
and then you can use continue to consume
models not necessarily train them or
even or even or even have to escape to
Python for data developers you know you
define your pipeline via datavac you
might import your model or you might
define it in Java like your model and
then you deploy with spring but-- logon
plate whatever micro
first thing you want to use and then you
can run training and inference on the
GPU and then this flexibility then
allows for new scenario is like I
mentioned earlier with Kafka's like
Kafka streaming to something like tensor
RT which is specialized specialized
inference code for running running
compute on GPUs where you're going to
see it three to five times speed-up
running things in production so a yeast
gate a great use case of this there NASA
so the NASA Jet Propulsion Laboratory is
actually a big user deal for J they
converted from Google's library tensor
flow to us because they had a java-based
pipeline to deploy turns out that's what
most do you guys do so the Apache tikka
project is actually adopted D pointing
for J for parsing images and so why did
they do that that's because they had a
whole lab of students using Python with
with an Apache tikka based pipeline they
spent three months trying to they
actually spent three months trying to
them but trying to do Java to Python to
Java
and that failed miserably so the second
we had snapshots out for just doing
transfer learning they switched over to
us immediately because we have the
ecosystem into the JVM integrations the
integrations with the wrestling
ecosystem not just some like not just
some bindings but actual integrations so
so just this is just something this is
something I think you guys think about
more than data scientists do but if
you're developer that's that's who were
for that's why I meant this conference
so with that I will actually take
questions thank you
come on guys crickets really come on all
right all right if you guys if you got a
so all right oh right okay I might have
forgotten about that so the question was
explaining the look explaining how how
you do low latency with the GPU so so
this latency hiding thing I mentioned
was you basically you have date you have
some data you want to load in GPU so you
in another thread we load data in the
background and what we do is we we then
when we allocate an Indy array it
actually allocates memory on the CPU and
the GPU and then it loads data into the
GPU automatically and so when you're
loading data from disk and you call and
you create an ND array or a matrix it
will automatically allocate memory on
the GPU so as long as you do that and
another fret or in the background you
you can basically benefit from having
everything pre-allocated when you go to
when you're gonna need that that array
when you're gonna need that piece of
memory so that's that's so that's that's
what you're doing is you're you're
achieving compute with with with loading
the data at the same time a lot of GPUs
actually have multiple lanes multiple
PCIe buses for loading data into it so
what you're gonna do is you're gonna
you're gonna use something called CUDA
stream which will say stream this diva
to the GPU in the background while
running compute on this thread all right
okay yeah oh good okay
so we import so so the question was what
pre-trained models do we have we mainly
have examples in we're actually going to
be importing a lot of the pre-trained
models from the Python ecosystem like
kerosene cafe that have that I already
have those libraries that's what our
model import is for is to be able to
support those so if you have a pre
trained model on the internet somewhere
that you that you want to run we can
probably support it via model import
that being said we're also building your
own model zoo models do as well
oh I'd indeed I did all right so I had
the yeah sorry like my I I was ranting
and when it went a little over so let me
so I can um so that I can show you real
quick so so I'm just gonna rerun this
here
okay so this this is this is actually
our training you lion action so what
it's so so what we're doing is we
actually update the training in in real
time and so you'll see you'll this is
this is M nest so this is this is
actually the learning rate going down
this is actually the error going down so
this is the error of the neural net as
it's learnt as its learning from the
data so that this is this is an example
of the model architecture this is what I
was this is what I was showing you guys
so you can actually click and you can
actually inspect like you can actually
inspect like the individual parameters
and and and and the actual like updates
for individual layers here's here's the
system so this this is actually this
actually also covers things like the JVM
memory off PLL allocated memory off heap
so this is what I was telling you is you
can actually track all your system
information via our UI so when you're
running training you're gonna you're
gonna want to instrument it with
something to monitor and see is my
neural net learning and how much memory
is it taking up and so you're at suit so
you can use the UI in real time as a
neural net as a neural that's kind of
learning so yeah so so here's here's a
model later on so you'll see the error
converged so this was gradient descent
like I mentioned so you'll you'll see
the error kind of is kind of it becomes
static very quickly
oh so the y-axis is actually the error
and the X is the iteration number so the
error so that's the objective function
so the neural network has an air like a
an error like these are the number I got
right and then this this is kind of the
error over time so this is your loss
over time and then so loss an iteration
number right so it's real it's relative
so the problem the problem with the
problem we're trying to set an absolute
magnitude for the error is its relative
to the batch size like the number of
data points you're looking at it once so
because it and the loss function so we
usually set it relatively simple it and
figure out what the what the bounds are
and then we dynamically update the
bounds as necessary this is all d
freebase so this is all gonna be dynamic
okay all right
okay good to go yeah yeah this is all
running on the yeah so this is all
running on the GPO so one thing with one
thing I've dealt with in d4 J is you you
just have to include something called an
NP 4G backend and you say nd 4j CUDA and
it automatically runs everything in the
GPU no code changes it's basically
hardware as a jar file okay
no so the only thing you have to know
the only thing enough to have installed
is if you're using the GPU as CUDA and
then we do the rest
so you install CUDA as you normally
would and then you run you basically
just run everything from everything
after that you run everything in Java so
no compilation required no no compiled
from source okay all right all right
thanks folks
good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>