<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Terracotta Ehcache: simpler, faster, distributed by Anthony Dahanne | Coder Coacher - Coaching Coders</title><meta content="Terracotta Ehcache: simpler, faster, distributed by Anthony Dahanne - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Terracotta Ehcache: simpler, faster, distributed by Anthony Dahanne</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-j6cNZc5wYM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello everybody welcome to this
session about Korea's cash but his name
taquerias cash simpler faster and
distributed for an hour
my name is Antoni de honey and i will
walk you through this presentation so
first let me introduce myself
Antonia honey so for engineer walking at
Quora also for Ag company we were
actually bought something like five
years ago and before that Cora also
acquired each cash but when I see this
dream complementation as well I am
currently working on with a current
management console and its integration
into our products so I got a good
overview of what our products at a
Corral are doing how we are performing
and one of interesting metrics together
from our products I'm also a stronger
supporter and actually I'm also using
docker for his presentation at the end
for villas demos but oh yeah sorry
I was about to forget come visit us at
booth number one and there is no
curative reef to win and also some other
goodies so and of course if you have any
questions about this presentation we are
here to help so what about the agenda
for this session
well first caching 101 so in this first
section I won't talk about basic caching
concepts what is important to understand
and where you can find caching in the
world of computing then we'll move on to
the specific cases of caching on the JVM
first we'll go through a little bit of
history and when we don't see what is
the specification and what is the
recommended implementation of course
each cache and then we'll move
- clustered caching with terracotta
server so I will introduce the concept
of cluster of cluster cache
we've attack our server I will end this
presentation with deployment examples
and by diplomat examples I mean
real-life diplomatic examples so was
kind of the deployment that we have at
customer site
okay so without further ado caching 101
so what's the cache so let's begin with
vacker definition according to
Wiktionary dot-com a cache is a store of
things that will be required in the
future and can be retrieved rapidly now
if you ask a terracotta engineer worries
a cache usually it will he or she will
answer that a cache is like a map or
even maybe a temporary map by map I mean
in Javas whichever sense of the word key
value mappings with capacity control via
eviction and freshness control via
expiry was our two most important
features of a cache so where is caching
use why should you
what should you worry about caching
information well first of all at the
most basic I mean I had at the most core
location in in the computing world CPU
so CPU we're using themselves caches so
when you CPU is trying to get some data
from the register and it's a myth so if
we couldn't find vedera bandits going to
try to load it from VL 1 cache because
via one cache contains what the register
as plus many other things and then if
here one cache doesn't contain his data
then it will fall back to be able to
cache and then to the RAM and as you can
see both sketchies
bigger and bigger but the thing is that
they're also slower and slower so
usually if you want some good
performance you want your data to be
very close to the CPU so because even
after of a memory after the RAM where's
the disk and maybe the network and so on
so caching is used in the most basic
architecture of hardware before going
further I would also like to remind you
with latencies but you could remember so
an l1 cache reference is halfa
nanosecond so we see the latency to hit
it and then for l2 cache reference 7
nanoseconds it's already 14 times slower
than an l1 cache then it goes on and on
and on for example main memory reference
even 20 20 times slower and we have to
cache when reading 1 megabyte from
memory it's 1000 1000 times slower and
then you can see that with a disk and of
course when it works it gets even worse
so of course you want your data to be
close to your application to your CPU
caching is also used in other locations
so let's take the example of a classical
example of someone who is loading a web
application from his browser so a
browser will send a request to probably
content delivery network so I see the N
so hopefully the CD and one of the CD
and endpoints is close to to his home so
it's going to be faster when we CDN is
gonna hit a web application server this
web application server is most probably
hosting your web applications so this is
how way well at least an application and
I represented it with a little block
this block this application we all
certainly need some information from a
system of records such as a database or
even from web service
so as you can see in on this chain while
there are many Network calls and a very
dramatic need for cashing and actually
this is already the case so maybe you
already familiar with HTTP caching so
you know when you send when you send a
request and get a response in the
headers your browser is instructed to
casual response for example for static
firing such as CSS JavaScript HTML and
others we see the end also is is acting
as a cache because it contains a copy of
the page from from your web application
and you also contains copies of your
static resources of course the CPU is
caching is caching data as we just saw
earlier on and there's also the notion
of disk caching on on a database on or
some over room system of Records finally
very is caching at the application level
and this is what I'm going to focus on
during this presentation let's move on
with some caching theory let's talk
about the others low so it's a little
bit of mathematic as you can see this
equation is describing a ferry to call
speed up but is always limited by the
part of a task but cannot benefit from
the improvement so well what this
basically mean if you look at the
example at the bottom of a slide is when
for example if you have an operation
that needs two tasks to be executed for
example tasks a in blue and task B in
red if you're getting the result just
after the execution of those two tasks
you will of course try to make them more
performant the thing that you need to
remember is for example if you're trying
to make B but is whereas smaller latency
you're trying to make be faster you
won't have a huge gain overall whereas
if you try to make faster operation
Avery's
taking much longer in that case you will
get a much better a much better result a
much much more reduced latency also a
long time so for example if we talk
about a website that is allowing you to
do searches on and then ice cream
flavors do you think that do you think
that the number of searches is going to
look like this I mean do you think that
each flavor is going to be requested the
same amount of time well no actually in
real life some flavors are more popular
than others and usually you would rather
get a distribution of searches that
would rather look like this and if you
look closely you will notice that with
20% of your data set is requested 80% of
the time this is what we call also the
Perito low on the 80/20 rule and as you
can see the long tail is appearing when
you start looking at such distribution
so the interesting thing is when you're
caching well when you configure your
cache you should not try to think of
caching all the values from the data set
but just a subset but a very very often
accessed subset before going further
let's talk about a real bit of caching
glossary vocabulary a hit maybe you've
already heard me saying this world so
her heat is when the cache is returning
a value and typically a Miss is when a
cache does not have a value the
difference between cold and hot is that
when your cache is empty we can say that
it is cold and when it's full it's
actually hot
it's a heart that I said of course in
real life what you want to well you'll
usually want your cash to be full when
you start implementing cashing in your
application when you start adding cash
cash into your application and you want
to measure what is the impact of cashing
this is one of the first thing but you
want to make sure that is my cash being
used or not so of course the cash usage
is a very interesting metric to look at
so is your cash empty is it full
so is it corn is it hot this is an
interesting questions you know for
example if after an hour your cash is
still empty well maybe there's something
wrong with it
also you you will want to have a look at
the heat ratio the heat ratio is a ratio
between the number of hits so when a
vakif return the values over over mrs.
brass hits so it's the total excess of
the cash so of course a good heat ratio
is very important and when finally the
hit rate the hit rate is the number of
hits per second and this this can
account for the performance of your cash
so if your cash is delivering a good hit
rate then yes you implemented it
correctly when your cash is badly
configured you will have a slow hit rate
so pay attention to two verse metrics
okay let's move on to the second second
section of the presentation ready is
cashing on the JVM first I would like to
tell you a little bit of the history of
cashing on the JVM I've got three
timelines in this slide one for GSA 107
one for each cash and one for Takara so
GSA 107 is is also known as J cash
shavax that cash and this is the
specification for cashing on the Java
Virtual Machine
jessa 107 well mystery of json 107
begins in 2001 when there was a review
ballot when each cash and Cara came
along in 2003 two years later so it was
the first release of each cash 2003 and
eh cash is well it actually means easy
hibernate cash so as you can guess from
from its name it's pretty it was pretty
tightly coupled well not couple but Wow
almost it was very popular to use along
with hibernate of famous or aram on the
JVM so it's a caching library actually
so you don't need to use it even at this
time you didn't need to use it with a
hibernate
you know just a caching library you drop
it in your class path and when you can
start caching elements caching requests
and objects and requests to your or a
system of work on such as a database
also what about Cara it was also
released in 2003
so by takara we usually mean vitarka
receiver and kora
in 2003 was all about trusting your JVM
and in particular it actually meant
clustering Java memory model so think
for example of creating your objects and
putting and running using them in
vitarka receiver both objects could be
split across several JVMs so this is
what Cara Sarah was doing at this time
when six years later Terra Cotta Inc the
company we acquired each cache so of
course soon after there was the first
3ds of each cache but was able to
interact with Agora server so the cool
thing is that making mom making kora
compatible OVH cache
I mean letting users letting each cache
users connect to
Crysta using the API it was kind of
leveraging the user base of eh cache so
that was a stronger win for akhirat this
time to gain more users via the use of
each cache to access with a correct
cluster and of course it also began the
era of a crystal stone on eh cache and
then quickly after came along of EEP and
of course is going to it was going to
give birth to the afib stone in eh cache
so for those of you who are not familiar
with what we call of EEP and Vijay
VMworld
well you're probably familiar with unhip
so the hippies were all your
instantiated objects laying in your JVM
we think we've hung up is right it's
subject to garbage collection so
whenever you start having heap sizes of
more than more than several gigabytes
usually the garbage collection time can
be can be actually pretty long I mean
long like several seconds or even more
so on the heap access is not really
predictable because you never know
whenever a GC is going to come in and
maybe if a request is going to be
stopped
you know of a stop the worm during six
seconds and when sometimes it's not
really acceptable if you have SLA to
respect so the cool thing with of EEP is
that off EEP is actually directly
storing your objects on the RAM of your
machine so there is no garbage
collection of EEP but there is a huge
drawback also is that you need to sell
your lies your object before stirring
them as buy it directly off it so
summarizing actually means latency it
means processing so even if off EEP is
more predictable thanks to the absence
of garbage correction it's also a little
bit slower
well it's slower than on heap but still
thanks to of EEP people could start
using each cache to cache a lot of data
just using one JVM so this is what we
called
scale up well back to JSA 107 12 years
later so involves 12 years Tara has been
pushing for trying to get G trying to
get a first 3ds of JSA 107 and finally
it was it was coming to an end
almost with a preview review in 2014 and
just one year later the public release
of GSA 107 it was actually from vets
moment that Java as an official
specification for cashing the cool thing
with a specification for cashing on the
JVM is that one for example you you are
spring developer or hibernate developer
or J hipster or any other kind of
framework under JVM when you are walking
on all those frameworks you just need to
support the integration with GSA 107 and
immediately after all the implementers
of all the implementers of GSA 107 are
compatible with your framework so you
just need to write one integration and
then all the implementers of GSA 107 are
going to work out of the box and today
already spring from version 4.1 and also
hibernate from version 5.1 or 5.2
via super GSA 107 so for vehicles
developer so one for those developers
walking at spring and hibernate we don't
we just needed to write with support for
GSA 107 and since each cash free as our
of about GSA 107 support you can
immediately use each cash free we've
hibernate 5.2 or using each cash free
inside spring application so yeah each
cash free and Takara 5 released this
year six months ago polish cash free and
most difference between each cash two
and three what did we increment this
number was because we studied
almost from scratch we've a new design a
lot of cleanup of you know legacy code
that was accumulated during the year and
also new features and very good
performance as well for Toccara Toccara
Toccara 5 is the companion for the
cluster store for each cache a real a
bit more detail about each cache each
cash freebie each cache reboots 3 dot oh
it was the first official release in May
2016 May of this year of course the main
features were compatible with JSA 107
Jerrica that cache also the possibility
to use a user manage cash for those
times where you just want to use a cache
not for the wall life of the application
but just for a smaller just for a small
lapse of time during the life of your
application so you don't need to to
initialize the cache manager copiers and
stylize errs and each cache free is is
bringing you as some default ones but
you can also implement Josiah lasers and
cellulitis if you remember what I just
said a few slides ago are very important
whenever you start stirring your your
data on on stones where are not on
stones when a not on heap so for example
for of EEP disk and the crystal stone
you need to serialize your data right
because you're not on heap so sorry
Eliza we play an important part of the
performance of your application when
you're using each cache
of also yeesh cash free brought some
strong typing so maybe if you were
attending the presentation from Louie
jicama two days ago a experimentation
was a tuning action named eh cash out of
his element is because in eh cash to
that X you needed to wrap your key value
mappings into elements now we've got rid
of this and getting rid of this allows
us to provide the strong typing for key
and values then in the release in the
3.1 release over happened in June 2016
the clustered store was added so it's
our fourth stone right on EEP of EEP
disk clustered and also it was the first
release for vitarka receiver tech or a
server five so what's the use of a
clustered here when you already got on
EEP of a pen disk
well the closed dot T can be actually
used by several clients so imagine that
you've got your web application you're
using caching we were just local caching
and when you start scaling up your
application when inverse well in this
case while scaling out sorry so if you
start adding several instances of the
same application you want them to share
the same cache and the crystal tier can
be shared right because each application
is going to open the connection to
attack or a server and going to access
is going to share its key value mappings
with other instances and they will go
back on this later when 3.2 was out two
days ago and it's bringing a high of
ability with a car server so high of
ability is the support of passive
servers so and also what we call
replication so meaning that your
terracotta server won't act as a single
point of failure because it would have a
back-up plan so
a passive one or several passive so that
when the server crashes then the passive
will be up and when four clients are
going to connect to a passive so no
downtime not that time possible with a
higher ability caching pattern so there
are several caching patterns that you
can use in your application when you
start relying on a cache so the first
one is no caching at all all right it
sounds dumb well it's really really
important to remember that your
application should work without the
cache so if you start developing your
application with caching in mind wow
this is a pretty good idea but you need
to make sure at all time that when you
unplug the cache your application is
still working
of course maybe the performance in
production is going to be different
right
but during your testing you should make
sure that you're not making a session on
values on values being present in the
cache or not
why because remember what each cache
developers think of a cache a cache
should actually evict at any moment and
should be able to expire any values also
any key value mappings at any moment so
don't trust values being in a cache so
if you want to test use with caching
Parham no caching at all
unplug it and see if if it continues to
work when another cache in parallel one
of the most popular one is cache aside
so let's take the example of an
application so here I divided this
implication and freecell combs the first
one the business logic so this is your
this is your business code and this
business logic is interacting with a
cache and a system of record so when
you're using cash aside you will you
will code your business logic to
actually get key mappings from the cache
so get KK is your key if value is not
valve and the cash is going to return No
so that actually means that your
business logic needs to go and ask go
and ask the system of Records so the
system of record can be a database or a
remote web service any kind of element
that is preventing you with data because
we can see here but the system of record
contains the mapping key key V so every
business logic since he couldn't get the
value from the cash it's going to ask
the system of record so it's going to
load the value maybe just doing a select
in the database so load K and see if it
gets a value so yeah apparently value V
I was matching the key okay once your
business logic retrieve this value then
of course it needs to insert it into the
cache so that next time the subsequent
course will go on leaf of a cache right
so next time but the business logic is
asking for for a key key key k when it
won't get it directly from the cache
which is the most popular one now
another one is cash flow so the same
actors here we've got the business logic
the cash wisdom of record that contains
key mapping K V business logic is only
in cash flow only interacting with a
cash is not interacting with the system
of record our own so your business logic
is doing a get on the cash the cash
doesn't have this key value mapping yet
so it's going to load K from the system
of record since the system of record get
as a result is got V when the cash
receives this value is going to pair it
internally and then is going to return
the value to the business logic
unfortunately this operation cash flow
is not magic it actually means that you
need to implement a class
where is a it is a loader writer class
but where you will well you will explain
to your cache how it should interact
with your system of record so why is it
the case because you know no system
records are the same so it belongs to
you the developer to know how to
interact with your system of record
let's have an example of those two
caching patterns right now so let's talk
about the example application and this
is V ash cache demo if you are
interested in it you can actually get it
from from github it's open source and
you can have a look at the source code
so in this demo what we wanted to
demonstrate is of course the use of eh
cache in this demo we have a list of
actors and actually it's a database and
this database is loading a dump file
from IMDB from the internet movie
database so we have a list of actors and
for each actor you can click on the
button on the right and the button on
the right is going to give you is going
to give you a weather report for several
cities on the date of birth of this
actor okay so if I choose for example
this actor when the application is going
to retrieve information about one is
going to retrieve weather reports for
this actor so it was born on September
12 in Copenhagen Denmark so as you can
see we've achieved several weather
reports for the date of birth of his
actor and also we have provided you with
a little performance analysis of a
resources that were involved in this in
the creation of this page and as you can
see the application needed to do several
web services con right so some calls to
the google bucharest api song call to
the dark sky weather rest api and so on
as you can see where something like tens
of them
and if you add them up it took two
seconds just getting the necessary data
to generate this page when is not when
you are not using caching whenever
you're going to reload this page well
you're going to pay the price one point
five seconds this time and there was no
cash involved and as you can see mom
it's pretty long so let's fix this let's
make this application a little bit more
performant so this application of
service application is based on spring
boot and actually more precisely it's
relying on je hipster for web
application generator so here in this
piece of code I am adding an annotation
at cache result and I'm providing it
with a cache name weather reports and
vermut the method that is being
annotated with AD cache result is the
retrieve weather report so thanks to
this annotation I'm going to cache the
reasons of of all the invocations of
this obvious method provided that well
and of course is going to make
performance better provided by the user
is loading is loading is calling this
method with the same keys with some
arguments and he have a key the
combination of location and local date
date
and what does does this annotation do in
detail well as you can see according to
the Java doc this annotation is the
perfect definition of casual side
whichever dog says that if a value is
found in the cache the value is going to
be returned immediately and via not a
team a pod is not is never actually
executed if no value is found the
annotated method is invoked and the
return value is stored in the cache with
a generated key this is the definition
of cache recite this annotation is not
each cache offspring specific it's JSF
107 so basically whenever you
you're using a framework with j77
support you just need to add visa
notation why you need it and you're done
okay so I forgot to restart the
application just to show you the impact
waiting for the application to restart
maybe you're wondering how my cash and
my cash manager I've been configured so
let's have a look first in spring you
need to you need to tell me what how it
should get the configuration so for that
we've got this fine here so here we are
specifying that cash that J cars that
config equals each car that XML so let's
have a look at each car that takes ml
and each car that XML is defining cash
template so viscous template as a name
simple and a few configuration elements
such as the TTL which it here is the
time to live so it means that each
element entering the cash will not stay
longer than 60 seconds
there's also another interesting
configuration element is the TT TT I is
time to interval and that would be each
element that is not being accessed
during a period of city of 60 seconds
will be also removed from the cache here
I'm just using hip the hip here with
1000 elements I got a bunch of J
hip-stir hip-stir
entities that are being cached and also
I've got my weather reports cache so
let's have a look let's have a look at
the impact now I'm going to reload the
same page so once again is going to take
a while to generate because if you need
to go and get all the values on all the
results now if I reload it
since it's cached bam super fast what
happened just two milliseconds 20
seconds was you know just the time to to
get all this data and display it so as
you can see pretty efficient right just
one annotation you're done pretty easy
thanks thanks also to the integration of
GSA 107 matrix in J hipster yeah because
I forgot to say that in JSA 107 you get
an API but there's also an ambien a
statistics am bin and you can actually
request some statistics from this ambien
it has been integrated here in this
application so let's go to the metrics
page in the metrics page you can see the
efficiency of your cache with few
information such as the number sofa cash
sheets so how many time the cache could
return the value but was being asked to
it so we had five hits five misses so
the five misses were of the first time I
loaded the page you know it just this
data wasn't cashed yet so you need to
insert it in the cache and of course
once the value of all the calls were
made we inserted those values into the
cache and this is why also we got some
hits the cache it is 50% the cash it
ratio is 50%
why is because we have done ten columns
then gets and only five times we could
the cache could be the return of the
result and we've got some other stats
I'm talking about stats because it's
really important to control to measure
the impact of your cache otherwise you
could you could lose sight of your
performance you know what let's say
let's try another example let's define
heap size of one element so what does
that mean it means that whenever we hit
this capacity of one element then for
the next call the cache
you won't have a capacity to insert some
of elements so if you're trying to
insert five elements just like we did
it's not gonna work anymore right
because they're just only one spot
available in the hip so the cash will
spend its time inserting values key
values in the cache and also evicting
this value for the next value right so
let's have a look let's have a look what
could be even impact let's go to let's
try with this example so here first call
of course is not being cashed so
everything is pretty normal
I'm going to reload the page and same
thing the cash wasn't used for a second
look second loading of this page so what
happened well just as I told you the way
the cache is configured it story
unusable for this use case right because
for each page road we need at least five
spots available but there's only one
right so the cash keeps on as you can
see it's still putting data it's putting
that it's trying to put data but every
time it's putting that event the next
time is going to try to put some other
thing it's going to be evicted so the
efficiency of your of your cache is zero
right it's very sure between the heat
and the heat press misses so once again
try to focus on measuring the impact of
your cache tried to dimension your cache
knowing knowing your use cases but it's
very important back to the demo very
quickly I told you about cash flow and
I've got an example from another demo
and actually this is the demo that we
are running in the booth right now maybe
you've seen it you know with huge
figures and TPS and latency so this
piece of code is configuring the cache
programmatically and the difference here
is that we're not using carousel 4
they are using a casual order writer and
meaning that in this application we are
only interacting with a cache right
because the SOR the system of record
loader writer is the one but is going to
retrieve the values from the system of
record you remember everything when you
are using cash true
the cache is your gateway to the system
of record so let's have a look how if we
configured this loader writer as you can
see they implemented cache loader writer
and implementing this interface we
needed to provide an implementation for
load and during the load we are just
doing the system of record dot load data
key so we're just getting the data from
the database or from any system of
record we also implemented load all what
we didn't implement though is write and
write all but because we were not
inserting new values in wisdom of record
but if you needed to insert values in
the system of record you could actually
implement write and write all so
whenever you're going to do a pert in
the cache the loader writer will
actually insert this value in the system
of record okay
section of his presentation clustered
caching with chorus server I already
explained it a real bit earlier on so
what is attack chorus ever here so let's
have a look in more detail right now so
this is the typical application of a
clustered cache use case you've got two
instances of an application and this
application has already configured a
cache with several tales heaped here you
remember very efficient but you
shouldn't put too many elements in it
because of garbage collection when also
we of epo very large very predictable
but slower than the heaped here and
finally vac listed here with as you can
see with the arrow but clustered Java is
connecting to the Terkel a server and
providing you this remote storage
provided by vitarka receiver JVM so the
hot data is cached locally hotter data
infrastructure so fast that Hills is the
hip TIA in this case a very cool thing
a very cool feature of terracotta server
clustering is that the data cached by
one application instance is available to
all cluster members ok so if you if you
did if you inserted some values in
application one when application to if
it's using the same crystal cache is
going to also have it also be able to
access it so as you can see thanks to
this architecture you are lowering the
burden on your system of record so for
example if your system of record is a
database it means that your database
will receive less requests before data
set is available to the cluster so this
is a very important architecture design
decision from the each cache team to say
that the larger the larger tier so this
is the casual fir
tea should contain every element every
key value mappings of the cache and then
the smaller Till's just a subset just a
very hard subset so one or more mirror
servers may be deployed to provide high
of ability this feature was released two
days ago for each cache free of course I
didn't mention it but in each cache -
and Tucker a Forex where is the previous
release it's it's already available it
has been available for a long time
already and also soon to come is the
ability to spend that across multiple
active servers and this is a feature
that we call striping and it's coming
pretty soon you can get this schema from
the ish cache that documentation is
pretty up-to-date another view of this
deployment so right now in the next demo
I'm going to reuse the same web web
application you remember the application
with V actors and the weather reports
for their birthday but this time I'm
going to scale this application I'm
going to scale it to three instances all
those three instances will be running
inside their own container using docker
containers there is one database also
being deployed one Postgres sequel
database running in its own container
and also one tecora server running in
its own container by the way you can
actually find on docker hub some docker
images for attack or a server for
terrorists server open source okay so
let's go to the demo so here I'm going
to it seems to be a little bit small can
you can you still read yeah
it's really really small
yeah of course
well so about this anyway okay I'm going
to try to read because it's really small
so the first line is so here this line
represent the container for our system
of Records so it's a Postgres sequel
database when the second line here is
showing the container for the Takara
server and you can see the state is up
meaning it's running and then we have
three lines for our free instances of
the web application connected to a
cluster and connecting to the database
oh let's have a look
both three instances we are running in
containers and the pots are bound to my
look at home so I can access them with
localhost and I just need to shift a
little bit of a pot number as you can
see here the first instance is running
in a container named 62 something when
we have a one in a container named or is
something and the third one is running
in a container named AC something so
let's try with this one and of course
let's sign in first so some application
let's let's find a bits find an actor
and then let's try to get the weather
reports for a for birthdate of this
actor here as you can see it took three
seconds and then if i refresh very fast
because all those containers are using
caching of course but since it's also
closed at caching it means that if I
copy just miss value you know the tale
of this star 1016 to any other
containers then it was super fun
as well because it's used because since
we couldn't get it locally in verb on
epoch of Appeals it just used the
clothes tortilla so we just connected to
voter chorus arrow and got the mapping
back same thing if I go to this other
instance super fast because it was in
cash so this is a really cool aspect of
a terracotta seven when you start
scanning out your application you can
actually leverage with our color cluster
to ease the pain on external resources
and also those resources could be
expensive right because actually I'm
paying when I'm doing a request to the
Dark Sky REST API so it can also make
you save money also I would like to show
you a little bit how it was configured
of the cache was configured very quickly
as you can see scanning out is just as
simple as adding a bit of configuration
in your eh card XML so here I'm defining
a service in my eh car that XML when an
important addition to each country
I forgot to say is that it support
services so each cache free is
extensible
so here we are defining a connection to
attack or a cluster we provide vl2 V's
correct roster and also a default
resource because with Korres server is
in your tank or a server you can define
several resources such as alphap
resources and soon-to-come fr-s
resources but anyway you can define a
VIP resources and was often resources
can be used by the crostata so if i go
to the cache template name clustered
expiry is defined and in the resources
I've got V hipped here and also a
cluster dedicated here with size 10
megabytes
there's also some other concepts such as
crystal shared where several caches can
share the same
the same inch of the same storage okay
back to presentation
well ready at the end of our
presentation I would like to talk to you
about deployment examples of course each
cash free takara Sava 5 they are all new
we just got out six months ago so I
don't I cannot tell you that we have
customers running running in production
with each cash free and cassava 5 but
what I can tell you though is what our
customers are doing
we've Korres have a four and each cash
to here in this example you can see that
there is a presentation zone but he is
made to retrieve information about the
user and also an application zone
interacting with system of record to
make their application consume less
resources so less cons to the system of
record
they used tech aura and with gurave
actually deployed tens after chorus
servers in the cluster so tens meaning
that if you have for example 10 stripes
and each stripe as one active on passive
is already 22 color servers running
hundreds of clients meaning that a web
application deployed in any web server
container having each cache could
actually well each of them was
connecting is connecting to attack or a
cluster so you see was figures you know
it's not just the simple example that I
just show you
earlier on you can actually scale with
attack or a server and each cache
clients and also this customer was using
the Terra Cotta management console to
have a look at the performance of his
application and also using the
integration into New Relic
this is the end here I'm sharing with
you
shootings and references about the world
of caching
and the JVM and also class that cash is
cash free documentation is pretty good
for that and also there is a very nice
presentation that was given at devoxx
devoxx UK cash in 101 by Luigi coming of
a novice new ski to engineers vs cash
team once again please feel free to
visit our booth downstairs booth number
one and thank you very much for
listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>