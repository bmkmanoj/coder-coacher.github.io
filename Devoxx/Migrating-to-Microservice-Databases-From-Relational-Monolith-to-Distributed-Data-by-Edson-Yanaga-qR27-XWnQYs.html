<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Migrating to Microservice Databases From Relational Monolith to Distributed Data by Edson Yanaga | Coder Coacher - Coaching Coders</title><meta content="Migrating to Microservice Databases From Relational Monolith to Distributed Data by Edson Yanaga - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Migrating to Microservice Databases From Relational Monolith to Distributed Data by Edson Yanaga</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qR27-XWnQYs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody before we start I would
like to say some words I really
appreciate being here and I know that
some of you were expecting a workshop
from me for about domain-driven design
I'm very sorry that I wouldn't be I
wasn't able to to give that presentation
for you because I was expecting to be
arriving here at Krakow yesterday night
but my flight got delayed because of a
thunderstorm in Amsterdam so I just
arrived like 1 hour 1 hour ago and so I
think the organisers also for changing
the schedule for me to be able to
present this session right now and that
that's the word that I want to say I'm
so sorry for not being able to arrive
earlier but I couldn't control the
weather so but anyway it's a great
pleasure for me to be here just to
present this subject for you but
unfortunately I hope to be able to come
to Krakow
another time to deliver the
domain-driven design presentation with
the workshop which is the subject of
being studying for the past few years
and I think I might have some tips and
some content that will be useful for you
but again it will have to be in another
opportunity and I don't know are we good
to go yes well I guess we are so if it's
recording now good afternoon everybody
again I'm very pleased to be here in
Krakow for the first time just in case
you're wondering there are not maiding
Asians here in this part of the world
and I'm a Brazilian Japanese and yes I
came straight from the zero so I took a
long journey may be the longest one of
all the speakers to be here today and
today I'm going to be presenting for you
a migrating to macro surfs databases
from relational monolith to distributed
data my name is Edson ienaga and I'm a
director of developer experience at Red
Hat my twitter handle is at the Onaga
just in case you want to follow me I
talk a lot about devops microservices
Java domain driven design software
craftsmanship and other subjects
I'm also a Java champion and a Microsoft
and VB and I know it looked like a kind
of weird combination but in the end it
just proved to us that the world has
changed a lot
in fact Microsoft granted me the MVP
award because they helped them to
improve the Java support on their cloud
computing platform because I think in
the Java world we value very much open
source we value choice and it was it is
very important for other Java developers
to be able to have many different
options for deploying our applications
on cloud computing providers that's why
I helped with them and it has improved a
lot since the since three years ago when
I decided this work started this work
and just to give you a tip I just
released the book from O'Reilly
migrating to Microsoft's databases which
is the same subject of this talk it is
available for as a free download on
these URL developers Red Hat calm
promotions migrated to Microsoft's
database but just in case you don't want
to type all of this this URL you can
just go to developers redhead comm slash
books and you have an index of all the
books that Red Hat is published with
O'Reilly so my book is one of them so
we'll be able to have the free download
and in just in case if we have some
questions or maybe I can just give them
away in the end I have four hard copies
of the book for you so some of you might
be able to get a hard copy so this will
be a very intense talk we have I have a
lot of content to be sharing with using
the line only 50 minutes so forgive me
if I speak too fast but just because we
have a lot of content to be covering and
I want to present you a very nice demo
in the end or how can integrate these
microservices databases using a
technology called division so let's
start I always like to start my sessions
with this sentence which is not for me
but from Forbes which says that now
every company is a software company I
like to say that you don't work for a
bank
you work for a software company you
don't work for an industry you work for
a software company because software is
changing the word and I'm using I'm used
to I used to be one of these people that
thought that
economy had everything to talk about
money but then after studying a bit of
economics and particularly behavior
economics I just realized that economics
has nothing to do about money but has
everything to do about people because
the economics is study how people
interact with each other into a system
how they change this system from one
state to another state and some of the
motivations that people have to change
this day state from one point to the
other can be like love hate power and
also money so when we're talking about
software when we're talking about
digital economy we're talking about how
software is changing the way that people
interact with each other and some great
examples of this new digital economy are
that the largest car transportation
company in the world owns no cars which
is uber the largest lodging company in
the world owns no real estate which is
Airbnb the largest online retailer in
the world owns no stock which is Alibaba
and the largest content network in the
world produces no content which is
Facebook all of these companies they
have something in common is that they
only exist because of software so
software is changing the way that we
interact with other people and it's very
important for us to remember that
software is made by people for people
every time we forget that we don't
deliver an optimal result of our work
and I also like to consider myself as a
software craftsman and the best
definition that I have a software
craftsman is somebody who cares about
what he or she does because the software
that we delivered to production can
really change people's lives and we know
that we can make people miserable with
the software that we do because we know
that sensation we feel miserable when we
use software that is not up to our
expectations and some years ago when I
was in a very hard moment in my career I
did it like a hard decision I decided I
own I in fact I figure out that the only
thing that I knew in my life was to code
that I decided if I wanted to continue
be a programmer a software developer I
should be better do it very well that's
why I decide to be a software craftsman
and that's why I care so much about what
we do and while how we deliver soft in
production and how can we change
people's done how can you improve their
lives for the better okay so we can make
a choice and if you're all here it's at
that nice big conferences such as dev
exponent if we're trying to learn and
uncover better ways of trying to deliver
better software into production then I
think we can all consider on Salvas a
software craftsman because we are
changing the world for the better for
ourselves for our kids for the people
that we love and for the people that we
yet don't know but we are doing
something which we can consider like a
legacy a good thing for other people and
if we're talking about DevOps and micro
services I'm not going to dig into the
subject but if I got to choose just one
single simple thing to introduce you
about DevOps and micro services I would
like to say that all of the discussions
about the subject in fact we're just
trying to improve something that we call
the feedback loop and if it black looks
something like the essential for every
every human because that's how we know
that if we're doing the right thing
right and that's how we were able to
improve the things that we're doing in
fact if you think about the the major
revolutions or evolutions that we had in
the past like 40 years of the software
development process we all try to
improve the feedback loop we will try to
to shorten the time that we had from
doing something and getting the feedback
also if you're doing something right or
if you're doing the right thing so I
think about in the past we used to like
use test editors we used to have to
think a lot of time about what we're
coding maybe we had to punch out a SIM
card but in my computer and wait like
some hours to check this if it code
compiled so just to check the syntax was
right so then we created now we have
this online modern IDE that why we're
typing they're able to check if the
syntax is right maybe some of them can
even compile your code to check if the
semantics is right for us then we
created automated tests we didn't have
to wait like a week to check if the
testers of with the testers if our code
was behaving properly we could just run
an automated build then we could check
the test if we didn't break anything in
another part of the code that somebody
was coding then we created continuous
integration to solve one of the hardest
problems
that we have in software development
which is to check if I'm not screwing
what other people are doing we try to
solve this problem by doing that more
often if you get to check the feedback
look is basically we try to shorten the
lead time that we have between doing
something and checking if you're doing
the right thing right so the shorter the
feedback loop the better is the general
result of what we're doing no matter
what activity we are doing and the
software can be different and if I want
to give you just one technical term
about DevOps and micro services I'll
have to tell you that the most important
measure for me in any developed and
micro services discussion is the bed
size and technically the bed size is the
amount of changes that you have in to
each one of your releases when you
deliver that into production so just to
try to explain what I used to be an
independent consultant we took for
joining Red Hat and I used to figure
well if the feedback loop is so
important for us developers why don't we
improve that why don't we constantly try
to reduce our bed size and I ask this
question what prevents you from
delivering software faster into
production and the number one excuse
that I always receive it was that we
don't release software fast into
production because when we release
software we have a lot of bugs these
bugs disrupt users that's why we don't
we can't release the faster because we
need more time for testing we need more
time for testing to solve the bug that
we are delivering to production but
people usually don't realize that the
more time you have for testing the more
time you have for coding so if you're
just really using software every month
you have more time to protesting like
two months you have more bugs to because
you would be will be doing more coding
so testing time usually is not enough
and it's not a good excuse for you for
not releasing software fast into
production so let's try to create a very
simplistic correlation I would like to
say to you that what causes the bugs
into production usually our changes in
changes in three different aspects of
our environments with our software
development process which are basically
we have changes in the code we have
changes in the data and we have changes
in the environment that our code runs
so let's try to simplify that even more
I'll tell you that what causes bugs in
production are basically changes in
codes so if you specific this
correlation the more changes in code
that you have into each one of your
releases the more bugs that you have
into production so the anti-economic
away the human a way of solving this
problem is we need more time and more
time means more time for coding and the
more changes that we have the code the
more bugs we have into production so the
economical way of trying to solve this
is to reduce the amount of changes that
we have into each one of our releases so
we have fewer bugs into production maybe
we need to improve our bed size we need
to reduce your bed size until like an
ideal size of a single commit so perhaps
if I have a suit fully software fully
automated software deployment pipeline
if I get the chance to give like one
commit one deployment into production if
I have any bug into production where's
the problem
maybe the it is that on that single
commit with just 15 lines of code so
it's easy for me to spot where the bug
is where the problem is in fact people
we've sufficiently mature software
deployment pipelines which are able to
do like one single commit one deployment
is much easier for them to not have a
rollback strategy it's much easier for
them to just well if there is if we
deploy the bug into production is much
easier for you to analyze the code that
we just deployed because that's like a
few dozen lines we just analyze the code
find the bug fix that and deliver that
in production very quickly that's the
modern way that's the ideal way of maybe
trying to reduce our bed size and
deliver sup better software into
production but even if we try to get too
into this ideal point and I'm not saying
that everybody needs to get to one
commit one deployment well for maybe for
some industries like banking I use we
have this very interesting case where we
have a Swiss bank which we were able to
reduce their bad side to one week and
one week maybe for a bank it's awesome
because I don't see people like checking
everything with all of the bank
regulations and delivering software
faster than one once a week so maybe one
bit can be good for you it really
depends on your particular use case but
if we're trying to aim for the one
commit one deployment even if you try we
as developers to reduce our bed size
until that this idea aside there's
something called the maintenance window
which is the specific time frame of the
week or the month or the year that the
opposite guys allow us to release our
soft introductions so the deployments
usually are only allow it to be done
when we have them enter this window and
because we're not allowed to disrupt
using two productions and usually the
ops guide through the establish a
maintenance window for us because we
break things into production so we have
to break this this visual cycle we need
to be able to to release software faster
than the maintenance meter allows so we
don't want to be releasing software like
every single week at Saturday three AMS
we want to be able to deploy software
every every day every hour in the middle
of the day we've loved this written
using two productions if we want to
break the maintenance window we need
something which is called zero downtime
we need zero downtime deployments and
for us to achieve zero downtime
deployments we have one requirement
which is we need at least blue green
green deployments and blue gray
deployments I'll try to explain to you
very quickly
blue green deployments in a traditional
architecture you have your clients
issuing requests directly to your
deployment which are called the blue
deployments if you want the blue green
deployments you need to add another
component to your architecture which is
a proxy which can be a load balancer or
a load balancer a reverse proxy doesn't
matter just some some entity which is
receiving requests and for when you need
to a back-end so if I have a proxy in
the front of my architecture now I can
create two separate environments - eco
environments each one of them is capable
of handling 100% of my production
requests at a given moment so I have a
blue and a green with the green
deployment and if I want to create a
Bluegreen deployment we observed on time
that maybe I can ask the my proxy to
stop issuing production requests to my
green deployments I can stop the servers
I can copy the new artifacts I can start
the server again I can wait for it for
warm up if it's using Java for example
maybe I want to issue some fake requests
just to warm up the JVM so it can be
fast enough and when it's fast enough
maybe I can ask the proxy well you
handle the they request now to the green
deployment because now it has a new
version and this way of this method of
deploying with Bluegreen deployments is
not only in our only as allow us to have
zero downtime deployments but also allow
us to have safer deployments because if
anything went wrong with the green
deployment if we decided well now we're
having too many errors on the green
deployment maybe you can we can just get
to the proxy and just group the request
back to the blue deployment because it
still has the old version so it's a very
easy strategy for you to roll back your
version because you always have like the
new version and the old version running
simultaneously okay but if the green
deployment went well then we can ask the
proxy to stop issue request the blue
deployments and start issue request to
the green deployments and we can do the
same with the blue we stopped the
servers we copied in your tracks we
start the service we issue some fake
requests we wait for it to warm up when
everything is okay
we ask the proxy well now maybe you can
handle requests you can issue requests
to both backends because both we have
the new version working are well you can
even load-balanced a request for an out
but that's just an option usually you
just use Bluegreen deployments for a
failure over strategy but it doesn't
matter how hard is your code or the
structure of your code when you're
deploying Bluegreen deployments to
production if you're thinking about code
code is very easy because the hard part
is the state state is always harder than
code and usually in some applications it
is very hard and when we're talking
about States usually you have two
different kinds of state into your
applications you have the summer state
which is usually the state that you're
storing your HTTP sessions and you also
have persistence state which is usually
stored on relational database I know
some people think about no sequel
databases and everything else I think
they're a great solution but we were
talking about enterprise software and
90% of the of the software developers in
the world that develop enterprise
software so maybe we should be thinking
about relational databases many of the
technique that I'm going to represent
for you today they are not specific to
relational databases there and so you
can modify the concepts to be applying
into any kind of persistent state
later okay so the number one question
that I always have well but if I have a
legacy old monolithic database and I
want to be able to create zero downtime
deployments with my relational database
how do I deal with that because usually
my application can handle I have a new
schema of my application and the old
version of maplocation can't manipulate
the data with the new version or the
opposite so the answer for you if you
want to be able to do this zero downtime
thing with relational database first
step you need to automate everything
you're not allowed to be applying your
sequel statements anymore your outer
tables or update statements do not allow
to do that manually you're not allowed
to mail the scripts to to the DBA to
apply that and when you release a new
version you're not allowed to create an
JIRA issue to be applying the the
statements in work you need to automate
everything so that these tools this
database migration tools they need to be
part of your software development
process they need to be applied either
on the startup of your application which
is one possible approach or you need to
apply that into a specific step of your
software deployment pipeline which is
particularly my preferred option so two
of the most popular tools that we have
in a java word are Flyway and liquid
base they are not specific to the java
word they just happen to be implemented
in Java but you can use them into any
specific technology I just have a
particular choice for a fly way because
I help it to patch and deployed the
sickle droid which is a JDBC driver for
Android and fly way works with Android
because of this JDBC driver so in the
end the last project that I was working
on we were using fire way to automate
the migrations both in our back-end
services and also in our Android
application that was being deployed to
the mobile forms okay so if you want to
achieve zero non-time migrations first
step we need to ensure that we have back
and formal of back-and-forth compatible
migrations into our deployments and to
achieve that we need to split our big
migrations into baby steps
so it will be used to do like many steps
into your Seco if your Seco statement
for database migrations used to have
like a hundred statements maybe now you
have just a single statement per
deployment each one of your releases of
software we have just a single statement
of database maghreb migration it just in
case you're wondering what is a
migration migration is the technical
term of the amount of sequel statements
or code statements that you apply in
your database to change your database
schema from one version to the other
okay there is this is the technical term
migration also if you want zero downtime
migrations you not allowed anymore to
have too many long locks so if you have
a table with like a billion columns or
sorry a bit long rows are you in and
this update takes like five minutes
you're not allowed to do that anymore
because lot time equals to downtime into
production so now you need to be able to
sharp these updates to avoid these long
locks and sharding is a very fancy term
in the database word to just say that
well now you need to split your update
statements if you're doing like if you
used to do like updating the whole table
Namie we only had a billion rows now
maybe you need to update your rows like
1 million at a time so you can have a
feasible downtime into production
feasible lock time like in how much time
down time are you allowed to have it
really depends on your application your
business use case that's why I always
recommend you to rehearse a lot your
migrations before applying them into the
production database so everything that
we want to do well we need to rehearse a
lot so if you use to drag to issue that
outer table rename something to
something else maybe you need to split
this thing into an add column first to
multiple update statement later and then
maybe you need to delete that later
right so I'm in the past year and a half
and almost few years now I talked to
many different development teams and
people worldwide trying to check how
they were dealing with this kind of
problem and I was able to gather four
different scenarios that people that
teams worldwide are using me in the
beginning
most of the teams that I was talking to
were start out things because they had
very mature software deployment
pipelines and they were using these
strategies but in the past year I've
been also seeing some enterprise
companies being able to apply these
techniques so maybe you want to try to
try that too
able to collect four main scenarios
which are at a column rename a column
change the type and format of a column
and delete a column okay and I'll pass
through them very quickly but if you
want to see them and study them in
detail I have a lot much in them much
more detail in my book so first scenario
add a column when you see steps 1 2 3 &amp;amp;
4 each one of these steps going to be a
different version that you're going to
deploy in the production then on the
first version you're going to issue out
a table at column 2nd version your code
computes the read value and writes the
new column and then wastes a compute
maybe can be a default value or if you
can compute the value of the column
using the the value of other columns you
should be doing that first step you
update the data using charts and you you
need to you need to be reminded that the
third step can be multiple steps because
maybe you need to charge your updates
and for step your code reads and writes
from the new column and I have to tell
you between each one of those these
software releases you will be open your
you'll be opening your database CLI and
you'll be issuing select statements just
to check if your application is behaving
properly if your application is writing
the right information into the new
column so this is not only the only way
for you to be applying zero downtime
migration to production is also the
safest way for you to be releasing that
because each one of these steps is
non-destructive you never lose any kind
of information they are always back and
forward compatible the new version is
always compatible with the previous one
if the previous version is always
compatible with the new version of your
database schema so you don't have to be
afraid to rely on a backup to restore
the information on your database schema
because if you do this the Democrats
this way you're never going to lose any
data so that's why it's so safe the
second scenario is reneedar column and
this is the most complicated one first
step you add a column second step your
code reads from the old column and
writes to both third step you copy the
data using small shards fourth step your
code reads from the new column and
writes to both fifth step your code
reads and writes from the new column and
six steps you delete the column later
and same delete the column you don't
delete the column you just mark the
column for deletion because deleting
that column is the destructive statement
you lose the data so you just mark the
column so maybe like several releases
later maybe several weeks or even months
later you just create a JIRA issue to
your DBA well in the next maintenance
window you check if the data is not
being used anymore then you can safely
delete the column okay so you always
delete the column much later not never
on the next subsequent release third
scenario change the type of informative
the column and you can see a pattern
here yeah that's exactly the same steps
it's a good thing is you can you're also
you're always able to be rehearsing the
same steps so change in time form a
column is the same thing the rename
column and for scenario delete a column
don't you never delete a column to
production but because at the moment you
do that you lost all other information
you don't delete the column you just
stop using the rid value but keep
writing to the column because maybe your
screen something else and you're in your
previous version still needs the data on
the column so you keep writing that and
if everything is going well then maybe
you can stop writing to the column and
maybe they later you can mark the column
for deletion
that's the safely safe way for you to be
deleting a column into production right
what about my referential tell you can
constraints do not have a miracle here
sometimes you I know it's if you think
about that you don't need this
constraints for application to work
properly they are just a safety net so
maybe at least on this moment should be
safe for you to drop them and recreate
them later when everything is working
fine right
that's what I had to tell you about zero
don't emigrations and you might be
wondering why I'm talking about zero and
time aggressions if the topic here is
microservices because in the past when
you had just a single monolith the
downtime me of your system was the
downtime of your monolith but now if you
have multiple micro-services multiple
artifacts the downtime of your system is
the dot that is the is the downtime
evolve or your micro services multiply
it so the problem just increases and
you're not allowed to have downtime in
your overall system just because you're
updating the database schema of these
macro service just because multiple
they're updating their microservices
multiple times per day so you don't want
to have this kind of dull time because
it would be very bad for application
that's why you need gears on time
migration it is a requirement if you
want to to be able to be using micro
service you don't you don't want to
allow this kind of downtime in your
overall system long let's dig into the
micro services part and marking father
did a very good job trying to collect
some characteristics on micro services
so he collected nine characteristics of
Microsoft's architecture and today I'm
going to discuss with you the principle
of the centralized data management which
means that each micro service must own
its own database that's the best press
because we want to uncouple the queues
of each one of those teams and I want to
I don't want to tie my deployment cycle
of with the overall database deployment
cycle just because my version of the
schema is not compatible with the
version of the other team so that's why
each micro service mods on its own
database and if the best spreads for the
plot the developing micro service is to
start with the monolith and just start
breaking some pieces of functionality
into macro services first you need to
decide which piece of your database you
must distract and I would like to give
you some tips about that but unfortunate
adei it is a very complicated subject
and I have to tell you that the answer
goes through domain driven design and
properly domain modeling properly
business domain modeling and properly
creation of your bounded context in your
application so I can't help you that we
do that today because it really depends
on your domain model but if you've
already chosen which piece of your
database you decide subtract then I can
help you to to make it work properly
maybe I can help you to integrate that
properly in your distributed system ok
so if you want to stretch your database
and each database must have you must
have wondered base pair micro service
and you still have an old man monolithic
legacy relational database the answer
goes through that for some time both
monolith and micro service we need to
access the same data maybe in the same
database or maybe in a separate database
and how do I do that I'm pretty sure
that speeding is not
but integrating that is also very very
complicated splitting is come you have
to consider the business domain model
that's the complicated part but
integrating needs that the technologies
you need to consider the set the
architecture and the technology and
that's what I want you would want to
show you today but before I dig into the
integration patterns I have to give you
a very quick explanation about
consistency models again it's an
oversimplification for us the important
ones for Microsoft databases are a
strong consistency and eventual
consistency in a strong consistent
system you have multiple nodes in your
system all of the nodes in the system
must agree before a client can read the
new value so if I say that now I equals
to five all of the nodes in the system
must agree now I equals to five before
any clients can read the I equals to
five okay and that's usually the
consistency model that you can achieve
using transactions or distributed
transactions and eventual consistency
and I got to tell you when we're talking
about relational database we don't we
only want the strong eventual
consistency model which is a very
particular consistency model which means
that if I have multiple nodes in the
system I have to choose one which is the
canonical source of information which
kind of makes sense because if I have a
monolith and other sides - right well
distract well I'm going to strike the
custom information from the moment I'm
going to create the customer
micro-service the only endpoint in my
system that is allowed to change the
customer information is the customer
macro service so we have a canonical
source of information we're we talking
about at least our enterprise systems so
I decided this is can the canonical
source of information and the other
nodes in the system in a strong eventual
consistency model they can have outdated
values but all of the values are always
correct you don't have data conflicts
right so if they decide that I want to
update now I equals to five and maybe
the old value was free the other nodes
can have this tool it can still have the
old value free while this one has always
the least the the latest information and
then it propagates the changes to the
other nodes so some nodes
can have the latest information but some
know that the system can have outdated
information how much time can you handle
can you afford to have before updating
all the nodes really depends on your
business case and you might think oh I
don't want to do that I'm not allowed to
do that well if you're thinking about
distributed systems you have to embrace
eventual consistency for performance
reasons and if you think about that most
of the systems that we use these days
they are already eventual consistency
consistent when you think about if you
have a web application by the time you
queue in your database you manipulate
the data you can't you build our HTML
and you give a reply to the user on the
screen by the time the user reads the
screen it's already updated so you're
ready handling with eventual consistency
models you're just not used to that and
you have to learn to live with this kind
of consistency model I also have to talk
a bit about crud and secure ass and
pretty sure everybody have heard about
crud which is possibly the simplest
possible teacher for us to be
manipulating information which means
that all the operations create read
update and delete operations are all
being you are all using the same domain
model and the same data store
this is crud so read and write
operations they already use the same
model in code and the same model in the
database what does it mean it means that
if I have a customer a crud architecture
it means that I have a customer class in
Java and I have a customer table and the
other dependencies I use the customer
class and the customer table for all the
read and write operations
this is squared this is very simple and
in fact I believe that for 90% of the
architectures of the user case this is
the best a possible architecture but if
we're talking about distributed systems
we need to understand properly what is
secure s and I know that many people
discuss the CRS but some people have
some misconceptions about what is the
security architecture and I'll try to
give you a very simple explanation what
is that first it's a very it stands for
commanded Kure responsibility
segregation which is just a very fancy
name for stating that in a security
architecture you can have
multiple domain models for read and
write operation what does that mean and
you all be using that for some time you
just didn't know that it had this name
for example maybe I can create a
customer class in Java for me to be
issuing the right operations to the
database so I populate the customer
object and I ask my REM to be storing
the customer information into the
database but for reading maybe the
customer has a lot of information has
email has addresses transaction history
has the past addresses too but if I want
to generate a customer report and I just
want the ID the customer name at the
email addresses maybe it's not a good
idea for me to retrieve all the
information that the customer object has
into memory just generate the report
with this free field
maybe you want to create another class a
customer DTO class which is stands for a
data transfer object is not a in an EJB
1.1 video I'm just saying that this
detail is just a placeholder for
information is a plain pojo an anaemic
podium so maybe you can create a custom
curie in your database that just
retrieve this free field and you get a
list or a stream of customer videos so
it's a better fit for you to generate
your customer report this is a secure
ass architecture every time you have
different classes for reading and
writing you are creating a secure secure
as architecture but then you think we
should think about security
architectures you're not many more
constraints to have a single source of
information on the persistence layer you
can change that so a very interesting
scenario for distributed systems and
Microsoft's architectures is the secure
s with separate data store in fact
that's the pattern that we must have
been likely applying into our market
serves architectures which secures with
separate data stores you can have
different sets of tables in the database
to be storing your write model and your
read model if you ever use it the
Creator if you ever created a view in
your database and created a Curia
against your view to retrieve the
results that you wanted to show the
information into a different class into
a different object you created the
secure s architecture if you ever
created a database materialized view to
be able to aggregate there is
ought to be creating a report you
already created a security architecture
and you get and you see the point well
if you if you're creating a database
materialized view you're creating that
for performance reasons it's much faster
for you to qu the materialized view
rather than all the times aggregating
the the results in memory using a Curie
so you're creating a secure s datastore
for performance reasons and in
micro-service architectures you will be
creating secure s architectures with
separate data stores for performance
reasons - and for integration purposes
that's the main strategy that you'll be
applying okay and since we're talking
about secure s I also have to talk a bit
about the event sourcing some people are
advocating event sourcing as the
solution for all of the integration
problems in micro service architecture
and I have to tell you that take care I
work with a lot of different teams with
event sourcing and I have to tell them
most of the teams they do event sourcing
wrong so they take a long time to do it
properly and that's why they suffer so
much because if you do events or scene
wrong you suffer a lot it's much worse
than just creating a monolith and not
using event sourcing at all
so take be very careful if your
designers use event sourcing but what is
event sourcing I'll give you the classic
example of a bank account you think
about about a bank account and have the
amount of money that you have a bank
account may be the canonical source of
information on how much money you have a
bank account is not a column in a table
in your database you mean in a bank
account you what do you do you create
multiple transactions debits and credits
transactions one that's something and
another one that subtract the amount of
money that you have in bank account and
if all of your bank accounts start with
zero if you add multiple debits and
credits if you want to know how much
money you have right now you just have
to carry all the operations that you
have in your bank account and you just
summon subtract all of them and you have
the current amount of your bank account
that evens that's even sourcing the
source of the canonical source of
information is not a single column in a
table it is provided by a stream of
events and in this case is a stream of
debit and credit transactions right but
you must be wondering
in a bank it might work this way but
doesn't perform it that well because the
more transactions the more depths and
credit transactions that I have and the
more customers that I have no more bank
accounts that I have the slower the
system gets so maybe you want to create
another table with another column just
for cashing purposes so they will have a
cash amount of money that you have in
your bank account just for you to speed
up your reads so if we well if you are
wondering from if I want the right
information I what I write to the
transaction table but if I want the read
information I read from this cash but
you see the canonical source information
are the transactions because the cash
can be wrong in fact all of the banks
are during the night they just replay
all of the transactions from the
previous day and just check that if the
amount of money that you had in the
auditing process is the same one that
you have on the cash table so you can
check if anything goes goes wrong then
maybe you can and you can you need to
raise a manual issue so some auditor
will have to check the information that
is reading on the transaction log so
that's how you handle you have a secure
s and event sourcing event sourcing is
the ability to write information as a
stream event and secure s you create
another table with the cash advance for
performance reasons
so that's secure S with event sourcing
and they play very well in distributed
systems right but it's not just not for
everybody so now that we know how this
this thing works okay I have to pass to
give to you these different scenarios I
have nine different scenarios for
integration purposes you can have shared
tables database view materialized view
triggers transactional code ETL data
virtualization even sourcing and change
data capture and just to be try to
explain a bit more why you should be
using these storage integration
techniques because basically what I've
seen in many different things what why
do that well we want to create
micro-services we decide to split the
customer information and create a
customer micro service here so they
think well everything that we need to do
to change in our old models is that well
we have the AOS we done the code
properly so now we create a resin bond
here so
our do instead of inquiries to the
database now we wish you an HTTP request
and get information and transform the
information into a Java object and
manipulate the information on that okay
the deployed that into production they
realize oh my god it doesn't scale it's
very slow we need to fix that next step
is oh so it's too slow well it's a
performance problem so we need to create
a cache so they create an internal cache
in the mono needs to be caching the
information that to your keyring but
then realize well internal cache is not
enough is not performing well enough we
need an external cache so they create an
external cache service they plug either
the monolith and so now you're you cash
in all of the Curie's for performance
reason they write well now it's
performing well enough but then you
realize well but in the past all of the
report that I had here my old monolith
they were joining directly into the
relational database with the customer
table but now I can't do that anymore
now I have to fetch all that information
on the tables and I have to carry the
custom information in memory because
it's in a cache that I have to join all
of this information to to generate some
kind of report I have to generate do to
do this join in memory I have to get the
objects and I have to get the result set
and joint information just to show the
same information that I had for that's
too much work well maybe why don't we
create a secure a stable customer here
which is a replica of the customer
information that I had on the customer
microservice and I Ana and apply some
techniques for integrating this data if
you are right here to update my data in
my secure ass datastore okay so when you
create a cache it table or a replicated
table in your old monolith with the
custom information you are creating a
distributed secure s datastore and some
other strategy that you can use for
achieving that are this one some let me
explain very quickly to you the
strategies first strategy you can use
shared tables which means both your
macro service and your monolith or your
other macro service are going to read
and write from the same set of tables
which is the faster that a data
integration possible you
always have strong consistency because
your reading and writing over in the
same database with transactions you have
low cohesion and high coupling in fact
you have you have such low cohesion and
higher coupling that I should consider
this the heck don't do that because in
the teams I've seen doing that share
tables are the perfect excuse for not
changing anything anymore
because if you want agility with
micro-services if you have both this
system and this system are reading right
from the same database I can't change
the tables in the system here because
the other systems is using this
information and the opposite is true
this system can change anything because
this other system is changing this
information in just for two end points
imagine if you have multiple
micro-services right because share
tables is a hack and you should only be
doing that if that's you know what
you're doing you're doing that just
quickly to deliver it something to
production but you know in the next
release you have to change this kind of
integration next kind of integration
technique if you want to create this
secret that is stored from the customer
microservice maybe you can create a view
and the view is the you want to
implement has the largest support from
the mms because of as of this year even
embedded database like hu and c coli
they have database views you can have
possible performance issues depending on
the DBMS you choose you always have
strong consistency because have
transactions that'll be transactions one
database must be rich by the other which
means that you have different database
servers you need at least an Oracle you
need to create a DB link between one in
the other and I know if you if I have
any DBA here it's going to curse me
because well the B links are not
supposed to be used for this but I've
seen teams using that and I know that it
works because in some integration snares
it really performed very well and worked
and even though I don't recommend that
these database views in an Oracle can be
created as updatable depending on how
you create the view next scenario
database materialized view which I use
it myself in many integration techniques
in the past
offers much better performance and
database views because instead of being
an alias to your Curia database view is
a store that is a truth table in your
database so you can optimize your
database view - as the same way as you
would an
in a table you can have strong or
eventual consistency depending on how
you update your materialized view if you
are updates on every commit then it's
strong consistence if you update that on
demands are on chrome trigger then it's
eventual persistent it rich depends on
your business user case one that a base
must be reachable by the other again it
means at least on Oracle a deep link and
a distant work again it can be updatable
if you can use the blink or not really
depends you won't be able to use on
commit update because it doesn't perform
very well but depending on your how much
delay can you afford on the stay on edge
of your secure s datastore you can use
the the debate materialized view because
I've used that a couple of times and he
works pretty well you can also create a
database trigger every time you write
here you write here you also has strong
consistent because you have consistent
because we have transactions but again
one database must be reachable by the
other and again at least on Oracle you
need the database link but database
triggers it doesn't perform well it
doesn't scale and I consider this a bad
press too because what does it mean it
means that it only works for
point-to-point integration because it
has performance issues and scalability
issues in terms of how many endpoints
can you handle with that because you
have just customer micro service and
monoliths every time I write here I
write you too it works because I have a
single endpoint but maybe now I have
five micro services and all of them they
need the custom information then I need
to create trigger code so when I write
here I have to write here here here here
here it doesn't perform very well in
fact it also it it is also tightly
coupled because well I can change the
data model there because I have to
change the code here too and the
opposite is also true
so it should be considered a bad press -
but I've seen some teams working
depending on their and the size of your
integration it might work you can also
use transactional code which is the same
case of triggers and when it says any
code can be a store procedure or
distributed transactions you always have
strong consistency because web
transactions again you have the same
problems as transactional code you can
also use ETL you have lots of available
tools you can use LaPenta whole you can
use click view you can
- beauty as your ETL - it requires on a
standard figure which can be on-demand
or can be chrome trigger you can
aggregate from multiple data sources
it's always a venture consistent because
by the time you read informations
already updated and it's already in a
read alone integration right I recommend
this approach if the integration is not
too big and if you already have people
in your team which are used to be
creating this kind of integration if
you're using to generate VI reports
you're already creating secure s data
stores maybe you can add another
integration in your ETL - to be creating
another secure s3 store so it really
depends on your business use case if
she's not too big and you already used
that maybe you can use another one you
have also have data virtualization which
is one of my favorite data
virtualization basically means that you
have your physical database and you
create a virtual database on top of that
which can have a different view of your
cure information right since your ex
always acts in the same physical tables
you have real-time access you have
transactions so it's a strong consistent
you can agree get from multiple data
sources and it can be available but the
cool thing about virtual databases this
is this showed that the basic
architecture of tea eat which is an open
source data virtualization platform I
just wanted to show you that you can
have multiple data sources you can
create multiple virtual databases on top
of your physical layer and look and I
think it's one it's one of my favorites
because it's also the safest one because
one of the problems of micro services
that usually split the wrong part of
your monolith and if you split to it
wrong it's very hard for you to put
information back but if you're using a
data visualization platform you just
need to create a virtual database for
your micro serves and then one on it is
still using the old physical table you
just click that and start to play if it
doesn't go well you can always discard
the virtual database and start from from
zero right so it is the safe but if you
did it right then maybe later you can
apply the changes that you apply on your
viewed or database to your to your
physical tables ok next strategy of
insourcing is described before the state
of the data is stream of events
it's easy auditing its eventual
consistent because you use a synchronous
message to use a message broker for
distribute the messages and again I
recommend even sourcing if your domain
model is already modeled as event
sourcing I don't recommend you take a
legacy code a code that is already
performing to production and change the
architecture that you have on
information to behave as events or see I
think that's a nonworking approach it
works very well if you have to
restructure everything or if you're
performing a greenfield project but all
else I don't recommend it in storage
because it's because as I said even
source is very hard to do that properly
the last one is change data capture the
change the capture again what do I do it
can call its poor-man's even sourcing
because CDC tools they plug into your
database and they read your database
transaction log so for each one of your
outer tables insert update and delete
statements that you issue against the
database the CDC - will capture that we
will stream that through html5 upfront
for a message broker and the audit point
end points will be able to consume this
information and manipulate that and
write to the secrets that is stored that
you have in your distributed system so
CDC is my favorite one because you don't
depend on any technology you just depend
on your database because the two plugs
directly into the database and you can
read from the message broker and
implement that - into any technology
that you wish to write and highly
scalable and everything else and I want
to show a very cool demo and all over
tumblr where it will be a very quick
demo I promise I'll show the B zoom is a
CDC - which is sponsored by Red Hat
currently integrates with my sequel post
degree MongoDB Oracle it's almost final
and the next one is probably sequel
server just in case you want to check if
you want to check another CDC - if you
ever use it or go ganda gate which is a
very expensive solution is also kind of
a CDC - for replication so how does the
vision work so my very quick demo is
this one if you get to check here I have
my I have zookeeper instance running
which stores the information for my
Kafka bus
I have Kafka running which is a very
dear message
these days everybody loves Koshka
particularly because it's
high-performance it's ordered and it's
persistent all of the clients they
always receive the message in the same
order and if you think about that it's
very important to be ordered if you
think it about replicating information
because the order that you apply the
transactions modify the state of the
data right so so you can have problems
if it's not ordered and it's persistent
so it never loses a message that's why
it cascades so so dear these days I also
here have a my sequel server running and
here I have a Mexico client so I get to
select from customer so I have four rows
here and I want to update so here I have
the B zoom run into and here have a
Kafka watcher which is this just an
application that watches the messages
that I've been propagated in a casket
bus what I want to show you is that I'll
get here let's update one row in my
database so updated that if I select
again you can see now the name is Alma
he and if I get here well the vision
propagated a message on the covers let's
see what the message has so if I show
you the message formatted here you can
see that the vision broadcasted the
content the the schema of the the data
before the statement and after this
statement to so if that was an outer
table statement it's very used for you
but since it was an update statement you
can see that the vision product has the
payload as it was before you can check
that it was n and after the statement
the it is n mahi and you can have some
internal information here and the
operation is an update statement you can
see if you have received the J's
information it's very easy for you to to
manipulate information and create your
own secure s data store maybe the
customer table is very big and you don't
want to play replicate all of the data
you just replicate the data that makes
sense to your macro service or your
monolith right so I think you can all
think about it's very easy for us to be
manipulated DJ's on message and work on
that so another question that I usually
people have which I won't show because
we were over time is that what happens
if
vision goes down if the vision service
stops well the point is the vision
stores on the zookeeper instance in
which in which point of the transaction
log it has to stop it so you can say to
stop the division and you can keep
transactions rolling on your database
when the business goes up get up again
it knows in which the point of the
transaction log it has stopped it and
you start to broadcast the broadcast new
messages from the point it has stopped
it so it's very safe you don't have to
worry about losing any kind of
transactions because the B's iam handles
that and of course you can you have some
granularity you can choose which tables
you which schemas you want to listen for
changes and everything else right so
that's what I wanted to show to you
today that's the vision demo that I
wanted to show today I hope that the
content was helpful to you I would love
your feedback if you have any other kind
of different strategy or if you are
using any kind of different strategies I
would love your feedback the best way to
reach me out is on my Twitter at Kanaga
or just in case in my email ya Onaga at
redhead comm and thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>