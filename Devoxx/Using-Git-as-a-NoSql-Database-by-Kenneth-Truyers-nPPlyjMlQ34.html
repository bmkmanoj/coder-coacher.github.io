<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Git as a NoSql Database by Kenneth Truyers | Coder Coacher - Coaching Coders</title><meta content="Using Git as a NoSql Database by Kenneth Truyers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Git as a NoSql Database by Kenneth Truyers</b></h2><h5 class="post__date">2018-04-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nPPlyjMlQ34" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I'm gonna start first of all thanks
everyone for coming to this talk on
using git as a no sequel DB I know
there's like a lot of other talks going
on that are done by very small people
with things that make a lot of sense and
you guys chose to come to the crazy talk
so thanks for that and when I talk about
using git as a no sequel DB the first
question I really get is why why would
you do that
so to give you a bit of background I
work at a company called IP parking and
what we do is we digitize all the
roadside markings so that means like
every line on the road every parking
sign there is all the rules all the
restrictions and we put that into an app
and now also into connected vehicles now
the problem is that all of that data is
not readily structured so what we get is
essentially CSV files Excel files plain
text files shape files any data that you
can imagine and then we put it into our
system now the problem is because these
are like massive files you can't really
know whether that data is actually
correct and to view it you have to put
it into your application but then when
it's inside your database well you know
how do you get it back out so one thing
that people usually say is like oh yeah
you have like a version number on your
row and then you can revert what we
actually need is like going back the
complete version of like the whole
database so in the beginning what we did
is do roll backs have a backup for your
database import and then roll back now
that takes quite a bit of time so we
started building like a system on top of
as your table storage and as your blob
storage to then be able to revert that
but it became very complex very quickly
and we noticed that well what we
actually need is something I get where
you can do like a rollback in second and
then do a roll forward and on top of
that you can create branches where you
can do an experiment with your data
that's when we started looking and to
get and obviously the first thought we
had was like that's ridiculous why would
you do that and we started building on
top of it and we noticed that it was
actually possible so I want to start
structure this talk into three bits
first is the mechanics of get as a
database so how do you actually use it
as DB then I want to talk about you know
what the advantages are and why it's
really a good idea
and then lastly why it's a terrible idea
and you should probably just forget
about everything I said previously so
first a few disclaimers in this talk
when I talk about a database I'm not
necessarily talking about sequel or
Oracle or any of these like big
databases I'm talking about anything
that is structured data held in a PC or
in a datastore and the second term is no
sequel no sequel is very overloaded term
but in the context of this talk I just
want to talk about it being scheme Alice
and non-relational so let's get started
so what was the first attempt that we
did and failed miserably so you could
simply create a new get database a new
git repository create a JSON file save
it into one dot JSON add it and commit
it and then you can get it back out cool
so now you have a database what you
actually have is just a file system
would get on top of it what are the
problems with this well if I want to
write through multiple branches at the
same time how do I do that that means
that every time before I write I first
have to check out my directory save the
file and then go back to the other
branch that works if you have like one
dot jason but if you have like 10
million don't chase them then you start
getting into a problem because every
write is like switching between these
branches on top of that you also have
all of your data duplicated because you
have the various different directories
and next you're actually writing twice
because first you're writing to the file
system and then you're writing to the
gate database so that doesn't work so
let's go back a bit and talk about how I
get repository looks so on the client
side we usually have like our checked
out directory where we have all our
files and then you have the dot git
folder so this is what we know from code
and then on the server side we have what
is called a bear repo so that's
essentially everything that's inside
that git folder but just that and we
don't have the actual files because that
git folder contains all of your files
and all of the version history of those
files so in theory we could do with only
the right side that is shown here so
let's look at the good data model and
we'll start with what's probably
familiar to those who've worked with git
on the top we have the branches so a
branch is nothing more than a reference
to a commit and this then has relations
to commits that came before so that's
how git builds up the historical model
by having a relationship between all the
commits now this is what we see as
programmers so that's the top level and
we work with that constantly what we
probably don't see is what's underneath
that sorry commit essentially points at
a base tree and you can look at that as
your your base folder your root folder
that's your top tree and then underneath
that you have blobs which we can look at
as files or other trees so every commit
points to a new tree object that
represents your data and then all the
files underneath and it consists
actually of two databases so on the top
part we have what is called a reference
database and on the bottom part we have
the object database so the reference
database is a very small very simple
database which has mutable structures so
you can change master you know by just
dating the committed points - and on the
bottom the object database is actually
an immutable store that means that you
can only write to it once you've written
a tree you can never modify that tree
anymore so it stays always there and
that's how git keeps track of all of
your history by never changing anything
and just adding on top of it the git
tools that consists of two types of
commands you have the top-level commands
the porcelain commands and if you look
at those and your work would get either
on the command line or true UI tools
you'll probably be familiar with a lot
of these like I get commits git merge
rebase pushing pulling those are all
like top level commands of get
underneath that or what's called the
plumbing commands and the plumbing
commands is what get uses to actually
write the files into the database to
make sure that you restore the correct
files and they compose those to build
the high level porcelain commands so
we're gonna have to use the plumbing if
we want to really work with git as a
database so let's start with the lowest
level so the first thing we want to do
is save a file so normally you do that
by saving a file in the file system and
then telling it to add it to its
database now we want to skip that step
of writing a file and we want to write
directly to gets database so the way we
do that is by echoing adjacent file and
putting that in to get hash object and
that gives you back a hash and this hash
will always be the same because it's
built based on the content so if you run
this on a different PC you will get the
same hash back now I can read that file
back out by passing in the hash or short
version of that hash and it will give me
back the Jason so now I have a key value
store I can put in a value give me back
a key and then I can later retrieve that
by that key
another problem is that when I change
that file later I'll get a different key
so as a key value store that's quite a
late lousy solution because my keys keep
on changing so I'll need to go to the
next level to the trees and I need to
write a tree which is a bit of an
obscure command but I'll tell get to
update an index I pass it the hash of
the file that I want to put in there and
I give it a file name Wando Jason and
then I write that tree into my database
that gives me back another tree which
has a hash and a pointer to my blog so
again I can get that back and I will
receive the contents of the tree and
then I can see what the hash is and then
retrieve the file again now the problem
is when I change the file the hash of
the file changes I need to change the
tree again and I mean essentially in the
same situation that was before so next
level up commits so how do I create a
commit I echo a commit message and then
pass that into commit tree and I pass in
the tree that I want to commit and that
then gives me back the commit hash which
you've seen either in UI tools on the
command line this is that same
combination so at this moment I have a
commit which points to a tree with one
file if you look at this in a normal get
repo it would just be one thought jason
with a commit message I can do a git log
on that and then you'll see that I have
indeed added one file with that commit
message that I gave it and now I can say
like for this commit give me the
contents of one dot jason and get will
run through the entire hierarchy
and give me back the contents now how do
I update it you would say that I would
update that blob and then everything
updates itself that's not the case would
get like I said before anything blowed
the commits the trees and the blobs are
all immutable so I can't change them so
how do I do that well again I'm going to
just echo a new item a new blob with an
updated name I get back the ash write a
new tree and then I create a new commit
now what's different with before is that
I pass in the - P parameter and say like
this is the previous one so now it
creates a new commit but pointing at the
nutrient I created but also pointing at
the older commit and that's the way that
gate can keep track of those files
because it now sees a tree that points
at one go chasin where the four had a
tree that pointed at a different one or
chasing so that's the way it can do
diffs now I can do a log again on that
last one and you can see that I have to
commit messages with twice the file one
dot Jason that was added or changed with
the new commit I can now do a good show
and I get my updated content I can still
get to my older version by just doing a
good show of the previous commit and get
the original content so if I do this as
a database that means that every right I
can go back to what the previous version
was now we're still not there because I
still need to remember which one was the
last commit so the way we do that is by
creating a branch and I could do this
through the porcelain commands or
through the plumbing commands which is
just get update ref and
I tell it's to point master at that lost
commit that I have so I've got their 991
and I just pointed at that and now I can
simply do get your master of wonder
Jason and I get the updated file so
that's all nice but you know these are
all she'll come on so what am I supposed
to do like go out to the shell every
time when I'm when I'm creating data so
there is something called lipcott - and
essentially that's a sea implementation
of all the get tooling now another C
programmer I'm a C sharp death so I like
to do that in C sharp and then there is
a library called lip get to sharp and
that provides bindings on top of lip kit
- there's also bindings for any other
language Java or node and a whole bunch
of other ones so if you want to access
that you can do it using these bindings
so I'm gonna show now how you would do
this in c-sharp it's very similar to how
you would do it in Java or note so it's
just showing you a bit the flow of that
so for a database a key value store what
do we need a get and a safe so I want to
save something giving it a key and the
value and I'm given that key I want to
get the value back out we'd get I need
one extra thing and that's which branch
I want to save it on to so the first
thing I'm going to do is initialize a
bear repository so using let get to
sharp that's simply passing in as bear
equals true and now I can start writing
my blob so as before I first need to
write that blob and there's a bit more
infrastructure code here I'm taking a an
object of T and then serializing it into
a jason and putting it into a memory
string and then I can create a blob and
it will give me back a blob object which
essentially has the Shah and
properties so next step is creating a
tree I get the current commit and then I
get the tree let that current can be
disappointing it and I add to that that
window chase and file and then I created
so again gives me back a tree with the
hash and everything on it so the next
step is creating a commit and a commit
has all the things that you would
usually expect like an altar commit
message and then the tree that it's
pointing to and then parents so before I
said like a commit has a single parent
and get it's actually possible that it
has more parents when you have merges
but I won't go into that for a moment so
just pointing at the current commit and
then lastly I need to update my branch
to go to that new commit so I get the
current branch and update the target to
that commit ID so now I've changed my
file using that save method and
eventually I'll just return the SHA or
the hash of that commit so I didn't get
that file
well again you get the branch then you
go to the commit get the tree get the
file and read the text and then I
deserialize it into an object so that's
essentially the the two methods like if
you want to make this robust there's a
whole bunch of other things that you
need to do but essentially this is what
allows you to work directly with the get
database bypassing the complete file
system so that's all nice but you know
we're actually you know we want to use
this and not rewrite this every time so
what we did is we built this whole thing
and put in all the extra checks that are
needed and open source this so you can
find this on github and you can download
it play with it
so I want to switch gears now a bit
we've seen now how you would do it
obviously the next question is like why
should you do it so the first thing I
mentioned in the beginning get the
schema list you saw that we just put in
JSON files so we can modify those
without a problem so if I have a user
class I can modify it to have extra
properties remove properties whatever
you want obviously this is not
particular to get any no sequel database
tends to have this what's more
interesting is the versioning and
rollback which is why we went down this
part in the first place so as I showed
before I can now do a good show of the
master version of one rotation and I'll
get my file back and if I want to
rollback
it's simply rolling back the branch and
I get my original document back now you
could do that with a database like if
you have a backup and you simply roll
back to that one but this takes about a
couple milliseconds if you need to
restore a database from a backup that's
gonna take a while and in that time your
database is essentially offline or you
know either one of the versions so we'd
get you can do this in instantly
essentially another cool trick is
deafening your content so what happens
if in a database you know you have
different versions of rows in your in
your table how do you then get the
difference between those well you'd have
to build something yourself to do that
we did it's all built-in I can just do
make it between those two commits of
Wando chasing and all tell me exactly
what has changed a trick here is I've
now output to chasten onto one line but
if you do it in a indented format you
can actually see exactly where the
things changed so that allows you to see
like what happened to my data
who modified it because you get to
commit messages and you get all the
exact differences in your data backup
and replication so it was written some
backup schemas like for sequel or Oracle
or something like that you know that
it's not quite that easy
first of all things you have to do is
take incremental backups you have to
think about how long do you do retention
of your backups so for example you take
daily backups but then once they are
older than a month you only keep one so
it becomes very complicated now one
thing you don't need to do would get us
take incremental backups because it
already has the complete history and you
can already roll back into any version
so you don't need that now how do you do
a backup well you just add a remote to
it and you push on all your data and
they are get automatically takes care of
you know only pushing up the difference
to that remote repo so and you can
create multiple remotes and just push
them there so we have for example our
main databases on our own servers but
then we created a new github repo a
private one and just pushed up to there
and you can do that with multiple
repositories and have backups all over
the place with two lines of code
how do you do replication then well in
get you have something like hooks so
that's a small script that runs every
time you commit so on the post commit we
just do a push back up or push
replication so that means that every
time we commit to our local or our
server repository it immediately pushes
up all of those changes to a remote
repository that means that this script
takes care of all the replication we
need yep
yeah well the idea is that you have one
main repo which is the only one you use
- right - and then the other ones are
kind of like slaves so if you're right -
boat yeah you'll get into geta yeah I
mean we wouldn't force push on our own
data no also like to your main repo you
actually don't push because you only
write locally to it and then you
replicate it out to another place so
what we've done is on top of a
repository that sits on a server we put
a REST API on top of it so you always
write to that one and it writes to that
single repository and then you replicate
it out
another advantage or transactions and
the first one are very similar to like
the transactions you see in normal
databases so you can write a whole bunch
of files then update the trees with all
of those files and then you either do
the commit or don't do the commit so
that's the same as in get adding
multiple files in a and it's um in a
single commit so at this point you can
just do nothing and nothing will be
added or you can actually do the commit
at that moment and this is a atomic
procedure so all the files will get
added at the same time that's very
similar to what you have in normal
databases are not very spectacular what
is quite nice is that you can also do
long-lived transactions and when I talk
about long list I talk about the
transaction that lasts a week so imagine
like you get in some data you do some
work on Monday of adding it then on
Tuesday then a bit more on Friday and
then but you don't want that data in
cite those intermediate periods you
don't want that data to be live rather
you would build it up and then when
you're done with it and you've edited
all of it then you want to put it
immediately into your live data as as a
single atomic transaction so you create
a branch called transaction you do a
whole bunch of commits on it you know
you can do whatever you want on that
transaction you can roll back roll
forward do whatever you want
you go back to your master and then you
just merge it back in and this again is
an atomic transaction so it just goes in
immediately and all of your data is live
in one go so that's the good bits about
it so now let's see why you probably
shouldn't do it so I talked a lot about
writing to get but how do you get your
data back out because data bases are
read and write so you can do queries by
key or by key prefix sort of this is not
very different from other key value
stores a lot of no sequel key value
stores have those same properties but as
before I will go into the more get
specific things further on so what's the
solution to that well if we can't read
from it then why don't we just write to
it and read from somewhere else so this
is what you know normally it's called a
CQRS system where you're reading your
right side are completely separate and
in our case we've done this where we
actually write to or get database and
that's our source of truth and then we
have a secondary database where we read
from and this is a thing that happens in
a lot of CTRs type architectures there's
one advantage here would get is that you
have committed
so what we do is we just write to get
and then we have a small script that's a
commit hoop that then triggers a
denormalization of get into
elasticsearch in our case but that could
be sequel that could be any other no
sequel database or data store that you
want the next thing is concurrency so
what happens if you start writing to
your database with multiple processes at
once so what happens if we add multiple
blobs at the same time well that's
actually fine because a blob is just the
hash of the contents so we can do
multiple connections at the same time
writing same the same blobs and I will
be fine there will be no concurrency
conflicts with that now what happens
when we start writing multiple trees at
the same time so let's see how that
works so the first thing I do is I copy
the tree which has a reference to blob a
and I add another one to it the next
thing I need to do is create the commit
that points at that tree however because
I'm in a concurrent state what actually
happens is that someone comes in and
creates a new tree it goes back to the a
blob because it takes a copy of the tree
a which was there and then it adds a
blob see so what happens is that I lose
that intermediate blob something similar
happens at the commit level so if I
create a commit I'll point it to the
previous one and I find that previous
one by checking where master is pointing
to but now before I can move master
forward someone comes in and creates
another commit so I take a reference
again to that one and then when I move
master I'll lose my reference to that B
commit so this is not
not as problematic as the problem with
the trees like if you with the trees if
you make a mistake while you actually
lose files here the thing you lose is
history so it's still not good because I
still want that history but you don't
lose the contents at least so what's the
solution while locking we simply what we
have done is we serialize all the rights
to a single branch so you can write
concurrently to multiple branches that's
no problem because those commits and
trees are all completely separate but we
do it on a branch level and the next
thing is performance we all know get to
be very fast but I don't know about you
but I don't create a thousand commits
per second when I'm writing code when
you're in the database scenario that
could happen though so we did some tests
and this is what we got to 125 writes
per second now I would argue that there
are definitely applications that need
more than that but there's also a lot of
applications that actually don't need
more than that so when I think about
like a wiki or a blog no one is gonna
write 125 blog posts per second so
depending on your application this could
actually be sufficient but it gets worse
because this is one I have 10,000 files
in my database what happens if I have a
million files in my database
we got 218 writes per second now this
starts to become a little bit
complicated because there's a lot of
applications that are not happy with 18
writes per second so to find the
solution first we have to know what is
actually the problem so suppose we have
a get repo which has a single commit a
single tree and then under that tree we
have a million blobs so if we write a
new blob to it so our tree looks like
this
we have the hashes of Wonder chasin up
to $1,000,000 chasing so the first thing
we do is write a new blob that's as fast
with one existing blob or ten million so
that's not an issue but now what we have
to do is we have to copy the tree so
that means copying a document with a
million lines and you can see that the
bigger that tree gets the longer that
copy operation is gonna take and that's
why we see such a drop-off in
performance when we go from 10,000 to a
million files and if you go beyond that
it drops off even more so then we add
that file to it and then write a tree so
what is the solution we did a bunch of
tests of that and there is multiple
solutions to this all contributing a bit
to the right speed so the first thing is
tree nesting so the previous example I
gave you would be a single folder with a
million files if you do that in Explorer
or you know any file system you also
have a problem so what's the solution
they are well doing tree nesting so now
rather than having one tree with a
million blobs we have three levels so
100 times 100 times 100 is 1 million we
still have the same amount of files we
just have it nested into 3 sub
directories so now if we add a file what
do we need to do well we just need to
update these trees so rather than
copying a single tree of a million items
we actually copied three that hold 100
which is a lot faster and when we did
that we saw that for a million files we
got back up to 250 rights per second
which is actually quite good and when I
say rights per second here what I mean
is actually commits per second if you
want to go in faster what you could do
is write multiple files in the single
commit because we saw before that you
can write files
currently as much as you want so if you
do it that way you get even to like a
thousand or a thousand five hundred
rights per second that depends a bit on
your file system as well and the
hardware that you have below but those
numbers are actually quite decent and if
you want to scale this further up one
thing you could do is partition so for
example if your data is partition Abul
you have a database per customer or a
database per country well if you can get
250 rights per second on a single repo
then just create multiple repos that are
on different disks and you can multiply
your rights per second so that's only in
the case when your data is partitioned
well sometimes it isn't and you can't
use this and then another solution that
you could do is change your back-end so
get by default rights to the file system
and that's on disk drives but lip kit
and lip get too short by extension has
the possibility to create a different
back-end so rather than writing to the
file system you write into a reduce or a
memcache elastic and you could add to
that because it's a plug-in system and
that means that you can actually scale
out your get DB over multiple servers
that automatically index the bits of it
one particular disadvantage here is that
with a normal and regular
disk based get repository I could just
simply remote into the machine and run
the get commands on that if it's an
elastic search for example I don't have
that possibility anymore I would have to
always pull the data and work on it
remotely
then merge conflicts so very often your
mergers go like this and everything is
happy and then you have these ones and
it's a real problem
and the solution we don't really have
one so what happens if you have like a
merge conflict of 10,000 files how do
you solve something like that what we do
is simply make sure that we work in
separate areas of our data so our data
is geographical data and we make sure
that you know you only work on regions
in a branch so for example I would work
or my colleagues would work on the
London region someone else in Manchester
and that way we assure that there is no
data conflict we have had some conflicts
though when people didn't listen and
still did it in that case one of the
solutions that we talked about was well
if we have a merge conflict rather than
solving it at that moment why don't we
just commit the entire merge conflict
and then when you read it you can
actually say like hey we have a merge
conflict in our database here are the
two different versions and then present
to the user which one do you want it has
a lot of complications as well because
what do you do if you know you're an
end-user and you just want to see the
data and you don't know which one is
right so there are some problems there
and you have to be careful with this
when you start creating a lot of
branches so one do I think using git as
a database is a good idea
it's mostly with content-heavy
applications so for example a CMS or a
wiki for example on github I don't know
if you've used the wiki feature there
you can pull those wiki's down and it's
actually a git repository on the back so
they actually already using git
a database as well and that works
perfectly fine for these because you
don't have all the issues you normally
have when you have lots of Rights
because no one is gonna create thousands
of wiki posts in in seconds and another
good opportunity is when your data is
partition about so even if you have like
a high write requirement you need to put
a lot of writes in at the same time as
long as your data is partition able you
can still do that you just create
different repositories and you multiply
your write speed by the repo one should
you not use this well as I said when you
need fast writes if your data is not
predictable and you need to do 5,000
writes per second then don't use this
because it's not gonna scale up to that
and another problem is immediate
consistency so as I said before like if
you want to do queries well you're gonna
have to denormalize into a read store
it's very fast to do that when you do
commit it typically takes like half a
second a second before those changes or
then replicate it in your data store
depending a bit on what the change is
but you do have some eventual
consistency there so that happens in a
lot of systems if you use git as your
primary data source well you won't have
immediate consistency so let's have a
quick look at a demo</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>