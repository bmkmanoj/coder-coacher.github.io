<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Cars, Big Data - How Streaming Can Help Formula 1 by Tugdual Grall | Coder Coacher - Coaching Coders</title><meta content="Fast Cars, Big Data - How Streaming Can Help Formula 1 by Tugdual Grall - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Cars, Big Data - How Streaming Can Help Formula 1 by Tugdual Grall</b></h2><h5 class="post__date">2017-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R0eI58gL7Ig" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">per game we play get into a place yet so
I'm tagging octagon I'm working at mapa
and today I will talk about fast car on
Big Data the this presentation we have
build that with some of my colleague
because we have been asked by one of the
Formula One team to help them to define
what could be the next architecture for
their telemetry tools they use because
most of the technologies they use today
in in all things very close to us close
protocol very proprietary in terms of
frameworks and they are looking for new
stuff to build the next generation of
their application so we build the
demonstration to show what could be the
next architecture unfortunately this
team has not yet made the decision to
work with us that at least I was able to
build a talk I want that so before we
start I'm working as a Technical
Evangelist at mapa
one of the big data platform you can use
on before I used to work at MongoDB
coach base exalted is a social network
for enterprise on Oracle so mostly as
you can see from a software vendor point
of view on mostly from data and
middleware technology when I was at
Oracle I was working on the Java EE
container and I also built with some
friends as a Java user group in not
where I live and as you can guess from
my accent if you don't know the city
it's in France and you have my contact
info and if you want to know - Brielle
is not French name it's a Celtic name
from Brittany's from what time so on
everybody call me tag it's a lot easier
to remember so I'm starting this
presentation with what means data in the
context of motorsport on then discussing
about the current architecture
and what we can build
and doing the demonstrations on the
discussions about the account framework
we use to do streaming and store the
data on process the data so I'm working
from a path that we provide on this is
only product slide that is not an
open-source project completely is we
provide this platform where we have the
storage file system database and no
secure day today is using jeezum HBase
API on even streaming and when I will do
the demonstration this is what I use to
store on process so it's a part of spark
cql on the distributed file system on
the database so the first thing is
what's the point of data and if you look
at a Formula One race on many of the
Motorsports you see in the paddock you
have these many many many screens
providing information to the engineers
or to the pilot but the cars the engines
different cars and the weather and so on
and it's used for many things it could
be used to track the health of the car
but to define strategy to analyze the
behavior of a car compared to different
level that have support of the same team
so they are the same car more or less we
could very often say we use all this
information to adjust the setting of the
car and some example will be here you
see multiple charts but the red chart at
the top is engine speed rpm the blue one
is the speed in kilometer per hours and
you have the g's lateral G's in green
how do you press which gear are you what
are the pressure in the engine on many
many many informations that has pushed
back from the car in real-time into the
platform into the tire but also all
these data most of the time on normal
you want to keep them for many things
for many reason and allowing you to
compare the behavior of the car based on
some settings on that you can capture
information in real time allows a team
to define some way strategy for example
when do you want to refuse the car so
impact on the speed on the weight of the
car the behavior so you have same car
but same team two cars with different
strategy about option air object B when
do you want to review what will be the
impact on the speed on the track and so
on so many information simply about
capturing analyzing and deal with it
knowing that it's not authorized to have
an engine complete engine somewhere to
analyze the data influence back to
setting in real time yet something that
is as a driver of time changes behavior
of the cars to stop in the paddock and
they have to change something because
technically we could almost lead to
drive on a racetrack right itself
without any driver rate so what is
interesting is to look at the data you
have on the data that it's are used on
generated during a wave or doing a
training so you have these numbers come
from 2015/16 some of them so probably
more information because you can add
more north of every day in car so 300
from cells in a car we saw some of them
on the chart to speed engine G's and
many things occur on gears and so on and
two hundred two thousand channels that
will be communicating in the car back to
the paddock to capture and some the
information and data are some to the
paddock in real-time
every me in two milliseconds it's a
specific network on the side of the
tracks that is used and for a ways it's
1.5 billion data points that are
generated by one car
on five million for our food ways full
weekend so Fridays at 5:00 for the best
Saturday for the test on Sunday for the
day of the race so it's between five and
six gigabyte of compressed data for a
car for ninety minutes driving switch a
lot and what is interesting is to see
the total number of data that are
generated for one weekend it's 200 on 14
so it's 2/3 - on 2,000 in the 2015 15 so
it's 253 terabyte of data that has been
generated during as a US chrome cream
and generating the data is easy the
sensor are here on the generates the
data capturing the data storing the data
processing the data stuff to be
challenging but more importantly keeping
the data forever in a useful way to be
able to compare what was the behavior of
the car in 2014 15 16 try to compare
with all the training you do different
ways imagine it's three days of places
to another 43 terabyte so one of the
goal of the new architecture is to be
able to keep everything at least all
these data and forever in a cheap way
so storage is not that expensive but
what is expensive it's to have storage
that you can data store that you can use
to do analytics to do machine learning
to do comparison a bit between different
stories and based on this we choose to
build an architecture that will explain
with new framework how you can build a
new type of application so the goal is
in the car you have a specific link on
the global link on the track that will
sum all the data for the car and then
you send the data to each team so like
that you have a single point where you
can capture and store all the data but
each team will have some information and
for each team
one of the goal is to deal with the data
in real time on the track so this is
when you see engineers this will be
directly on the track doing so local
analytics on local storage and it could
be interesting depending of the
technology used in this case we say you
can for example in real time take the
data from the track and replicate them
in real time into the factory in UK in
Italy in France depending of the team
but keeping the data and processing and
summing the data in real time in this
case it will be some data store on
processing layer cleans on the track in
the paddock for each team but also the
same architecture on tools in the
factory so as part of the process and
base of the volume of information they
provide we define a simplified demo for
this where I don't have a formula one at
all and I'm not sure I could drive it
even if I add one so we use a simulator
to generate some data and we use a
streaming technology to send the data
and show two things data processed in
real time into a dashboard and data
store into not secured engine in this
case to do for to do analytics so this
is a first part of of the demo and for
this we use talks talks it's as a name
state open source racing car simulator
and it's a very interesting framework so
it's an open source project available on
SourceForge developing C and C++ and in
terms of volume of information is not
that exciting I could not generate
terabyte of data or unread of gigabytes
of data for 60 minutes on these kind of
things but what is interesting it's a
very good
physical model so this tool is used a
lot for artificial intelligence or
people that want to simulate speed
breaking the GS and so on so I use this
as a tool to generate data on so this
architecture let's show you the demo so
the simulator is running in this
specific VM so we'll start on your wave
and I didn't enables any game or
interaction it's just life is happening
by itself and what you see at the bottom
is you see for the first one of the cars
a car that is in a web so the resolution
I don't know into the internal number
seven I think you see is a gear the RPM
so 77,000 so speed the break on the
throttle and also the GS on the
different matrix at the bottom so what
happening here it's something data not
only to the game but also to the data
platform in real time using a streaming
technology so let's go here and this is
the data generated directly by the car
something into a UI so I use in this
case quite simple WebSocket listening to
a specific topic and my lap is maybe a
little too long because one of the very
interesting things people compare
especially during training but also
during waves it's a behavior of the car
obviously in a real time when you can
put a left on this kind of things so
based on why on rpm you see a car that
is a lot lot faster in terms of number
of rpm it's because if you look at the
simulator you have a car that look more
or less like a Formula One so the engine
itself may be will be seek the red one
as you see it in the back sometime so
this engine is a lot lot faster in terms
of a key and not necessary in terms of
driving as you can
but what is interesting in this for
example is to be able to compare tulips
so you see here this is just by time
overall of the waiting at the top or
this is default lap so every time maybe
some signal rpm as yes this is every lap
and you will be able to compare the
behavior of the car from one first lap
second lap and so on so be able to
compare the behavior of the driver of
the car on analyze what will be what we
have to do in terms of refuelling
changing some braking on some breaks and
so on so not only here I am generating
on pushing data in real time so if we go
back to a smaller architecture slide
this is simulate risk our simulator
something data to restraining
technologies that push in one consumer
that at the top is a real-time dashboard
I don't store any data by itself besides
the stream and but also I have another
consumer that takes the stream and save
the data into a database and why I am
doing that
it's this is where you will keep the
terabyte of data for later use and I'm
doing it to our people to do analytics
and you have many tools to do analytics
but one of the most common language to
do that is cql and what I use in terms
of technology we show you the cql in a
minute also code by publishing data
consuming data is based on Casca so this
is a cafe API that I will go in a minute
so producing event and consuming event
the dashboard is a simple java WebSocket
D 3GS UI and I start Italian to map our
DB so it's a well as a no sequel
database for not are going to stop G
zone document and I consumed using
sequel using the project Apache Tejo
Apache rail it's a sequel on everything
engine
that can consume data from values data
sources so let me show you the heal
itself so when I say it either you to
consume many tiny type of data you see
at the top of class box we don't care
that I have the file system distributed
file system I can consume later from
HBase from high fork we do that is
another load of storage for Mongo and
you have many extension one of the big
things about the hill is allowing you to
do any type of query on any type of
content more or less in this case I'm
using the file system with a non-secure
database so one query for example will
be somewhere I have a table where I have
all the data that has generating that
are saved
so we'll do a select to get the car on
the right ID and go by car on race ID so
you see stored obstacle very basic
serial information so if I run this
query
I have a car the ID so I can for example
say Council number of car number of size
of each car so just basic theme so in
this case I'm doing also I want to do a
join with Jesus and five so I am
losting all my I will use this one and I
will show you the food statement
now we come back in a minute about cql
on how important keys even after 40
years of relational database in this
case you see I have use the Joe add in
the inner query I have a select star
home select field from all cast table
and then I do a join with a G's on file
that contains just a car some
information about the driver group by
and I do some calculation basic speed
for each car average speed for each car
and it's true that in many case you you
may have developed with any tool to
analyze this data but in many many cases
people want to be able to integrate that
with the reporting tool including very
technical data like that
in a racing team so it's a joint between
car one model on drivers coming from the
JSON file on miles per seconds on
kilometers parallels its path is based
on the data that has been generated in
this case we saw we stored G's on
documents on the table so we have
specific function if you are familiar
with CQL you may have seen this flatter
is just two and wind a different element
in the list energies and document if I
take only this query and I keep the week
of this way
one single roll is survived circa on all
the Sun so data so the flatten is just
to create kind of a reverse drawing to
kind of with each lead line as to be one
hole when you do the query and one of
the interesting part on why I'm spending
some time on Dale on sequel it's because
in many many case in front is this use
case it gives you access to a JDBC ODBC
layer with a powerful language that is
secure for calculation analytic on
analogy okay so the biggest question you
are if if you start to work on Big Data
project on Big Data project it's not
necessarily the volume itself is 20
terabytes of data big not necessary you
take any relational database any no
secure database
you will be able to manage 20 terabyte
it's not difficult what is odd let's say
I want to add one terabyte every day and
I want to keep everything this is where
it's becoming a challenge and this is
where you have to think about which
platform do you want to use to store on
process information and what we see in
the Big Data Platform is usually you
have two ways of selling the data if you
in this specific use case where you need
to continue to scale the volume of data
it's either the distributed file system
on very often on a secure database on a
very very very often you will mix the
two storage so whole data will be
storing to the file system as some
specific formatting could be text file
but very often you will optimize that
with a compressed format that could be
distributed on process entirely not all
the node of the cluster so this is what
HDFS
the Hadoop file system of not a file
system of RFS will give you the
capability of scale out as much as you
want distributed on replication
information and for example here we
could imagine instead of logging
everything in the database
begins that into specific file and then
you do the analyst analytics on this on
the cql the big big difference between a
file system and no sequel in this case
so database will allow you to access
very very very quickly one or few we
codes even modify them on a file system
if you have a two terabyte file to
modify one single line in the middle it
will be very expensive because you have
to find on you have to scan HDFS you cut
out 25 file on mato FSU card but on the
2.2 terabyte file you won't scan
everything to go to a specific line you
will probably use a no cql database and
the other benefit of using the no cql
engine on the file system schema is
managed by the application not by the
database itself because if you say I
have 200 on 200 terabyte of data every
ways we can and I want to keep such the
information for the last 20 years we can
guess as a number of information sent by
the car from 20 years from today into in
20 years you will have a lot more
information so you need something that
is easy to store and evolve over time in
terms of data structure so this is one
of the interesting part of metal Hadoop
and other big data platform so the key
part here it's how do you capture the
information and you want to do that in
real time when I call it fast car on big
data
I could I could say fast data hungry cow
Alaska because one of the Monro when I
walk it's all about moving to real-time
processing the data capturing the data
every time is generated so the data
stream it's about moving the event
moving the data it could be every hour
but it could be every second so it could
be every milliseconds depending of what
your system is doing and for this in the
Big Data world
Casca on many O's alone also that at
least when you look at what we do today
with big data it's really around Casca
Casca is the tools that you use on calf
cars being built by linking in 2011
implemented in Scala mostly and you can
develop applications easily to put
publish and subscribe information and
the key part Kafka has been built from
day one to scale out and be distributed
if you have whatever message every
minute you don't need after if you have
303 million message every seconds we
have a customer it's one it's 11 million
every second on the distributed on
multiple data centers but they have to
capture that and the way it works
classical publish subscribe so you have
producer in this case it's a tox
application on you have consumer and in
the middle you have a cluster of
something in this case it's Casca and
the way it works to make it scarier if
efficient it's like always when to talk
about volume of data that you want to
wade on right efficiently
you have to partition you have to divide
the datasets you use in multiple
partitions that could be this the
distributed on many machines so when you
are pushing a message into a topic this
topic will applicable partition and
usually this partition will be on
mythical physical machines so like that
you can write in parallel on make it
very efficient to scale out but in the
context of Kafka this data are all also
store where the broker is running so if
you look at the way it's working
you also need so you also need to read
in parallel and so I will come back to
the storage in a second but the way it
works when you have the topic with
multiple partitions you need to not only
be able to write in parallel on many
machines that you also need to read in
parallel on many machines so each
partition
could be read by one consumer and to be
sure that when you want to read one
topic that I have multiple partition you
we create what we call a consumer good
multiple consumer will read in parallel
from different machine and what is
important and this is why on a house
Casca scale each partitions are
independent we don't try to grantee the
order of grid on white between transit
between partition because what is what
is very expensive in traditional
messaging system it's all the
transactional path that when they have
to synchronize many many things to be
sure that it's read only one so they
want to be sure that all the message
arkad we're in the same order the ground
three of Casca is on one partition you
will always have the same as you we want
is the order that between to between two
partition you cannot so if we look at
the way we generate data with the
waiting car each car without it's not
its own Turkic but at least a car you'll
be sure you will be on a single
partition like that all the message is
done by one car will be always in the
same order but may be between two cars
you cannot compare exactly the data from
Kafka if you have the time stock in the
message generated by the car you can
start to work with the time stone so the
way you work when you deploy it is you
have multiple producers that you have
multiple workers because you feel how
you will do partitions partition will be
on each of the worker and you will read
out of light from these different things
so it's putting a small cluster for
heavily high availability in Casca is
three worker plus zookeeper sort we do
keeper three worker so what we have done
with Mota is we use API but we change
the storage so we use a Kafka a pi/2
publish on subscribe so like that as a
developer you don't have anything to do
but we use a storage of napa to do all
the partitioning replications on on all
these just to simplify the type of
topology and most of the time because
you want to integrate with the database
with the file system to do the
processing and I will show you some
processing in a minute
it's just one single cluster that can do
everything instead of having multiple
cluster so it's just using the same API
with a different storage so when you
publish a message you have a producer so
it's in a it's in Java and if you are
familiar with Kafka if you already use
cascade you will see that the name is
the first string up racing string this
is not compatible with Kafka as a name
because since we don't have a server so
big cluster is just a location where it
will be saved in the file system all the
logs all the event so topic is named
some sort data in this case but beside
that the code is just cast a curve in
the ordered line API for producing the
message and for consuming the message
you have a loop that we wait for
messages on a Vanita later on which you
can do some information so you see the
pickup value this is for example where I
push it to the WebSocket I save it into
the database so one of the key
discussion we had with the team is also
based on this because you use strata API
Kafka open API for the storage pure file
system on HBase or JSON database it will
be very easy to build new version of the
application
added new services and the first example
was to focus on the data capture more
information so today in the
demonstration using talked I use jeezum
to limit on consume messages given is
very nice for developer it's easy to
look easy to debug obviously when you
work in real life
with the Carbonari format very optimized
with almost very cryptic data that you
have no clue what it is
you decide if you have a specific
posture for that but at least you see
here for a car it will emit an event for
a specific way style on timestamp and
you have the some sort of space speed
distance on rpm by just adding a new son
selling the car for example you can
generate new data in this case you are
the throttle on the gear you see a total
32 and gear number 2 and this is where
it's interesting to have a dinner
flexible schema when you start the
information because everything that I
have created for example with drill with
my code in the you are doesn't know that
we have new data until I want to use it
but also when I insert data in the
database because the schema is managed
by the application itself we have no
exception so when you look at gnostic UL
don't look only for my world it's mostly
big data because we don't do small
cluster but if many application will
benefits of the gnostic ql part for
flexible schema where the application
drive the skin especially when you use a
document database Recker exists and the
part that is the most interesting it's
because of the fact that we use Casca
like matter stream to the producer on
the consumers are totally disconnected
like when you talk about micro services
when they talk about cookies subscribe
architecture so you can without changing
the existing curve add new service and
in this case the service that I want to
use for example it's a new service that
we do processing of the stream what I
have showed you wise now it's very basic
streaming moving data from one point to
another with almost no business logic to
change something of the process in this
case I just put the data in the
dashboard I served a time the database
stream processing is about keep
receiving the event an accident process
transform use some functions to
aggregate data and you just have to
create a new consumer when you cast a
consumer and walk with in this case what
we could imagine each to us the stream
processing that we
calculate on look at some other way to
lure compare what is a speed the
position on the track the engine
topology and so on and emit another are
safe some aggregated data into the
database every 10 minutes and so on
depending of what you want to do and
also if I want to switch from this
earthly dashboard to something new I can
add a new service without changing my
existing color to develop a new
analytics or pushing a dashboard
technology and this is easy because of
the messages you have on Casca so in in
the map are on Hadoop ecosystem what is
used the most is spark as a distributed
and computing system what if you have
the MapReduce al concept in the way you
want to build so your receive messages
you are new transform them into a map
you apply functions and you do that on a
distributed model in memory with
splittin on the disk but it's in memory
on this is one of the reason it's a lot
lot lot faster than Apache mately
MapReduce that was using the disk to
store all the intermediate state and one
of the benefits of SPARC is you have to
spot the streaming paths that can get
the data directly from the Casca message
or some other messaging layers unstruck
is the most used today in the context of
a duper author platform but you also
have apache flink the difference between
spark and flame is spark as being billed
as initially batch oriented approach
when you on your own job in parallel on
many nodes in a very efficient way
compared with apache MapReduce and the
swimming path has been added after where
flink has been built first to do
streaming receive the string process the
stream and there then I did some stuff
about that
they have similar capabilities at least
as an introduction we can say they are
very very similar in terms of adoption
spark is a lot more deploy than think
but I have built a demo with flink just
to show you the streaming on how easy to
I consume the information so the idea
here is to take the same I don't charge
anything to existing code and we just
want to create a new job this new job is
somewhere here
we'll listen as you can see it's a fling
cast a consumer 0-9 so this is a pretty
standard API for flink I have another
element just to show you how do you
initialize I do have a streaming
recreation of the unknown some
properties in the way you want to read
because you want to be able to read that
in parallel on many nodes because link
Blackstar you may learn that on many
nodes and you want to run that on many
node if you have a lot of big volume of
information and then what I do is I get
a stream object and this object I will
process it as first part which is an
element
goodbye key the key will be Sokka and do
a I will go back to the time window but
do a radius do a flat map so do the
other weights reducer on calculate the
average speed of the car so we start on
your way
and I will I don't deploy spark in this
case because I want to show you two or
three things that are quite interesting
design I start my spark application and
you see we have goodbye keys of the user
car then you have the speed and you have
the time window of five seconds so it
will calculate automatically to speed
for the last five windows five seconds
oh and in this case you have for each
car to speed for the five five seconds
you cannot you have many way of
calculating that if you want the current
speed at least from the beginning of the
job the average speed it will keep all
all the point and you have anything to
do you will calculate the speed of each
car all the time every time you have an
event and you are you have many options
in the way you want to deal with with
time on event in flink so where you deal
with windows windowing of edom so here
if I want to close to the dashboard of
the event that shows average speed close
to the to the chart I just have to
consume this data I've not done it yet
and I will because I'm modifying the
application so just adding a new service
is very easy in this case and flink as
an advantage of being very very easy in
terms of streaming technology processing
straining position
so what we what I showed you is when we
talk about this project it was really
providing a framework to build new
application so one of the first thing is
to be able to have a distributed on
scalable processing on storage storage
is a no secure data date on the
distributed file system the processing
is a mix of streaming technology with
Casca amato streams or and spark and off
link and you have many many many years
of tools that are able to consume
Casca messages to work with processing
ETL machine learning depending of what
you want to do so it's an open API and
open tools that were built and the key
part is to be able to decouple the
source of the consumer to be able to
create services and I'm not talking
about micro services here that it's
really to be to have each part of your
application being one single process
consuming the event imaging new event
and make that very very easy to add new
use case or even to do if you are I will
if I will take an example when I show
you this it's everything that I said
that Formula One you can apply exactly
the same architectural tools to many
many industries and I have to say that I
have not yet work with this in
production in Formula One or any kind of
motor sport but in telco finance retail
an IT content I personally didn't work
on tantas website not work on this kind
of project that if you look at the
finance if you look at all the
transactions each transaction a swipe is
you swipe a card to go on a website you
analyze the user each one event what you
want to be able to do is capture this
event its million of event every seconds
she will get a credit card company and
you want to be able to process so I
wanted to do a fraud detection a very
very basic thing that is done for many
years with
companies that do not need that but it
will be easy today which will be that is
the same credit card could not be used
in London and in Paris physically
point-of-sale unless if you have 30
minutes between London and Paris it's a
fault so you have to review that and you
have to be sure you analyze that quickly
enough to refuse the payment but more
importantly you want to be able and to
process the data in real time and be
able to compare that with some machine
learning model so in one side you need a
lot lot lot of data to learn this one is
a long process but you also need to be
able to apply the model in a real time
every time you have the event and this
is what you you do in a financial telco
also that the behavior of users and what
is interesting by deck opening the
source and the target of the consumer is
you have a Ford model that is a v1 of
the Ford model you want to be able
without changing anything to this create
a new v2 model consuming exactly the
same message and you will see over time
if you this v2 model is not efficient
that the v1 you keep it in prediction or
decommissioning so all the ad testing or
adding new feature of on the tool on a
service is very easy with the specific
architecture so I like to finish my
demonstration with my small toy it's
just the same demonstration that instead
of using the talked as a simulator I'm
using this honky device the you know
ocular drive you imagine that on the
Oracle booth and I will try to make it
work it's so whiskey bar so this cars
are connect bluetooth car and I will
connect them with a laptop
and I have energy s middleware that use
Bluetooth Low Energy to connect on
communicate SDK that we send some
information to connect set the speed and
so on so my I will start my server here
and you will sell all the event and I
will come back to Kafka using the HTTP
interface so in this case for now this
is this initialize just a connection
between my laptop and Bluetooth on the
card and then using because it's a waste
API to communicate with the car I will
use a very basic script because I have
not done all the UI to control the cars
where if you see it I will connect with
one of the classical with another car
turn on login to get all the data from
the caster's on up top connect to
another car and then put the speed so
let's do all this
if you don't say scoot down blue just to
say I am connected and they should move
on okay one two and what happened here
is you see the message at the top it's
all the message turning the cars onto
the laptop so what I do now I consume
this message on I push em to a Casca
cube and it's supposed to and this is
where it's okay
so you see one of the car is not showing
the data that you see the speed here of
scale on the battery level and if I
change the speed it should I should
control the car using this post
it's try to see the speed with the
castle go faster
and some the data faster so you see it's
a little faster than sending the data
and what is interesting with this the
reason why I use that is just more
physical than the software but also it's
an ID to organize hackathon to be able
to develop any type of intelligence you
want on the wait works you have almost
zero intelligence in the car a track
look like that inside the inside the
track you are this small information on
under also Cal you have a small capital
it's small canal so this is why follow
the same track all the time I can move
speed I can move track on all the
intelligence you want to build you can
do it from the application it's it's a
game of the street it's not innate for
only for hackers and all the
intelligence is a mobile application but
you have companies that explaining and I
have not done it yet how you can do
machine learning how you can do kind of
connected cars behavior with this very
simple track and what is interesting you
can even build and we stopped it because
the noise is bothering me you can print
your own track with this framework so
the interesting part here and if you
look at what I was saying at the
beginning where formula one you cannot
get the data in on women's back soccer
yeah I can do it because I can get the
data do some intelligence push the data
back and quite easy to doing in terms of
information and this specific tools that
is generating some paper track has been
built by a German company in automotive
that is building software to control
cars so I don't know exactly what they
do because it's still closing in terms
of what they do that they do machine
learning artificial intelligence in
automotive industry and they use this
too
test on stuff it was just a fun part of
it this is what is closest for me to a
Formula One so I'm not able to generate
the height of data yet if you are so the
source code of all the demos are on Vita
this one or the other and if you want to
learn more about the technology that I
use is an especially the architecture
globally you have three ebooks you go to
map out how many books you have what is
interesting about these books they are
small they're 50 to 60 pages so it's
very easy very quick to read and give
you a good idea of what you can do which
type of approaches for example in the
streaming architecture path we have a
chapter one micro-services how you will
use streaming technology to assemble our
website web services working
microservices working together one last
comment is as a developer as an
architect in an enterprise if you
believe you need this kind of technology
usually it's very easy to of you you
should test
Casca take an evaluation of Casca or
Mata stream a no cql database try to
understand what the benefits of the not
secure database if you don't do it yet
because as I said it's a flexible schema
part on the scalability it's very
interesting things when you want to
build you type of application some time
is just for time to market because
faster to develop because you have
flexible scans flexible schema sometimes
just because at scale it's a lot easier
to scale no cql database and a
relational database and what on you need
choose if you work with a lot of data
you need a way to process the data in a
way all time using training or not and
this is why I have put a link on SPARC
you choose on the market the most use
today stock
but it's interesting to learn one of
this technology to see how it works
because first of all artifice all the
bulk insurance starters are dealing with
large volume of data they need this kind
of skill and the biggest challenge for
us as a software vendor it's not to find
projects is to find skills to help
people to deploy on write applications
so do you have any questions on
everything I said no thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>