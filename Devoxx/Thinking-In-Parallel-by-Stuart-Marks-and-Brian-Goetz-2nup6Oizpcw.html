<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Thinking In Parallel by Stuart Marks and Brian Goetz | Coder Coacher - Coaching Coders</title><meta content="Thinking In Parallel by Stuart Marks and Brian Goetz - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Thinking In Parallel by Stuart Marks and Brian Goetz</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2nup6Oizpcw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning
wow this this room is incredibly packed
I think it must be because venket isn't
speaking you know I mean and this is a
talk on parallelism so maybe next year
they can upgrade to parallelism and add
two doors instead of using only one so
good morning my name is Stuart marks I
work with Java I work with work on the
Java platform my colleague and Java
language architect Brian gets is here in
front I am going to start start off the
talk give the first half of the talk and
then midway through we're gonna switch
I have designated a hashtag for
questions and comments
it is hatch devoxx parallel so if you
tweet with that hashtag we will see it
can't promise that we'll get to every
question in this session but we'll try
to follow up afterwards if there are
ones that are not answered in in this
session all right
this talk is thinking in parallel but I
don't want us to immediately start
thinking that way I think I believe that
the best way to think about parallelism
is not to think about parallelism
instead we should be thinking about the
fundamental properties of the code we
should think about the problem that we
are trying to solve and not about how to
implement that problem for decades we've
learned how to program using specific
styles and that has really influenced
well more than influenced and as colored
our thinking into particular ways of
implementation and that is working
against us so what I'm going to do in my
segment of the talk is illustrate this
with two trivial examples and I'm going
to go into fairly deep dive analysis of
those examples so this is the first
trivial example convert an array of
strings to uppercase and I'm going to
show this implemented two ways first is
using a conventional or an iterative
approach and the second using streams
are an aggregate approach
all right so here's the iterative
approach it's it's pretty simple right
we get an input array we allocate an
array for the result and then we have a
for loop that runs over the indexes from
0 to the length and assigns the output
result by calling upper case to upper
case on on each element to the input and
returns the result this is I mean it's
really simple so let's look what happens
when this executes so this is our input
array along the top we have the letters
ABCDE and then it converts the first one
to uppercase then the second then the
third fourth and fifth so pretty obvious
I think everybody can intuitively
understands this but I think it's
important to look at it at this level
because there's actually a lot of stuff
going on here so what I wanted to do is
tease apart some of the issues that are
going on in the code so we have a for
loop and the notion of the for loop is
we want to operate on every element of
the array but it doesn't say that
exactly what we do is we have to set the
indexes and the boundaries and then we
have to increment the index each time
and so that sort of mixed in with this
notion of operate on every element of
the array now with this for loop the
actual computation is sort of it's it's
buried in the middle there so this this
we're familiar enough with this idiom to
say oh yeah we're upper casing every
element to the array but that's that's
only because we're so used to it if
there were a subtle problem with the
index computations or something or the
boundaries were off then we might think
it says operate on every element of the
array when in fact it might not be so
we've we've there are these little
details that I'm talking about my next
slide here there are these little
details that are obscuring what we're
what is going on so if we if we look at
what a for loop is doing it is
inherently sequential and it is also in
a typical idiomatic fashion it is also
processing things only left to right and
those are accidental they are not
essential they're not inherent to the
problem that we're trying to solve
remember I said convert every element to
the array to uppercase there's nothing
that says it has to be done one at a
time there's nothing in that problem
statement that says that it has to be
done left to right yet that's the way we
wrote the loop because simply because
we've been doing that for years and
years and years that's the way we all
learn to program and so yeah well what
other way can you do it so there's an
observation here that's not in the code
which is that the computation of every
string being converted to uppercase is
independent of every other similar
computation so that's not particularly
important for this example but I think
you should remember that each
computation is independent of all the
others now Java has an enhanced for loop
that says a little bit more precisely
that we want to operate on every element
than a variety of a collection but it
doesn't help all that much all right so
let's move on to the streams example so
here we say we take our input array
convert it to a stream and then we map
each element to uppercase and send the
result to an output array and return
that so notice this doesn't say
left-to-right this this doesn't say
things are done in any particular order
because of the way we've implemented
this this is input this is this will run
sequentially it won't run in parallel so
if we look at our input array and how
the output array is created we don't
care about the ordering in in which
these operations are done or anything
like that in fact a better way to think
about this is to say this is not an
operation on each individual element
it's an operation on the entire array as
an aggregate not one at a time all right
couple trivial examples what do we think
about them so we can look at the streams
version and say oh yes the streams
version is better it's more compact
it's new and cool it's more functional
functional programming is good we like
that so that's why this means version is
better or we can say wow a for-loop is
better because you know streams have
some overhead so a for-loop is more
efficient the JIT can optimize it better
it's more familiar so we can understand
it more easily so if you're unfamiliar
with streams in lambda then the streams
code is not better and the for loop is
straightforward especially for such a
simple computation so I claim this is
wrong
every individual statement is is correct
you can argue about various subjective
things about those but I think this is
the wrong way to compare those pieces of
code because it's really focusing on on
all these little details instead our
claim is that the streams version is
better not because it's new and cool or
it's calm concise or uses lambda or
functional programming but because it's
operating at a higher level of
abstraction so this notion of the
independence at each computation on each
array element is expressed much more
clearly in the streams code and that's
that's not true of the loop code there's
there's less accidental complexity we
don't have to worry about loopy indexes
or incrementing their bounds or
off-by-one errors with the streams code
implicitly the stream code is saying or
operating on all the elements not on
individual elements and so what we can
do is this allows us to focus on the
problem statement allows us to focus on
the desired results instead of saying
okay one element at a time left to right
and so forth okay so this is talk on
parallelism why hasn't he said anything
about parallelism yet so notice I did
not say oh the streams code is better
because you can paralyze it now you
might or might not ever want to run this
code in parallel in fact that's what
Brian's part of the talk is about but we
claim that this code is still better
than than the for loop code and in fact
for the reasons I just stated because of
the better abstraction the notion of an
aggregate operation and so forth that's
why the code is better and
as a bonus the code is better but also
it can be parallelized if we want to
alright so let's move on to the second
example it's it's actually pretty
trivial but it's somewhat less trivial
than the first one it's the problem
statement is splitting a list let's say
we have a list and we have a predicate
that will match certain elements that
meet some that have some property and
the result should be a list that has sub
lists where that are are bounded by the
where the predicate matches so so the
sub lists have been the sub lists is
where the I'm sorry the original list is
split into result sub lists so you can
see the example there we have a B hash C
hash de so that the result will be sub
lists of a B and then a sub list of C
then a sub list of de so the the
predicate is telling us where we need to
do the splitting okay so I want to put
in a shameless plug for stack over them
wow that's that's the biggest and best
QR code I've ever seen so so this came
from a stack overflow question and
there's a there's a back story here not
going to go through all of it that's a
link to it and the QR code is a is is
the link to my answer to the stack
overflow question and I recommend you go
go read this at some point after this
talk but it's interesting because that
this is actually very close to the
question that was asked on Stack
Overflow and if you look at the other
answers many of them are wrong many of
them have many of them are more complex
than they need to be even the other
answers that are correct are much more
complex than at my solution in fact
considerably so so there's there's a lot
of a lot of interesting things going on
there Oh another thing is to Brian Brian
and I are both on stackoverflow and I'm
a few thousand points ahead of him and I
want I want to stay that way so please
go here and upload this answer so but
what this tells me is I said this was a
trivial problem but you know maybe it
isn't so trivial after all and you might
not think so
or you might agree after looking at some
of those other answers all right so
let's let's work through solving this
problem all right so we have our input
array a B hash C hash de now you have to
know a little bit about the sublist API
in Java a sublist takes the start and
end index it's half-open but it's a
little detail but basically it takes
indexes so conceptually what we want to
do is number every element of the array
so then if our predicate is equals to
the hash character then that's where the
split points are so we've we've what
what this tells us is that the sub lists
are between the split points but then we
have these ones at the end so what we
need to do is synthesize edges or split
points at each end and now we have the
result sub list that we want is between
these edges all right so we have to do a
little bit of thinking but it's not too
bad but if you look at the indexes the
way you get the sub list you want is by
starting at the element two to the right
of the split point and then ending at
the element at the end because the the
I'm sorry that's not quite right but
basically you do the index computations
based on the edges that were computed by
by where the predicate is on and look at
the indexes of the elements where the
predicate matches so anyway so we get
these sub lists and then we we coalesce
them into a list
and that's our result all right so let's
look at how to solve this using a fairly
conventional iterative approach so I've
so let's phrase this as method here
called split it takes an input list of T
and a predicate on T and so I've left
off the wildcards B which which really
should be here but they tend to confuse
things anyway so given a list of T input
predicate of T we want the result to be
a list of lists of T and those are the
sub lists okay so let's start off we
create our result list so we create an
ArrayList of the right type and well
let's see we we have we have the start
local variable because well if you think
through it we need to start somewhere at
the left so we need something that
starts at 0 what we're gonna do is we're
gonna work our way let's see if from
your point of view from left to right
and build up sub lists until we hit
something that matches a predicate and
once we get once we get that sub list
we're gonna put that into the result so
we start our for loop with curve being
the index of the element we're operating
on and then when we hit an element that
matches the predicate we take the sub
lists from our start point and the
current point and create the sub list
and put it into the results and keep
going through the loop and I think it's
interesting if convince yourself that
this code is correct think about that
for a while because there's actually a
lot going on so so here's kind of the
the the the meat of this algorithm which
is here's where we call the predicate
we're taking the current element and
calling the predicate on it and we're
doing we're changing some logic or we're
basing some logic on the result of that
test and then in the second line I've
highlighted here we are creating the sub
list with the right bounds and then
inserting that into the result and then
we have all these other other things
going on like we have this index
computation
we have the start variable which I kind
of stumbled over explaining because it's
sort of hard to explain it's sort of now
that we know what we're going on it's
the it's the starting point of the
current sub list that we are working on
or something like that so it's it's it's
kind of hard to explain and that that's
that's an interesting and telling point
so what else oh we had to remember to
initialize some state up front our loop
mechanics are exposed so we have our
usual for loop with index variable and
increment to the right place oh and then
also well we have this start variable
which is the the the the sub list work
we're working on and we had to when we
when we added a result in we had to
remember to increment that to in the
right way and that's easy to get wrong
and then at the end of the for loop we
actually have an open sub list that we
also need to put it to the result so we
we need to add that in after the end of
the loop and that's it's that's not
obvious now it's it's sort of obvious to
me because I've implemented this problem
about ten times and so you learn a lot
from a problem by by implementing
different ways ten times but it's
interesting and telling because like on
stack overflow you bang something like
this out and you make a few mistakes and
you run some unit a oh I forgot that
case oh that's right start has to be her
plus one not cur or oh gee I'm missing
something you think oh that's right
after the loop I guess I have to add add
this this open sub list so in fact when
I implemented this I forgot that the
first time and look I fortunately I had
written some unit tests and caught it so
it's like oh yeah I have to do that and
I had a puzzle over it for a while why
do I have to do that the end oh yeah
okay so so there's a lot going on here I
think I covered some of this before but
but I think the the main point is the
way I set up that diagram with with
problem where we created the edges by
applying the predicates to each element
and then deriving the sub list
boundaries from the position of those
edges that's almost invisible in that
code there's there's a lot of clutter
around it
index computations and so forth so if
you didn't have that diagram think about
what it would think think about writing
that sorry
suppose somebody else had written that
for loop and you were reading it think
about reconstructing that diagram from
the for loop you might have to pull out
a piece of paper and start modeling and
say oh I see it's building up sub lists
incrementally and the sub lists look
like this
and and that's that's a lot of work and
there's this problem here because
there's a large conceptual distance
between the problem and the way it's
expressed in for loop now there's
another thing going on which is that
there is a data dependency between the
results of every iteration of the for
loop and the previous one and that's
really important right because we're
building upstate we started with our
result list we started with this notion
of our current open sub list and we're
poking at them continually so when we're
midway through the computation suppose
we decide to do something all right so
we we make some change we create a sub
list and add as the result or something
like that that intermediate result
depends on all of the previous
calculations and all of the previous
calculations or each of the previous
calculations depends on all the ones
previous to it so the way we wrote this
for loop immediately constrains us into
a left-to-right processing model and
it's extremely hard to break out of it
but notice if you go back to the
original problem there's no such
dependency in the problem statement we
just said split the input list at the
places where an element matches the
predicate and if you think about that
you can perform that operation
completely independent of all the other
computations of the split points so this
for loop has done something very strange
to our algorithm because we created this
data dependency that's based solely on
the fact that we decided to use
iteration to implement it all right so
let's
reapply or re-implement this this
problem using streams instead so what we
want to do is avoid iteration but we are
interest
in indexes and so I'm going to use a
technique which is instead of streaming
over the elements of the array which is
a pretty common thing to do instead I'm
going to stream over the indexes into
the array or actually into the list but
I'm treating it like an array so that's
done by calling in stream dot range and
again the the computation of a sub list
edge is independent of all the others so
we can apply that predicate at that
point and it has no dependencies on
anything that occurs to the left or
right and so you know think about how
different that is from the looping
approach all right so here's that
diagram again so again we number the
elements of the array and we apply the
predicate to each one and that is where
our edges occur and then the sub lists
are computed based on being between each
edge all right so our outline is to
derive this we have a three step
approach here so the first is filter the
indexes to find the interior sub list
edges so those are in this case the
edges at two and four so that's the
first step and then the second step is
to synthesize the exterior edges at the
end and then once we have our complete
list of edges compute sub lists based on
this edge and the next one all right so
what does that code look like so we
filter the edges fill filter the indexes
to get the list of edges we start off
with an in stream of range over the
input and we filter it by calling the
predicate and notice we're operating on
the indexes so we just take the results
of a filtering sorry we apply the
predicate to the the element at that
location and then we collect the results
which are a list of indexes into an
array okay so that's pretty simple
now we I'm sorry we have to do something
a little ugly here because
is in Java it's hard to hard to do array
manipulations so in order to add things
at each end of the array what we do is
we have to create an array that's too
larger copy the original array into the
middle and then stick things at the ends
and so that's kind of painful but that's
that's exactly what that does but we're
gonna clean that up in a minute all
right and the third step is now that we
have an array of the edges we take each
edge except for the last one and map or
transform that into a sub list based on
where it is and where its neighbor is
and that's what this last range does
alright that's what this last stream
pipeline does all right so let me go
back and clean up this this actually I
was a little too fast so the problem the
problem is we need to add things at the
ends of the array and that's hard to do
in Java so what I'm gonna do is this
little trick which is in the first
stream I'm going to adjust the indexes
to be minus 1 and plus 1 and then I'm
going to alter the predicate a little so
that the ones at the end just fall
through and that that lets us get rid of
this gross array copying in the middle
so anyway so this is actually the the
solution that I ended up posting at
Stack Overflow and in fact this is so
after several several evolution steps of
that answer so it took a took a little
while to get to this point but I think
this is this is a really nice answer
alright so which which one is better
well again we claim that the streams
code is better not because it's new cool
more functional or whatever but because
it's at a higher level of abstraction in
looking at the streams code we're
talking about things like apply the
predicate to each element compute the
edge list derive the sub lists from the
location of each edge as opposed to okay
I'm working on this index here and I
need to establish loop invariants and
what the and it's certainly possible to
prove things correct using loop
invariants
but it's it's difficult for many people
and also it's hard to relate that to the
original problem statement an important
characteristic is that the streams code
treats all of the all of the
computations independently so again we
have an aggregate operation apply the
predicate to each element to the array
instead of stepping through it left to
right and we don't have to worry about
very many loop mechanics so a couple
observations about this this is a useful
technique which is streaming over the
indexes instead of streaming over the
elements lots of problems can be solved
by streaming over elements and but
there's a certain class that can't
there's a certain different class of
problems that B can be solved by
streaming over the indexes and so it's
it's another useful trick in the in your
in your bag of tools to apply for
streams programming so if you get into a
fight with the streams API sometimes
sometimes this approach might work so I
I think we ended up with a much nicer
solution with the stream approach and as
a bonus it can be run in parallel now
the question is should you run it in
parallel that's a different question and
I'm gonna turn that over to Brian to
answer that thanks George yeah so an SS
Stewart knob you know showed there are a
lot of problems that can be nicely
expressed with streams and one of the
character one of the nice
characteristics of the stream library is
that it allows for what we call explicit
but unobtrusive parallelism you can make
the choice of do I want this to do this
calculation sequentially or in parallel
it's explicit but it doesn't really get
in the way you don't have to change the
whole rest of you of the stream pipeline
so that's a powerful thing now one of
the things that we've noticed as we've
you know looked at what people have done
with streams is that we see that
parallelism often gets overused so what
I'm going to talk about is when should
you use parallelism with streams and the
answer might be a little bit surprising
so parallelism is
about using more resources more
computing resources more cores to get to
the answer faster right and it's
important to realize that this is
strictly an optimization you know if you
only have one core more resources aren't
available you can still compute the
answer sequentially and you're gonna get
the same answer
so if parallelism is just an
optimization then parallelism is only
useful if it actually gets to the answer
faster and that isn't always the case
you know just because we're using more
resources doesn't mean that the parallel
computation will be faster than the
sequential one or even as fast which
means that if we want to use this
optimization we have to do what we
always do with you know with performance
management we have to use analysis we
have to use measurement and it's often
an init of process so you know starting
guideline is prefer a sequential
implementation to a parallel one
certainly when you're getting the code
working start with sequential and then
maybe your code is fast enough and you
don't need to optimize it so for the
rest of the talk I'm gonna talk about
how to analyze a streams pipeline to see
whether there is a chance that
parallelism will help you surprisingly
you can if you understand the cost model
you can pretty much look at a parallel
stream and get a pretty good prediction
of whether there's any chance for a
speed-up or not so the way we measure
the effectiveness of parallelism is what
we call speed-up how much faster is the
parallel version and the sequential one
and clearly if the speed-up is less than
one you definitely don't want that
you're you know hoping to get a speed-up
that is you know large compared to the
number of processors you have you know
if you've eight processors you'd like to
get a speed-up of four five six seven
eight a speed-up of 1.2 with eight
processors is not necessarily going to
be an efficient use of a use of power or
resources even if it's faster so the
thing to recognize about parallelism is
a parallel computation will always do
more work than the best sequential
implementation and this should be
obvious right because it has to solve
the problem and it's doing extra
dividing the problem up forking out you
know different bits of the tasks to
different different threads decomposing
the problem combining the sub results so
it's always going to be more work to do
the parallel version what we're hoping
to do is make it up in volume we're
hoping that there will be an economy of
scale that says once we get a bunch of
cores busy and they're all humming on
parts of the problem then we will get a
speed-up after after we've paid off to
all the initial setup costs so in order
to actually have that we need a couple
of things we need a problem that is
inherently parallelizable to begin with
we need an implementation that extracts
that parallelism and you know as Stewart
showed you know if you start with the
for loop version of the split the split
the list into into bits it's going to be
very hard to extract the parallelism
when you're using techniques that are
fundamentally left-to-right you're gonna
need good runtime support for parallel
execution fortunately you get that for
free in the form of the fork/join
framework and the streams library and
finally most importantly you need enough
data if you have a small data set then
you're not gonna get any parallelism at
all because it's way more it takes way
more time to you know take a list of ten
elements
divided into two lists send half of the
list over to some other core for
processing that has you know some
latency and then combine the results
it's much faster to just you know just
operate on the ten elements in front of
you so you know III see plenty of Stack
Overflow posts of why is this parallel
version so slow it's like well they're
they're operating on out on datasets
with you know a few tens or hundreds of
elements there's not enough data there
to get parallelism out of it so I'm
gonna go through all these factors okay
so as my running running problem I'm
gonna use simple problem adding up the
numbers 1 through N and let's like think
about where the dataflow dependencies
are in this right so here's here's my
code I'm sequentially summing something
an array so I use a for loop updated a
mutable accumulator and return the
result the data flow graph implied by
that is a a very left branch e1 we
there the first thing I have to do is
add you know add zero to the first
element and then then and then and only
then can I add the second element and
only then can I add the third element
etc so that's not the data flow graph we
want what we want is a data flow
dependency graph that looks like this I
want to be able to do add the first two
in parallel with add the second two and
add the third two and add the last two
and similarly you know do that all the
way up the tree so the question is you
know what kind of code do we want to
write to get that pattern right so if
you look at the sequential version we've
already got a problem in the first line
we've got a mutable accumulator that
we're initializing to zero that is
generally a sign of you know a sort of a
sequential anti pattern that is going to
get in the way of parallelization so you
know we're used to coding like this
because it's so easy and you know it in
if we were running sequentially it
doesn't bite you but you know you start
coding like this way immediately you're
gonna have trouble with parallelism so
it's something we need to unlearn so the
standard tool for parallel execution is
what we call divide and conquer or
recursive decomposition the idea is you
solve a big problem by dividing it into
two smaller problems you solve the two
smaller problems hopefully concurrently
and you get a partial result two partial
results and then you combine the partial
results somehow to get the total results
and you can apply this step recursively
when I have when I've divided my big
problem into two smaller subproblems I
can do the same thing again
on one of the subproblems and get a
smaller subproblem and a smaller
subproblem and eventually I get to a
subproblem that's small enough that it
actually makes more sense to do
sequentially than to do in parallel if
you're adding up ten numbers the fastest
way to do it is you know the version I
had on the previous slide which is just
a for loop over them so eventually
you're gonna divide things down into
pieces that are small enough that it
makes sense to do them sequentially so
you know this is close sort of pseudo
code for that if the problem is already
small then solve it sequentially
otherwise divided into concurrently
solve the left and right halves
we get a left and right result and then
we combine them right so if our if our
problem is adding up numbers 1 through n
you know what's our combination well
we're just gonna add together the two
sub results so we add up the left half
we add up the right half we have two
partial sums we add those together
addition is associative so that works so
recursive decomposition divide and
conquer is nice because it's simple and
it's it's even simpler if the data set
you're operating on is already defined
by recursion like a tree there's no
shared mutable state there's no
accumulator
there's only partitioned reading right
so m'as doing things with with threads
and locks and I would you know had a
mutable accumulator I'd have to have a
lock that protects it and one subtask
wants to update that locks at wall and
then another one has to wait for it here
what I'm doing is saying work on the
left and right halves independently
nobody there's no two threads that are
working that are accessing the same bit
of data which means we don't have to use
locks we can use partitioning
partitioning is much simpler you know
i've if you have a lot of papers to
grade you know you fight you know
calling your grad students and you say
okay you do this stack you do this stack
you do that stack and then you go to
lunch right so it's very simple
partitioning is something that we you
know we do every day in the real world
and it's very efficient you know weight
a way to do come to do a shared
computation you want to one of the
benefits of this as it starts forking
work early right if you have 32
processors you start out only the first
one is doing anything you want to get to
the point where all the processes are
doing work quickly because every cycle
you spend setting up the problem is 31
cycles that are going unused in prop by
processors that are waiting for work the
other cool thing about divided about
divide and conquer here is it it tends
to to be self balancing the the
decomposition is is dynamic so if you
have a small data set it doesn't
decompose at all if you have a large
data set
and you have but you only have one
processor it doesn't add a lot of
overhead if you have a lot of processors
available you know think things balanced
fairly nicely with a half the code
having to think about how many cores do
I have what you know what size chunks
should I divide this into so the
recursive decomposition code is
independent of the number of cores I
have to work with it that's kind of nice
it's a sort of a portable expression of
a parallel computation so you know as an
illustration if I want to sum an array
in parallel so I have an array obviously
that's a very small array because I
couldn't fit realistic size array on
this slide first thing I'm going to do
is divide it into two sections I'm going
to divide those sections
you know I recursively into more
sections and now the problem that I have
to solve is I can in parallel add you
know one plus two that's actually this
should be a three plus four typo and
slide five plus six seven plus eight and
this can you know these operations can
occur in parallel the the partial
results propagate up the the tree and
eventually I get the answer right so all
right let's analyze the performance here
where would the costs be in doing this
in parallel compared to doing it
sequentially well I have to split my
input set that might have a cost I have
to dispatch work to different processors
so there's a handoff there that might
have some cost I can do a fair amount of
work and the time it takes to handoff
data to another thread when when the two
subtasks are finished I have to combine
their partial results that might have a
cost and it depends and these cost
depend on what your problem is and what
your data set looks like a significant
aspect of cost and this is sort of like
the you know the the elephant in the
room it's invisible in the code but it's
very real as data locality
what you want is for each each task to
be working on data that is located near
each other in memory so that when you go
and access the first element the second
third fourth elements are already you
know hot and
and that you know you know that that
task can spend its time that Cora can
spend its time doing computation and not
waiting for data to come back from
memory so each of these are factors that
can steal away performance when we try
to go parallel and you know I like I
said we always start out behind when we
paralyze and we're hoping to make it up
in volume so we need a lot of data in
order to make that work so that's you
know general about any parallel
computation how does this apply to
streams so streams is about you can
think of it as possibly parallel
aggregate operations on data sets
now streams are pretty efficient in the
way they deal with computation if you
have a stream that's likes a filter
MapReduce it's going to fuse these
operations into a single pass on the
data either a single sequential pass or
a single parallel pass it's easy to say
do it in parallelism do in parallel but
streams aren't magic parallelism dust
you don't just say make it parallel and
your code gets faster so it's on us to
decide whether parallelism is going to
be effective and so we can ask ourselves
some questions the the data set its
source how is it stored is it an easily
splittable source or it's an expensive
operation to split a source
similarly the problem that we're solving
has a combination step is that an
expensive step or a cheap step adding
two numbers together as cheap
concatenating two arrays might be
expensive and what kind of locality does
the layout of data and memory afford to
the computation if I'm just adding up
numbers in an array I'm gonna get great
locality if I'm doing graph traversal
I'm gonna be doing a lot of interactions
and I'm gonna be spending a lot of time
fetching stuff out of the cache so you
have a much better chance of getting a
speed-up with an array based source than
then with a sort of a more pointer II
pointer rich source a pretty simple
model that goes a long way for
estimating parallel performance is what
we call the NQ model basically has two
parameters and they're both pretty
simple n is how much data have I got I'm
doing the same thing to every data
element so how many elements am i doing
the thing to and Q is how much work is
it you know that I'm doing to each
element right so how much data do I have
how much work am i doing on each element
and as a rule of thumb
NQ needs to be large you know so if
you're doing trivial kinds of operations
like adding things together filter
MapReduce then you know you want to have
a pretty significant amount of data
generally the breakeven for array based
operations like summing arrays and such
is like 10,000 elements most of the
examples you see both in the
documentation and posted on Stack
Overflow and such are very low queue
operations they're doing trivial amounts
of work which means in order for
parallelism to work you need to have a
large data set some operations are
pretty expensive you know if you're
doing you know cryptographic attacks on
you know on message hashes that's a lot
of work per element that's a high Q
problem you can get away with less data
in that case but most of that most of
the examples that people show me of why
aren't I getting a speed-up they're very
low Q problems okay so let's talk about
all of those sources of slowdown in turn
will start with source splitting some
sources just split that in than others
and when I say better I mean the cost of
making the split the evenness of the
split and the predictability the split
so arrays are great arrays split cheaply
because you all you have to do is
maintain indexes into the array you
don't actually have to copy the data you
just say this task is operating on
elements 100 through 200 and when I want
to split it I say okay
100 through 150 151 through 200 I don't
have to copy the data I can just you
know make a little description that has
the starting and ending elements they
also split evenly I always know where
the midpoint is and they split
predictably I always know how big the
split is going to be that enables some
optimizations to reduce copying and such
like that now the worst possible source
is a linked list or something that's
like a linked list
right how do you split a linked list
well you can split it into the first and
the rest and then you can split the rest
into the first of the rest and the rest
of the rests and you end up with a tree
that looks a lot like that first
computation the diagram where you have
this very left heavy branching and
that's going to give you terrible
terrible parallelism because you're
you're not going to be able to you have
this dataflow dependency that keeps you
from be able to do anything in parallel
so arrays split great linked lists but
terribly other sources like hash maps
and tree sets are kind of in the middle
tree sets but reasonably well they're
reasonably well balanced they don't have
as good locality as a Rays do but they
you know they're there okay there's a
parallel to this no pun intended in in
the generators that we use for
generating streams you can use this this
generator iterate where which takes an
element at a function you apply
repeatedly to it and that generates a
you know as a stream of values so you
could generate integers 1 through n by
saying start with 0 my interation
function has add 1 to it I could also
generate the same stream with within
stream range in stream dot range
paralyzes great in stream down iterate
paralyzes terribly because they're
basically isomorphic to array vs linked
lists
I can't compute the third element with
iterate until I've already computed the
second element whereas with range I
don't have to compute the elements I
know where the midpoint is anyway right
so I've seen people say why doesn't this
stream paralyze and they're using in
streams iterate as their source and so
they're not getting any the their source
is splitting in a way that makes it very
hard to get any parallelism so this is
one of those things where you can just
like look at it the top line of a stream
pipeline say what's my source has the
data laid out in memory house that's
going to split and you should be able to
tell immediately whether it's a
candidate for parallelism or not
similarly locality is a big deal
parallelism only wins if you can
actually keep the CPUs busy doing work
if they're just busy waiting for data
from the memory subsystem that's not
very efficient we're not getting a gain
out of that on a lot of low-end systems
they're very much memory bandwidth bound
and so if you have an array of
primitives you get great locality out of
that if you have an array of references
not such good locality you get good
locality on the references but then you
have to traverse the reference and
you're unless you have some reason to
believe that your data is clustered near
each other you're you're likely to spend
a lot of time waiting for cache misses
and to give you an example you know
here's the here's the diagram and array
events the ants are all really near each
other so caching works for you you know
you pull the first one into memory and
they're all you know the next you know
you know the next bunch are in that
cache line waiting and you have we start
to traverse the the array and you have
prefetch working for you everything's
great in the array of integers it works
much worse right because every time you
go to fetch one of those integers you're
doing it in Direction potentially taking
a cast matters cache miss so summing up
a stream based on an array events
paralyzes really nicely summing up an
array of integers you get a horrible
parallel speed-up or no parallel speed
up because the bottleneck is not the
computation applying more cores doesn't
solve a fundamental problem because the
fundamental problem is never bandwidth
okay
look at another source of an impediment
to parallel performance is encounter
order some operations in the stream
library are tied to encounter order
which means the order implied by the
source now some sources don't have an
encountered order like hash set it
doesn't have a defined encounter order
and so it doesn't try to impute meaning
to it but if your source is an array or
a list it does have a defined encounter
order and operations like limit or skip
or fine first are defined in terms of
the encounter order fine first is find
me the first one and that means first
going left to right so
this is introducing one of those
left-to-right dependencies that Sewer
talked about when you use these methods
and that means you're going to get less
exploitable parallelism now sometimes
there is it there is an encounter order
but it's not meaningful for your problem
and only you as the programmer know that
so if you know that I have an ordered
source I'm doing one of these order
sensitive operations but I don't really
care about the order I just want any
three elements I don't necessarily need
the first three elements you can say
make the stream unordered use a stream
operator calls unordered and then things
like limit instead of giving you the
first ten will give you any ten and it
gets much faster and much more
parallelizable similarly you can use
find any instead of fine first and get
the same benefit so be careful when
you're using stream operations that are
whose semantics are tied to order to
encounter order because that introduces
a dependency that's going to rob you of
potential parallelism okay another big
cost that you know that can end up
making things not effectively
parallelizable is merging some you know
for some operations like summing up the
integers the merge operation is really
cheap just add add the two sub results
and you're done but what if I'm
collecting to a set now The Verge
operation is I've got a left set I've
got a right set and I want to merge them
into a new set that can be a pretty
expensive operation it's so expensive in
fact that like the parallel version
might be five or ten times slower than
the sequential version so the operations
that involve a lot of copying merging
sets concatenated strings you know up
every level of the tree can really
overwhelm the benefit you might get out
of parallelism and so if you take a
stream pipeline like this like take you
know the first and integers and then
collect them to the set and you try to
do it in parallel
you know you'll routinely see
significant slowdowns because it's just
easier and cheaper to sequentially add
to an existing set than to merge sets
you know repeatedly you know so you know
illustrating that same thing you know
I collect each of these guys into a set
and now I have to merge these two sets
into another set and i merge these two
sets and then I take these two big sets
and I merge them in this last make big
merge is sequential because there's only
one core left with any work to do right
so the cost of doing this in parallel is
dominated by merging these two sets that
are half the size of the input so you
should and but this is something that is
a characteristic if your problem you
should be able to look at your code and
look at your problem and say is the
merge step expensive or not and you know
and if it is then parallelism might not
be you know might not help there now one
of the things that you can do is there
are some concurrent collectors where
rather than doing collection by merging
you can say let's have a big concurrent
hash map as my target and just have all
the threads fire data into that
concurrent data structure for problems
that are amenable to that to that
approach like grouping by concurrent and
in the collectors library that can often
break this bottleneck but you know you
should be looking at your problem and
saying what's the cost of my split
what's the cost of my merge okay so any
of these factors could conspire to steal
away your speed-up maybe you don't have
enough enough data maybe you don't have
you're not doing enough work on each
element maybe you have too many in
directions in your data source and so
you're spending all your time waiting
for data from the memory subsystem
instead of doing computation maybe the
source is expensive to split maybe the
results are expensive to merge
maybe the semantics of your pipeline use
in you know I encounter order sensitive
operations these are all warning signs
that you're not going to paralyze well
so and these don't involve any measuring
right these just involve like
understanding your problem and reading
your code so it's important to measure
but even before you measure you should
think and you know this this these are
all amenable to thinking looking at your
problem and saying what reason do I have
to believe that I'm going to get
parallelism out of this and you can
probably eliminate you know quite a lot
of
without having to implement and measure
and iterate so summing up as Stewart
said streams are cool and parallelism is
also cool but parallelism is just an
optimization and parallel streams aren't
magic they're just an easy way to
express a parallel computation so before
optimizing you should recognize that
what you're doing is optimizing and if
you're optimizing you should have
performance requirements you should have
performance measurements in place and
you should before you even start
optimizing you should make sure that
your performance isn't already good
enough this is one of the biggest
mistakes I see people making with
performance is they don't have
performance requirements or they're not
watching their performance requirements
but they optimize anyway because it's
fun this is like just wasting your
employer's money maybe you hate your
employer so okay but you know this is
not doing your job
right if you've written code that is
clear and maintainable and meets the
requirements then optimizing is just
making work for yourself and probably
introducing potential maintenance
problems down the road so before
optimizing make sure you have something
that's optimizable make sure you have
performance requirements make sure you
have performance measurements and if you
don't have any of those things if you
don't have performance requirements or
measurements then your code is already
fast enough right in the absence of
requirements not having performance
requirements and measurements is
basically saying performance is
unimportant if performance is
unimportant don't optimize so anyway
streams are cool parallelism are cool
but don't get carried away thank you
very much and we can take questions
all right we have a few questions here
so here's one camp JVM decide if the
problem can be solved faster in parallel
so I don't I the programmer don't have
to choose parallel stream or stream so
40 years of research says no there have
been many attempts at Auto paralyzed and
compilers going back to the Fortran days
and they've largely been failures except
in the cases of toy problems the
compiler doesn't know how much data you
have and in a language like Java where
you know you tend to code things with
using you know small methods as units of
behavioral abstraction it's pretty hard
for the compiler to be able to see
through all the code paths that you
might get to and say yes I know this is
parallelizable because it doesn't
necessarily know what code is actually
gonna get run on your on your data so
there's been a lot of research in this
over many many years and nothing
suggests that the compiler can make this
decision for us okay a couple couple of
other questions here so there was a
question about fold and would that be
better than what better way than
indexing over the streaming over the
indexes I think fold is a really useful
operation we don't have it in streams
yet but fold the fold operation has the
characteristic that every intermediate
result depends on all the previous ones
and that's you know that's the theme of
this right and with the the with the for
loop we stumbled into that and it wasn't
necessary for this problem and so fold
has the same characteristic might be a
very nice way to program no question
about that but it creates this
dependency where none is necessary
there's another question there's a bunch
of there's a little bit of chatter on
about this the sublist problem what
happens at the end is there an open sub
list etc one thing I didn't do with
stack overflow because it doesn't kind
of fit was I actually wrote a bunch of
unit tests and then ran all the
solutions including my own against those
unit tests and that's how I got my
results but that's not visible on Stack
Overflow so what I think I'm going to do
is post those unit tests to
- you know put a Geist up or something
like that because they carry they cover
all the edge cases and stuff and you can
see you can see what you can you'd see
how the algorithm is actually work when
run against those unit tests so for the
people who are leaving it's fine you can
leave but if you could respect the
people who have chosen to stay and leave
quietly so they can hear what's going on
that'd be great
you want to talk about units of Q what
do you well yeah you've caught me on
that boy I tried to get a fast one by
you and so so the answer is well I
didn't read the question so so Brian
have the end Brian had this presented
the NQ model and so n is the number of
elements what is Q and what are the
units of Q and yeah and the answer is it
doesn't really matter you know it could
be number of byte codes number of
instructions number of lines of code it
doesn't really matter that the the the
point is you're looking at an operation
saying how much work am i doing am i
doing a few instructions worth am i
doing thousands of instructions worth
for very low Q problems like adding up
numbers you know like I said typical
rule of thumb is you know you're looking
for like 10,000 elements or more but but
really it's more of a quantitative
description of am I do I have a chance
of getting parallelism out of this
either and you either need to be doing a
lot of work or have a lot of data or
some combination of the two so here's
another question how do you know if you
have cache misses so you can measure
this most CPUs have performance counters
these are Hardware registers that are
readable with tools there are you know
there are lots of tool chains that can
read these and correlate them to code
like the the Sun Studio analyzer tool
chain gives you that the perf Azzam tool
on Linux can do that if you're using the
jmh micro benchmarking harness it has a
mode where you can have it invoke pref
Azzam for you and associate performance
counters with lines of code
you know so so there's lots of ways you
know if you're serious about it to get
it to get at this data if you're not
serious then your codes fast enough so
so to add to that I'm I I think it's
interesting you can write you write
little programs like the the the inter a
versus integer array benchmark and and
measure the difference in what happens
and it's and you can use a benchmark
harness like jmh and it is it produces
some fine grained enough results that
you can really see the effects of
locality and memory latency on even very
simple programs so it's it's interesting
too I mean so it's a very educational
experience to to dig into that here and
I think that's our last option yeah so
the question is we've only talked about
cpu-bound problems people also want to
use parallelism for i/o bound problems
is parallel streams a good tool for that
and the answer is it could be in the
future currently it's not a great tool
and the reason is has to do with the
splitting heuristics the splitting
heuristics about whether to split or
whether to just do the whole you know to
process the whole problem that's in
front of us in the current thread is
tuned towards the assumption that the
problem is CPU bound now could we change
that yes of course could we provide the
ability to provide tuning hints that
says this is an i/o bound problem this
is a CPU bound problem yes we could we
would like to what we haven't figured
out is what's the right form in which to
provide those hints to the stream
framework so we've seen we've been given
a lot of suggestions about how to do
this
the obvious place is the parameters to
the parallel call where you're
describing the characteristics
the problem we've had many suggestions
about what to put in there they've all
been bad our own ideas have been bad so
like I said in my previous talks it's
sometimes it's better to do nothing than
to do the wrong thing and hopefully the
right thing will emerge there isn't a
lot of positive advice from the the
research literature about you know how
to characterize these things either so
we're still working on that problem it's
largely an API design problem it's not a
that's not a fundamental technical
problem and I think with that we're our
time's up so thank you thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>