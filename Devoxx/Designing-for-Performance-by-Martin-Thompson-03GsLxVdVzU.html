<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Designing for Performance by Martin Thompson | Coder Coacher - Coaching Coders</title><meta content="Designing for Performance by Martin Thompson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Designing for Performance by Martin Thompson</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/03GsLxVdVzU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello all and thank you for all coming
up to the session this is a huge room
with a lot of people it's quite
intimidating being the little person
down the front here
well what I'm gonna talk about today is
designing software for performance and
it's not what many people think it's
actually something much simpler a lot of
people will think it's all about
compiler bit tricks and all that sort of
stuff this a lot of the time take it
back to think about it in a different
way so kind of start off with the
question here who thinks it is difficult
to design software that has good
performance few people I see a lot of
people think it must be fairly easy well
I think it particularly is if you do a
lot of this so if we just copy and paste
everything from Stack Overflow we
probably don't even understand what
we're doing in the first place I think
probably a slightly bigger problem is
resume driven development many of us end
up we just try things and we try to put
them on our CDs or resumes and that's
what we tend to do a lot of things can
not particularly for the right reasons
something this worth burning in mind
there's a paper came out last year if
anybody hasn't read this I would
recommend going and doing this this is
where the silicon industry get together
and they discuss trends and where things
are going for the future there was some
really fundamental things that came out
of this report so in this K Mart one of
the things that have highlighted is we
are using more and more energy in our
data centers and this is increasing at
quite an alarming rate how alarming is
this actually will by 2040 if we
continue on our current trends there
will not be enough energy produced on
the planet to feed our data centers
think about that for a second so when
we're writing all of our code
maybe parsy know a lot XML and Jesus and
then sort of burning energy and hate
thinking about why we doing it that way
is there different ways of doing this so
we've gone through a world of four by
four years when viola said let's just
throw
hardware the problem if we throw
hardware at the problem it's gonna kind
of be okay
and we did get and get away with that
because as Moore's law continued we
tended to get increasing clock speeds
that has stopped now
but we then went on we could get an
increasing transistor density which is
still happening to an extent but slowing
down greatly we're not reaching the
point where by economics is going to
drive our future and not the physical
side of the world and let's look at what
that actually means and should we
actually care about performance so I
want to go back to the start and look at
well what does it mean to design for
performance at a kind of fundamental
level and what can we do so gonna answer
that sort of fundamental question what's
the structure there's gonna be well
let's spend a little bit of time talking
about what performance is itself how do
we actually end up doing this well a lot
of it's about making clean and
representative models and I'll get into
that in a little bit of detail then I'll
go into how do we actually implement
those models themselves and then how do
we test this to prove that it actually
works so let's start off with what is
performance itself we often hear let's
make this fast let's just make it go
quicker but what does that really mean
performance is a very high-level concept
we need something much more concrete
we're gonna deal with so one of the ways
we can look at performance is throughput
and throughput is units of some work
done per unit of something of time and
will often hear the term bandwidth well
bandwidth is maximum throughput at a
given level so if your network can do 10
gigabits per second that is the maximum
throughput you'll get so you'll hear
bond with some throughput use that sort
of way it's a really interesting measure
so it's one things that we should be
going for it also matters what the
response time is for many different
things so we deal with throughput but
what is the response time at a given
throughput is really interesting so if
we test something at a low amount of
throughput sometimes we can get a good
response time if we increase the
throughput response time can go wrong
and I want to get into why that kind of
happens and what we can do about keeping
things responsive because any system
that doesn't respond in a timely manner
is effectively unavailable so we need to
look at both of these measures
we'll also look a lot of things like
scalability so this isn't strictly
performance but it ends up being very
related and as we look at things like
scalability what we mean by this is if I
add a proportional amount of a resource
do I get a proportionate increase in
throughput or do I get a proportion to
disk a decrease in response time and we
need to be able to deal with these so if
I add hardware to a problem and things
get slower that is not a scalable system
it should scale up but there's some
fundamental mathematics behind that now
a lot of performance can actually be
summarized in things like this so this
is a lot of my life I travel around a
lot I end up in queues given what's been
happening in the world this year I
probably don't want to travel quite as
much in the future because I get a hard
time from immigration in different
places but as we stand in these queues
we start thinking how does this actually
give me an idea of performance well
there's a number of things going on here
this comes down to how we break things
down queuing theory is fundamental to so
much of what performance is a part it's
actually some very simple arithmetic and
probability behind it but it really
explains a lot of what's going on so if
we're dealing with the system like this
what is your response time is kind of
interesting what is leave and see what
is service time what is cycle time here
all these different terms some of them
are actually the same thing that it
seemed named for it to add two different
names for the same thing something's are
fundamentally different so the major
things that really matter in a system
like this is what is the service timer
cycle time they're actually the same
thing that is the time you're at the
front of the queue getting serviced at
the desk that is the time you being
serviced that will be your service time
not starts to matter and by getting
serviced time down we can make systems a
lot more responsive because they're feed
into what is response time and that's
the total time for the system which is
made up of service time plus the weird
time that you're dealing with something
if you're weeding the queue for a long
time it's not a pleasant experience
yet a lot of our software has a lot of
these problems because we end up weeding
in queues for something to happen we can
open many other queues so that we can
get more throughput we can go parallel
to parallel sides kind of
interesting has some issues with it as
well what's this look like if we start
throwing a lot of load at a system what
happens to the response time well you'll
have seen sort of jaya curves like this
before where as we increase load on a
system the response time starts to slow
down it doesn't slow down linearly this
is what's quite interesting about it
there's probability theory coming into
play here and what we've got here is the
graph off response time versus
utilization this is typical queueing
system as utilizations low response
times fairly good but a GUI zation
starts to ramp up response time starts
to get quite / and then starts to go /
really badly very quickly towards the
end so what's kind of going on here what
do I mean by utilization so say this is
a service and that service can process
our requests in 100 milliseconds and if
things are turning up at 5 per second
you're using a 500 milliseconds off the
time of that service it's actually 50%
utilized
it's fairly responsive at that time
because there tends to not be queues
forming as you start to increase the
arrival rate so things arriving at a
faster rate set 6 a second 7 a second
either sack and whatever happens do you
take in the utilization up and once you
start going over but 70 percent
utilization response time starts to get
bad really really quickly like so what
can we do about this sort of thing well
we could limit the writ of arrival which
we typically don't want to do with our
applications we want to deal with this
in a better way and the better way to
deal with that is to reduce the cycle
time of the service time by reducing
that we greatly reduce utilization so
let's say in this case we were running a
90% utilized the queue will typically be
tan deep at that stage so it's a long
time to be weeding to get your service
if you were to optimize that service so
it runs in 50 milliseconds now rather
than 100 milliseconds it's going to make
this thing much less utilized so you're
gonna half the utilization so we go from
90 percent down to 45 percent and
actually a unit of Processing's half of
that again so we're in a really good
situation
rather than queueing for 10 plus units
of time were done to queueing for just
over half a unit of time that's a factor
of 20 improvement in the response time
of the system because she changed the
service just by making it twice as fast
as before so some of these fundamentals
start to really matter so what does this
actually mean for us well if we keep
capacity sufficient so that we keep
utilization Dyne we can have quite
responsive systems this applies to all
systems so any project monitors in the
room this equally applies to you if you
run your teams at very high utilization
you will not get good response from them
because their system just like
everything else governed by the same
mathematics we need to think about this
so can we go parallel to speed up in the
picture I showed we get have multiple
queues and those queues will allow us to
get through the work faster this comes
down to can we split this piece of work
up if you look at this sort of earlier
you'll come across our dolls law quite
quickly
most people know of it um does law
wasn't coined by G Nam doll himself he
wrote a paper called um dolls argument
and the whole point of this paper is he
wanted to scare people off using mid
rating systems so the guy who actually
wanted to stop people doing parallel
programming gets associated with it
welcome to the modern world he didn't
want us doing this what what was his
argument what was he trying to achieve
well what he's trying to show is how
difficult it is to make something
parallel to get a scale up so let's say
I've got a job and that job can be split
up into a part there and a part be if I
can throw 4 processors at the problem I
can get a reasonable speed up and that's
because I can split up the part air and
do it in parallel over four different
processors if Part B was the only that I
could split to run across those four
processors my speed-up was much more
modest I can't get so much more out of
this how does this look when we start
looking at what percentage of an
algorithm can be done in parallel
well if we look at this if it's really
simple and it's set for example half of
the job can be done in parallel half of
it still needs to be sequential at this
stage you can only ever get a two-act
speed-up but it doesn't matter how many
processors you throw at it you still
can't go more than sort of having the
time to do in there so we get the two
acts speed-up if we look at it as it
goes further up so even up to the
extreme of a job that is 95 percent can
be paralyzed you still cannot get more
than a 20 act speed-up so it's actually
quite a grim situation that look at
going to parallel if you have any
contention at all becomes a limiting
factor very very quickly and this is
what on dolls argument was he didn't
want to go and bind these mid-range
computers because he didn't believe it
was easy to program for them and you
should have bought his mian frames but
that was an argument just to scare
people what's it like when you look at
this from a real world perspective so
when Neil Gunther was at Xerox PARC he
was looking at parallel competing at the
time and he was finding you couldn't get
armed dolls air scale-up predictions in
fact you could even get close to that
when things got interesting and he dug
into it a lot more and as a result of
that piece of work he came up with
something called universal scalability
law kind of slightly scary formula but
it's actually not that difficult when
you get your head run it what he
discovered was that whereas um doll was
right about the contention part of the
penalty he did not cover the coherence
cost that's involved so when you get
multiple parties all communicating and
they're sharing work they need to
communicate between each other what
their current state of the world is that
has a coherence penalty so everything
takes time nothing happens in an instant
even between processors on the same
socket there still is a time penalty
getting this thiet and each of the
caches coherent this this needs to be
factored in so let's take the 95 percent
kiss and then with the 95 percent kiss
I'm gonna add in a coherence penalty of
150 microseconds this is typically what
you would see in your sort of Amoz
in clusters if you were doing a large
Hadoop job how do things scale up sort
of it's my world the sort of things I
get to see quite often well the blue
line is armed are scarce so we're
looking at that nice speed-up we can
eventually get to a 20x speed-up if we
throw lots of processors out the problem
but you factor in 150 microsecond
coherence penalty and all of a sudden
the speed-up isn't looking so good it's
kind of fine when you're in the small
numbers off nodes that you're applying
this to but soon as you pass about 16
the two curves start to diverge and then
after a while the coherence penalty not
only is having a minor impact it's
actually becoming the major factor and
you slow down rather than speed up so we
cannot escape these problems easily this
is simple mathematics that will hunt us
down and we cannot run away from it so
if we have any contention and our
algorithms we are fundamentally limited
in how we scale up now here's a nice
little graph what we've got here is
multiple threads going out a problem and
time to complete so what is the weird
time to de finish doing this job and
we're seeing that it's writing about
sort of 17 microseconds per operation
when we've got one thread doing
something we add more threads to this
and it's taking longer to complete does
this look like it's a fairly parallel
job
and we think this has got good
scalability someone's thinking yeah so
much like so if two threads are going to
do this taking even longer so what what
you're seen here is a perfect queue for
me things are joining the queue as you
add processors you've got a contention
penalty here of a hundred percent this
is like the worst scale-up algorithm you
can imagine what do you think that is
that's our log in frameworks pretty much
every one of them works this way they
take agrippa clock and that serializes
everything that goes through the logging
so the more cores you'd apply to the
problem the slower it gets the more you
increase contention and that all has we
made coherent the number of systems I
get to see where people are just
bottleneck on logging and you really
don't even need a pick out any logging
frameworks they all suffer from the same
thing it's a kind of really bad design
pattern I if anybody wants to build
highly scalable systems I recommend you
study logging systems and then do
completely the opposite you've got some
hope of making a scalable system or
high-performance system at that stage so
with that in mind let's kind of go
forward let's look at what can we do how
should we be thinking about things just
even had a very very simple level so
let's look at clean and representative I
love getting the dictionary when you go
to name things because our industry is
just riddled with things named really
really badly I love things like random
access memory do you want memory to give
you random values back no you want
arbitrary access whoever named off
didn't have to look up the dictionary
non-functional requirements the list is
kind of analyst so let's look at what
means by being clean because we talk
about the clean code movement a lot well
morally uncontaminated pure innocent
whose code is like that it's morally
uncontaminated pure and innocent
wouldn't it be wonderful if our code was
most of the codes can't very unclean and
really not that nice so I'm kind of all
behind the clean code movement please
can we have some of this also what does
it mean to be representative well it
should be serving this portrayal of
something a symbol of something so our
code should actually steer that's intend
to be really quite clear and if you kind
of start following these things it kind
of plays out in an interesting where I
love the fact that code is the one thing
that will always be running on your
project so documents get out of stab
comments get out of stab the code is the
place where we should capture our
understanding and we should keep it
alive which you constantly keep keep
updating it like gardening we tend it
all the time and if we do we kind of get
the some of the right things
but if code is really that important
that should mean that some of our design
approaches really really matter so this
is the bulk of what I want to talk about
here is like how we do design really
impact so many different things you
could go on about clean code you can go
on about an attainability testability
all the different quality attributes but
I spent a lot of time on performance and
I find that clean simple code tends to
also be the most performant code so it
has to follow good design principles so
let's look at some of those design
principles one of the ones I keep
hearing a lot of people talking about
abstraction and abstractions one's kind
of fascinating subjects that I've got a
very simple way of looking out at now
I've got my rules of abstraction so rule
number one don't do abstraction rule
number two don't do abstraction
getting a little bit Fight Club here is
sort of joking aside being a bit more
serious we can't abstract but we should
be seeing at least three things that are
something like if we're gonna have
inheritance for example using
polymorphism we should have a proper is
our relationship not a kind of sort of
maybe a bit like that sort of thing
because it's sort of convenient for me
right now
we need genuine is our relationships
then we can treat things in a sensible
scene sort of where and all abstractions
have a cost everything we do has a cost
does it pair back for the cost of doing
it it's some really simple things we
need to think about so if we gonna start
doing abstraction let's be clear while
we're doing it we've got a genuine is
our relationship if we're just talking
about polymorphism here and the
attraction is going to pay for itself
the one thing to be very aware of is our
industry def are obsessed by dry for a
long time so don't repeat yourself drive
your code make sure you don't have that
commonality it's kind of interesting hi
it's often better if you're gonna
implement something new is even copy and
paste and I know that sounds like heresy
but copy and paste try change the code
do what you need it to do exactly then
maybe start thinking about it drying it
out afterwards we shouldn't abstract
before we should always abstractly
whenever we know we're gonna get the
benefits the dangers we rush to these
things we build our cathedrals and we
then find it so difficult to go back
again if you get a look at basic human
behavior if you've created this thing
that that's been a work of many years or
many months or whatever it's been it's
really hard to undo it it's much better
to keep it simple workout for
commonality then created later just it's
the order that we go into it because if
we go down this route we're into a kind
of interesting world where soon as you
get many of a certain type we're now
into mega morphism and this is one of
the things that our compilers on our
hardware has quite a lot of difficulty
dealing with so if we're gonna get one
of something that's fairly easy even two
of something we can deal with it quite
well once you start dealing with three
plus of something it becomes really
difficult so we end up with junk tables
there's certain optimizations we can do
but it becomes quite limited this
doesn't suit how our modern processors
work so if we're going to do it we got
to get a payoff for it otherwise it's
not worthwhile and also if it's not
representative it's kind of the wrong
thing to be doing there's so many
abstractions are just plain wrong where
we try to create base I'll kind of come
back to this a little bit I found one of
the biggest things is founds I didn't
this is big frameworks is everybody
wants to sell you a big framework that's
gonna solve all of your problems but
generally it doesn't all of these big
frameworks are Jesus Lee just a promise
that never delivers the truth is we just
need to do a lot of hard work ourselves
and kind of get to that point and I see
understanding what's important I think
this is actually a really good example
of this so if you ever in the
backpacking or traveling around you'll
meet the individuals who start off and
they want to bring everything they think
then girl all the stuff and actually
they're a proud of all of their stuff
but you very quickly start to realize
there's only a certain number of things
you really need and not what you can
travel much lighter it should be the
same with our code the last code we have
the easier it is to maintain these are
is the deal with easier is the reason
about these years to make it fast so
we've got to keep it simple we're gonna
start thinking of how do we travel light
so if we're going to start abstracting
only
abstract who was sure of the benefits so
let's just pick on fill out a little bit
more the who's who of Joel Spolsky
lawfully gay abstractions she probably
quite a few people he makes a couple of
interesting statements in this they all
non-trivial abstractions are to some
extent leaky the detail of underlying
complexity cannot be ignored this is how
he talks about it to me this is just
screaming out that this is not an
appropriate abstraction this is someone
has abstracted for the wrong reasons
let's look at how someone else describes
abstraction back in 1972 when Dijkstra
accepted his cheering award in the
middle of his speech he had this little
gem the purpose of abstracting is not to
be vague but to create a new semantic
level in which one can be absolutely
precise I much prefer that that Joel
Spolsky see of it it's basically showing
is that abstractions can be good but
only when you're giving you more
precision not whenever it's sort of
papering over the cracks of something
that's imperfect so let's try and make a
concrete example of this so I spend a
lot of time dealing with memory systems
and hardware and they're fascinating
complicated beasts but can we actually
abstract what they do to understand what
they give us
well our Hardware friends basically take
three vats they're betting that we're
gonna write soft for following three
very simple rules one is the temporal
back is something that you've used
recently you will use again in the near
future this is what most people do
understand about caches is a fairly
simple thing that the least recently
used hype concept what's more
interesting is things like the spatial
bet so things that are used to things
that are together typically used
together so we gotta keep things close
together by cohesiveness so the absolute
opposite of coupling and getting this
sort of thing right so how does this
play out for example if you have an
object to feels in that object are
likely to be right next to each other
when how you lay out the class or that
object for the class they're likely to
be in the same cache line that way you
get the benefit
it's only a single access to memory
you're gonna be hitting the same cache
line in your l1 cache so they're falling
in those bats seem with slightly larger
structures like things appearing in the
CM operating system page table which
means that part is hot and the cache
where we translate our virtual physical
addresses there's all sorts of
interesting layers to this but it's
actually get some nice abstractions in
it and if we understand those and we
follow them we can get benefits the
third one quite simple is that they take
a bet on patterns of access they're
expecting us to access our data
structures with certain patterns and
what they do then is their prefetch that
data to hide the latency to the main
memory so we get that it's already there
when we go to need it and if we follow
those three to three bats it actually
starts to matter so let's dig into this
and see how can this matter so if I'm
gonna implement the model I want to go
back to design basics and what do I need
to think about tooth very important
concepts are coupling and cohesion most
people will have heard of them but do
you really understand them do you
practice them every day this is the sort
of really interesting things that if
we're gonna get good at our craft these
are things we have to start thinking
about one simple example I have of this
this were I look at many objects and
they're effectively just a properties
bag it's just where so many wasn't stuck
that property to do requirement at a
given point in time and there's certain
things that start to stand out so who's
heard of a design pattern called feature
Envy I heard of that few hands
well what feature envy is is about two
objects one object is constantly asking
for fields from another object to do its
own work its envious of the features
that are in this other object it
basically is telling you that you have a
coupling problem some fields that are
over here should be over here and if you
look at word the things get access if
the majority of accesses are actually
from another object rather than within
the object itself you have a problem
you'll see this in code with things like
tree and racks with dot dot dot dot all
these sorts of things you see but sort
of
hell don't ask us different patterns
there's loads of really interesting code
smells that are out of there when you
start getting coupling and cohesion
right in a model so fields are where
they're actually used they're even
better if they're well encapsulated
inside there so there's good information
hiding what you'll tend to do is you've
got things specially together they'll
also tend to be temporarily together as
they get pulled through the cash you get
the benefits of that one of the things I
love to do on projects when I'm learning
the code base is just go around it sort
out a lot of the coupling issues making
things more cohesive and as a result the
code becomes easier to understand easier
to work with easier to maintain and
usually always a good bit faster I find
by just going through some of those
exercises I've typically got a 30 to 40
percent increase in throughput our
reduction in response time just by
making those sorts of simple changes as
you get to understand the code base so
it kind of screams out that we really
need to think a lot about our coupling
and cohesion and so if we start
respecting that locality of reference
this starts to really matter and as our
hardware moves further and further for
there's three bats that are there and
how our memory subsystems work are
becoming stronger all of the time
because our caches have become such the
critical part of how our hardware works
and here's the kind of interesting one I
see lots of different languages and
where languages are going in the future
just to really tell how were they are of
some of these issues so if you're gonna
hit your l1 cache you're typically
talking three to four cycles if you get
a myth and you go to me in memory you're
typically looking at around a hundred
nanoseconds in that sort of order with
some cueing a fax has will typically be
your best-case scenario might be dying
at the sort of around the 65 to 70
nanoseconds but you're typically looking
at a run about a hundred for a cache
miss whereas that's only one nanosecond
heading l1 if you're running a three
gigahertz so that's two orders of
magnitude difference so you've got to
get these things working well together
what is a good task for some of this the
way I like it the thing that
you got to keep things together
particularly and so I kind of a question
for most people who are implementing
languages is can you implement an
efficient B plus tree in your language
if you can't you really need to think
about where the hardware's going in the
future because your language is not
going to scale up it's one of the kind
of litmus tests and why do I pick
something like a B+ tree because the
reason release don't lead abuse has
become successful at the end of the
eighties it was the discovery of the B+
tree because an operation to disc was
such an expensive thing so you wanted to
get as much work done for one of those
operations as possible so you pulled in
a block of data it's the same now as we
access memory we need to pull in a cache
line and we need to have as much in that
cache line as possible so if you've got
a tree it's implemented memory if it's a
binary tree you're gonna have far too
many steps to get to your endpoint if
it's an N way a tree like a B+ tree or B
star tree or B tree you can have many
steps reduced by that by going much
wider much faster it suits these things
much better we need to start thinking
this way there's loads and loads of
great examples of this and how if we
don't do this we're basically leaving
two orders of magnitude of performance
on the table for anything that's in
memory and this comes down to its
relationships we need relationships to
work well between the different objects
we have and we need to think about this
there's like other skills we need to
think about it so I spend a lot of time
in finance I look a lot of odd financial
orders so these orders rest on order
books most people may draw a simple line
but does that tell you enough about
what's going on if we dig into this well
there's actually two relationships there
there's bids and offers and they're
one-to-many they're also always
qualified on price and they can be
ordered and have FIFO semantics so
taking the time to IC understand the
relationships tells you so much but
which D the structures you should use to
implement that everything isn't just a
less than a map there's many more
interesting data structures and these
are really useful things that will stand
with you for your whole career
two major pro tip on this is really get
back to studying data structures learn
much more about them right
you could learn loads of different
frameworks now and it might be valid for
you for this year might be valid next
year if you helping a program in
JavaScript it'll be out of date by next
week if you learn about data structures
they will be with you for your entire
career because we get advances but they
tend to build on what's there these are
kind of some of the fundamental building
blocks of our discipline and then like
the fundamental building blocks of
physics that we have in other sciences
and stuff so we need to think about that
is there any other things in this well I
think botching is another one of those
subjects and the key to batching is all
about omelet rising the expensive costs
there's a number of things that we have
to do that are just expensive we're
gonna go to desk we're gonna go to
memory we're gonna go to another machine
if you're gonna do one of those
operations let's get as much work done
as possible that way we can Alma tries
that cost and do better from it like so
what's the simple example of this so
let's say we go back to that login
framework problem
so in our login framework if you go to
like write to desk and I said we weren't
gonna lose anything we are in a
situation where we get a queue to access
our contended resource so let's say I
had 10 threads wanting to write at the
same time the problem I'll end up having
is one will arrive first with all the
Rast arriving behind it we end up with a
perfect queue there and the outwards
response time is going to be 5 units of
whatever time it takes to write down to
that that becomes a major problem so you
get one thing add one unit of time and
one write up to 10 units of time with a
full distribution between that and
that's kind of not a great response from
a system it's also a kind of bad
experience depending on where you turn
up in the queue let's look at this from
a batching approach instead so let's say
many of the things turn up at the same
time you write them into some sort of
data structure and you have a separate
thread that's batching those Dyne to the
underlying storage the worst case
scenario is whenever the tantor
the Badger picks up only one to begin
with and then that comes back and it
picks up the other nine and writes them
down the entire thing is completed in
two units of time with an average of
less than two units of time and that's
the worst case scenario we get much
better performance out of this how does
this play out when we design our system
so the red was the queueing theory a
fact that we seen from before if you
build systems with batching into the
design right through it you tend to get
the blue line I find the day I spend a
lot of time in high frequency trading
and other high-performance things and if
you want to win against other people you
typically win by a B just being better
than them and the algorithms are often
modern more than anything else so
applying things like batching techniques
because we get burst scenarios the real
world is full of burst scenarios and
then we end up winning so you can
achieve the blue line from response time
and also get much better throughput
through your system so go back and study
biting biting isn't just about you wait
for a timeout and do stuff there's lots
of ways we can look at algorithms and do
things and batches and as a result we
can get much more out of it branches
branches branches we have branches
everywhere in our code and makes our
code hard to understand like how many
times we've seen just pages of stuff on
a screen that sort of indented many deep
with all sorts of layers to that all of
that stuff every time you get the for an
F awhile to do whatever a case statement
in your code you have a branch and our
processors are trying to make as much
progress as possible and they speculate
if they see a branch and they don't have
the dealer to know should they go laughs
to go right at this given point in time
they will speculate based upon some
history that they are stored and if
their history is good they've predicted
well and we'll go forward and continue
axud kidding code and then we'll find
out whether their bat was right if the
bat was right as grid the venire
progress if their bat was wrong that
throw it away and start again so just
even unwinding a branch is typically
about 15 cycles on a modern processor
but that can have other impacts on how
architectural steered you've used
different buffers it can also just be
generally worse and how this ends up
working so we really don't want to have
too many branches in our code we often
learn by following inference on
everything there's much more we can do
with mathematics which is a huge subject
on the zone but let's look at just some
really simple stuff like I see lots of
code like this where we're just gonna do
some work and people actually start off
with the best of intentions to make
something more efficient well I'll check
the things empty and if is empty I won't
actually do anything oh but I've also
got a check for null cuz people pass
nulls around our code like seriously
this is 2016 it's the only the end of it
we got to stop using dolls in our code
let's get over some of that like get rid
of code like this and just keep it
simple
you should be passing in something
that's either empty or it's got stuff in
it and deal with them both the same lab
because typically you're gonna have
stuff to deal with keep your code simple
keep it clean start working with things
like that
there's loads of examples of branches
and I could go on for days on this alone
just different examples on it but just
start thinking about your code certainly
do you really need all of those branches
because it gets hard to read hard to
understand and generally respect the
principle of least surprise like people
shouldn't be throwing nulls and stuff
into there that's trying to be a bit
cleaner with some of our code a much
bigger subject is loops we spend a huge
amount of time in our code I've seen all
sorts of studies that say sort of 70 or
80 percent of the time the spent in our
code is spent inside loops when we're
not doing i/o and I think this statement
is really very relevant to it so many
people think of the Mark Twain
Commonwealth actually goes back to
further before Mark Twain please Pascal
and a little French is the first case
I've seen of this but the statement is
like if I'd had more time out of written
a shorter ladder and this is so true
it's so applies to our code as well now
what do I mean about this and like how
can we get it like if you go and write
anything like write an email write a
book write a blog write a code whatever
we will pour out of our heads onto
whatever medium we're going to
or this and we'll do a reasonable job of
it maybe but if we leave it some time
and come back to it again we can always
refine we can always do things better we
can make it cleaner more elegant and
particularly you can't just do this
straight away because there's an
interesting thing that happens inside
our heads that if you write something
now and you go to read it straight away
to review it you biblically replay it
from the cash in your own short-term
memory and you don't truly read what you
put on the page you need to leave it a
period of time come back again and then
you truly reread it when you do that
you'll see things that you didn't see
before you'll see mistakes but you'll
also see many ways of making things
cleaner and more elegant and by making
things cleaner and more elegant we can
get code this easier to maintain but
also code this much faster how do we do
this well we need to start thinking
about how we work and change how we work
we don't just do a piece of work and
then move on and never go back and
revisit we need to be doing a piece of
work and a long time to go back and
revisit lots of what we do I've seen how
some grid academics have many papers on
the go at a time the delivery not a grid
writ but what they're doing is they're
taking a bit longer in the gestation
period for that and eventually you're
getting stuff going out after it's been
reread and reviewed many times it ends
up being much much better well why does
this really matter from a hardware
perspective well behind our level one
cache even we have a level zero cash in
our processors many people are not even
aware of this and not as a cash off are
decoded instructions so our acts 86 are
kind of high-level macro ops our
professors don't work on those they work
on lower-level
micro op instructions so they have to be
decoded and not decoding takes a lot of
energy so if we can stay within that
decoded cache for many of our loops
things are much lower on energy they
also tend to be faster and more
efficient they also work better with our
branch predictors so keeping our loops
small and elegant really helps this and
we're typically looking at 1,500 plus
micro operations
that's not fifteen hundred lines of code
it's a much much smaller
than that but keeping things small and
elegant really starts to mater it even
goes further than not slightly further
back in our processors there's the
instruction dispatch Q and that Q is
typically only 28 micro ops per thread
that's going through that and if we have
really nice small little elegant loops
they fly through this part of the
processor so like running over an array
of doing a summation with only a few
branches maybe or not code that's the
stuff fits really nicely and there so
keeping stuff really clean and simple
works really nicely with our hardware
and we can find that we're tripping over
some of these thresholds there's ways of
measuring using CPU performance counters
to get that so the major point of this
is we need to craft all of her major
loops we treat them like pros make them
clean make them elegant make them simple
and by doing that we end up with much
cleaner code so it's kind of respect the
single responsibility principle I think
we shouldn't be just jamming another bit
of code into some existing code just to
meet a requirement in a hurry that will
cost us we will get bitten by that
keeping it clean and simple is actually
one of the better ways of keeping it
much faster and much easier to test and
then we compose all of this stuff so if
we're gonna make things small we're
gonna make them elegant
we've got to think about composition and
the best way to think about this is
thing but that keeping the size smaller
but also the semantic footprint as well
as the physical footprint of it
because if we have things that do one
thing and does it well and is not
coupled to its context it's so much
easier to reuse it's easier to compose
with other things and this starts the
really modder with our compilers and
especially in things like I'll manage
runtimes like grip a quote from cliff
click before is he said that inlining is
the optimization so if we make things
nice and small
the in line really well and our money's
wrong times can make the inlining
decisions based upon theater of what
they're actually saying so we can have
lots of nice little small methods that
will get in line that combined to give
the ideal code for the data that we're
running through it if we make things big
monsters they're not good for
they don't suit this sort of things burn
it in lines all of that function call
overheads gone at that stage and we can
let them run time make these decisions
based upon the right thing that means we
have to really focus on single
responsibility sister thinking apart one
statement one thing one method one thing
one class one thing one module one thing
just if we keep things really simple and
focused it's very easy to make it
optimized and perfect for what it's
doing and with these small items we can
pose them up to make really interesting
things probably the biggest thing you
can take away from this is really
thinking about RFP eyes this is the
thing that I find makes the biggest
difference to almost anything the wrong
API limits so many of your design
decisions behind that if you get the API
right it enables so much I spend a lot
of my time doing networking code and
that means unfortunately I have to deal
a lot with niÃ±o so if I want to read
from a socket with an i/o the code is
actually pretty hard so what have we got
here it's like I want to slack now from
a selector then I want to get the key
set off what's available I'm gonna
iterate over that I'm gonna then see if
any if that deal is readable and then
process it and has got to remove it at
the end of it like that this is pretty
nasty it makes my eyes bleed even
looking at this and particularly it
causes so much allocation it imposes
wears of working on the calling code you
could do this very very differently if
you think about it from a different API
perspective so like particularly looking
at high things like the selection keys
are done they're an ANA all of the
boilerplate we've got to do with that
already and over this so let's say you
pass in the collection rather than
return the collection just taking that
difference in design approach means hey
I can reuse my collection as much as I
want I can choose which collection I'd
use for that and I select my and I pass
in
the set of keys that gets populated I
can even provide a filter and then I can
for each over it no people say this
looks like functional programming and
it's kind of grid that we're getting
this way with Java yet but we could have
done this at the very beginning we need
to be thinking about API so do this
because now we've got choices over what
data structures we don't have to have as
much allocation we can control this
around much better spatial locality just
thinking about the API there's loads and
loads of examples to this and I find one
of the fundamentals in the design is is
you give the caller the choice the
mindset should not be that your caller
is dumb and you're gonna impose a way of
work you know know if the mindset is how
do I enable the caller to do things how
they would like to they can choose the
collections they can choose how they're
gonna process this afterwards it makes
it so much cleaner so it's a kind of
mindset and cultural change we you have
to think about enablement it ends up
with nice cleaner a code it also ends up
with code that's much faster so how we
think about that starts to really modern
going forward we also have to deal with
a lot of data and typically data is
really just big tables of stuff now we
could implement this in a number of
wares in your typical old language
what'll end up being is an array of
objects and those objects are randomly
all over the heap and we've got some
interesting issues that we don't play
well with our memory subsystems well how
can we deal with this differently
especially we start looking at filtering
searching finding doing all sorts of
interesting jobs across this if you just
change how you think about the data so
rather than having a pile of objects how
about having a collection of her as
where each one of those res represents a
field within what would have been those
objects so we start looking at data in a
different way and if they're all just a
razz
it opens up some really interesting
things where we can go down and search
the whole array we know apply where well
for memory subsystems that we were
thinking about before we can start doing
cool things like factorization this is
where the processor can do the same
instruction over multiple items of data
at the scene
time and be fed free and really really
high rates we can get much more
interesting instruction level
parallelism out of our processors if we
program to them different and if you
want to see well what is the whole
object view at a given stage well we
just treated as everything across that
index is kind of there we can do these
things right now with all our modern
languages start thinking in columns as
well as rows there's many dimensions in
which we can look at a problem but we
just start thinking about it and that
way we can deal with these things in a
different way what this is fundamentally
come down to is start embracing many
different paradigms you may be
comfortable with though I'll start
looking at functional look at set theory
set theory is one of those grid
overlooks subjects there's so much
goodness in set theory that works really
well when we're doing any work on data
so much of the applications we write are
effectively just queries against given
data and we start thinking about how do
we deal with them assets there's so much
more we can do so how do we start sort
of pulling this together and how do we
test it well if we're gonna start caring
about performance the first thing we've
got to think about is well what are the
goals were trying to satellite so what
will be a good performance goal for our
software don't just say I want something
to be faster target I want to given
throughput and I want to have a response
time at different levels of throughput
and that way you can kind of start
driving this and also know when you're
done like knowing how fast something is
doesn't mean you take action against it
to optimize it or whatever but knowing
what it can give you starts to become
really an important I think it's it's
actually a responsibility of us as an
industry becoming more grown-up and
professional that we know what our
software is capable of dealing with if I
was designing a bridge and I didn't know
how many cars could drive across that
that would be bitter responsible a
building and you don't know how many
people it can deal with her how quickly
can evacuate it but most people have no
clue at which point their sulfur is
gonna break just knowing that it's a
really useful thing so typically we need
to start measuring another measure
things like response time people will
use things like averages and averages
are so misleading
if we capture say in a histogram or what
I've got here is the number of
observations on a given response time
I'm plotting it out like that
what does this start to tell us well
let's start applying averages what's the
mode in here so mode being the most
common occurrence that's reasonably
useful tells me something median where
does that fit in well if we line
everything all up and we take the
midpoint of the line we end up here of
observations not telling me so much the
mean which is typically what most people
think about an average I see doesn't
tell me anything useful here at all so
this is a system that I'm putting
transactions into and my mean response
time is coming out giving me a figure
that's not very useful it doesn't tell
me what is the most common case we're
dealing with
it also doesn't tell me what have I got
these outliers over here that are taking
quite a long time and really throwing
off my averages we gotta stop measuring
and averages we've got to capture
histograms of stuff so we want quantile
there are percentile distributions over
data gives us a much much better view of
what's going on and we also got to be
careful that we don't get tricked so if
we just match a service time and that's
like the time within our application to
do a job it hides a whole pile of issues
it particularly hides the fact that
queues are forming outside of our
systems so say for example your system
is taking a long T C pause that's not
stopping the customer sending requests
in those requests are all building up on
the network buffer and just weeding on
your system you'll be deluding yourself
thinking well I'm fairly responsive I've
only got a few requests that didn't take
so long you're not accounting for all of
those queued up requests and so you're
omitting things look up coordinated
emission it's a kind of really
interesting subject it describes some of
the problems here capture this stuff put
it in histograms we should be histogram
mean all of our major services so every
service we should be capturing what our
response times what our cycle times will
do all of the different service times
that we have inside this this way we can
start plugging in the queueing theory to
know what's going on if we want to start
testing it look at
we're tools out there and I things like
Jam hitch will let us run micro
benchmarks but also much larger
benchmarks and run them repeatedly and
plugin in tune really nice profilers
that help us know with what's going on
this profilers not you go down to great
levels of detail we can see inside our
cpu seen branch missus cache misses
so the predicts neuron predicts all
sorts of really interesting things about
our CPUs we can start getting
experimental evidence that tells us what
is going on and then we can have our
theories tested on this and we can
improve things once you get this going
put it in the continuous integration we
should be doing this all of the time
see I should be a constant thing that's
running and so we should be feeling if
we get big regressions the really
important thing about putting this into
CI is when we have a regression we know
straight away we know what the cause was
that caused the regression just like we
know when we functionally bracts
something it's good to know when we end
up introducing a performance issue we
can go straight to it we know the costs
of different things probably much more
interesting is what do we do with our
live systems we should have telemetry in
our live systems as a first class thing
and by this what I mean is we should
have things that in real time we can
inter spec the system without impacting
it without slowing it down without
causing any other baaad a fax to see how
well it's doing in any given point in
time I'll make that point again please
build Clemmie tree interior life systems
if you look at other disciplines so I'm
a huge petrol head I love motor racing
the reason it's got so good is because
of testing unlimited and getting that
measurement back there's so much time is
spent on that there's lots can be
learned well what would that look like
for our own code well we should be
capturing cue lance we should have cues
explicit and we should capture them we
should know things like what laughs look
concurrency were experiencing we should
be capturing things like exceptions and
not just putting them into the log
making sure we tracks that's all really
well and keep characters over all of the
things that are really really
interesting the nice thing about working
on some open source stuff is I can put
out examples of showing this so if
you're into
what does this mean I happened have
worked on a messaging system called air
on and it's got a system contour section
in it and it keeps counters of
everything all important see it happens
to get written in a way that's so easy
to introspect so easy to observe what's
going on that helps us get a lot of
performance but it also really helps
with debugging and understanding what's
going on in the system because it's not
a black box anymore then same with their
histograms to capture Encounters of real
time stuff maybe sample those put them
into histograms and then we can start
getting well okay I may have a good
average but what is the distribution
looking like I'm not how do we get that
sort of stuff right so now to kind of
wrap this up and covered a lot of
material what are the major takeaways
here well think from a clean code
perspective we got to stop thinking
about how do we contaminate with loads
of stuff it's also we want a simpler we
want to have a lot less code that we've
got to deal with and that way we can
make it much much faster and much
cleaner and much easier to deal with I
think good performance is based upon
just some really signed design
fundamentals these are just good things
we should be practicing these are the
things that we should be practicing
every day so that it becomes second
nature we see you shouldn't be
questioning things like if I'm gonna
pass an object from one thing to another
do I pass the whole object do I just
pass what I need think about the levels
of coupling these sorts of things start
to really matter and you can see this in
code you get to the point where you see
code and you know it will do really well
very similar kiss was bill Lear who
built the Lear jet he had a great
expression that if it looks good it will
fly good I think it's very similar with
code you can tell code that looks nice
and this nice and simple and elegant and
to the point and it tends to perform
well it tends to be robust the tends to
do what it needs to do when it's hugely
complicated and hugely contorted it
tends to not be good on any of those
sort of fronts so we want to keep it
really really simple and on that nearly
out of time have five minutes left
I'll hopefully take questions if
anyone's got any or all lectures all run
away so thank you
okay question can a good question so
we've got many levels off microcode like
be a bytecode via the soundly beaten
micro code should we look at skipping
them maybe maybe not I think if we do we
should do it based upon measurement I
don't think our issues are at some of
that level at the moment there's
probably an issue around energy required
for some of the decoding steps but I
think personally I would do it based
upon measurement okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>