<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Billing the cloud: Real world stream processing by Pierre-Yves Ritschard and Marc-Aurèle Brothier | Coder Coacher - Coaching Coders</title><meta content="Billing the cloud: Real world stream processing by Pierre-Yves Ritschard and Marc-Aurèle Brothier - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Billing the cloud: Real world stream processing by Pierre-Yves Ritschard and Marc-Aurèle Brothier</b></h2><h5 class="post__date">2017-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7vLBZU1s4nU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thank you for coming to this
talk I hope you're not too tired today
after all this talk this morning and
afternoon so we're going to talk about a
reward stream processing use days
compared to the Alex one this morning on
the Swiss railway system and it's
actually to build our customer at
exhaust game that's where I work so I'm
the senior engineer I'm not this video
you couldn't make it today so we need
the backup plan so what's nice i could
comment tu vas pick which for you whoo
if you are using cloud services today
I've ever personally over personally for
these guys you can leave your hand up
and who have you as using them for on
your daily job as well and running
projection okay so st room for us to
grow that's nice and so today we're
going to give a problem with try to
tackle we're going to see the scaling
methodologies which has been applied i
could have been applied and the approach
we took on them so first what we doing
is what you're going to see as a
customer coming to her console and you
can also create the ends and use the s3
storage it's free compatible storage i
can we have or using dns services for
api or having a status page and so from
this picture what we have is weak to a
concept of it it's the control plane
from which we have events communication
with a Parizeau with all these vm system
we also communicate with the storage
back-end system and we third a TI like
for example for v dns
and on more human side we can see it's
even starting VN or stopping the ends at
the n as well sending some data or
objects created delighted as well
someone with usage on them which need to
be taken into account for us and of
course infrastructure isn't free so we
need to have a business model on top of
that to provide the cloud infrastructure
as of a big first step of a service to
do more of it like for example with the
s3 compatible storage we have and also
to do some profit to be able to continue
and run business so from this picture we
want to get a bidding for our customer
and to have something simple so y1 which
would be something like this where you
have signs of uvn you've been running
for how long which has a price depending
on size and same for a disk and for the
traffic and at the end you get a grunt
at all which will come every month and
so looking at the big picture of this
with what we had before is we have a
frame of events coming in with a user
and account which started a vm at a
certain time and we get over events like
an IP which has been sending out a
number of megabytes and over objects
we've been created under an account and
has a size attached to it so from this
we can see two things it's categories
countries sorry and resources point it
is it's the 10 megabytes and it's a
simple thing because you have almost
everything very into the event and the
resources it's like an account which
started something
with a profile at a time and do
something else on the same reserves at a
later time if we look closer to what we
really have in production it's this kind
of message with a typing the usage for
an entity vm action create at a certain
time music and template which in case it
was a windows would have added some
licensing fees from which system it's
coming for which account having a unique
identifier and of course offering the
signs of a vm on the network side we
have IP measurement which comes in every
minute and sends an heater to address a
number of bytes and have a valid ID from
which time it was taken so in verisign
of it the quantities i will think is
pretty simple because you can just
simply attach a price tag to you even
having the IP and the 10 megabytes you
know the price per megabyte so it's easy
to just put a tag on it and to build the
customer who you is using this IP
address so that and the interval as well
is well known so it's coming every
minute so you don't have to think about
changes in between but roses are harder
because sin is is you have to keep track
of the time of each actions and what the
time has been pastoring between whose
actions so you have to keep a state
machine or each of your resources and
for each of your accounts and compute
this interval so this is on their
account and if you multiply with all
your customer it's getting to a pretty
big size of a state machine
so if we wanted to solve this in the
easiest way most naive way it would be
just to fetch all the events and
grabbing the identifier the time of that
event for the end for example and if it
starts you keep at what time it's in
this event and if not for example it was
a scale or anything we can then compute
the time from the previous state you
kept to compute the usage and send it to
the maturing which would be being later
and the only thing where for example
would be that since you only acting on
even sir you are not being able to send
a metering if you don't get any events
coming in if I started the machine VM
and it's been running for weeks and
weeks there's no stop even there's no
scale even there's just nothing so you
have to find somewhere so you can build
your customer so in the real life side
this is can be seen as a never-ending
process because we open 24 hours day or
year round we also want a minute
precision billing so to be fair with you
so if you're just using that at the end
for 15 minutes it's just going to be
paid for 15 minutes and we also want to
apply this once an hour so that you
could see you credit going down more
naturally than just at the end of a use
each event of a stop of resources and of
course we want to avoid at all costs to
overvalue because otherwise we going to
lose you and also to earn the bill
because of a wide we losing money and
we're not going to be able to pay our
engineers so practical matters as well
was to keep a smaller operational
footprint on it because it's not our
main business I would say to be our
clients it's everyone has to do this to
get me to
the money to pay the engineers but we
wanted this to be able to be on what to
be running on one sign like forget it so
we could have our engineers focusing on
over stuff more fun stuff and as well in
terms of resources hardware not using
many big servers which is set so the
naive approach we took first was as
everyone would have done it I think
using a cron job it's been there for
years three arrival in with working
running every hour that's what we do
it's perfect so from where you would
extract events for an account computes a
metering and we use age with this type
of data so an even has an account versus
tied with society timestamp and also
even type on that you can deduce use age
with the account and user to type
compute the duration and attach it a
time stamp from Vera you can deduce the
transaction which needs to be done on
the account for what kind of use age so
like in using v n loud size large for 20
minutes flat price and the advantages of
his approach all rights reloj a personal
overhead because it can sit down on one
server and you know it's not going to
sale con job is rewrite reliable the
functional boundaries are simple and
it's easy to test because you know what
you have as an input and what you want
the output so testing this system is
pretty simple but on the other side so
more successful you're going to come
more clients you're going to have a more
even seem gonna have so you're going to
get high pressure on us square server
just by retching Orleans so it you're
going to reach a point where it's going
to be really hard to keep up running you
cron job in the timeframe you want so
you're going to end up with overlapping
jobs and that's going to result in
longer metering intervals so you're not
going to have you credit coming down
after an hour but enough stone over five
and ten and fifteen and as well a
never-ending growing problem so then you
have to decide and you can you in a room
full of overlapping cron jobs and you
can hear the screams of a dying my
sequel server so as in the right tools
under here to the west us map reduce or
to the wet east streaming so let's see
and if you want to buy a bigger hardware
scale vertically but it's not going to
be a solution at the end you over to
reach the limits of a system so you've
been eaten by a grue going west in the
MapReduce batch processing word there
were two the most natural choices were
which has been a dupe and spork aduke
with HDFS and the MapReduce which hike
you with parallel processing they're
perfectly simple because it's like a
distributed front ask you spit on you
cluster it spreads easily because it's
pretty easy to scale originally and add
more notes to your cluster to do more
jobs and you what if i bring you is this
data locality a word processing is you
have to move data to all your notes but
after the processing is done on each
node with the data they have so you
don't have to move again over data in
your system
so an example with your like an arrow
hello world example would have been an
issue with multiple big texting text and
you want you to count each word in them
the count of each word you would have to
load all these texts into your grocer
and written into you notes and then
where the map function will just break
each line two words an account of the
world in the line and then reduce
mention witches will aggregate these
results to give you these words has been
seen that many time in all the texts so
it can be seen with the HDFS like et al
because you have first to move a big
bunch of data into the system so where
you have a high latency and as where you
have actually got of effort to be put to
run them because it's not really easy to
set up a closer and run them daily and
there we never found really another use
case to use them so let's I can already
say it wasn't really a choice for us and
if you want if we were doing an analogy
to the energy sector would have been
like if you were doing a energy you
would have a coal mining facility and
you have to move a coal for your welcome
to the program factory and burn it were
so you could scale and has more burners
for this more rectory city but still you
have to move all this goal to get your
power so going another direction as
Gasca storm for stream processing so who
can be written be seen as a continuous
computation on an unbending frame so
it's never-ending theory of process and
as villains are coming in into the
q but immediately processed so when you
re have a very low latency which was
which is really nice and it can be seen
as a never-ending reduced function over
what's coming in and the result your
computing from that so in closure an
equivalent of this would be over
collection to apply a function and to
still have out of that an access to the
computed value all over all the time but
it's a bit harder to consider eyes the
system to make it running it is still a
to think where you're going to store the
intermediate results where do you store
the data and how does you data flows
between each computations that you have
a new process so back to the energy
sector it can be seen more like now as a
damn water dam which binds and where you
can really see you as very low latency
upper to sleep producing electricity and
you can just scale as well as you want
so better so to take our decision we set
up of course our shopping list see what
we need indeed we needed operational
simplicity so really like how engineer
could just focus on doing new stuff fun
stuff and having to run the projection
and Tuvok rember prediction or just
being piled out of the production we
also wanted to be integrated into a
stack of language in Norwich because
even if we like sometimes try new things
and stuff it's still a critical part of
the business we need to get aid at the
end of the day so we wanted something we
knew about and we also were looking for
something we could reuse for me
really like the logs or other stuff we
have in the system we're not really
giving out to the customer yet and so
for the personal simplicity of course
experience matters because that's why
you could not you get out a proper
knowledge of how things are working and
spork stone point me dating hatch
beyhive we didn't took them it's to over
break you have to put on top of a duped
to make stuffy here to be able to query
and store your data over and buy at the
end of the day you're just having too
many dependencies too many stuff to make
the world system up and running so as
well for more dependencies with more
chances you asked with the exam problems
later on when you want to upgrade of the
flag red and the integration um HDFS or
for Hadoop would require a pretty simple
it's us connector for java that you just
have to write over bindings for a system
to communicate with always tools we have
a spark would go with end-to-end with
cassandra and we are already a running
Cassandra cluster in prediction so that
we knew and dome actually is using you
could be using gasps gasps oh why not
just simply use Kafka instead of both
and of course so room to grow we have a
ton of logs ton of metrics coming every
day out of what our customers are doing
so would be the next use case we sing
with them and the coefficient which
actually made way to die for Python the
beginning we say is not me but sure if
CTO new Casca from his previous job so
it was we easy peak to take in that
world
so Gasca who uses Gasca today okay in
prediction or just to play prediction so
whisks can be scenery as q where you
have apps I have a writing messages to
them or reading messaging out of Kafka
you can have even a nappy I to process
stuff inside kashka directly and you
have a storage you could still say
anything in database so publish it's
really a classical pub/sub system like a
message queue rabbit and Q and Q Series
or levels it lets you so process events
as they occur into the system and it
lets you store even in a fourth orient
way which we handling all the cluster
and master slave replication of
partition things for you so you can just
forget about women which is pretty nice
when you want to run something freaky
cutting production so for me it's kind
of a message queue on steroid so publish
crime so the messages are produced and
the stream Gasca are called topics so we
arrive in topics the topics have a
predefined number of partitions and the
messages which are sent to a partition
to decide on which of them it's going to
be sent it's based on the key of a
message the consumer get a fine set of
partition the read from and the consumer
stove the last consume offset in the
queue so they don't always be read by
the way they know where they are in q6
and messages and brokers who own the
partitions like a node play on the very
replication
so here we got the like a more
descriptive picture of how it works so
you get on the left side or you
producers so it could be for us
cloudstack the compute engine the
storage engine ens whatever which are a
client to send a synchronously messages
into Casca with a key being the account
and a payload with messages we saw
before and then Cass Cass check the
account the key decide to which broker
so which node in the cluster it's going
to be sent and the broker knows which
consumer is reading this message from
that partition and forward it to that
consumer what is very important here is
you have one consumer to read one
partition because otherwise you're going
to have if you end up with multiple
consumer reading the same messages from
a partition you not sure what you're
going to process the message is interval
x in your system and the consumer can be
grouped into an ID so it's one consumer
in a group ID to read one partition
let's and we have a stable consumer
topology as long as the consumer on the
right-hand side don't change we can then
disaggregate the memory used by the
system by speaking the load into all the
humor and we can rely on the in-memory
the rage like for example if you message
is where to be with a key URL that you
need first download and processed scribe
data wherever on the consumer side you
could just handle the handle cash
knowing but for that URL message should
have always been coming to the same
partition and into the same consumer so
then can you really use the cash to not
redownload it if it is already
before applying whatever he needs and of
course you need to expire a bit stream
because of wines just going to grow
bigger and bigger and bigger and what's
nice you have two choices you can do it
as usual way I would say by numbers
saying you want to keep only the 10th
last messages into the queue or what
with staff car you can say expire them
by time and say I want to keep my
messages for the last two weeks only so
that's really the main difference with a
normal pub sub system mrs. meet you so
back to a review of what we have for the
events our customer I'm making and we
using as a bidding input so the vm
studying at a time I piece and object
created on an infinite roll of paper
over then we have to have keep estate
for all our accounts and here we end up
with the partitions so you see the
events coming in the messages with hits
a keeper partition that you will imagine
that multiple accounts on the same
partition of course so you have a Miss
event arrive and then the consumer can
already compute a state and keep a state
for example on the icon a you know that
VNV be has been started at that time and
later on got to stop events we can
compute in running for 15 minutes and as
well another example would be the
scaling thing where you have a start
scale and then stop so when you scale
you can have a usage for that price of
avian and then change you internal state
because it's a higher ego VN and compute
something else for the next stop so here
we gettin resold
all of our shopping list and what we
need to do so what happen when the
process crashes the consumer crushes on
the other hand what happens if we don't
get the messages from the original
system like the ends engine or the
storage by can because we had some
network troubles August in was maybe
down happy working and but we don't get
everything we didn't either fix the
double billing and we still have to find
a way to charge you every hour so you
can see you credit going down so when
the process the consumers crashes it's
changing the number of consumer /
partition so cascade do a rebalance of
all the screen humor and then you lose
memory you head on them and you send you
don't have any initial states which is a
problem for us so that's where arise a
reckons I reckon see later conciliation
we had to write its system takes a
protest at X picture of inventory of
everything we have every hour it's done
really efficient way to pull speciality
eichholz to really not put too much
pressure on our system and be very fast
and this gives you an initial state at
regular in the hole so we could start
again it helpers them if we didn't get a
message to change the state in the
consumer because if we didn't get I
would save a scale message for vm these
snapshots gonna say for that consumer
vnd database is being changed to a large
but you had a medium one so you can
datastage of course where are we going
to lose the time between where it
originally change the size of the
machine to the time we got these are we
can see 80 value it helped us as well to
not overdo our clients because it acts
as a logical clock in the system it's
how to explain we use it so it's a
picture so get a picture so you can get
initial state if the consumer crashes
but are we doing the bidding we let's
say start with this snapshot of
inventory so you have a state in each
consumer you get events for all the vm
and objects and after when you get the
next events of the snapshot coming in we
can say we now have a full picture so we
start to send the usage about that top
fat time frame of an hour but where we
have to still handle things like if the
processes crashes while doing by
computing the usage so we only
committing the messaging at the time
well of a snapshot from the time before
in case string the computing something
happens and then finished the wall
inventory billing and as well then
I when we use this offset of a snapshot
as a unique as a database integrity
check but all the usage correction are
going to have his ID because if it
crashes in the middle we just have been
bidding twenty percent of our customer
and we're going to restart the process
because we need to we read the cue we're
going to come back at the snapshot
inventory one step before we read all
events so we know you studied ECM change
scale and things but forward twenty
percent of our clients with user Jeff
and already sent so if we use this
offset which is unique we're going to be
sure that sending the usage to the
database we're not going to overbill
lose twenty percent client but we keep
on winning V over eighty percent so we
have rare handled crushes and without
overbilling you and I yeah one thing too
is about this transaction commits
usually when you read the message into
kafka kafka will consume automatically
automatically commit the last one you
read but ten because of this system we
have to do it manually so it's another
process inside with further commit when
the use H billing has been done
completely you can commit the offset
into Casca so looking back at what we
have today we put happy because we just
have a 600 line of code ultimately so
it's pretty simple and we got still room
to grow because we're still plenty of
things to do and the size of a system
how it's running it's really low on
verbal sources and really efficient
it's a very stable resilient because
it's been running since if i remember
correctly last summer which and
forbidding and since then we didn't have
any problem with it and we are already
using it now for the dns to do with the
reverse dns of the ends and we want to
use it later on now to be able to send
out with logs on matrix of the system to
our customer so like if I reserves or
whatever I can use them to do their own
billing in their own system so the what
about batch processing what it's not
dead the streaming doesn't work for
everything example would be if you were
to have a system to analyze tweet or
text messages you will want first to
create you data model with a large set
of texts with tagged with emotion so you
can have somewhere better to have a big
patch which going to create with data
model for you and then when you have
these models you can apply the stream
processing to the rigid to analyze nails
tweets or whatever you want because then
it will move through put at the end that
matters and we have a rotten sea of a
system at the beginning and any question
about yes
yeah so you highlighted a problem that
comes up quite often we this will be
streaming which is duplicated messages
which may happen with Kafka and there
are other kind of problems like messages
coming out of order or ask you between
the processing time at which the event
comes into the system and the actual
time at which the event occurred if you
have had any kind of problem with these
kind of things and have you have
considered any kind of solution with
existing tools like for example passive
link that make up into helping you with
this kind of problems well for the
messages are arriving out of order it's
well if you get the one say for example
you get the guys starting a vm and
stopping in right after so we gotta get
the stock first on the start later I can
be sure but I would think now because i
can write these more code of spot of
this stuff we would just discard with
two minutes because I mean the order of
order will be really damn of three short
things and so with what we have the
messages we don't really have any
problems which is what you told so it's
in our UK it's fine
yes hangul low and I didn't check before
coming here to see how much is coming
fro but from what I think we have a
cluster of three four nodes I know I
should have checked I was a good
question I can I try to put it on
twitter if you follow on with yash tag
of a conference so I get you reply ya
think so you took I had a question I
think it we are we really appreciate
that you don't try to about charge or
its imposing it was scared but then
somehow if the if they are very rare
events I know the Cure 11 percentage or
the one percent of billing would be
changed what shortcut do you take you're
not making a bank retailing system
yeah where you cannot afford to miss
nothing yeah you got evident if someone
if you miss one minute for my usage or
you charged me twice once every month
frankly no one will ever see it and one
Kevin how do you put that into
simplifying your system then we can make
a few shortcuts because we don't need to
be exactly one hundred percent
consistent well we do the one hundred
percent consistent on the overbilling if
we will I explain about the transaction
the ID integrity into the database so we
know we can't put same usage convection
for the same account on the same type of
resources for the underpinning we're not
one hundred percent because one over use
case where we going to lose metrics is
if we want to scale up and change the
number of partitions we have in the
system because it's getting too big then
from the time you're going to do a
switch because not even Casper to add
partitions we're going to most likely do
some searches
that's going to be for an hour or 40 on
me so that we're gonna have to take the
ghost of this and there afterwards I
understood you put the data into camera
right now afterwards it's into my school
oh you're headed back into my secret do
you do any machine learning or post
analysis of the data too no not yet it
just started with GPU instances so it's
going to come into the picture to also
have a oil use H of boosters but now we
out is for weekly more focused on
putting new features to really cloud
infrastructure and to use the data we
have and see what we can do with them
but because we already graphing all the
events so we do it by visual human and
seeing what's going up and down and
what's going is thank you any more
question oh thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>