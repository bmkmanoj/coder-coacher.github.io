<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards a rebirth of Data Science by Andy Petrella/Xavier Tordoir | Coder Coacher - Coaching Coders</title><meta content="Towards a rebirth of Data Science by Andy Petrella/Xavier Tordoir - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards a rebirth of Data Science by Andy Petrella/Xavier Tordoir</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QTG66OZsbdo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi welcome everybody I'm glad to be here
for this talk on towards Weber
of data science before I would like to
know a bit more my audience who is or
consider himself as a data scientist
here okay a few of you who is already
you know working or playing let's say
with distributed systems cool awesome
and both of them doing machine learning
or whatever on distributed systems a few
okay that's why you are here Dan okay so
about this talk first so it's gonna be a
talk that she should have been given by
me and my body exhibited war but
actually is in Mountain View giving the
talk attached to a word on machine
learning on genomics so it couldn't
attend it with me so I'm gonna give it
myself alone and sad so so my name is
mud sab I don't have any names not just
kidding anyway so the outline for today
first I would like to introduce a bit
what was or still a bit is data science
pipeline or product what change since
then finally what is on in my opinion or
in the opening of the tavella's a
distributed data system data science
system what are the challenges for this
kind of system and what we want to do to
go beyond that so to solve the
challenges and to move forward so that I
fellows the company that I created with
Xavier six months ago so I pre-new
myself the gorilla and the Petrella
Notes app so I'm basically a
mathematician I did a lot of just
spatial in my in my world time I did
plenty of the stuff doing my first time
I created this part notebook by the way
who knows park near great who knows the
spark notebook a staff you so it's a
tool that I created in order to to play
interactively with with spark and
disputed systems I'm a sparking scanner
trainer and I did machine learning since
my studies by the way exactly is a
just bioinformatician PhD in physics
actually distributed computing we does
that all the times in since 2012 i think
i just kinda and a polished guy a also a
spunk trainer with type-safe you know
machine learning is the core of our art
right so first let's time with the
legacy stuff so what is data science
pipeline / project so everything sadly
enough start with sampling so we when we
deal with data when we start a new
project we start always by sampling the
data and I will get back to that for why
we are sampling data so we get the data
we clean that so in sampling I consider
also the cleaning phase because it has
to be done of course because all the
data are always crappy and you know that
she turns it out so we try to clean the
 first so the sampling is this phase
right so when we have a reasonable
amount of data we can stand modeling so
what does that mean modeling finally so
it's not finding rules that can describe
the word is more trying to fight the
best function family and then the best
function instance that can in some given
that some data represent the generative
function so this is what exactly is
modeling when we talk about machine
learning for instance so we have modeled
using the data and now we have to tune
it it's a back-and-forth process of
course we choose we model with you we
mobile model and when it's done when
it's done we create a report we created
the report y can be a report can be a
PDF can be a static web page it can be
something that has been generated based
on the data that we had initially
sampled and when we have this report we
pack it into a male or a package or
whatever and we send it to some people
that can take some action based on the
data on the function that we created or
the eraser that we came out with so and
this guy who has
mainly the knowledge guy so he knows the
domain so it has you know the key to a
to to act on the system to maybe two to
solve the problem that you have discover
or maybe to tweak it so only at that
time this guy is able to take a decision
however this decision is only taken on
static results so it only take the
report analyzed it many may be sorry
charts tables and then it takes an
actions some actions however during this
process static results has been
generated and a lot of information has
been completely lost in translation
because the data scientist has to
explain what the report is all about so
you have to dump its mind somewhere
right and also we have sampled the data
and doing the sampling afterwards might
be model we might we should have or
generally we have to to create the
features we have to select features
recombine them so that means that again
we lose a bit of variability of the of
the of the universe that we are dealing
with so the a lot of information has
been lost and also it really sounds like
waterfall or i would say what or fail
because at some point when the guy there
you know the interpreter has to take an
action right there is a high chance that
the universe has changed since then
right and if you want to to react on
that you have to come back to the
sources may be on the model you don't
even know actually where you have to
come back because you had again lost a
lot you have lost a lot of information
backwards and it'll really look like et
al finally right you extract transform
load which is the kind of thing that we
cannot we want to avoid these days so
this is what I call legacy sterile sands
pipeline and there is a bad phase
they're sampling why do we have to
sample Jerry we have to sample for two
main reasons the first is pretty obvious
first we had to deal with data on mono
machines
unless you are doing hpc but how many of
you guys I've deal as dealt with hpc air
okay let's find 0 so generally we had
mono machines right and manna machine
means we have a limit the CPU that we
can use so it's a very a constraint on
the computation and we can run and a
limit constraints in memory so we cannot
go beyond that so we have to cope with
that that means that if we cannot load
the whole bunch of data that we have in
memory or we cannot process it
efficiently in streaming matter right on
the money machine we cannot go to the
market so you have to be faster so you
want to be you have to be smarter of
course because they are not of sampling
method they are very very smart but
still this is sampling another reason
why we do we do have to sample is to you
know this be as variants program that we
have in data science so generally what
we have we have the full universe we
have a very small fraction of the data
of this universe and in order to not
over fit on under fit data so we want to
generate more version of the data so we
can have different version of the
function that we are generating to
represent the word and then we can
aggregate at the end so this is why we
have to sample but it's bad still it's
bad why because all world changes so I'm
not going to be very descriptive air
actually this is the the fact that we
are you know airing for two years now
about Big Data whatever it means so it's
like way yeah okay now we have data that
gets bigger and bigger and bigger or in
my side would say the number of sources
are increasing it is it is increasing I
should we have more and more sources
more services that are popping up
everywhere we just had to talk about
startup actually if you go to San
Francisco there is a start-up on data
science every day you know so that means
new sources new ways to deal with the
data and when I say that a science I
mean just also not really doing the
designs but really generating data only
so yeah this is a problem or this is an
advantage that we have to take
and concentration the second thing is of
course that the data is getting faster
and faster one example is simply you
know you can know what you know netflix
on your smartphone while waiting the
trial on the IRS or the plane on the
airport it's like remember in 15 years
ago in diavik's when we had to wait for
three or four days to get the a frakking
CD that we had to share with buddies I
mean it's done and man now we just have
to light on your your smartphone and you
can watch whatever movie you want to and
it's very I'm some so and we also have
to take this into account that means
that some consequences we can detect
some consequences on on the data science
pipeline that we are used to to to use
first sampling is very bad as i said
already first I mean it's more or less
impossible not to do it to do a good
sampling maybe it's not even necessary
anymore why first it's very hard to do
sampling on Big Data offers fast data
because there is a high chance as a
sample in it itself is going to be big
so if the problem was that you cannot
cope with your money machine right with
an amount of data right sampling doesn't
work as well or you have to general so
many examples that you will hit the
bound of the CPU about across
validations is that still relevant
actually the prominent we want to solve
it caught validations with K folding or
whatever we want to avoid to have
overfitting beyond the data that we just
saw right to a small portion however now
that we have so many gigs of data like
we can process right is still it's not
necessary anymore in some sense that do
cross validation if we are able to take
into account the full data set right we
can be almost sure but pretty sure but
I'm not sure that we have enough
variability in our data set so that we
can just have a test that i may training
data set and a validation data set and
that's it which can be two terabytes and
12 revised by the way but
still reports so there yeah of course
and samplings means that it's ephemeral
write the data is coming faster that
means that the sample that you have
created at time t 0 at time 1 is not
good enough the schema have changed
maybe the the world has changed some new
events has occurred in the news and then
everything is crude let's take a about
maybe trading systems I mean those guys
cannot cope with sampling anyone so it's
very fmall reports same thing a report
is based on the sample that we just
created and the first at first so that
means that of course is very quick
deprecated so you can really take
actions made on that that's mean that
what i mean by that you cannot really
take action on that and it's also a
realistic view that means that you have
12 tables for charts that's it and you
had two turbines initially maybe it's
it's a bit damn right so other
consequences is that the guy who was
very important at the end the guided was
able to interpret the guy that knows the
characteristic intrinsically of the of
the domain you know he's living in the
domain this guy has to cope with charts
and tables that are getting bigger and
faster so this guy has to what look at
the screen the whole day and trying to
detect life what's going on that doesn't
work like that I mean you know we need
to be smarter than a more clever than
that right so how to walk around this
this problem so we have to to work on
the problem that this guy cannot be in
front of his computer or maybe it won't
see the problem because if it happens
every every two minutes ago you just
won't see it so we need to to be able to
work around that and also it takes a bit
of time for him to read that I mean it
it it can take days before you come up
with a solution and it's it's way too
slow to get an error I efficient and I
enough enough sorry enough to to
validate the system that you want
but in place okay so how to work around
these problems so there are several
needs actually that we cannot depicting
the in the words of today so first maybe
we need a novelty system and not
descriptive descriptive chart of stats
something that can tell you a there is
nothing going wrong there and for that
of course we need to have more accurate
results if we have a lot of you know
force positive for instance it can be
very very boring for the guy to to to
use your tools so it's going to
duplicate it it's himself and the
reverse is is the same if you have too
many force- so are so you need to vary
to very take care on the accuracy of
models and to be able to do that so you
need more most private to have more
harder or just more models that you need
to aggregate at the end right like deep
learning so there is a lot of buzz
around deep learning and this is what
exactly was talking about in a mountain
view by the way so this is something
that you need because actually you have
so many parameters that you can take
into account with the amount of data
they will that you will deal with that
something like deep learning can make
sense and also we have the resources now
available to deal with this can of very
resource intensive models and of course
if we do that with a few data is not
gonna work Duncan network you will have
not enough variation in the in the data
that you had to be able to do something
relevant for the future and robust with
with changes so that means I need more
data and of course as I said faster data
that means that you need to have a
constant flow of the data so you need to
be reactive on that so the data should
be taken into account Hawaii are
arriving not today not tomorrow or it
may be a week after and finally online
interaction so the guy at the end
interpreter is still really relevant
meaning this guy has destined to be
there because II still has done full
knowledge even though you try to dump a
part of his knowledge in the model right
you still need this guy to be able to
take the right action
because it's the only guy that can
really interpret in the system what to
hit has to do of course if you'll have a
direct feedback from the action to the
system so you can detect what the action
has the implication and the impact of
the action on the system that means that
the system can also learn what action
can be done in which situation right so
that means that you could also help him
this is a direct feedback something like
recurrent network or just a feedback
systems so I'm in opinion in the other
fellows we just need distributed systems
to do that you cannot cope with the
manna machine we cannot cope even with
hpc you can do that with spc but there
are a lot of problem that with spc that
we talked about on Monday if you came to
our talk on machine learning air so we
need distributed systems of course but
what does that mean disabled systems and
data science what is the distribute data
science finale or system platform
project whatever so you want to start
analyzes right and you have this
distributed system in hand so first most
probably you want to create a cluster
for you yeah but there is a big cluster
and in my opinion it's very dumb to say
yeah you can you can take whatever you
want just just use the cluster there are
one of the machines take as much as you
as you want it doesn't worry I mean if
you have five data scientist is gonna
fight for the resources that is gonna be
not very efficient so you want to maybe
create a smaller cluster in the cluster
right so you want to dedicate some
resources to these guys so this is what
the very first phase that you want to do
in your in your pipeline next you will
have to find resources in the end
sources of course it's not different
than before you still have to do that
but now you have so many sources of a
level that you your work your new task
is harder now you have to find across
all these sources available and it can
be very tough and also trying to see if
they are relevant for your data science
prom and aorta killing so a nun means
you know it you have to understand
context of the data that you just found
the content see what quality its quality
the semantics of the different rows and
and the structure it can be very tough
afterwards you need to connect these
sources to your system of course that
means that you need to know the
structure right you need to know how
this information is structure on the
desk are in the database or whatsoever
in order to to get enough information in
order to be able to to process it and
model it afterwards it not that adds a
black box yeah put all the data there
and I'll just wait for some magic to
happen now you need to know the
structure need to know the data from the
core and and as it's very helpful to
have the scheme out of types of course
if you have the scheme and the types is
going to be where is your for you
otherwise you have to have these eerie
sticks that has to find if it's
enumerations if it's doable if it's if
it's a dummy variable or no I mean it's
completely crappy so you need to have
this kind of data books or types and
skin life possible next phase you need
to create a pipeline a distributed data
pipeline which is which is hard I mean
it's hard because you cannot really use
the tool that has been shaped so far you
can already use MATLAB you secret learn
all these tools that you were used to
used to do this kind of stuff now it's
just duplicated you cannot reuse them
because actually your data is completely
spread around did I just split it all
over the places it has different
structure it is it is chunked it is
compacted so you can't really use this
way you have to otherwise you have to to
collect all the data locally and process
your data is going to screw your machine
right so you da you have to have models
that have been thought in such a way
that they can run in a partition manner
so you have to distribute the function
then collect only the result at the end
when the process has been frozen run on
the system on all the nodes on the
cluster so he has to do that any can be
a tough task of course and that will
require love tuning tuning the accuracy
of remodel right so you have to you know
you have the targets you need to reduce
target and you need to reach its target
a within certain time bound so it's
about performance
of your of your mana so you have to tune
that back and forth you need to you know
check this mobile you know maybe tune a
bit do I parameters so and then you need
to maybe add one node or reduce the
amount of memory dedicated to the to the
nodes and increase the number of no Gina
now it's going to be quite tough as what
to do right but you have to do that
otherwise you might lose a lot of money
and running system that are too big for
for your system and then your Reggie you
reduce the number of models you can run
in the system at the same time so it's
very important to time to think about a
new thing is writing the result two
sinks this is something that wasn't
really dead of the in the legacy
pipeline now you you know you need to to
write this information to some somewhere
not in a report right actually the thing
is that your data gets bigger your data
gets faster that means that i mean the
result themself will be big or fast they
want you cannot stick them squeeze them
into a report it's not gonna work right
or otherwise you were going to fall back
in the same problems invest in static
results or the deprecated result very
fast and so on so forth so you need to
write this information back to something
that you can consume afterwards so you
have to consume them this this result
are also big data fast data so you need
to you know to access them so they are
maybe not gfs Cassandra whatsoever now
you need to access them that means that
you need to to have something like
services so we do services to access
data is in 90 so maybe even before with
a so and so forth so these are still
relevant now we have you know they're
there was a there were a lot of talks
about microservices then it makes sense
even there with big data between coats
because big data for me just a dumb word
I prefer to talk about disability stems
so err microservices is too relevant why
because actually you can create a kind
of micro services with the few function
that can access the result of a model of
a pipeline that has run on a system or
still running a planning system by the
way
the user access is the other side of the
report right is your you have services
so now you have to have another layer
outside that can consume this
information so that means that you need
to provide a way to access this
information so in using services using
relevant technologies that can scale
because you don't want to have your
users of your services using the same
cluster as your processes because if at
some point you have done used 10,000
users where you have only one thousand
at some point in time you don't want to
scale your computing computing cluster
right you just want to scale your
services that means that this thing has
to be targeted correctly so that means
that the information sent to this user
access has to be very condensed at least
binary right scheme oriented if possible
and then you can have connectors to two
other third parties like you know other
services or dashboard whatsoever you
don't want to provide PDF with tables
and schemas or in charge of course you
don't want to do that i mean with
turbines of data that are changing every
now and then so that means that you can
create other pipelines based on the
results of your previous model or the
services themselves this is a resuming
of a distribute data sense science
pipeline would say however hey what
about productivity man creating a
cluster you need some hugs guy that
might require so many things like you
know maybe messes so it's a it's a
coincidence so messes you might have to
use HDFS install in HDFS it stalked
action there is a talk about tachyon in
at three o'clock I recommended to attend
it spark a Sandra elasticsearch Kafka
whatever so you might require all those
tools to be installed over there in
order do for you to process correctly
the information and
you need then you need to find the
available data okay so you need to a
data and engineer that will require all
the stuff like again tachyon Cassandra
elasticsearch DFS connect to the sources
now it's a combination of the apps guy
you know that knows where are the
machines and a data engineer that knows
how to connect with Eve night to these
tools in the machines so you might need
no spark Cassandra acha HDFS I've row
drift please Jason if you already have
to do that but please consider binary
and schema oriented formats and finally
you can start your pipeline only back
then I mean we have already a messed up
slider and then there you have spark you
can use sparkling water from h2o you can
use all the toolkits like Dell at Intel
or cysts FML from IBM or deep learning
forge a spark ml from sky mind or
whatever so you have plenty of different
option right now and we added a finish
we are creating another deep learning
library with Stanford and with nitro
right now so it's gonna be yet another
one so you have plenty of options so
that means that you need to you know
understand the models you need to
understand the code correctly so this is
something is part of the on the pipeline
and finally you need to tune de Cressi
so you back with some medicines park and
sparkling models because this is many
tuning parameters then you need to tune
the performances so a scientific guy
needs to deal with the knobs guy in
order to head machines maybe for CAF ki
yeh i need to to the memory for my for
my Cassandra nodes or increase my HDFS
memory on face add new data nodes
whatever so this is how you you want to
to process this phase of tuning the
performances writing results two sinks
okay again you might want to store them
in HDFS something which is really
relatable really resilient and and
replicated so spark can also be will be
used for instance to store data but you
will need some nodes of
to run it some icfs again to to get to
get the data elasticsearch maybe if you
want to index some stuff kaif kaif you
if you want to write it back then the
y-axis layer okay access layer which is
just the accepting the different
services you can run you know I can
spray tycoon you need measures to scale
it up and down you can use vertex
whatever or anything that works or not
GS no just kidding user access user
access it's the easiest path for us
because it is not the overall to do it I
mean just the connector to tableau for
instance using the different binary
serve binary format to qlikview maybe
just d3 dashboards or just services as
well you can do that is it like this but
not only that not only the tools are
very important so you had there like a
very messed up you know slide is just
unreadable right now is so many things
of course they're dialed of that are
recurring everywhere but still a lot of
stuff to to learn man right or to use
them or to install them to maintain them
right not only that you have a problem
because all those guys all deuce web and
jr data engineer herbs guy scientific
are talking a different language some
are talking python background chef the
orders java scana maybe I don't know
Ruby whatever actually all those guys
can talk a different language so you
want you may want to try to choose one
language but then you have to choose
correctly otherwise you might upset some
people right but still I mean I
recommend to choose one single language
otherwise you will get lost in
translation again so you will have to ik
i'm going to write my mold on python
because you know i have do tools i can
use to get learn sample it yeah they
already done then I can you know I can
create some my model then we can run it
in the system okay do that and then the
data engineer should take the model
understand it port it into Java Scylla
and
and running the prediction so it's
 error-prone right you get you
don't want to do that you you want to
try to stick with one single private
language for the main and the important
phases are your of your pipeline so this
is all about I mean and I forgot some
languages of course on purpose to not
screw again more the the slide and I
mean we already dead by the way so right
productivity some may be a recap so now
with that in mind and the ends we can
understand and we have a longer
production line right it's going to be
quite tough to be very agile lean and
target the market very very soon we have
more constraints not only the resources
on the machines but also the time that
we have to spend in may be communicating
between the all of us more people more
with different skills very important to
not overlook that otherwise we are going
to be kicked so no sooner it's very
tough to deal with that kind of thing I
mean for manager I mean is this very
tough but you know it's like that unless
you want to solve it beforehand so you
don't want to overlook it so you want to
solve it both before end so add in some
sense what do we have to do what where
we do we have to be productive I mean we
have to come up with fast results but
still very accurate otherwise our you
know a lot in system is going to be very
interesting to use is going to be
completely skewed and then nobody will
use it so you need to be very reactive
and how you will deliver your your
models and your and your results right
so you need to be very picky at that and
the responsibility to external events so
that means if there is a new stuff that
arrived in in the world so you want to
be able to vary take it into account
quickly so you don't want to have a Mac
let's get back to create a cluster
and then you know check new sources of
information and we're gonna take again
two weeks or three weeks to to do that
and then finally we screwed because the
the client is gone so we have to deal
with that one we am team fight and the
team finally seemed like this by members
of the team right so it's like super
arrow that fight for the good but they
are all fighting on the cell all
together and to each other so they are
fighting the good way versus the good
the bad words to the bed they don't even
know what they do right and this is how
its seen by by the members because they
are just fighting for languages forum
info resources and for something like
this and this is just money which is
spanned very sadly for nothing so you
have to fight like this and this is
simple managers looks like two gentlemen
cattle I fighting from far you know and
why is still not predictive because I
mean there is nothing happening out
there and it's very very very sad I mean
because nothing happens at that time the
guy on top the employers it just see
puppy fights I mean you pussies fight I
mean what what the hell is going on
there I mean this is just fight or
playing I cannot even understand what if
they are just playing or or just having
fun or are they are very upset about
what's happening right now of course
slide deck has to have cattle of course
in inner in there otherwise it doesn't
work in the box actually right and
finally the customers you know those
guys I mean what the heck I mean I don't
have anything it's been a month man I'm
losing money right now I don't want to
have this one slice of fish come on so
this is something that we don't have to
overlook because this is just spending
time from nothing this is something that
we have to do to take care before ends
so that's why I did I fail as we think a
bit differently we want to be
interactive and reactivity interactivity
and reactivity has been our first target
so we want to tighten and to shorten the
frontiers and the time spent between
teams between members of the team right
so that's why we have this panel that
talked initially and which is able which
allows people to deal with their system
interactively directly so you just have
a web page with cells you at you you you
type some code you hit enter it you run
the process on on the full cluster of
200 notes if you like actually we ramp
until the seven seven hundred easily so
you have this entire eyelid attractivity
and then around that we build share and
this is what a concrete example of a
distributed eps the data science which
can solve a part of the program because
h actually an integrated reactivity not
only and at the machine learning phase
but also within the different steps
required into this phase faces so what
we do so we go from you know analyzes in
notebook so you know okay I'm stardom
analysis I can run my mobile I can use
my data then I want to have my project
generated in a system so I we have a
prodigy narrator that goes from a
notebook to the system then we have
something that can generate the services
directly because we have the information
it's going to be binary format because
I'm a binary freak and a type freak and
we put that into into distribution
system we have all the times we have a
binary format so we can have rendering
they're not before just there right
because we have the services we can hit
them and we can render whatever we like
and tableau again qlikview whatsoever
this all information we want to index
them into a discovery system that can be
accessed from the notebook again where
you can search for data right and
everything is generated I mean stops
right so we cannot do the work for you
guys I mean we at least we generate all
the border plates for you but wait let's
take a concrete example if you like on
Thursday with some buddies from datastax
mesosphere on typesafe we had this
hands on arm on going from big data to
micro services and for that we use many
tools in three hours Adam ikea Cassandra
dicas DCOs sorry Michael this OS spark
and the spine notebook in three hours we
were able to tighten all these tools to
streamline them into a production line
that went from genomics data because we
do a lot of genomics and data for us
genomics data towards a micro services
deployed as well acha on the JVM right
and if you want to take a look at that
there is a a docker image that you can
just pull and take a look at it there is
also a github that we will publish a URL
soon where there is a plenty of
information okay the example is so we
take some data on the file system this
is a doctor instance so it's very
limited so we take some data from the
file system we process it with park we
create a data frame a data frame as a
schema he has a notion of the data
structure okay and then we store it as a
park a locally or an urge the fs if we
like right this is the first step so we
take the road data the very crappy data
from the the sequencer okay we process
it variants boom into something
structure that we store in the file
system then we want to take this data
this is structured prepared right and we
put it into Cassandra to do that we
create a schema and then we read the
data in parking which is highly
optimized for distributed systems and
then we process it slightly compute some
statistics and we dumped it into
Cassandra like this again from the
notebook then you want to have a service
so you can create some services like
Union did with akka against TTP so you
have the data you can create maybe 22 2
end points that will render sadly with
the JSON by the way you can render in
JSON so we have this you know mechanism
that will take the data from Cassandra
shape it create a model or to it and
then render it
to JSON and produce a nun point on
listening on the 11 11 so this is the
service part that is serving the
computation on the genomics information
then from the notebook you can connect
to it and as you can see there is a
problem because in my rest client I have
to define myself the case class the
class that defines the structure which
is very bad because this information
should be distributed as well with the
data I mean the schema should be given
to the user right so air because I know
the structure I can do it otherwise it's
just a JSON right I don't have much
information era know it so i can define
it i can define the reader and so on so
first and then i can define my my client
right so I've my service my client my
processing jobs and so on so forth but
what do we need now so okay we reach a
point where we have a complete pipeline
but what can you do actually we need to
deploy so the just web page right now
just process defined this code right so
we need to deploy these things then we
need to connect the dots we had two
different notebooks that just one just
generated the preparation version of the
Dana the order was dumping in Cassandra
right so we need to connect these dots
for instance when to try the
dependencies as well ok so this data is
actually generated by this notebook and
so on so forth and this service is
actually did serving the data from the
other notebook so this tracking system
is to be has to happen and then we need
to scale it to scare both the jobs and
the services this this is very important
otherwise you cannot do the job normally
should be good enough to to to scale but
still you need to think in like this and
the services has to scale as well so you
need to have a cluster may be dedicated
to that that can connect the different
things so what we do we go from the
notebooks of course and we generate an
ass BTW project as BTW / melvin /
lending again / whatever it's just a
bill to write that we produce a jar
which is important part then around that
we create a docker image and the
marathon so unos meses for instance air
okay if you so meses adjust the cluster
manager right you just note right and
manage it for you roughly and then you
have a marathon it's a API on it's a
tool that it can be can access the
cluster and take care of the job so it
deploys it on node and keep it alive if
it fails it deploys it that just
basically that right so we deployed at
marathon that means from the notebook
system boom we deployed the job directly
so you don't have to ask you know ops
guy to know a man where is the marathon
what is the structure of the JSON file
and I have to structure in order to be
to have my job there I could do i
created a docker the the analyst doesn't
have to have the data engineer ed how do
i create an ass BTW project on a maven
project that you don't have to do that
because the stubs are all generated for
you how does it look like I mean you
have just some cells or a headache we're
right so this is just up
there you go
so afterwards we have the problem that
we need to create the services and again
you know you aren't in front of your
notebook you need to gender in you just
generated some data in Cassandra and now
you have to create a service but it's
not your real job you don't really care
how to create this kind of job you're
you just create the model right so what
we do for you is just we create the stub
for you again but because we know
actually that used you dumped your data
and Cassandra or you read it for my gfs
Parker we we can connect the dots all
self so that means that we generate an
SBT project with the driver to Cassandra
or that loads the machine learning model
or as already d different structure from
the pocket file already loaded in it so
you can just read the data without
having to do anything you just used
plain plain connection and objects so
it's very very easy and by the way we
generate Avro onto this different schema
so we take the machine model machine
learning model the different Cassandra
model and we generate an Avro layer on
that so everything is typed in binary
next I'm going to hit them quickly from
the notebook we also you know we
generate Avro with general services so
we generate also the different services
as I said that can go into marathon so
we can just deploy them on merit and
then scale it up and down since we have
Avro we know the types that means that
we can rather easily you know create the
connectors that can read the information
from tableau and render it easily and
beautifully or click view or d3
ourselves right so so we have all this
information available we have so a
notebook I can show it afterwards you
have a bit of time a notebook that has
marked down text code data sources
linked into them outputs saying services
our schema so everything is around the
notebook that means that we have of
information out there
right we have a lot of information
available out there right so I mean we
should reuse that and we really should
reuse that so what we can do is maybe
put that into elasticsearch Winston's
take the whole bunch information and try
to index everything with a bit of smart
processes try to link things are
together okay this sources has been used
with this model and this model has been
stacked or assembled with this Auto
model and we had this kind of accuracy
and then we dumped it into this
Cassandra table this way and we had the
schema so there is a lot of thing that
we can do we can in fair we can run
inductive process on that so at me then
we can put everything in there and
discover information so in concrete
concretely what we can do what we can do
with the kind of distributed data
science pipeline you know you're in
front of your new project data science
oriented right the customer has you to
maybe you know do some variant analyzes
so in genetics you have genes very long
sequence of SC TG ok very very long I
mean right along say million of entries
for one row of course so in this
sequences in a population there is a lot
of information which is shared so in the
population there is a few position in
the genes which are varying right that
means that this is the information that
you want to take care about when you are
doing an analysis to discover some drags
impact whatsoever so you take this
information right and you want to take
to do some analysis on that however
where are they where are those variants
where are these information where are
the files oh I don't know so you want to
have the system ok we are they this
guy's say oka there is a service for
that I mean there is mentum plenty of
services that serve this kind of the
information right so discovered an and
you say okay this is the end point a new
one news
so you load that in the notebook and
since it's alvaro schema you get the
schema you know how to use it it's like
soap who used so bear really I mean
don't be shy i mean i know so right it's
not interesting but still okay so web
services with that whatsoever so we have
the information what is an operation
what is a method what is in a request
response i am a error whatsoever what is
the protocol to internal debate the data
and actually we have the same
information with avro so we get the
schema we can generate we have the the
API to access it so we just have to sit
tab in the notebooks in order to
discover what kind of method we can use
and what are the different models we can
build in order to create recast on the
on the system and what kind of response
we have so everything is streamlined you
don't have to do much okay so you had
this data as you process the data you
say maybe it's not exactly what I want
so maybe I want to go further maybe I
want to check my data because finally
ma'am I'm dealing with life there I mean
this is varun methods I want when we
insert some many scenes to some people I
don't want to to mess this around right
so let me let's check first from where
this data is coming from right so air I
can just check the notebook I have the
code I have the metadata I have the text
have everything I can read this it's not
just called it's just a very documented
code that has been a written SBT jar
whatever they're behind but still you
have this information you can read it
you can fit ok finally it looks not that
bad so but where is this information
damn where is the information accessed
by the avro service is in Cassandra ok
let's check in Cassandra so I can load
the cursor on tables directly notebooks
and check what's going on out there ok
so you can play a bit with the tables
you can see what's in there if they have
the exact accuracy if the elimination
process of the features and so on hasn't
ditch too much information so you can
read this information back from
Cassandra
and maybe many at that time only you say
ah damn it's not exactly what I wanted
the process looked good but actually it
doesn't take intercon all the sources
unwanted or maybe ditched too many
variants because it didn't took the full
population let me took a portion of the
population so I don't want to just use
this information I want to go beyond
that and then you just copy the
notebooks and you turn it you turn it
you just have to tune it right it makes
another notebook link with the Builder
the previous one but still it's a new
notebook it's a new job it's a new
generation process and then you can run
the job on the processor on the grocer
you have two new data set in Cassandra
and then finally you can start your your
your data process on the first problem
did you have it looks like yak shaving
the thing that we do every day right but
at least you're you're the system can
drive you and this is like a cool trophy
needs so this is all I get right now but
because I may do a bit of show of the
notebook if I'm asked for but actually
does several things that I want to
mention so first may you can follow us
for sure on Twitter so there is a handle
data fellows but also Xavier and myself
newts a bar on Twitter the spy notebook
has its own Twitter account we wrote a
blog at typesafe about this kind of
philosophy and architecture and by the
way in March 16 will give a online
training O'Reilly proposed that to host
it so at all Riley will give an online
training on shining all these tools
together from Messrs to Cassandra you
know to do some data science pipeline
right yourself using the right tools so
if you want to join us there is a form
actually write this like this actually
just a form to get notified about what
what's going on on there when it will
happen is going to be the three and
forth march three to five march by the
way
is not really committing of course it's
just asking for some information about
the course if you are interested in but
we are very excited about this online
training with o'reilly i'll create a
detour by the way there is this
assistant yeah this getup repository for
a spy notebook if you want to check out
since park is very shitty at version
there is a website that can generate the
right package for you so you don't have
to yourself clone the github repository
and then make distribution passing all
the fancy parameters it requires to have
your right versions in there it's just a
hug lee form by the web at very helpful
because i mean i have quite a lot of
people that generated the home version
as you can see is very crappy I
distribution so I had to do that
otherwise I would have to create them
myself would take ages and there is a
good website that I just started just a
few months ago by the way no etl not org
who relates ye TL is not relevant
anymore I recommend you to go there to
check it out there are a lot of
information interesting I mean there are
a lot of people that are writing in
there so people from come free on from
data breaks and from Nitro and so and so
forth and we we have been proposed to
write in there as well so we will
probably soon or write our ideas about
that very soon there is sadly I didn't
put the new girl there but there is also
a data science manifesto didn't you
might want to check has been initiated
by a body in in London that works back
lay that tries to to to dictate or to
propose ideas or or thought about how to
be more agile and the way we are dealing
with data and even in the discipline
matter so it's just a manifesto like
like the others
so yeah so in minutes it's fine so I'm
done and I'm open for any questions if
they are about anything even the mo if
you like Oh beggars not now maybe I
nobody cool so guess I'm done</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>