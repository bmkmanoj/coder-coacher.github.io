<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Flying services with the drone by Krzysztof Kudrynski and Blazej Kubiak | Coder Coacher - Coaching Coders</title><meta content="Flying services with the drone by Krzysztof Kudrynski and Blazej Kubiak - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Flying services with the drone by Krzysztof Kudrynski and Blazej Kubiak</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aaZMh4IqMcM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is was a Kubiak
my name is shift of kudrinsky and the
difficult part of this presentation is
totally over we will talk about our own
technologies which we implemented on the
drone and this presentation if during
this presentation you have any questions
please do not hesitate to continue
listening and wait until the end of this
presentation what is the best time to
answer questions by the way we are here
until the end of this conference so
there's plenty of time to assess
question afterwards okay a few words
about us we are from TomTom in Poland
where we deal with algorithmic design
ranging from signal processing to
artificial intelligence and the very
interesting project that we are recently
involved in is a self-driving car so we
are using our competence to in image
processing video analysis as well as
laser point cloud analysis GPS I&amp;amp;S
positioning to create a
three-dimensional centimeter level
accuracy a line level map for a
self-driving car available at global
scale now this map is paired with the
model of surroundings which we use for
localization and we also create some
algorithms for automatic detection of
various Road features to make it quick
the production but the most interesting
part is the localization this is a ROM
DNA a set of two deaf maps to both sides
of the road this is our invention which
lets the car ideally position itself on
the map so when the car goes it can
compare what it sees with what is in the
map and we have the precise position
which
GPS is not enough so on a daily basis we
are working on the car to drive itself
safely and comfortably but the problem
with the cars is that they don't fly and
we think that flying is awesome
everybody wants to get high so today we
will talk about flying and that Center
for this presentation is very simple
first we will talk shortly about a short
project and then we will talk a bit more
about the long project it all started
with competition in our company where
the aim was to follow a running person
with a flying drone and film it the
finals took place in Amsterdam which is
a city so famous for the fact that you
can absolutely legally visit windmills
there is no sound I think there should
be sound so by the way the vision of
flying was growing strong in us we
expected this to be a story of pure
success the story of conquering the
skies the story of flying into the
unknown and reaching the Stars and we
were almost right we were almost right
this was a story about fighting the
story about constant irritation the
story about struggling about constant
falling and rising from your knees the
story about determination and
desperation we are professionals and as
professionals you all know you need to
have a certain plan of your actions you
start by the drone open the box
learn to fly manually gather some
characteristics and then continue with
incremental develop
test develop you'll know that but at the
same time we are enthusiasts and as
enthusiastic started with a final design
and then we bought all the electronics
that came into her head and then we
bought the drone we quickly rushed with
the development and it was going really
great after celebrating the first
successful versions of our code we
finally decided to open the box the
drone was inside so we put all the
electronics there and we gloriously went
for our first test in the field which
turned out to be the last test of our
first drone with the next drone we
decided to learn to fly manually and
then we continued with almost
professional approach so our system was
based on the parrot ar.drone with which
you can communicate easily using Wi-Fi
and there's the plenty of open source in
the web which you can download easily
what we used is a drone a framework
which you can download for free well
under apache license so you can use it
and it promised easy control over the
drone which turned out to be true
there's like one function which in fact
we used it is a move function where you
just set the angles for the drone and
the speed and some listeners to listen
to the drone Stane
it's in Java it is 100% and it is pure
it also supports Android this time maybe
not 100% and not pure and without the
video but we managed somehow to do it
what I will later tell you a little bit
about that but anyway the system
consisted of a drone which was supposed
to
follow and film the runner and the
runner with the mobile phone in his
pocket which from the point of the view
of the system was just a mobile phone
the task was for the drone to follow the
phone so we needed to find the
localization and both of these devices
had GPS so the first rough estimation of
position was done by GPS and then a
better distance was found using beautify
beacon and finally a better angle of
view was found using image processing
tracking of the person in the frame so
with this we had a quite precise
localization between the two and from
the application in the phone we could
send the commands to the drone to turn
and to accelerate so that we will find
ourselves in a nice fish suitable prey
place to make the video of the running
person so the application in fact is
just that the system is just the mobile
phone application which listens to the
drone State over Wi-Fi compares it state
with own state of the telephone and
sends back some commands to the drone to
follow it another played part of this
application is the Safety Officer
application which is another mobile
phone external person which is able to
kidnap the drone in case it wants to
kill the runner we learn a lot about
security here this is a question of life
and death so we are enthusiasts of
graphical user interface it needs to be
beautiful and simple there was no time
for both so we choose just beautiful
there are some buttons for manual
control and the button to start the
autonomous mode if for those of you who
are a bit disappointed by this design
there's fortunately Safety Officer
application which looks much better
beautiful or not our system worked well
our enthusiasm and also lack of skills
in dealing with this flying piece of
hardware costed us lives of three drones
one injury and countless losses in
equipment but with the last remaining
drone we managed to follow the runner on
its entire track which was a reasonable
achievement however we don't like the
reasonable achievements we want
extraordinary achievements so with the
junk of the three drones we created the
drone of the last chance the new hope to
fulfill our sick ambitions
these ambitions and the agenda for the
following presentation is to create a
technology to make them up from the
drone bottom camera localize the drone
in this map and then use this map for
high level orders high level orders for
the drone in context of very nice
applications like please bring me my
glass of water thank you
we approached all of these issues and we
will present that with some details in
our algorithmic algorithms but focusing
mostly on the show so during the first
few weeks we managed to get undistorted
video from the drone photon camera and
we put some books on the floor in our
office and were flying the drone over
these books to get some frames and now
these frames were the input to our
simple java application which you can
see here so what you can do you can
browse through all of the frames if you
find the frame useful you add it to the
map and you continue browsing as you can
see using blending you can see the map
below you can rotate the zoom in zoom
out shift so that you can manually find
the best fit and continue browsing so
with that we created a beautiful map
well not very beautiful so we were
working on making it more smooth so
we're working on blending the frames
into the map so that it will look better
and better how we achieved that well
this is the diagram of the drone flying
over some high objects and the dots show
the places in which the frame was taken
and each picture captures a limited
angle view of what it sees below and it
is not aware of the height of the
objects below in fact it is inconsistent
with any other frame which is taken by
from other angle the only place which is
consistent with the top view is in fact
the
place that the drone looks just down
which is the center usually if we drive
smooth flight smoothly it is a center
pixel of our image so what we did we
create a mask which is one in the middle
and it goes smoothly towards the edges
so now when we want to combine two
frames when we want to add a frame to
the map
we simply perform weighted averaging
we've weight taken from the mask that's
why that's how we create a blended frame
and then we accumulate the mask and
whenever we want another frame we want
to add another frame we can do it freely
with the accumulated mask and continue
with this process so this way we were
able to create a nice and smooth map map
manually but it was taking a lot of time
so we wanted to automate that and in
order to do that we created a brute
force algorithm first so we were when we
had some frame already we would be
looking for the next frame in some
reasonable neighborhood but for all the
possible sheets all the possible zooms
and rotations so you can calculate that
if we assume that during one second a
drone can move three meters it can
rotate by 90 degrees and change altitude
by one meter this would mean that for a
pixel resolution of our map we would
need hundreds of thousands of operations
per one frame which would be for working
in real time millions of operations per
second which for working in real time is
exactly too many of course you can go
down with this number quite easily with
simple tricks but if you really want the
real time operation you need to choose
wisely
these places where you want to correlate
and even then you need to correlate fast
how to be wise I will tell you later
now watch I will tell you how to be fast
hello everyone so ASCII stuff that I am
going to tell you how to be fast so my
goal was to develop fast method to
localize frame capture by drone camera
in the map that has been created
continuously while drawing is flying so
we need to consider many possible
locations when which we treat could be
correct for particular frame in the map
and we need to do it very fast so to
achieve real-time performance of around
2000 consider location per second and
the confidence that we assigned to each
situation must be smooth which means
that when the frame is going away from
the correct position it should decrease
smoothly and it should increase when we
come back that's it so we started with
very simple solution by some template
matching this is basically some ink
product of pixel values in overlapping
area between frame and map so it's quite
simple but unfortunately it's not fast
so we achieved only hundred location per
second so we did another try we want it
to be smart this time so we would like
to avoid processing whole frames for all
frame pixels and we process only edges
creating so use edge base distance this
kind of distance the idea behind this is
very simple we need to detect edges in
the map that again Jason frame and check
how close they are so we just iterating
over pixels edge pixels in frame and and
some ink distance to the closest edge in
the map so only question here is how to
calculate distance between particular
pixel and the closest edge efficiently
and fortunately answer is also simple
we need to recalculate all possible
the stances for all edge pixels creating
at a distance map so in that map at each
pixel we know what is the distance to
the closest edge in the in the map so
when we put the frame over them up we
can just iterate over pixels of each
pixels and frame just reading
pre-calculated values that are lying
behind in the mouse so it's it's simple
and it looks promising for our goal so
this map is created in linear time and
it is created only once and reused for
all possible considered locations so it
should be fast so let's see if it is
indeed so we perform around 3000
transformation per second and it was
quite enough for our our application and
obviously it was much faster than
template matching so now we know how to
localize fast but fortunately we had run
framework we didn't have video so we had
to add it we wanted to have the video
over in Android application also in
desktop application so we use the fan
back library on both both platforms for
Android there is nice tutorial how to
compile ffmpeg and how to use it on for
Windows we found pre-built binaries so
it was connected to video stream over
network in the drum and we just render
frame and also used capture frames for
our video processing but unfortunately
we only capture frames we saw that there
are distortions and these distortions
could spoil our creation process so I
wanted to remove this to remove these
persons we need to use some
correction equations some of them are
for ballad distortions some from this
tanker shell distortions and to use them
we need to find a few parameters and
finding this parameter is called camera
calibration after calculating camera we
also get focal length of the camera
which is used to have metric resolution
of the map which will be used later so
that's it I want show more details let's
calibrate our camera so we take a few
pictures of calibration chessboard we
find corners using some nice open CV
function and we try to make straight
things straight so straightness is
objective function for calibrating
camera
so after that let's see our example
frame and add another frame in
overlapping part fits much better and
with another example we see the same
okay so now Christophe will tell you how
to make wise choices so we have limited
the time for each coloration now it's
time to limit the number of correlations
and I told you that we in the
brute-force algorithm we were
considering all the possible shifts of
the from the one frame to the other
another which we don't need to do
because we know from the drone sensors
something about the drone movement so
instead of using all of the possible
places let's use the sensors to say more
or less when the drone might be and only
for these locations apply the quick
correlation however simple it might
sound there is a huge theory behind that
and it's called probabilistic robotics
and it needs over 600 pages for brief
introduction and I'm not going to tell
you this brief introduction now instead
I'm going to tell you a story
from one point this will be a story
about the beautiful piece of machinery
which we used it's called particle
filtering on the other hand this might
be a story which could happen to you to
me to anyone else and for the reasons
that would later become clear we will
call it the hangover so let's imagine
this day when you wake up and you don't
know where you are and you don't know
what happened yesterday and you are
trying to think where you might be now
you are yet too afraid to open your eyes
but in your imagination you are trying
to spot all the places that you might
probably be in and your first belief
about your state is not satisfactory for
you so you finally find the courage to
open your eyes and make your first
observation it's the beach so probably
it must have been nice yesterday but
with this observation you are already
able to update your belief about your
state because you are not drifting in
the ocean you must be somewhere at the
coast or maybe on the near the lake site
or maybe a big river so this is some
probability now only in this dot where
such observation is possible other
places the probability is 0 so you
decide to start to walk and you are
working for I don't know one hour two
hours and what you still see is the
beach which means that you are no longer
possible to be on the lake side or in
the small river you must be on the coast
so you update your belief and then you
start to work more and more and you are
able to make another observation you see
two soldiers guarding your passage and
you immediately know you are on the
border so the place is not on the border
have the probability zero now while on
the border the probability is very high
but the guys are very unfriendly they
have guns so you turn around
and you walk with the C on your right
hand side and you walk for some time and
you see a tourist and finally you are
able to make your final observation you
look at him closely and you know
immediately that you are in Poland so
what was happening here
we had some state we had some position
velocity we didn't know exactly where we
are in fact we didn't know anything
about what what we had and we wanted to
know we wanted to know where we are
what we had was some previous belief
about our state maybe it was totally
random we didn't know anything maybe it
was already crystallized and we
presented that as a collection of dots
spots where we might probably in so a
lot of hypotheses that when were in our
hand what we also had was our control we
were moving we were turning around maybe
we were counting our steps we were
thinking how long we are going and this
measurement we were applying to all of
the hypotheses all of the particles that
we might be in creating the new belief
about our state created by our movement
and then for each particle in this
belief we were comparing how probable
this might be in comparison to what we
see to what we measure so we had a
control to move our previous belief to
the current state and then the
measurement to check the probability in
each of the spots and these
probabilities which were high because of
the measurement were stayed in our
belief and others were turned to very
small probability and they were no
longer present in the next so this is a
very simple explanation of what a
particle filtering is in fact we could
talk about this for hours but I will
just tell you a few nice properties of
that in other approaches for that
usually you are constrained
with some limited distribution for
example Gaussian which only gives you
one hypothesis while in particle
filtering thanks to the particles you
can model whatever distribution of
probability that you want there are some
problems with that but this is very
flexible in the case you might consider
different hypotheses also it is nicely
scalable if you are on the mobile phone
you can use only a few particles of
course you will longer achieve the nice
precise solution but if you have a big
processor may be multi-threading or
whatever multiprocessor machine then you
can use a lot of particles and you will
achieve the solution faster and there is
a nice recovery mechanism so if by any
means your sensor was wrong and you lost
the place where you are and only other
places left there is always possibility
to create some random particles all
around and finally you will find you
will hit this place where you are and it
will generate particles there so that
you you know you are there so this is a
very nice mechanism and implementing
that is not a challenge it's quite easy
in fact what is a challenge is the
design of the state and modeling of
noise our model our state for the drone
was its position its velocity its
rotation and zone obviously the control
was the accelerometers in the drone and
the change in rotation and height read
read from sensors and finally the
measurement was the frame and it's
correlation with the map so although now
after this presentation most of that
might seem obvious but in fact the
control is not so obvious acceleration
well you can just read it from the
sensors easy yes no I will tell you I
the drone is controlled by angle so when
you move the drone backwards when you
bend it backwards
we'll start to gain acceleration forward
and it will start moving in the forward
direction however if you compare the
acceleration that you get from the
sensors with the ground truth that you
measure somehow externally like making
photos and making I don't know what the
ruler you will see that it has no sense
absolutely this is totally different and
it obviously has no sense because what
we want to measure is the acceleration
in X and y direction while what we
measure using accelerometers is the
acceleration in the drone reference
system but now come on you have this
angle sorry
this is pitch and roll you have this in
the sensor so it should be just a
question of simple trigonometry it
should be so easy but the gravity is
there and it is huge and even the
smallest error in the pitch control
sensor sorry I don't know wise even the
smallest error in the pitch and wrong
sensor would make the gravity leak into
the horizontal axis and the results
would be terrible and in fact it turns
out that the pitch and roll sensor in
this drone are very cheap and they
suffer from significant error so in
order to correct that we were flying
hovering in one state for some seconds
since we're hovering in one state the
pitch and roll should be zero mean it
was not zero mean so this is how we find
the drift we could calculate it subtract
it and without the drift it looked much
better so we could use this de drifted
pitch and roll
to finally correct our acceleration the
problem is that in this drone the
acceleration sensor is very cheap and it
suffers from significant error
so what we had to do we had to use the
leveled acceleration in the buffer to
calculate the drift of the accelerations
subtracted to get the rear leveled
acceleration and then as you can see it
looked much better of course there is
still some statistical error here it
looks much better as a shape but
statistically they're still quite
significant error you can see it's up to
half a meter per per second which is not
so little also there is a lag between
what you see in the camera and what you
get from the sensors and lags are
killers
for particle filtering it is even better
to say we don't know the acceleration
than to say we have the acceleration
measured and underestimate this
uncertainty so if you are in a such
situation that you are afraid that you
will underestimate something exaggerated
that's what we did and it worked another
discussion would be needed how we dealt
with orientation and going from
ultrasound sensor to scale and how we
dealt with the relation of noise when
going from acceleration to to position
but there's a lot of literature about
that you can read that this is not so
interesting there is not enough time to
toll to talk a lot about our system
architecture for flying the drone
visualization getting the logs and so on
I think some of you might be a little
bit disappointed some of you might not
even believe this but we didn't use
microservices
what we used was plain Java a few
threads and the state machine and it was
working really nice what's very
interesting about that is that behind
the control interfaces we have the real
drone implementation to go into the
field fly gather data try how it works
in the field but under the same
interfaces we have a simulator
implementation which uses the
pre-recorded videos and sensor values to
replay the flight of line this is very
nice if anyone of you dealt with the
bugging of applications working on
hardware and in real time this is a
nightmare and such application made it
possible or reproducible and fast ok you
know our tools our algorithms now you
know our inspiration let's show how it
works in action so this is a video of
our drone in action in the bottom right
you can see the keyboard layout for
controlling the drone in the bottom up
you can see the external camera which is
filming the event this is just for
presentation purposes but synchronized
and in the middle right you can see what
the drone sees this is will be dead
frames which will be used to create them
up which will appear on the left it's
black because we are in calibration
state when it appears it will be a bit
stretched because we don't use aspect
ratio here it's just whatever okay you
can see so we can see now the red dots
are the particle particles so our
position hypothesis we are flying and we
are revealing the terrain the fog of war
while we go into the enemy territory in
our office as you can see while we are
flying over the same places again they
changed because of the blending in the
map whenever will find a better
perspective on that
- blending we are updating it now in the
upper right corner you see the this
camera filming the event but it will
switch the front camera of the drone in
a second just for fun and you see our
friends working in the office after 10
p.m.
we got spotted behind behind the wall
they were here illegally after 10 p.m.
we've had a permission to stay in the
office for the whole night
romantic night me was a and the drone
okay we continue our mission and now we
will this will be very interesting we
will fly over this desk and you can see
the update because now we are more over
it which means we have a better top view
perspective than before so thanks to
this blending it gets updated so this is
basically how it works
it was our main aim we are very happy
about this it was our free time project
so we were yeah this works finally but
this was not the end we had them up we
know where we are on the map so let's
tell them up where we want to go and
stop controlling the drone let the map
control the drone it knows where the
drone is it knows where it's going it
knows where we want to go because we
specified the target so let the map
calculate the controls of the drone so
that we go closer to our target we did
this and the test for that okay we
didn't know exactly our position yes we
had our particle filters so we could
apply it to every particle and calculate
the resultant this is what we did and in
the in this mission the drone will wake
up not knowing where it is it will
localize itself using particle filtering
because it has them up and it sees what
it says and then it will go to target
one the green one it will pick up the
target go to target oh and land with the
this is a feasibility so only thing that
is not working here automatically is the
pickup mission this was done manually so
when we go to target to a person we'll
take the over the control and we'll pick
up the cargo and then again automation
so this is the next video you will see
some lag between the camera because we
use the different camera here but
nevertheless it will be nice so we are
calibrating in in a second you will see
the particle filtering it will gradually
quite quickly but gradually go into the
place when it finally has enough
confidence the map will take over the
control over the drone and it will fly
yes it is starting to fly towards start
with a bit of lag but it is flying
towards target 1 we are almost there
yes and now the manual take over by the
way the technology for the pickup is the
magnet hanging on a sticky tape and
sometimes the magnet goes into pendulum
motion and it obstructs the view of the
height sensor of the ultrasound sensor
which means that the drone sometimes
misjudges how high it is you will see it
later how the drone works with that it
will be very interesting but now let's
focus on the pickup mission which you
have to believe me need a lot of
patience ok
yes I think we're ready that's the
pickup now that's the misjudgment
but you are flying towards target - we
are landing we are landing we
conclusions we broke the glass
we broke the drone and we displace the
ceiling but we managed to localize
ourselves we went to target one we
picked up the cargo we went to target -
and maybe not in the right place and not
too smoothly but we delivered the cargo
to the ground so however simple or
however hard this might sound to you but
reading the title of the presentation
you might probably expect it what what
was going on here what we are going to
show you now it's totally unexpected
because what we'll show you now is the
toilet idea if you don't know the toilet
idea is when you go to the toilet and
you have an idea
our toilet idea was about blending
however that sounds so I told you before
that we use a center perspective for
every picture apply it to every frame
and we have a top view map because of
that that's a very nice approach it made
them up nice and smooth however what if
we apply a different perspective apply
to every frame and create a layer from
that map and then save it and create
such layer for every possible
perspective that we think of what we
would have we would have the map from
different perspectives and we would be
able to play with that in the simplest
case if we go infinitely far away
it means we use only the center
perspective because we are infinitely
far away so we have the top view so this
is exactly the thing we were talking
about up to now we will create a top
Douma but if we zoom in we are starting
to use all of the different perspectives
at that point not only point of view but
the top view but also the side views we
zoom in and we create a virtual view
blending all the possible perspectives
so like the right side will be blended
from frames which saw this place on the
right side the left side will be taken
from the left and finally we will create
a virtual view in this place notice that
the drone was never here this is purely
virtual view we implemented that the
there was a small problem because to
make it work it would need precise
localization particle filtering which we
designed we need a lot of development to
make it so precise a lot of trials we
didn't have too much time for that also
it was very sensitive to pitch and roll
but we at Lee
wanted to prove the feasibility of our
approach so we moved our toilet idea to
the kitchen and in the kitchen
we prepared professional laboratory
setup to make manual photos of the floor
which consisted of the ribbon over the
scene this ribbon was supposed to give
us reference height and pitch and roll
to take manual photos so this will be a
manual project manual photos of what we
see below this ribbon is hanging over
the scene attached at one side to the
kettle and on the other side to the
microwave oven supported by milk with
such laboratory setup we prepared 11
layers with the center layer and extreme
layers you can see on the slide now with
the standard map while zooming in you
will never know that below this chair
there is a Spider Man magazine which you
can clearly see while zooming in with
our freedom app browser giving you the
perspectives about which the drone has
never even dreamed zooming in browsing
the map is available in 3d in our
browser this is not very smooth yet but
this is something that we are quite
happy because there are a lot of 3d
reconstruction algorithms available in
real in literature there is structure
for motion there is image segmentation
the recreation modeling these are a lot
of sophisticated methods in our approach
we use no image processing at all just
simple image
blending at our knowledge no one has
ever done this before
that's why we are happy so to conclude
that during this session we are talking
how we did some implementations of our
own technologies on the drone to make it
possible for autonomous follow-up
mapping localization and mapping troll
all available in 3d for selected regions
now I hope this is this will be very
useful for you especially for those of
you over and enthusiastic about the
hardware project yeah we will do it let
it be the discouragement but for those
of you freaks
let it be motivation if to no-name
people can do this you can do it for us
it was hard work and we are quite tired
but considering these moments when we
felt pure creation it was worth it
thank you very much any questions well
we are here for we have still some some
minutes left so if you want to come down
there is no problem I see nothing so if
there is any question you have to speak
loud or or come down here yeah sorry I
don't know I will have to ask my my son</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>