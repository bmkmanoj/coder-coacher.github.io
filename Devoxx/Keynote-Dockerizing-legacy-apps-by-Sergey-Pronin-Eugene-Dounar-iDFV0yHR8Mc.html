<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Keynote: Dockerizing legacy apps by Sergey Pronin &amp; Eugene Dounar | Coder Coacher - Coaching Coders</title><meta content="Keynote: Dockerizing legacy apps by Sergey Pronin &amp; Eugene Dounar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Keynote: Dockerizing legacy apps by Sergey Pronin &amp; Eugene Dounar</b></h2><h5 class="post__date">2018-04-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iDFV0yHR8Mc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">another one good morning thanks for
coming so early so today with Yujin
we're gonna tell you about the bane of
building Intel platform Emporium
so I know you guys are mostly developers
but it might be interesting for you to
know like what issues would taste when
we build internal platforms of docker
so as an intro so our company's called
Orion it is grant which runs after es
sweet capital is we Campbell it's not a
market the graph this is fun and
basically owns a lot of well-known
brands like or ignite this other course
always one of them as well and it all
adds up into 60 companies and this 60
companies there are different stacks and
it sums up into 100 of different
products and we are quite big we have
like 3,000 in place and one of the
things that we like the most is that we
will work remotely everybody and this is
thanks to crossover it's really great to
find best talent from all over the world
and working pajamas so really guys
welcome so and another thing is we used
thing which is called economy of scales
which means that we try to follow a
single set of processes across the whole
organization and
basically this allows us to have not the
60 engineering teams for 60 companies
that work but to have a single
engineering team that can deliver value
for all these 60 companies and the same
goes on all levels on customer support
our operations we have single teams
small teams and they work for all this
weekend which is extremely great and we
have extremely aggressive acquisition
strategy like this year already we're at
seven different countries and this
companies they add some complexity a lot
we run them on Amazon when we find you
company will move their staff to Amazon
and it's really hard to manage so we
decided okay let's build a platform
let's build a single platform which
would deliver like docker and DP service
for our customers businesses and it
looks like really simple okay let's
build a platform but it's not so
yep so we have the following constraint
one of the constraint is run docker on
the biggest Amazon instance can offer
its x1 that is Excel it's 180 128 horse
and 2 terabytes of RAM so you may say ok
wow you have a lot of resources but now
we need to get a high density which
means that we put containers on this
dollar course till we reach a standard
utilization on this token which is
extreme right Iran 600 containers on a
single dollar course and they clash with
each other so it's really hard to manage
and also another constraint is no higher
ability at all we want to be simple want
to move extremely fast we have my
company's coming every week and we want
to put them into dock very fast if we
have some scheduler high mobility
it was slow as well and we want to be
focused and at the same time we want to
meet all the requirements that are
coming with applications like web
applications that need SMTP services
right so they want to send emails they
need persistent ongoing it works this is
harm also we have companies that run
some networking services they run PGP in
context they require privileged
containers network and network abilities
and we need to meet them all because
this is our goal right and of course two
main goals that we want to have is
reduced Amazon footprint
Amazon costs maintain at the same time
without higher ability without schedule
so
I want to share some stats with you so
this is how we look like now we decrease
this band on Amazon from 30 million to 6
million yearly and now this looks like a
bit differently
it's already this year we acquired some
companies that adds up to this spend and
also run our experiment humanities which
inflated the cost of our central account
our uptime is ninety nine twenty nine we
track uptime in three different
dimensions first one is cost what cost
fails
well did it's not right if you don't
have a change it means you're down
another thing is docker
ATM when API fails your products still
working it would have library store so
daughter is down containers around and
everything is fine but we still practice
an outage for us so it ends up to this
ninety nine twenty nine and we have
contain the issues we have lots of them
so I don't know how many of you use
docker can you raise your hands okay
nice one I don't know guys if you face a
lot of issues with dork like stack
containers containers are going into
totally unresponsive mode we're facing
daily and we have lots of run books and
lots of how to deal with that so it's
also that adds up into our downtime QC
graph on this graph you can see an
average utilization on our hosts where
we run 650 containers so you see we
almost reached our 80% code which is
quite nervous and this cost is pretty
stable I mean it's not failing
we have this fights all over it's good
so today we're going to tell you about
the issues that we face and how we solve
them first I want to mention that on dr.
corn 2017 our teammates gave a
presentation which is called dr. Isaac
Orion I really encourage you to watch it
it's really interesting one they were
talking about bad neighbors basically it
is our internal tool doc enforcer which
enforces our customers to comply to all
internal policies so they don't run the
container if it's non-compliant this is
great - it's open source you can find it
on github and another issue is
networking that they were talking about
is networking on Amazon is simple right
when you go to Amazon app later clicking
your network is up but when this singly
city implements Amazon limits one
question with each other with customers
requirement you are like whoa what shall
I do here and you will face a lot of
issues of networking site with Amazon
especially with our high density
approach so and today we're gonna tell
you about two different problems first
one is high density itself like what
issues would face because this high
density of containers right in this
countries of containers and when I say
run hundreds of containers it's not like
nginx container right
it is casandra's it is Soler's it is
containers with networking all kinds of
stuff we don't know what our customers
are going to put tomorrow on our dollar
for suite like xposed api and another
thing is mean time to recover this is
how we deal with recovery of costs of
docker api of containers it's really
interesting as well and how we
decided to go to queue minutes I'm going
to give a microphone to you Jim he will
tell you more about high density hi
everyone so so I'm going to tell you
about a few dishes we have and hopefully
you can draw some conclusions hopefully
it will be useful for you I will tell
the symptoms and how we investigated it
and what was the fix so our first issue
was appearing exactly on their force
which have a lot of containers which is
more than 300 for us and most of these
containers on this particular force had
docker health checks so this combination
resulted in occasional docker comments
especially docker stop to cut stuff so
products just try to stop the container
and the comment got stuck and the
container was in a dead state you cannot
do anything with it so what should you
do if you fail decision and basically we
went the straightforward way to better
understand what's going on
why some comment is stuck you just need
to look inside docker itself
there is an excellent tool inside docker
if you can you can send signal to the
dr. T process and it will save all this
tech dump all the state of the docker
the so called dirty Instagram then you
basically go through there there can be
out of go routines which are basically
threatened going there can be like tens
of thousands of them but you need to
analyze this and find the go team and
the stack which handles the this
particular API request and figure out
why post-op so in our case in our case
it was simply waiting for the other
continent which is container D container
D is the demon that does all the magic
stuff photographer and it's also written
in Golding is child project of docker
which is now in different organization
but it's suppose the same to set so
basically you do the same thing you take
the stack down and go through these go
routines try to understand which one is
handling the talkers requests and try to
understand why what is it doing you
wobble you obviously needs the docker
source to at least have some idea of
what's going on but I encourage you to
do this it's not that difficult
go link is excellent is it simple to
understand language so in our case we
found out that container D was waiting
for the init process to exit and this
init process was in a pretty weird state
it was it was in so called
uninterruptible sleep interruptible
sleep at this stage as you may know
usually means that the process is
reading from the disk but it can also
mean many other things and the bottom is
the important thing here to mention is
that you can do nothing with the D state
processes you cannot kill them and if it
gets stuck due to some kernel bug the
only thing you can do it is to remove
the host there is no remedy and this was
a large issue for us so what what should
we do
we cannot reproduce the issue it's quite
hard to understand or file into
somewhere so the only way was to try to
debug it in production and the most
simple tool to get some idea of what the
process in linux is doing is to take a
look at the progress and progress can
show you the eternal stack in the kernel
explaining what is this process to it so
in our case we found out this zappiness
process function call in all the steps
of all this stack docker stuck container
processes and we try to look at the code
inside the kernel but it's not the
dopplers so the first thing you actually
should do when you see a kernel issue is
to check for new updates and we checked
and there was some released patch which
which did basically nothing in the apart
from just changing the accounted state
so previously it was in this state and
after the budget was just marked as
interruptible just
it became just a zombie and zombie
processes are much better for us because
zombies can be ripped you can repair in
the process and and basically do the out
recovery so the permanent fix was
actually very simple we pushed the
canonical to make the better to backward
dispatch we updated the kernel it will
be just a few weeks as far as I remember
and although we didn't exactly resolve
our issue but it allowed us to have the
out recovery love any reboots and that's
what's important for us ok let's go to
and the next challenge was slow dollar
startup as we mentioned recovery is very
important we don't have high
availability so we attempt to just
recover things as fast as possible and
when we added more and more containers
to the to power costs docker start was
getting slower and slower
we of course first thing we checked was
general CPU memory usage and it was
pretty pretty fine doctor was not
spending any CPU so once again you need
to take inside take a look inside docker
to understand what was going on but this
time like looking at the stack dams
won't help you because this is a
continuous process you want to
understand why is what is waiting what
is waiting for and there is an excellent
to include it in gold in Golan called P
probe and you can easily enable it for
docker and the probe has this blocking
profiling what people filing basically
gives you an idea what the go things are
blocking to either either UPC skulls or
it's waiting for other goroutines and in
our case we found out that weirdly
enough it was it spent doctor spent
almost all the time waiting for by the
tables binary to completely IP tables is
expected to be rather fast and we tried
to understand what's what was going on
inside the kernel but it was but we had
no clue
sometimes it was fast sometimes it was
relatively slow but should be okay so we
choose we had to choose a hardcore
solution and we actually punched the
component inside docker to make less
epidemics calls maintaining a fork is
pretty hard so I would not recommend
doing this as a first go but actually we
reached the time to recovery of less
than one minute
basically by compiling all the IP tables
calls in the kind of transaction so it's
intercept of the IP tables calls and
compiles them in the IP tables restore
script and then if you push if you run
this IP tables restore by array it would
be pretty fast
still the root cause was not known and
later we stumble upon it anyway so we
were getting we were we not hit that
from as time passes other processes were
getting stuck too and it was pretty
surprising because even simpler things
like LS or load average where sometimes
two seconds to finish and to make things
even more confusing everything they were
still fast inside containers so if you
run load average
it was slow in the namespace but was
fast inside containers from the same
cloth so first thing you do when you
stumble upon some slowness in a run in
the one time run is obviously straight
and straight showed a pretty surprising
thing when the process was when they say
uptime come on was slow it was busy
opening proc load average load average
so progress is just an insight it
doesn't he
professor should be fast it's a kernel
inside it doesn't float it doesn't use
disk doesn't use anything and once again
the only thing to do was just take a
look inside Linux and this time it was a
bit hard so we had to use this
we had to use earth tools
that's an excellent token by Brandon
Gregg there is also tools like BCC which
basically can you can on production
service you can get the events inside
Colonel which functions got triggers and
ran and we got that the processes were
waiting for the lock on the inode of the
director in Prague and this was the easy
part the hard part was to understand who
is actually holding this law and since
the bite since by default a lot of
profiling
is disabled in Linux and it's pretty
it's pretty unacceptable for us to
compile our own camera production we had
to walk through the goal of the parts of
the kernel potentially these mixes and
in a few weeks we found the place and it
was simply our monitoring software
listing the processes in listing the
proc directory so it was just getting
the number of process in the server but
since we have such a high density we had
like enormous number of processes and it
helped this musics making a lot of
system utilities getting stuff so the
fix for this is pretty hard you may
think but once again we just check for
Linux updates and since the version 4.7
Linux produced multiple reader single
writer looking for a notes and basically
our problem was gone we just upgraded
the kernel and it is eliminated
dominated the problem
okay now Sergey will tell you about our
recovery goals
so as Eugen mentioned we have like
enormous number of problems and we're
good at fixing them even rolling out new
patches or fixing things and doing
changes on servers but still what we
found is that even fixing all this stuff
it still brings us 1u outage 1u problem
the week so it's really not possible to
reach 99.9 in anyway because when you
face a new problem it takes you around
30 minutes best to solve it because you
need to go through the logs you need to
understand what is causing it and this
you ischium never faced it before so we
said okay to get into 99.9 what we need
to do is to set common-sense target
goals we need to be created in recovery
so we set eco for container we need to
recover in 30 seconds for dr. API we
need to recover in one minute and for
host we need to recover in five minutes
so even if we have an outage on edge of
these domains the week we would be great
right it's less than 10 minutes recovery
but we failed so on container level it's
simple when you can Dana get stuck we
receive an alert and support team has a
monitoring or sorry Rumble and they can
remove this thing but we need to wait
for customer to redeploy this container
and we still practice an outage time
case for us is it means that we are
affected customer customer cannot use
our platform so it is hard it's 30
seconds is unreachable in this state
then dr. API as ujin said would batch
docker and this is great right we have
like one minute recovery time but still
on huge horse where we run a lot of
containers it takes around 5
seven minutes so it's not reachable as
well with our high density approach and
then post we tweaked good times great we
removed everything we don't use how
horses like flying through the space
when it boots up really and we also have
a script which automatically reboots the
cost when something goes south
so you see okay this was a strong
response if I don't want to make a
decision
let automation do them and post reviews
in five minutes
ready but still we have dr. to stop and
it takes again five or seven minutes so
we are still not fitting into this five
minute goal so what we did is we decided
okay we need to be completely agnostic
to this outages we need to have another
strategy and at first we will say okay
no AJ no scheduler but now we see we
have data that we can tell everybody
okay
we cannot survive in this state and we
need a scheduler and here is where cue
bananas comes into picture and Eugene is
really great with Cuba ninis is a guru
so if you guys want to have like the
discussion talk to him really okay so
I'll briefly I will not be taking taking
the details but in order to reach out
99.9% goal we had to introduce some kind
of container of recovery and we choose
kubernetes mainly due to its large
community it's supported by Amazon by
Google so on and it's incredible
flexibility still you have to understand
that cheaper net is is not an install
and enjoy a system and for example we
have such custom companies like
authentication we did it ourselves
networking for our outgoing traffic
routine storage and resource management
expect to do things yourself
so I am going to briefly mention some
key ideas specific issues to humanities
will is not a silver bullet you will
still have some weight issues so one of
them was to brunette his notes sometimes
were marked is not ready and check the
logs of cubelets
the cubit was marking was reporting
daughter is not healthy
we again that takes a stack times of
talker container D and figured out there
sometimes doctor runs C got stung it's
not a demon it's not supposed to the
currencies fire-and-forget process so
it's one one time process and it's not
supposed to run for a long time so
without bothering to dig further into
this issue who just introduced another
how to recovery just to kill this
processes if they run more than a few
seconds and it was basically without any
another thing is specific to containable
yes we have our events it buried behind
Adobe's and sometimes knows lost
connection to the costume so we simply
analyze the both cubelet logs and keep
API server logs and it was clear that
sometimes when we have an outage when
this connectivity is lost
traffic was before the artist traffic
was coming from for IPS API servers and
after the knowledge is coming from to a
piece so we just ask the a the glyphs or
a table your support is very awesome
they very helpful and professional and
they quickly ruled out all doubts and
said that you probably have these
persistent connections so he'll be does
not kill any disappear connections they
just change the DNS records and expect
you
not have any persistent connections
lasting longer than seven days but the
cubed Goulet uses HTTP to to multiplex
all of the requests and since you have
these health ships this HTTP to
connection into leaves basically forever
so we introduce another Auto recovery
which was just to kill this TCP
connections okay so oh no this is the
scheme of knowledge required for
managing a shared container based
platform I will leave it just for the
sake of presentation you may check it
later so all in all this are the key
takeaways yeah so key takeaways is as
you've seen if you gotta build this
platform at home you will face issues
guys really and no matter what you do
you're gonna face them so prefer
recovery to fixing at first you need a
great recovery strategy you need to
recover extremely fast and then you can
spray it on fixing stuff then of course
acquire skills the debugger
yes going into door going into cuban
areas it's not like okay i set it up
it's working it's fine tomorrow you will
face an issue believe me even on Amazon
what Amazon does or Google does they
give you masters for keeping it and they
say manage your work announced on your
own we don't care so you need this
knowledge even if you use anybody's
platform and the third thing is keep
things simple we learned it the Lord we
we wanted to start our platformers
extremely simple simple builder right
but then we got requirements from
products and we added complexity and
where the complexity and our plots and
extremely complex
now when human to human is we're adding
complexity there as well where this
complexity is different because you have
building blocks and they're decoupled
one from each other it's easy to manage
so try to things try to keep things as
simple as possible
and that is the only way to build a
platform is you want support</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>