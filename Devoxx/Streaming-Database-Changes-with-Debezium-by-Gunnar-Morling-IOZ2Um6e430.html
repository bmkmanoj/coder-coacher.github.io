<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Streaming Database Changes with Debezium by Gunnar Morling | Coder Coacher - Coaching Coders</title><meta content="Streaming Database Changes with Debezium by Gunnar Morling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Streaming Database Changes with Debezium by Gunnar Morling</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IOZ2Um6e430" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right
hello I'll run and good evening just
starting my timer here good evening and
welcome to my talk about change data
capturing with Tibby's iam I hope you
enjoyed the conference so far I bet it
has been a long day so I really
appreciate you show up here I know
there's some tough competition at the
slots so pretty good to see you all here
so what is it you can't expect in the
coming 50 minutes so I want to talk a
bit about what has changed data
capturing to begin with what it is about
and then what are some use cases where
we can benefit from change data
capturing I will talk a bit about
possibilities for creating data streams
of changes and finally I will talk about
division which is a tool for doing
exactly this and I will show the bees
iam in a small demo which I have
prepared let me introduce myself I work
as a software engineer at Red Hat and
nowadays I am fully focused on the
besoms where I took over the project
lead of this a couple of months ago
before that I worked as the spec lead
for beam radiation to that o so I've
worked with a group of interested people
on a new revision of the beam radiation
spec and it happens there is a quickie
tomorrow at the lunch break so if you
would like to know more about being
relation to the DOE I would like to
invite you for the quickie tomorrow and
another side project I'm working on is
map struct which is a code generator for
beam to beam mappings and I'm doing
another quickie also tomorrow noon so
two of these quick is right after each
other alright so if there are any
questions during the talk please feel
free to ask them it just might have to
shout because I might not be able to see
you easily due to the lights all right
so what is change data capturing about
and generally the idea is this so you
have your data in your database
obviously and now you create a stream of
events which represent all the changes
to tables within this database so for
all the inserts all the updates all the
deletes one corresponding events
representing this change will be emitted
and be made available to consumers of
such a stream and here in this
picture I'm showing Apache Kafka as some
means of having a messaging
infrastructure for it but it's not
implicitly tied to a Patrik Kafka so the
BGM is based on Kafka at this point so
that's what we are using but the change
data capturing concept itself it's
nothing which would be specific to Kafka
necessarily right and what what could we
do if we have this string of change
events so all the enzymes all the
updates all the leads we have eventful
it's what could we do with such a data
stream and well the first thing would be
just we could replicate data to other
systems to other databases so it might
be just some means of backing up your
data propagating it to into a replica
essentially but it also could enable
many interesting analytics use case so
you might have your production database
let's say it's my secret or Postgres and
then you have this cluster of a petty
spark or flink
or maybe it's hard whatever Teradata
perhaps so you have those dedicated
analytic systems and you would like to
run analytics workloads there well you
need to get your data from the
production system from the productive
database into those other systems and
again such a change data capturing
stream will enable you to do this or you
could think about feeding data to other
teams so let's say you work on this
honouring order management application
and now your marketing team your company
they would like to run a specific ad
campaign or a specific marketing
campaign that it would like to target
users which maybe have ordered a
specific product in the past and so of
course they need to find out about those
customers who ordered a specific product
and well you don't want the marketing
folks to run those kind of queries in
production database but instead they
should be doing it in a separate
database so again they change or a CDC
stream could be useful to just propagate
your data changes to the marketing folks
their database and then they can run
whatever queries they want there so
that's replication anyway the sense
another very interesting use case for
CDC is micro servers architectures and
very have different services or you have
essentially cut down your domain in to
differ
services and again well those services
they will very likely need to interact
with each other and the example here
would be you have three services like an
ordering applications of this managers
with managers customers customer orders
you have another micro service which
deals with the item catalog and the
products which which you are selling and
finally the third service which keeps
track of the stock so how many items do
you have for specific articles let's say
those are these three macro sources and
now if a customer goes to the other
application well very likely this
application will need data from the item
system and it will need data from the
stock system to fulfill its purposes
right so the customer they need to be
able to understand if there any stock of
this act mwoya but what's the item
description and so on and the question
is how could you do this and well the
first idea of course would be that the
order system talks to the other two
systems and it calls let's say rest
api's and this could work but ties those
three systems very closely together so
if the stock system goes away then these
rest requests couldn't be executed and
the ordering system would be affected
right so in some people call this
actually not a system of micro services
but it's more like a distributed
monolith the course those services also
close linked together so that's not a
good idea what else could we do well we
could again use CDC and the idea here
would be that the item system and the
stock system they provide a change data
stream with all the item changes
whenever a product changes is this
description or the stock system it
provides a data stream whenever the
stock count of an item changes and then
the ordering system could come and
subscribe to these streams and create a
copy of this data or maybe just a copy
of the subset of this data which is
interested in its own local database and
then essentially if those other systems
will go away for some time
still the honoring system would be able
to fulfill its purpose and it would
function it might be might fall a bit
behind
perhaps the stock system is just
unreachable but still its state changes
so the ordering system might
behind but still it can work and it is
not linked directly to the uptime of the
stock or the item system so that's how
you could use CDC to propagate data in
micro servers architectures and finally
so this talks a bit about well linking
or propagating data changes and stream
stuff between different systems but
actually CDC also could be very useful
if you just think about a single system
so let's say you have your data in a my
secure database again and then you have
this cache of data which you use to
satisfy specific queries or specific
lookups very very quickly so you need to
update this cache of course and whenever
the data changes this cache must be
updated or it must be invalidated either
way but those two things need to be kept
in sync or you could think about
full-text search so very often you would
like to be able to to provide the user
with powerful search functionality so
they can search with wild cut operators
and proximity and so on and usually
databases are not so good to do this and
instead you would use something like
solar or elastic search for those
full-text search functionalities and
again the elastic search or the solar
index it must be kept in sync with
primary database and you'll ask where
are you interesting use cases secure as
command query responsibility segregation
so this means you've worked with one
data model where you just apply your
rights to and then you would have
different multiple data models which are
optimized to satisfy specific read use
cases and of course those read models
they need to be updated whenever this
write model changes and again the cdc or
CC could be useful to implement this
okay so I hope I could establish that
CDC can be very useful and it enables
lots of interesting use cases and now
the question of course is how could we
do this how could we get hold or how
could we achieve it to have such a data
stream of data changes and the first
idea you could have is well my
application which changes its data and
it's in its database it also could go if
you think about this last example it
also could go to the cache and it could
update the cache and it could
also go to the full-text search index to
update the stuff there and this might
work if all goes good but actually there
are some some problems who said for once
it's very invasive for your application
right so this application needs now to
know that there are these other things
which I need to think about and which I
need to keep in sync so this is very
intrusive into your application
architecture and the probably the bigger
problem is it's very hard if not
impossible to do this in a reliable way
because mostly those things are not part
in what we would call a distributed
transaction so if you think about
elasticsearch there's no way this could
be part in the distributed transaction
with your database which means it always
could happen that if there are
concurrent writes going on that the data
in the full-text search index ends up in
another state then it is in the primary
database and just to give an example so
let's say there are two concurrent
requests in your application which both
effect the same customer let's say it's
addressed get changed and now out of
this in such a dual right approach as
it's called we would omit to write
requests to the full-text search server
but due to the nature of this HTTP
protocol you're not in control how
quickly those requests would be
processed so it might be that actually
the full text the request to
elasticsearch which was emitted for the
first data change it arrives only second
at the full-text search server and the
other requests arrestor first meaning
the data in your full text search
service drifts off the data ended state
in the primary database so this is
something which we need to be aware of
in it we would need to think about how
you can compensate for that by running
some batches and that's actually it's
not the best idea to do it next idea
could be calling so you could have some
means of just going to the to your
database and frequently you know pull
for updates
obviously this creates some sort of
conflict between how often do you do
this
and so how current is the data and the
performance hit which this gives you
right so you could ask every half a
second but this will probably have quite
a severe load impact on the database so
it's not something you would do too
often and all the question is of course
how do you actually find to changed rows
so usually you would need to have let's
say some update flag in such a table so
you know when it has been changed for
the last time so if this affects your
data model in the database right so this
is something which is not something you
might even be able to do depending on
you on your syrup and finally the
question is how would you deal with
deleted rows so if I Paul if I go to a
table and ask for the rows well I won't
be able to learn about any rows which
have been deleted because well I just
came free for them so that's essentially
not very possible okay so those
approaches are not good so what else
could we do and well the idea really is
that we access and query what is called
the database log file and essentially
all the databases they have either a
trends a transaction log or a
replication log and this file you could
imagine as a log of all the transactions
which have been executed successfully in
this system and which are in a few life
form so they are an ordered in
essentially and now by going to this
database log and essentially tailing it
you can get hold of all the strange
events and all this information you need
in a way which is fully transparent to
the applications which write this data
right so they are not affected by this
at all you just can you essentially
connect to the database and you get hold
of this log and you clear it right so
and usually those lots are either used
for transaction compensation so let's
say some a system or the database goes
away in the course of a transaction when
it comes back up again this is to we'll
be able to find out about this by
examining this log file and then I'm
doing all the trends or all the
transactions which haven't been
committed yet so they can get the system
can get rid of those or the other use
case for this is replication so between
a marster note of the database and slave
nodes this replication lock will be used
to transmit to apply all the changes to
those secondary notes so essentially you
could also think about this from a
conceptual point of view just acting as
another slave in such a wrap
educated structure and we act as a slave
and retrieve all the changes through the
replication mechanism and all those
databases they have this kind of locks
so my cichlids called bin lock in
Postgres we have to write a head lock in
Maumee beats the a block and all these
databases have this unfortunately
there's no standardized way to access
these locks so for us as the developers
of division it means we need to
understand about this those API so is
there an API to begin with and yes if
yes how is this API structured how can
we do it but then but then for you as a
consumer of DBZ 'm this is largely
transparent so that's that's our job and
the idea is let's use those log files to
implement CDC all right so we know now
how we can get hold of change
information and in the beginning I
mentioned where there will be some sort
of messaging infrastructure in such an
cdc design and very using apache Kafka
or divisible based on Kafka and this is
because Kafka has some very interesting
semantics which are which make it a very
good fit for the purposes of CDC so I'm
not going to give too full introduction
to Kafka I guess you have further than
other talks but let me talk a bit about
some of those semantics which Kafka
gives us which are so useful for our
purposes and the first one is all the
messages in Kafka they have a key and a
value and this means we for instance can
query essentially for all the messages
which share the same key and this marry
nicely maps to our CDC in this case
because we can structure the key of a
message based on the primary key of the
table which our change stream is about
so if the primary key of the table just
has a single column well then this
message key it would be just like a
single long or a single string or
whatever or if you had a composite
primary key well then this message key
in Kafka would also be a complex
structure and then you where you of the
message we will see that in a bit it
will press represent the actual change
so that's the first thing let's just
have a key then the next thing which is
very important there's a guaranteed
ordering so Kafka guarantees or gives us
guarantees that if a message has been
submitted to Kafka before another
message then we consumers
we'll get this first message first so
this applies within a single partition I
get to that but that's very vital for
use case because if you think about two
events one for an insert in the database
another event for an update of course
consumers need to see that insert event
first so it must not happen that the
update event overtakes the first one or
if you think about two updates where you
need to see them in order in so you are
able to achieve or to arrive at the
final proper final state it is pool
based meaning consumers themselves they
are in charge of keeping track of how
far have they read this particular topic
or value whatever would like to start so
it consumer it might just be interested
in you know following changes going on
from now on it isn't interested in
changes from the past but another
consumer it might be interested in the
entire history of data changes of a
given table and it itself is in control
when should it start to read this this
topic there's this feature of Locker
compaction which means well we have if
you think about tables which have
records which changed very very often
where we would see very many CDC events
in the corresponding Kafka topic and
this might be a problem at some point
and now the log compaction feature and
Kafka means that all the messages which
belong to the same key all of them will
be removed except the last one so this
means just last message pertaining to a
specific record and our database it will
be kept around and this will still allow
consumers to read the current state of
this particular record it won't have
access to the entire history anymore but
still at least it has access to the
current state and depending on the use
case this might be a good thing or not
but it's it's a useful option to have
and finally its gate spells I mentioned
there might be very many events and
Kafka allows us to scale the topics can
be partitioned and Charlotte across
nodes and again that's that's helpful to
Dewas with such a big load of course we
just need to keep in mind that the
ordering of messages is just guarantee
for a specific partition so if I have a
topic with change events
and this topic is distributed about
multiple notes the order is just
guaranteed within those specific
partitions and all the events which
belong to the same key they will go to
the same partition so I would still have
to guarantee for the events which belong
to one key but I would not have a
guarantee for the order of messages it
say for all the events which belong to
one table but depending again on the use
case it might be need it or not
so that's death Kafka and I think it
should be clear it's very useful for our
purposes and now we actually could go
and implement this CDC functionality on
plane Kafka but there is another very
useful thing in the Kafka ecosystem
which is called Kafka connect and Kafka
connect is essentially a framework which
makes it a bit easier for us to
implement data connectors and data
connectors they are either in charge of
getting data into Kafka or they are in
charge of getting data out of Kafka and
so they are called source and think
connectors and our CDC connectors of
course they are a sauce connector so
they they deal with getting data into
Kafka and I have a framework or there is
a framer provided for me which makes it
which gives me some structure if I would
like to implement implement such a
connector so that's good but it also
deals with the offsets and this means
well if our connector it reads the micro
the my signal bin lock it will read
always the events from specific
positions right and it could happen that
my connector goes away and when it is
started up again it needs to know okay
how far have I read the bin lock at last
time and I need to read on from there on
right and Kafka Connect is helping with
that so every now and then it will go to
the connector and essentially commit the
offset which has been processed so far
and if my connector has restarted I can
access this previous offset and read
again from there so this means I might
have committed an offset from a couple
of events ago so I would reach the bin
lock or I would read certain sections of
the bin lock a second time meaning
my consumers will receive same change
events another time but that's something
which you usually need to handle or need
to be able to handle in cough cannabase
so it's always mostly deliver at least
once semantics we have there you might
see some events more than once there's
support for schemas meaning cough
connect comes with some type system and
the way which allows me to describe the
structure of my keys and my way use of
my messages so I could make sense out of
out of those messages later on again
it's clustered so Kafka Connect could be
clustered depending on the specifics of
my connector it might make sense to
distribute it and have its tasks work
being processed on several nodes or not
and the very nice thing is there's a
rich ecosystem of connectors so if you
go to go to the confident website on
Kafka connect you will see lots of
connectors being listed there there are
many many connectors I'd say for there's
one for elasticsearch and for all the
databases terrasaurs connector sync
connectors and so on all right let's
cover connect and now let's take a look
how would such a CVC pipeline look like
so let's say I would like to stream
changes from those two databases here my
my secret my prosperous database so the
first thing I need to say Apache Kafka
cluster so I might have one already
running or I might start up a new one
but that's the first thing I need then
the next thing would be Kafka Connect so
that is a separate cluster a separate
process is separate from Kafka itself
and connect will manage those connectors
for me and finally I need to deploy
those connectors into Kafka Connect so
there is a DBZ M connector from my
sequel there is a DB zoom connector for
Postgres and I need to deploy an
instance of those connectors into Kafka
connect it will start them up and they
will begin and read the bin lock from
Isaac read the writer headlock from
Postgres and create corresponding change
events in Kafka and of course I would
not I don't need those events in in
Kafka for their own sake I would like to
use I would like to do something with
them and this means I will configure at
least one sink connector which then is
subscribed to these topics and which
amidst the data
into some other systems so here it's the
elasticsearch sync connector meaning
those three tables which we are
monitoring here and so we have those
three topics all the changes will be
reflected in corresponding elasticsearch
documents so I have spoken a bit about
the messages and I mentioned they have a
key and the value so let's take a closer
look so all right let's talk about the
key so this is resembling the structure
of the primary key of our table which we
are monitoring or tables so there would
be a specific message type row table of
course and the value is a complex
structure which comprises of the
following parts so there's it before and
in after states so in case we transmit
an update event this is very clear so
the before part of the payload it would
represent the old state of this change
data row and the after state it would
represent the new state of this changed
row phone insert the before would be
empty because there is no before and
foreign delete event where the after
would be empty because the record is
deleted so there's no such things as an
after state there is some source
information about this is a metadata
about the origin of this change so this
is like information about the database
server the position in the bin lock some
some the name of the table where this
event is coming from some time stamp and
so on and so that's that's the logical
structure of these messages and this is
independent from the that's a physical
structure on the transport layer so
Kafka essentially deals just with any
kind of binary data in its messages so
it doesn't care about this and what I
have here if I'm in the Kafka connect
ecosystem I have this notion of a
message converter and message converters
deals with or deal with were creating
the representation which is transmitted
for those messages and there is coming
already with Kafka connectors say Jason
connectors so this will give us the
representation we see here on the right
side so this is Jason and for that one I
actually have two
opportunity to include the schema of the
data within each and every message so
this schema would be a description of
the types of the before and after state
and consumers could then use this schema
to make you know to interpret interpret
this data accordingly of course this is
quite verbose jason itself isn't super
efficient but it's human readable so
this might be a nice thing for during
development for instance what I notice
when people talk about their production
purposes usually they use this other
converter which is coming with Kinect
and this is the ever of converter so
this is a very efficient and compact
binary representation so you cannot
relook at the message and make sense out
of it it just contains binary the binary
representation of the data and
importantly there is no schema
information within the messages
themselves there's just an ID of a
schema and the schema version and this
can be resolved using the schema
registry which is another part of this
Kafka ecosystems or the schema registry
just deals with schemas and aversions
and defined as a consumer take this
version identifier I get for a message I
can make sense out of this binary data
by getting the schema from the registry
and interpreting the binary stream
accordingly alright so and that's with
that I got everything in place so I got
a source for my CDC events I got Kafka
as very powerful messaging
infrastructure with very useful
semantics I have Kafka Connect as this
framework for implementing those
connectors and the final thing which is
missing is is this as though if I go to
the database and let's say this database
I would like to capture changes from it
has been running for a long time so it
might happen that I go to the bin lock
or to its transaction lock and this
actually does not contain all the
information about all the changes
anymore because if let's say this is the
replication lock from from Postgres and
the data has been replicated well the
database can just get rid of this
replication dog it doesn't need it
anymore and of course you can configure
this but at some point those files may
go away mean
I cannot get all the information I need
out of those change locks and the
solution for this is the besom can do an
initial snapshot for you so this means
it goes to the date to the tables you're
interested in essentially scans them and
amidst events for all the records in
such a table so essentially an insert
event would be created and sent to Kafka
for the tables which we are interested
in and then if the snapshot is done well
we of course can fall over to reading
the bin lock right after the point where
this snapshot has been taken so it's not
that's quite tricky to implement I would
say but for you you don't really notice
this you just see in the monitoring of
the connector you would see okay the
snapshot is going on at some point the
snapshot is done and it's going over to
the log reading mode but this is
essentially transparent for for you and
for the consumers all right so what what
connect us do we have our already
mentioned my secure and Postgres there
is another one for MongoDB and this one
is a bit special because well long way
to be is a bit different than the other
databases so for once there's not the
notion of a strict or fixed schemas well
it's not something you would have there
but most importantly changes are also
represented a bit differently so
essentially what we get from the long
way to be a block is some kind of patch
format in in MongoDB z-- own notation
which describes such a change so the CDs
events for more me to be they would have
contained this patch format whereas for
my secret and Postgres they will look
eventually like DeRose like the entire
rows we are working on other connectors
so the next one would be oracle secret
might be very possible and some others
so this way is depending a bit on the
feedback we get from from users like you
are from people interested what are
databases you are using and what
connectors would you like us to add and
if well if you here for instance Oracle
have been asked about this very often
many times well so I make some kind of
judgment okay this could be a good thing
to have Meriel DB we would like to add
this and you might think okay that
should be just very easy because it's
essentially the same as my secure the
problem here is that they've been locked
format in Moorea DB
has diverged a bit from the one which is
used by my sequel and we use a library
for reading this bin lock and this
library does not yet support Mario TV
severe a bit well in a tough spot years
we need to decide okay should we fork
this library should we create our own or
can we you know enhance it and
contribute back so that's the decision
to make right and we try to work with
the same always the same messaging
formats across those connectors mum will
be being a bit the exception as I
mentioned we also try to have common
options so there are options for filters
which are the databases or the tables
you would like to monitor how would you
like to deal with snapshots we do not
like any snapshot at all are you fine if
you just get changes from here on or
would you do sniff at once often never
again and this kind of thing and there
also is monitoring via JMX so Kafka
Kinect comes with its REST API from
monitoring connectors but this is very
cost clean essentially you see okay the
connector is running or not running
there is here this jmx based monitoring
it gives you some more insight into the
connectors so for instance if a snapshot
is running it will tell you how far is
this as the steps are proceeded or not a
very interesting metric is is how far
are we behind the behind the pin lock or
the data logins database so the
transaction docket will usually give us
some sort of timestamp and we compared
can compare this to the current time so
we know how far we are behind so that's
some useful information to work with all
right and with that I would like to show
this a bit and practice on any questions
so far before I do that yeah
okay so the question is in the bin lock
of my sickle do events representing
updates do they tell you the before
state and there are different modes for
the bin lock and if you are working with
the role mode you will get to before
States so our change events are able to
expose this okay so let me go to the
demo and I have prepared for that I have
prepared a little web application this
is just your regular crud application
and it deals with managing hikes so you
could other hikes subscribe to hikes and
so on and just to show you that it's
working let me create new hike so let's
say I hiked from Hamburg where I live to
enter up I see it here I can search for
hikes and so on and this is based its
Java EE application it's based on Wi-Fi
and it's using my seek for underneath so
my secret of is already running here so
the next thing I'm going to do now is
I'm going to fire cough-cough connect
and all the bits
I'm meeting for this and I'm using this
doing doing docker so I have a docker
compose file so this takes a bit just
let me show you the file to compose file
sorry
so f service is configured here for
zookeeper which is needed to administer
the cluster Kafka my secret that one is
already started before post class which
I will use in a bit as a downstream
target for my changes I have started the
schema registry so I will work with the
everal message format here so I need the
registry and yeah and of course Kafka
Connect and then there's some
configuration in Kafka connect so let's
say here the key and where your
converter so here I'm saying okay
disconnect instance it should use the
Avro convertor and not the JSON
conundrum okay I hope everything is up
let me check
ok this looks good for my the way to
phone it's big enough right you can read
it okay let's go mmm
right so I have started now cough cough
cough will connect next thing I need to
do
I need to deploy the DB team connector
and for that I use this end point of the
of the local sorry of the kafka connect
note so this is a REST API and I post
this JSON document to it for configuring
this connector so let's do that and if
all goes well it gives me back the
description of this connector so we can
take a quick look so it uses the my
sequel connector provided by the
division project it uses a specific host
name and password and some port and so
on so it knows where to connect to it
has a whitelist so this is the data
basis within this my secret process I'm
interested in and that yeah that's
pretty that's pretty much what's
interesting fast so the connector should
be running so let's see I just used the
status API from cough connect okay it's
it's running that's good so I should be
able to see some events so what should
happen now is that the connector came up
and it took an initial snapshot of this
hike table which this is about so I can
start now just to which is called the
console consumer which is kind of is
Kafka and it allows me just to look into
some some topic so I'm using here the
Avro console consumer because I'm using
ever as a message from it I say there's
the schema registry because it needs
that to make sense out of the binary
data and most importantly I say which
topic it should connect to so that's the
topic name which is used here by a
divisional connector so it's dv01
inventory hike it's comprised out of the
some database identifier out of the
schema name essentially and then the
name of table hike so I will run this
one and indeed I'm getting this data
here which was expecting of course and
so this is rendered here as JSON but
actually the data is ever so this is
just a with visualization here for us to
read and where we have a key here with
the primary key of the records we have
to before state which is not the cost in
it was never that just will omit the
insert events the after state
it's pretty much what we expect so its
destination Antwerp it's the start
Hamburg and then in the source element I
got this metadata which which I
mentioned so let me go back to the
application
sorry go back to the application let me
create a new hike so let's say I came
from Berlin I went to Amsterdam and
quite quickly I see this one coming up
here let me also do an update let me
change this and now in this update event
now I see here the before state with the
old destination and I see the after
state with the new destination by the
way if someone knows the Afrikaans word
consumer and can tell me how I could not
deform it listen I would be very happy
to learn about ok mmm right and I can do
a delete let me do a delete for you so
let me remove the last one and here I'm
getting actually two events that's
interesting so I get the one was before
State which is the old state of gyro
after its null and then I get another it
went for the same key with an LP note
which is called a 2 stone event which
essentially allows us to get rid of this
message eventually so it could be
removed from the queue if that's needed
ok so let me also show you a bit schema
registry so this is also exposing a REST
API and here I'm asking now for this
object with this name DB 0 1 inventory
high query so I'm interested in where
your schema and in version 1 and we can
take a look at this and this describes
the schema for us what can we see so for
instance before
structures described here and this has
all the fields which are the columns in
this table which we have the monitor so
destination start and so on ok so that's
that's pretty nice let me mention or let
me go back to the slides and discuss one
more thing and this is message
transformations and so kafka connect
also comes with a way for transforming
messages it's call
single message transforms as empties and
those smt stay for instance allow me to
extract information from the data from
from a message so I just can take a part
out of a message and propagate this or I
could convert parts of the method so
let's say I get a message which has a
time stamp using microseconds or
milliseconds what ever since epoch and I
would like to convert this into a nicely
readable string such a message converter
or such a transformation could do this
or I could use this for routing so I can
influence the topic of our message goes
to and those SM T's they could be or
they can be applied either on the source
side or on within the source connector
always in a sync connector so you would
do this in a zinc connector if you would
like to the message to be altered before
they ever show up in the topic then you
would have this transformation in this
on the source side whereas if it's a
transformation which you just like to
apply for a single specific consumer
then you would apply it for this
specific zinc consumer there are couple
of asthma T's which come with the museum
and the one st what we call the logical
table router and this lets you influence
to which topic a specific change message
is routed to in the use case of one good
use case for this is if I deal with
charted data so let's say I have my
customer data I have many many customers
so in my my sequel database
I have charlatans across five tables and
by default there would be a topic for
each of those five tables but probably I
would like to have one large topic which
contains all the changes for all the
customers from those five charts so I
can use this SMT to route all those
events into a single topic and then I
also could amend the primary key to
include for instance the shard
identifier and the other one which I'm
going to show here is the event
flattening SMT and this takes just the
after state of an event and pushes this
forward and the reason for this being
many of the connectors which are there
they don't expect such a complex the
structured CDC event but it just expect
a flat event which essentially sends a
single row it essentially would expect
just what we have as the after state in
our
CDC events so this smt it will just take
this after part and propagate it okay
and for that I have I will show you a
consumer and I already started Postgres
here so i i'm using now here the
Postgres client and i just asked
give me please everything from this
table which should contain the hike so i
can do it
and nothing is there so this table
doesn't even exist which makes sense
because we have not set up to sync
connector for post press yet so let me
do that so again I'm using this API to
now post this connector and this is now
using the JDBC sync connector which is
provided by confluent it has this
connection URL it has this topic so this
is subscribed to the DB 0 1 inventory
Hika topic and it's applying this
transformation of this unrep from
envelope type which we provide and this
makes fro it just takes the after state
from the CDC events so if I go to
prosperous again now I should see this
table so it has it has been created and
it just contains the data as it also is
existing in my primary database so let
me go to the application again let me
create a new one let's say I came from
Berlin and like to London and now if I
go back to post class and create again I
see this latest event there all this
latest record there so this works and
now the very nice thing about Kafka and
caf-co connected basing CDC on this is
it gets very manageable for us to deal
with failures so let's say Kafka Connect
isn't available for some reason so let
me take down sorry let me take the
connect service so this means the source
connector won't be listening to my see
anymore so I can apply sorry it can
apply some more changes so let me create
another hike let's say from New York
to Newcastle and it's great learn more
from classes to enter Europe and now if
I go to my prosperous database I won't
see these latest changes right because
Kafka Connect isn't yours so it's no
surprise nothing is happening but if I
started up again now so that's what I
mean so the data if we can fall behind
but it still is consistent to this
specific point so let me start up Kafka
connect again and this will then start
up the connectors and they will continue
to read benlloch after the point which
had been committed last right and it if
it might even be the case that this
connector had been down for a very long
time so the bin lock isn't giving us
enough information so then it could be
configured in a way that it actually
would start and take a full snapshot
again okay so this should be alright so
let's see as what the stuff stare and
and indeed now the connectors have kept
okay caught up again and we see those
changes which had been applied while
Kafka connect wasn't actually running so
that's that's quite nice I would say
okay right so to try to out yourself and
as you have seen this is we provide
docker images for every things which are
deployed to docker hub so you can go
there you can get for instance a connect
image which contains two DBM connectors
and this makes it quite easy for you to
get started so we also have those
compose files or some compose files
which essentially let you do what you've
seen just here yourself if you have a
docker installer say extensive tutorials
so this walks you through the setup and
describes very specifically messages and
format and so on and finally we have
instructions for openshift so if you are
running your stuff or your applications
on openshift we have instructions for
that and we also provide templates for
that essentially so there's the ammos
project from Red Hat which the works of
which works towards messaging a service
and one thing they are doing is the
very good templates for running Kafka
and openshift and is based on that in
terms of where are we with releases so
currently the current versus zero 6.1
there will be one or more micro releases
for that for sure the next one will be
out this week likely then the next minor
will be 0.7 which will move to Kafka 100
which you might have learned or noticed
that they went 100 recently and most
importantly this will enable to run the
museum on Postgres on Amazon RDS so for
to give some background there on
Postgres we need a server-side plugin
which is called a logical decoding
plugin and amazon RDS we are not really
control what online logical decoding
plugins which exists there but they just
recently added one which is essentially
omitting Jason messages so we are
working now with this logical decoding
plugin which lets you use the bazoombas
Postgres and Hardees arias there will be
further CX releases with new connectors
so as I said hopefully Oracle being the
next one we might add support for a
Finnish band which is this data grid or
key value store you could say bye-bye
Jamis and we're going after DB zoom one
dot oh we might well there might be
further connectors but one thing which
we have in mind is we would like to
support you with creating of with event
aggregation this means so those CDC
events they currently relate to specific
tables so you will get let's say you
have orders and all the lines the CDC
events you will have CDC event for
orders and for all the lines and very
often for instance if you talk to
elasticsearch you would like to create
an aggregated event which contains an
order and all the other lines in a
consistent state and so we have some
ideas how we could provide you with some
tools and some building blocks so you
could quite easily achieve such an
aggregated event was that I'm almost
done so to wrap it up Division brings
you change data capturing for a number
of for a growing number of databases
what I find very useful very interesting
about it it's very it is transparently
in terms of setting this up so you don't
need to alter your source applications
ACF as you saw I was running this hike
manager application and I didn't have to
touch it I just could set up DBZ 'm and
it would talk to database and capture
the events from there the database it
must it may require some sorts of
configuration so I should admit or I
should say that but the application the
upstream application it's not affected
as we've seen this works kind of
reliable and in failure situation so if
single components aren't there well
consumers they might fall behind a bit
but still they will catch up eventually
and data is always consistent various if
you think about this dual right approach
you might end up in a state very your
full-text search index doesn't represent
the same data or it just represents some
wrong data and which is not something
which you would want if everything it's
open source it's on github so you can go
to github.com DB z 'm and just catch the
source code there the website to be some
dot io and yeah we would be very happy
to learn about your requirements for
that or even better if you would like to
contribute that would be great
and finally there's the DM Twitter
account so if you would like to follow
up on the news releases and so on that's
the canto subscribe to this that there's
some time left for questions anyone with
questions
so that's always it could be a good sign
or bad sign either everything is clear
or nothing is clear you don't dare to
ask anyways otherwise feel just proof
free to come to me afterwards and we can
have a more informal discussion thank
you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>