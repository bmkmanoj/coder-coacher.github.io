<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Resilient Architecture by Matt Stine | Coder Coacher - Coaching Coders</title><meta content="Resilient Architecture by Matt Stine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Resilient Architecture by Matt Stine</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9Y4dGIaEqPk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi I'm Matt Stein I'm going to say a
few words about resilient architectures
if you want to interact with me probably
one of the easier places to do that is
on Twitter if you're looking for me to
say profound professional things that's
probably not the place to get that for
me I mainly use Twitter to amuse myself
if you want to kind of see the stuff
that I've written and other talks that
I've given my websites kind of useful
for that so diving right in I want to
start out by looking at some anonymized
recent technology headlines you may have
experienced one or more of these maybe
even in a crowd this big there's
somebody who was a part of one of these
I'm not trying to shame anybody let's
just get that out there the point is to
draw awareness to the problems that were
actually trying to solve so system
failure cost a well-known retailer
significant revenue on the biggest
internet shopping day of the year this
has happened several times in the last
two or three years retailers make all of
their money in a very short period of
time and in that short period of time
that's when their systems are put under
the most stress and when they fail and
when they fail spectacularly then you
don't make that money and that's a big
deal because you usually don't have
another opportunity to catch up until a
year later this one is near and dear to
my heart because of my lifestyle system
failure causes cancellation of hundreds
of flights stranding thousands of
airline passengers ultimately costing
the airline millions in revenue if
you're familiar at all with the way
airline travel works you know that if
you lose even a few flights that not
only strands passengers but it also puts
planes out of position and so actually
recovering from those misplaced planes
and recovering from those misplaced
passengers has a cascading effect where
a few hours or a day of problems can
lead to weeks of recovery and once again
a lot of money is lost in that equation
beautifully designed online store
crumbles under the pressure of a
thundering herd of customers trying to
purchase the latest tech gadget I used
to stay up until three o'clock in the
morning to buy iPhones don't do that
anymore not nearly as excited about as I
once was but myself and actually another
speaker who's speaking against me right
now and we did that one year and we're
texting at 3 o'clock in the morning hey
have you gotten in yet no I haven't
gotten you know going back and forth and
about five minutes after the iPhone that
we were buying that year went on sale
basically every site that sold the phone
was down and this was several
generations in but obviously they've
recovered from that fairly quickly but
again people who are trying to get in
the store can be great it can work
really great for a normal load of
customers but can it handle that kind of
a spike even if it's planned and then
you've got security breaches customer
credit card numbers other personal
information again leading to millions
and lost revenue due to in this case
resulting loss of trust
so a lot of the focus today like what
we're trying to learn is an industry is
on I t's lack of speed you know how can
we go faster how can we be more agile
how can we be more productive but it
sounds like even when we're going slow
we struggle to keep systems resilient so
how can we do it going faster well this
actually requires a fundamentally
different approach
and if you think about the approach that
we've had up to this point usually it's
about mistake prevention so if we just
do a little bit more testing if we just
do a little bit more release engineering
if we have a few more reviews if we have
a few more approval processes then you
know we'll be able to prevent things
from happening and the problem is is we
don't prevent things from happening and
then we create this process that's
pretty heavyweight and makes it kind of
hard to get changes into production and
so when things break we don't actually
have time to execute our process so that
we can fix those breakages and so we end
up fixing the problems outside of the
process and so we end up doing break fix
work sometimes in the production
environment and sometimes those changes
never make it back into the real process
and that makes resiliency even harder
and we create more problems and so we
kind of have this chaotic cycle that
kind of feeds back on itself so what a
lot of these disruptive companies are
doing is saying well it doesn't seem to
work to try to prevent mistakes doesn't
matter how much work we do we don't seem
to be getting any better at in some
cases we get worse so maybe we should
just stop trying maybe we should just
embrace the fact that failure is going
to occur we know what's going to happen
and in fact as we'll see a little bit
later on sometimes we might make it
happen intentionally and let's make it
really cheap to recover
from failure so we start to think about
well how do we measure success
well the way we've measured success in
many ways traditionally is MTBF mean
time between failure we want that number
to be really big this is actually a
metrically borrowed from the
manufacturing industry I grew up you
know driving past my my father's
manufacturing plant that he worked at
and there's a big sign out front that
says number of days since this plant has
had a safety accident and they're really
proud that that number is really really
huge and it anytime somebody got hurt
that clock went back to zero and that
was not a happy event and so we kind of
translated that metric into software but
maybe it's not the best metric to
measure success in this case so we're
kind of moving in this direction of MTTR
mean time to recovery
it does matter how many times we fail
mean time between failure can be really
small but as long as mean time to
recovery is also really small then maybe
we can actually recover so fast that our
users don't experience it now if we want
to work this way we really need I say
better maybe different tools and
different techniques and ultimately we
need the architecture to give them to us
like we can't just assume that we're
going to go build a bunch of software
the way we've always been building
software and throw it into production
and were somehow going to magically be
able to depend on an Operations team to
make all this stuff happen we actually
need to build the software differently
and so that's where we're going to get
into talking about this notion of
resilient architectures and I really
just want to tell you three basic things
about that what do resilient
architectures do the first thing is that
they enhance observability what is
observability it's our ability to see
into
the software is doing we can't tell if
the software is sick we can't tell if
the software is recovered if we don't
have the ability to see resilient
architectures also leverage resiliency
patterns the interesting thing about a
lot of the problems that we're going to
run into is they're pretty well
understood and we talked a lot about
micro-services and distributed
architectures right now we've been doing
distributed systems research for a
really long time if you want to figure
out how to build distributed systems
well don't go read a bunch of blogs go
read a bunch of peer-reviewed papers and
you'll actually learn everything that
you need to know and a lot of the
solution patterns already exist we just
need to learn them and leverage them and
I'll talk about a few of those and then
we need to embrace chaos we need to get
to the point where our architecture
assumes that it's always going to be
under some sort of an attack and we'll
break down exactly what that means when
we get there so let's dive into
enhancing observability spend a lot of
time talking to teams about architecture
and you know they tell me you know we
want the architecture to be highly
available and I just kind of come right
back with the questions and what does
that mean like highly available how do
you measure highly available we actually
have to understand our resiliency
requirements much more resilient ly than
that like this idea that oh well the
system should never go down ha the
system's going to go down the question
is you know what do we want to happen
how do we want to recover from that
anybody read the site reliability
engineering book from Google anybody I
can hardly see any of you anyway so it
doesn't really matter if you wave if you
haven't read this book you don't
necessarily need to read all of it this
book is about basically how Google runs
Google
and a lot of the stuff in the book is
very specific to them but there are some
chapters in the section about principles
that are really really helpful really
chapters three four and five or where
you need to focus and you can read this
online for free so Google's published it
on a public website so you can go read
it without spending any money and it's
really worth your time and so one of the
things that they focus on is
measurements and what types of
measurements do we need to have and how
do we need to work with them and they
divided it up into basically three
different categories so the first thing
that we look at is this notion of a
service level indicator and this is some
quantitative measure of a level of
service that we're providing so you
might look at something like request
latency what is that over some window of
time over some cluster of servers
give me a measurement of that but this
is not enough just knowing hey I can
look at here's the numbers for request
latency doesn't really tell me much
because I have to think about all right
well what is acceptable request latency
what is normal request latency and what
is unacceptable request latency what is
abnormal request latency and so for that
we need to design the focus here which
is a service level objective this is our
target value or range of values for a
service level as measured by a specific
SLI so we come back to request latency
we might think about okay request
latency should be between say a hundred
and fifty milliseconds and 750
milliseconds over a period that we
measure and so then we have the ability
to say for each of these service level
objectives what's acceptable what's
normal what's unaccepted
what's abnormal and we can watch these
service level indicators to tell us hey
are we meeting our objectives or are we
not meeting our objectives now we can
take these and we can transform them
into the thing that everybody talks
about which is service level agreements
SLA we talk about SLA s and not a lot
but probably most of us aren't actually
doing SLA s because SLA is actually have
consequences associated with them based
on some contract so you know if I don't
meet this objective then I have some
financial penalty or some other penalty
associated with it I might have a
contract with a customer that says hey
if I don't mean my SLA then you don't
have to pay me as much money this month
so we need to have the foundation laid
before we can actually do that well
another thing that's important is to
focus on distributions we talked about
mean average a lot turns out to be not
that useful most of the time it's
entirely possible for a lot of your
requests to be really really fast but at
the same time there's this long tail of
requests that turns out to be really
really slow so this is a graph that I
actually yanked right out of the sre
book and if you look at it you can tell
that well for a typical request you know
we're actually serving that up in about
50 milliseconds but 5% of the requests
are as much as 20 times slower if we
just look at the average you know that
purple line around 50 you know we see
maybe a slight increase but really not
so much detectible seems like things are
mostly the same all the time if we start
to get up into
these higher percentile buckets we see
that we go from a mostly even set of
performance to a really jagged and spiky
set of performance and if we're just
looking at the mean we can't even
actually see that now we have to start
to think about well which of these is
more important is the average what our
users are experiencing or is it you know
our 90th and 95th and 99th percentile
you know which which one of those is
more reflective of the normal experience
there's really great blog post out there
that you can go read in fact if I tell
you to read something please go read it
it's worth your time I don't have much
time to read and so when I say read
something this is like go do it tomorrow
it'll be worth your time this is a
fantastic blog post because most of us
actually don't understand how latency
works I didn't understand how latency
worked until I really started dealing
you know diving into it and this
summarizes a lot of the really
interesting problems and it's you know
cleverly titled well everything that you
know about latency is wrong there's this
really nice table that summarizes a
bunch of popular web pages that people
go to and ask the question well how many
visits to these web pages is actually
going to experience a 99th percentile
latency so you can take the number of
requests generated so in this case
amazon.com you visit amazon.com that's
going to cause a hundred and ninety
requests to actually get fired off in
one visit and then we could do a
calculation that says well of those 190
requests how many are going to
experience a 99% latency and then we can
come up with the chance that what
percentage of visits will have that
inside of it and so for amazon.com
eighty five point two percent of the
requests to that website are going to
experience a 99% latency you go to the
bottom of the table
even google.com which is what a
textfield and - and - buttons 26.7% of
requests to that page are going to
experience a 99% latency what that tells
me is that anywhere between 25% and 90%
of my users depending upon how my site's
architected are going to experience
quote-unquote the worst case latency not
the average case latency and so worst
case latency turns out to be much more
indicative of what our users are
experiencing you go back to the Google
SR e-book they talk about how a lot of
teams at Google don't even think about
averages they optimize for 99% latency
with the assumption that if a 99%
latency is good
Meli average latency is probably going
to be pretty good as well so we need to
measure a bunch of things maybe we're
not used to measuring things how do we
get started Google talks about for
golden signals that if you don't know
where to start here's a good place to go
we've already talked about one of them
latency you know we want to have a way
to measure things so that we don't find
out that the website is slow by user
picking up the phone and saying the
websites slow we don't like those phone
calls because they're really hard to
figure out okay well exactly what do you
mean by slow sometimes that slow can
actually be normal
sometimes that slow is abnormal we don't
necessarily know which one it is
we want to measure traffic so here's our
latency right now here's how much demand
is on the system right now and that can
start to tell us well under what
conditions is latency acceptable how
much traffic can we actually handle we
probably have some notion of acceptable
error rate so depending upon what type
of loads of systems under what's
actually going on we're going to
probably have some
proportion of requests that are in the
200s and some are in the 500s saying you
know I'm never going to see an error
also again not going to happen so how
many errors are acceptable and then a
really interesting one which is called
saturation every system has a tipping
point you can't design any system any
architecture that's going to work under
all circumstances relative to load and
well it's it relative to you know what
is the bottleneck for that system so in
some cases your memory constrained in
some cases you are io constrained CPU
constrained whatever you need to
understand how full is your service
right now if you hit that tipping point
all of these other measurements are
going to to go out of whack and a lot of
systems actually de creating before
performance way before they ever get to
100% utilization and so we actually want
to have a utilization target we want to
understand you know how much traffic are
we going after and if we're going after
a substantially larger amount of traffic
then we may need to scale the system we
may need to re are connect the system so
how do we measure these things once we
know what we're going to measure well we
want the architecture to give us
different types of telemetry so I've got
two different categories here one is
service telemetry one is integration
telemetry so you know I want to know
traceability I want to know when I'm
looking at a process what is that
process you know what code and get
represents that process what package
coordinates in maven what API version
are we serving up what features are
toggled on or off right now I want to be
able to measure health you know what
what is my health
does that am i healthy because of what
is that the process is up and running
is that the process is up and running
and it's listening on a port some of
those things may be healthy some of
those things may not be this can get
complicated and sometimes my health can
actually be based on my dependencies you
know micro-services aren't going to
necessarily save us from health problems
fact they might create health problems
and so we have to think about well what
are our critical dependencies what are
our required dependencies and if those
are unavailable can we actually do work
we talked about quantitative
measurements we talked a lot about
technical metrics you know resilience
I'm sorry of latency traffic error rates
but there's also business metrics and
how many shopping carts are completed
how many shopping carts are abandoned
how many signups do we have things like
that those are also important and then
qualitative measurements things that we
want to know about but that we can't
actually attach numbers to so this is
kind of where we get into our logging
and then we need to start to measure for
a distributed system what's actually
going on in the system as the behavior
emerges from all these nodes
communicating we're going to talk about
the circuit breakers a little bit later
if we're using those circuit breakers
can actually help us to measure what is
the behavior at the edge what is the
health at the edge of a graph
distributed tracing request comes in
it's going to flow through a huge
distributed system who's contributing to
the latency can we actually isolate a
system level latency problem down to a
specific component we're using messaging
using the measurements of our message or
'ti middleware like rabbit or Kafka to
understand what's going on there and
then taking our logs also and actually
correlating those I'm going to show you
some examples you know how we do this in
the text stack at pivotal a lot of folks
using spring boot
you know spring boot has created a
really nice set of features in what's
called the actuator that allows me to
you know out of the box have a lot of
this to limit
we just work so one of the really cool
things is the health endpoint that for
as I add more dependencies to my
application that are interacting with
different things each of those
dependencies whether it's spring data or
spring security or any of the other
projects can actually bring health
indicators along with them and then the
health indicator subsystem is going to
aggregate all those components to give
you an understanding of what is the
health of this particular service and
then I can write my own health
indicators for things that don't come
out of the box just by implementing a
very basic health indicator interface
same thing from a traceability
standpoint next to the health endpoint
we have an info endpoint that you throw
some very basic plugins and properties
into a maven or a Gradle build and you
can automatically get here's my get
commit information here's exactly what
Shaw what branch who did the commit when
did it happen I can get maven
information I can get any other thing
that I implement an info extension point
for and then distributed tracing this is
a screenshot of Zipkin and Zipkin is a
tracing system that will accept
telemetry from a bunch of services that
are engaged in a particular request
process and those time stamps that we
gather between when I've received your
request and when I've service that
request and when the client sends the
request and when it receives the
response are tagged with correlating
information to tag it with that
correlating information you did a tracer
embedded in your application that's
actually going to instrument those
events and so this is where spring
clouds sleuth can actually come into
play just by adding that to
the classpath of my application it will
automatically start to instrument
different flows inside of a spring boot
app so here are a few different examples
I'll post these slides after the talk so
that you can get at these links a little
bit more easily but these are some
different tools that spring and then
pivotal Cloud Foundry one of the
interesting things that we've done there
is we've taken our web console and
detected you know is an application of
spring boot application and if it is we
actually pull the information out of
those actuator endpoints and surface
them in our application dashboard if
you're not using pivotal Cloud Foundry
there's actually a handful of
open-source dashboards out there that
people have created that do essentially
the same thing so now that we have
understood what we want to measure and
we've started to measure those things
we're going to need to build the
software differently for those
measurements to turn out the way that we
want them to and so we're going to need
to start to leverage resiliency patterns
cool thing about resiliency patterns
again as they're mostly well
characterized and understood and also
they're not that hard in many cases just
the act of thinking about using these
patterns and is going to make your
system better because you're going to
write your code differently so we'll
start really simply which is the notion
of timeouts timeouts really matter when
you're going outside of your zone of
control I'm going to cross a network
boundary you know I can never assume
ever that I'm actually going to get a
response to the requests that I send out
at some point I have to be able to give
up and so again thinking about timeouts
is half the battle
I'm writing my code I'm going to make a
method call and that method call is
going to
doulton some distributed operation and
if I'm still building a system with
threads as a concern I'm still in a
blocking scenario and a lot of the stuff
that I'm going to do for a while even
with all this reactive goodness that
we're doing it's going to have blocking
scenarios so okay is the thing that I'm
going to do going to block a thread yes
okay it's going to block a thread that
means that if I wait forever is that
thread ever going to unblock no okay
well do we have an infinite supply of
those things how many threads do we have
right is it possible to run out yes if
we run out of threads are we able to do
more useful work no what what might that
mean what might mean that clients of my
service now are going to get into a
similar scenario if the clients of me
are not thinking about timeouts then
they'll start to fill up and lose all
their threats and we start to see this
cascade across the system so we need
something to interrupt and get us out of
trouble if you're looking at an API that
you're using you know some library that
you've pulled in you will get method
calls if you have a method call that has
no timeout argument and then you have
another overloaded version of that
method that I'm sorry that that has an
optional timeout argument always call
the version that has the timeout
argument why well you might think well
if it doesn't have a timeout argument it
will actually have a sensible default
timeout probably not probably the
default behavior is wait forever believe
it or not rest template in spring
delegates timeouts to the underlying
requestfactory the underlying
requestfactory if you're using the
defaults will actually wait forever and
so if you want rest template to ever
actually timeout you have to go in when
you create that beam and configure it
with connect and read timeouts very
simple to do if you don't do it
just know that risk templates not going
to save you
when it comes to an HTTP request that
never actually gets a response and holds
an open connection it will wait as long
as you will let it wait retries
sometimes we're going to have transient
failures you know we have a hiccup on
the network we send a request that
request would have gotten to an
otherwise healthy service we would have
gotten a response back or we have a
system that is under load right now we
send a request and it's overloaded at
the moment but if we had turned around
and sent that request even a second
later that load would have gone down
enough to be able to get a successful
response so sometimes just a simple act
of hey a request failed instead of
giving up and popping an error up to the
top of the stack we just retry that
request immediately sometimes that's
going to be enough to get us going
forward again now don't take this to
mean that oh I should always retry every
request that I send if it fails you need
to understand what are the systems that
you're talking to and what is their
behavior sometimes doing this is going
to make things worse sometimes doing
this is going to make things better but
when you're in the scenario where you
know I know that a system has
reliability characteristics to where
retries can be useful we can throw this
in sometimes we don't want to reach
immediately sometimes we want to wait a
little while so back off some time and
we might retry again we back off a
little bit more we could use a uniform
back off we could use an exponential
back-off lots of different well
characterized algorithms that we can
employ do this and then obviously we
want to decide well when are we going to
give up we're going to retry forever or
are we going to retry only a certain
number of times and then pop out and
handle that behavior some other way
regardless of what we're doing here you
really want to log all of these events
so I've had a failed request I'm going
to retry it on the other side obviously
we're logging the request that we
received and responded to and then if
we're correlating our logs with
something like spring cloud sleuth being
able to show hey we're having lots of
retries here what's going on on the
other side of that and we can watch for
those patterns and determine you know is
this just normal behavior or is there
something that we need to re our c't
exon into the client side or on the
server side of this to be able to
improve that behavior now as a
little-known project called spring retry
that makes this really easy to do so two
annotations to point out here the first
method is annotated with a tree tribal a
tree tribal will give me a very basic
immediate retry with a uniform back off
strategy actually no back off strategy
and a maximum of like three retries and
if that's good enough you've got that
and then the next method is annotated at
recover and so for this bean anytime one
of these a tree tribals actually falls
out and has a maximum number of
recoveries then we can actually execute
this method instead and so this is a
very simple example of retry or request
for some things for a few times
and then if we can't ever get it then
actually give just some default response
now if we want to introduce a back off
strategy that's pretty easy as well so
retrial takes a back off argument and we
can hand it and another at back off
annotation where we can enhance the
behavior by adding delays you know what
do we want to start with what we want
the maximun to be what multiplier do we
want to use between them
do we want to randomize that back off in
any way I want to move away from a
uniform distribution back off strategy
to an exponential back-off strategy I
can simply add a bean of type back off
policy and there are several of these
provided in the library one of those is
exponential back-off you simply declare
this bean and then all of your retries
are going to become exponential back-off
based retries bulk heads so we take
shifts and we divide them up into
watertight compartments with these walls
that we call bulkheads why do we do that
well if we get a hole in the ship and we
don't have bulkheads then the whole ship
can fill up with water if we have
bulkheads with watertight compartments
we get a hole in the ship there's a
limit to the scope that's going to fill
up with water and that can give us a
greater chance of survival that can give
us a greater chance of being able to
contain and fix the damage before we
lose the ship so we can translate this
idea into our systems you know how do we
divide our system up into watertight
compartments well one of the ways that
we think about doing that is
microservices by dividing our systems
that have different types of failure
modes and picking those failure modes
that we want to isolate and putting them
into different services then we can
build bulkheads into the system
architecture now you know that this
comes with once I've isolated that
failure that way I have to deal with the
other failures that come along with
being a microservices distributed system
so none of this is free if
within again a single application that's
dealing with threads again dividing
different types of work into different
thread pools so that if one of those
types of work becomes sick or behaving
badly there's a limit to the number of
threads that it actually can consume
from the system so maybe other parts of
the system can continue behaving
normally and if you're if you're using
historic switch is actually most famous
for being an implementation of the
circuit breaker pattern that we'll talk
about next it also actually implements
thread pool bulk heading out of the box
we can zoom back out of the architecture
and think about the notion of
availability zones so if we're using a
cloud provider or we're using multiple
data centers we can actually deploy our
application across multiple availability
zones and so that if we lose a data
center if we lose an AZ in the clouds we
can have some understanding that that
failure is going to be isolated from the
rest of our application and our
application can continue behaving
normally and finally the circuit breaker
pattern circuit breakers are again
really simple and that's where all other
power comes from is a very basic three
state state machine requests are going
to pass through a closed circuit just
like electricity passes through a closed
circuit when our circuit breaker is in
the closed state so we put this circuit
breaker between my clients and my
service close state request just pass on
through if we get a failure of any type
so an error we get a timeout then we're
going to count that and we're going to
look at the number of failures that we
have in a specific threshold of time and
once we hit that threshold we're going
to trip the breaker and that lands us in
what's called the open state open state
open circuit electricity doesn't pass
through open circuit breaker in
and software requests are not going to
pass through meaning that we're never
going to talk to the other service in
this state we're going to say that thing
is sick
so I'm not even going to issue the
request instead I'm going to fail
immediately and leave it to the system
to decide what to do with that failure
and circuit breaker library like history
gives you built-in abilities to define a
fallback behavior to go along with that
open circuit now if this is where the
state machine stopped even if that
downstream system became healthy again
we would never talk to it we would never
know and so the way we break out of that
open state is by periodically
transitioning into this half open state
and in the half open state we behave
just like the closed State except when a
request comes in we're going to process
one we're going to send that request to
the downstream system if it succeeds
then we're going to transition back to
closed and go back to normal if that you
know canary request that we just sent
out fails then we're going to go back to
open and so what kind of oscillate
between open and half open while we're
waiting on health of our downstream
service to come back and then we'll go
back to closed so if you're using spring
cloud bringing circuit breakers through
history into your application again
pretty simple out of the gate I can
annotate any beam method with at history
command and I now have a default
configuration circuit breaker wrapped
around that behavior passing to at the
end of the argument fallback method just
like with spring retry where we have the
at recover here we're going to give it a
method argument to say call this method
when this circuit breaker is open or
when we experience any failure in a
closed circuit of course we can tweak
all of those thresholds and counts that
I talked about by passing additional
arguments to the history command
annotation so again take a look
at spring retry take a look at history
or you can leverage history directly or
go through spring cloud to get this
annotation of Spring beam behavior okay
so last point embrace chaos we've
measured all the things we've used all
the resiliency patterns our system is
resilient now yes no maybe we don't know
how do you know your system is going to
tolerate failure if it hasn't failed yet
we want to actually practice failure if
you think about recovery exercises if
you think about disaster do you want to
learn your disaster recovery procedure
in the disaster or do you want to learn
it before so that you're prepared so one
of the simplest things that you can do
is what's called game day exercises if
you think about sports teams they
practice so that when they're in a game
situation they can go off muscle memory
and not have to be thinking about well
how do we actually play this game so we
can put a date on the calendar to say
I'm going to simulate a catastrophic
event and we're going to practice our
response and we're going to evaluate
both our system's ability to respond as
well as the people who maintain the
system it's kind of hard to separate the
software from the people who created and
maintain the software so we're going to
evaluate our ability to respond we're
getting crosstalk please welcome to the
stage one of the world's best self
developers thank you
so you don't want to evaluate your
emergency readiness in an actual
emergency
but gameday exercises maybe aren't
enough so Yong Chun wrote this paper in
1975 called the design of self checking
software and in the paper they made this
pretty audacious proposal that we should
insert fake ghost planes into an air
traffic control system and the premise
was that if the air traffic controllers
could land all of the ghost planes and
all of the real planes you know they
don't know what they're talking to then
we can actually trust that system now
this was never actually implemented but
these ideas kind of percolated and
eventually turned into things like the
chaos monkey chaos monkey gives us the
notion of an unscheduled random game day
exercise we run processes that will kill
things randomly and run that during the
normal business day while we're working
and actually simulating pillagin we know
that when we build a service our service
is going to have to withstand this type
of chaos which means that we build
better software we build more resilient
software we get better at handling and
responding to failure and ultimately all
of this contributes to make the system
better I want to show you one of these
two examples from the Cloud Foundry
space so we're going to look at two
micro services protected by a circuit
breaker I'm going to use siege to
generate load on the client micro
service we're going to look at the log
tale on the cert the backend service and
then we're going to create chaos events
with this module from the Cloud Foundry
ecosystem called chaos lemur
I'm sorry chaos loris
so top left pain is our load generation
top right pain is our logs and this
video is like way far ahead of where it
should be let me bring that back to
normal okay cool and we've got a circuit
breaker that we're monitoring over there
to the left so what I'm going to do is
I'm going to post some things to the
chaos lures API I'm going to give the
chaos floor as a handle to the
application that we're talking about and
so now chaos Loris knows about my app
I'm now going to create a schedule I'm
going to run chaos every 15 seconds you
probably don't actually want to do that
in production but this is a demo good
thing I want and now we're going to
create a chaos event that ties that
application in that schedule together
where the percentage of times that we
should end up killing that downstream
service so now we're going to watch the
logs on the chaos Loris and it's going
to start telling us every time that the
chaos schedule actually runs so we've
started it up and we're going to start
to get some chaos events so we'll speed
up time a little bit here to make things
not so painful to wait on and eventually
we're going to get a chaos event that
actually is going to shut down the
downstream service so we got that right
here we see that we're going to
terminate the application we look at the
logs for the application says okay we've
successfully Detroit destroy the
container that that applications running
in we watch the circuit breaker we see
the circuit breaker go unhealthy and
trip to open and so now we wait
now Cloud Foundry is going to
automatically recover that application
that it determined is no longer healthy
and it's done that the funny thing is is
while that happens
chaos Laura strikes again and shuts it
down while it's trying to recover
and so we end up actually having to wade
through a couple of these failure events
before our application comes back to
healthy again and this actually turns
out to be a really good example of
reality as opposed to you know kind of
an imagination in kind of a fantasy
system like are we always going to
successfully recover immediately or are
we going to potentially go through a few
different failure scenarios before we do
and so this randomness to the chaos can
actually help us to get really good at
handling these types of events okay so
let's make sure we understand where
we've been
stop trying to prevent mistakes embrace
failure focus on mean time to recovery
make failure cheap make make recovery
fast make sure you know what you need to
measure to understand that your system
is healthy and measure those things and
make sure that your architecture
enhances your ability to see learn these
resiliency patterns and leverage them
where they make sense and then embrace
chaos start to practice failure
continually so that you can actually
make your software better I think we've
got 15 seconds one quick question all
right I don't see any hands thanks very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>