<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Chaos Engineering by Lorin Hochstein | Coder Coacher - Coaching Coders</title><meta content="Chaos Engineering by Lorin Hochstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Chaos Engineering by Lorin Hochstein</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vq4QZ4_YDok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right good afternoon everyone let's
talk about chaos engineering so I don't
know if anyone noticed this last week
but there was an essay posted to the
Atlantic by Ian bogus he's a professor
of computer science at Georgia Tech
there's almost more of a rant than an
essay about how programmers at least
some programmers shouldn't refer to
themselves as engineers interestingly
low-cost is not a software engineering
researcher he does computer science and
media he's written some interesting
things about video games I'm not here
today to talk about whether a programmer
should call themselves engineers or not
I think it's a really fascinating topic
and if you want to talk you know over
beers on that with me I can talk for
hours but I would like to talk initially
a little bit more generally about
engineering and I'm happy that I'm able
to give this talk this is the first time
I'm giving this talk I'm happy I'm here
in Europe doing that because I think
there's two different engineering
cultures or traditions so engineering
sort of evolved differently in Europe
and in America and I'd like to talk a
little bit at the beginning about the
differences between these so European
and American traditions or cultures of
engineering and I think to start off
with talking about the the European
tradition I want to talk about these
these two men here the man on the left
is jean-baptiste Joseph foliate and the
man on the right is PFC multiplies and
so anyone study electrical engineering
in college any electrical engineering
majors in college now all right
so I bet that all the electrical
engineering majors in university are
very familiar with the names for DES and
the plas even though these two men
didn't actually do research specifically
in electricity and magnetism they're
contributed to mathematics and physics
but they didn't do work in electricity
magnetism directly but now I studied
Computer Engineering as an undergraduate
at my school Computer Engineering came
out of the electrical engineering
program so it was really Electrical
Engineering with some computer science
sprinkled into the program and the
reason electrical engineers know all
about 4a and the plas is because of
these two equations
the equation at the top is the Fourier
transform and the one on the bottom is
the Laplace transform now for I'm sure
almost everyone here is heard of Fourier
transforms they have a lot of
applications in the software world and
things like audio processing and image
processing so you know Fourier
transforms deal with basically sine
waves transforming a signal into
sinusoids Laplace transform was just a
generalization of that in addition to
sinusoids that handles you know rising
and falling Exponential's and the reason
that electrical engineers study Fourier
and the plas transforms is that they
provide a very powerful mechanism for
reasoning about the behavior of
electrical circuits so let's say you
take an electrical circuit you build it
out of
you know resistors capacitors inductors
and even transistors you know we're used
to in the computer world thinking about
transistors is just being little
switches that go on and off electrical
engineers also use them as basically a
little little amplifier so you can
actually run them in the middle where
they're not just acting like switches
but as filters or amplifiers and it
turns out that so in general if you have
a big circuit and you you know stick a
voltage input on one side that varies
over time and you want to figure out
what the outputs going to look like
another you know two points in the
circuit you've got to solve a series of
differential equations but if you're the
pieces that you use to build the circuit
are well behaved in a certain way and
you can actually describe the behavior
of that circuit using what's called a
transfer function which you can
calculate using Fourier transforms and
the blast transforms and once you have
the transfer function you can basically
figure out how that circuit will respond
to any possible input and so it's a very
very powerful mechanism for reasoning
about the behavior of a system based on
having a formal mathematical model of
that system and when I was a student I
love this stuff it's you know if anyone
ever asks you why am i learning so much
about polynomials or or calculus
you know you can tell them you can
actually you know reason about the
behavior circuits at someday it's
actually useful someday to deal with
polynomials and in calculus and so on
the Americans tradition I think is is
well illustrated by these two men these
are Orville and Wilbur Wright more
generally known as the Wright brothers
and you probably have heard of them
they're sort of credited with having the
first airplane that was powered that you
know first heavier-than-air flight
here's a picture of that that initial
flight this is the Wright Flyer one blue
this is in 1903 flying in Kitty Hawk
North Carolina and the east coast of the
United States this this flight lasted
about 12 seconds when thirty-six and a
half meters you know not not the very
great distance but as a very very large
accomplishment now these two men were
not educated in a university in an
engineering program they're largely
self-taught they worked in a bicycle
shop they did and they had a lot of
experiments they worked a lot with
gliders to get the designs right it's
not the say they didn't use any math at
all they did hat they used a an equation
to try to compute the lift that they
needed to help them design the wings
with a they really used a very sort of
empirical experimental approach to doing
their designs and you know traditionally
in America that's how engineers became
engineers there wasn't they didn't go
through universities they were sort of
learned in the shop floor or they
learned in the field you know you learn
through apprenticeship whereas in Europe
you know in the you know they called
play technique developed in France and
students were educated you know in
mathematics and physics and I believe in
Laplace taught at you know called play
technique and it was more you know
formal theoretical mathematical
education that engineer Scott now you
know over time the European model won
out over the American model and there
was some resistance in America the
engineers didn't think this math stuff
was really useful but you know
ultimately today all engineers in the
United States go through either a you
know polytechnic school that model got
imported into into america or they
either go to a large University that has
a you know an engineering program and
then learn a lot of mathematics and
physics so you know the European
engineering tradition won out over the
American one went over time now in
software
I think we still have this other
European the American traditions but
there hasn't been a clear winner yet and
there's still sort of tension in that
world now so out of curiosity how many
people here so I seem almost everyone
here you know programs how many people
here do not have a computer related
degree you don't have a degree in
computer science or computer engineering
or IT or software engineering okay so I
you know a non-trivial number not you
know majority but you can still work in
the field of software without being
formally trained as a software engineer
and I think the tension in the software
world between the sort of the European
and the American traditions is very well
exemplified by these two men up here I
imagine many people in the audience will
recognize the man on the left that's at
scourge extra and then on the right
probably fewer people will recognize in
my face that's air grease so Jack Straw
is you know a very very famous Dutch
computer scientist if you did take
computer science and undergraduate I'm
sure you've encountered Dijkstra's
algorithm for computing the the least
cost path through a graph that has
waited waited edges you've probably also
seen the dining philosophers problem
whenever you're reading a book on you
know multi-threading on concurrency that
jackster came up with that did a lot of
work in in algorithms and programming
languages and compilers and distributed
systems and operating systems but one of
the things he was really passionate
about was this idea of using sort of
formal methods to develop software and
he wrote this book now general the idea
behind formal methods is that you have
some kind of formal specification that
describes how the software is supposed
to behave it's really like that sort of
formal mathematical description of your
of the software that you want to build
and then you you know you implement your
code in some kind of implementation
language like Java and then you there's
this verification step where you prove
that the your code actually implements
that formal specification and you know
Dexter was a very very strong advocate
of the sort
formal approach I think he describes you
know one way to do that in this book and
you know the idea of using formal
methods to verify software still very
very popular in universities here in
Europe much more so than in America the
lots of research and formal methods and
this sort of having this this formal
mathematical description of a you know
of a software system you also see this
sort of notion of a formal specification
you know in computer science in Europe
and research here and things like in
functional programming language work
like you know Scala and you know oh
camel and Haskell a lot of that work is
done here in Europe and a lot of that
draws upon category theory from from
mathematics there's also a lot of work
done in what's called model driven
engineering where you you first create
some formal specification and you
actually generate the implementation
from that spec a lot of that that model
driven work is done here in Europe in
universities and in research now Eric
Ries as I said probably fewer people
know him he's a you know an entrepreneur
software engineer in America he was a
CTO of a company called IMVU which is
sort of like a 3d social network site I
know if anyone ever used second-life a
few years ago but IMVU is kind of
similar to that where you have an avatar
and you're you're interacting the 3d
rolled with other people like that so
Reis is not particularly well known for
fryin vu but he is very well known for
writing this book called the Lean
Startup anyone read this it doesn't
evolve read it so yet so in the Lean
Startup he he describes basically his
ideas for how you should run as a
software based startup and the idea is
that you're basically doing experiments
really you're building very small things
and then you do an experiment to see
what we know what's what works well and
then based on the result of that
experiment you decide what to do next so
if you've ever heard the term Minimum
Viable Product the idea where you
actually build you know the sort of the
absolute smallest thing you can that has
some value to a customer that was
popularized by Eric Ries or if you've
heard the term pivot where you know the
business model that your company is on
right now you know isn't going to be
profitable and you should switch to
something else
and was also you know popularized by it
by Eric Ries so you know slack is an
example of a company the pivoted where
they're actually third I was a video
game company that had you know like chat
built in and they decided to extract the
chat and make the business about that so
you see you know Jackson on one hand a
recent the other there's this tension
between the having you know reasoning
about formal systems and the idea of
sort of doing experimentation and trying
and seeing what works in terms of
engineering now I'm talking about chaos
engineering and that's very much in the
American tradition of you know
empiricism and experimentation not
necessarily having a formal model of the
system you're you're you're trying to
improve but just trying things out and
seeing what works and what doesn't so
I'm work at Netflix unless a senior
software engineer there I hope most of
you've heard of us but in case you
haven't we are a entertainment company
we stream video over the internet movies
and television we make some of our own
we produce some of our own movies and TV
shows but we license a lot most of it
from other content providers you know
you can watch us on smart TVs or on
set-top boxes to plug in the TVs like
Apple TVs on your phones and tablets and
on your laptops and desktops now you
know I'm under the team that deals with
I'm on the traffic and chaos team and we
sit under performance and reliability so
one of our main goals is to you know
achieve good reliability for Netflix we
are not a safety critical system like a
you know nuclear power plant or
something that controls a pacemaker
nobody's going to die or be physically
harmed if Netflix goes down and we're
also not like infrastructure like a
telecom company or even like a public
cloud so we don't have those kinds of
you know availability requirements but
you know on the other hand people do get
upset when that flex goes down we're not
favorite company people can't get their
entertainment they get they get sad and
they like to talk about it on on Twitter
and you know honestly if we're not
reliable enough then people are going to
stop subscribing to us and we're going
to lose customers that way so we need to
achieve some certain you know threshold
level reliability or we're going to lose
lose customers one of the interesting
challenges for us is that if you know we
depend on some third-party services for
some things and sometimes those are
single points of failure for us and when
those goes down even though it's not
Netflix code that that had a problem we
always somehow end up in the headlines
because we're so big so for example this
is two outages that happen in the past a
couple months our DNS provider ultra DNS
went down and Amazon had an outage of
DynamoDB service which affected their
auto scaling in on the East Coast and we
had a brief outage because of that so
when we go down everyone tells us about
it now so we want to achieve a high
level availability and so I really like
this quote from Jim Gray Jim Gray is a
computer scientist it was an American
who passed away a few years ago American
computer scientist he's a Turing Award
winner
he runs the Turing Award for work he did
implementing acid transactions in
relational databases and he wrote this
great paper called what white a computer
stop and what can be done about it the
time he was working for a company called
tandem computers which is now owned by
HP which is now HPE and tenon was a
company that sold very reliable fault
tolerant you know computer systems the
kind of their customers were you know
like financial companies that were doing
transaction processing or telecoms you
know the stuff that couldn't couldn't
you know go down and this quote that I
really like is you know a way to improve
availability is to install proven
hardware and software and then leave it
alone right so you know the hardware
works you know the software works you
put it out there and you don't touch it
anymore now for Netflix this is not an
option we can't use this approach on the
hardware side we run entirely on
Amazon's AWS cloud but that was a
business decision we made we did not
want to maintain data centers that was
not what we do so we outsource that to
Amazon but Amazon doesn't use the
highest quality you know enterprise
grade Hardware right none of the cloud
public cloud providers do that they're
they're competing on cost so they use
commodity hardware and it's up to the
you know software developers to you know
engineer
redundancy at the software level because
you're not gonna have another hardware
level and we run like thousands and
thousands of virtual machines and you
know disks fail quite often and so
there's always some you know machine
that's going to go down so so we can't
do proven hardware and we made it
business dude and we're not gonna spend
the money on that and on the software
side you know I I can complain about
Amazon but really most of the outages at
Netflix are self-inflicted and the
reason is that we are very very
frequently making changes to our system
you know to stay competitive we have to
constantly be doing new development and
we do I think like thousands of changes
to production every day that could be
like pushing new code or changing
runtime configuration parameters that
are picked up right away and change the
system and that that causes outages so I
really like this couple plots this is a
histogram that shows the distribution of
Netflix outages by days of the week and
you can see that there's a big drop-off
as you get to the weekend and the reason
is is that because you know most people
don't work on the weekends they push
code during the week there's another
plot that shows the distribution of
outages by hour of the day and you can
see this big you know ramped up at 9
o'clock
where people generally start to work and
then it it tails off at the end of the
day if you're wondering why it is
there's a like a dip at 11 o'clock so
Netflix like many Silicon Valley
companies they pay for lunch for us but
we don't have a full service kitchen in
the cafeteria so we get catered lunch so
it comes in and it comes in at 11
o'clock all right that's one lunch comes
that's what everyone goes down to lunch
and so that's my I haven't tested this
but I'm pretty sure that's why you see a
drop off in an outages at lunch time now
you know it's a decision another
business decision of the company is that
we're not going to slow down the
engineers to improve availability we
have to figure out ways of making the
system more available without saying you
know you can't push as often as you want
and so that's one of the goals of my
tío and their traffic and chaos team so
I should be talking right well because
they're being taken here's a
visualization of a piece of a Netflix
architecture this is a screenshot of a
tool called flux that was developed
mostly by one of my teammates Justin
Reynolds which shows a visualization of
basically traffic patterns inside of our
architecture how many calls are being
made between different services so
Netflix has what people call now a micro
service architecture we have a whole
bunch of different services that run on
on the Amazon Cloud so all of our
services run inside of Amazon the actual
you know video files themselves are
served from the CDN basically a big
geographically distributed cache that's
mostly at ISPs
but all the actual software services run
inside of Amazon or we call that our
control plane and so here you can see a
visualization of the different software
services each of the services is
potentially operated by a totally
separate engineering team we don't have
dedicated operations people so the
engineers are all responsible for
managing their own operating their own
services we do on-call rotations so
engineers are on-call for those for
their services if they're in the
critical path and one of the things that
was really interesting to me getting to
Netflix I've only been there for five
months was that it's not possible to do
an end-to-end integration test at
Netflix we have a test environment you
can launch your services in the test
environment but you can't bring up the
replica of the production environment
and test and just run tests against in
fact you know we don't even know what
the architecture looks like in general
right that's what we need tools like
this that introspect and do like
sampling of requests to see what the
architecture looks like right now but
there's no one person that actually even
knows what the Netflix architecture
looks like so we can't even do sort of
the traditional you know integration
testing that that you know you may have
learned about or that you can do if
you're you know working with a smaller
system that didn't evolve the way ours
did so we have to have other ways of
ensuring that the entire system remains
available right the engineers can test
their individual services but we need to
know that the whole thing is working
properly so one of the
approaches that Netflix truck was to
build this tool called chaos monkey this
was built several years ago way before I
was there and what chaos monkey does is
it basically tests to make sure that
this that all the services can handle
virtual machine instances failing by
going and randomly terminating virtual
machine instances running on services in
production right so it runs it runs only
during the week and only during work
between like 9:00 and 3:00 but it's on
four and you can opt out if you really
want to but it's on by default and most
people have it on and the reasoning is
that the engineers are responsible for
architecting redundancy into their
services so they have to have multiple
instances running and they need to be
able to implement you know check that
their downstream services they can fall
back and you know do timeouts and stuff
like that and the way to keep them
honest is to basically constantly
terminate instances and make sure that
the system can handle it and then if
there's a problem you know what happens
during the day and so it's it's easier
for people to fix than it would be if
you know that Amazon you know happens to
terminate an instance at 2:00 in the
morning because the you know one of the
discs failed and you know Casa mochi is
really sort of an instantiation of this
this concept that I think was really
well described in this book continuous
delivery so this is a very very popular
book in the in the DevOps world written
by Jeff humble and Dave Farley I believe
it is and what they said is basically if
something is difficult to do then you
should do it more and more often and and
the reasoning is that if you do this
more and more often then the engineers
are going to do what they have to do to
reduce the pain and for software that's
automation right so if you have you know
if you're not doing continuous delivery
if you have like a manual like you know
nine step wiki page that describes how
to deploy your app and you force the
engineers to keep doing it over and over
and over again eventually they're gonna
automate that darn thing so they don't
have to you know manually do that and
and we're going to typos and stuff like
that so you know chaos monkey is great
for you know dealing with problems that
occur to a single virtual machine
instance but sometimes the the failure
domain is larger than that
now we run on Amazon and Amazon if
you're not familiar with the way it's
structured the Amazons cloud they run in
multiple geographic regions and Netflix
services run basically in three of those
regions on the US East Coast the data
centers are in Northern Virginia and one
of the u.s. West Coast data centers
which was in Oregon and in Western
Europe and in Ireland so our services
run in those three regions and those
regions basically run independently
except for the database layer which has
to be replicated and Netflix the clients
of the Netflix services basically are
routed to whichever region is closest to
them so you know reduce the latency now
sometimes we have an outage in a region
it could be because of Amazon sometimes
Amazon has a region what it's not it's
not very common but sometimes something
will go wrong in an Amazon region or
sometimes you know will do some bad code
push and we make sure we don't push to
everything at once we push to one region
at a time and if there's something goes
wrong and we can't roll back for
whatever reason then we want to be able
to handle that situation and so what we
can do in Netflix is we can redirect the
traffic so if there's a problem with say
Europe then we can redirect those
requests from Europe to the East Coast
at least we think we can right we have
the mechanism for doing that but we need
to make sure all the time that that
actually works because it might have
been some change that we don't know then
broken that so we run what I call these
Chaos Kong exercises and that's why that
that that's Kong over there so once a
month we will shift all of the traffic
outside it will evacuate one of the
regions v all the traffic to one or two
of the other regions and make sure
everything's working properly and then
we'll hold it there for about a day or
so and then we'll shift it back and so
we have to do these drill we haven't
fully automated this but we have to do
this once a month to make sure that
everything is still working properly
that we can that we can actually recover
for that so chaos monkey and chaos
Connor are two of the ways were you know
trying to ensure where we can achieve
high availability we also have other
approaches we have like a failure
injection testing framework or you can
do things like cause a request from one
service to another to selectively fail
maybe for some percentage of users or if
particular ones or we can add latency or
things like that so we can run these
kinds of experiments to make sure that
fallbacks work correctly if the cache
goes away then the database
handle things stuff like that and what
we've been trying to do more recently is
kind of systemize our approach and we're
calling that chaos engineering and the
truth is that you know Netflix is not
the only company doing this sorts of
things we're just the ones that like to
talk about it the most and the reasons
we like to talk about that most is
because that we see the sort of public
outreach stuff that we do these talks
and the open source stuff as a mechanism
for recruiting we want people to say hey
Netflix is a cool company I want to work
there and that's why well it's why you
know we have a lot of incentive to tell
people what we're doing but you know we
talked to other large tech companies and
they're doing similar kinds of things
that what we'd like to do is build a
community around this approach and we're
calling a chaos engineering and so in a
nutshell you know we boil it down to one
sentence this is what chaos engineering
is it's really about doing experiments
and it's doing experiments on a
production system and and the motivation
is we want to have confidence that
system is working properly and we want
to base it of confidence that it can
take whatever it is that you know the
world might throw at it right so we
throw things at the system and we do
experiments and make sure you know that
it behaves the way we expect it to and
if it does great you know we we believe
today that it's still working properly
and if it doesn't then we need to figure
out what went wrong and and go and fix
it so we have a set of principles of
chaos engineering these are not you know
laws but these are sort of guidelines
for designing the kinds of experiments
that you can do to improve the
availability of a system like ours and
to talk about the first principle I need
to first talk about this graph this is
the graph that's called we call it SPS
so lots of Engineers look at this graph
at Netflix and basically every time a
Netflix customer hits the play button
and successfully is able to stream a
video you can think of us Okin scepters
incrementing a counter and you reset
that each second so this tells us how
many streams successfully start every
second and we look at this plot because
this tells us that basically if Netflix
is working or not
because the people could stream video
solution video then it's working enough
right there may be some
failures in some places maybe the you
know the recommendation system is not
working perfectly or something but
people are able to watch video and if
that goes to zero then we have a huge
problem but if it's not we think it's
generally okay and you can see this is a
prod of SPS that's plotted over several
days and you can see they're sort of a
natural rise and fall right
it sort of bottoms out somewhere around
you know 3:00 a.m. and and tops out
around around 6:00 p.m. so there's a
there's a natural variation that we
expect because people watch you know
more more videos in the you know in the
evening than they would you know the
middle of the day and you'll also see
there's sort of a black and red line
that are superimposed on each other and
what we do when we look at these graphs
is that we you know we plot it for this
week and then we we plot last weeks
superimposed on top of it and the reason
is that we don't see much variation week
to week in this graph and so we can
quickly see if the today's plot is
deviating from last week's then that
means probably there's an issue that
there's something we need to be aware of
now I think most companies have some
kind of dashboard like this or if you
don't you really should have something
like this that tells you like the
current health of your system based on
some you know business indicator not
necessarily like you know CPU usage or
memory or something like that but
something that's meaningful and it
really describes how the system is
behaving over time and this is the thing
that we look at when we design our
experiments we ask you know what's the
effect going to be on SBS and hopefully
it's usually zero and so that brings us
to the first of the principles which is
when you're when you're building these
experiments you're starting with a
hypothesis right you think well what it
you know what's going to happen what do
I think is going to happen when I inject
something into the system and generally
the hypothesis should be around the
steady state behavior of the system you
should have some characterization of how
your system behaves and you should have
some idea when you inject the thing that
you're injecting what's going to happen
now you know if you think of chaos
monkey is an experiment our hypothesis
is that SPS is not going to be effective
chaos Kong as well the idea isn't when
we shift traffic or terminate one
instance we're not going to see any drop
in in the number of people are able to
stream videos and if we do there's a
problem now you may have some
experiments where you do see some change
so you know if we design an experiment
where we want to degrade the Netflix
service for a subset of customers just
to see if it'll actually still work in a
degraded mode then we may expect I don't
know like a 20% reduction or something
like that and so it's not always the
hypothesis is that there's no effect but
generally speaking you know most of our
experiments are like that that we think
that what we do or system can handle it
and the customer won't be directly
impacted all right for the second one
this is the happy path is anyone
familiar with the happy path all right
many people so the happy path is that is
that trace path for your your code where
nothing bad happens right where all of
your input that comes from the outside
world is well-formed and all of the
responses from third-party services is
ok no errors none of your API calls fail
there's no exceptions ever thrown right
when you're first writing your code and
writing your tests you're usually
testing the happy path first right
because that's the functionality of your
code but the problem is that generally
when bad things happen in production
it's because of things that did happen
off the happy path and it's generally
because we usually test the happy path
first right that's what we do is that's
just our sort of instinct as software
developers we don't think about the
other things that can happen but and so
that brings us the second one but that
bad things happen in the world right
unexpected things happen hard disks fill
up the run out of memory
garbage collection happens in the
inconvenient times packets are lost or
you know responses fail sometimes like
there's a huge latency in a response if
you do post mortems on your outages I'm
sure you have like a very rich source of
history about the kinds of bad things
that can happen to your system that can
cause outages and they're not all
necessarily you know a transient
failures sometimes it can be like an
unexpected spike in traffic your system
might not be able to handle but the it
is when you're designing these kinds of
chaos experiments what you want to think
of is you know what could the world
possibly throw at my system
and use those as sort of the stimulus is
the treatment in your experiments to
when you're designing them so for the
third one so recently I've been reading
this book called engineering progress
through trouble so this is a book that
was written in the 70s and it's actually
a collection of case studies from
mechanical engineering about basically
design challenges that engineers had
because of some kind of failure
somewhere and one of the the interesting
chapters is on ocean engineering and it
turns out that the ocean is a really
harsh environment for for building
systems in right like this sea water is
really really salty and that is really
bad for certain metals and in one of the
case studies they talked about this
patrol boat that had a propeller shaft
that was made of high-strength steel and
the problem was that the high-strength
steel was being corroded by the seawater
and then the propeller shaft would fail
and so one of the things that the
engineers tried to do to compensate for
this was to use nickel plating on the
propeller shaft because apparently
nickel doesn't doesn't react to seawater
the same way that steel does and
presumably that worked you know fine in
the lab they nickel plated the propeller
shafts they they put these you know
patrol boats out to sea and what
happened was when that when the boats
were moored barnacles would attach to
the propeller shaft and the chemistry of
the barnacles was such that they would
cause these crevices to appear on the
nickel so there's something about
barnacles it sort of eats away at nickel
and so there was little crevices little
holes on the nickel plating and the
seawater basically got in those crevices
and corroded the steel and caused that
that propeller shaft to fail again so
that was sort of a a failure that that
approach didn't work and that brings us
to the third principle which is that
there's always those darn barnacles in
production right there's always these
things that that you're not going to be
able to predict when you're running in
your test environment that are going to
happen in production you may be able to
get your test environment very very
close and the closer you can the better
but if you can the the the best thing to
do is to actually run these in in you
know directly in production and I know
that can sound scary but if you think
your system is designed can handle the
things you're throwing at it then you
really have to test it and I can tell
you that like eventually people will
adapt to this right and you can actually
you know you can design your systems to
reduce the failure domains in production
you can use things like Canaries and you
know when you run your experiments you
can restrict the amount of people that
you know of requests that are going to
be involved in the experiment right so
if you have good enough infrastructure
then you can have some confidence that
you know you can do these kinds of
experiments you should be able to roll
back quickly but all these things that
I'm saying you should be able to do
anyways right to build a lot of the
available systems so I know not
everyone's gonna be able to under
production but the closer you can get to
this the better and the last principle
is really about automation the whole
idea of bringing the pain forward and
the the motivation for this is that
because you know these kinds of systems
change so much over time the more time
that passes between when you run an
experiment and and now the less
confidence you can have the results
still hold right you've got to keep
doing them over and over again to make
sure your system can still withstand
that that's why we run cast monkey all
the time that's why we run our cast Kong
drills all the time because you know any
change can can break things and so if
you can't automate them that's really
really great
so just to recap on on what these
principles are and the first one is the
idea of having some notion of the
steady-state behavior system and
hypothesizing how your experiment is
going to affect the steady-state the
second one is about how we know what
kinds of stimulus what kinds of
treatments that you apply to the system
you want to you want to throw things at
the system that you know your system
might actually encounter so real-world
kinds of things the third one is to
actually you know run your experiments
right on your production system rather
than just running them in tests and the
fourth one is to automate when you can
so once you've got your spend mint up
and running
just run it continuously so you can have
confidence that your systems still going
to be available
so as I said earlier we're trying to
build a you know community of practice
around this so you can go to principles
of chaos calm you can see some more
details about about these principles if
you want to hear what kind of stuff
we're doing at Netflix you know with
chaos with in our open source stuff or
any of the other things you can go to
our tech blog if you are interested in
this stuff in chaos as I said we're
trying to build a community send us an
email chaos and Netflix calm the goal is
to you know eventually have things like
meetups and conferences where we can
exchange information about you know what
works how do you get things adopted in
your organization you know what kind of
tooling is available things like that so
I apparently speak really quickly so
we're quite early so I've got time for
questions if anyone has any any
questions to ask me
so I don't quite understand so you said
we we still have one point of failure
there well so so he asked is that you
know even what the system where certain
services can fail you still have single
points of failure and in example he gave
lessly it was the the log in service so
there's it's a couple of things so we do
certainly have a set of services that
are critical services that if it's not
that in fact I think we call them my
tears now there's certain services where
if all those instances went down then
the system wouldn't you know people
would not be able to log it the stream
video like the the SPS would go to zero
if some services were out and they can
withstand individual instance
terminations but there are some services
that are that are critical those
services themselves have to have
redundancy so we run multiple you know
they sit behind the load balancer and we
can shift between regions but there are
still some services you know how about
login typically users don't log in that
much so you know they log in and then
that token lasts for a while but new
users wouldn't be able to log in if they
had lines so if the log in service went
down and there are there are so there
are critical services we can't get
around that but we can build redundancy
so they're in different regions and you
know within the service there's
different virtual machines that are that
are running that service
yeah so uh it brings up the issue the
failures are not independent and that
they can be correlated and that's very
true and I think you know many outages
and complex systems are because are
because of coincident failures and
usually some kind of soft failure - it's
not like it goes off or not but it
operates in some sort of degraded mode I
would say I would love for us to be
better at doing that kind of failure
modeling to figure out how do we you
know simulate those kinds of failures
where you have correlated issues I'm
sort of that's an open problem
I believe that's true I I think we need
to get better at at injecting those
kinds of correlated failures when we do
our testing all right there's no more
questions that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>