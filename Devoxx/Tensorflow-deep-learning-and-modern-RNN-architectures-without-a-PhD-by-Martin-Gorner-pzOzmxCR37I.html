<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tensorflow, deep learning and modern RNN architectures, without a PhD by Martin Gorner | Coder Coacher - Coaching Coders</title><meta content="Tensorflow, deep learning and modern RNN architectures, without a PhD by Martin Gorner - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tensorflow, deep learning and modern RNN architectures, without a PhD by Martin Gorner</b></h2><h5 class="post__date">2017-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pzOzmxCR37I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome back and so it looks like nobody
left that's good so for the second hour
we will be talking about recurrent
neural networks which are which are a
completely different architecture and
please welcome Nitin Payne who works at
Google jigsaw and in a minute he will
tell you what he does you will see it's
interesting but first let's dive into
these different architectures called
are n ends and first of all I don't know
if you have been on Google Translate
lately you know Google Translate has
been around for ages and it has this
reputation for being you know what is
used to produce the the the badly
translated manual in your Chinese
electronics but really try it again try
it again
let me show you this
devoxx is a conference organized by
computer enthusiasts and this is
actually 100% spot on even translated
past unit on automatic which is how you
say this in French into a completely
different expression which is computer
enthusiasts which is how you say this
very specific thing in English and this
is down to of course more data and and
more work and so on but mostly these
advances come from better neural network
architectures and what I what we want to
do today with you is go through a couple
of neural network of recurrent neural
network architectures which are the
architectures used in Google Translate
or other language problems and show you
the the advances that have been made and
mostly show you what is called an
attention mechanism but first what is
the recurrent neural network cell so by
now you should recognize here on your
network we have inputs we have the layer
of neurons those neurons still do the
same thing they do weighted sum of all
inputs add the bias feed this through an
activation function and at the end I
have my soft max if I have to if I want
to classify something now the
specificity of recurrent neural networks
is that we do this this middle layer of
course it has outputs that will go down
to the next layer but the same outputs I
also take them out and re concatenate
them with my inputs in the next time
step so this is something that we'll be
working in time step I have one input at
time T I feed it through the neural
network gives me some output and this
end gives it gives me some output and it
gives me and the intermediate layer also
gives me an output which
use as a state for as a state vector for
the next time step and the way I in a
given time step I combine my input
vector and my state vector from the
previous time step is that I simply
concatenate them before running them
through my neural network so this can be
represented schematically as a cell with
one input one output one input state and
one output state which usually is passed
around and also traditionally the the
softmax layer is represented outside of
the cell by this yellow circle here okay
so the green box only represent those
green neurons here so it's a state
machine it has it has a state that is
associated to it and it is going to be
very good at processing sequences of
things like sequences of characters or
sequences of words how do you train it
well as usual you take your inputs you
feed your inputs into your neural
network your neural network uses weights
and biases to produce something you say
no no no no that's the wrong answer you
compute the difference between what it
said and the correct answer you Retro
propagate change your weights and biases
and and yeah that works if your weights
and biases about the problem but here we
have this additional input state what if
you are producing a bad answer because
your input state was in some ways bad
well you're stuck so to get unstuck the
solution is to replicate this cell
multiple times it's the same cell with
the same weights and biases replicated
and you will still be stuck with the
first input here but if you say for
instance that h2 has some dodgy values
in it you have all the weights in this
cell to work with to produce an h2 that
has better values so unrolled you apply
a sequence of inputs
you go through the network produces a
sequence of outputs and then you you
force a sequence of correct answers to
that retro propagate and that's how you
train the system but you will always
have this limitation that you need to
unroll those recurrent cells in a
sequence and that sequence will still
have a limited size and the first and in
that size will be a limit to your
training
anything you want to teach the system
will have to be taught on examples of
that limited size and that's something
that that is inherent in this
architecture okay what if you want to go
deep and have multiple layers well
actually if you stack two cells like
this still a same API you can say this
is one cell it has still an input and
output an input state or output state
those states are a bit bigger but it's
still the same API so that's a very easy
way of stacking multiple cells to go
deep and to learn through the system you
unroll it exactly in the same way to
cell itself
one thing that people noticed is that
this tends to be a very deep neural
network you enter data here and it has
to cross this neural network and this
one and this one and this one and this
one and this one until it reaches this
value and produces something so
recurrent neural networks and of course
you want to unroll as much as possible
because you have this limitation that
you can only teach it tricks on on
training sequences of that maximum
length and the problem people were
facing with those recurrent neural
networks which end up being always very
deep was that they would not converge
they solved it and here you will have to
watch my talk from last year they solved
it by inventing different recurrent cell
architectures like this oh that's scary
let's hide it but the only thing you
have to know is that those new new
recurrent cell architectures
solve this problem of lack of
convergence and and since there are many
variants you might ask yourself which
which variant is the one that I should
be using for that let's ask the
specialist what do you recommend well
there are lots of variants but as long
as they have something called a forget
gate they usually perform equally well
and so grew cells which are the ones in
the middle in your diagram there are the
simplest cells that have this property
and they get the job done so that's what
you use we use grew cells always in
general are still being used to it
ok in first approximation whenever you
build an Oren end-user grew cell and
don't worry too much about this all
right let's build something a quick
let's say a language model this is what
what I shared with you last year let
let's do it again very quickly so we
have our own end sequence and we train
it on a sequence of characters we train
it to to generate the same sequence
shifted by one character so in effect we
are training it to predict what the next
character is how do we put characters
into our vectors well if we use one hot
encoding which is the most basic
encoding you can think of for an
alphabet of size 100 you said you take a
vector of size 100 you fill it with
zeros and you mark with 1 the index of
the letter you're encoding and that's
all what this was doing so last year we
had a bit of fun
and we generate we trained it on on
Shakespeare and generated new
Shakespeare plays let me run this again
for you here we have Shakespeare Company
of Basterds how now my lord why should I
say there is no more than the manner of
my son which I have said it was most
noted I would I weep yourselves don't
John lalala so it's it's a hallucinating
Shakespeare it's called a language model
so that's one thing you can do but now
Nitin will share the real extent of his
work with you what are you doing that's
right so this isn't the new example that
we'll focus on for the next little while
so I think so I work on a product called
perspective which is building a set of
tools through an API that can be used to
improve online conversation and today
we're going to focus on one of those
tools which is to try and build
something that can detect toxic comments
so if you spent any time online in
comments sections or on social websites
you've probably run into some kinds of
toxic comments things which are rude or
disrespectful or make you just want to
close the browser and maybe leave the
conversation and we choose this example
because it gives us an excuse to fill
slides within with a profanity and still
be okay yeah
and so today what we're gonna try and do
is build a neural network that's able to
detect these kinds of comments all right
let's go and so the the framework we're
gonna use is the embed encode attend for
Digg framework this was introduced by
Mathew Hana bol and basically it's just
a way of encapsulating some of the most
common techniques in natural language
deep learning it's into useable blocks
blocks they play well with each other
so embed will be how do you transform
words into vectors that's right in order
to do deep learning you need to
transform words into numbers that you
can do computations on encode you will
run them through some kind of neural
network that will do something yeah
something basically the original numbers
you get from embedding might not be the
most useful one so encoding turns them
into more useful numbers and that's a
neural network and that's a neural
network that does that and then we will
talk to you about this new idea of an
attention mechanism yeah not all of the
numbers from your second step are going
to be equally useful so attention lets
you focus down on the numbers that are
the most useful and finally the network
will predict whatever it was built to
predict so first word embeddings you
could also encode words using one hood
encoding if you have a vocabulary of a
vocabulary of 100,000 words it is a
possibility to 202 to create a vector of
a 100,000 zeroes and mark with one the
word you are encoding it's highly
unpredictable it sorry
really big vector so rather than doing
that we tend to represent words with
shorter vectors which can have
continuous values in in their values
it's called an embedding and the easiest
way of embedding words is simply to take
a big matrix and look up for the index
of the given word what are the these
values in this big matrix initially this
matrix you will sorry
you will initialize it with a random
noise and those will be your n bearings
here you see the tensorflow code for
embeddings so you we created this
embedding matrix as a variable that in
tensorflow has a very specific meaning
and it means something that will be
trained and we use it using the
embedding lookup function we give it a
word index and it gives us this vector
of M bearings for this word now what we
can do with it is foreign for example
build a word by word language model in
the same way as we build a character by
character language model previously we
can embed words into vectors feed them
through the exact same neural network we
had previously and for example train it
to predict the next word in the sentence
and when you do that you realize that
not only there are Network trains but
you start seeing in the in the embedding
switch will be generated you start
seeing very interesting properties so
interesting in fact that you will be you
will probably be able to reuse this
embedding matrix that you generated with
this problem for other problems as long
as it as they involve the same language
so what kind of properties are we seeing
there yeah so as martin says anytime you
do a deep learning exercise with natural
language you're going to generate some
embeddings and these embeddings
be useful in other problems so some very
useful
embeddings that were created were
something like were Tyvek by Google or
glove by Stanford so they're they
constructed specific problems with the
idea of creating embeddings for the
words which were maximally useful across
a wide range of problems and these
metin's do have nice properties like
martin said so for example words which
are similar to one another like france
and italy will appear close to each
other in the embedding space and that
means that if your network learned
something about france like how it can
be used in a sentence
it'll for free get to know the same or
similar things about italy or words like
running and swimming will appear close
to each other and so you when you solve
one of these problems you actually have
three options you can start with
completely random vector of embeddings
or matrix of embeddings and train them
yourself you can use one of these pre
train embeddings and just leave them
fixed but that will work only if there
is some similarity between the problem
you were solving and and and how these
embeddings were generate that's right if
people are using English in a completely
different way than these embeddings
might not be very useful okay so the
rule of thumb is if the omitting were
embeddings were generated for the
english language and you have a problem
involving the english language you're
good you can usually yeah you can
usually benefit from using these
embeddings that were trained on english
and there are betting's for other
languages as well so you can go out and
find those and the third approach you
have is to do something like a hybrid
approach where you start with some set
of pre trained embeddings and then let
your model tweak them and train them as
you go along and that's the approach
we're going to be taking in our in our
demo and there is a setting in
tensorflow when you define your when you
load your embeddings to tell tensor flow
that this is not a constant this is
something that is initialized but should
be still be trained during the training
so that's how you do it that's exactly
right it's just a flag all right so how
do you do classification with an Oran n
which is what we want to do here a very
typical example is to classify articles
news articles into geopolitics economies
sport and another example is to classify
comments into toxic and non toxic so you
take your unrolled sequence of Orleans
apply a sentence on the inputs now we
know how we encode each word in this
sentence into a vector and then simply
take the last output coming from the
last cell run that through a softmax
layer and produce a probability
distribution across our classes
geopolitics economy sport and predict
the class of the sentence now there is a
better way with bi-directional networks
right that's right how does that work so
when you're using regular are n ends you
only learn the dependencies of one word
of each word in one direction each word
can learn from the words before it but
not the words after it and sometimes in
your use cases it's useful to have the
whole context in both directions of a
word so there's a simple trick for doing
this you just use two are n ends one
that goes forward and one that goes
backwards and these two are n ends will
now give you context a context for each
word will give you a context in both
directions and so you obtain two output
vectors and what do you do with them
yeah because now you have two vectors
and previously you had one you can just
glue these two vectors together by
concatenation and you'll get have one
vector again and you can use this for
prediction all right so you know we said
we will take this last output from the
last cell of this unrolled sequence all
the other cells in the sequence they
also have outputs is it okay to throw
them out yeah so that's a good question
so like you said when we do a prediction
we you know if you if you stopped before
attention you still have a classifier
and it should still work but it's only
using the output from this last cell and
you're feeding that into your predictor
but of course there's still information
in these other cells and maybe for long
sentences especially this information
might get lost by the time it gets to
the last cell so you'd like to somehow
keep this information okay so you know
you can take this information and you
know you now have the outputs of all the
cells you can do something simple like
just take the straight average this of
these vectors and feed that to your
predictor but it's also true that not
all of this information will be equally
important so you might want to use a
weighted average so you want some
weights that
you which words are the most important
and which words are less important and
the question is how are you going to
come up with these weights you could try
and learn them from scratch but that's
not so good
you probably want some information about
each of the words in computing these
weights you the word though is probably
not very useful so the model sees the
word though it should maybe ignore it
and so you train a little neural network
a very very small neural network from
the output of the cell whose whole job
it is is to vote on how important that
word is so this little neuron network
gets its vote it gives you a little
alpha here which tells you how important
the cell is then you do the weighted sum
and then you feed that into your
predictor and that's how you do
attention attention is just how much how
big these weights are that's how much
attention you're paying to each of the
words and in this structure this meaning
your network again there is only one and
is replicated that's right just like
these cells are replicated from time
step design step this mini neural
network is replicated so it's lovely you
can do the unrolling trick all right so
we have all we need now Oh No maybe you
want to normalize your venture yeah so
that's the last important trick and it's
just a technical trick these office when
they come out of the neural network can
be anything they can be negative they
can be positive they can be big they can
be small so again we're gonna use
softmax and this will just turn them
into numbers between 0 &amp;amp; 1 that add up
to 1 so that'll make for a nice weighted
average so are we ready to build a
toxicity detector yeah so now that we
have these four steps we can build an
end-to-end toxicity detector just using
them so let's just review we start with
a sentence we then do the embedding so
we turn this this these words into a
vector of numbers here we're gonna use
glove vectors that stanford published
then we're gonna do the encode step so
here we're going to feed these embedded
vectors into a bi-directional RNN and
again you're going to get two vectors
that's the output of each of of each
cell of the RNN and so you can catenate
them together to get one you feed this
into your little attention network in
order to get votes for how important
these are then you do a weighted average
and feed it to your predictor and you
get a decision as to whether or not
something is toxic and that's it and
that's it Wow no maybe not
the one question I always have what I
see these diagrams is my sentence here
is exactly six words and I see an
unrolled sequence of exactly six cells
but in in real life that's it
not as easy your sentences have
different lengths so usually the way
this works is that you have to use some
padding but I want to share with you
this function dynamic RNN that was quite
recently added to tensor flow and which
is very useful because this phone with
this function you don't have this
function is what does the unrolling of
your RNN sequence so you give it one
cell and it produces a sequence of that
cell unrolled across a certain number of
steps but it's very very practical
because if you are processing a whole
batch of sentences of course none of
them are at the same length using
dynamic RNN you can give it the batch of
sentences give it another vector which
contains the length of all those
sentences and in the batch it will
automatically unroll the sequence
exactly the correct number of steps for
each sentence and why that is important
it's because when you have your your
sequence the output stage that was
generated at the end of the sentence is
actually very important and here dynamic
RNN will give you in the outputs the
output state that was generated exactly
at the end of your sentence and not at
the end of your sentence plus some empty
empty cells running in the void so now
we are ready to build this toxicity
that's right so now we're going to take
the diagram that we saw earlier and turn
it into code so the first step in the
diagram is to do the embedding and we've
already seen this function and bed and
lookup so this just takes two variables
one is the the words that we're
interested in in embedding and the other
is a matrix and this matrix you can just
load from a CSV file so you can go to
stance first and for its website
download there glove of embeddings and
load that CSV file into a variable in
tensorflow
and then use the embedding lookup
function the next step is the encode
step so here we want to pass these
vectors these embedded vectors through a
bidirectional RNN and like we said
before you have to tell the iron and
what types of cells you're going to use
and so here we're going to use grew
cells
here you notice that we're defining the
GRU cells twice and that's important you
want to define one forward cell and one
backward cell because you don't want
them to share parameters you want them
to learn different different things and
so once you've defined this two cells
it's a matter of just tossing them into
this bi-directional dynamic RNN function
which is the same as the dynamic current
and previously but for bi-directional
yeah it's basically exactly the same and
there you just tell it which two cells
to use and what you're feeding in which
is your word vectors that were embedded
previously and then like like we said
this produces two vectors for each word
so we're going to use the concatenation
to combine them and then you're just
gonna use attention now here we're
cheating a little bit because I wrote it
if I wrote a function to compute these
attention weights but you'll see that
it's not a very complicated function in
fact we're going to show it to you right
after this slide and then the final step
the predict step is of again just one
usually a very small dense layer that
has output to because we have two
classes toxic or not toxic and then the
prediction is just which of these two
classes is bigger well which of these
two numbers is bigger which is what the
R max does and then to compute the loss
and train the network we're gonna use
the softmax cross entropy that we've
always been using so R max we haven't
seen it yet it's a function that in a
vector tells you the index of the
biggest element exactly right yeah and
so that's the embed encode attend
predict framework this is basically all
the code that you need to train the
model function of your neural network
and the only missing piece was this my
attention function that I had to split
out into another slide but you'll see
that it's a very simple function again
it's just too dense layers back-to-back
and then a little bit of reshaping and
softmax so basically a two layer neural
network density yeah a very small two
layer neural network which whose whole
job it is is to compute these votes for
how important the hit the the outputs of
the cells are so you're telling us that
in your job you get paid for producing
ten ten lines of code
absolutely yes and so yeah now it's time
for a demo yeah let's go so we trained
toxicity classifier in a jupiter
notebook for just a few minutes and we
were using publicly available data on
fixture and the link to this data will
be
the in the slide notes so we start with
a very nice a few idiots yeah we
this we've this is a little function
that tells you whether or not the
computer thinks that the sentence is
toxic or the model thinks the sentence
is toxic and the another nice thing
about attention is it gives you some
kind of interpret ability because it
tells you what the model was paying
attention to so for this sentence the
model thinks that the sentence off
you idiot is toxic and it's paying
attention to the words an idiot
great let's try something else
thank you for your help
editing this this should not be toxic
yeah some of these are real-world
examples the model again correctly
determines that this is not toxic and
you can see that the attention is sort
of spread out throughout the sentence
it's not paying attention to anything
that much in particular all right let's
let's try something without an obvious
curse in it yeah so by now maybe you're
thinking that this is just a bad words
detector so we'll try something that's
not quite so obviously a bad word I'm
going to shoot you so this is more of a
threat and again the model thinks it's
toxic and pays attention to the word
shoot it's noticing the word shoot is
maybe being used as a threat here so
maybe it's just detecting shoot as a
curse word yeah perhaps it is let's
let's let's try another word another
sentence with the word shoot oh shoot
well alright okay and he's not tux yeah
it's still paying attention to the word
shoot but here it doesn't think it's
toxic anymore
and now we'll try another another word
another sentence where there are only
positive words except that they're kind
of negated you learn notes that's smart
are you so here there's nothing
obviously negative about it except if
you understand that not negates the
sentence and here it's able to detect
that this is toxic without really
knowing without any finding any
particular bad word and if I try you are
very smart yeah and you see as you come
in you see that it's paying a lot of
attention to the word your so we'll try
another sentence with your and smart and
see how it does and here it's able to
say that it's not toxic and it's pays
attention to your but it's not toxic
yeah
so in about two slides worth of code we
were able to build a detector that does
a pretty good job of distinguishing
these things and that uses the attention
mechanism that is a big part of why it
works so well that's right
so let's continue now we will I start it
with Google Translate will you believe
me if I tell you that Google Translate
is not six lines of code but ten let's
see let's see if we can build it so this
is how you would build a translation
neural network you have two recurrent
neural network which you tag back to
back one is called an encoder and the
other one is called a decoder and you
will feed an English sentence to the
first one then feed the output state of
this one into the second one and the
second one will generate a French
sentence there is always a lingering
question is which is what do you put on
the inputs of this second neural network
and we'll get to that in a minute it's a
bit complicated but during training is
actually very simple what is supposed to
be happening a little bit like in the
language model we saw previously each
one of those cells in the decoder is
supposed to do to produce a word and to
produce an output state which is fed
into the next cell and you are supposed
to feed also the word that was produced
before as the input into the next cell
at least that's how you train it but
people have noticed that instead of
actually going through this softmax
layer and computing exactly what the
word is it works even better if whatever
your half-trained bad network predicts
what you put on the inputs here is the
correct answer is immediately the
correct answer so the correct answer
during training will appear in two
places once here in on the inputs and a
second time in the loss function so it
will be used in the difference between
what the network predicted and what you
know to be the true answer which is its
normal place isn't it isn't it a problem
to are to be predicting you know
sentences like like that because if I
look at it I see this this softmax you
know a little layer and that produces a
probability across my entire vocabulary
yes so it produces vectors of 100,000
elements yeah that that's right in the
previous problem we only had to produce
predict two things now you have to
predict something massive and computing
this every time you're doing a training
step is extremely expensive so there are
shortcuts and tensorflow has a nicely
implemented shortcut called sample
softmax loss and all this does is
instead of computing the entire softmax
it computes an approximation because you
don't actually need the the output
sequence a sentence here the only thing
you need is the loss yeah you in fact
need the loss or even just the gradient
of the loss or even just the gradient so
the way they did it is that they wrote
those gradients and and then try to
optimize them to mature to to compute an
approximate gradient that still trains
and there are many different ways of
doing that it's just an optimization in
tensor flow you can you can use this
sample softmax locks for loss function
for that during training during
inference when you are actually
predicting something it's a bit more
complicated so once you have trained
this network to actually translate a
sentence you feed in into the encoder
the cat ate the mouse and you have an
output vector here then let's look at it
through code I like reading code I'm a
developer then I will feed this input
vector into my first decoder cell and
this decoder cell needs also something
on its inputs if this go token which is
like a word so I will need to embed it
which is why I use embedding lookup then
I run this through my cell dynamic RNN
can run something through one cell if
the input is
sighs one after and this gives me an
output in an output state then I
implement my softmax layer here so
that's a dense layer in a neural network
followed by a softmax activation
function and this activation function
gives me a probability across my entire
vocabulary of what this word is supposed
to be so if I have the probabilities I
can say well let's pick the most
probable word that's hard max and I have
my word next thing I feed this word into
the next cell and I feed the the state
into the next cell and continue and it
doesn't look like a very good
translation can you debug this for me oh
yeah so so this is a kind of a classical
error what you're trying to do isn't to
try and predict the next word that's the
most probable what you're trying to do
is predict the most probable sequence of
words and at each step you have a
conditional probability of what's the
most probable so in trying to go week
knowing what came before knowing what
came before okay so in trying to predict
the largest sequence given these sort of
conditional probabilities this is again
a well studied problem in math and
instead of just using the arc max you
can use something like beam search so
tensorflow kindly implements beam search
for you so that's a bad idea is a bad
idea I have to use beam search which is
available in the second sequence to
sequence API and I have a good
translation thank you
and we won't go into the details of how
beam search works exactly it's not that
it's super complicated but this is just
a super standard problem most probable
sequence from conditional probabilities
has been solved multiple times use the
function now can we add attention to
this yeah so I'll let you do this it
turns out that the previous the previous
model works works fairly well but if you
try and add attention you can get an
extremely good model that's yeah that's
part of the reason that Google Translate
is so good today so let's say you start
with the black cat ate the mouse and you
feed you know if you're doing the
decoding step loops if you for doing the
decoding step you're gonna feed the
output of this cell or the state of this
cell
into the decoder but there's a problem
here in fact there's a couple problems
one is you're taking all of the
information from this cell and you're
turning it into just one vector here
again and we already saw why this is
kind of a bad idea you're losing all the
information from these other zones and
you're feeding it into this cell and
you're basically trusting that cell to
pass it on to all the future cells so
again for very long sentences this might
not work very well all right so what can
we do about losing the information from
these cells well we've already seen the
trick let's just compute attention so
again let's just use the the outputs of
all of these cells have them vote on how
important each of them are and then do a
weighted average okay that's pretty good
but we can do even better because in
this context we actually have an
additional piece of information we have
the output of this cell here so why
don't we use the output of this cell to
also help compute the attention and so
that cell is the word that is currently
being translated yeah it's it's the
decoders attempt to it translate the
next word so we feed that as additional
information so now when these these
little blue boxes are doing the voting
they're voting with both knowing the
word that they represent and the word
that they're trying to predict you've
seen what happened here we usually we
had this blue box generating attention
from the output of the encoder cell and
we changed the blue box a little bit to
me to accommodate for bigger inputs and
we added a second input which is the
output of the decoder cell yeah we made
it a little bit better at the base yeah
like this great so now that we have this
weighted average what are we going to do
with it well it turns out we can feed it
into the new network in a couple
different places okay so yeah so maybe
the weighted average this time puts all
of the weight on this word cat here
that's pretty useful so let's feed that
to our network firstly we can take the
output of this weighted average let's
take this hidden layer put them together
with a tiny neural network like a one
layer neural network here and feed that
to the predictor so this hit this this
neural network sees the word cat oops
sees the word cat and sees whatever the
output of this cell is and it's able to
correctly predict shock and then the
other thing we can do is take this this
this attention
vector and feed it into the next cell so
the next cell gets a bit of a hint about
what we were paying attention to
previously and the next cell so you
should click now I see a problem there
you're cheating because up to now we
always had those orang n cells with one
input one output one input state one
output state how can you make one with
two inputs yeah well it turns out that
we can use our old friend concatenation
again and just make the cell a little
bit bigger so here the input from the
word sha is going to be an embedded
vector for this word and then we can
just make the input cells a little bit
bigger by concatenated also the
attention cell that we got from the last
and so the first cell if it has two
inputs what is going on the second input
oh yeah so you notice that there's no
red arrow here so but this cell has to
be the same shape as this cell so we're
gonna fake a red arrow with just zeros
oh okay yeah that's all we're gonna
start with zero attention and then you
could keep going and so you see that the
second word pays most attention to the
word black here
mwah and then the word eight and then
the word the mouse and you get to see
something in this sort of picture of
what the network is paying attention to
you get to see some cool features of the
language like the fact that here we are
visualizing the values of alphas that's
right yeah we're visible dark darker
color is big alpha yes that's right
darker color is the the more quote the
closer alpha is to one and you see cool
things like the word black cat gets
reversed from English to French or the
word eight gets expanded to two words so
attention is really nice because not
only does it make your network better
but it lets you interpret some of the
results so this who thought this was a
bit complicated a couple of you this
looks like a hundred or maybe two
hundred lines of code to implement right
well maybe we'll see let's see
so let's first embed our words using
embedding lookups then we need a guru
cell for our encoder and actually just
to show you that these cells can be
wrapped to implement various
regularization techniques like like drop
out I put a drop
wrapper around it and then I use my
dynamic RNN to unroll this encoder cell
so here basically I implemented this
part the encoder then the decoder is
again a guru cell I will use this beam
search trick to produce from the
unrolled decoder the most probable
sequence of words instead of just the
most probable world and in my sequence
to sequence API I also have a dynamic
decode function to which I feed my
decoder cell and this will unroll the
sequence and build my decoder so now I
built
I built this part and the only thing
missing is the attention mechanism so
this is how the attention mechanism
works in the sequence to sequence API
this grooc cell here is a repeat from
the previous slide this is my decoder
guru cell I will do a little change to
it in this API I can wrap it with an
attention mechanism and in the code I'm
writing the only thing the only place
where I'm instantiating something a
little bit complicated is here the
longer tension here I'm actually
instantiating a certain little neural
network with with a given shape that has
been devised by mr. Luang and so that
works well in this case so let's let's
use along the attention I wrapped my
cell in wrong attention and then I use
this API at the sequence to sequence
attention wrapper sorry that's what I
write I wrap it he risers instantiated
my attention and I wrap my cell into
this attention wrapper to obtain my
decoder cell and then I just plug this
decoder cell wherever my guru cell went
previously in the decoder that's it
that's the entire implementation using
the sequence to sequence API in
tensorflow
of this sequence to sequence model with
attention and it's exactly
seven ten lines of code so yes Google
Translate is actually 10 slices ten
lines of code we are skipping some of
the glue code that loads the data in and
all that and yes Google Translate as a
few more lines but let's see if this
actually works if this translate
something so let's go I have a script on
this machine that runs inference on one
file containing German sentences I will
show you in a minute which sentences and
let's clear the screen so that you see
it clearly currency you bastard son can
you translate if such damaged I do not
understand contains Aveda hewan do you
repeat again yeah almost the confidence
we have fun computer and enthusiasts an
organism to the conference is organized
by the Congress it got it wrong on this
one so yes be reassured for the for the
jobs of people were working on Google
Translate what they do is more than ten
lines of code but with these tens on ten
lines of code we are translating German
to English in a very impressive way and
that's mostly thanks to this attention
mechanism so this is I mean this is this
is significant what what we are sharing
with the here with you was published in
a blog post from from from Google brain
and they built the sequence to sequence
API to be able to experiment very easily
with different architectures of
recurrent neural networks and so you
have seen a couple of them one for
detecting toxic comments one for
translating and you see that again it's
a game of Legos where you piece together
different pieces and tensorflow now has
very efficient api's like sequence to
sequence for doing these pieces in in a
very simple way
and indeed those models end up being 10
15 20 lines of code which is really very
very nice so we can translate little
word from the future now what is
intriguing for me is that when I see
good translations I think about a
translator and think what you need to do
to produce a good translation you need
to understand the text can you translate
mechanically that doesn't work a good
translator needs to understand the text
to be able to translate it and you know
into a new language so do you think that
this network is actually understanding
the text it's difficult to say so let's
try an experiment there is an experiment
that has been published in a paper by
Oriole vineos and he said well let's
take this neural network for
translations and now instead of training
it on pairs of English and German
sentences let us train it on movie
subtitles dialogue instead of
translating it's going to generate the
next line in the dialogue and I want you
to show it to your life but
unfortunately it hasn't yet finished
training but I'll show you anyway it's
it's kind of hilarious it doesn't do
what I want it to do but let's let's see
anyway in here instead of movie dialogue
we trained it on tech support tech
support transcripts so it will be
generating a lot of computer stuff about
tech support
let's see nope not this one almost there
this one
so is it a security problem to disable
antivirus yes do the low in connection
to anyone means that the system will
acquire EP from network before system
start a password has been entered that's
not a good idea
weird it is and then I I tried to draw
it a little bit so I said what is the
meaning of life and it answered yeah I
have no idea just kind of fun and and
and then well just to show you that it's
not yet so good I asked how many legs
does a cat have and it answers yeah I
have to use the output of the desktop
and see if you have to run the same
partition no no no not the point but in
this in this paper Oriole venules he
publishes the answers he got from a
movie dialogue database and I find it
fascinating how many legs does a cat
have four I think how many legs does a
spider have three I think it got the
answer wrong but the answer makes sense
the number is wrong but synthetically it
is syntactically it's correct the reason
there is a number and in one of those
two cases it's even the correct number
and when asked what is the purpose of
dying it says to have a life kind of
interesting what is the purpose of being
intelligent to find out what it is these
are certainly terrific examples but what
I find exciting here is that it it's
it's for the first time you see here an
interaction with a neural network that
produces well not completely a human
level type of interaction but an
interaction that that shows that it has
some knowledge embedded in it and it is
able to express this knowledge in
response to meaningful questions and
this knowledge is entirely contained in
the weights of the neural network so
it's it's stored in it in the same way
as we store nor knowledge if you compare
this to the way
that we do chatbots today it's really
completely different to do any chatbots
or basically a first very powerful
neural network that decodes the spoken
language a second one also very powerful
that parses the the sentence and finds
the verbs and objectives and all that so
this is all state-of-the-art super
powerful and then after that you have
someone who tries to map whatever was
parsed to a sequel query and that's
basically you know an expert system from
the 70s which is no not to work you know
great and that's why you get this kind
of mechanical interaction when you ask a
question sometimes you get an answer
sometimes when well it fell through the
cracks and there is no hand coded
mapping of what you said to sequel query
it says I can help you with that
here all the information is contained in
the neural network of course here you
have the opposite problem
a sequel query is kind of useful because
you can insert a given piece of
information into a database very easily
how do you insert a piece of information
into a neural network if it is missing
some piece of information well there is
a second paper that was published which
I find also quite fascinating what they
did is that they found a corpus of of
news articles and they were summarized
in bullet points at the end so if the
article would say the President of the
United States visited some country and
made the Prime Minister and blah blah
blah at the end you would have a summary
in five bullet points the president
visited France he met the Prime Minister
and so on and they said well these
bullet points can very easily be turned
into questions if the bullet point says
the president visited France you can
transform this into the president
visited X and and ask the system to saw
and train the system to solve for X
knowing the tech
so that when trained is actually a
neural network that can read an article
and answer questions about that article
and here I'm showing you some of the
outputs look at this article about Dolce
and Gabanna the way they did their
latest fashion show the question is who
basically who dedicated their fall
fashion show to moms and the the the
neural network correct the answer the
Dolce and Gabanna if you think this was
easy look at this one this is an article
about a Navy SEAL who died in an
accident the question is basically who
died and you have a host of things of
names which could be good answers like
the right answer Jason quartz that's his
name but Paris California Riverside US
Navy CNN Highlands Ranch Colorado all
these are names and you have to
understand the article to actually pick
the correct name out of all those
candidates which you want you could have
implemented something since and similar
same simple that just says well if you
see names with one of those and here
again this this neural network correctly
identifies Jasmine quartz as being the
good answer I don't know where this is
going but on one hand we have a way of
embedding information in a neural
network in a way that produces natural
interaction where this information can
be surfaced on the other hand we have
this or at this research that has built
a neural network that can read text and
answer questions I don't think anybody
has a scale connected these dots but I
want you to share this with you because
I find this research quite fascinating
and if you ask me how far we are from
the singularity I will still answer well
I have no idea there are too many steps
and we have a map
even even a tenth of them but I think
with these things we make one of those
small steps
it keeps me quite excited how about you
well there's lots of interesting
research in learning learning facts
about the world having memory but one of
the most exciting fields in deep
learning at the moment for me is
reinforcement learning which is what
Martin will be talking about right after
right after the break
thank you so ten minute break again and
after this break we come back and this
time for reinforcement learning where is
my reinforcement learning slide
somewhere well what time is it
11:45 so five minutes before 12:00
please be back here for the last half an
hour session on reinforcement learning
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>