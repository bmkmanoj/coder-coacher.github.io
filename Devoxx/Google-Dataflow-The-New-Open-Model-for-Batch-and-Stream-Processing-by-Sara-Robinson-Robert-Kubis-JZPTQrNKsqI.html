<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google Dataflow: The New Open Model for Batch and Stream Processing by Sara Robinson &amp; Robert Kubis | Coder Coacher - Coaching Coders</title><meta content="Google Dataflow: The New Open Model for Batch and Stream Processing by Sara Robinson &amp; Robert Kubis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google Dataflow: The New Open Model for Batch and Stream Processing by Sara Robinson &amp; Robert Kubis</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JZPTQrNKsqI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright everyone so today we're going to
talk about google data flow which is one
open model for batch and stream
processing just a quick show of hands
how many of you have used data flow
before all right looks like not that
many of you so hopefully you'll learn
something new about it today first a bit
about us my name is sarah robinson I'm a
developer advocate on the Google cloud
platform team I'm based in New York
super excited to be here in Antwerp
today first time here you can find me on
Twitter at s Rob tweets and my github is
Sarah Rob hi everyone I'm Robert
Coover's also a developer advocate based
out of London you can find me as well on
twitter and always mention like what is
a netbook at we are basically your
megaphone into the Google organization
that build the products for you so if
you have like challenges with using our
products or you like our products we
also would like to hear that you can
find us on Twitter and that's just read
us and we basically we try our best to
make it the products better alright so
today we're going to talk about data
flow and the rotor data flow so we got
to start with a little bit of history so
you want to frame the whole discussion
like how did we get to data flow and
what is it all about so Sarah is going
to talk about it a little bit of the big
data history thanks Robert so we have a
timeline here and I'm going to start
with MapReduce which was a paper that
was published by Google in 2004 and the
big change with MapReduce is that
instead of running a job on one computer
to process data you could distribute the
job and run it on many machines in
parallel and this is what a typical
MapReduce batch processing pipeline look
like so you have a lot of elements as
input and then you divide them into
chunks and you divide these chunks among
many different machines and you run a
mat function on them so this is going to
be an element wise transformation that
you run on each element and then from
these mapping machines you distribute
them to a series of reducing machines
where you do some sort of aggregation
function so maybe I'll group all the
elements by key and that's what you'll
do in the reducing phase and then you'll
produce your final output
so engineers at Google started using
MapReduce for any sort of big data job
and it was great but it was it was
fairly limited it was a single select
followed by a single group by and people
at Google keep to reinvent cut
reinventing systems to chain multiple
MapReduce operations together and
thought maybe there could be a better
way and the next paper that I'm gonna
talk about is flume Java flume Java was
published in 2010 and essentially what
flume java was is a higher level API
built on top of MapReduce so with flume
Java developers could focus on the
algorithms and not the underlying
systems to run their pipelines so the
system would automatically transform
their job into a series of map produces
and there was some higher-level easy to
use functions to build their pipeline
and this is some flume Java code that
I'm going to walk you through it's
actually pseudocode because if we wrote
it all out it would be a bit longer
wouldn't fit on the slide and this looks
very similar to the current data flow
API which you'll see a little later on
in our presentation so in this example
we're computing the mean temperature so
let's say we have a lot of IOT sensors
all over the world in different
locations and they're emitting data on
the temperature in whatever location
they're in so we start with a series of
raw events and flume Java introduces the
concept of a pea collection which is
essentially just a collection of data
that we want to process so we have this
rock collection of events and then in
the first phase of our pipeline we're
going to transform each event into a
more structured key value location
temperature pair and we can do that
using a par do which is a parallel do
function so this will perform this
function on each element in RP
collection and then we want to do an
aggregation function so we want to find
the mean temperature / location to
change our P collection in into this key
value pairs and then we'll write that
output somewhere so this is a quick
intro to what the flume Java API looks
like so many people started using flume
Java to process their data it worked
really well lots and lots of data even
more data and a common thing to do is if
you have constant data coming in is to
divide it into Chung
maybe chunks by day so let's say we have
a lot of log files and one way to
typical way to process them in this
situation would be to periodically write
a job that processes all the data from
the beginning of time or we could
process it in chunks so we can process
all the data from Tuesday Wednesday and
Thursday independently so it seems like
we're done right we have a solution a
batch solution where we can process all
our data from the beginning of time but
we can and we can also divide it into
chunks there's actually a couple
problems with this solution so for
processing the data in chunks like by
day let's say we have a pipeline that
does some sort of abuse detection where
low latency is really important we need
to do something right when we detect
abuse in our system well the thing I
mentioned before won't work in this
situation because if we're waiting till
the end of the day the end of the hour
to process all of our data will miss
those abuse situations another way this
will work is if we're dealing with user
session so bursts and user activity and
we still process our data in daily
chunks you can see in this example two
of our users their sessions were split
into two chunks because they were
between two days in the in the data we
were processing and often today we're
dealing with continuous and unbounded
data so we don't have a finite set of
data it's always coming in and it has
unknown delays like network delays so
here we can see a bunch of elements that
we're processing here the red element
occurred at 8am and it was processed
just slightly after 8am so just maybe a
small network delay we can see the
yellow element occurred at eight also
but it was processed about 30 minutes
later maybe a little more of a network
delay then we can see this green element
over here which there was a delay of a
couple hours maybe the user was offline
maybe they were climbing a mountain so
it took many hours to process this data
so we need a way to deal with this
situation I'm going to hand it over to
Robert talk about how we deal with
unbounded data yeah so we needed a
system basically that can work on this
unbounded data on a stream of data
if I select more fresh results more
real-time results so you can imagine
like Google became braids or big through
search right and surg is also about
freshness of data so if you search for a
game for instance a soccer game how
about some plays against France or
something like that you don't you want
to have basically an accurate crew a
score at that moment in time so what we
developed in between Google is a system
called milvia and we published this
paper in 2013 to like talk about it like
how we deal with that so basically what
is me about it's basically framework
very build a directed acyclic graph
which does an element wise processing on
your elements that come on the stream
and then the system manages for you the
persistent states and like the movement
of data between certain graph nodes so
if you look at it like it's pretty much
like what you do in a hadoop like manner
like you want to extract the key from
your elements and then group them and
whatnot so basically instead of doing
that or no on a batch of data you do
that basically on every element that
comes in so you have element-wise
transformations or extracting a key
cooping then doing the cooping and then
doing the application but you still like
have the challenge like you want to
calculate some results in terms of you
need some you want to do some
aggregation so we're still at the point
that we do need to pound this data into
into so-called windows so we see here
like again we have these windows for
every group this data in but most of the
streaming systems if you look at them
today what they do is basically the
window the data by the time they it
arrives in your system and that as we
have seen that as a zara zara has
explained data can be delayed what means
basically if you window the data by the
time it arrives in your system you could
get incorrect results because it it
doesn't actually reflect when the delay
when the event happened so another thing
is so what we need basically to do is we
need to window these things by the time
and these events actually happened
so we distinguish between event time and
processing time so we went I meant the
time like when the user crash the candy
and his on his mobile game and then
processing time is when the element
actually arrives in your system and but
we need to know like how long do we keep
these windows open and things like that
also do you have other mechanisms that
we have seen before we want to maybe
window these not only by time we also
window them by a user burst with
basically in a session basis and even if
we window them by this session basis we
need to know like when can we actually
close these windows when do we like know
that all the data is there or like when
do we expect all the data is there so
what we actually adding there is like we
formalize the event time versus versus
processing time and we basically what
you see here in the graph you have on
the y-axis processing time and under on
the x-axis and event time the dashed
line that you see there is basically our
real time line every element that
happens in the lower right corner please
let us know I want to know how do you
did that that's basically if you time
travel and you see an event in your
system before it actually happened that
would end up in the lower right corner
so that would be pretty cool but still
even Google doesn't know how to do that
so you have this watermark and the
watermark is basically i talked about
how we need to close this window at some
point and we need a heuristic to know
like okay we believe that now all the
elements until a certain amount to a
certain point invent I'm arrived in our
system so that we can close the window
and so the watermark is basically the
hora stick to have the estimation when
we have all our data the school that you
see there is basically the delay delay
between when an event happened and the
rent when it comes in alright so there
are a couple of scenarios where you have
to think about like okay what do I use
do I use a batch system or do we use a
stream system to tackle this big data
problem so I going to talk about a
couple of these scenarios and I want to
use three KY three areas to talk about
them one is completeness in terms of
like how core
like I want to have correct results or
not the other one is latency basically
when do I get the result from my data do
I get it like in real time or do I have
to wait and the third one is cost like
how much do I have to actually invest to
run these pipelines so you see a graph
which shows you on the y-axis the
importance of these three criterias and
on the x-axis basically the three
criteria before I talk about this like
there's one challenge in this graph we
have in the in the middle the low
latency graph and it's kind of a little
bit of reverse thinking if the low
latency graph is low it doesn't mean low
latency that means the low latency is
not important so I just want to point
that out because it wriggled me up a
little bit in the beginning alright so
first first example is a billing
pipeline so in the building pipeline you
want to be correct so it's really
important that we are completing our
calculations you don't want to charge
your customers or wrong amount or you
don't want to get back to them and
saying like hey like I need a little bit
more money from you from last month so
it's really really important that this
is complete latency is not that
important in a way like it can take a
little bit longer after you close the
month so it can take a day or two or
even three till you charge your customer
it's really more important that it's
correct and then low cost I course it's
not that important basically you
actually want to have correct results so
you want to spend some money to have
actually correct results so next element
also in the building scenario is a life
cost estimator and there it's like a
little bit of worse so basically in the
live customer estimator you don't need
to be a hundred percent correct it's
okay if you a couple of cents or maybe
you were also off but you want to give
your customer like kind of a runway like
where he's heading to in terms of cost
to the end of the month so low latency
is important there I mean like if it's
very delayed then it doesn't give a real
picture what what this runway is that so
you really want to have this like really
fresh and low cost in terms of leg yet
you want to run that like continuously
and you don't want to spend all your
audio profit into running the
pipeline so you want to make sure that
this is not too expensive but you can
basically make the trade-offs between
practice and and cost here another
scenario is abuse so like it soon like
yeah like everything you have basically
a system where you would want to
Detective use and training a model
basically you can do that offline but
detecting abuse you want to do that in
time so again here low latency is really
important sooner you detective use the
better for you because you can avoid a
cost that you don't can that you can't
recover all right completeness it's not
that important in terms of like you can
do a mistake there you shouldn't do too
many mistakes of course you shouldn't
throw out too many of your good
customers but still like there are them
the ways to reconcile and low cost is
here not important again like you can
spend some money on this pipeline in a
way if you detective use early on you
can save money by detecting abuse so you
can spend some money into that pipeline
but for actually training like usually
what you use is like you can use machine
learning to train your abused models and
you can do that offline in a batch
processing their low latency is not that
important you run that maybe once a day
or maybe once a week you want to have
low cost if you run it like even more
times and completeness here is somewhat
in the middle of importance alright so
like if you look at these four scenarios
they covered both screaming and veg
scenarios and if you look at traditional
systems how these scenarios were solved
basically what you have done is usually
lambda architecture where you have one
system which to it does your bed
processing and you have one system which
does your stream processing and the
caveat here is basically you have one
system for bed for setting which means
you have to maintain the infrastructure
you have to have experts for your batch
processing all that and then you have
one system for stream processing which
means you have an infrastructure for a
stream processing and you need experts
in your stream processing so data flow
what address is exactly this this
challenge we wanted to have a
they can do both in one system with one
program model within one for
infrastructure and sara is going to talk
a bit about data flow thanks Robert so
as Robert mentioned data flow gives us
the best of both worlds so we get the
powerful batch processing from MapReduce
we got the easy to use API from flume
Java and then we get the stream
processing from mill wheel and to talk
about data flow to you we've built a
demo application which you'll get to try
out the end of the presentation it's a
super fun mobile game and this is an
overview of how it works so it's a game
where you see a bunch of different
colors flashing on the screen you'll be
assigned a team and your goal is just to
click on that color follow it around the
screen so we're capturing all the click
events to see if you selected the right
color if it was a hit or a miss and then
we're summing up points per team so to
do that we send each click event to
pub/sub which is essentially a messaging
queue and then we send that to data flow
where we have a pipeline and then we
send the data back to pub sub and then
on our client we're using WebSockets to
listen for whenever there's an updated
team score or players score so that we
can display that in the UI and then
additionally in our pipeline we're
writing all of our data to bigquery
which is our big data analytics
warehousing tool which you'll see a bit
more later on so we're archiving all the
raw events in bigquery so throughout
each step as I introduce data flow I'm
going to relate it back to the gaming
example and then you'll get to see it at
the end so with data flow we have four
main questions the first is what are you
computing so what type of
transformations do you want to do on
your data and this is essentially the
MapReduce step the next thing we want to
think about is where an event time we're
calculating our results and then finally
how does the time that the event arrives
in our system affect the way we're
calculating results so when in
processing time do you want to admit
results for our pipeline and then
finally if we're emitting multiple
results in a certain window we need to
decide how we want to add up those
results so first we'll look at the what
the what are you computing and there's a
couple different types of
transformations you could do on your
data so the first is the element-wise
transformation which is some
the mat function in MapReduce so you're
performing a transformation on each
element which you do using the parallel
do function provided by dataflow another
type of transformation you could do is
an aggregation function so grouping by
key or combining or counting and then
finally we could have composite
transformations was essentially chaining
multiple transformations together to
create some sort of custom
transformation so one example is you
could do a parallel do followed by
account followed by another parallel do
so let's take a look at some sample code
for our gaming app so here we've got a
collection of raw log lines that are
just score events in our game someone's
tapping a button or game and we start
with a P collection of just raw log
lines so each line in this file is an
event and then we want to create a more
structured p collection so you want to
turn these raw lines into key value
pairs of a team and the score that that
person got for that particular event and
we'll use parallel do to do that and
then finally we want to do a composite
transformation so we want to aggregate
all of this so that we can get the score
per team for each window we're
calculating for so here's a graph that
so throughout each step I'm going to
show you a different graph so this is
one team scores in our game and on the
x-axis we have event time and the y axis
we have processing time so here we can
see this dotted line is ideally we
process each event exactly as at the
same time that it occurs but in the real
world this isn't actually what happens
so we have an event like this score of
three right here which occurred right
before 1207 and was processed just
shortly after 1207 so that was pretty
good not too much delay but then we have
something like the score of nine here
which was processed at 1200 dat 1201 it
was processed just about seven minutes
later and now let's see here so the
white line is our system processing the
events as they occur and you can see
that when it turns blue that signifies
that it's calculated all of the results
so here it's waiting for each
event to be processed to omit the final
results we don't admit anything in
between and now i'm going to hand it
over to Robert to show some code from
your app alright so we going to build
actually this app like them that we
talked about this is the app basically
we ask everybody later to play it with
us but right now we want to have you
concentrate on on how we do the coding
so basically this is the app that like
every time I click on any of these
fields and inventive emitted and send to
pops up and then to data flow so let's
build this pipeline so what I have here
is basically in this scenario i'm
reading from my pops up pipeline so i
hope this is visible so this is my input
basically this is my game event stream
where all the events are happening and
then as mentioned we are we are
archiving all this data so this is
always a good practice to archive the
data so the first thing that I want to
do is actually which you are very
familiar with in terms of like hard to
like if I do batch processing what I
want to do is first thing I want to
extract a key so I basically go on to my
P collection this here and the P
collection is of type table so I have my
stream and the first thing that I want
to do is I want to extract the keys so I
do off key and then I do ki by team
Sookie by team does nothing else and
basically going into my stream it's a
JSON object that I get let me make that
a little bigger I basically get out like
the team element which has a string like
red green yellow or blue alright so now
I have that basically keyed so next
thing what I want to do is I want to
group it like mapreduce basically
extracting key next thing you do a group
I key so I do the coop by and that
basically groups my my like by these
four teams that we have and the next
thing what I want to do is I want to do
an aggregation so here I want to do
actually and I want to do that in peril
of course for
the the group's I want to do a
calculation for my team stats so let's
have a look at the team stats like it's
very straightforward basically what I'm
doing is I'm extracting hits like hits
is basically if you hit the right color
so your team red and you hit the red
color field that gives you a score and
then we extract things like the device
and the hit rate and things like that
for the aggregation so pretty
straightforward all right so if that
basically we have our batch processing
so this is basically if we have abounded
bounded street and like about that data
set we could execute that right now and
it would create as a result alright so
we're going to go back to the slides so
as robert mentioned that works if we
have abounded finite data set but in our
game people are going to be playing all
the time so we need to tell our pipeline
when we want to calculate results
because if we if we just use batch
processing it's not going to work in
that situation so we we're going to use
windowing to our divide our data into
event time finite chunks and there's a
couple ways that we can window our data
the first is fixed windows which is the
most simple that just says give me the
total results every two minutes every
hour every day or we could use sliding
windows so we could say every hour give
me the results from the last 24 hours so
it's sliding as time goes on and then
finally we could use session windowing
so we could window by bursts of user
activity and so here's what our code
would look like if we added fix
two-minute windows and back to our graph
now we can see that we're calculating
the results every two minutes for this
team's gameplay so but we're still
waiting for all the results to be
processed to calculate the final result
and now we're going to switch back to
the code alright so basically what we
have here is like what we can do with
the
during is you can see like a development
of a score or throughout the day for
instance but you still have to wait till
the end of the day to get actually this
development of the score so we do the
same like what we decided when our demo
we want to do a rendering into fixed
window so what you do is basically you
apply to that stream that we have we
apply a window and it's like you can
almost read it it's window into and then
with a fixed windows and then we say off
and then a size so this is the duration
we want to have minutes and we say like
okay let's do a five-minute window so
basically like we chuanghe our data in
five minutes windows but now it's still
like yeah we like we need still to like
a couple more steps to get actually to
stream processing so what is the next
step all right so our next step is we
need to think about when in processing
time we're going to calculate results
and we do that with triggers which will
control when our results are emitted and
triggers are relative to the watermark
remember the watermark is that here is
stick about event time progress so it's
our systems best guess that it's seen
all the data for a given window so here
we're going to trigger at the watermark
which is actually the default we put it
here just for clarity so we're going to
trigger just when the watermark reaches
the end of the window and let's take a
look at a two graphs of how this would
work so on the left side we have the
perfect watermark in an ideal world the
water we hit the water mark just after
we finished processing all of the data
for a given window but in reality we
have our heuristic watermark which we
see on the right hand side and there's
two problems with our current model so
right now you can see that the score of
nine is completely left out of our
processing pipeline because it comes in
way after the watermark has already
passed and then we can also see that in
two of our windows we have multiple
results so it might be good to emit
speculative results so that we can see
the results before we've before we've
hit the watermark all right so how does
it look in our code like basically we
need to basically trigger the results
once like the window that
the watermark passes our processing time
so what we actually do we add a trigger
to our window so what we do here is we
want to have enough to watermark trigger
past the end of the window and then
basically yeah yeah so basically like um
we do this and because we have the
different elements here we need to
actually cost this table okay so
basically now we have like the
triggering but as as Aaron mentioned
this is basically like when it passes
the watermark it emits a result but
maybe you want to have some speculative
results maybe what do we do about this
late data this is boils down to
correctness here so what do we do with
this so we're going to add some rules
here to account for early firings member
i showed you in the graph before we had
two of our windows where there is a
couple results that came in before the
watermark so we're gonna emit results
every minute and then we're also going
to add this rule to account for late
firings and the one they're just
indicates that every time there's an
element that comes after the watermark
we want to admit results for that window
again and so this brings up the question
of as we're emitting multiple results
per window how should we account for
those results so should we MIT them as a
running sum or do we only care about the
values that have come in since the last
time we emitted the data so to do in
this example we're going to add a rule
to accumulate fired pains so we're going
to accumulate the total as we omit the
results for a given window and here's
what it'll look like again on the left
side we have the perfect watermark and
then on the right side we have our
heuristic modern mark so we can see that
in this case we did account for that
nine because we added that rule to emit
results whenever there's late data that
comes in after the watermark and then
you can see in our second and our fourth
window we're able to emit early firings
every minute for every time so we get
all those early elements that came
before the watermark so let's see what
that looks like alright so basically we
have our trigger
we saw before and now we need to add
like the basically to enhance our
trigger for early and for later data so
we add in early firings and we say after
the processing time basically that is
the time when when in your window like
wild the processing time advance past
first element in pain first element in
pain especially the first element that
is windowed in one of your windows is
the first element in that pain and then
we add a delay of adoration so what I
want to have here is I want to give for
results because we want to have like
kind of a real-time demo here we want to
actually have a result emitted every 10
seconds all right so this is our early
firing now we want to account also for
late data so we have to enhance our
trigger for late firings as well and
here I do an after pain basically which
basically means like after the window is
closed anything what happens after that
and what I do I want to do here is like
whenever an element comes into our
system late like after the window was
already closed I like every time I want
to emit a new result so I say like a
cave like even if one element only comes
in our system I want to admit a result
alright so after that we still have to
say things like okay we how much
lateness do we actually allow so because
like the system has to basically keep
the state of like what are the closed
windows and what are the elements in
there so if you would just do infinity
or you would run out of memory at some
point so what you want to do is like you
want to set a boundary in terms like how
late of data to you do you allow
actually so in our case I wanna have
let's say 10 minutes of late data that I
can arrive so now we have like accounted
for later we know what we're going to do
like in terms of your hair if we have
early results we omit them every 10
seconds but they're still like missing
here the fourth question and the first
question basically I have to define like
what am i doing with the results that
are emitting
so in my case i want to accumulate I
want always like add basically each new
element that comes into mind system I
want to add that to my application so
having this is basically our pipeline
and I gonna talk a little bit about the
other things that we have here in our
code so once we have like basically what
we do here is our windowing so we window
we too early and late firings and we do
accumulated fires we have seen me to our
grouping and our keying for team stats
we also do that for players that's i
also want to know which like what is the
hit rate for player and things like that
and then last but not least what we do
is we write it out back to pops up again
and also to bigquery just like just
because we can basically and then like
what what data flow is all about you
basically you build a graph here and you
don't like fill it with any kind of data
the data leg comes in when you actually
start and run this pipeline so you have
to at the end you have to actually
trigger the run like now I want to run
this pipeline so I going to show how do
you actually deploy this so we have here
it's based on java i'm using the java
sdk s and version 1.8 right now so you
like I'm running this on the Google
cloud platform dataflow service and you
have basically to define the main / and
you have some job arguments in this case
what we have we give a job name we give
the project that we are using we give a
staging location where like our code is
staged in the cloud and then we define a
runner so data flows supports a local or
honor or like the dataflow runner and
then we also to find that we want to
have streaming since this is like a
rather small room I decided to run this
on one worker but you could run that on
like 10 not thousand machines if you
wanted to next thing is like the zone
like we're in which like cloud region do
your one hour run it and the absolutely
true is pretty I have the job running
right now but I want to
dated so that you're gonna see like how
that looked like all right so i have
this now like basically probably most of
you know maven runs through and deploys
my my job right now so what we going to
see is like in the data flow a console
we see all the drops and you see here i
did a little bit of trial and error i
had some arrows here but it shows you
all the jobs that are running it shows
you the streaming and debate for setting
drops so you see here right now that we
started our new job it's running 9
seconds it takes a little while we
basically have to provision the machines
and and install the SDK and set other
like Apple of things so it takes a
little bit to actually get started and I
have an error of course but okay I'm not
going to back that life right now but we
have to extol the streaming running when
I can show afterwards that it should run
as well so we have that running and
we're gonna show a little bit of like
yeah okay so just to summarize what we
talked about when we built our pipeline
just now is we started with classic
batch and the amazing thing about all of
this is we didn't we just had to add a
few rules we didn't have to change our
algorithm much at all so we started with
batch then we went to batch with fixed
windows and then we started to build a
streaming pipeline so we looked at a
basic streaming pipeline and then we
added to our pipeline to allow for
speculative and late data and then
finally we ended up with a pipeline of
streaming data with accumulations and we
didn't have to change much about our
code at all we just had to add a few
rules to do both batch and stream so now
we're going to go into a live demo the
Robert just deployed just to summarize
again this is how the demo works so with
every touch event we send that to pub
sub every click event we keep track of
whether it's a hit or a miss and then in
our data flow pipeline we're windowing
the data in five minute windows as you
saw but we want to update the results
every 10 seconds because we want this to
be real time games that you can see new
results see which team is in first as
results come in so we're sending that to
pub sub and then over web sockets and
client we're listening for updates for
either a team or a player and then we
update the UI and then we also send all
the results to bigquery so we can run
some analysis on the top players later
and then in addition to the streaming
pipeline we also have a batch pipeline
so we read all our raw events from
bigquery read them in data flow and data
flow we just get the highest scoring
players and then we output the results
to a different bigquery table right so i
want to show you like this is one of the
main points actually if you talk about
data fluid is the thing that you can do
batch and stream processing with the
same SDK with the same model so you
basically learn the system once and you
can do both of your of your big data
tasks so here I have a sample pipeline
so basically like you it looks pretty
familiar what I do here is I'm keying by
a team I'm grouping and then I
calculating my team stats it's the same
classes that you have seen before i'm
not changing their anything and then i'm
reading in this case from a bounded data
source so i'm reading from bigquery here
because I need to bound the data source
for a bad job and then I'm writing my my
like aggregations back to bigquery so
it's very simple to get like you
basically can build up your data
pipelines you can start with a batch
process like you have like some batch
data you start building your pipelines
and then you can easily transform them
into a streaming pipeline if you want to
alright so let's play a game so it'd be
awesome if all of you went to this URL
dataflow color smashed firebase app com
or you can use the QR code and here's
how the game works so once you get to
that page you'll automatically be
assigned a team color and as the colors
change you score points for clicking on
your team color you can only click once
per interval and then we'll do some epic
real-time data processing so I'm
actually going to open it up and play
the game yeah me too i'm going to
refresh the page here we go I'm on Team
Red so I'm going to try to click all the
red ones hopefully everyone's joining
the game I'm you see that you ID on the
bottom which will
tell you why it's there later oh can we
show the QR code again yes there's a QR
code oh there's also a short link down
there if you want to use the short link
so right now it takes a little bit i
actually found the back and the restore
the pipeline so the pipeline starting
daughter is about two minutes and let me
show you actually really quickly how
that looks like so we have here the
pipeline running as i want to show you
this live demo like so we have here the
pipeline running its elapsed time right
now two minutes so it shows you
basically the pipeline that we have in
in data flow and Google cloud platform
we are reading from pops up we are
sending the events to bigquery and then
we are keying by a team and by player so
let's see if we actually get some
results here takes about three minutes
to get to the point that it starts
sending some elements let's give that a
little bit of time alright so let's
switch back case anyone needs URL again
dataflow color smash that firebase app
calm and let's take a look at how it's
doing there alright cool so it looks
like here's the ranking so red team is
in first place right now so it shows the
hit rate for each team and your
individual hit rate so I've had red
twenty-three percent of my hits as we
can see that red is still in the lead
you can see it move around as different
teams change so while you're playing the
game we're going to go back to the
slides and I'm going to hand it over to
Robert to talk about apache beam yeah so
we talked of like the title of this talk
is basically an open model right and
right now we have talked only about data
flow and Google cloud platform and using
the data flow service and Google cloud
platform but one one thing like what we
actually did is we donated data flow to
the apache software foundation as an
incubation project to actually
commoditize it and basically give it to
the open sore
community which enables you to actually
move your data pipelines from like your
on-premise systems to any kind of cloud
providers so Apache beam is basically
the unified model that we have that we
talked about the four questions when
where what and how and it supports
multiple runners as well so it has
multiple SDKs right now the Java and the
Python SDK is available the Java ISM
like the most sophisticated most
full-fledged supported one since this
was the initial SDK that for data flow
that was built but python is coming up
as well and we are looking basically for
others to contribute other languages as
well so you have multiple SDKs to write
your pocket or pipelines and then you
have multiple runners as well one that
we show it right now is the dataflow
runner you have can run it locally on
your vm on like your laptop and you can
also run it on other for like projects
for instance Apaches fling and apache
spark we have runners as well so we are
still in the process of growing the
community around apache beam pleat
please try it out it's on github check
it out like what it is how it works and
it basically enables you in the future
to write your data processing pipeline
in the petty beam and also run them on
Google cloud platform so let's recap
sure like how this whole like like how
did we get to to data flow and what do
we want to actually achieve with it so
if like mapreduce undoubtly kicked off a
whole like industry of like processing
large amounts of data and google like
developed and improved their data
processing pretty much in a closed
environment so we had only a finite set
of file formats and we can basically
improve and iterate much faster
basically because we don't have to
custom all all of the things so we
bought out a couple of like you many
many papers but nothing that actually
people could get their hands on so we
are
really glad actually that these papers
inspired the key open source community
to build the systems and the open source
Vista Community also went even further
and innovated on themselves so this is
like really great and really encouraged
that a lot and we also support a lot of
these projects so if dataflow basically
what we want to bring we want to bring
together these worlds we want to bring
together the world but which was
developed within Google and the the open
source community so we have basically
also a different product on the Google
cloud platform called data proc where
you can run your Apache Hadoop or spark
cluster and 50 Apache beam model we've
actually basically want to have the
bridge of like bridging the Google
environment and the the open source
communities alright alright so you were
playing a game and I mentioned something
about finding the top scorer in the game
top players I'm to do that we're going
to use bigquery which is where we were
outputting some of the results in our
pipeline how many of you have used
bigquery before anyone in here it's like
not too many of you so a quick overview
bigquery is our big data analytics
warehouse tool it's a managed service
and it scales really well so you can
query terabytes of data in seconds and
it's all written in familiar sequel it
scales as i mentioned from bites to
petabytes and bigquery has a REST API so
you can access it from any client
libraries if you're not familiar with
sequel bigquery also integrates with
many data visualization tools like
tableau so you can easily create
visualizations based on the data you're
storing in bigquery you can instantly
share your data sets so you just need to
add a user by email or an alias you can
add a group to your data sets and they
can instantly have access to all of your
data and finally everyone gets a free
monthly quota so you can query up to a
terabyte of data for free every month
we've got a bunch of sample data set so
if you want to learn more I definitely
recommend trying that out but right now
we're actually going to do some live
queries so you can access bigquery
through the REST API another way to
access bigquery is through our web UI
here so you can visualize all of your
data here and actually run the queries
directly in the browser
so this is the table we're going to be
looking at and here we can see the
schema for our table we've got a
timestamp column a UID a team color and
a device for keeping track of whether
you're playing on desktop or mobile
feeling most people here are playing on
mobile so this is what some of the data
in our table looks like and I've got
some queries saved here that I want to
run so first I want to see well let me
make this a little bit bigger the hit
percentage by device type so I want to
see if there's a bigger hit percentage
while you're playing on desktop versus
mobile so here we're just counting
whenever the team equals the color so
whenever you had to have a hit we count
that as a hit and then we divide the
number of hits by the total um and we're
ordering by hit percentage so let's run
that query and we can see that on
desktop the high percentage was actually
quite a bit higher fifty-six percent
compared to forty-nine percent on mobile
but we did have many more hits on mobile
so that is our device type query let's
take a look at another one so we can
also see the hit percentage by team I
think the red team was in the lead last
time we checked this is the same as the
other query but instead of device we are
grouping by team and we can see that red
had the highest high percentage 56% I'm
also had the most hits and then followed
by green yellow and blue so the last
where I want to run is I want to find
out actually who the top players were so
who in this room had the biggest hit
percentage and as I mentioned before you
saw in the bottom of the game you have a
UID I'm we actually have some prizes I
believed to have the prizes here we do
the prices at our now we have prizes oh
we have the price yeah awesome Thank You
Lexi so we're going to give a prize to
the top layer for each team so this
query will tell us who those people were
and if it matches with your uid you can
come on down so Row 1 2 3 and where's
blue oh we don't have a blue not alone
oh ok here's the blue one
so i'm gonna i'm going to go back for a
second so if you if your uid matches any
of the ones in the top three rows come
on down take a picture and then yeah
take a picture in the screen i'll leave
it up for a moment and then i'll go the
next page it's not we didn't have a lot
of blue team members it seems ok i'm
going to go to the blue one shortly
maybe can copy the moment yeah anybody
have a match yet anybody match oh we
have one Mitch hold on down you've got
some prizes go back to the other screen
so again these three which teams which
team are you on which team which team
yellow or even another match what team
are you on congratulate ok well very
nice we have yellow and blue any red or
green two members congratulations which
team are you on read a perfect
congratulations missing yellow yellow
again we just had yellow this one the
yellow all right i should have compared
the you ids 89 yes you are so the other
yellow you were a child oh this one I
know it's just said just the first thing
yeah yeah the first one from each side I
guess we are out of our devices so good
enough all right um thanks everyone for
playing hope you had fun you can
continue playing if you'd like I'm going
to go back to the slides alright so if
you want to get started with for
instance back where you can go to cloud
at google.com / bigquery and learn about
all about bigquery but we're here to
talk about apache beam
so if you want to learn about Petra beam
go to beam to the incubator the patch it
at work or search for Apache beam and
you can learn all about the new one
model for veteran stream processing we
also like to get our your feedback after
session how you liked it what we can
improve what we should do better better
demos maybe have some idemo ideas so
please go to the short link or QR code
and give us some feedback thank you that
is all we got any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>