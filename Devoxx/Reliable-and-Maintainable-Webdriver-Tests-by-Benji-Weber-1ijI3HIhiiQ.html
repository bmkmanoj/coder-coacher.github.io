<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reliable and Maintainable Webdriver Tests by Benji Weber | Coder Coacher - Coaching Coders</title><meta content="Reliable and Maintainable Webdriver Tests by Benji Weber - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reliable and Maintainable Webdriver Tests by Benji Weber</b></h2><h5 class="post__date">2017-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1ijI3HIhiiQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone
thanks for coming I want to talk to you
today about I guess some of my
experiences not managing to write
reliable and maintainable tests with
webdriver end-to-end tests and some of
the mistakes are made along the way in
the hope that some of you can learn from
my mistakes and have a bit of an easier
time how many people here have
experienced writing end-to-end tests of
any sort with nearly everyone great I
guess that's why you'd come how many of
you found it easy to make them reliable
maintainable and fast about three people
okay and pick your brains later so when
I knew I was going to be doing this talk
I asked a few people what their top tip
would be for reliable and maintainable
and fast and 2n tests and one thing came
up more than any other and that was
don't they don't have the best
reputation for being reliable easy to
maintain fast for good reason and I
guess in some ways this is good advice
right if you don't need to our
end-to-end tests if they're not going to
give you enough patty then don't bother
because in my experience they are a lot
of work and ongoing investments needed
and there's bigger challenges at each
scale you get with them so if that works
for you then great but as you're not all
running out the door I imagined that
some of you have good use cases for
end-to-end tests so i'm benjy weber
i'm benjy weber on twitter i'd love to
hear your questions comments thoughts
heckles afterwards or during I won't be
checking Twitter due home but yeah I'd
love to hear from you I work for a
company called unruly
a ton Bulli I have the privilege of
working with products that are I guess
universally beloved you could say you've
probably appreciated these every day
even today they've enriched your
experience as you browse the web I'm
talking about online advertising yeah
groans so advertising doesn't have the
best reputation it's it annoys us often
but it is important it pays for a lot of
the content that we enjoy online and it
brings with it a number of technical
challenges and responsibilities that are
relevant to end-to-end testing and the
detail is really important this is a
graph from a few weeks ago and this was
our CDN costs and they suddenly shot up
and we investigated why turned out to be
a file that was literally empty that we
were serving zero bytes in size we
deployed with the one caching headers
and suddenly our costs had shot up
through the roof the detail is really
important we have a reasonable amount of
scale not the kind of CERN's and bubas
and Facebook's we've been having about
the conference but we're still
collecting terabytes of data every day
hundreds of thousands of requests per
second the details important and there's
I guess three reasons why the strategy
of not building these tests doesn't
really work for us in advertising first
one is user experience user experience
might seem like an odd thing to talk
about in the context of advertising why
does the user experience matter well I'm
I expect you can all think of a time
when you've had a poor user experience
with an advert maybe it's popped up in
front of the thing that you were reading
its blasted audio in your ears
distracting you from what you were doing
you've been hunting through your
background tabs trying to find which
one's actually playing sound
so don't know how that made you feel but
it probably didn't endear you to the
band that was doing the advertising so
the user experience is important and
it's important that we can test the
actual user experience or as fast as
possible that so people are really
experiencing are their real devices real
browsers speaking of browsers we can
directly measure the cost of not
supporting different browsers we can see
the amount of traffic the amount of
money would make if we supported an
extra browser so it's really valuable to
us to be able to support as many as
possible and well we could manually test
all these different browsers being able
to automate testing end-to-end with the
real browsers is really valuable so you
allow us to make sure they our stuff
continues to work
and finally there's there's a lot of
danger in advertising right we're
running code across thousands and
thousands of websites across the open
web there's a lot that we could mess up
and there's a lot of people's use of the
web that we could mess up if we cut
things up and there's a lot of we see
thousands of new ads every day as well
and they can interact in unknown ways
with things so you end-to-end test
helped us to mitigate this danger to
some extent so we got all this this
danger that where they were trying to
live with but there's also a lot of
value to be able to learn fast
advertising in advertising we have a lot
of data that's one thing we're not short
of and if we can build things fast
deploy things fast learn fast we can run
experiments in production we can see how
things affect our data and we can
iterate on our products more quickly but
this this only works if we have a safety
net right so if we had to manually test
everything and every browser every time
we made a change then we wouldn't be
able to make use of the data we have to
learn quickly
that's unreal II worked quite well set
up for this kind of fast iteration we
practice extreme programming if you're
not familiar with extreme programming
that means you'll see us doing things
like pair programming to people working
at the same time on the same computer
solving the same problem so everything's
reviewed at the time is written you'll
see us working in bigger groups so
called mob programming there's a talk
about this yesterday where you have
maybe the whole team sitting around the
same computer working on the same thing
at the same time so everything is
reviewed at the time is written it's
good quality as ready to release to
production you'll see us doing things
like test-driven development will always
write a failing test first see it pass
refactor improve the code and iterate
improving our design and building up our
test coverage so or test automation is
really at the heart of everything we've
always done and our team is also
collectively owned everything about a
product so they're not just building new
features they're also doing testing
they're also working with stakeholders
to think of new valuable things we can
add those products and they're also
keeping them learning in production so
really cross-functional teams a totally
this because as a bit of context because
I want to tell you about our journey
with automated end-to-end testing and
some of the successes and mistakes we've
made along the way
when I started a ton really this this
this journey I guess is over the course
of eight or nine years so quite a long
time when I started the it turned out
that there been some experimentation
with end-to-end testing and we're in the
process of purging that those tests for
our code base it hadn't gone very well
when I inquired as to why it turned out
that they'd been worse than useless
how can tests be worse than useless well
if they're unreliable then they erode
your confidence in
entire test suite right so people have
taken the view probably correctly that
it's better to delete these tests and
have confidence in the non end-to-end
tests that remain so for a long time we
did without and twin testing at all
we've relied on what I think Martin
Fowler calls subcutaneous testing where
you have this kind of thin layer of UI
that's you try and keep as thin as
possible and then you could have test
everything that's underneath that 3d API
is so you avoid some of the challenges
of testing everything through the UI
that I'll be talking about that did us
for quite some time we were basically
following the advice that people have of
not building these tests at all but
unfortunately that didn't do this for
very long there were things that pushed
us to improve the most memorable of
these was an occasion I think we were
doing something mundane like a brand
update to our application and we
received this email
it was quite understated it was from our
CDN Account Manager and they said
something like by the way this this
should have long alarm bells because in
Britain if someone says by the way it
usually means my main point is so by the
way 90% of your CDN traffic is commonly
resulting in internal server errors it
was as we like to say not ideal so how
could this be we'd like to think of
ourselves as a good team we do all these
things like pair programming and tester
of and development we'd like to think
we're building quality software could
this be true well it was you can
probably guess something about the cause
if I tell you that all this traffic was
to forward-slash undefined on our CDN
Dre
yeah it was JavaScript or more
accurately our terrible JavaScript what
happened was that so we had a loop in
this thin UI layer which had an end
condition that was an equality check and
in one particular new version of a
browser the loop counter was getting
double incremented due to a perhaps
legitimate difference in behavior
jumping off the end of the loop
condition and making requests that
resulted in this slash undefined traffic
so it's step three there were a number
of similar incidents that made us think
it's time to level up we basically
inflicted ourself distributed denial of
service attack against ourselves it's a
final straw I think for me and it made
me say enough is enough let's do
something about this now was having to
come into work on a Saturday because we
had a problem where UI elements were
misaligned and covering things up this
was a big problem to come in on a
Saturday the user experienced things I
was talking about bad user experiences
can be caused by bugs as well as
intentionally and I had to come in to
fix this and I wasn't very happy about
it it's amazing how developers can be
motivated to fix things that make
improvements to their development
process if they're responsible for
looking after stuff in production I
recommend it so I brought it up with a
team you may be able to tell it really
as myself put it out by the team and as
you might expect if you've tried to
instigate any change there were a number
of different perspectives on it there
were kind of the cynics the we've been
here before this doesn't work we know
that they're worse than useless let's
not waste our time with these end-to-end
tests then you've got your kind of
enthusiasts the people that think these
would have solved all these problems
that we've had let's do it now and if
you're more the
list people like yes it would be
valuable but it's a lot of work to
invest in this there were some concerns
so myself being quite motivated by
having to come in on my day off and I
ended up building a prototype but in
extreme programming circles we call a
spike spike solution so-called because
it's like a long thin thing that you can
drive through something else end to end
to prove that something works end to end
but it's very thin in itself so a build
a little prototype that demonstrated
that we could retrofit our the better
fits meant to end tests on our existing
ads and applications without having to
change them significantly and also
mitigating some of the concerns that
other people in the team had it's often
disputes down to differences in
understanding or uncertainty and
prototypes a great way of resolving that
sometimes so in a way we decided to
proceed with building out some
end-to-end tests and there were some
things that we did early that paid off
really well one of them was investing in
building them with the page object
pattern people heard of the page object
pattern about half okay a page object
pattern is basically it's just
abstracting your tests away from the
mechanics of interacting with the page
that on the browser so there's a number
of reasons for this it paid off quite
well for us because well not only did it
help us with some of the were liability
challenges that I'll talk about it also
meant that as we were going through
multiple iterations of our UI we could
often keep the same tests and just
change this mapping layer our page
objects that sit between the tests and
the mechanics of interacting with the
page this is a little example this is
probably not the best page object test
example because it's one of our early
ones
it's rather low level abstraction wise
but you probably get the idea there's no
webdriver API stuff in here and there's
no CSI let's selectors or XPath so it's
checking can we share the ad on Twitter
we get an ad we try and share it and we
check that we go to Twitter to actually
share the ad so that it paid off as I
mentioned for various reasons but it
wasn't long before they can respective
sporadic failure reared its ugly head
again and then you hear people saying
the test just ran tests and they failed
but I ran them again and they passed so
it's all fine no it's not and then you
hear people saying well this is what we
said was going to happen all along and
we told you these don't work let's go
and delete the tests or let's delete the
unreliable ones which is generally good
advice if it's unreliable and delete the
test then I'm thinking have I wasted
everyone's time by investing in this
test suite so we thought about deleting
the unreliable tests but we thought
maybe we'll delete one test and then
another one and another one will end up
deleting them all because we didn't
really understand why they were
unreliable so we realized that we needed
to invest in Diagnostics because if a
test fails one in ten thousand times
good luck trying to be produced that
when you're trying to run it on your
workstation so we built some j-unit
rules a unit rules are a great way of
hooking in some code before after during
your test execution wrapping your test
phases we built a little rule that
looked out for an annotation that we
could annotate our tests with called
quarantine if we quarantine to test it
effectively meant we were deleting the
test in that the test
non-deterministically failing would not
block our test suite from passing but it
was a bit more subtle one that if the
tests deterministically failed it would
still fail the test suite because we
might vote on something
but importantly if it
non-deterministically failed it would
file a ticket to our ticketing system
with some Diagnostics about what had
happened to help us investigate the
cause now this is a little bit dangerous
or there's some we published some of
these rules on github don't worry about
copying the URL down I'll tweet the
slides later if you want them it's a bit
dangerous without discipline right
because this rule was effectively
running the tests until they pass or
fail deterministically a few times and
it could quite easily at that point say
well how Suites passing now we can
ignore the problem again but with
discipline it proved to be really useful
because we've got Diagnostics and we can
work out what's going on so some of the
things that we started collecting not
just the traditional like error messages
and stack traces but the browser request
logs all the HTTP requests that the
browser's made and how long they took
and what they resulted in the ha archive
javascript console output screenshots
doing the test execution and so with
these things it was a bit more like
having the dev tools open in front of
you to try and work out what was going
on and this proved to be really useful
it's quite easy to do with the j-unit
rule as well so we just have a rule that
laps the test execution in a try-catch
catches the exception and wraps it in
one of these we call an additional
Diagnostics exception that just adds the
Hart archive screenshots etc that bit
queries using the webdriver API so what
we learn from these Diagnostics well the
big one and this is one you'll come
across if you look at any article on
reliable webdriver tests is you need to
wait for things right you need to wait
for the tests to get into a particular
state before doing the next thing
because you've got all these
asynchronous operations going on in your
browser and maybe the best note when I
click on hasn't loaded yet or maybe
things are taking longer than normal so
this is an example of where our early
investment in page objects paid off
really well because over there two
places where we were doing things like
trying to click a brand bar in the ad
and rather than updating all of the
tests that we're doing that we could go
and update the implementation of that to
wait for it to adhere before clicking on
it a bit
similarly we ended up building a load of
utilities they've got this wait until
that can take references to our page
objects and wait for something to happen
where our page objects expose a
condition I'll show an example of that
in a moment it's basically just a little
method that takes a lambda expression or
method reference and waits for the thing
to be true using the webdriver API so it
wasn't just waiting for interactivity
that we needed to do but also waiting
for assertions we were very used to
writing tests where we say assert that
something was true we realized that most
of the time we wanted to wait until
something was true instead because we
didn't know how long it was going to
take to become a tree so we wait for it
become true where they timeout so we've
built a load of little utility
assertions that used weights under the
hood rather than just asserting
something at a particular point in time
again taking a method reference here's a
little example so we've got a whole load
of these but we're just taking a
supplier boolean or a condition and that
might become true in the future and then
underneath the hood we can use a
webdriver weight it just makes things a
little less verbose because we've
wrapped this utility that looks a bit
like the assertions that you do you
normally so really the the vast majority
of our early reliability challenges were
down to timing and either having
unexpected explicit assumptions about
how long things would take in our tests
or more subtle things
but that didn't last very long it wasn't
long before people were saying the tests
are flaky again or worse ignoring them
because they they were flaky but really
the next time this happened it turned
out that the tests had been falsely
accused of being flaky when we
investigated it using our Diagnostics we
realized it wasn't the tests that were
unreliable it was our implementations
it's it's one of the reasons why it's
really important to make sure that tests
are reliable because otherwise people
just assume it's the test problem and
not your implementations and it's really
easy to write non-deterministic
implementations of web stuff as well it
raised the question though does that
does that really matter
people have people on the web but quite
used to things not really working if
your page doesn't load properly you'll
probably reload it or blame your network
provider or something it mattered to us
because even if you've got a 0.01
percent failure rate and you've got
100,000 tests then you're quite likely
to have a bad about time things are
going to fail a lot but what can we do
about it that we rely on loads of third
parties for web stuff people you might
not even think of CDN providers DNS
servers how many people think about DNS
as dependency of their web stuff working
networks if the network's congested
maybe your JavaScript doesn't load for
twice as long and things don't behave
the same how much do we test for this
stuff so it's really hard so what can we
do about it well it's one thing we
realized was that their tests yes
they're they're more complex tests with
more infrastructure and browser level
complexity but they're still tests and
we can apply the same kind of principles
that we'd apply to our unit tests all
right well we can step out dependencies
maybe we're making a call out to a third
party service in all of our tests and
not particularly reliable why do we need
to do that we probably only need to do
that in some tests and in others we can
stub out that call and things will be
more reliable also a lot of our
flakiness turned out to be down to
networks because they frequently don't
work they get random requests failing
one in a thousand times and yeah dealing
with that is hard so what we do in unit
tests we isolate things so we ended up
bringing all these resources images
JavaScript HTML stuff all onto the same
boxes where we running the tests and
that eliminated probably our next
biggest cause of test flakiness just by
getting rid of the network involvement
as the our web stuff is never really
working it's in a constant state of some
things don't work for some small
percentage of users and requests so we
really need to think about what was a
tolerable failure rate rather than
thinking about things should always work
one of the tools for that we have
another Dre unit world that just
repeatedly runs our tests over and over
again with looking for validation again
on the test and gives us a report about
how reliable it was but it led us to a
realization that maybe we should be
treating our tests more like our
production monitoring we don't expect
our production systems to be 100%
available we work out what an acceptable
availability is and we bake that into
the way that we monitor things and maybe
we should be doing the same thing with
tests wait there's a huge amount of
complexity and the stuff we're testing
maybe it doesn't matter if the it
doesn't work to the business one in a
million times maybe but it does matter
if it doesn't work one in ten times so
what is where does that lie and we we
can bake that into our test suite
winners as well and only have them fail
if they determine the failure waits too
lies above an acceptable threshold once
we started thinking about our tests that
way it they used some opportunities
because if if our tests if we're
thinking about our tests like monitoring
then we can maybe we can use them as
monitoring we would by this time built
up a test suite that checked lots and
lots of aspects of behavior in an
isolated environment but we were still
quite vulnerable in production and we
realized we've got exactly the same web
stuff in production the only difference
is the URL we're pointing at so we were
able to point the test suite at our
production systems by changing the URL
and have them tell us whether the
behavior was working in production as
well now it's a bit of a simplification
there were a number of challenges to
overcome
like not generating the side effects in
production isolating test data but these
things are over chemical subject for
another talk probably but it's an aspect
it's an example of some extra value that
we were able to get out of these test
Suites another opportunity that we found
was that we could check invariants
things that should always be true for
every bit of behavior that we had in our
tests us a big one is sound I mentioned
earlier people generally don't like it
when you play sound at them in ads on
the web so we could add in a variant
that checked by capturing input to the
sound device that no behaviors we have
result in sound for the user that turned
out to be quite useful because we
develop on work stations that don't have
speakers and it's really easy to not
notice you've done something the results
in sound or doesn't mute things properly
so this caught some problems but
interestingly it also introduced some
flakiness and this really underscores
the importance of having good
Diagnostics because
this took us quite a while to track down
we couldn't work out why the there is no
sound assertion was failing sporadically
and it turned out to be because
occasionally you'd get a big ding and
you probably come across this when
you're doing an important presentation
but there were things on machines that
have a graphical environment set up that
we can run real browsers and that makes
sound like there were updates to install
or your antivirus is out of date or
other things like that and this was
happening and causing our on our test
runners and causing things to fail so
good Diagnostics are really useful that
loads of other stuff actually I'm we've
run into problems with browsers
themselves crashing which you can
mitigate to some extent by capturing in
fact that's happened because it
generates in specific errors and then
restarting the browser but something is
much harder like we've run into graphics
driver bugs where the service were
running the tests we literally hit
graphics driver bugs hard looks the
machine and your tests fail which kind
of leaves me to think that that there's
it's basically unattainable I'm trying
to get these things perfect there's
always going to be something I think
even if if we're hitting graphics driver
bugs there's always going to be
something that's that's not working
we're experimenting with headless
browsers to get around that but headless
browsers don't support all the features
that we that were using at the moment so
at this point we've got a quite a big
test suite and we could have got a good
enough level of reliability from our
tests and we're dealing with the
remaining non reliability by treating
our tests like production monitoring and
then as our test Suites get bigger and
bigger we ran into other more nice
problems to have like we've got so many
tests that are now taking a long time to
run so I wanted to share a few tips that
we
found useful to keep these tests first
the XP book has the concept of a
ten-minute build so in extreme
programming there's the idea that your
build process your test suite should not
take longer than 10 minutes to run
because well that's about the length of
time it takes to cup of coffee and it's
about the amount of time you can
tolerate the maximum if you're going to
wait to something to happen and take
action on the results if you're going to
notice that the tests are failed and fix
it if it takes longer than that you lose
that feedback loop
we've been reasonably good at sticking
to that so you first it is probably the
most controversial and that is to make
these tests asynchronous part of your
process this is the thing that extreme
programming advises but as developers we
love making things asynchronous things
are slow and annoying let's make it
asynchronous run it in the background
you can orbit and it doesn't matter how
long it's taking and before you know it
your tests are taking 24 hours to run
and then you've got more problems but if
you keep it a synchronous part of your
process then a you can respond to the
feedback from your your tests failing
where they fail and you've still got the
context about why they might have failed
and you can respond to that but also it
incentivizes you to keep them fast
because you're just gonna get them
really annoyed if you're bored sitting
there waiting for things to happen so
you're constantly encouraged to invest
in speaking them up lots of simple
things you can do delete tests tests
have an inventory cost to keeping them
around if they're things that are slow
and they're not providing enough values
and why not delete them there's no
nothing special about test code really
I'm we talked about we were using tests
as production monitoring in that case
though some things that we don't really
want to be broken for a week but if
they're broken for a bit it's not that
big a deal and we
can just have them as production
monitoring and not have them as part of
our test suite if they're very minor
features home and then I talked about
stubbing dependencies out in the context
of reliability because dependencies can
be unreliable it also helps in
performance if you have to go halfway
around the world to fetch a resource
that's going to take time and and
stubbing out responses that are slow
speeds the tests up but the big one that
I guess if it's not a silver bullet but
it solves a lot of problems is
paralyzing and unless you do silly
things like having one test depend on
another generally these end-to-end tests
can be run in isolation and we run I
think six or seven at the same time on a
single node and you can scale nodes up
and then you get a speed-up I guess
there's there's nothing I don't think we
could stop you running a thousand tests
on a thousand different servers really
quickly I'm waiting for someone to come
up with a a complete way of running
these on Amazon lambda there were some
attempts there using a headless browser
there's some missing features but I
think we'll get there and yeah running
every test with a separate server would
get them really fast there are other
kind of nice to have problems that we
ran into you so if there was performance
there was also kind of test smells and
maintainability so this was commits
maybe halfway through our journey we'd
built up quite a range of end-to-end
tests against our kind of ads and when
we thought we were getting pretty good
at writing these tests for maybe a
little too smug and we decided to take
the same approach and apply it to a much
more complex web application and we ran
into test smells the the tests heard a
lot of repetition they were doing thing
it's like every test would have to log
in navigate around the app
and before getting to the place where we
could actually do any useful assertions
and I'll page objects were quite smelly
as well they're getting big and fat
loads of operations on every page
because there was a lot of things that
this app could do and that's when we ran
and we came across the thing called the
screenplay pattern anyone heard of the
screenplay pattern not very many the
screenplay pattern I believe was coined
invented by someone called Antony
Marconi and it's so called I guess
because it makes your tests read a bit
like a screenplay you've got actors and
actions things that happen but it's
really what you get if you take a test
written with a page object pattern and
you ruthlessly refactor it to the point
where you've extracted lots of single
responsibilities out that you can reuse
and compose here's an example from one
of our test tweets this is checking what
happens if a user fails to log in so
we've got a little DSL going on we've
got an actor a publisher in this case
and then we say that when they use a
browser and attempt to log in with the
wrong password then they have wrong
credentials message gets shown to you
them but the the things I wanted to
highlight and there's still some
boilerplate there and this was a real
example so it's not completely clean
this login as that's a unit of behavior
that we've extracted out from our tests
and we can reuse in other tests so for
this have incorrect credentials message
SI unit for behavior it's basically just
a function right and the real valuable
thing that I wanted to highlight is that
that we've got functions you can compose
functions and the great thing about
screenplay pattern is that you can have
these actions any level of granularity
so you can go low level like clicking on
something you can compose those up to
higher level things like logging in
and go up still further so with this DSL
you can do something like attempt to log
in as a publisher then attempt to
navigate to an earnings report then
attempt to view a saved report that
they've seen before and maybe we're
saying we're doing that in a few
different tests you can extract those
compose those actions together and
create a higher-level action called say
view earnings we just extracted those
functions imposed them and then we
attempt to a higher level action I mean
this helped us out with a lot of the
maintainability challenges we had in a
bigger more complex web application some
links the flick is a little DSL we've
built internally for writing tests in
this style various other people have
built similar dsls and then there's this
example project in the Serenity BDD
project on github that apply is this
kind of style of testing to a kind of
dummy application I mean it's got a good
example again I'll tweet the link to the
slides if you want the links later so
wrapping up the things I've talked about
there were a number of opportunities
that arose from our investment in
end-to-end tests it meant that we could
test it the entire user experience
including the user interface which in
itself is really valuable meant that we
could run the same behavioral tests on
lots of different browsers and check
that things work in all different
browsers him in that we could check
invariants things that should always be
true like sound is not on for us and we
could reuse the same test Suites for
monitoring our production systems have
some performance tips make the test a
synchronous part of your process if you
want positive encouragement to keep them
fast delete the things you're not using
stop your dependencies out your networks
that are slow and run the test in
parallel so you can learn more at the
same time and then I've talked about
some of our successes from the page
object pattern how it paid off that we
could fix some of the maintainability
challenges within just our page objects
without having to update thousands of
tests joke about the screen play pattern
how it improves on that if you
ruthlessly refactor your tests then they
remain more maintainable and then we
talked about a load of things with with
related to reliability if you can get
away without writing end-to-end tests
things are going to be a lot easier not
surprisingly we talked about the
importance of good diagnostics study you
can work out what's going on because you
get some really obscure things like a
sound and your browsers dying that are
quite hard to work out otherwise I
talked about waiting for things to
happen because timing is important in
these tests and we talked about treating
our tests like production monitoring
hollering tolerating a failure rate
that's acceptable to us rather than
expecting things will be a hundred
percent reliable because the
infrastructure we rely on is not a
hundred percent reliable that's it thank
you very much for listening
got a few minutes left for questions I
think if anyone's got any questions yep
did we test against different browsers
yes so that was one of the extra bits of
value that we got out of them and we've
been using third-party service a browser
stack for getting access to lots of
browsers to test accounts yes lots of
flakiness with relation to different
browsers browsers crashing some web
drivers more reliable than others yeah
some of it we found workarounds for such
as restarting the the connection from
scratch moving the test to a new
instance others are just part of the
failure rate that we need to tolerate
and decide what's an acceptable failure
rate to us any other questions nope okay
thank you for this thing I'll hang
around if you've got any more questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>