<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices Data Patterns: CQRS &amp; Event Sourcing by Edson Yanaga | Coder Coacher - Coaching Coders</title><meta content="Microservices Data Patterns: CQRS &amp; Event Sourcing by Edson Yanaga - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices Data Patterns: CQRS &amp; Event Sourcing by Edson Yanaga</b></h2><h5 class="post__date">2018-04-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bdBfbIiR95k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and first things first oh man
bucuresti
vomits cavity preemie was good enough oh
thank you very much for those that don't
understand Romanian so my name is s
vinegar I'm a director of developers we
said redheads Twitter on Afghan ago and
today we are going to discuss a beam of
some Microsoft's data matters especially
secure s in the vents or C and I chose
this topic specially specifically
because I believe there is some
confusion in the architecture community
when do you apply or what is these or
what is that so I think will be useful
to make some concepts clearer for for us
developers I'm also a Java champion and
the Microsoft MVP and I work for
redheads and I also happen to be
Brazilian Japanese which I bet I'm the
only one here in the audience so it
makes me a unique combination so you
want to see many Brazilian Japanese
around here I didn't notice any other
ones and this this talk the first step
of this discussion about distributed
data micro steps words I gave this book
so just in case you want to have an
electronic version of the book you can
go to this URL and download the free
copy or if you don't want to type all of
that you can go to my Twitter profile
dependent tweet has a link to the e-book
so this is the starting point of this
discussion but I believe that it was
wasn't enough
consider this talk occur a second step
around the discussion about distributed
data and I quote this from my book
called is easy state is hard because
most of the people when they're
discussing distributed systems they only
talk about behavior so it's kind of easy
for you to replicate behavior or speak
behavior between multiple nodes but
we've data is always much harder so
before we dig into secure as event
sourcing we need to introduce some
context so let's go back ten years ago I
think about how was data management 10
years ago if you were a child Java
developer you have to think what which
were the technology that we were using
ten years ago so ten years ago we're
still terrified about entity means I
don't know how many of you are still
using HIV 1.0 or 1.1 I hope not many oh
sorry
but you know that 10 years ago it was
much more common so we had like a bit
more than 10 years ago we had hibernate
to help us on this entity beans words
hibernate 3.0 was released in 2005 now
it's a long time Java 6 released in 2006
and JPA 1.0 was released with Java in 5
on 2006 - so it's more than 12 years
that we had this different technologies
to be able to monitor our data models in
our systems so basically 10 11 12 years
ago we were able to start replacing that
lots of XML configurations into more
annotations we have XML house some
people say that now have annotation hell
but you know just pick your poison we
always have to add this metadata
somewhere so most of the people these
days they prefer to add this metadata
into your code we also started to think
about Oh Jose
and with this purchase we created what
now we call an emic dummy model because
we only had the black as poachers we did
in that behavior this is this classes we
just we just used it that is containers
for data and this created some kinds of
into parents and at the same time ten
twelve years ago their architecture
community started to discuss about even
sourcing and if it's sourcing at that
time was in the widespread concepts on
some people start to apply discussed at
some conferences in some systems and I
of course I have to give a brief
explanation what is it and sourcing so
let me give you an example the
traditional way for us to think about
how can we model and store data in a
system it's like this we have an
accounts i'm although account like I
have an idea customer ID and maybe I
have a balance but you might think that
most banking systems they don't store
your money and then grow in a column
saying well you have a thousand lei late
right in your a balance and you have a
thousand late in your as the balance of
your account or 0 or minus five hundred
among overspent bit there and this is
the traditional way of doing crud
applications right you have this account
model your codes in your memory and you
store this data in an account table with
this rows and columns model like that if
you want to use them in sourcing you're
not having snapshots of your data
anymore because this model represents a
your your system state in a certain
period of time so as the as time passes
you can see that the the values on the
rows and columns they change
and with a bit sourcing we need a
different approach so we have immense
source systems instead in your have
snapshots of information like I look at
the system right now I know this the
state of the system at this point of
time we've been sourcing we think of the
system as a stream of events so we're
not hardening account balance anymore as
a single row and a column we're thinking
about the amount of money that I have in
a bank account as a series of
transactions so maybe instead of story
that visit value I can assume that all
of the bank account starts with a zero
amount of money and if I model like
depth and credit transactions if
everybody start to zero if I want to
know how much money do I have in my bank
account right now I just have to get
from the beginning of time and apply all
of the depth and breadth transactions
after the calculation I will have how
much money I have in the bank account so
event sourcing is a very nice
architecture because we have a very fast
writes need a - boots on right on your
system you usually would model as events
or see events or see also gives you easy
auditing because if you want to know
well it is the amount of money that I'm
seeing right now correct you just have
to start from zero and we applied all of
the transactions back again you can
check if the value is correct you also
have a free time machine I want to know
how much money I had one year ago at 3
p.m. on the last last Friday of March
you'll be able to start from zero and
apply all of the transactions until that
exactly timestamp so these are some of
the advantages of using an event store
assistant so but you might be thinking
it's not that easy for us to use this
even sourcing model in the system that
we've all do these days but one of the
advantages of using event sourcing
well one
or more advantage of using inventory
systems is that it enables you to think
about the events that happen in our
system instead of just thinking about
this state the current state right and I
am guilty to and I think that most old
people the system analysts the
architects they came from like a psycho
background we started and if you think
about like how we use its model systems
10 or 20 years ago if you think about if
you were talking to the customer well
let's try to model your application what
your application have is that I have a
customer a customer has an ID has a name
as an address has a phone then I have
the this this invoice invoice has these
and that the very first discussions that
we had we were customers when we were
modeling a system were about which kind
of types and information we had in our
data structures and only later we
started discussing well when I have a
customer this kind of thing happens and
I have to change the state to these and
that so I think part of the of the
problem that we have these days we're
talking about distributed systems is
that events are much more important but
we're still used to think about the data
structure of the problem first before
thinking about the behavior and some
people might advocate for that in a
distributed system is much more useful
to for you to fit first about the events
of the system rather than on the
structure of the data so that's one of
the discussions that we might have to
another important thing that I have to
tell you is about CQ s command series
separation I have this beautiful phrase
pointed by better hire the the inventor
of the aether language and the design by
contract concept to asking a question
should not change the answer right and
if you feel it if you read about the
secure s principle command kiri
separation it states that you your goal
you should have methods that you read
the information and you should have
methods that update the information you
should never have a method that does
both like if you like incrementing gets
which is basically a violation of the
disk us pattern so now you should have
its different methods for QE and
updating data and ideally you also
should have separate interface so if
your code in Java you should have an
interface group in all the right methods
and you should have a read interface
group in all the read methods probably
when you're coding your java class you
implement both interfaces in the same
class if you have a very simple model
but if you do separate your
inter-service interfaces to read and
write methods it would be much easier
for you to later to be able to implement
this read and write interfaces using
different technologies which is the
casing and that we're interesting right
now in distributed data okay so it all
started with seek us in your head if a
size the code again
and now let's start the discussion about
Sakura s command here irresponsibility
segregation
Greg Gunn was the first person to point
and to talk about this problem and it's
a very fancier problem for a very simple
work that you it basically states that
you can have different read and write
data models okay so you can consider the
dad's Sakura s might be an evolution
about cqs because if you do in sync you
guys properly you very likely are going
to have security architecture so just to
give an example if you have security
architecture traditional credit
architectures you have a customer you're
modeling customer you have a customer
class you have a customer table and the
palaces you want to write you just
populate a customer object and you ask
GPA or JP or Hammond a to write this
object to the database if you want to
give me your customer from the database
you just create security and it
retrieves all the customer object into
memory and you can perform your
operations in Java this is the simplest
possible architecture and it's the most
appropriate one for like 90% of the use
cases but sometimes you might have some
issues if you using credit and if you
have this kind of problems you probably
want to have a secure a circuit at you
so how do you model for example here if
I have a customer by the name foam
address and MuRF
I have many rows and this columns in my
system if I want to insert something
like product entry or just generates a
super statement insert into customer and
I passed
each one of his columns if I want to
give me this information as I just
select everything from customer and I
get all of this data but you might think
that customer the customer table evolves
over time so here is a very simplistic
model but you can see that the reward
customer table can have a lot of data
and can have a lot of dependencies so
you're select everything from customer
might become expensive right and you're
going to populate a lot of things in
memory just to use maybe just a couple
of fields so then somebody comes with my
requirements I need a report and this
report only requires for me like an ID a
name and the phone of a customer
you see customer has a lot of formation
but I need to create a report with just
this free fuse what you do for
performance reasons you create a custom
curri curri only this information you
just retrieves information memory very
likely if you zjk or hibernate you're
going to create like a customer DTO
which will just hold the information for
you to create your report and we're done
we solve the problem and congratulations
to us we created a secure s model
because we're still writing using a
customer class but we're reading using
the customer DT real class and if we if
we permitted that correctly we also use
its separate methods and separate
self-service interface for that and this
is a simplest possible security
architecture that we might have in our
system and another day somebody comes
with an hour requirement I need a
different report we did three different
fields you just create a select ID name
address from customer to create another
DTO you generate your other report use
over the problem but that's the simplest
possible security
that you might have then after that you
realize that you're not constrained into
having a single source of information on
the database you might have separate
data stores for writing and reading and
this is by far the most popular denture
that we have in the distributed data
word we will have security architectures
with separate read and write stores so
we're going to write our information in
a certain data store and we're going to
read information from a separate data
store so many people think that well I
want to create a micro service
architecture and the typical use case
that are being run into many different
companies and teams like I decide I want
extract a piece of information from a
monolith I extract this information like
for example if customer again I create a
customer micro service i strengthen
customer information the customer tables
and relationships because I'm doing
everything right in my monolith and I
have a customer I do all the data access
to my customer informations for Dao so
what do I do to keep everything running
properly I create a customer micro
service with a rest endpoint I change
all of my Dao information to perform
HTTP queries with an enter to the remote
endpoint there everything work will work
fine by the board that production it
fails miserably because you have a high
latency your everything's as low well it
doesn't handle the demands so you think
well it's low that's a performance
problem right how do you solve a
performance problem you had cash
you start with an eternal cash you just
add a map in memory to your long list
well now it's faster but not fast enough
then you decide that you need an
external cache so you create a key value
store you store the things there and now
it's faster but then you have another
problem because customer Dao wasn't the
only entry point to your customer data
so now you have a lot of different
reports in your system that use it to
join the information from customer in
the database and on you have tables and
you have objects in memory and you have
to join this information with codes then
later to decide well that's not a very
good approach why don't we create a copy
table of the customer data that I need
on my monolith database and I still
right here on the customer micro service
but I find a solution to replicate the
data from here to there congratulations
you just realized that you created write
that store and a retail store in a
secure s architecture so the discussion
that we're going to have right now is
which techniques and technologies we can
use to replicate the data from the right
the restore to the read the restore this
is a typical use case for most
implementations of distributed data and
microservices architecture or simply
distributed architectures right so but
distributed data is not the only
application for secure s like maybe you
want to generate a report and this
report has some complex aggregations or
you have to join like multiple tables
and you have a large frequent access
that maybe you want to improve the
performance of your system and what do
you do you create a view so when you
want to generate your report in memory
you create a query against this view
right a view is a super s read the
restore because you're still writing to
that table and you're reading urine from
a
from another source so that's another
form of cigarettes read the distort
right so you can use views you can use
materialized views and this is mainly
the discussion that I had on my book but
we can't discuss further steps right now
so we know what is events or see we know
what is to us and against aggressive at
sourcing why am I discussed this day
because they couple very well usually
again it's not a true absolute truth but
usually when you have an event sourcing
architecture you also have a secure s
architecture and when Gregg young
pointed the term security architecture
is dead we had a great model in the mind
of all of the architects and they wanted
to introduce him in sourcing but it was
so hard for people to grasp it and
sourcing properly that they decided well
maybe we need an intermediate step
between having a correct model and an
immense source model that's why they
created secure s but secure has proved
to be such a useful solution that we use
it for many different user case these
days right so first step was to make it
as an intermediate step between both
events or single bread but now we use
for many different things one of them
simplest possible applications for
secure s and events or see I will use
the bank account example and maybe I
chose to use image sourcing to model the
amount of money that I have in my bank
account with debit and credit
transactions but you might have guessed
it it's very fast for writing but there
is no for reading so every time I need
to give you how much money do you have
in your bank account I have start from
zero and start adding and subtracting
the value steps and credits and the more
transactions I have and the more
customers I have the more slow the
system gets so what can we do maybe we
can create a secure s read data model as
an account and this account has a
balance too but this balance
on this table is not the true source of
information is just some cash ins then
because the truth source information is
a student event source but whenever I
write a transaction in my transaction
table I can synchronously or
asynchronously update the amount of
money that I have in my balance in
behind account table right so you might
have similar transactions might have a
synchronous transactions being updating
the balance your bank accounts but if
you want the true source of information
is always the event log but you carry
against your balance for performance
reasons right so typical use case of
security architectures performance that
that's why sometimes you go to your ATM
you withdraw some money then immediately
after you just get your mobile phone and
cue how much money do I have my bank
accounts and called the monies - there
don't worry the back never loses money
but you just got some delay between
writing to the transaction log and
updating their secure s create the
restore so that's how secure estimate
sores are very tying together
so here the transactions are the right
model and the account is the real model
so let's discuss a bit more why we
should be used to us first discussion
was performance I wanted to use
cigarettes because of performance and
that's the main discussion of Albert
secured a the past 10 years but as of
2018 we have other reasons for creating
cigarettes architectures we can use
cigarettes architectures for
distribution
to the main use case for Microsoft's
architecture distributed systems because
we want to make the data available in
order and points in our system and we
can't just simply rely if I just decide
that different my custom information is
going to be stored in this micro service
if this micro service goes down nobody
can curate the customer information
right so first we need to make this data
available across our our nodes so we can
we need to distribute then we can have
availability because I don't want to
tolerate like I need to tolerate
failures here in my my endpoint I also
need to use secure s for integration
because if you want to distribute your
data you need to find a way to integrate
how your writes and reads or all of the
nodes in your system and another thing
that before discussing this day is that
whenever I have distributed read they
store and I write the restore the
traditional data is read that this
remote with data models is to create
events like low-level events for example
a third update delete and the scallion
may be out alter table or modify scheme
or something like that so you generate
these kind of events and you don't
propagate the world data you just
propagate the changes in your write
model so whenever you insert a record
you created insert event you propagate
that and the remote with the store gets
this event and updates their local cache
right so whenever you create these kind
of events you might be thinking well
whenever I have my stream of events
add some analytics on top of that that's
that's why people are using message
brokers and real-time analytics systems
to extract these events from this event
sourcing architectures and generate some
useful statistics for the business and
let's assume enterprise information
system the most common using case is you
only have one write node in your system
and I call that the canonical source of
information so if a customer
microservice they're only at the point
of your system that is allowed to change
information is the customer microservice
right and you just replicate and
distribute the data through this
different read data stores and you might
be thinking customer information again
can be big you only replicate on your
read that is stored the data that you
need to be processed on the remote end
points so I might have different views
of your information on your read
datastore it also changes the way that
you think about the technology that you
might be using to be creating this read
data stores so you have write the
restore you will generate your events
and you propagate them to your reader
store to create this replicated data
some people call that replicated data
some people call that secure as with the
restore ajiz like I'm saying right now
some people call that caches it doesn't
matter is the same concept and we're
going to discuss right now some of the
requirements that you might have for
distribution and some of the technology
choices that you might use for each one
of these requirements
because different choices of Technology
depending on the requirements and these
requirements might be latency signs
staleness
ownership security and time I put a
question mark there because I don't know
I still don't have a good name for this
this kind of requirements but maybe
somebody can have an idea and later give
it to me okay lekha see the question
about ladies is how long does it take to
propagate the change or in other words
how long can you take two responses
change and when they say latency says a
change happen in the canonical source of
information how long does it take for
that information to be propagated to me
does it need to be near real time like
milliseconds or can I afford further
minutes one hour and some people believe
that I have a very strict latency
requirement because whenever something
changed there I need immediate changes
in my remote interest or but if you
think about the word that believe is not
usually the case whenever for example
suppose that you when you want to
generate the reports if you're kiri it
takes like 15 minutes to be to get a
result the result that you're getting
when you see the report is already 50
minutes updated if you think that most
strategic business decisions are taking
like we've report generated in the past
night using a DI to like people are
using the data from yesterday to take
strategic business decisions so most of
the world that we leave is already your
eventual persistence and you might be
thinking about different about your
latest requirements because most
applications can afford a reasonable
amount of time between updates and get
it is updates on your read
but it also implies different
requirements of the technology science
is another discussion that we need to
have how big is your data set and how
big are the changes in your diet data
set maybe sometimes you change your data
only once a day but when you change that
it's like one gigabyte of data that is
changing every time so but maybe you
change your data like every second and
you need to propagate just one kilobyte
of changes into your remote read the
restores so it is another discussion
that we need to have stay honest how
often the data gets changes which is
different from latency lay a disease
data has changed how long can I afford
to wait for the update and stableness is
how often the data gets changes okay so
update it there I can I can't stand like
half an hour to get the updates or the
data gets updated every five seconds
stay there stay honest ownership who
owns the source end point of information
is it you is it your team is it your
company is it a different company it
also indicates different kinds of
technologies that you need to have to
implement secure as with their mottos or
the discussion of these security can you
expose the all of the information that
you have your read/write data model to
the public maybe you might have a
security breach so one typical use a
place for security is that well I need
to expose the customer names in a
website okay it's authenticated it's
behind the faro everything else but I
don't want to have the liability is
something breaches in so maybe here I
have like social security number
information password and something like
that so I have these separate
environments which is secure
and I just replicate the information
that can be made public to another
remote read data store and I made these
web server q it is remote read their
store so
don't have like any liability concerning
this kind of information so security is
another discussion and last one time do
you need all of the events that are
streaming through your bus or do you
need only the latest information for
example if you have an IOT application
that is managing the temperature do you
need all of the temperature changes or
you only need the current temperature in
your sensor to to take an action so it's
not a decision but if you're using a bad
system you have transactions depth and
credit transactions you certainly want
all of the transactions even if they're
late and most likely you want them in
the proper order or if you change in
life customer I changed the name of the
customer using the right data store I
want to have this information propagated
and change it on the retail stores in
this case if I'm changing the stage of
information which in which the source is
not even sourcing I want to have all of
the events because the order of the
events matter in transaction was their
order doesn't matter that much because
they are specific they are CR the T
constant with free replication data
types which is a specific term for
distributed systems with transactions
they are CR DT but most of the surprised
use cases they're not so order matters
so you need to think about this when
choosing your implementation technology
so now I'll discuss some different
technologies that you might use for
solving your distribution problems first
use case in memory data grids and I'm
saying in-memory data grids and not just
playing key value stores because you
have like different products in the
market then solve this this problem in a
different way and I'm saying memory that
did a great spooky because data grids
can perform some computations on the
notes too and in this case they differ
from plain and simple retail stores so
how can he never did agree to help you
to create your secure
with the restorers on the remote notes
first in memory data grids allows you to
share your information so maybe you want
to share your information base it on
some geographical and a condition you
want the users from Romania to be stored
inside Romania so you want a shard here
so the cue is candy fester and you don't
want to store our users from Brazil
because nobody's going to curate
information here or at least not that
often
ok so you want to sharp this kind
information to in memory type that
agreed if you really if you really have
like low latency requirements you need
the information to be updated very fast
of the remote notes you want to use any
memory data grid because we can have
like very fast distributed transactions
you write because everything is a memory
you write here and it gets propagated
almost instantly depends again on the
latest sort of network but that's
something that we can control so the
fastest approach is in memory data grids
you can also have some real-time
analytics if one performs perform again
you want to have updated results from
analytics information that is updated
very quickly maybe you want to use any
memory data great because again it's a
memory it's very fast and another
feature of memory that it reads that I
think there is very nice is that
traditional way for us to process
enterprise information is that I have an
application and my data is on the
database I creates a QV I fetch all of
the information from my database through
the network I get that in memory I
process the results and then I write
something back to my database so I'm
getting the big thing which is the data
moving through the network processing
and sending it back and the more latest
you have from between the endpoints the
worse is your performance so remember
that accretes they have something called
contiguous queries which allows you to
instead of indexing the data and
applying the QE you did different thing
you index the QE and you applied that to
the data
you get the concept you don't need some
index the data and the database you
create indexes and you apply the curie
with continuous queries you do the
opposite you enix the curie and you
apply that to the data how does it work
well maybe they show fetch of the
results from your QE can be big so you
have to move that your application but
the next results you just disappear well
this QE is looking for information from
customer and is looking for changes in
the customer name or the customer
address and the customer phone number
so whenever any kind of data in your
grids changes these free values you get
a notification and the data grid pushes
the notification to your application so
you're only moving the data that changes
there's the typical application of a
memory database okay you have a huge
data set you haven't charted from memory
and you won't want to be carried because
traditional way for you to know me if
anything change it used to be balling
your database anybody ever created at a
chrome trigger with a select QE against
the database you do that every five
seconds you get information every five
seconds and maybe nothing change it from
the last cue with continuous queries you
say I want to be in format whenever this
data set changes you get the immediate
notification so this is a very nice
application if you read if you need very
long latency updates and another the
nice thing is distributed processing
again traditional way is fetch the data
from the database application processing
and getting it back and you might be
thinking that in this use case the data
is very big but my code is very small so
maybe you're not too in the right way
why don't we get this small part and
over to the data instead of doing the
opposite so a memory denigrates usually
they have
their capability of distributed
processing so you can have your data
shard it across the globe you create a
custom code in Java or other technology
depending on your product or project you
just created this small code which is
very small like one kilobyte two
kilobytes of size you send this code to
the grid the nodes of the grid process
the Kiwi and return for you just
aggregated results that's another very
nice application of in memory data grid
of course I'm showing you if you expand
is an open source project that
implements the the memory data grid
another way for you to create your
secure s with the restore your
distributed secure s with the de stores
and you might have to forgive me a bit
data translation doesn't truly
distribute the data because you still
have a like a centralized and databases
and where all the information is stored
but many people told me that they are
translation is especially useful when
they're trying to create their very
first secure s read their stores why is
that if you get your monolith your data
and you want to split that information
to separate endpoints it's very easy for
you to make mistakes and after you've
done that mistake in the physical
database it's very hard for you to get
everything again and be able to break in
the proper way so data localization
allows you to create virtual databases
that can be read through the remote
endpoints and if you have like a high
latency between the endpoints for your
central database most virtualization
solutions already have some kind of
cache which means that you create a
secure Esprit they store and they were
transition solution creates their own
cigarettes with the restore so you have
lots and kind of interception there but
if you want to solve this distribution
problem
virtualization solutions can that cache
so you can solve it and data is
replicated availability is guaranteed
again because you're creating a cache on
the other side and it allows for easy
it's not usual to see people using daily
realization but I was able to talk to
three different teams one here in Europe
and Jun us that were using data
transition solutions to them we're using
deeds or j-bars data fertilization the
product one and another team was using
the Cisco solution for that
and they told me that they were very
happy that they use a data
virtualization to create this secure as
with the restores and after some months
or years of using this distributed
secure as with the restore fruit data
utilization
they said they they concluded that well
we need it right now we're going to
physically separate the data but then it
was just a matter of splitting the data
and cutting that virtual database okay
that's why I I usually suggest people to
have data virtualization I've never seen
a tea stalling every 12 is a shoe
solution just for that but the three
teams that already had a per to this
solution we're very happy that we were
able to play with the database before
doing any kind of physical speeding okay
another technology that you'll probably
use when creating your secure as with
the restores our message brokers and
again you have might have different
kinds of message brokers you for using
JMS you might be using ActiveMQ or if
you have other kinds of requirements
you're probably going to be using Kafka
which is like everybody's favorite
today's you might be seeing a lot of
talks talking about Kafka because Kafka
has some very nice features and where
should you be using like a JMS topic if
your data is always stale like for
example an IOT sensor measuring
temperature or else
camera trying
revolt process with the camera trying to
count the amount of people in this room
it's always counting and sending you the
information so you can simply discard
the previous information or if you miss
the last message doesn't matter for you
so Jen messed up might be good for you
so if your data generation is high and
you always receive an information you
choose a JMS topic another use case is
stock trading information you're
guessing you always get new stock price
so it doesn't matter the old value but
if for you persistent is important and
you need order delivery that probably
should be using Kafka and also one
person at least complaining to me well
Kafka is not truly ordered Kafka has
ordered guarantee order guarantee on the
same partition right so you have a big
cluster you might want to partition your
data like these specific queue is going
to this partition so if this partition
is guaranteed to be ordered and that's
the use case for example when you have
customer updates insert update delete
these kind of things you need these
events to be ordered when you process
them on the remote end points and Kafka
is a favorite solution for using this
data great its high performance it's
distributed it's ordered and it's
persistent right if you have a big
enough disk you can't store your message
forever which is not typical typical use
case for an enterprise but you can have
discs enough for a story or message for
like one week or a month the bets on how
much time can you afford for your remote
endpoint to be unavailable right if your
remote endpoint can be one week off then
you need at least one week of persistent
storage for messages so it greatly for
eyes depending on your use case another
interesting technology I didn't think
about that initially but then I visit
the team in France and he told me this
very nice use case that they were using
platform for creating this UPS pretty
store that's why I decide to buddy here
so suppose that you won't use a reactive
platform like vertex or other platforms
you have you know this the current word
current world you have like you don't
have just a single instance of your
application running your back-end now
that everything is containerized then
you probably run your application on
Canadians or rubbish it or something
like that
you have multiple instances of the same
service queueing some kind of
information and suppose that you hire a
service from a third party and you need
to all this information through HTTP and
now you have like ten different micro
services each one of these micro
services have like five different
instance running on that so you have
like 50 different instances queueing for
the exactly the same information to the
remote endpoint but of course this
remote endpoints if you've hired a third
third-party service they charge you or
thousand Curie's so you used to have one
remote that point it was good enough for
you to queue it information but now that
you have 50 and everybody's queue for
the same formation the view is going to
be very expensive so what did this
company do and then of course
that's a very smart solution they
created reactive point well that's a
that's an end points which may mix the
remote API they just happen to demented
to implement it using vertex because you
get very fast changes and it's a very
lightweight solution for for high
throughput architecture so basically
what they did they mimicked the remote
API and all of the internal endpoints
they curate this reactive gateway to get
the latest information and the reactive
gateway it q is the the remote end point
you can configure the boiling interval
and what that what they were able to do
is that the reactant point is still
using all e to the remote API so they
need to configure that how how how often
they they can afford to kill that or
much time they can afford to wait until
you get an update but after they they
created here a reactive end point they
were able to simply well now that
everything is internal I can have a
message bus through my endpoints instead
of having my endpoints to give me the
information I can propagate the changes
from the an event bus right in this
particular case they were using the
vertex event bus but they could be using
the regreted information in a JMS topic
or kafka bus to originate again it
really depends with your request but I
found it very interesting that they were
using this just to save money on the
remote API because it might be the case
for many of you you don't want to insure
with your money or maybe your boss
doesn't want and less technology that I
want to discuss this change that data
capture most developers are not aware of
CDC change their capture but it's a load
time favorite of many operators and DBAs
what is change data capture basically
when you have an endpoint in which you
do all the the right endpoint suppose
that you have a legacy system and you
have like multiple legacy applications
that are reading and writing from the
same tables so it's not easy for you to
just well let's rewrite this part of the
application and start sending events and
even sourcing so my distributed bus
though maybe it's written in COBOL and
you don't want to mess with that so you
have this kind of legacy application so
in this particular case there is this
possible way for you to start generating
events to replicate the data is to plug
directly on the database so you can use
a CDC solution you plug the driver
directly on the database so you start
reading directly from the database
transaction log for change events so a
very nice open source fault solution
that implements the CDC pattern is the
vision the vision you might seeking I
think I think the name is very smart
because they started from TB and they
wanted it to sound like a chemical
elements so they are added the easy
so the Bisou it's an open-source project
the bc of iron
currently it supports bicycle post
bicycle MongoDB Oracle support is about
to be final but I can't promise anything
because years and they took that
somebody might have told me that the
next one on the line is single server
support so you just plug division on the
database you start reading from the
transaction log and it propagates the
message from Kafka bus so your remote
read and points can start consuming
these messages and updating the read
data stores anybody ever use it Oracle
GoldenGate one which they some people
say it's very expensive I never had the
pleasure of using myself but or go go
the gate does much more than that but it
also can be considered as part of it can
be also considered a CDC solution
because Golden Gate is a very big
solution ok and this is the information
that I wanted to share with you all of
this information is or will be available
at the developers don't read HazCom
website I would love your feedback on
anything that I said or any new use case
that you might have for me and the
easiest way to reach me is from my
Twitter at the Onaga I also have my
email Anagha
at redhead calm but usually I will buy
faster on Twitter thank thank you
I forgot my Romanian so I could</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>