<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Three years of breaking things to make them better by Donny Nadolny | Coder Coacher - Coaching Coders</title><meta content="Three years of breaking things to make them better by Donny Nadolny - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Three years of breaking things to make them better by Donny Nadolny</b></h2><h5 class="post__date">2017-04-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bEGinWILidY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone I'm Johnny I'm a developer
at pho duty and I'm one of the people
who run the failure Friday exercise then
I'm going to tell you about what failure
Friday is how you can get started
running it and some of the new things
that we've done with it so we're right
after lunch some of you may be jetlagged
or tired so I'm going to start off with
my conclusions number one failure Friday
is awesome you should do it number two
when you do do it don't automate it at
least not right at the beginning and
number three when it starts to get
boring switch it up and add a new things
to keep it being exciting and to keep on
finding new things so can I get a show
of hands how many of you have heard of
Patriot II pretty much everyone okay for
those of you who don't the very short
version is that usually you have lots of
different monitoring systems all those
connect up to page duty and one may
detect a problem they'll send an event
to us and then our software will call or
text or email you to let you know that
you've had a problem so you can fix it
quickly one thing is that we care a lot
about high reliability because you guys
rely on us when your software is down
it's very bad when we're down and we
can't tell you that you yourselves are
down so because we care a lot about this
we put a lot of effort into making sure
that our own systems are reliable so I
want to talk about that for a little bit
one approach that you could take when
you're trying to make reliable software
is take one machine get really high
quality components get a really nice
machine you can put it in a data center
like this which is a data center that is
built in a repurposed nuclear bunker so
that you can survive whatever
environmental catastrophes you have but
of course the downside of even if you
put it in that you're still at the mercy
of network connections if you have any
problem with network connectivity it
doesn't matter that you're one special
machine is up if nobody can actually
connect to you your service is still
down anyway
there are cables fiber-optic cables
through the ocean that transmit a lot of
the data this is one of one of those
cables and this is a shark attacking one
of those cables so that this actually
has happened a few times in the past
sharks have gone bitten fiber-optic
cables and caused network disruption as
a result of that now in reality this is
actually extremely rare most of the
network cable disruptions are caused by
things like boats dragging their anchors
along and destroying the cable rather
than shark text but still failures like
this can happen and the best way of
dealing with them is just to accept that
they're going to happen embrace that
failures will happen and design yourself
where around that expecting that well it
will happen one of the exercises that we
do at page your duty to to try to deal
with this is an exercise we call failure
Friday so your friday is a fault
injection test that we run against our
production environment so false
injection test being things like
rebooting machines or using iptables to
cause a network partition and we do all
these things against our production
environment now there's a lot of fun
sayings on the internet about testing
and production like I don't always test
my code but when I do I do it in
production or everyone has a testing
environment some people are lucky enough
to have a totally separate environment
to run production in and failure Friday
is not about this kind of thing it's not
about testing and production because you
have no other choice it's about testing
in production because you're confident
that your software will work and you
want to verify that and production is
the true test of whether or not you can
actually handle these kinds of folks so
I mentioned before the fault injection
test portion these are the basic attacks
that we run against all of our services
all of our machines so stopping and
starting a process suspending and
resuming a process network isolation
things like that
on this one I have the commands that we
use to run these kinds of things so
basic things like just stopping a
process waiting to see what happens
making sure that other machines take
over another one so suspending and
resuming one killed I've stopped this
will leave a process still running but
it won't get any CPU time so it won't be
doing anything this is good for finding
things like server-side timeouts that
should be there but aren't for example
if you have an application server that
has a database connection maybe it
starts a transaction and starts doing
things if you do kill dash stop then you
have paused it there but from your
database service perspective it's just
not getting any new information but the
TCP socket is still open so if you don't
have any kind of server-side timeout
then it will stay like that and your
database might have tables locks or
other things like that so doing kill
Jeff stop and then waiting would reveal
those kinds of things another very
simple one is just rebooting a machine
and making sure that everything comes
back up as it should another is
simulating Network isolation we do that
just using iptables in our environment
it's a little bit easier because we use
IPSec so we can block just a couple of
ports that all of the encrypted traffic
goes through and that will leave ssh
unaffected but it will block all the
other stuff but in your environment you
can do basically the same thing just set
up the IP tables rules to allow ssh or
whatever other special ones you want to
allow and then block everything else and
the way that we run this kind of
experiment is will first run it on one
machine like one application server and
then after we've done that and we
verified that things are fine then we'll
move on to doing it all the application
servers in one data center or in your
example it might be in one availability
zone whatever your unit of fault
tolerance is and then another one that's
really fun adding network latency and
packet loss so this command will add 500
milliseconds of latency plus a random
amount from 0 to 100 and
twenty-five percent packet loss on top
of that this one you might think that
it's not doing too much different than
network isolation but actually it's a
lot harder to to survive the network
isolation is having a slow machine is
usually worse we have a slow machine
sometimes it can still connect it'll
just be flaky rather than being just
hard down where you can easily detect
that one thing that you often find with
with that last one of adding back loss
and network latency is clients that
should have had their timeouts soon so a
lot of the times when you're using an
HTTP client or any other kind of client
like that you just leave the timeout as
whatever the default of the library is
and you don't pay much attention beyond
that but if you start adding this kind
of stuff network latency then you'll
find that it really does matter what
time out you have it can affect the
throughput of your service a lot when
you have this kind of network latency
you also find that it pushes you to use
a little bit of a smarter client one
that might have built in health checks
of the servers that is trying to talk to
rather than naively talking to the same
server even if it's going to be slow
let's be on the on the opposite side of
the one that you had added packet loss
and latency too so the the sailor Friday
exercise that we run the people that we
have there are usually the on-call for
whichever service it is that we're
targeting then usually some of their
teammates particularly the newer ones
it's a good way for them to learn also
having an ops person or an SRE person
there just generally useful when you're
doing this kind of experimentation we
also run this similarly to our outage
calls so we have one person their role
as incident commander they're the one
making the decision about whether we
should move on with it or whether we
should cut it short and not kind of
stuff we also have someone who again
this is like how we run our outage calls
they describe they just record what
we've done at what time to make it a
little bit easier to figure out if
things have gone
wrong or things do look a little bit off
what it was that caused it especially if
you're going back a little bit later you
don't have to go through logs to
reconstruct what things people were
doing and we just use a simple a
slapshot room we have a Google Hangouts
call or a blue jeans call going on to
coordinate and broadly we just follow
the plan that we had come up with before
you you do one experiment you wait for a
few minutes and see what happens and
then you undo it and bring your system
back into a healthy state after so
before that was some of the basic things
that we do to everything we also have
some kind of more unique one larger
scale ones that we do one of them is if
you have a disaster recovery site you
can use this to ramp up traffic there so
most of our services are run in multiple
data centers they're clustered we don't
what we have all all data centers are
active but for one of our older services
it is still in kind of like a main data
center but with a disaster recovery data
center for failover and it's not a true
passive system because we we have it set
up so that one percent of our traffic
goes to our dr site all the time but we
can take a failure friday and we can
ramp up traffic to a hundred percent to
make sure that there isn't anything
we're that we can basically still handle
that otherwise you just have this idle
data center and you wouldn't really know
that it would be able to be used in an
actual disaster another one is again for
most of our services we use cluster
databases stuff like Cassandra but for
one service that the same one is the
first one it has a master at least for a
lot of the traffic and we use failure
Friday to failover that master to make
sure that we still can another fun one
for us is taking down one entire data
center we expect to be able to handle
this and so we can use failure Friday to
test that very slowly take down a data
center and make sure that everything is
still going find in your environment
it might be one availability zone or
again whatever whatever kind of faults
you expect that you should be able to
tolerate there's a lot of benefits that
you get from running these kinds of
exercises there's a lot of the technical
ones like finding out machines that when
they reboot it processes don't come up
or we found a few times now a single
machine that you can add latency to and
cause many others to to fail because of
that but there are also some other more
human benefits like training new people
getting them comfortable running
commands in our production environment
in an outage you might have to be
running stuff on production anyway it's
a lot better if you're not nervous doing
that because the first time you're doing
it you instead have done it many times
in failure Friday before another big one
that the second last one making sure
that you're monitoring and alerting
works well we're running these
experiments we're watching what alerts
come in and making sure that if things
should be paging us than they actually
are you're monitoring and alerting is
usually something that's pretty hard to
test in like a unit test way especially
for something like when when this one
service is being slow I want to get
alerted for it a good way of doing that
is just to make it slow and make sure
that you actually do get alerted another
good one is learning useful dashboards
that people have I don't know about your
companies but I mine we have lots and
lots of dashboards too many to keep
track of during failure Friday when
things are going on people are pasting
and graphs from the most useful ones
showing what's going on and then you can
learn from that to see what the actual
useful dashboards are during an outage
my advice for getting started on this is
number one again don't automate it at
pagerduty we sorted this about three and
a half years ago almost four years ago
and it's only very recently that we've
started adding automation up until then
we've just run commands and annually you
ssh into a machine and you do that there
I think it's very easy to follow into
the trap
we're all developers you hear about a
cool idea you want to do so you're going
to write a bunch of code to automate it
you could take a day and have a really
nice automation scripts for doing this
kind of testing but then if you haven't
gotten people to buy in that it's a good
idea to do it won't get used it's also
safer to do it manually at least in the
beginning whatever script you come up
with won't have as quick fall backs as
you just going in and undoing what was
what was done that caused the problem
the other thing is that you should pick
reasonable things that affect your
environment so a few of them are going
to be pretty common things like
rebooting a machine that can happen to
anyone but the other stuff about about
network isolation or a pack of awesome
latency the the unit of failure that
you're using you should tune to whatever
it is you expect to be able to handle if
that's one availability zone sure if
it's one region and do that but tune it
to whatever is reasonable and also when
up shirt
the question was how do you build these
recipes for failure Friday we have a
basic set that we do for pretty much
everything the ones that I had outlined
stop a process reboot Network isolation
but on top of that you can also add
specific ones for whatever service you
have but you can get pretty far just
with a basic set of things that can
happen two machines essentially when oh
sure another question how do you make
sure it doesn't impact production so I
will finish bullet point to and then
move into that which is when you first
start doing these kinds of experiments
they're riskier so you should be testing
in a staging environment first ask
you've done them for a while and you're
more at the stage of just kind of
keeping up the status quo making sure
that you still fulfill the guarantees
that you have you you can probably skip
doing it in staging first so when we are
first doing these when we're doing
something that I would consider risky
we'll do it in station first but now as
a general rule we don't do it in staging
first we do and in production and if if
you do have production impact you you
can't guarantee that you won't have
production impact from that it it does
happen and has happened to us but it's
much better to have it happen you know
early in the afternoon during business
hours everybody is awake everybody is
paying attention a lot of people are
monitoring dashboards manually so in
some ways you have better alerting than
you would otherwise if you have your
automated alerting and humans watching
dashboards and on top of that you can
generally undo what you've done really
quickly so if you do have an impact from
adding network latency for example as
soon as you see that undo it or wait for
a little while to to verify what the
impact is if it would settle down after
maybe health checks would kick in and
stop using the slow server or you could
undo it but broadly you will have some
impact if you keep on running these
eventually the pitch is that it's worth
it to do it during business hours and to
find out early while you can
while you can undo it and then go and
fix it asynchronously like create a task
to go and fix the underlying cause of
that problem any other questions before
I go on cool so that the last thing is
that it can be helpful to track whatever
results you have from this kind of
exercise in whatever ticket tracker you
use use a label or a tag or whatever and
you can use this to justify the cost of
doing this kind of exercise especially
if it does cause any kind of down time
you'll be able to look back and see all
the things that you've caught before
they had an impact or if it isn't
catching things for you then maybe you
don't need to do it but by keeping track
of what stuff you found you'll be able
to figure that out so ideally your
failure Friday should look like this you
you come up with your plan run it in
production and everything goes well
ideally with the less less celebration
than that it should be the normal thing
that that it goes well and for the most
part it does I like that good so that's
kind of the the basic thing that we've
been doing for several years we've also
added a few new things to this exercise
over all the the basic tax that we have
from 2013 around the middle of 2015 that
I showed earlier these apply to pretty
much any service and now we still use
the exact same ones for the basic set
that applies to everything we haven't
found a need to extend beyond this some
other stuff we've done though is adding
on kind of a game-day type thing what we
wanted to do was improve our major
incident response and so the idea was
that we planned out an attack that we
ran in secret actually I should mention
for the failure Friday's that we do
normally the plan isn't secret we have
it in a wiki page it gets sent out in
the calendar invite anyone can look at
exactly what the plan is
and people do look at it as we're
following through with this one we tried
it something different we tried doing
something in secret so I prepared a
change that made it look like one of one
of our services was having a major
incident even though it wasn't and it
had no impact and I had a co-worker
deploy it and then the idea was that
people would respond and then we would
have kind of a retrospective after just
on our incident response so after we
have a major incident we always do a
post-mortem anyway but usually that
post-mortem happens a few days after the
incident and the focus is more on the
technical challenges that we had if
there was any really big communication
challenge then we would talk about that
as well but it's easy to forget after a
few days so with this we were doing it
and focusing only on the communication
because the technical change was just
some little change that that's someone
deployed and this one that it was the
first time we done this and it was kind
of okay it it worked out a little bit
weird where it was kind of this half
surprised thing where we did it during
failure Friday but people didn't really
know what was going on and what happened
was people got the page and then nobody
really responded to it and then a few
minutes went by and people still want
weren't responding and eventually what
happened was someone who wasn't in on
say Leah Friday came in to the room and
saw that we were having this and then
started getting people you know what are
we doing to investigate this and then we
went from from there normally but the
thing is that that never happens in our
real incidents nobody ever gets paged
and then ignores it and like throws it
away I think it was just something kind
of weird where it was this half
surprised thing and maybe there was kind
of a bystander effect of the first few
people didn't respond and then everybody
took their cue from them and didn't do
it so if I were to do this over again I
would either do it completely as a
surprise or more likely just be more
upfront and say that where we're going
to cause an incident respond as normal
and then we're going to talk about it
after and see what we can do to improve
another one that I that we ran was be
caused a small database issue for this
one we did cause a real issue just one
with minor impact but right before the
page hit I messaged most of our op team
and told them that they weren't allowed
to respond and that other people had to
so for this we actually did have a
pretty good run book for this particular
incident but there was still some value
in other people going through and
running through it in this case the
runbook was just run this command on the
database server take the top result from
here copy and paste it into this other
command and then you're done but it took
a long time to get it done because
people were kind of uneasy about it I
think with a lot of run books people
often forget to add in the little things
that you do like when you run that first
command maybe there's a second one that
you'll often run to check to see whether
the results were valid or maybe you run
it and then you wait for a few seconds
and run it again things like that that
increase your confidence or after you
run that second command that's supposed
to resolve it knowing how long it's
supposed to take before it will resolve
should it happen instantly is going to
be five minutes so things like that made
people uneasy and there's a lot of
variations that you can run on things
like this so one which it might be true
for a lot of you here is that most of
your team is dev ox what would happen if
you were paged for a service that you're
responsible for other ones in our case
we have two major offices one in Toronto
one in San Francisco what I'm what we
would like to do at some point is run
one of these but one of the offices
can't respond pretending that there was
some natural disaster there another one
that that we've added is a tool called
chaos cat which is a complete ripoff of
Netflix is chaos monkey if you've heard
of that but a monkey to me kind of has
the connotation being evil whereas cats
they're not evil they're just
indifferent I'll just you know knock
over your server and goes down it's your
fault
so what it does is automates part of
this routine failure Friday plan that we
do right now it automates rebooting a
random production machine and also
adding network latency and packet loss
to a random machine for a specified
amount of time and the goal is
eventually it will run the entire basic
failure Friday plan that we have to free
us up so that we can do other things
during that time and i also want to put
another reminder that it was three years
before we actually started doing this
kind of automation you can go very very
far just manually SSH into a machine
running commands there may be using a
tool like knife with Chef or other
things like that to run it on a bunch of
machines at once but not having any
automation beyond that also a comment
about rebooting a random machine for
this at least so far it really is a
random machine there are no exceptions
this includes not just application
servers but also database servers load
balancers any machine we don't have any
exceptions coded into it and it actually
hits database servers and load balancers
and things like that more often than it
should in the initial version it did
just take the list of all of our
production hosts pick a random one and
then reboot it but the problem is that
we have a lot of machines that are
basically just clones of each other you
might have a service where a hundred
with 100 application servers they're all
basically the same is not going to be a
problem that's specific to one of them
so really you're just wasting time if
you're going through and rebooting all
of those so the solution that we used is
that we first pick a category of machine
so something like service a application
server or service be database server or
service be application server those are
those would be a category of machines
and then after we pick that we reboot a
random instance of that category and so
then you don't have this problem of
having one type of machine where you
have a ton of them and they get hit by
this all the time instead you get each
kind hit fairly often
another one that we've started to do is
that having a single failure Friday for
the whole company can be a bit of a
bottleneck if you're trying to spin up a
lot of new services so we're we're
spinning it out to let individual teams
run their own failure Friday type
scenario for these though what we've
done is we focused a lot more on getting
the team ramped up in on call stuff so
its a mix of doing failure Friday type
stuff of testing the system but also
getting new team members to understand
the system better so typically what
we've done is we've had the most
experienced member of the team come up
with what kind of scenario they want to
run and for these it usually is pretty
focused on something specific to the
service some kind of page that they get
fairly often or somewhat often and then
they'll cause it and for this one we do
have it done in secret they come up with
a plan they run it and then the their
team members don't know what it is that
they're causing they just get hit with
the page and they have to go and solve
it another thing that we've done with
this is pulling up a previous period for
a dashboard and in some cases pulling up
a period from an outage in other cases
just pulling up the period that kind of
looked a little bit weird but Mabel is
fine a good example for that is if you
have a service where you don't deploy to
it very often the period where you do a
deploy usually looks kind of weird kind
of iffy even though it's normal and
initially I had picked those kinds of
things the the dashboard review just as
a filler item in between causing pages
but it ended up being one of the more
valuable parts they're usually when
you're on a team if you're trying to
learn dashboards someone might run
through and tell you what each graph
means and then you forget it pretty much
right away it doesn't really sink in but
instead if you're shown the the graph
for a certain period and what we would
do is we would pretend you're on an
incident call and you have to answer is
your service healthy or not then you
have to actually start going through and
really thinking about what the graphs
mean and then it really sinks in which
ones are useful what they mean and then
you actually
learn it
the other part is for for this kind of
thing I didn't want it to be like an
interview where the focus is on one
person trying to solve the page but I
also didn't want to let the whole team
do it because then some people like
might not learn as much because they're
not really thinking so what we've done
since the beginning which I think has
worked pretty well is we always do these
impaired so the one experienced person
will have caused the problem and then
you have a pair of people they're both
allowed to investigate and to take
action to try to fix it and the rest of
the team can watch but they can't help
that way the pair learns quite a bit and
even though it's not exactly how you
would respond on a normal incident you
would just have one person I think it
works out better because then you're not
kind of in the hot seat but everybody
focusing on you maybe some of them
already know how to solve the problem
and you're just struggling there so I
think doing it in pair works pretty well
i also want to talk about some of the
plans that we haven't done yet but we
would like to do so when i mentioned
before there's lots of variations of one
team is unavailable or most of the team
or one office is unavailable that you
can run another good one is if your CI
server is down if you rely on that for
doing deploys how would you fix it would
you need to spin up a new one do you
have some manual process that you would
have to use another good one is how many
of you use github for work stuff like
hosted repos a few of you and how many
of you would be able to deploy if get
help us down one person so for everybody
else is that a problem like if github
goes down and if you needed to do your
deploy you would have to come up with
some other way of doing it maybe you
have it planned out already and you're
super prepared or maybe you need to come
up with the one on the fly in any case
trying it out in a scenario like this
technique that we've used for similar
things is just do a fake bad deploy just
have it log something on every request
and then you can watch to see how long
it takes you to do a to do a roll back
or to deploy something new
another one is that we rely quite a bit
on chat off-site stuff we have one chat
bot in particular that's extremely
useful it's used during a lot of outages
for figuring out what IP maps to what
machine we have that other places but it
gives you a nice command where you can
just paste in output from some command
and it will do it we also use it for
provisioning new machines for deploying
for rollbacks so if slack our chat room
was down then we wouldn't be able to use
that and it would be quite a bit more
difficult so it would be nice to do with
simulation of if sockless down you're
also losing a communication mechanism
but also the chat BOTS what would we do
would be need to spend up another one do
we have workarounds for these things
only good to find out another cool thing
that you can do is you can do certain
kinds of capacity planning if you want
to find out what your capacity is for a
linearly scalable system of like app
servers just take down app servers until
you start having problems and then you
found out what your capacity is easy
thing to do so all of this might sound
maybe a little bit scary but hopefully
pretty reasonable this is going to be
probably the the controversial part of
this presentation which is about air
budgets how many of you have heard of
the air budgets that are proposed by
Google sree a couple people okay good so
an error budget stems from this idea
that going for a hundred percent
reliability is the wrong target for
pretty much everything and there's lots
of reasons to back that up I'll give you
just a couple one of them is that
because there are so many other things
in between your users and your service
things like you know their their machine
could crash or they could have a flaky a
Wi-Fi connection or their isp might be
down because of all those things they
can't actually tell a difference between
one hundred percent availability and a
certain very high level of number of
nines
availability they can't tell the
difference anyway so why pods are aiming
for it the other is that you can go very
very far you can put in a huge amount of
effort aiming for really high
availability and to the extent that
you're aiming higher it costs you more
in terms of complexity in terms of being
super careful before doing deploys all
of those things so the idea is that
instead of aiming for a hundred percent
availability pick some target and aim
for that so if you look at the number of
nines of availability over some period
of time like a week or a month or a year
that gives you the amount of time that
you can be unavailable but still remain
within your target so if you have a
service where maybe it needs to be up
one day per week just to run a batch job
and it doesn't even matter what day of
the week it's up it just has to be up
then you can aim for ninety percent
availability it can be down for three
days and you don't care because as long
as it runs during week you're fine as
you start aiming higher though pretty
soon you need to have that one machine
be a bit more reliable before you might
have been able to just have it may be in
your house or in the closet whereas if
you're aiming higher just have it in an
actual data center when you start aiming
higher still you have to have instead of
just one machine a backup but maybe you
could have kind of a manual process of
switching to the backup if you go higher
still you would want an automated
process to go to the back up even higher
you start having to use clustered
solutions distributed systems type stuff
and the idea of those that you have some
target that you're aiming for now when
you use the this budget or when when you
use this concept as kind of a budget it
can help you make some of the decisions
about what trade-offs you need to have
with your system so one thing that you
can use it for is determining the
trade-off between doing new product
stuff versus focusing on reliability so
if you treat this as a budget if you
have your team your shipping lots of new
features you're moving quickly
you're within your budget then you can
go nuts and keep on going you're
obviously making the right trade-offs of
how much effort you should be putting
into reliability versus new feature
stuff but if you start task downtime
then at that point you can hold off on
new feature development and instead work
on features that will improve
reliability like canary deploys or
automated rollbacks faster world lat
rollbacks things like that now one of
the things that you can use this error
budget for and something that I'm going
to argue that you should use it for is
basically burning it so if you have a
service and it happens to have really
good reliability for a few months you
get lucky and it stays up what you can
do is use up your budget by deliberately
taking down that service take down every
instance leave it down for a little
while and use up your budget now when I
first heard that idea I thought it was
stupid like if if you get lucky with
your service and maybe you have a
service running on one machine and it
happens to be up for the whole year why
would you want to take you down you got
lucky you should be happy but instead
that can cause some problems that that
taking it down would fix so one of them
is that if you have a service that
happens to have better reliability than
it was designed for you might have a
high-availability service depend on your
low availability service so back to that
example of you have a service that runs
on one machine maybe you know it's only
running on one machine if it dies you're
going to have to restore from backup and
it'll take two or three hours but if you
have a high availability service
accidentally depend on it and you have
really good availability just by chance
if it goes down for real then your high
availability service is going to be down
for a long time while you restore from
backup instead it's much better if you
take that low availability service down
intentionally and have that other team
come running to you and say no you have
to bring it back up right away and then
you do that but at that point you can
you you've realized that you have that
hard dependency
then you can fix it so you either have
to take that low availability service
and put more effort into bumping up the
availability or you can do things like
introduce a caching layer or somehow
removing that hard dependency maybe you
can work around so that if that other
services down the high availability one
can operate in some kind of degraded
mode where it can still do most of what
it needs but it's much better to find
that out when you bring it down in a way
that you can quickly bring it back up
rather than having a crash and having to
restore from backup or whatever other
wait for the network partition to heal
now the other thing that it does having
this error budget and taking it down
intentionally is it provides a really
strong gut check that the level of
availability that you picked is the
right one so this target for what
availability you're going for should be
decided by your product team or your
business team with whatever your
customers expect of you or whatever you
need to provide them now they would want
you to have a lower availability target
because it it would let you move faster
so going for a higher target you have to
put a lot more effort into designing
things it's going to take a lot more
time to build up the system so they
would want to if they could have you go
lower and maybe hope to get lucky with
this but the the thing that pushes you
against aiming for too low is if you
deliberately take it down if you were if
you happen to get lucky then it provides
that check of they can't aim too low
because you will take it down and they
don't want to aim too high because it's
going to cost a lot more in terms of
complexity and engineering time and so
those two things should balance together
to to push you towards whatever the
right target is for you and then if you
have picked the right target you're
going to make better decisions on
whatever trade-offs you have to do in
terms of engineering effort to hit that
target so my conclusions once more value
Friday is awesome and you should do it
when you do don't automate it at
at the beginning and if it gets boring
start adding new things do some things
that I've talked about here that we
either have done or that we're planning
to do and keep it interesting and that's
all I have thank you all for listening
do we test for multi-region failures and
timing how long it takes to switch over
so a multi-region failure is not one
that we designed for we run across two
different providers in a total of three
what you would call region so to AWS
regions plus we have a third a third
what what would be a region but with a
different provider so we only expect to
have one region go down at the time so
we don't do multi region failures for
timing how long stuff takes to switch
over not exactly we pay attention more
to how many things are within SLA and
making sure that errors are reasonable
or low to non-existent while we're doing
that switch over any other questions yep
is it regular practice for all our teams
pretty much we have two people myself
and one other person who pick a target
for failure Friday each week and it
could be any team that gets picked we
pick the way we pick is basically any
new service is kind of automatically at
the top of our list of what we want to
test and beyond that ones that we
haven't tested in a while will push them
so by default yes it is basically any
team could get chosen yep what do you
mean
alright i'll repeat for the mic so it do
we have some kind of knowledge base of
the patterns that we find useful for
building systems to withstand these
kinds of attacks we don't have an
explicit knowledge base but it is just
kind of within the engineers you learn
it because many of our systems are built
that way and so all you have to do is
look at an existing system to know about
it and if some of this stuff probably
should be written down but the way that
it does tend to happen is if we're
running one of these scenarios and the
new service is having problems people
who are there a lot of the people aren't
just specifically on that team there are
other people who are just interested in
attending of failure Friday to see what
happens they'll pipe up and say oh yeah
that this happened with this other
service here's how to deal with it so we
we help in that way other questions yep
you
so it's Hezekiah Friday ever gotten out
of hand or do we only do one of the
services that we know would survive if
we knew for sure that it would survive
it there wouldn't be any point in doing
it yeah you're not going on nodding
along but we we won't do anything that
we know would cause an outage and if
anything is particularly risky then we
were tried in staging first and if we
found something there then we would fix
it with development time but then we
would run it again in production so
having a major problem stemming from
failure Friday is pretty rare it does
happen but they are usually short-lived
I don't think any have been basically
uncontrollable any other questions yep
where
so it's the cut the the comment is that
it do we have so few incidents that we
need to cause more in production and if
you're if you're in a situation where
you have a lot of incidents going on you
need to do this kind of thing I would
say you probably don't need to do the
individual team tail your friday that we
do we call it on call osmosis that's the
one for training new teammates basically
to be able to operate the system to be
able to deal with it if you're already
drowning in operational work then if you
if you run fail your friday and you find
more things are they going to get done
first if not don't bother doing it so it
like if you're already drowning an
operational work and you're already
going as fast as you can to fix the most
important things that are actually
happening then you probably don't need
this it's after you have a bit of slack
time when you can kind of work ahead of
not just the things that you were
suffering right now but being proactive
about what if a data center goes down
what if a navy goes down then you would
want to run this to find new things this
is only going to tell you more things
that are wrong though any other yep do
we still have incidents in production
environment to the answer in any way is
yes uh-huh so we we have a lot of
incidents that we have our minor ones
like one individual team gets paged in
fact just one on call gets paged but
it's something that doesn't affect the
core service if one machine goes down
you might get Paige to go and fix it if
late and field is a little bit high on
this service for liking someone will get
paged and they would go and investigate
but you like those things generally
don't take down the service overall if
it does escalate to affecting multiple
people beyond a certain threshold then
we kick off a major incident and then
one person from every team will get
paged as well as an incident commander
and the back of I see who will then
active scribe and then we start a video
call and coordinate their
and try to fix it as quickly as we can
but yeah those things do still happen
the best that we can hope for is just to
be able to fix them quickly and we do
failure friday to try to be proactive
about finding new things that would
cause these kinds of problems but every
time we have a major incident like that
as I said before we run a post-mortem we
figure out what the causes were and then
we go and we fix those things this is
about finding extra things before they
actually happen
okay the question is about do we monitor
how often the basically the things that
we're testing actually happened in
production basically no so we for for
those kinds of things a few people just
kind of keep track of it in our head for
example the numbers that we pick for
latency and packet loss they're just
kind of things that we've seen we don't
keep super good stats on it it's just we
know that we've suffered latency that
was as that is that pack a loss that was
add as bad as that we do have regular
production node reboots just from AWS or
whatever but we don't specifically keep
track of what stats we have it would
probably be a good idea to do but part
of what we do with chaos cabin with
failure Friday is just bumping up that
base error level so that they happen
often enough that you catch them it
would be nice to prioritize to to do the
ones that would that would actually
happen more often first but any other
yep go ahead
do you have metrics on how many we had
before we started failure Friday vs
after not exactly and it's very tough to
determine because we've been doing
failure Friday for a long time so right
now for example the company is 250
people for years ago or just under four
years ago before we were doing failure
Friday it was like 40 people we have
many many more services now so I don't
think you can definitively say based on
stats in that way that it has helped the
only thing we can say though is that we
found tons and tons of things they go in
our our JIRA tracker we have found tons
of things that we know will happen
because we have actually made them
happen by rebooting a machine or network
isolating so from I think it's more
useful to look at from that perspective
about what things you've down there and
potentially try to tie them back to the
stats about how often they would
actually happen any other questions can
you not rely on page you duty on Fridays
uh you you probably can no yeah for the
most part when we run this exercise we
find nothing in a way it's a little bit
sad sometimes they get a bit boring
you're just sitting there kind of
looking at graphs a couple of times
people have played music past the time
just because so often we run through the
scenario and nothing bad happens how was
our response to the s3 outage what can I
say we have a public post mortem you can
look at that the oh but but it that
makes it actually sound quite quite a
bit worse than it was broadly we were
just suffering from too much load for a
sustained period of time so for the
first I think two or two and a half full
hours of the s3 outage we were still up
Andrew still find it was only after a
sustained load we had a problem in one
of our Cassandra clusters that was
basically
a little bit of bad luck in terms of the
hash distribution of a few hotkeys
focused on just a few nodes and bad luck
because those keys were rotated
regularly but they happen to hash to
what resulted in the same nodes so we
did have some downtime men but it came
up in the postmortem that we were up for
the first like two and a half hours
anyone who was affected was very
thoroughly alerted by us before we had
any problems and even then it was just
some delays in notifications yeah any
other questions yep
so as I am the one who picks which
service gets hit how do i restore
maintain my reputation a lot of the
cultures that we we care about
reliability if failure friday shows
something is wrong then not that we
would assign blame but if we were it
would be your fault any worry for having
that problem no we would blame the
person who discovered it and we wouldn't
blame the person who code is it either
we would just work on figuring out what
we need to do to make it not happen in
the future any other questions now cool
thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>