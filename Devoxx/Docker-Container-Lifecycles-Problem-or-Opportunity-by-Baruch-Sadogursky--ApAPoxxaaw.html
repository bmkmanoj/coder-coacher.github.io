<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Docker Container Lifecycles – Problem or Opportunity? by Baruch Sadogursky | Coder Coacher - Coaching Coders</title><meta content="Docker Container Lifecycles – Problem or Opportunity? by Baruch Sadogursky - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Docker Container Lifecycles – Problem or Opportunity? by Baruch Sadogursky</b></h2><h5 class="post__date">2015-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-ApAPoxxaaw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right should we start great a so
good evening and welcome to this what it
will be almost the last session of the
day in thank you for staying there's
probably beer outside but you are here
and I appreciate it we're going to talk
about container life cycles with docker
and the lack of them or the existence of
them are they done correctly or not and
that's me my name is Beryl I'm developer
advocate with Jay frog and the Frog is a
startup company but we have office the
biggest one in Israel in santa clara and
in France as well we obsessed with frogs
as you can see here and here we have
cool t-shirts in our booth so tomorrow
you're more than welcome to come and
grab some we have three products of the
factory a bin tray and the latest
addition the Mission Control and both
artifactory and betray our about
managing binaries in artifactory is an
artifact repository that manage binaries
including docker images and a bin tray
is a distribution platform we also have
the ability to distribute your binaries
from a docker images from that and so we
we have this relationship with
documenting we support their images and
you can actually use a our tools in
order to work with that so and what I'm
talking what I want to talk with you
today is actually a promotion pipeline
and we envision promotion pipeline as a
pyramid which have this so in the bottom
you can see in a lot of builds right we
start at the bottom we start usually
with a lot of bills and as we go to the
App of this pyramid towards our release
we have less and less binaries and
usually the quality of those binaries
increase and between those slices of the
pyramid there are the quality gates
which actually filter out the binaries
which doesn't cut to this security gate
so think about integration test is
thinking about unit tests and the bottom
and all the bills that don't pass in
unit tests actually fail on this on this
step and the next one will be
integration tests and again some there
longer and some builds pass others don't
so we have less and then we have stuff
like stress testing UI testing etc etc
they all got longer and longer but they
actually test less and less artifacts
every time right I I hope that something
that makes sense and at least it should
and I mentioned quality gate the whole
pipeline is actually built as a pipeline
in which you have multiple quality gates
that those artifacts pass through them
and of course less and less and each
other on each and every step all you
actually can see my pointer that's nice
so on the each and every step of this
pipeline you have some kind of test
right the test that I spoke earlier and
the visibility of inch and every step is
different so for example developers
developers see all of them and on the
other side here it will be the customer
it will be the release repository and
here that's the only repository good
customer can see or the user or the
deployment tool that takes your stuff to
production whatever this is the last one
and here in the middle we have maybe the
QA test they can see this repository and
further and maybe some automatic tests
for automatic tools for testing can see
this repository and further etc etc
right again I hope that makes sense
that's something that we do for years
this
is actually a great diagram for a
relatively old book in the agile ILM and
I think it's maybe four years ago or
something and and and nothing changed
this is exactly the same that we are
doing now so what about docker the thing
with dog here is the docker build is
extremely easy powerful and fun right I
don't know I am every time I compose a
docker file and then I run it I enjoy it
a lot because it is very easy and
powerful way to construct hardware and
software to other late the software and
and that's a powerful thing and of
course when we use it the natural thing
about it is you did more because it's
easy so what we see a lot is that this
promotion pipeline is implemented with
docker build so we have this docker file
here in the development and then we
build it we test it we throw away the
images and we build it again in you and
the next level if the test our past and
if the tests are passed here we build it
again in you on the next on the next
gate etc etc this is a very compelling
because it's very easy it's very simple
it's understandable it's no-brainer
because all we need to do is build from
same docker file again what can go wrong
when you build from sources well we have
this discussion for years because we
believe that you should promote binaries
and not sources and we're devoxx which
used to be a java conference and i think
that most of you still are java
developers
and and that's the question that we hear
a lot from Java developers what's wrong
with built from sources you take your
let's say Meghan so you take your palm
file it has all the dependencies listed
you need to take care of that you take
the same version of Mahlon the same
version of Java and you're good to go
and as we all know we nail the
dependencies with their variants and
then everything's good and it usually
works with doc and file it's even more
appealing because the build is much
easier but what we learnt with docker is
that not simple and cheap is not always
the way to go and that's why Daka file
is one of the most fragile build script
that were ever created every line in the
dockerfile actually refers to the latest
version of some dependency chances are
that every time that you will build this
dagger file you will actually get
different results it can be because that
one too late has changed or maybe Python
late is changed or maybe not late has
changed or maybe you're up changed all
of them don't declare versions at all
this is extremely fragile and don't
thing that I made this up this is
actually an example of how to build node
application which doctor this is how
they say you should do and of course the
problem is that whatever you test in any
part of your development pipeline
behaves you test something entirely
different in the next part of your of
your pipeline and it doesn't make any
sense
so of course when you build those
pipelines you actually should build them
with mutable and stable binaries instead
of building from scratch each and every
time and this is how it should look you
build your images with docker file only
once and then you actually promote
pre-built binaries through the quality
gates right so i hope that hope that
makes sense and there are different ways
to build this pipeline and different
ways to visualize this pipeline and one
of them is the artifact or promotion
that i'm going to show you a demo real
soon and we will talk about what we have
and what we don't but the idea is that
you have some build this is build number
5 and it goes through the development it
goes to the staging and then to
pre-production and then we release this
build to production and usually when we
are talking about docker we release
build is we are going to deploy
containers of this image or if we
distribute images we're going to send
this image to distribution right so
that's that's about it and daughter
actually fights with us when we try to
do that it fights with us because of how
the tag docker tag is built so what we
see here is a structure of the tag and
you can see here that the registry host
is actually built into the name of the
artifact that you work with more so not
only that it's limited to host only and
we just saw that we have multiple
repositories that we want to promote
those are the repositories which are
separated with
security gates we have dr. staging local
doctor preferred local doctor prod local
we have context after the host so think
about the host is my artifactory calm
and then it goes / one of those what can
we do here we cannot do that anymore
because from Doggett's protective one
repository or one registry / host should
be enough for everybody like you know
640k should be enough for everybody why
would you want possibly to have multiple
registries in one host doesn't make any
sense well it does but we cannot do that
and there is no there is no I would say
decent working out through that so we
would try our best and how can how can
we have more than one repository more
than one registry / host well so first
of all we can minimize the number of
repositories the doctor doctor interact
with and we do it by using virtual
repositories virtual repositories are
aggregation of multiple repositories
under the same URL which actually can
give us this a single point of contact
with the registry that we need to read
and write so we deploy to a virtual
repository which is backed up by by dr.
dev and then we promote within
artifactory itself through the different
repositories and if we have test tools
which actually means docker clients each
and every darker client in each and
every environment will work with the URL
of this repository that is exposed to
the correct level inside
the promotion pipeline so let's say
there r QA guys and they actually need
to deploy this image to their QA servers
in order to check and so they will
interact with a concrete repository but
only with one and then the end of the
pipeline will be exposed through virtual
repository as well and it will contain a
production-ready images so let's look at
the diagram I think it will be clearer
so this is what we have so here we have
our build being it either of the
developer or maybe Jenkins that actually
need the base image right this is how we
start we start with the from directive
which means that we need some base image
and this base image usually come it can
come from one of my internal images and
then we will use the docker prod local a
repository physical repository that will
be exposed to the developer or Jenkins
it can be one of my development images
and of course they will be resolved as
well or it can be some canonical image
from docker hub it can be an image from
from bin tray or from any other
repository and those are exposed as well
so the developer here can actually
interact with multiple repositories by
declaring a single registry as dr.
insists by nailing you down to a single
host and then once the image is already
one which was pre built now it should be
deployed and it deployed through the
same through the same registry and that
of course simplifies the configuration
and it actually goes to the doctor they
have to the first repository in our
pipeline and then we actually start to
promote this image through the
qualifications and we can do it by rest
p I we can do it by Jenkins but anyhow
it now start traversing between their
pocket doors and if we have longer
pipeline it will go one and another and
third etc etc and eventually it will end
up in our production repository or
release the repository from which it
should be rolled out to production or
distributed to the customers and here we
will actually expose this dr. prade
local to the customer or to their
production server so the production end
point other customer or a service will
actually see only what is available from
dog and prod local and from dogger hub
or any other remote repositories okay so
that's that's how it works and of course
you will see it you will see them are in
a second before that let's talk about a
promotion meta data and the promotion
method of is also extremely important
and we are talking to and we're talking
about information what was promoted so
you look at the artifact and you the
image and you try to understand where it
came from what built produced it because
you want to trace it back to the docker
file created it or what was the
promotion action from which repository
to wish a positive it was moved the
status of course was it promoted was it
released was it rolled back all this
information why it was moved from
repository to repository who did it
obviously in comments about the
promotion we think that it's decent
quality let's release it or whatever and
of course custom metadata in custom
metadata that I would say one of the
most important things about the
promotion because it
allows you to express additional
information about the images and one why
they were promoted so for example list
of tests that this image successfully
passed or compatibility with different
in tools that it should interact with
and etc etc so that that's also very
important and that's an example of the
JSON file that you actually issue with
your promotion and it includes all this
information it can come again from you
doing this promotion through REST API it
can be automated and come from Jenkins
or any other bill as a CI server
whatever you choose and here you can see
in here's all the information the status
the comment etc etc and those are
actually the custom properties that I
mentioned and those properties will be
attached to all of the images and I
mentioned that we are talking about
promoting builds instead of promoting
files and that's also very very
important aspect of a of correct
promotion pipeline because what you're
actually interested in the level of
abstraction that you care about are
built and not particular images or
particular artifacts and that's because
builds are your tool builds are your
product your module whatever you really
care about it consists of many files but
if you don't need to trace those files
between them it's actually much easier
when you can manipulate this higher
level of abstraction and talk in a token
in builds instead of talking in
particular files what we know for sure
is that we don't know your builds might
be completely different and your
promotion promotion processes might be
completely different you might want to
change files on their ways you might
want to trigger additional systems you
might want to tag or
your version control that that you
actually release it can be anything and
of course we have no idea how your
marriage might vary so um what won't be
nice is if you could actually write it
if you could have a hook that you can
write your own promotion into and
express what a operations should be done
during this promotion and actually
wheeler to factory it's very simple
because we have this notion of groovy
promotion plugins in which you just
write the code that you want and it does
exactly that so you can do that right
and probably dr. register cannot and
that's that's the example and you can
see here that you have some kind of
promotion and it got the build named the
bill number and boom you can do whatever
you like as long as you meet eventually
a message and status okay and so now I
would like to show you a demo of this
promotion pipeline and how it actually
works hopefully it will work em because
with docker you can never know that's
not the correct window is it that's the
correct one okay so i have very
complicated you don't sit don't do you
oh okay i think i need to exit the
presentation from that mm-hmm and now
this yeah i worked ok so and of course
it's making me do so i have very
complicated docker built here that you
can see serious serious job I made him
and the thing is it's really not matter
but it may adjust some kind of
docker file and we are going to build it
with a we're going to build it with
Jenkins so here I have Jenkins okay here
i have Jenkins and I have a very
complicated built in Jenkins as well so
let's see what I have yeah okay so let's
go and look at the bill that I have ok
so I run docker build and I tag my that
the image that i'm going to create with
you remember the thing this is the host
and this is the port and then my busy
box that's what changed from the
original beauty box and i will use build
number as a version i actually don't
really care about the version of this
stage because as we mentioned i'm going
to manipulate builds and not images so i
don't care about the version of an image
so this is what i'm going to do and then
what ones are created i'm going to push
this a image and when i push it I don't
need to provide any additional
information because the tag already
includes where it is going to be pushed
and then I'm going to express additional
metadata this matter that i mentioned
i'm going to add two additional
variables the build name and bill number
to the tag inside artifact on so m let's
run it and see will it work should be
very fast over that's the benefit ok
yeah it's done and it's ready so now
from here we can go to artifactory and
see what actually happened ok so that's
the build browser in artifactory i'll
make it
it's small I think that's good enough
and and we can see the information about
the build again metadata it's all about
meta data so we have the bill number 18
m it was a Duggar pipeline number teen
built with Jenkins and it took almost
two seconds a anonymous in art factory
it was admin and that's actually the
link back to Jenkins and for example I
can see the environment variables so
here I can see all the environment
variables that where to during my build
and of course I can filter them and see
interesting stuff here as well and i
also have of course the artifact being
deployed so here i have my daughter live
local that the busy box and here is our
a tag number 18 so you can see here all
the information about it how it was
built that the virtual is a
visualization of the docker file so you
can see here my a tremendous changes
that i did so i run another command
instead of the command that was run i
think that will be mine yeah that will
be mine and of course here here are the
meta data that i added okay so you can
see here it's built name and then build
number which of course make it
searchable as well so now if i want to
find this busy box with a from build
number 16 for example i can do that as
well build number 16 and not not focus
on what did they do Oh 18 right here how
do I clear that clear no anyhow 18 will
probably found to find it okay
em right so now let's see what happened
I have a client here which is a
production client and this production
client expect to see which artifacts do
I have so it will run something like
that dogger pool private docker 5,000
might be the box number 18 what is this
port and how it's different from the
previous port here we used ah here we
used another one right in in Jenkins we
deployed to another port where's my
Jenkins it was closed let's go to build
and go back to Jenkins here yeah so here
when we deployed it it was deployed to
another port it was actually deployed to
5001 eighties he'll configure here we go
5001 what are those ports do I have
multiple installations of artifactory
how it's different that's actually the
workaround that we did around the thing
of not having multiple repositories so
you remember the picture we have two
virtual repositories now one which the
development work with to which we deploy
this stuff and the other is for
production when the production client
actually try to pull from and what we do
is we have an engineer's installed
without the factory which actually maps
5000 to the release repository and 5001
to the dev repository so here when we
are looking
at our repositories least we can see
that we have two virtual doctor
repositories daughter and doc a in dr.
death and docker includes only the
production and the docker hub and that's
the one mapped to port 4000 and the
other one the dev includes this one
which means those two and dev and the
dev as well so we commit to this one and
we try to fetch from this one from the
5000 so that's why I'm running here the
pool from this and I expect not to find
it image not found image not found
because the visibility of this port is
limited to production only so now we
want to promote our and want to promote
our our image and we promoted we have
number of ways we can use the rest api
or we can use jenkins for that and we
will do it from Jenkins so here we have
our build number 18 and we can use the
promotion button to promote to release
or to any other repository and the
promotion plugin that I'm going to
select that's exactly this groovy code
that actually takes care of the
promotion and here i have the plugin
installed that is called promote docker
and we can change the comment and the
status of course and we can select the
target repository i'm selecting the
docker prod local repository which is
backed up by the docker virtual
repository that is mapped to port five
thousand and now after my promotion I
expect of course my client to be able to
find it so let's run this promotion it
should be very fast and now we can see
what actually happened so going back to
the list of artifacts
have now my build number 18 in the
production repository and in my boom
browser I can look at this build and see
the history because something happened
to the build itself so here we have the
religion story which actually shows the
step of the release with all the meta
data that I provided and now hopefully
if everything worked well I will be able
to pull this repository right it worked
so the only question is what do I have
which is this magic that I have in the
groovy a script that actually triggers
this promotion and what I have there is
actually a call for rest api that does
the promotion inside of the factory so
here i have HTTP call to RT factory that
does this promotion and you can see here
that i can do a lot in it and after the
promotion i want to write this release
status so I go and I take the build and
I find the release status and i append
this new message that I got and etc etc
so here this is completely your code
that you can write and do whatever a
whatever you like with it and and of
course there is a lot of room for
improvement there this is a kind of a
little bit new field and if you are
familiar with artifactory and our
promotion capabilities for other tools
like java.net and vagrant and and others
it's it provides more information for
example from provides information about
which artifacts being promoted etc etc
and those are early days but and but you
can see here what will be in what will
be the general direction another
important thing is this port mapping
which of course it's also very painful
but a
you're there is no nothing much we can
do we will of course make this
configuration as painless as possible
but you will probably always until dr.
changes something end up with mapping
different repositories to ports as
counterintuitive as it might sound and I
think that I'm done that was a really
short talk on the other side you have
beer outside so that's probably good and
if you have any questions I will be more
than happy to answer
yeah sure yeah yeah so that's actually
that's actually good question and and so
I have a slide that presented the built
a built file which is full of latest
versions and and people go like okay you
want to bash a docker so you display
this slide actually no one uses it no
that's okay that's that's that's totally
fair right and then part of it my
previous versions other my dealer latest
or a good configuration management can
say you know what i will do it all these
versions but the interesting part in dr.
is that versions are not final think
about to boom to for example let's say i
put here a version what is the latest
version it's like 14 10 or something
right 15 them reaching them and can you
say that version Ubuntu 1510 it's
something final is it final what do you
say I have the security patches all the
time and they all go into 4 into 1510 so
even if I named aversion here that
version still doesn't mean that this aim
that this binary that I rely on heavily
that's my baseline is actually something
stable okay of course I took the most
ridiculous example because that was the
easiest way to pass my message but even
if you try to do it correctly with
docker you will fail because of the type
of the binaries that you rely here right
let answer your question okay and you
know what trust me there is no way you
can run the same build again in almost
any technology really almost anything
ecology and be one hundred percent sure
that you will eventually end up with the
same in with the same software and I can
go with you one by one through maven
dotnet python ruby in whatever you like
and there is always something which is
not stable enough which is fragile and
you might hit it and you might not hit
it but you cannot rely on that and here
with docker that's like absolutely bold
that you cannot rely on rebuilding from
source every time anything else yes well
that's a very good question and the
answer is we don't know and what we know
is that they of course trying to improve
their registry and trusted register and
docker hub etc etc but I'm until this
problem of a tag which is hard-coded
with one single host is solved I don't
see how it's possible to build a good
promotion pipeline without those tricks
that we did with with the port and we do
those tricks because we don't have other
choice but they do so probably good I
would say a good sign that something is
coming will be when we will see that the
host of the resolution is detached from
the software itself like we have in any
other system right think about it any
other system we have some configuration
file that manages their repositories or
the sources of the binaries and the
binary name and version is something
else that's really unusual and pretty
weird i would say
all right so you got what like 20
minutes extra time for beer I hope you
liked it thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>