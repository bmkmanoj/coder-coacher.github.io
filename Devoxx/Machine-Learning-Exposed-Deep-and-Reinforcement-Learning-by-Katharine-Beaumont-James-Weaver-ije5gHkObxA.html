<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Exposed Deep and Reinforcement Learning by Katharine Beaumont &amp; James Weaver | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Exposed Deep and Reinforcement Learning by Katharine Beaumont &amp; James Weaver - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Machine Learning Exposed Deep and Reinforcement Learning by Katharine Beaumont &amp; James Weaver</b></h2><h5 class="post__date">2017-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ije5gHkObxA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so show of hands who would rather just
listen to tunes for the next hour rather
than listen to us talk
anybody over there we're get out
just kidding you can stay
well thanks for coming
show of hands who caught our session
right before lunch okay who didn't I
should have asked who didn't okay all
right so we're gonna do a little review
for for both of you first of all this is
Kathryn Beaumont hi my name is James
Weaver I work for pivotal she works for
Vox
she's a mathematician and a developer
I'm just a developer and she's going to
introduce yourself Larry as you've heard
I'm Katherine I'm sorry for the people
who attended the first session you've
already heard my life story and but I
work for Vogue focuses behind the Vox
today's conference
I write content for the Vox website
interview speakers and the websites
actually accepts contributions so if any
of you feel like making my life easier
and sending me articles I'd be very
grateful
I'm James Weaver and I've been a Java
developer forever written several java
books j2e or je raspberry pi was my
latest book Java affects things like
that and so this I played this clip
earlier I'm not going to play it now but
this this person is pretty influential
we've both taken andrew's course he's a
founder of one of the founders of
Coursera and there's a great learning
our machine learning a program or
curriculum on Coursera
he's also the chief scientist of they do
Baidu which a Chinese technology company
and has is just kind of a luminary in
machine learning as are a lot of other
people like Yann laocoön or face book
and some others but he's these slides
are available online and so if you come
across this one it would be good to
actually take that course so machine
learning is good for lots of things that
you've heard about these days like
self-driving cars being able to generate
image descriptions you know what's going
on in this image and supervised learning
you know there's three types as most of
you know now three types of machine
learning can somebody shout out the
first time
the first one it's right there on the
slide okay
somebody say supervised learning ready
okay good
what's the next one unsupervised
learning so supervised learning there's
the house price prediction where you
know we're given we're given a feature
or more than one feature and we're
trying to predict then some outcome in
this case we're trying to given enough
given enough data samples we're trying
to predict for a given house price and
maybe a number of bedrooms or whatever
what the price of the house would be so
then there's unsupervised learning in
which you're trying to find structure in
data like clustering or something and
then there is reinforcement learning in
which you reward you have an agent that
you're rewarding as it goes through and
is successful in learning something
you're giving it rewards for being
successful so alphago which is a google
deepmind thing where they had a
championship and beat the leading go
player so we're gonna dive in this
supervised learning and so a lot of
times supervised learning you're
classifying things so this is the
example where you've got a date a set
that has dimensions of an iris plant and
then and then three different species
that it could be and so you're teaching
based upon that data set you're teaching
the machine learning algorithm what how
to classify it by giving it data points
so the data points are these the data
set which has the features of the petal
lengthen with sepal length and width and
then what species they are and then the
machine learning algorithm then is is in
this case an artificial neural network
is bringing those in and adjusting its
internal weights to represent and to be
able to link up just like your brain
does you know when it sees an image of
something it can tell what it is it's
it's linking up the looking at a
particular dimensions of an iris date
flower and then figuring out what it is
and that's the secret sauce in what
we'll be doing now is we're going to be
talking about how does a network learn
specifically an artificial neural
network so as I said this morning you
know if we have four features we've got
four dimensions there's four features
length width length width of the sepal
and petal and we have to be able to
distinguish between the boundaries of
those in four different dimensions to be
able to successfully classify them and
and so again catching up a little bit
the brain is an artificial neural
networks are modeled after the brain
where you've got inputs and outputs and
neuron and synapses those are modeled in
in artificial neural networks and then
I'm just going to show you one other
thing that you might have seen this
morning and that is this application
where I wrote an application that helped
me kind of understand what's going on
inside of a neural network and it's
written in Java and spring and and
angular 2 and so I'm just going to go
ahead and do the do the iris data set
one and so we can see the neural network
if I if I click on iris flower we can
see it learning we can see the input
layer here which is the four features
that we're learning we can see the
different hidden layers and we've got
these are called neurons but they also
have activations and Katherine is going
to talk about what activations are these
happen to be hyperbolic tangent of
activations and then here we have an
output layer which has softmax
activation x' softmax activation then
are guaranteed to add up to one so a
probability for example that it
you know that it'll give you a
probability if I type in you know a five
centimeter length for a Siebel and a 2
cent 2 meter width and then a a 2
centimeter length of a pedal and a 2
centimeter with and I hit predict then
we see that there's a 72% chance that it
is a virginica but 28% chance that it's
a versicolor but notice those add up to
1 the probability of 1 so softmax layer
if if I use softmax activation functions
at the last layer it's guaranteed add up
to 1 and so that's really good for
classification problems so and now
anytime you see this this dog we call
this math dog and so anytime you see
this it's time to get into some math
some intuition and math about what we
just talked about and then kind of
looking ahead so Catherine please
illuminate okay so one of the key things
behind neural networks we saw the input
features there there's C for lengths and
different features is what if we can
work out what those features are so in
your brain you might look at a chair but
you don't know how your brain works that
it goes okay it's the shape of a chair
it's black I don't know what else makes
it a chair we don't consciously decide
what how we make these decisions so the
idea between behind your networks is can
we mimic the way the brain works and
magically work out what the features are
so thank you so for example these are
the input features again from the house
price example we saw this morning can we
predict the price of a house based on
the size in square feet and maybe the
number of bedrooms and it might not be a
simple linear correlation it might be
more complicated or it might be that
it's there's a lot more to it than just
bedrooms might be garages and garden
sizes or something as well so just a
reminder of this hypothesis from this
morning we're going to work out what our
hypothesis is by combining these theta
values this is just a way of saying
is a wait so imagine we have the size in
square feet and it turns out that the
price is five times its growing their
size in square feet and that five that
number five is the weight that we
assigned to it it's how much do we scale
this feature to get the answer
so those theta values represent weights
so if we're trying to model this in a
neural network way there is X 1 and X 2
their size and square feet number of
bedrooms inputs coming into this neuron
here where some kind of calculation
happens and we got our output this is
the idea that in the brain you might see
something visually and that's your input
something magic happens in your brain
and your brain goes nuts chair so in the
very very basic explanation you get an
input which is a spike of activity from
a synapse and that charge is something
in the neuron and when there's enough
charge something goes out and it goes on
to the next neuron synapses are actually
really simple and really slow well not
really simple sorry okay they're really
small and really slow but they can adapt
they can adapt how much charge they send
into the neurons and they can adapt how
much charge
I think they conduct helmets largely
except as well so it's not simple
information going in the neurons in your
brain is that the synapses can control
how much so we want to mimic that with
some kind of adaptive Network and it's
not that simple you've got 86 billion
neurons in your brain so we're gonna
look a really simple example with like 3
or something later so here are input
units we've got on your own and we've
got our output so we've got inputs
coming in this is our charge some kind
of thing is going to happen in the
neuron that's going to activate them and
send a charge out which is going to be
the answer so the crucial thing is that
we're going to somehow activate it and
we don't actually know how neurons do
that but we know that the charge we can
really simply describe the charge is
a 1 or a 0 is something being output is
nothing being output but actually it's
that there are different amounts of
output so one of the big problems with
neural networks is how do we describe
what's happening in a neuron and
transfer it to a computer situation so
there are a few different things you saw
the tangent you saw the softmax function
I mean talk about the sigmoid function
ways of mapping this charge and saying
what do we think this charges is a real
number is it a 1 or a 0 how do you
describe that spike of activity so
that's one of the big comes with neural
networks your brain is so complicated
how can we possibly put it in
computational terms but we're gonna try
so this is a really basic look at what a
simple neural network might look like so
we've got our input layers we've got a
bias unit we saw a bias unit this
morning their y-intercept I don't know
if that helps you understand at all
probably not and then we've got x1
sizing square feet x2 bedrooms this
hidden layer is going to try and learn
some features because maybe it's not the
size in square feet maybe it's not the
bedrooms maybe there's a complicated
relationship between them we call it a
hidden layer because the idea is that we
don't know what's happening from your
input your output we don't properly know
how it makes these decisions so it's
hidden from us because we're not going
to tell it what to do we're just going
to feed it information in and let it
train itself the hidden I by the way I
forgot to mention that we we do accept
questions during the talk but to keep on
track we may give abbreviated answers
only we may say we're gonna cover this
later or we may just say you know let's
take it outside well I mean not like
that now let's take it but you know I
mean right ok so anyway I guess there's
math bugs so anyway just to recap a
little bit what Katherine said we have
input layers hidden layers output layers
synapses and neurons those are also
called deep belief network sometimes
you'll see dbn instead of artificial
network yes question ok the question is
are there always two hidden layers and
the answer is no the it could be one
hidden layer it could be you know 20
hidden layers and those are the number
of hidden layers the number of neurons
and various layers and lots of other
configuration parameters they call those
hyper parameters in the context of
artificial neural networks thanks for
your question ok so here's a very simple
neural network trained for XOR logic
you're all developers you all know what
xlrs so I'm gonna go ahead and go to
this application which is in the slide
there's a link to the github by the way
if you wanted to create this application
but I'm going to hit the XOR and it
takes in a 1 or 0 1 meaning true 0
meaning false it takes two of those and
then it's going to help put either true
or false so if I put a 1 and a 1 in here
what will the answer be it will it be
true or false that one act through XOR
true which one false okay so predict and
we get a hundred percent you know a one
that it is false so how did that work
well these numbers are are responsible
for that working as far as the
calculations so we're going to cover the
what's called the feed-forward
calculations and how it came up with how
it got from these features to to this
classification and then we'll also be
talking about how to train these numbers
that's part of the secret sauce so with
a feed for two feed-forward calculations
I'm gonna just go through some
brute-force math with you and then
Katherine will deal with the nuances and
kind of things behind it but what
happens is let's say for example I want
to compute this and there on he
then what we do is we we multiply the
inputs so here's a 1 and a 0 so those
are the inputs so we multiply its inputs
by the weights so it's 1 times 8 point 5
4 that's the weight of this synapse if
you're if you're familiar with directed
a cyclical graph terms then you know
these are the I could call these nodes
and edges let's say so so this edge
times this node is plus this no times
this edge is 8.54 and then I'm going to
add the bias now that bias is here so
I'm actually subtracting 3.99
and getting four point five five so in
terms of the slope and y-intercept
remember that model with the housing
prices etc the bias is the y-intercept
it's analogous to the y-intercept the
these weights that we put here the rest
of them are kind of analogous to the
slope so so we add the bias and then we
use this sigmoid activation function in
this particular case in which we do this
calculation so we take e to the negative
whatever this was add 1 and then take
the inverse of that to get 0.99 so
that's then the value of this neuron
after the activation function has been
polite applied and then we do the same
thing here and when you run through all
those calculations for all of the nodes
then that's what you get and so now
please enlighten us on the nuances so
I'm just gonna go over what Jim said
again but with some of your favorite t2
values so here we go so here's it the
example again of a simple network so
we've just got one hidden layer here
we've got our input layer and we've just
got one output which is the equivalent
of our hypothesis and all of these pink
lines represent the
synapses so there's the data going from
one place to another I'm going to
collect all of the data in the neuron
you know activate it and then we're
gonna fire it back out so to move
through the layers oh sorry this is the
sigmoid activation function so when we
keep mentioning sigmoid you can imagine
it as a function that squashes whatever
number we have and fits it in between
zero and one
so if even if we get an output of like
90 that will be close to 1 for example
if we get an output of like - node you
will get something that's close to zero
so it's you can think of it as a way as
transforming whatever number you get and
just squishing it in between 1 and 0 so
in order to move forward through the
network I'm just going to show you going
from the input to the first unit of the
hidden layer so if you ever if you watch
this back and you're like why is that X
- why is that a - what are you talking
about I've got a handy key here if you
look at so to get from our inputs to a
to a stream showed what we're doing is
we're going to times 1 by this first c2
value we're going to times the first
input like the size and square feet for
example by a different weight and we're
going to times the final one by another
weight we're going to add them all
together in this neuron and then we're
going to activate it with say for
example the sigmoid activation function
so that's how you get from the input
layer to one of these hidden units if
you have a big network obviously it's
more complicated the idea behind these
theta values if you're thinking why
she's talking about theater is the
concept that we don't know how much each
one of those inputs affects the final
output so what you're actually training
in the network is these these values
these and what we're going to call here
is weights so that's how we get the
first one and the second one is exactly
the same it's said we've got a different
weight now for each synapse so each
synapse has its own weight on its own
way of controlling it so we're going to
times them together add them all
together to get our second unit of the
second layer a we're calling it a
because we're going to have activated it
I'm not going to do that here
okay if you want to vectorize this then
you can so what does vectorize mean oh
sorry I'm probably using the wrong term
anyway if you want to do it if you want
to do this rather than going from
example to example if you write your own
program it's much quicker if you use
matrices and factors so you can use
matrix multiplication to make this a lot
computationally faster and in fact it's
so expensive from a computational point
of view to run your networks that you
have to do this if you don't want to be
waiting about five years for your answer
right so different languages have
different libraries for linear algebra
like you know for matrix multiplication
and things so like python has numpy Java
has libraries for that so these are all
libraries that you can you don't have to
understand all the detail to implement
but you can you can use the api's to do
matrix multiplication I should say I
said this in the this morning but it's
not necessary to understand all this
stuff just to do machine learning there
are higher-level api's but the name of
this talk is machine learning exposed
and so we're trying to kind of expose
all of the technologies underneath so
that as you're using the higher-level
api's you have great intuition and
understanding about what's really going
on under the covers one of the biggest
things for me actually when I was
learning machine learning is just
working out how matrix multiplication
works and reminding myself because if
you get the shape of your data wrong
then just errors errors everywhere so
and so what we can do is from those if
you remember the first three input units
going to the two hidden units we had six
pink lines six synapses so that matrix
there just represents each of those
weights going in so it's three of them
and this vector here of X represents a
bias unit first input second input so
size in square feet bedrooms and I'm
going to gloss over these because I'm
sure you don't want to look at them on a
Friday afternoon and but if you want to
go back and look then that's how you
would do it from a
to its point of view so we can imagine
here we've got the bias first input
second input going to the first layer
there and the colors correspond to those
colors so you can flick back and through
and the slides are online if you want to
have a look at that and then here again
we've got the mappings to the second
unit of the hidden layer it's
straightforward but tedious which is why
we like libraries to do it yeah this is
the thing in principle it's very simple
because you just times in hands together
and you're adding them up and then
you're performing some function on them
like the activation illusion and the
thing that makes it not simple is the
fact that you might have 50 input units
you might have three hidden layers and
kind of synapses everywhere so it very
quickly gets very complicated because of
the scale yes okay so the question was
what's the first note in the hidden
layer why is it what yeah Catherine why
is it there that's a very good question
and I actually took out the slide that
explains this so I regret that now
but if you think back to when we're
doing a linear problem and we're mapping
we're just trying to find this simple
linear relationship I'm gonna use my
arms I'm sorry of a slit stupid so here
are the axes a Rudess one alone that
goes through the data points and we can
change the shape of the line as much as
we like but we might want to shift it a
bit to the left so it intercepts Y a
different point so with the sigmoid
function if you remember that it was
this s shape we can change the shape to
be steeper or further spread out but we
might want to shift it to the left or
the right so it allows you to manipulate
the data a bit more by adding this bias
term and so typically you add a bias
term every layer of the neural network
part from the output layer where it
doesn't make any sense and but part of
that configuration and but it gives you
a bit more control over the data over
what's happening to the data
I have this I have the sigmoid up there
while you were talking and and so you
can see by changing the bias then you're
moving the trigger point really right
there so you can think of shifting the
whole thing maybe over here or over here
with the bias term but if you don't have
the bias term you're just compressing it
or stretching it okay please repeat the
question was it the small pea in the
calculation before do you mean it was
the be there was a B in the calculation
yeah in the in the one that I went
through okay
so we'll move on here okay so it's quite
easy actually now we're going to the
outfit layer because we've only got one
unit that were mapping to so we've only
got three synapses so we're just going
there by our friendly bias term and then
we're going the activated results of a
two times this theta value so just
imagine that we've transformed this a to
of the sigmoid function and then we're
doing the same with the final unit and
that should give us our output and again
here's a vectorized version of the
calculation so you can imagine here
again we're just going by a stone first
hidden unit second hidden unit and were
mapping at the output just with these
three synapses in these three weights
are there any questions and for
propagation yes
so the question is what number of
parameters should we use for the input
layer and the number of hidden units to
have a calculation under 10 seconds
depends very much because if you're just
calculating one example then it depends
on your laptop and and but typically
what happens is you'll run this through
all of your data so it can take a while
we'd have to play with some different
different sizes to work out under 10
seconds but you can do it quite quickly
if you use a language like MATLAB so I
do all of my mock-ups with MATLAB
because it's it's a mathematical
programming language it's optimized for
dealing with matrices and vectors rather
than Java you'll probably get a much
faster resort whereas if you try and
code out in Java then it might take a
while so it really depends but one of
the reasons why neural networks have
grown in popularity because have
actually been around for a long time but
you might notice now that everybody's
talking about them is because now we've
got the computational power to deal with
large neural networks so it's a lot more
accessible
so the question is how do you find out
the optimal number of hidden layers I'm
gonna show you a resource that will help
you with that
yes is that okay yeah absolutely good in
terms of the cost function there is a
way to work out the cost of a neural
network so for people who didn't come to
our first session the cost function is
can I compare all of what I know to be
the right answer with the output that
the neural networks given me and find
some way of working out the cost so it's
the exact same principle it's that you
know taking away what it's predicted
that the answer is from the actual
answer and summing up those differences
so it's sometimes called the squared
error because it's the square of the
differences okay questions on we will
address the optimization issue other
questions yes
okay so the question is I mean if I use
a day a particularly data set and I
train a con on this data set maybe a
very large data set right and then I
maybe I erased that training I wipe its
memory right and I train it on another
data set then it's very likely to be
different yes because it's trained up
based upon that data so the question is
even on this very simple one and the
answer is yes because it'll totally
depend on on the data that it's trained
on right okay
other questions now in this very very
very simple when on X are there are only
four sets of data right and they're
always the same so yeah that'll
hopefully it'll always be the same all
right but but you got any more complex
than that yeah it'll be different
question yes okay repeat the question so
the sigmoid function will never return
what is the question
technically not because it converges
because of the nature of the function
it's only when you get close to infinity
that you get to one so in reality what
you'd probably do is you'd round it up
or down or something within the
programming language that you're using
it will make that decision for you great
questions thank you okay so the next
topic that we're gonna broach out I'm
doing hand waving on this topic and then
Karen what Katherine will dive into it
but it's this thing called back
propagation so the feed-forward there
are actually two distinct things that
happen with the neural network one is
you train it and the other one is you
you use it to make predictions so the
training is where you're ingesting data
you know the data set into it you're
training it on features versus labels
the classifications and that training
can tell
a while you know that can take an order
you know seconds or minutes or whatever
but then the the feed-forward process of
actually making a prediction using the
weights it's been trained on that's
instantaneous so we've just covered how
you do feed-forward and now to make
predictions and also as a part of the
training but the backpropagation that's
where the real magic is that's where
we're assessing what the weights and
biases are for the various synapses and
nodes of a network and that we use
something called back propagation good
luck explaining this one oh by the way
here's here's a graph when you train a
neural network in your your training it
on a data set you do it iteratively so
you're you train you know maybe going
through a batch of the data set or maybe
the whole data set if it's small and so
you're adjusting the weights and biases
biases and then you run through it again
because it's it's iteratively getting
closer to zero cost and so in deep
learning for J and tensor flow and other
packages there's always some type of
visual that lets you know that you're
getting closer and closer to zero cost
so for example in this particular data
set and neural network that I was
training this was the graph that came
out so it came out with you know after
after six hundred iterations it came out
to that where the cost the score was
point one where it started out with with
one and so that's the kind of
improvement you want to see especially
when you are kind of converging into
almost a zero score now take it away
Catherine so I'm going to do an
injustice and slightly gloss over back
propagation when it comes to newer
networks and because it's very
complicated
so please touch me after this and I can
direct you either to my own knowledge or
resources but basically what we've done
now is we've walked forward to the
network and from our input and we might
randomly initialize these weights so
just randomly pick them because we don't
know what the whole point is to find out
what the best ones are and we get our
output and then we're going to compare
our output with what we know the right
answer is now we need a way of changing
these theta values to get closer to the
right ones and that that is one of the
the big problems
that's the whole training that's that's
everything that you're doing and if you
get it wrong it's a nightmare I spent
three days nearly crying to myself
trying to make something work so if you
remember linear regression if you came
this morning and we're looking at house
prices we've got this idea of the cost
function which she showed you a graph
before the the cost decreasing over time
and what we want to know is how did each
of those theta values affect the output
how did each of them contribute to the
answer that we're getting so when we had
like one input so we just had the size
in square feet we can really easily see
how changing the weight on that input
affects the costs that we get but now
we've got one two three four five six
nine dimensional space it's now we've
got the cost function according to nine
different variables so that becomes
really difficult and I'm just going to
explain it I'm gonna borrow an
explanation from under ng from Stanford
where imagine you're on a hilly
landscape and this is just three
dimensions so you'll just have to go
nine dimensions in your head to scale it
up so you're on a hilly landscape and
what you want to do is you want to get
to the bottom of the hills so you take a
step in downward direction and then you
look around you and you go okay what's
the next steepest step down you take
that step and eventually you step down
the landscape until you get to the
bottom if you've got a three-dimensional
landscape where you just have one hill
and one valley it's really easy to find
the right way to step down but you might
not have a simple landscape you might
have a landscape with mountains
everywhere and hills everywhere so you
might not find the one deepest place
because if you take all of these
downward steps and you get your deep
place you're not going to come back up
again and that's the problem with
gradient descent when we had one
variable we could see that cost function
over time changes a bit like a bow shape
with as the weight changes so it's
really easy to say okay there's one
there's one way down there's one optimum
there's one point there's one value or
most cost function is the lowest but
when you've got nine values you might
have one point where the cost function
is pretty low but another point over
here actually it's even lower but you've
taken these steps down you're stuck
there because the gradient descent
algorithm doesn't say okay now try going
back up and then down another slope and
back up and down another slope so in
reality the principle of gradient
descent is used but you'll probably use
a much more complicated optimization
function to account for that but yeah
these are the steps down the hill the
problem is your hill might not be simple
and your hills and nine dimensions
well it's Billy this is actually the
reason why
if the question is if you I'm sorry it
was it sounded like an assertion right
you're you're asserting that that if you
have different data sets that it may be
that the the gradients are different the
locations of the hills and valleys and
local minima yeah that's so it was
different data you have a different cost
landscape and you'll find different
local minima it's why there are several
really important things to change it's a
in a way the easy part is setting up
your model and the difficult part is how
do you choose the data do you get more
data or do you get more features and
that is optimization which will come to
Shirley very good thank you for the
question okay so now I'm gonna switch
over to to a site actually it's called
kaggle and you may have seen that Google
just bought kaggle a few days ago
Cagle is this great website for data
scientists or machine learning
enthusiasts and it has has lots of nice
datasets and it has contests and things
like that and so what I wanted to do
when I was learning about machine
learning I wanted to play with the data
set and so I looked through them and I
found one that I thought would be kind
of interesting on speed dating and so
this this data set has it has actually
many different features many different
columns in a but I just picked three for
input features one was weather in the
speed dating basically you spend two
minutes with a person and then you
decide whether you want to actually date
that you know go on a second date with
that person and so after after their
speed date they would fill out a survey
and and so three of the features were
did they feel that other person was
attractive intelligent and fun you know
and so rate those on a scale of one to
ten and then say whether you would date
again zero being no one being yes and so
that's our data set and so I created you
the deal for J library I I expressed a
neural network and so first before
showing you that walking through that
code I'll go ahead and run the example
so here's the speed-dating example right
here and so we have a very small neural
network actually only four neurons in
the hidden layer and as you would
predict three features and two labels
second date no second date and so so
let's say that you think that someone is
real attractive nine and maybe
intelligent six and fun let's make them
let's make him eight and so so in that
case yeah
date again but you know what's uh what's
a really kind of eye-opening maybe not
so eye-opening but in our society it's
kind of sad with with attractiveness is
nine and an intelligence at six
there's eighty-one percent they would do
a second date but if I but if I reverse
those who thinks it would be lower right
so so I guess we think that
attractiveness is more important than
intelligence when when deciding on a
second date but noticing now that that
that we've got the inputs we've got the
weights we're doing one different thing
here we're normalizing the end point
input and that's that's a kind of a
thing that we deal with when we optimize
but but you want to normalize features
so that one feature doesn't have too
much weight you know doesn't carry too
much weight and so we normalize them
either between negative one and one or
maybe zero and one or around there we
try to make them so that the the numeric
values for different features are
roughly in the same range so here's the
code then that configures that
we have a seed number remember Catherine
said that that we randomized the weights
well the seed is what helps us
randomized the weights the iterations
the more iterations that you have if you
keep making improvement the better
you're going to the lower you're going
to get your cost function to a point to
some limit and then your activation
neurons what are they going to be in the
first layer or actually by default
learning rate when you're stepping down
the hill the gradient how biggest steps
do you want to take you know if you're
going down a hill if it's a valley if
you take two biggest steps you're gonna
you're going to skip the bottom and then
you're not going to converge on on the
lowest cost and so here's the here are
the layers here's the the input layer
where we have three and out and and
sending four out and then here's the
output layer where we've got we've got
four neurons coming in and two going out
and we're articular you're expressing
that we want the softmax function and so
here is a way in in deep learning for J
and Java to express that there's similar
ways for tensor flows as a matter of
fact for tensor flow it's now in 1.0 it
just got released about a couple of
weeks ago and there is this library on
top called layers in which you can
express layers just kind of like we did
here where your art you're expressing
the different layers and this by the way
in partial answer to your question about
optimization this is where if you for
example your score is low by the time
that you're done running your training
then you have some things to look at
like for example this is pretty this is
a pretty low number of neurons in a
hidden layer it's it's a low number of
layers
and maybe this is maybe I need more
iterations and so I could probably get
this particular cost down to where the
the results are more accurate by
changing those hyper parameters question
okay so the question is why doesn't why
is it necessary to normalize like
normalize the features inputs wouldn't
all the wouldn't the training you know
and optimizing wouldn't that take care
of it and it it's just that when you
have different features of different
ranges then it can make I guess make it
harder for the neural network to to do
proper training okay I'm going to move
on here so we we tried it and yes
question so so your response is that
it's more of a numerical issue in
calculations okay all right so then this
one is this is just showing an output
from training the speed-dating data set
after a given number of iterations and
the scores and then here we've got some
information about not only the score the
the cost and how the cost function is
going down but also how they're
classified and so we can classify can
say in terms of how accurate it is where
where we could take a a test set of data
that we didn't use with training and we
can then test this to see with with the
test when we did a feed-forward a
particularly ball was it correctly
classified by the model and if so how
many times so when I did the tests on
this particular one an example that was
labeled as 0 which was no second date it
was correct that many times but then but
it was incorrect
I done this many times and so the same
thing with the yes answer and so there's
some scores down here that then model
that and we're out of time now but if we
if you went farther in the slides you
would see some slides that the deal with
what these numbers are in terms of
precision and recall there's also a link
to a website so we've got three minutes
remaining
and I would like to know if you have any
other questions for us I'm sorry one
more thing the optimization information
if you go farther along in response to
your question there's this great
resource for optimizing your neural
network and as you as you suspect it's
kind of a dark art right now right it's
a very it's not cut and dried like for
example a recent model that I trained
you know I had to try all sorts of
different configurations
you know scaling up the number of hidden
neurons in a hidden layer to 260 before
it would kind of retain information and
be able to bring the cost function down
lots of different hyper parameters you
can use but in this article and there's
a link to it it's got some good
heuristics for what kinds of things to
look for and and to try first okay now
do you have any more questions yes
question
okay so the question is what is the seed
in the Java program and so that's just a
seed for two seed around a number
generator and so that way when we're
generating the random numbers it's going
to be if I seed it with a different
number it's gonna put different random
numbers in in the model and so it's just
a way of putting in the random numbers
and causing it to be different random
numbers then maybe the last time you
tried it so so if we seed with a
different random number or a different
number could we avoid the local optimum
optimization up to the local minimum
problem and the answer is probably not
because those those will generate very
very low random numbers and it's just a
way to keep them nonzero and random so
that then the data set can be ingested
one more question and then we'll we'll
go yes
I'm gonna let you handle that one and
please repeat the question Oh can we use
in your own network see if I can
remember this no to solve mathematical
problems and few computers I'm I'm
actually not sure I'm sorry I'm I
imagine I imagine that you can I mean
your networks can solve mathematical
problems it would be down to how you
most of the problem in getting a neural
network to solve something is deciding
how to input the data in the shape of it
I think Derek might have an answer keep
in mind keep in mind where we have 20
seconds neural network is not algorithm
that gives you exact answer so it's it
gives you some answer which is might be
not correct so generally from matte
metallic point of view it's not the way
to solve any problems that's not
algorithm there's a holistic basically
axial it guesses something in that case
if you accept this guessed that ins
could be great hoc but it's not really
from mathematical point of view correct
yeah
thank you very much for your attention
and for your great questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>