<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>10 simple rules for writing great testcases by Steve Poole and Stuart Marks | Coder Coacher - Coaching Coders</title><meta content="10 simple rules for writing great testcases by Steve Poole and Stuart Marks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>10 simple rules for writing great testcases by Steve Poole and Stuart Marks</b></h2><h5 class="post__date">2017-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n6Nde8TgB2Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thank you for coming as Stewart has
been pointing out to me we appreciate
that you have a choice of presentation
to attend and we're very glad that you
took our flight so this is the ten
simple rules of writing great test cases
it's a talk
as you'll see which is a joint talk
between us and mostly we agree in a few
places we probably disagree we will try
to keep the time but you know if blood
starts flying then there's the door so
so this is me Steve Paul I work for IBM
I'm a been working for IBM for I think
all of my life I feel like I've worked
for IBM for 50 years I've been a JVM
developer for the majority of that time
so 25 years and I did a bit of DevOps a
couple of years and that was fun and now
I'm back out being developer advocate
for Java runtimes and things like that
so I have a lot of experience with much
of the pain that we're going to talk
about ok hi I'm Stuart marks work for
Oracle on the JDK and open JDK and Java
group I've been working on at Oracle and
previously Sun Microsystems for as many
years as I've been working long enough
that I have to do the arithmetic - so
pretty long time so yeah along the way
I've done a lot of have a lot of
experience developing and debugging test
Suites and a lot of a lot of experience
gained from that right so first question
how many people here write code ok good
everybody how many people here write
tests excellent so the same looks pretty
much the same hands here so what to say
there are lots of people out there who
think that their job is to write code
and not write tests and we're going to
go through these ten simple rules that
going to explain to you that actually
you have to write tests which are we all
know
and the guidelines that we think you
should be following when you're writing
good test cases and we think some of
this some of these you may consider to
be teaching yourself to suck eggs but
actually we think some of these have
good reasons for us still existing even
in the modern world with cloud and
containers and stuff like that and a lot
of this is all about this sort of bid in
the middle how do you maximize your
effort when you're writing test cases
how do you get the best out of it
right so oh do you understand anything
oh yeah I just wanted to add I mean so
part of this so you all of all of you
write tests so maybe maybe you already
understand the value of writing tests
but sometimes there's an argument with
with somebody who perhaps is not in this
room which is why you bother writing
tests because like the the slide says
and get paid for writing code not tests
and the thing that I've learned over the
years is that the tests end up knowing
more about the code than you do so when
you're working on code you run the tests
and hopefully all the tests pass and
well yeah we'll talk about good test
cases and stuff yeah but occasionally
I'll make a change to some code and I'll
be surprised because it causes a test to
fail and that that's a reminder that the
test suite knows more about what the
code is supposed to do then I can keep
my head at once so that it's a continual
reinforcement of the value of having
good tests and a good test suite yes so
unless you have a really big brain you
soon find what yeah I was just the toy
exercise I did where I was just you know
adding it was a kind of a TDD exercise
you know have a little toy program write
a test for a feature that doesn't exist
yet fails implement the feature now the
test passes everything's green again so
that's the that's the the test infected
to exercise that that's that's been
around for quite a long time and I think
I did this 15 years ago or something
like that
but when I did I quickly got to the
point where I was surprised by failures
that I caused and so what that was
telling me was that there was more
knowledge in the test suite than I could
keep in my head at once yeah yeah okay
okay so here are the ten rules
as you'll see what we think this sort of
like pirate rules and we'll explain what
that means at the end but you may get
the joke already so here's the 10 we
agree and disagree about you know the
order and what should be on the list and
maybe there others that should be there
or not but we're going to go take it
through take you through and you know
you can get to say our opinion so let's
start with the first one
think before you act ok so you think
when you're writing test cases it's very
easy just to start writing test cases
and this piece is they obviously the
first thing before you start writing
test cases you need to know what you're
trying to achieve okay and the other
rules the other nine rules will fit into
this because they've they're all
reflections of why you're doing all this
sort of stuff okay so as Stewart said he
wrote sand tests for a little tiny thing
and he found out that actually what he
was doing was he was capturing the
behavior of the system right and that's
what we're doing right tests are a one
level sort of like the inverse of your
code this is your test captures using
both embodying your test what you expect
the behavior of the code to be so when
you're writing tests you should be
thinking about the most effective way to
do that right what you don't want to do
lots of people do is they go oh I'll do
code coverage or test coverage and I'll
do the easy bit so I'll write tool I'll
write tests or the getters and setters
because that's easy they say well there
doesn't really help because they're
going to get tested anyway what you want
are test cases that explain to you and
to your descendants when they pick up
your test suite what the test was trying
to do you know and obviously what it's
trying to do is exercise the code in a
particular way so we also talked about
you don't use this we talked about
testing quality in know so if you don't
test if you're going to test your
product do you use test cases to test
quality in and we had a good
conversation about what quality men and
so that's what we came up with more
likely this is about writing test cases
to for embodiment of the behavior of
your system and you only have so much
time you get
it's like the quotes at the beginning
you only get so much time to do this if
you spend all your time writing simple
test cases you're not doing yourself a
favor so you should forget about those
sorts of things and focus on the
important activities that you should put
in your test suite not the bits and
pieces so Stuart what do you think about
you know we have this conversation about
testing in quality do you think that's a
you know you think that's a reasonable
thing to achieve or do you have an
opinion right I think I think there's a
well definitely had an opinion yeah
there's a there's a subtle relationship
between testing and quality and and I
think it's it's very interesting
listening to people talk about them and
and there's a lot of subtlety is often
lost in those conversations and the way
I think about it is that you could you
spend a lot of time talking about
quality but I'm this is not a quality
talk this is really talk about testing
and so testing is a measurement of one
aspect of quality which is the functions
that you intended to do and so if you
have some software you intended it to
perform some functions tests are
verification that those functions are
implemented correctly now whether those
functions are valuable to the customer
or or whether they run fast enough or
something like that that's kind of a
separate issue and other aspects of
quality that we we can't cover here but
I think I think it's a measurement or
verification of functional aspects of
system and occasionally run into maybe
you've I've certainly run into project
managers in the past where gee they'll
say this product isn't doesn't have
enough quality or if this product is low
quality therefore we should write more
tests it's like no that's not quite
right there's something more fundamental
going on yeah now maybe as the if it may
be that that hypothetical project needed
to improve its quality and so a bunch of
things happen to be you know bug fixing
or better you know better design or
refactoring or something and as part of
that effort
better tests need to be written maybe
fewer tests need to be written some
garbage tests might need a whole bunch
of activities regarding testing might
have needed to be done as part of
quality proven effort but just
writing more tests does not improve
quality as a cause and effect thing so
so sometimes that so that's what that
means it's like can you test in quality
no you can only test the quality that is
already there and we all know if you
write more tests you find more bugs so
if you test you tried to test quality
you never finish because every time you
write another test case you probably
found on the book yeah okay so let's
move on to number two Stewart do you
want to kick this one off yes okay so
make your tests understandable
so here's a bunch of interesting things
about code quality comments behavior
Diagnostics and so forth
and so what this is what this is asking
really is what's the relative importance
of your tests versus the quality of the
code in your product and occasionally
you'll you'll hear somebody say
something like okay oh gee there's you
know this this code and this test over
here isn't very good maybe we should go
clean it up now that's just test code we
don't need to worry about that and I
think that's a mistake because tests are
just as much a part of the software
system as the product is itself because
you need to understand it you need to
maintain it and as you evolve this the
the product you will also need to evolve
your tests so both are living things and
so I think it's a mistake to say gee
that's test code that's unimportant
because that will get in your way in the
future so Steve what happens if your
test code is not well maintained and not
understandable you spend a long long
time trying to figure what's going on so
if you have test cases that you can't
understand so go back to the first point
about the testing of your tests is an
embodiment of the behavior of your
system so if you read a test case and
you go I don't know what it's trying to
do well then how do you know that it's
supposed to work well at all okay you
get it somebody says my test failed and
you go is it because it's the wrong test
that it's badly coded is it just because
you don't understand how it works
so if you don't make your test
understandable when you write it you
know it's like when you come back
tomorrow or six weeks time you trying to
understand what's going on you go what
did
do that for right so the the other way
of looking at this is this whole thing
about debugging your prop your
application takes twice as much
brainpower as writing it so we shouldn't
use half a brain and writing the code
okay and I could point out that actually
it's the same as testing right if you
spend if you put all your effort into
fantastically hard code then your tests
are going to be fantastically hard times
too and then you have to remember how to
debug those so if you add it up you know
if you have really complicated tests
that you don't understand you need four
people to figure out whether it's
working on them so you have to start
right at the beginning so it's simple
easy well commented understandable tests
right number three so related to that
keep your tests so it's small and simple
so what we mean by that is is where if
you look at tests sometimes it's really
hard to work out which bit of all this
code is the test right now it's easy if
you've got frameworks that have got like
J unit that can do assertions and things
but if you don't have that or you only
have one of those and you go I've seen
tests with there's lots of if statements
and then there's an assertion that says
this thing should not be null and you go
war yeah but but which bit of all this
code did it work it out
right so you want to start thinking
about you know you want to keep it
separate so you have whether these
frameworks have great setup and teardown
stuff you know it's so much easier
separate out your tests you keep them as
small as possible makes it easy to
understand and as you'll see later on
actually means execution benefits yeah I
think so cool yeah I think the the I
mean it's it's it's one of these things
it seems sort of obvious structure your
tests you have set up yeah perform an
operation make some make some assertions
about it then tear it down yeah but it's
it's obvious until you run into a test
suite that you inherited from somebody
else where that's that that wasn't
followed yeah and then it sort of this
test is doing a bunch of stuff okay well
what is this testing right is this the
setup or is this an assertion or
something right and so so anyway so yeah
do this do you
yes okay so I think you're next yeah yes
yeah that's one thing only
okay oh right okay so yeah so so
actually that's kind of a continuation
of what I was just saying and so when
you're doing when you have a test case
that has this well-structured set up
operation verification and tear down
then put only one operation or one test
scenario in each test and I think it's a
very important rule to follow but it's
also a little more subtle so even if you
have a well-structured test what I find
some people do is they say okay I've
some setup code and I do an operation
and I do some assertions well well well
I'm here I'll test this other thing and
do some assertions Azamat oh and oh
that's right there's that other thing
over there and I'm going to do some
assertions of that so they end up piling
a bunch of operations and assertions
into the same test and the problem there
is that now you're you're you're mixing
a bunch of different concerns
so maybe the third operation actually
had a subtle dependency on the first
operation being performed correctly and
if it didn't then if your first
operation left something in the state of
the system or the test fixture that
wasn't caught properly by an assertion
it might cause a failure downstream and
now you have a big debugging chore on
your hands so maybe you got an assertion
halfway through it's like well why did
it get this way now instead of saying
okay I have some setup and perform the
operation and then you can go in with
the debugger or logging and tracing and
figure out what's wrong at that point
you have to go through all of the stuff
that led up to it in order to get to
that point because you haven't separated
out all the different test cases that
were being that we're being tested for
there yeah so now Steve do you agree
with that yeah I do this is terrible not
only does it - I'm just thinking about
circumstances where this has been broken
and one of the things I've seen
overnight is test cases that have a
hundred asserts in them right and then
there's another test that's got 50
asserts and you go you've got the same
ones people get carried away
it's very good to write asserts to say
I've just
this method and has returned something
is it no I do in a certainty Knoll and
that's great but then you also have a
test somewhere else with the same thing
happens right and it's like why are you
doing that right when you wanted to what
you want to be able to do is go it's
very clear to me what I'm testing is
that test testing the very first
assertion or the very last assertion or
all of the assertions now there are
times but it's when it's reasonable a
scenario to say I'm gonna make it cool
I'm going to get a complex object back
I'm going to make some assertions
because if you if you don't do that you
might find that you're doing lots of
calls to get objects back and you do it
you know yours you're wasting effort but
if you just generally keep going you
know assert a certain do something
assert something then he just muddy the
waters and if you mix your scenarios
because it's complicated
then you under the same thing so you
want obvious less obvious Ness so
obvious Ness so you can go I can see
what failed and then faster debug right
which one failed right this one here let
me put a breakpoint on that point easy
to see if you have to work through the
code to work out that's the thing that's
important then again you fail okay yeah
fast test only yes so this is a cool one
right because you have to think about
you know the Oviatt of the the opposite
so let's think about testing so what you
want to do is you want to run your tests
instantly
that's our nirvana I write some code I
changed in code they save it I want
things to run right we can't do that
because we know our tests so you know
something that by nature gonna run take
a long time I don't have infinite supply
of compute power so I want to get away I
want to have to back off that argument
and go okay I'm gonna make some wrist
I'm gonna take some risk assessment and
I'm gonna say I'm only gonna run these
tests in these circumstances if you
can't ever run test Suites quickly then
you don't do that what you end up is you
just start excusing yourself the running
test cases you go yeah we're running
once a week because they take hours
right and you just start perpetuating
that so fast tests are that's you know
it comes back down to write the least
amount of code to do what you want
run the test as fast as you can right
because if you don't you get this sort
of attitude we've all done this I've
made my code change and I've submitted
the build and then I'm going to run the
test and samuri all tell me what the
results are
right so we can do things about
improving the build and we do focus on
that what do we do to improve the speed
of our tests actually we tend to take
them out right and so let's not get
there let's start by making sure that
the test that you write is as quickly as
you can in runs as quickly as possible
and there are clever ways of doing that
like mock objects and things that reduce
the amount of effort you spend to get to
the problem your test runs ah
there's me yes corner-cutting that's
what you just don't do this have fast
tests I think I've just given given away
your piece do you have some things to
add
yeah so so I think that I mean you know
we're we've been agreeing with a lot so
far so I think the main thing is to get
a tight feedback loop between the code
you just wrote and the test results so
if you wrote some code and you get some
test failures it's like hmm those test
failures must have been caused by the
code I just wrote and if you can only do
that if your tests are fast yeah if your
tests are not fast the farther apart
they get the the causation is weakened
that's true right and so I've been on
projects where the test suite is so big
and it's so arduous that we have to have
a whole nother test team to run the
tests and they only deliver the tests
every two weeks or something like that
so you can't sit around waiting two
weeks for for your corner features so
what you do is you know I'm gonna work
on my feature check it in yeah and then
I'm gonna work on the next feature and
check it in and so I'm pipelining my
stuff then I get the test results and
there's a failure which one of the ten
change that I pushed over the past two
weeks cause that failure okay now I have
to okay now I have to do by section or
and I have to so you know try to try to
do some you know basically I have some
detective work that I have to do instead
of saying I
some code test failed therefore this
code is buggy yes right so totally Greek
so it says if you don't if you try you
make your test run as fast as you can
because if you don't then they will be
run by somebody else because we're all
getting to what the other test team will
run it it'll do it later they'll do it
less often they'll do they'll do things
about trying to deal with that problem
and then you end up with you have no
tests to run locally or you have your
favorite set which is the other problem
is you just because it's be honest what
developers do if they have a big test
suite they find the ones that run fast
and they run those because they could do
those in 30 minutes
it's repeat the question is this the
heuristic about how long you would allow
a test to run before you would speed it
up
yeah milliseconds I'd actually that's
the wrong the wrong it the right answer
is how long it's like the hole how big
is your test away how long does it take
right so I would say so they say I
didn't give an example there's a project
back where I work and they have been
working on moving their test suite time
how long it takes to run the hold the
test suite and they today say going from
ski to tea they said they used to take
the whole and used to take the length of
a ski trip to run all their tests ie two
weeks right and they said this is
ridiculous we need to get it down to how
long does it take you to drink a cup of
tea right and I know K and so it's an
order of magnitude thing that makes more
sense
you know because you will run the tests
and if they take too long you won't run
the tests right so so in open JDK we
have a test suite that we run regularly
from our continuous integration system
and we try to make sure the whole test
suite runs in two hours or so so you can
push code multiple times a day and get
test results multiple times a day and so
it's less about the speed of any given
test
and the aggregate but it is something to
keep an eye on right so if you have you
know and and we have high variability as
well right we have some combinatorial
unit tests that run hundreds of
thousands of test cases and they take a
few minutes maybe you have one thing
that takes you know 100 milliseconds
yeah something like that but the
aggregate is the thing to watch out for
yeah I mean to be honest once you get
into test Suites that take a long time
to run then you start applying different
sorts of techniques so then you start
talking about which of these tests have
you know do I run this test or versus
this test and you might be how long is
the test take to run but you might go
this Tevon this test has never found a
bug so maybe don't run it so often and
that's the domain of you know of the
system test team who's put their
expertise and trying to figure out how
they matrix this massive problem so down
to something manageable right okay
number six
absolutely Peter okay yeah so this is
this is a fun one so this is I think you
know it's you can't be absolute about
saying absolute I think this is this is
definitely something to strive for
but I have to admit that it is probably
not possible to achieve in practice this
is something certainly our experience in
open JDK is that we have intermittent
test failures and we have been working
on this for years and it is still not a
problem all right excuse me it is still
a problem it is less of a problem than
it used to be but it is still a problem
so so how many of you run test Suites
regularly and get intermittent failures
we have show hands yeah at least 3/4 the
people in the room yeah okay so that's
you know it's it's a tough problem and
so I think I think the first thing to do
is to get an attitude check because
occasionally talking to developers
they'll say you know I have this test
and yeah it passes 99% of the time so
what isn't that good enough
no because if you you know if you if you
have a continuous integration system
you'll be running this test several
times a day in several different
configurations in the course of a week
you might run this single test hundreds
of
times across many different platforms so
therefore this you know you think
ninety-nine percent is pretty reliable
but it isn't because this is good this
this flaky test is going to generate
maybe a dozen spurious failures every
week in a continuous build system and
then if you have more than one test if
you have thousands of tests in your test
suite and a handful of them are flaky
what this is going to do is give you
kind of a noise floor of test failures
and so you're gonna look at that and say
oh okay well yeah that's our noise floor
and that that's that's normal and the
problem that will occur when that
happens is if somebody introduces an
actual bug and it causes this test over
here to fail regularly
you're never gonna notice it so having a
noise having this this this noise level
of intermittently failing tests is going
to obscure real quality problems the
problem is in practice it is very very
difficult as you all know since you all
raised your hands and as we know it is
very difficult to get a test a test
suite that is 100% reliable but it's
really important to work on it and so I
think the I don't have there are no
magic bullets here I think the thing is
if you you know you have to hit it at
all levels if you're reviewing test code
and you should be reviewing test code
but when you're reviewing test code you
can look at it and say oh is this a
potential source of the intermittent
failures and they do creep in
occasionally people will put oh I'll
just I can't test all of them so I'll
use a random number generator well okay
so that's the source of intermittency
people will I you know it's hard it's
hard to imagine but people will
inadvertently introduce intermittent
failures from time to time and and not
realize it sometimes there's sensitivity
on the environment that you have to
build up robustness in the tests so
there are a lot of things that there are
a lot of damaging things that can occur
if you have test suite that is not
reliable and
you know be nice to be able to wave a
magic wand and say this test this this
test suite is reliable but I think what
you need to do is is just get down and
and do the work yes do you ever look at
tests that have been written from this
point of view do you do you have do you
consciously think about the design of a
test to see whether it's actually
deterministic or do you think you
probably just deal with it when it goes
wrong well so so again so when I when I
when I ever I've working on the JDK I
sort of in it wasn't my test suite only
but there were you know there was a
large test suite mostly written by
people long ago and so what we what we
did was we we worked on tests that
attracted attention by just you know
gathering statistics over how frequently
those tests failed and then examining
the ones that failed most frequently and
you know I think that's a very pragmatic
approach the problem is that if you have
a large test suite sitting down and
attempting to review all of the tests is
just impractical so we we looked at the
tests that attracted our attention so
but another thing is I think it is
important to - we don't have a specific
slide on this but I think it is
important to to consider code reuse of
tests people have code reviews for
product code you should also have code
reviews of tests and and there there are
occasionally times to use randomness in
in test cases that's fine if it's well
controlled but in particular one thing I
remember is somebody introduced a test
that tested a random set of test cases
but it had an off by one error and so if
you look at the code very carefully
you'd realize oh in this case it'll
index off the end of the array and so
that means and then you could do some
statistics this test will fail one out
of a thousand times because there's a
bug in the test and so you know so I'd
like to think that I contributed to the
reliability of our test suite by
preventing that test from going in right
by fixing that test before it actually
was checked in yeah so are there are
there environments like the cloud for
instance where how many people like yes
we're deploying to the cloud okay so the
the thing the generalization about cloud
is it's a different environment every
time and if you think about if you go
look at Amazon or IBM SoftLayer or any
of these cloud provisioning things the
chances are you're running on a
multi-tenancy environment you're running
in a different data center you're
definitely running a different machine
hey every time you farah pahlavi n if
you have docking containers you're still
deploying them in places and the
underlying infrastructure changes and
you can be absolutely certain that you
won't get absolutely repeatability
there's no doubt about it waiting the
cloud actually amplifies this so this is
classic quote by the Amazon guy who I
cover his name now who says everything
can the cloud breaks all the time and it
does and it breaks in ways you would not
believe so what so the classic is you
have two machines okay and machine a can
see machine B and can talk to a machine
B can't talk back to machine a and then
suddenly it flips right or you get will
you get this guy can talk to this and
you get different timing routes you get
all sorts of weird stuff so absolutely
in the cloud you have to accept that
your tests are going to fail for weird
reasons and in those sorts of
environments you may not even want to
debug it because it's just intermittent
but at a higher level you have to accept
all your tails tests are going to fail
one way or other for weird things so
you've got to figure out how you write
your test case to separate out the this
is the expected failure that I was
looking for versus the Droid that it
wasn't so it is it's unfortunately
that's the way it is and it's just it's
just going to get worse because we're
pushing everything into the cloud so you
can be defensive by spending more money
in buying bare metal and not being
multi-tenant but at the end of the day
the majority of us are going to be
running in multi-tenant environments you
look at things like server lists and
stuff you've got the same problem you've
got your piece of code is running in
some opaque environment and you're gonna
get failures is too
built-in yeah so I was thinking when you
were talking mm-hmm
but one of the things that absolutely
bility forces you to do is when you're
writing test cases one of the things
that you will do especially if you have
a team that's writing tests is they will
go I'll put some timers in I'll put some
sleep see I'll wait right and what you
what you want your guys to be doing is
if you find yourself in that position
your system test time team find that
that's an indicator that you need an
event so you need to go back to the
product team and go how do I get told
that this has happened right and you
need to start pushing that back into the
process because all of these weirdnesses
are going to happen loads and loads of
times and so you need to be thinking
about you your product has to deal with
them so you should be thinking about how
you start to change the architecture of
a product to deal with this environment
okay off soapbox okay yeah okay
independent tests only yes these are
tests that have grown up and will go off
in the wild and send you postcards no
independent tests are well we all think
we write independent tests we think that
it's fine I can run that test and then I
can run that test like run that test but
in reality what actually happens is we
tend to run all our tests in one single
order it's just common right and so
here's an example of where that break so
those of you who are of a certain age we
remember yeah there was some time ago in
fact wasn't quite sure what it was there
was a change in the JVM and I think it
was to do with something again methods
who'd get declared yeah so we'll J unit
J unit runs test methods so it makes a
cool and does road trip a reflection
gets the list of the test methods and it
executed them the order that he gets
them back in was defined we're not
defined it was just what it was and then
there was a fix in the in the JVM of the
class libraries so that method returned
things in a different order lots of
people's tests week
and the reason they failed was because
they ran tests in a different order
right and so what we're saying is is
you've got to think about writing your
tests so that you can execute them in
any order you know you want the
flexibility it does mean that if you're
going to debug something it's a lot
easier because you can just run that
test and not worry about having to run
some other tests first to set things up
which is a bad thing to do anyway and
whereas and it also you know like
subsetting that's cool isn't it if you
come off of things independently when
you change some code in this area you
can say just run those ten tests you
don't want to be situation that I'm
going to run all these 2000 tests just
to get to the two that I want to test
know you want to be able to run them
separately you know it makes perfect
sense to me so if you had any any
problems in this space oh yes yes
definitely
so before I dive into that though I
wanted to observe that so on an earlier
slide we talked about right when you
write a test have it do one thing this
is this is kind of the opposite of that
so so instead of having well I think
it's true right right a test that does
one thing instead of many things but
also have a bunch of tests that are all
independent of each other
and so it's kind of the kind of the dual
of the other thing and so again maybe
this is sort of an obvious thing but
maybe it's not so obvious because we
keep running into this right so so just
recently we stumbled over yet another
test in our test suite where it was this
is the this is open JDK right so it was
it was writing it was a test that was
testing some subsystem and the way it
configured that subsystem was by writing
a configuration file into the JDK that
was under test and it left it there but
it passed so that's okay right it passed
we have a we have a passing test right
well the problem is that particular
configuration caused some test
downstream to fail and so bug was logged
and some of our quality the quality team
took a look at it they said oh this test
is failing okay so they they ran the
failing test work perfectly well why is
that right
well so now they have they have to do
some detective work because it's like
what what could have caused this test to
fail because we can't reproduce it when
we run it on a you know on a clean JDK
well so it turns out that there's this
this it's really a false dependency but
it's a dependency between these tests
where one test had a side effect that
caused some other test downstream to
fail and so it's a clear violation of
Independence but it's also there's I
think something more fundamental which
we which which was so obvious we didn't
even bother to say which was don't
modify the system under test yes forgot
forgot about that one
I mean some of these you know if you've
been testing for a long time there's
some of these things that are just so
ingrained right and so don't modify the
system under test and certainly don't
don't have a test that that leaves
modifications persistent because it can
cause tests downstream to fail and so
anyway so it that was I I'm not sure how
long they spent on that but it was it
was a lot harder to figure out than it
should have been because if you have a
test that fails you should be able to
run the test and haven't fail again and
that wasn't the case this time
and we said about the storage of your
tests you know you've got set up
processes you've got teardown processes
if you've got tests when you're going I
have to run this test after this test is
because you've got a setup problem so
you better abstract the problem away and
again you've got things like mock
objects so there are ways of faking out
your environment to test the bit that
you want and to take more control and
what can be quite useful in the
deterministic non-determinism as well
because you can you can remove some of
the the weirdnesses right okay all right
next one number eight okay all right
divided diagnostic test line failure
yeah okay so so this is this is an
obvious one one of the things I hate is
a lot of it you know you know where
these all come from this is like I hate
it when this happens
and so one of things I hate is when you
get oh this test failed so you go look
at the logs and it just says failed
right we have we we have this problem
where we'd have these these tests where
they'd be some complicated multi-process
thing and something would timeout and so
you get this this whole
log and it said you know test failed
timeout why did it timeout what was it
doing when a time that what was waiting
for what when it timed out I don't know
right so now we have a huge debugging
problem so the the lesson here or the
recommendation here is to pay attention
to what information is going into your
test logs and provide as much diagnostic
information as you possibly can
now so for most Java tests especially
especially unit tests this is pretty
good because when you when you say
assert true or sir two equals or
something like that you get an exception
stacktrace
and usually the position of that the
exact you get the exact line in which
the test failed and usually it's clear
from context what failed and and so most
of the time that's not a problem in it's
the uncommon cases that get you right so
in that case the usual problem is if an
exception is thrown that indicates a
test failure or depend on how you
classify it a test error and so again a
stack trace will point you right at the
place that it failed great excellent
information there's an uncommon case for
instance which is you want to test that
an exception of the proper type was
thrown from the proper location and it's
a little too easy to say oh I'll just
put I'll just use it like a j-unit
annotation or I'll put a try block
around a bunch of stuff and catch an
exception to make sure that it was the
right type but the problem is that it's
a little too easy to do that and
accidentally catch exceptions that
originate from the wrong place and in
that case what you have to do is you
have to tear apart the test and figure
out in particular the case suppose
suppose there's a case where somebody
wants to to make sure that illegal
argument exception is thrown but if
there's a whole block of code how do you
know that illegal argument exception was
thrown from the right place the one that
you were expecting instead of say maybe
something was went wrong with your setup
code that also threw illegal argument
exception so anyway so there are cases
like that that come up where if you have
a failure make sure that you know
sometimes sometimes it's not so simple
as just saying assert equals sometimes
you have to go through a fairly
complicated algorithm and then decide ah
okay yes
I'm gonna explicitly signal failure here
and when you do that make sure that you
write down enough information to say how
you got there so Steve what's your
experience with us this is where I was
screaming in the dark so I have you love
this I mean this is this is just
terrific so very long time ago I just
joined IBM it's very long and a very
long time ago that very very long cut
yeah you know we went when computers
were wouldn't only so it wasn't a Java
story because Java was around there but
it was it's very very sort of to the
point I was asked there was a bug and
the bug was something a line error code
53 right and it came out IBM a lot of
IBM products of this thing called an FFT
see first for any data capture the idea
is in support of this when it goes wrong
give me a unique string that will let me
get back to the piece of code that's
failing and also it means that if
there's already a fix for it it's very
easy to spot it I've automated that
process so I we have a problem with that
and I get this can you figure out where
this error code 73 or whatever it was
coming from so I start digging and I dig
and dig and I discover that the product
has its own error handling system and
eventually after a long time I realized
that because that error handling system
was being used everywhere and it was
generic it means it meant that my error
code 73 could have come from any line in
the product right and I'm sitting again
because it was late I remember this and
and she and eventually I got down to
understanding that he was even worse
than that because this error code
whatever it was 73 wasn't even a product
error code it was just sending up a
return code he got from some IO call so
I didn't know all I had was a number and
I had no idea what the problem was we
had no what we didn't know we're in the
product was being thrown and we knew
that it was coming up from the operating
system but we had no idea what could be
made and I mean this is really cool
lovely yes so I would say I mean that's
the extreme case but actually you've got
to have this information when it fails
you've got to be able to quickly get
back to the problem
I would also say that this also a way
that you should be looking at whether
your product is giving you the error
information you want as well because you
know we all know that one of the really
great thing that happens is is that when
there is a failure in the product there
might be some logging information to
report it and you know nine times out of
ten that's going to fail because they
never get tested so actually what you
you there's some the programmer who's
created the product and rittany the
error handling code didn't test it right
because he didn't think about all that
field is null when when it's when in
fact you know he thinks it shouldn't be
no but it is null so that line of code
fails and so you don't get in the error
report earth okay it's just horrible
everybody must have had that experience
right nine no hard coding of your
environment another war story I'm glad
you're sitting down for this one
thank you so when you have a big product
you run lots of tests when your big
product is is the underpinnings for
other big products they run lots of
tests and there's always this fight
about well if you would just run our
test you'd find our bugs quicker because
back to your point about if tests take a
long time to run then the chances are
that the thing when the test is fails is
discovered discovered it's so far down
away from whether from what the code the
developers changing to him it might be
weeks it could be months and the
developers so far down ago I have no
idea right so you get this pressure and
we all do it we all look for ways to
find the bugs quicker and so like we did
this with with Java work with with
modularity as I going out and finding
all these product external open source
things the saying could you test for us
so imagine that scenario I can't imagine
it RIT last so I have a large product
and they said would you run our test for
us anywhere okay then right so we've got
this test in and we ran them up and it
went
what's the C Drive I can't find C Drive
oh we're running on Linux oh you ran out
of windows well that's not good
so we start to fix these right and then
you start finding things like Oh what do
you mean you've got a local NFS melt oh
we haven't got one of those or you've
got a local there's a this a system that
you're referencing we haven't got one of
those hard-coded right we had we spent
equivalent of about a man year going
through many tens of thousands of tests
going oh that one's hard-coded it's
making local assertions and you it just
killed you so I mean it's silly but
actually portable tests in the new world
back to the cloud stuff you're going to
run your tests you're going to want to
run your tests in these environment that
are different if your tests don't put it
on portable right then you're dead in
the water
so you have to resist the urge to
hard-code stuff right the very least
have a configuration file but hard
coding isn't just Linux versus windows
you know it's all these other things
it's like what's the URL for a system
what's the port you know what database
are using where it doesn't make a
difference to your application you need
to find a ways to abstract it out so
that you can change it because it is
going to change so can you think the
reasons why you might not what a hard
way you might want to harder code your
environment no it's it's it's pretty
hard to and actually one observation I
have after talking to this guy for a
while is if you're sharing war stories
with an IBM guy you can never win no I
mean hard coding the environment don't
do it I mean kind of the only the only
thing I can think of that's even close
is sometimes you have to hard code
things like ports because they're
various things that are on well-known
ports and you might want to have some
function whose semantics are the default
port is 80 and you want to test that the
default port is 80 so you kind of have
to hard code that and that means
anything else running on port 80 is
gonna it's going to interfere with it so
but aside from that yeah try to avoid
hard coding at all costs yeah it does
put your other ways there was also there
was that little problem with MongoDB
where the port the port was hard-coded
and all the all the player themes so all
the products all the gifts all the guys
are written code that used talktomom go
we're expecting to use a certain pool
and then there was a security exposure
and and it was like well could we change
the port number you know no because all
these products assume it's going to be
exactly that port number so there are
times when it's just hard coding that
you can do about it but that's that's
not an excuse it's just a situation
right next the last one no extraneous
output yes do it so I think this this
mainly applies to unit tests if you have
a set of unit tests like 10,000 unit
tests you don't want those unit tests
writing writing out verbose logging you
you want those to be silent so at the
end I ran all 10,000 unit tests and they
all passed great on the other hand I
think if you have a big complicated
system test like that one that had that
timeout I was talking about before
sometimes a test is so complicated it
needs to do for both logging about
everything it does because if it does
end up in the case where it's timing out
you need to know what it was trying to
do when it you know before it timed out
or before it got some unexpected failure
so I would say this is this is mostly
true but it also depends on what kind of
tests you're writing yeah so Steve you
don't really if you have if you run
tests and you put out lots of logging
you put a lots of information I've
loaded this file I'm doing this and we
all do that because you're sort of
saying how far through the code did I
get right the trouble is is that when
you do that by 10,000 tests or hundred
thousand tests then you won't spot if it
changes so you're the main thing is is
that you're asserting to yourself the
right word there that your test because
your test said it passed that it passed
but sometimes you have tests that fail
but they say they passed right you get
an exception in the log now if your log
is quiet
then you'll spot that if it's busy then
you may not right and and that's why
it's the main thing I have with this is
I would prefer to have rely on the test
suite framework to town as the test
passed or failed and then look at the
log either look not in
tea I would say okay I'll go do some
investigation right and we don't do that
we tend to go the other way we tend to
put stuff out like that and the
consequence of that is that it can be
harder for you to support these these
failures and if you're running this in
large environment then you end up
investing in tools to analyze your logs
to see whether it can find problems
right and it's like well we just made a
problem for ourselves so if you could
keep your logs as quiet as possible and
I would go for I'd love to have nothing
I'd love to say all test passed
I ran ten and that's it that would be
great no okay
cool think we need to I think when you
get right up here wrapping up so pirate
rules okay
so we said there were rules well they're
just guidelines right okay and you may
think you know they seem obvious they're
worth restating because you're trying to
get back to why it is that you're doing
this in the first place right and you
know let's people you know we can be a
patent about some of this stuff because
we know what the rules are and we know
when we're going to change them and why
we're doing this and believe me you may
go no no no this is not a problem the
first time you get hold of somebody
else's test suite you will be writing
your own set of rules again why did they
do this why did they do this it's just
terrific do you have a wrap-up uh yeah a
little thing I think ultimately I'm a
pragmatist and what I never want to hear
somebody here say is well those guys
from IBM and Oracle said you have to do
it this way so you have to do it this
way yeah so no I think I think these are
the really good guidelines to follow but
it's also driven by results because our
experience has been if you follow these
you get better test suite better code
better quality and it's tempered by what
you're trying to do with test suite and
also what kind of tests you're writing
and so if you're writing a different
kind of test then maybe this rule makes
more sense than that one yeah something
like that so I think that's the thing
keep in the back of your mind why you're
doing it not just blindly following the
rules yeah be sensible about yeah it's
cool so before you thank us for giving
you the benefit of our wisdom I would
just offer do my IBM advertising you can
go win some good
at the IBM booth you don't need to do
very much you just need to get some
stickers
so there's t-shirts and stuff like that
but there are some nice goodies out
there so you know I would suggest you
the IBM booth and maybe you can win a TJ
bot which is a robot which is quite cool
the UH glasses a drone you know why
would you not want to do this anyway so
I think we're done all right that's
great
thank you over already
yeah sorry we'll be around so if you
want to ask any more will at the
conference most of the week so just
catch us or or catch me now cool thanks
so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>