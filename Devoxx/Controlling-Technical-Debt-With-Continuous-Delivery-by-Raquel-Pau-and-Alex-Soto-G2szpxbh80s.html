<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Controlling Technical Debt With Continuous Delivery by Raquel Pau and Alex Soto | Coder Coacher - Coaching Coders</title><meta content="Controlling Technical Debt With Continuous Delivery by Raquel Pau and Alex Soto - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Controlling Technical Debt With Continuous Delivery by Raquel Pau and Alex Soto</b></h2><h5 class="post__date">2017-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G2szpxbh80s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thank you for coming to this
thought that we will discuss automatic
techniques around a continuous delivery
to get to control and reviews as much as
possible technical debt so first just
few details are allowed me I'm currently
working on Walnut and in fact we are
creating tools to reduce technical debt
using automatic approaches in fact I'm
telling myself as best practice and
fussiest
or software engineering lower because I
have papers about some talks about it
and you can find me on Twitter on this
name so and this is the logo of my
product how my name is Alex I'm a
software engineer at hat you can follow
them in several open source projects and
yeah you can follow me on Twitter I
think that is the best way to approach
me yes I mean it's at least it's fast if
you send me an email I will and I will
answer it when I can and in Twitter it
will be almost automatically if I'm not
sleeping and also I'm the author of
testing Java micro services and Apache
Tomcat Rev card so please if you have
any question please just raise your hand
and just answer it just at that time so
we have all the contacts and the first
thing that I would like to say is that
yeah so where is it in the world and I'm
not talking about like for example
Google or or Microsoft or you know any
of this technical or technological
companies but companies that start to
create a software model with the
traditional business for example Netflix
for example Airbnb for example you were
all these companies are taking the
traditional business and then just
implement it as a software model and let
me show you one example this is one
really fun this is Windows 3.1 it's a
still alive and it just
for an airport and you said al-qaeda's
news from 95 but not Feliz check here
it's 2015
so you know one year ago yeah all that
all the air traffic was manoj manually
during two days because of immigration
of Windows 3.1 another one which is what
finance this is the 75 t $7 million slow
machine we blaming at the solar casino
won't pay and at the end yeah I always
think about that I'm going to the Sun
machine I put the coin you know it push
the button and this day I'm rich and
then I said no you're not rich sorry
there is a so error so when we right so
worried yeah sometimes we don't care
about this kind of things and at the end
you can read here it really funny say
they offered him $100 and a free meal
instead it's a really fun if you go to
the casino you think that you are
written then or sorry you have a free
meal so yeah software so actually we
will say that we need a faster way of of
deploying to production maybe because
our competitors are doing something and
we want to you know reach also faster
sooner because maybe there is a back
like this one machine or the fringe
airport so yeah we need to fix it and
release it as soon as possible and of
course better better in the sense of
better software in an automatic way in
without manual steps and so on so far so
yeah we need some process to do all
these kind of things and obviously one
of these mythology or process to do this
continuous delivery and and I don't want
to extend about continuously reboot
because we can we can stay here talking
for hours but some you might think that
continuous delivery is a build tool
maybe it's automatic tears maybe as a
continuous integration system maybe it's
you know report or automatic deployment
maybe you think all these things is
continuous delivery and maybe you're
right
it's all these kind of things but I
think that the best definition about
continuous delivery is that delivering
business value more frequently and this
is the key of continuous delivery just
put your business value as soon as
possible to the final users and
basically you get it this by defining a
pipeline probably you've seen this
pipeline several several places in
different ways but at the end it's
always the same where do you develop
something you commit something to your
SCM you build your project you run your
test I mean here it's commit test unit
insertion test component test automated
test I mean all kind of tests you put it
to a stage then there is a deployer step
because we are talking about continuous
delivery which means that someone must
say yes okay this product is purified so
please put it to production then you put
it to production and then you start
earning money and getting feedback from
the users so maybe you deployed
something and the you said I don't like
this change maybe it will be better in
the old way or or trying to find another
way so you get this feedback and then
again you to pull up new futures and you
get all the pipeline one on one again of
course this build must be synchronized
somehow because we've seen that these
several stages and we need to
orchestrate all of them to execute in a
good way in the best performing way may
be paralyzing some tests and so on so
far and I think that the best solution
or the best way of orchestrating is
Jenkins and you said well it's an
opinion yes of course my opinion but is
this this yeah this polling that did my
friend from Revell Labs and notice that
Jenkins it 60% of penetration and then
the near competitor is not Travis it's
not hot
it's not boo it's with a new CI so I
would say that or we can say that the
 total is Jenkins right so and
basically with Jenkins you can do
everything it integrates with everything
you can see think about in any of the
stages deploy releases can build config
front tests whatever I mean that you say
this integrates with everything so how
you define a pipeline well in Jenkins
but if you have the concept of freestyle
job or a job where you define which are
the steps that you want to do cute in
this job and then you can cut you chain
all these jobs to build a pipeline and
for example you have some kind of a
stage which is called edge acute shell
where you for example here execute that
you want to compile Java and you want to
report the g-unit results and so on so
far probably all of youth who has worked
with
Jenkinson fills has seen this window
sometime and that's okay I mean that
it's quite manual you need to create
jobs and you know do we create basically
shell I'd built a step to get another
shell and so on so far it's okay
the problem is what happened when we
start with micro services where we don't
have a one monolith service so we do
this freestyle job only once for that
project but we have micro services where
we have a lot a lot of services then it
means that you have to go to the Jenkins
create a new job for that service do all
the or construct all the pipeline then
you need another service you go to the
Jenkins you create again all the
pipeline and so on so far so if you have
like 100 services you need to do this
100 times this is a summary of what's
happened with micro services each of the
service is deployed independently so it
means that each of the service has its
own pipeline it might be all of them
exactly the same or not but at the end
you've got all you need to define the
pipeline every every time so probably
you're saying oh please no more jobs
because if you have to do 100 services
and you need to go manually registering
one job and when your one job is like a
tedious job so what you can see with
Jenkins - it's something called pipeline
of code which basically you would you
define is you're built as a groovy
script actually you will not notice that
it's a groovy script because it's a in
fact it's a groovy DSL but so one of the
features of the pipeline mokou system is
that the configuration is in the source
repository it means that you have your
service and in the service in your git
repo if you are using it you have your
pipeline there define it as code it
means let's click on type and more code
since it's your pipeline is the finite s
code you don't need to go to the Jenkins
administration and go there create a new
job you know add all the build steps
post build the steps and all this step
all this jobs that you need to do it's
support from simple pipelines to complex
want it suppose I mean that you can
think about all the more complex
pipeline that you might have and can be
implemented as as a pipeline one of the
great things that it have is that it
survived
Jenkins restarts and connection losses
basically what's happen if you are
having Jenkins and you're running you
know your automatic test and then
netball goes down and the Jenkins is a
stop okay the bill fails and you said
then I need to go again to the job to
play and I'm going to compile again all
the project run all the tests again and
you know all the stages until you deploy
again but in case of of Jenkins pipeline
for every common that you ran its
persist a disk so when you restart job
it just simply checks if there is
something in a disk and if this is
something at this
about that pipeline it restarts from
that point this means that if you have
some for example that your machine has
go down you can boot up again the
machine and continue the pipeline where
it was left just before the crash and of
course since it's code you can reuse
that pipeline across all your services
so instead of defining one pipeline in
each of the services you can have your
repository with your public definition
and just say to the junk in it all these
services just use this pipeline so in
fact in one central place in for example
ingot and all the services uses that
definition and notice that this means
that since it's code you can branch you
can revert you can open issues you can
do contribute and so on so far and the
last thing that offers is that you can
build a history and thread serrated /
branches so instead of saying okay I
have the master branch in the future
branch and you need to configure for
master branch one job for future
branches you need to define another job
and so on in this case it just no you
defined the SCM URL and Jenkins will
start a new job for each of the branches
that it has at that moment and also the
ones that you can push as a feature
branch let me share some example in this
case is a really simple pipeline which
just runs the test and build or store
the reports the pipeline is that let's
say the root element and then you do an
age in any this means that as you know
Jenkins have an Asian model that is like
the slaves so you said I want to execute
this pipeline in mania slaves that you
might have in my Jenkins infrastructure
and I have just one a stage which is
called test I need 20 Jesus steps which
is gradual check and finally I do what I
finish all these steps I create a post
it's not an a step per se but it's a
post action that is executed always
which means that I report the g-unit
results this is another example which is
quite better because what I'm saying
here is that it's exactly the same as
before but I said please
the agent is a docker so it's going to
happen in this case is that Jenkins will
said ah you want to use docker okay then
I'm going to start the my maven Alpine
docker container it's going to start it
going to mount your workspace inside and
then execute maven clean verify and
notice that as an argument I'm passing
the maven lock of repo to avoid having
to download all the internet over and
over again every time that I try to
build the project and then execute the
the stages and
finally this is quite more peak what I'm
doing is a stage then I do a post then
here there will be maybe some tests and
finally now I said if there is a failure
not always just send an email with at
this this address with this subject and
this body and notice that since it's a
groovy script I can use this anotation
of automatic placeholder resolution
passing the pre evils that you have like
the current build all the environment
variables also another thing that you
can do is just do a sanity check which
means that I want to be sure if this
should be deployed to production or not
for example and I do input and Eve does
this staging environment looks ok and
then if you do this Jenkins will you
know show you a pop-up and you can
decide if you want to proceed or
continuing the pipeline or just stopping
it and to the finite it's really easy
you just define a new pipeline you put
the it SCM and that's all
just with this you're going to have all
your pipeline running getting the old
information from your repo and that is
okay so now we have all our micro
services running and all with each of
them with a pipeline but I don't know if
you know the concept of two pizza teams
this is something that comes from Amazon
and it's like all each team of a service
should be should be composed by the
amount of people that you can fit with
two pizzas if not it's too big team so
you need to change the size of the team
and split the team or whatever so now
it's great because we have a lot of
services a lot of teams and a lot of
services there and each of the team
might have different practices or
different ways of doing things so what
this happen is that Red Hat created
fabricate fabricate is a microservices
platform that for developers it's just
focus on developers not on the
operational team but just developers and
it's based on docker terminators and
Jenkins so the things that you can do is
just create micro surfaces using wizards
it provides visas for Jabba spin boot
wifeless form Java EE PHP go I'm not any
of the you know typical languages you
can get there and you said I want to
create a microcytic micro services with
a split boot and then it gives you the
information and then it automatically
configures that service with a Jenkins
pipeline for deploying to kubernetes
automatically for you
so when you create your project you push
a button and magically your project is
built and it's move to tower deployed to
kubernetes you if when you want to
release a new version of your service
you just
the button an automatically fabricate
will row back the previous service and
put the new service there and everything
is managed automatically and it yeah I
mean it's open source and it can be
installed on from eyes on public on on
every cloud just the requirement is that
should works with I mean that it works
with kubernetes conventions with
infrastructure part of your whole system
right but we have multiple teams that
are working in parallel in different
services and the only way that we can
share knowledge is by code review at
that moment if every team works in a
different way and with different
programming practices what we detect is
that in some way in some micro services
we are starting to have technical depth
because no no everybody knows how to
apply good practices good programming
practices right and for that reason we
need tools that argument water or
internal opinions about what is right
and what is wrong that's true that
technical debt is not only coding a
style issues right but currently the
only the only way that we can measure
that is using a static code analysis
tools and also test coverage tool and
for Java right now the most common tools
to measure de the code quality are P and
E textile fine bugs and soon our tube in
fact P and E is very well-known because
it doesn't require to compile your
source code it just passes what
you have and the text common bad
practices Koda smells in the structure
on your coat textile is more focused on
formatting issues indentation and right
now you can choose if you want to apply
a will programming style or even the
previous well-known Sun coding a style
and finally we have fine bugs that
request to compile the whole project and
looks for potential bugs into the class
files and finally sonarqube with which
was a solution that opens of course that
have started to integrating the other
one solutions and right now it's
creating its own rule set the main
difference between sonarqube and the
other solution is that soon our cube
allows to work incrementally okay
however if we are in a continuous
integration environment how these tools
can be used from Jenkins the common way
to do that is if we are using a git
workflow we check if these tools are
creating new bi reporting kneubuhl
asians during the build okay and when
they will fails we need to work again
into our source code and create another
commit and launch again the build but
what happens when we have strong
deadlines usually these solutions are
very good when when we have time to work
on it and when we detect that we have
technical debt and we need to work on it
right but especially when we are in MVPs
projects or we have a strong deadlines
being stopped by your jenkins because
there are common violations that can be
easily solved
it's not approved
way to work on it as a consequence when
we run as soon our cube and we measure
again or that that it's the amount of
time that we need to fix everything we
have days in fact these are real value
the current value of technical debt for
the struts to project however many many
of this kind of guidelines that are
tested validated by a static code
analysis tool can be validated probably
we are talking about 20 or 40 percent in
fact the most common automatic rule that
we are using as a developers is there a
nice import action or for favorite
editor before pushing the the source
code right so that's so thinking about
it I started an open or an open source
solution to fix those packed practices
automatically and well you can say you
can implement this as part of a plan of
your favorite editor right but what what
enough what happens in an environment
like Jenkins we we cannot execute this
kind of tools in in English and in a
control a continuous integration
platform so now that's easy to do from
mavin or Radl just type in one mod patch
a as a as a value of or of or comment
for command line and this produces a
patch that we can easily validate if
it's good for us or not
you can select if you want to create but
for P&amp;amp;D find bugs or sonarqube
and the the main thing is that we can
easily validate good changing because
all our formatting options are respected
so well note is not
reformatting all your source code after
fixing whatever you have the faintest
your own roll set and in this case as
you know we are removing an erroneous
import as a new news attribute we are
solving for example the usage of final
parameters that detect common issue that
open source as for it when people ask
what I can do for this open source
project and for example a common rule of
pamiri is simplifying if statements if
we have one if estimating SEC another
and there is no an else condition we can
rewrite this condition as just one and
how internally works walmat walmat is
very similar to the existing a static
code analysis tool but one important
thing is that it has what is called
semantic analysis in the world of
compilers and what does it mean that
walmat is able using your class path to
resolve the symbols in your code and
then the types of its expression and
then when an is a symbol is used or not
and this from famous organized import we
can write with just three lines of code
in fact wall mode was started like
framework to run code transformations it
was a generic concept and now we are
focused only on fixing existing rules
that and that others have have thought
that are interesting to have a clean
code and the most important thing when
we apply changes these are the minimum
changes that we need to write in the
final source code so for example if we
are in the same line using tarps or
spaces the next lines we need to add
new methods will use the same the same
combination so it's learning what is
doing in every single file there is a
discussion about this kind of tools if
you can't rest on them or not
in fact what sir it's surprising is that
when we are working in a legacy code
base formal companies prefer to prefer
to trust in tools than in people right
because they control it changes as they
are developed as open source solution
they are already validated whereas
people just can write well errors that
are not voluntary produced however my my
personal recommendation is to always
have test coverage in that way you can
simply first at all and see if your
confidence it's good or not so talking
again about automatic approaches to fix
things what we run after we configure or
pom.xml with may imply implying for
walmat we just run mavin war mode pad
which produced a war mode pod file and
then we can easily apply it into a
repository with git and then validate if
these changes are correct or not
afterwards in fact get has and a special
flag to report this kind of comets ok
that it will fix up and finally what we
need is to integrate or digit into our
current branch now that we have received
we can integrate it into Jenkins using
pipelines right and the most important
thing is that
in this commit we need we don't need to
a stop to fix our own conventions and if
we are in the context of microservices
we start from scratch we always will
have clean code without effort
specifically we have built a tanking spy
plane library in order to avoid to
repeat again and again the same
instructions in every single build and
you can freely use from this get up okay
and then you just need to put here the
reference anything more
and finally what we need is an
especially step to fix code dimensions
and that's a simple call to wall
multiply and internally that's what we
have seen in the previous receipt and we
can define a different behaviors that we
can permit is here but the defaults are
if we want to validate a patch before
applying it into our Jenkins there else
where we want to put the changes and if
always want to apply the change and
believe the the unfulfilled because if
we fail the build since we have
committed again some changes if we have
or tank is properly configured it will
launch again and you build than
successfully will will end yes we do
just need they were multiply invocation
in your build right but internally it's
using other calls that are in the same
repository and you can freely use
instead of using the main call if you
want to implement your own behavior with
this kind of fixing to the others are
has for motor pods that internally inbox
the mailing goal to generate a patch and
then the integration methods that
applies the patch into our git
repository and
is the tinnitus so let's see in a life
limo
testing okay okay right yeah we have
used
GOx in this case to a store or
repository that it's a open source
solution built on go very similar to it
up so here what we have it's a file that
is the the PMP MD configuration in this
case that it's hacking the same
conditions that you have check it before
unconditionally fair statement
collapsible Aoife statements on US
private fields earnest imports and
method argument could be final and here
ordnance pipeline
we have a little step and the next one
with which also checks if family is
supporting new relations right now I
have here the project and if I run BMD
check we we have here the relations
produced by PMD and now if I run so here
is the procedure that I have committed
before with a remote API here is the
configuration to use the pipeline in
Jenkins and now if I run the build
now it's running well not which isn't
that allows to create or own plugins and
then around quick fixes with our own
practices well notice I said they create
a semantic analysis and for this reason
we are building here the project because
we need a class path now as front and
here is asking for if I want to validate
the path and where is the path we have
printed here the path so we can decide
if we want to just continue or not and
indicate in this case we have to find
that at the end they built always will
fail right exactly
so now if I come back to the project
well you can navigate well let's go in
the Albu because the other one was the
ocean blue plug-in if I take in my field
I have here the report about the last
changes that we have submitted to
continue
and then if I run again we will see that
now there are not pending changes to to
add and finally the pipeline will be
green and the most important part is
that we are taking PMD again because
this step you can never forget to do
that because there are some rules like
the lack of documentation maybe that
cannot be automatically performed right
so if you want to to conserve this kind
of rules you need to take again this
static code analysis tool but the the
automatic the automatic part that it's
valuable always that we have starting a
project make sense when we we don't have
time to imagine it to invest on it so
yes we have a green a green color and
now if I create my repo I have a new
commit here with this prefix fix up that
has been automatically created by it for
this plugs that we have commented before
okay
that's true that what it's useful when
we are starting a new project and we
want to have a minimum of quality in our
code base right an important thing is to
start from the beginning because in that
way we will get incremental patches
always but what happens if we have a
monolithic application okay and we want
to add quality in a fast way
the main recommendation is to add it
incremental way no because you cannot
trust in the tool just because when you
need to validate your changes in during
during the code review having a big pad
it's not useful you cannot understand
everything that has been changed and
then just trust or not on these changes
although you have complete test coverage
in fact when what I was working with
with these automatic changes are and and
which undiscovered which is the opinion
of open source projects to adopt this
kind of tools I have started to create
big patches according their own
conventions and just I was asking if
it's useful or not because I was
completely convinced that this patch of
my pull request will be accepted but
this is what I commented before when you
have big patches since code review just
need to take less than 20 minutes
because the main purpose is to a spread
along then the knowledge you cannot
validate lots of things because maybe
one of this line is removing some tests
and such and this patch that it's so
weak
you cannot detect things like that right
for this reason we have built one mod
hub tool that you can freely download
it's a docker image and you will see all
the possible patches that you can apply
for its rule that for off Bimini and
then fix it automatically and so you you
have matrix to control if you are
improving or not or the amount of work
in fact we are doing some bonus for
those users that for example are
removing code that has some issues or
are just in males investing on fixing
existing code to make it better so it's
a kind of communication and also what
you can also do is invest the quality of
each commit and the most important thing
is that you can prioritise what are
those issues but you need to fix because
belongs to the host the hot files that
you are continuously changing and that's
and these files are what matter most for
the business so that's all for me if you
have any question so just raise your
hand well know the rule in this case is
checking in case that the variable is
not final and is not know if I'd put
final but the code always need to become
needs to be comparable
yeah exactly yeah
you can configure exactly which files
you want to process and then manipulate
to to finally fix yeah
opinions well yeah exactly
just for one line no you just can
invalidate and a specific rule in fact
the you can you need to configure which
is the engine that contains the rule
that you want to fix right now there are
some there is a subset of this rules
implemented let me show you how to set
up or not in your poem XML because maybe
it can help
in this case I'm sitting here in the
properties which is the engine that I
want to use that it's PMD okay and here
I'm I'm setting which is the rule set of
PMD that were mod needs to interpret to
understand what our affixing today they
need to apply and that's all because the
other part consists of just adding the
plugin and in regal is exactly the same
happy how many of you have used static
code analysis before awesome right
currently there is support for PMD
futile formation of textile and NASA
sonarqube
in fact sonarqube I think that it's one
of the the point width with more
transformations that was initially
created by a company that was the name
was met extreme well you just need to
pass the same configuration as you have
in your static code analysis tool and
then one must try to use as as much as
possible but like this the one mode also
is a command line tool and you can
manually set the list of transformations
that you want to apply but in this case
is more complicated because you need an
especial configuration for one mode but
you can do that yes now in the case of
in the case of sonarqube well not
connect to your scenario server to get
the rules and just run the rules that
it's able to fix yeah you can for it
rule there is a transformation and in
this case you need to set from a common
line one mode at this rule because you
can in the documentation you can inspect
the plane see the the amount of roots
that are implemented and then you just
need to one motor this rule one motor
this rule and it generates an an XML
file that will take in execution if it
exists if not is using what you are
computing here
now you can also implement a fixing for
this rule with a pull request on
deployment because all the plugins are
open-source so that's all for me thank
you very much for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>