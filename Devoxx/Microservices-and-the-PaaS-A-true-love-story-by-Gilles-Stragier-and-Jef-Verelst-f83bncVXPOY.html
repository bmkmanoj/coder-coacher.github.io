<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices and the PaaS : A true (love) story by Gilles Stragier and Jef Verelst | Coder Coacher - Coaching Coders</title><meta content="Microservices and the PaaS : A true (love) story by Gilles Stragier and Jef Verelst - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices and the PaaS : A true (love) story by Gilles Stragier and Jef Verelst</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/f83bncVXPOY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody to our talk in which
we are trying to convince you that micro
services and platform-as-a-service are
not only a two story but also a two love
story
let me first start with giving a bit of
context of what we are going to present
you so the platform as a service
offering that we are going to show you
is hosted in the G cloud so first
question is what the hell is this G
cloud so G cloud is an initiative of the
Belgian government where we have a
complete program of creating synergies
in this government IT and the idea is
that yeah we have to deliver some some
benefits and by increasing the
collaboration we want to increase also
the efficiency and the quality of the
services that we deliver as a government
IT and very important since we are all
taxpayers at a lower cost so to do that
we have to have a lot of capabilities
that we can influence in this this large
program so we are going to attack
procurement that we can buy things
together leverage our power with with
the vendors we are going to do projects
together we are creating services that
can be reused and especially we are
going to share the knowledge and the
expertise that exists in all the
government agencies so this cloud is a
very broad program there are a lot of
different entry points to allow
everybody to fit in nicely so you can
start with housing as a service and move
up to the more cloud like services that
we are going to present you today like
the platform as a service offering the
main idea of this different entry points
is that you can come in where you need
but it is clear that if you want to have
all the benefits of a cloud you have to
be more to the white because the main
idea of this cloud is to reduce the
customer operational complexity by
handing over that complexity to the to
the cloud and that way you will be able
to get these benefits so I'm from smiles
and we are working in this program as a
service provider we are
the service provider for some services
already in productions with virtual
machines and also for this platform as a
service that we are going to present you
today a platform as a service offering
that is created also on open-source
products so we have OpenStack we have
our OpenShift which is the the real
platform as a service core and around
that there are a lot of other projects
that are needed to get this service up
and running and we are not only using
the open source community material but
also the G cloud itself is a community
cloud where we are participating
together and having that community
vision from from the beginning so how do
these different entry points translate
to to to more practical things like I
said you can start really low end by
just sharing the housing in in the data
centers and then you are coming with
your complete rocket but you keep all
the the possibilities and all the the
administration at your site and then you
can go to bare metal as a service
wherever you just went the physical
Hardware you can get a hypervisor and
then you we start becoming more cloud or
where we say well I don't need to have
access to the hypervisor I just need a
virtual machine and then we can start
mixing those virtual machines on the
same infrastructure and start getting
these benefits and one step further is
even saying ok the virtual machine is
too too large for me I want to have a
more fine great control and we are going
to talk about containers I'm afraid so
what we see is in these different entry
points is that the more you are to the
left the more you'd still think in terms
like ok I have this kind of server and
that is my vendor and this is my
solution but if you want to reduce that
complexity we have to become more
agnostic of that we have to see this
just as a computer resource that is
scalable and that is elastic so both
worlds are there it's clear that by
putting this into production we can't
replace everything that that's there we
will need both words and align them so
if we look in more detail
specifically to the
platform-as-a-service it is already used
this is a graphical overview of the
different containers that are that are
running in the various stages of the
software lifecycle you can see them
color-coded as you can see they're still
every large circle is an institutional
memory institution that is that is
working on it the smaller circles define
the logical groups so you see there's a
lot of testing going on on different
levels but there is already some green
there are real-world applications that
are alive and especially if you see it
visually on the lower left side there is
this one circle who is really well he
presented in all different stages and
that is Chi me has a TV so we thought it
what would be a good idea to let them
explain why they were opting for our
platform as a service offering
so Jill okay hello everybody so I'm from
kami has a TV and I was going to
introduce a little bit this institution
so kami has the device the federal
Mutual Insurance
so it's one of the Belgian mutual
insurance okay but is the federal one so
it's an institution of social security
whose goal is typically to help people
who receive healthcare services people
who get Hill to get reimbursed to get
paid back it's not the biggest mutual
insurance so it's a rather small one it
has 80,000 members in Belgium and 15
regional offices a little bit everywhere
in Belgium okay
internally in at cami has a DV we have
250 users 250 agents using our ICT
applications and in 2014 cami has a
device started a big modernization
program so there were lots of legacy
stuff legacy applications which tend to
give give us problems so they we started
to modernize that the first project we
started with is okay the healthcare
reimbursement projects so I'll just try
to
explain it really really small summary
so our members here they need of care
services maybe they have medical
programs so they go to the doctor or in
a hospital ok they've received those ill
health services and eventually they get
a green paper doctor consultation
certificate which tells who gave what
service to which member which person and
home and how much they had to pay for ok
whenever this is done the certificate is
given to cami has a delay and internally
people have to compute the amount of
money we want to pay back to the people
it might seem kind of simple but it's
not as simple as that there are lots of
parameters
whenever the amount is computed of
course it is paid back to our members
when we started modernizing we chose a
micro services architect you instead of
explaining you are letting
interpretation of micro service
architecture of whatever let's say go
theory I will just list let's say a
bunch of problems we had at the time
that made us choose micro services so
first off we had three or four big
monolith ok we were organized
monolithically around them which team
dedicated to each layer ok they would
grow large and large ok one of them for
example weighs 600 thousand lines of
code and these typically wouldn't give
us much productivity we started with
problem with that we started to have
problems to migrate to make it evolve ok
in those monolith he used was done
through homemade frame working that was
used by each of the monolith and access
to the same database ok that data
integration and so with the time it
started to be a problem people had to
maintain the framework it was really
touchy if you touch it you could break
it all ok and on the end one of the
consequences of that is that we started
to have really aging technology we
starting to have really all database
really old runtimes that were not
supported anymore and that we couldn't
even deploy on virtual machines because
they we have to world which was not
possible that sort of stuff on the other
side for organizational stuff the
deployment the configuration which was
completely manual ten just to give it's
a really horrible weekend's of
deployment nightmare where you just try
to fix it in production and so on so all
those all those stuff we didn't want
anymore so we search for an architecture
that would give us solutions to that and
we shows microservices another they say
other facts that typically pointed us to
that architecture is that or project
there can be kind of complex okay there
are lots of parameters as I said so
typical numbers we they can be 50,000
type of worker services there are 50
type of a care provider each of them as
a list of qualification there are an
infinity of family composition
differences and so this all together
gives you some mix that in fact it's
difficult to say how many money you have
to pay back to somebody that went to the
doctor so basically we ended up with
five thousand business rules which tend
to be regularly modified okay they
depend on loaves so every month we have
to modify them we have deadlines and so
on so we needed something really
flexible okay so that's one let's say
big fact the second one is that in the
diagram I showed you saw healthcare
providers there on the on the right you
sue members okay and typically other
projects other processes that are that
exist in cami has the delay used the
same concept more or less a bit change
okay so this typically give us time
without okay we need to do something
that will enable he use lots of reuse
for the next years for next the next
projects okay then we started developing
the system okay we started creating
services and we ended up thinking okay
we have one five ten sir
now where are we going to put the thing
is that before we just add one monolith
in a virtual machine okay but it was not
really feasible for us to have one
microservice per virtual machine as on
the end we thought maybe we'd have 50
services okay we'd have 50 virtual
machines per environment okay it was
complex and not really adequate for us
so we started looking at containers
docker typically okay we we got a docker
we installed it and try to play with set
up our development environment in it and
it was it was easy it was friendly it
was nice and flexible but it was OneNote
okay OneNote and typically without ok
that's not possible for production we
need more we need something a bit
resilient we need a cluster and there
were lots of clusters solution no
computing whatever but they all shared
the same characteristic they were kind
of complex ok and maybe we thought ok
we'll need an expert we will need a full
time expert to maintain that it's not
easy so we started searching from a note
of the box solution and then that's
where we started collaborating with the
G cloud yeah because we in the G cloud
had already this offering based on a
readout openshift for those who don't
are unfamiliar with a with a solution I
have here a quick diagram so in the
middle the red sections are the the
actual compute node so that's what
she'll was talking about just docker
host where you are running your
containers clearly as indicated here the
platform doesn't care on what you put
into the container you can mix whatever
technologies together but openshift
creates a complete layer around that so
first of all it makes sure you can run
your you note or whatever kind of
infrastructure it will help you in
handling the persistent storage that you
want to inject in your stateless
containers that are running somewhere in
this dis cluster it handles registry and
all kinds of things it has a routing
layer that exposes things to the outside
world and most important
everything is handled by masters these
masters are the the only entry point
that is used by both your developers and
your operational team so that is hiding
the complete complexity of the cluster
that is behind it so this setup we are
going to show you some real-world demos
with that we didn't bring our complete
g-cloud but on that little machine we
have in a virtual machine a complete
OpenShift installation which we hope is
still running after the demos of this
morning you know what to what to expect
okay so we've seen that night schema
we've seen that nice platform okay with
lots of complexity love squares with
colors and so on but now the question is
how can we take a darker image and then
just put it in it so how does it work do
we need do we need dr. Campos do we need
do we need whatever deployment
configured something bit strange can you
explain that to us Jeff yeah so the way
it works in openshift this is really
that this platform is resource driven so
you are presenting your requirements via
the resources you need to the platform
and then it will decide we have to place
it in the in the cluster so if you would
have looked at the diagram in a lot of
detail you would have seen that it's not
containers that are listed there but
pots and what is a pot well it's simply
a group of containers so the simplest
pot is just a single container but you
can do more sensitive eye sensible
things with it and combine them for you
the resource with which you are defining
how to to create that that pot is via
the deployment coffee
the deployment config the platform which
image to launch in how many instances
you you want it to do the way you want
to deploy that if you need persistent
storage yes or no and how to inject that
and all these resources are file based
and the system both supports llaman or
JSON it's a it's equal whatever you you
like you will see that in the in the
examples the interaction as we we said
you can interact always with those
masters they
have an API you can attack that directly
clearly they have a web console so in
the web console you will see a resource
based a few you see here you see the pot
you see on which image that that is
based but they also have a very powerful
CLI so to CLI it's easy to use for photo
developers but it's also a very easy way
to script things that are the or the API
so okay it's time for a little demo yeah
so first thing I'm going to show you is
that is a case study application I made
okay so it's let's say based on the
example I gave the earth care
reimbursement business so here I made a
small elf care provider REST API okay
which will basically return a list of
elf care providers okay we have two
types we have some hospitals and we have
one doctor okay so this basically is a
really small spring boot application we
typically use spring boot you know
micro-services architecture but it's a
bit specific it's a bit cleaner so for
the demo ok so now I have that typically
I should build a darker image I won't
show you in the demo how that works but
it's already done okay I have a docker
file here which is not so important with
that transform that to docker image and
then I have my gamal files here that
contains the resources from my
healthcare provider service so basically
here we have a deployment config that
tells which part we have ok and how to
deploy it so this is the docker image
here that will be used ok then we use
service ok which is a network natural
type abstraction ok that will expose our
pods so they can be cold
ok and this is a route there ok which
will allow me to query the the pod using
my browser typically huh ok I'm going to
take that ok and copy/paste
that into my own OpenShift here okay so
I'm locked in I have okay a dummy
project open ship project that that's
there and I'm going to add the ml file I
just show you that's done okay so here
we see the resources it's starting all
those nice colors there it's starting
it's blue so it's okay so I can click on
the route and co5 ol tree okay
in fact it's normal we'll see it we will
see why just a little bit later so now I
refreshed it and I have my healthcare
provider service which is running there
if we look a little bit more so we have
here the route I clicked on it we saw it
we have here the service ok which is
basically virtual IP ok which typically
points to my path and then I have my
pods there you come back here and we can
see ok the image I mean we can typically
see maybe stuff like the logs of the
application that just started ok so this
small demo show you a bit how to run one
micro service in an open shift so in
production right now since April we have
FAC we use that kind of solution this is
a screenshot of our production
environment we have I think 35
deployment configs running with
something like 60 pods and lots of
services and routes
ok but I only show you how to just put
one pod in one project this will
typically not be sufficient of course
ok so we need a little bit more
mechanism so the first the next
challenge the next question I'm going to
ask is what if I want multiple times the
same application to let's say isolate
them to have environment well how can i
model environment so basically what we
want on the end is to be able to create
a pipeline
we're going from source code we can go
up to production with different test
phases okay using the same binary but
maybe a bit different configurations
okay some in the pipeline here we start
from test environment we go to quality
assurance and then production is a
rather short one okay but typically you
might have different requirements for
the different environments so Jeff can
you show us what to do here yeah so the
first interesting points that you waste
is how to isolate your application in
this environment so in the demo we
already saw one project and in OpenShift
you can create as many project as you
want and that you can use very easily to
to model environments so here we have a
setup where we say ok we have a
production environment a QA environment
and a testing environment 3 different
projects to put in in the end our docker
container first of all each project has
its own user management so for each
project you can decide who can act on it
you can apply your own governance let's
say you have an operational team and the
dev team everything is up to you and you
define that role based with role based
access control secondly for the
isolation each environment automatically
is created in its own isolated Network
zone so that means that if you don't do
anything special the containers can't
mix between the environment which is
really nice because we don't want to
link your test container to your
production database to say to give just
that example also related to to every
project is a number of quota where you
can say ok this is all the number of
resources that I am giving for this
environment just to make sure that if
your production environment needs
additional resources that the guy who is
creating 55 test containers isn't
stealing the resources you need later on
and this with resources that you are
giving are about the amount of memory
that you can give to your containers and
the amount of CPU so what the platform
is doing is taking all this metadata and
applying that into toker commands in the
end it still dr. that is providing the
isolation and noise
doing the do real work it's just the
platform who is handling the
orchestration for you
your second question was okay now I have
this nicely isolated environment but I
only have one image how do I make it
behave like the production version and
the first way of doing that is simply
using environment variables that is
supported by every solution it is easy
it is it is well known and you can
specify them in this deployment config
and you can list them all you can also
change them individually we are the CLI
or via the web console as you as you
like
in the browser they're all listed so
it's really nice it's really clear the
only problem is yet that is working for
text but I also have this this key
stores and certificates and things like
that that are specific for my
environment so they are environment
variables it's a bit it's a bit of a
nuisance so then we can use a second
offering which is called sequence and a
secret is array of the platform of
injecting binary data as a file on the
fly into your docker container so it's
using the the volume mechanism that is
offered by docker to inject that on the
on the fly but the platform adds a
complete security element on top of that
you can decide who can see your your
secrets you can give access to certain
people to have access to certain things
of the container but not the secrets so
that way you can really keep those
things very very much protected and
since the platform is orchestrating that
over the color containers if you have
for example this key store example and
that key story is used in multiple
services you only have to create a
secret one so you reference it in all
your different faults which makes it
really easy to update that secret if
your certificate expires also one of the
things that the platform offers are
templates so templates is what you you
expect of it a way to describe how to
create this this application you can use
that to create your own internal
software catalog the example that we
show needs for example for Jenkins
service so in this template we will
describe all the the resources to be
created although the port's the routes
how to handle the persistent storage and
then in the template you can specify the
PI
meters that is what the user can can
influence with I can decide how the
template will be instantiated so what do
you think she'll are those mechanisms
that you can can use in your solution
yes yes even if we don't use everything
okay so we choose some some solutions
and I will just show you how that no
works so first thing is I took my case
to the application okay and I treated
let's say behavior that's dependent of a
variable so here you know demo I just
set a variable that says if the service
should show the hospitals or not in fact
if you should hide new hospitals or not
okay which is the opposite as it just
bring boot application I just inject
here the value of how those Patel's
which can be a system property but can
also be an environment variable so on
the end if you said right environment
variable you'll have to write behavior
okay and then I changed my ml file there
which was before a list of resources
into a template so I created a template
with two parameters
the famous hydros ptl's value okay and
an instance variable
okay the instance variable I have to use
to parameterize the name of the route
okay because I can't just create two
stuff with the same road name so I had
to do that okay for the rest it's kind
of similar the only thing that change
here is that I added an end block here
where I define the environment variable
hydros Patel's as the value that was
entered into the template so that's just
as simple as that so now I can go to my
OpenShift environment here okay yup here
we go login
okay where I have my different
environments okay what I'm going to do
is I'm going to the test environment and
I'm going to deploy health care provider
service but no okay
might not going to do that well use that
template it's not there okay it's there
so I just changed my mind and stuff it I
want you to debate it in QA so I'm going
to deploy it in QA that's not a test
anyway the serratus queuing I'm going to
say that I want the hospitals to be
hidden okay and that my instance is
queuing okay so I can create that's it
here we can see that we'll have all of
the right there okay starting if I go
there again the typo tree huh mm-hm
come back here okay here you see we
don't have hospitals okay so basically I
could show you exactly the same stuff
into another environment and change
another variable okay what's important
here is that now I can parameterize my
deployment for various environments and
have them isolated so okay
as Jeff just said it's a few minutes
fine two minutes ago
there are different way to to use
openshift here I showed the console okay
the opposite console but in let's in
reality I don't go to deploy every
application on every environment by just
clicking in the console so this is
completely unrealistic so we basically
in more production environment we do the
exact same stuff so we use templates
with parameter and environment variable
but we do that with scripting basically
okay if you have the CLI all the the API
depending
so now I'm going to show you you know
protection environment
this is a typical pipeline so this is a
really real pipeline we have in our
continuous delivery tool where every
step in fact execute scripts that will
figure out deployment and so on on open
shift so we we use go CD this is the
pipeline of the healthcare provider
service the real one not the Deming one
with just two providers and so in fact
how that works on the left there right
here okay we create a template with a
specific binary version just hard coded
in it okay then we save that somewhere
when we need to deploy let's say in some
step here maybe that one okay the QA
environment when we need to deploy we
are going to go fetch that template for
the healthcare provider service which
here you have an excerpt of the
parameters okay and resolve every
parameter with an environment variable
that we that's linked to the QA
environment so we have a list of to
hundreds environment variables that
really give all the parameters of all
the services in that environment and
then using that we generate the final
list of resources where we are going to
put into openshift what's kind of nice
is that that list of resources we can
save them okay we can put them somewhere
and if we want to reuse it later we can
just make maybe a rollback or just go
back to another version okay so it's
really flexible for us to exactly deploy
what version of what service in which
environment okay so this really allows
you to to coordinate the deployment of
all your services and just go back in
time to so whenever this is done we just
put that in the G cloud okay I see just
put that okay it's a bit more
complicated than that because when you
just saw me clicking okay and since
hitting a template in fact I treated new
resources but if it was already there I
would have received errors okay so I
would have to
using my scripts to do a get and then
replace and whatever so there is a
little bit of mechanic there to make
that really work and be able to deploy
what you want when you want depending on
every situation okay so this brings us
to our last question which in fact is
okay I have now done it for one service
but what if I have two or three or four
and how could they communicate because
as you said okay it's a cloud there are
lots of layers we have just well IPS so
where is my pod how can I can I use it
what's his name what's his address port
and so on it might not be easy so I
might also have multiple ports so I
might need a list or something
complicated what do you think Geoff yeah
so we already saw some of the the
concepts but let's look at them in a bit
more detail so the first way that that
is offered is creating services so the
service as you'll already showed you in
the the first demo is a virtual IP that
will be fixed and constant for the
complete lifetime of your service which
is completely different to the lifetime
of the container that is delivering that
that service so the container yes it is
being recreated multiple times and every
time it will get a different IP address
but the service address will remain
completely stable for the whole time
this IP address will also hide the fact
if your service is delivered by one pot
two pots or whatever pots so that is
already a way of simplifying your
configuration it works for all protocols
because it's only the IP layer and okay
how can you get to know this magical IP
address well the platform acts as a as a
DNS and your service name will resolve
to this IP address again it's not the
only possibility there's a lot of
different ways of doing the same thing
sometimes every service is also injected
as environment variables so we have the
IP address and the port on which the
service is exposed as an environment
variable and then you can use it
directly in your application as well
the services are bound by the project
scope this is a very good idea because
that way it simplifies your
configuration if you simply say I have
to reach out to the the what was the
name of your service again healthcare
provider the healthcare provider service
you just configure healthcare provider
service and an integration it will be
the integration healthcare provides the
service in production it will be the
production healthcare environment
service so that is already one of the
things that you don't have to create
environment variables for yourself but
clearly in these projects you can put
everything in the same project sometimes
you have to communicate outside over the
project and then you have the routes and
the routing layer that we saw in the in
the diagram of the of the OpenShift
solution is in fact just an automatic
reverse proxy layer so in the beginning
it was only HTTP now in the newer
versions you can can do other stuff with
that as well but you simply expose it as
you would do on the reverse proxy but
with a resource handled by by openshift
and that way you can communicate between
your projects and with the outside world
it also has a lot of support for HTTP
and and all those kind of things
installing the the certificates out
automatically so ok small demo so for
this step i typically can just go right
to open shift here so basically what
happened here is that i developed a new
micro service ok which is typically the
green paper and coding application okay
that will need to have access to that
list of elf care providers so i log to
that's it okay so here we see if I have
now to do pot okay only one is exposed
by a road so it's the the green paper
stuff same brush cement request that's a
general name of the certificates are
given by the doctors and as it uses the
as it uses the other service I had to
modify its deployment config here and
see that here okay and create
environment variable with the you were
hi here okay and then I just bind it to
the internal variables of woven shift
which will in fact be reserved to the IP
of the service to same IP that we saw in
the first demo the same kind of IP so I
can show you how that works here okay so
here okay here we have the list of
healthcare providers that just directly
comes from the other service the thing
is that that IP variable is just
resolved on stuff them on sat about the
container so if you end up by deleting a
service the IP just will change and all
the clients all the other services that
use that IP will just stop working so
you have to pay attention to that but on
the other hand you have really almost no
reason to never delete a service it's
almost never changed so it was treated
and and that's it if I go back to let's
say what we do in practice or real world
application here thank you
okay this is an excerpt this is a
dependency diagram from our production
environment so we have okay various
services different service which are
linked together
this is generated from the service
registry in fact of workmanship so we do
command line stuff to to get that and
generate the diagram so we see that our
services are kind of communicating lot
okay and we always use that virtual IP
stuff IP injection via environment
most of ours all our parts have services
and only the UI stuff that are used by
our business people have routes and we
use hearty TPS routes everywhere
so now let's go back to or let's say
last question which is okay what what
about that 503 yeah you were really
lucky because doing the demos when we
practiced it most of the times it simply
worked and we never got the 503 but
luckily today in real we have seen that
500 twists are happening so how can we
make sure that you don't see that in
production because then it's less fun so
one of the main goals of this platform
as a service is to get zero downtime
deployments your customers should not be
aware that you are changing your
containers so what we are willing to do
is doing a rolling deployment where we
have the service completely up and
running then we are going to to add
let's say the new version in another pot
and what we want to do is that the
system is intelligent enough to say ok
this this new pot is also delivering
that same service and then if that is up
and running
yeah ok scratch the link with with the
old pot and you can delete the the old
pot for that to work it's very important
that the system is aware of when your
pot is ready and that was the main
reason why he was sending this this five
whole trees up to to now because we
never told the platform when our
application was ready to serve so what
they provide are two kinds of probes and
the first is the readiness probe which
is a way of telling the platform okay
my container is up and running I'm ready
to serve the traffic so the network
effect is that if you don't say it is
ready the platform will simply not send
any traffic to it that's nice that can
help in this in this deployment but then
if it's up and running and something
happens and other service dies your
database connection is it's lost
what are you going to do
yeah you can with the headiness removed
the container from traffic but that
won't solve the solution if you add at
the same time a lightness probe then you
can tell to the the platform this
container is simply dead it won't be
coming back and your platform will
simply restart your your container and
maybe that will solve the solution so
you're in complete control for each
container you decide whether you put a
probe or not you decide on what you put
in they support out of the box HTTP gets
where you can simply use the HTTP
response code to pass on the status you
can also plug in a script that is inside
your container that it will launch the
script and you can play with that with a
with the exit code and very important is
this timing so for each hope you will
say how frequently it is used and what
are the timeouts that you can can expect
maybe for you the service is not not
valid if it takes more than five seconds
okay then even if it will respond HTTP
200 but after six seconds it is still
considered to be to be that so you're in
control - to do that
so shield can we have a try on doing
this demo and let's hope that now the
500 trees are gone are gone I think so
okay log in again
okay my chrome doesn't seem to
appreciate this
hello okay so we go now there okay
so I've used the same case study as the
first demo step so just health care
provider really simple but I've updated
it okay so that now we have rolling
deployment so I show you that so the
deployment strategy is here put on the
deployment config okay and I put a
little bit of parameters who I asked him
to check every second so that it worked
which is kinda beefy okay with a very
big timeout and maximum zero and
available path so we always try to have
one working which is a bit of D the goal
of all of all that and then I defined a
probe I just told up and shift to do
exactly the same thing as I did just go
to the route and see that it works okay
on port 8080 which is the internal port
okay now we can see how that works
so to be sure that you can really see
that there is no impact okay I've made a
small script here okay that will just
pull the micro service and okay here and
it shows okay the content of the answer
and code which is a 200 ok I'm not sure
if you see that really good but here you
have a date which is the date when the
deployment started which here is this
morning typically okay so now I can show
you I'm going to redeploy and typically
the date should change but the response
status should be 200 okay so I go here I
go there I'm going to do a deployment
that's it then we can go back here and
see it in action so basically it's the
same stuff check Jeff describe it just
create another cloud now it's let's say
blue a light blue because it's not ready
so he's probing it okay it's done and
that's it if we look back go to our
script here we'll see that there is
nothing else than 200 okay and somewhere
around here the deployment was executed
and the date changed so that's just it
no impact let's say that basically it
really works in position right now so we
can typically do deployment just
whenever you want no more we can't work
that's it's okay so we'll typically go
to the conclusions now what can we say
about our journey into the G cloud okay
so it started a bit a year ago we
started to play with that a year ago and
let's say the first step we're not two
ways easy we had problems okay typically
but we kind of work together and make it
make it happen make it work so we're in
production since April the thing is that
it just gets better and better features
keep coming okay so there are new
feature like to scaling like pipelines
inside of openshift that we just receive
and so we can evolve and let's say at
that it's two architectures in
requirements okay so that's good one of
the things you might have noticed is
that okay I presented a strategy one way
to use the different things okay but in
fact you can really define yours and
because there are lots of possibilities
lots of tools lots of way to use those
tool so we this doesn't come with a
rigid process you just can do what you
want so that's one stuff and then for
the flexibility so yes we are really
happy because it's really flexible we
can really throw in a new service and
just put it up to production really
fastly without any manual intervention
just like that by creating new
configuration so it's really a good
point for us and really a good point for
architecture to have that tool really
goes with it and then on the end last
but not least
it works and doesn't crash we never had
any cascade crash we never had any
unavailability for I don't know some
hours whatever no we sometimes lost a
note but we know the impact so we're
kind of happy with with that right now
so I leave you let you conclude with the
related stuff yeah so the the main
message that we wanted to give here so
it's not slide where we showed you the
the play the play examples because yeah
we can't bring the complete G clouds on
to a to a laptop but it's really out
there so Jill is there with with over
six six environments with all the micro
services on it and they're scheduling
360 pods for that but they're not the
only ones at this moment we are serving
for for nine different tenants so we
were up to two to nine hundred
containers on that and okay they're
clearly not all new developments like
Julie's is doing so this platform is
more than just a one-time for micro
services we are also using that to
migrate existing software that we that
we had in more traditional ways and that
is flexibility really place because the
the approach that you take from a micro
service is not we did the same that we
are going to take if you are migrating
an existing application so the the
platform offers a lot of flexibility
already and like Geocities it's
improving on different levels in what we
have shown you here the version that is
now in production there are still things
that that that remain a challenge for
the for the user so we haven't showed
you monitoring because we we don't have
a lot of integration there how to get
your your application looks centralized
how to get metrics from that but that is
also the the vendor who is in the
community because openshift is a
community version what we showed you
there is the community version of
openshift in fact they are adding
features but also the g-cloud team is
adding more services on top of that is
integrating this platform with a lot of
existing services in this G cloud
so more is definitely coming and like
Jill already said integration with CI
father for example so that's still these
are some some 10 minutes for for
questions if you have a question we were
asked to direct you to the mic that is
located at the the light post there feel
free how how tighten is your application
code to the OpenShift platform now how
easy would it be to migrate to another
platform as a service for example so
except for some specific two links that
we used like the graph generator okay
the application themselves are not tied
to openshift they just docker image
using environment by others sometimes
okay files but nothing nothing specific
there is no open shift library or
something like that - just regular in
our case java applications that are
exposed like a process that's just it
and also the configuration files that
you use are OpenShift is using
internally the kubernetes scheduler the
configuration files are not open shipped
specific so if you take another
kubernetes based solution or you install
kubernetes yourself a lot of these the
deployment configs
the pods the routes and then things like
that they will also just work so it's
really an open an open system and yeah
that is one of the the key reasons for
why we chose that to build our platform
on it just to make sure that we are not
linking into a one specific solution but
to remain flexible to see how the the
solutions are evolving
no more volunteers okay so thank you
very much for attending if the questions
still still arrives later one smalls has
a boot in the exhibition area you can
pause that we will be there for you
simply give you a question there and we
can get in contact thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>