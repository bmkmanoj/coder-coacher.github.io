<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Performance Puzzlers by Douglas Hawkins | Coder Coacher - Coaching Coders</title><meta content="Java Performance Puzzlers by Douglas Hawkins - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Performance Puzzlers by Douglas Hawkins</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wgQBz2Ldhvk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome everyone thank you all very
much for staying to the end of today for
this talk my name is Doug Hawkins
I work in absolve systems on the
just-in-time compiler and related things
around it and this is a talk about Java
performance in the spirit of Josh blocks
book on Java puzzlers this is Java
performance puzzlers we're going to look
at a few cases of some surprising
performance outcomes where you think
something should be faster and isn't
faster or there's a non-obvious
compositional problem or something along
those lines and it is worth saying that
unlike most JVM developers who actually
don't write a lot of Java believe it or
not we write in C++ I was a Java
developer for 10 years first so I've
actually been on both sides of this I've
written genomics applications financial
service applications ecommerce
applications in Java so I've done both
so the agenda is pretty simple we're
just going to look at some interesting
and often unintuitive and sometimes
funny performance cases and see what we
can learn from them some of them will be
serious some of them will be well not so
serious and we're going to start with a
not so serious one okay a B or C do you
think it takes longer to add the numbers
from one to 1395 one to 1396 or one to
1397 if you guessed C you are correct it
is going to take a little bit longer but
but how much longer well that's what
we're going to find out but how are we
going to measure this
measuring java performance is actually
pretty hard if you came to my talk
earlier today you learned about all this
jet machinery going on and it compiles
methods at different times and it's
going to start slow and it's going to
run fast later and that makes
benchmarking difficult a typical Java
program is going to go through something
like this it's going to start off
running in the interpreter it's going to
take tens of thousands of nanoseconds to
do the simplest of things if it's doing
it enough times but after that happens
for a while you'll run in the first tier
JIT and things are going to get a bit
faster and if you keep running that
piece of code it gets truly hot and the
only parts of your system that matter or
those that are truly hot then you're
going to get to the second tier jet and
if your code didn't get to the second
tier ghent well it just doesn't matter
performance wise you might have heard
you spend 90% of your time and 10% of
your code no that is a lie it's about 3%
of your code it's a vanishingly small
percentage and then it's actually
possible for your code once it's gotten
to this optimized here to go backwards
and go to the interpreter and run slow
again and then to see one first go jet
and then to see two again and all this
is going on underneath and you can see
just from looking at this chart if I
profiled at the very beginning if I'm
that's what I measured I would get very
different numbers for this particular
one where I get like thirty thousand
nanoseconds while I'm in the interpreter
versus it's actually only a couple
hundred nanoseconds once it's in the
second tier jet so you really want
something to remove this for you and the
tool of it we in the java community has
largely settled on is something called
jmh the java measurement harness it's
basically a tool that lets you write
micro benchmarks kind of like j unit
tests so you say using annotations how
you want to measure i want to benchmark
them using average time tell me the time
output in nanoseconds if i got some
setup code i annotate a method
set up and put it there set up all my
test data and then I can annotate not
with test but with benchmark to actually
have my benchmark methods and then I can
have a few of those so I can compare how
long they all take and then jmh will
take care of running my code enough to
make sure it gets to the second tier JIT
and truly is hot and we're really
measuring the right thing so let's see
it let's check adding some numbers now
I've chosen to add the numbers in what
we might agree is a non optimal way
here's my benchmark if we scroll down
this is how I've chosen to add the
numbers from one to 1395
yeah maybe a loop would have been better
don't worry I used a loop to generate
this code I know how to use a loop to
generate code that I should have used a
loop for but we're going to see how long
it takes I'll let you imagine what 1396
and 1397 look like to get started using
jmh you kind of have to use maven jmh is
doing a lot of interesting plugging into
java c under the covers and it's just
easier to get all that to work if you
use the maven archetype okay there we go
it has built I could choose to run the
entire benchmarks jar you do not want me
to do that that we will be waiting well
past six o'clock so instead I'm just
going to run one benchmark at a time
using some scripts that I've already
made
right by default if I had not annotated
my benchmark jmh would be
extraordinarily thorough it would do ten
warm-up iterations it would do ten
measurement iterations and it would
actually do ten forks of the VM and if I
did that this simple program we would be
watching the benchmark run for about
twenty minutes so I've cut this down to
two forks of the VM to warm-up
iterations and five measurement
iterations you're going to just have to
trust me
that that is not going to change the
outcome much all right so here are our
results 1395 numbers takes 3.7
nanoseconds 1396 numbers takes 3.7
nanoseconds 1397 numbers takes 20 680
nanoseconds while that lasts ad was
expensive what that makes no sense what
happened here
okay so 3.7 nanoseconds is about enough
to return a constant and that's about it
and that's in fact that's what's
happening here when the just-in-time
compiler gets a hold of this code it
goes ooh
guess what I'm really smart one plus two
is three
mmm and 3 plus 3 is 6 and 6 plus 4 is 10
and it can actually pre compute the
entire result and all it's doing is
returning a pre computed value once the
just-in-time compiler gets ahold of it
so why couldn't it do that for 1397 oh
it could it chose not to there is an
arbitrary limit inside the VM about how
big a method we are willing to just in
time compile and that is what we have
run afoul of performance is full of
surprising performance cliffs like this
in this case
there's what's called the huge method
limit inside of the VM at a certain size
we just say now what is wrong with you
and it turns out that adding 1395
numbers is about seven thousand nine
hundred ninety three bytes of byte code
adding 1396 numbers is 7999 bytes of
byte code and adding 1397 is eight
thousand and five would you like to
guess what the huge method limit is
eight thousand yes and that is the
difference here but there are all sorts
of arbitrary things like this I actually
worked for a company where the hot
method in the system was some indeed
nine hundred bytes like wow that's been
scary
oh don't worry this thing already took a
hundred and fifty arguments and there's
another arbitrary limited 127 arguments
so that just wasn't getting jetted for
like years okay okay but this example
silly maybe we should just use a for
loop that would be a little more sane
and what number would we get ah then
they're all nice and consistent four
hundred and thirty five nanoseconds
they're all consistently bad they're a
hundredfold worse but hey there you go
nice and consistent the compiler should
frankly be able to optimize this and it
it failed
okay let's try something a little more
practical I want to copy an array of
integers how might I do that efficiently
I could use system array copy I could
write a for loop myself or I could call
array clone as far as I'm concerned this
is the one acceptable use of clone okay
which might be fastest well system array
copy whoever looked at this thing it's
got the craziest signature I've ever
seen
takes object int object and int because
the only parent type of all arrays is
object should we have just overloaded
the method prop
probably did we know well that seems
like it might be kind of hard to
optimize maybe on the other hand it is
native and that sounds cool and fast
okay or I could just do this for loop
and I get these nice strong types maybe
that will be faster hmm let's see if we
actually do this for an array of a
hundred thousand integers okay they all
turn out the same they all come in at
about ninety-five hundred nanoseconds
that's not too bad so it really didn't
matter in the end but you know what I
really want you to call a ray copy I
really really do it is actually faster
under a particular circumstance when the
VM is starting up yeah this code is ice
cold but let's go ahead and make it fast
if we can do a one-shot measurement in
the interpreter so in the top left we
are measuring doing a manual for loop
and in the lower right we are measuring
doing array copy using both with system
nano time and if you run this program on
this laptop this is what you get the
copy loop is two orders of magnitude
slower why well system array copy
despite its crazy signature is this
thing called an intrinsic we have
magical knowledge of what it does there
is actually no code Java code for this
it is inside the VM itself and we will
actually generate machine code optimized
to the Machine you are running on the
very first time you've got a V X 128
great we'll use that AV X 256 even
better we'll use that avx-512 well those
aren't out yet but if you had one of
those yes our VM would actually use that
too
and you just get to have that it's free
from the very beginning eventually it
will get there with the manual copy loop
when that code gets hot but at the
beginning it's going to be slow and
performance is full of all these little
intricacies your homework for tonight is
to go find the list of intrinsics for
the VM and memorize it it's only about I
think 270 today and I'll come back and
ask you all of them tomorrow but one of
the other things I want you to see from
this example is you know both of these
are nominally order in there's really no
difference here from a Big O standpoint
but clearly one has a slightly larger
constant factor and often times when it
comes to optimizing your code it is much
easier to do something that reduces the
constant factor than it is to completely
rewrite your algorithm from you know N
squared in log in for instance if your
collection has a bulk put operation on
it like at all or put that takes an
array on a buffer you absolutely should
call that it is likely to be faster by
the factor of ten okay let's move on
something else I want to take an integer
and I want to take an int and I want to
turn it into an integer how I'm like I
do that I could call integer value of I
could call new integer or I could just
you know put the X in an integer context
and it'll get auto box what do you think
is faster well the answer is it depends
you can't get this one right so it is
correct to say that a and C are
equivalent auto boxing is just syntactic
sugar for calling integer value of you
could actually go and disassemble this
which LVPD and you could see that for
yourself and hopefully you're willing to
agree that if we give it the same bike
code as input we should get the same
result
the reason that you can't quite get this
right though is behind integer value of
there's a cache it caches the numbers
from negative 128 to 127 by default you
can actually configure the upper bound
you can't configure the lower bound for
some reason and it's just an array
that's all it is we pre compute the
array there's no hashing or eviction or
any of that it just calculate index into
array return it super fast super simple
how could this ever be slow well let's
see
to test this cache we're going to need
to parameterize our tests and jmh lets
me do that i can parameterize a test by
just using a pram annotation and telling
it what values I want this variable to
have and then in my setup code I can use
that
so I interpret the value of 100 to be
used the range of integers from negative
100 to 100 so that would always hit cash
negative 1000 to a thousand will hit
cash some of the time and then negative
ten thousand to ten thousand well okay
it hits cash some of the time but
basically never and here are the results
if you did this with jmh it would show
you the range column this is a little
bit hard to read so I put it in a nice
table here you go
so new new is very very very consistent
because it's only doing one thing it
always comes in at somewhere around like
6 nanoseconds Auto box and value of well
yeah they are the same when they hit
cash there absolutely better about 5
nanoseconds for the left column there
that's 20% improvement or 15%
improvement pretty good um but when they
miss cash they're actually worse even a
simple simple cache look up like that
does impose about 10% overhead in this
case I mean
this is a simple look up and so for
return so don't take those relative
values too seriously but it is slower
but you see the really strange numbers
here it's to an orange Y is negative 1/2
thousand to a thousand even slower still
why would that happen because when we
always hit the cache the hardware will
predict that we will always hit the
cache when we most almost always miss
the cache the hardware will predict that
we almost always missed the cache guess
what happens when it's about 10% of 1 in
90% of the other the hardware gets
really confused and so it actually goes
slower even for something as simple as
this yeah unfortunately in performance
every layer of the system matters
hardware included so there's a really a
lot to absorb to try to get to the
bottom of this to take another example
where the hardware has a strong
influence
I just wanted to iterate over a list of
integers or an integer advantage array
of integers or a linked list of integers
and see how long it took to sum up all
the values the blue line here is an
array events and you see it's rock solid
very low from 100 all the way up to
website a million array list is the
Green Line it does sort of go sideways
at a hundred thousand because of the
amount of memory that the boxed integers
are going to take up and then there's
linked lists that's the yellow line if
you were wondering but all we're doing
is iterating over cements it's all
nominally order in why are they so
different well link list is like this in
memory
everything you're putting in there is
actually two objects right there's the
actual integer itself and then there's
the node in the linked list and that
takes up a lot of space and it's also
just kind of jumbled in memory hardware
doesn't like things being jumbled in
memory it wants nice contiguous compact
things ArrayList is better in this
regard the blue things you can imagine
those being ents and the array at least
in hotspot is one contiguous piece of
memory and that's good when you load the
first element of that array a cache line
on x86 is 64 bytes so you're loading
eight or even 16 in all in one go
that's really good but it's better we're
just going through this thing in order
right we're going memory offset plus
zero memory offset plus four memory
offset plus eight memory offset plus
twelve guess what the hardware can guess
what's coming next memory offset plus
sixteen and so it will see this and it
will actually start prefetching the next
cache line into memory into or the next
cache line into last level cache and so
you will always hit the l3 cache more or
less with the link list you could wait
50 nanoseconds to go out to main memory
this you'll wait five maybe probably
less much more predictable but we're
still having to go off into this other
part of memory to get the objects we can
do better still if we used an integer
array well then you can kind of imagine
the data looks like this the data is
just in the array and that's super
simple for the hardware to deal with and
your performance is much better
obviously there's less to garbage
collect everything is good if we could
get value types and do generics over
value types that would be awesome
and again everything here is order in
but clearly there are some performance
differences in fact linked list has a
curve that doesn't look order in at all
because of the effects it's having on
the memory subsystem and memory accesses
taking variably variable amounts of time
okay before we move on from lists I've
got a list of strings and I want to turn
it into an array either of objects or
strings which do you think is the
fastest way list to array list to array
with a new object array of size 0 or I
should size it properly what do you want
to guess well if we use fine bugs flying
bugs would tell us we should definitely
size the array ok let's just see what
the benchmark results actually say hmm
okay first off string a razor a lot
slower like a lot threefold why well to
answer that question you have to
understand this piece of code object
array objects is equal to a new integer
array objects 0 equals 2 the string foo
does this code compile yes it does what
happens when you run it it raises a
exception hopefully most of you have
never seen an array store exception
which means I'm sorry we we have an
inner right here why are you trying to
put strings in it but this is done
through runtime
checking so every assignment to an array
potentially has an extra bit of check
type checking associated with it and
yeah that gets expensive but of course
the object array is kind of special if
we see that our array is an object array
which objects are not objects
none of them so we can make that cheaper
and in fact that's that's what is done
so when we're copying the contents of an
ArrayList to an object array we can
safely get rid of all those checks but
when we're copying to a string array we
have to check every single element as
we're doing it and that's where the
overhead is coming from but findbugs is
also wrong about the sized elements as
well you can actually see that two array
on a sized array is ever so slightly
slower than unsigned or just not having
an array at all and letting it create
one and the same is actually true for
Strings but come on I created an array
of the right size for you I mean I tried
to make this easy why would that be
slower well it depends if you give me an
array
you've already constructed it and you've
already had the 0 allocated or 0
initialize it but if I create the array
if you give me an array that's too small
or you just don't give me one then I can
do a malloc of the array and then
technically yeah sure I'm supposed to
zero initialize it but I'm just going to
copy over that immediately anyway so
then the just-in-time compiler can go
well I don't really need a zero it let's
just copy over it and we're done but if
you give me some array I don't know what
state it's in then I have to worry about
all this I don't get to have that so you
know that's where the difference comes
in it's really unintuitive but that is
the
not that it's a big difference mind you
but that is the case okay let's take an
integer and convert it to a string
integer to string concatenation with the
empty string which really I don't know
it offends me somehow I just don't like
the way it looks but okay that's an
option or I could use a stringbuilder
scheme seem pretty good to me
well first off I didn't actually tell
you what num was you probably assumed it
was a static final because I made a
capital that's a good assumption but I
didn't make it a runt a compile-time
constant
I made a runtime constant that's
actually random so we're not going to be
able to pre compute anything at compile
time and then we run this thing and we
find out that builder is the slowest and
that just seems a bit strange to me
because if you know what concatenation
compiles to it just uses a builder how
did a builder beat a builder the actual
bytecode for the builder as I wrote it
with something like this for
concatenation is this if you're not
liking byte code the Java code would be
this so we do a new string builder app
in quote quote append num it's a very
logical transformation from the
concatenation okay but what did I do
I created a new string builder I just
appended numb to it and I called to
string on it I think I did less why did
that run slower okay well let's try a
straight-up comparison of these two I
will write that code manually the same
thing I should get from concatenate to
read I wrote before
and we'll see if we still see a
difference yes there is a difference and
then if I write the code that the
compact but the concatenation like one
okay it's like concatenation weird
really really weird what happened here
ah so the VM has a very specific
optimization that knows what
concatenation looks like and it goes and
looks for that but it only represent
recognizes very specific patterns don't
add a local variable that will confuse
it it will recognize a very specific
pattern and it will optimize that
pattern and that pattern only basically
what you need to understand is compilers
are basically glorified reg X's all we
do is pattern match code and then we go
okay well I can replace it with
something a little bit faster still now
I assume all of you have had to write
reg X's at some point in your life you
had some log file you had to turn
through and you wrote some rec X's and
then you got your second log file did
you handle every single case correctly
now and could you debug your reg X no
that is what we have to deal with so
what do we do when we first made this
reg X so to speak well we took the input
we had we took a lot of your Java
programs and we said well a lot of
people seem to be doing it that way
let's make that fast
excellent and then the other person
comes along and they read it a little
differently
whatever and that's sort of what we do
like even if we knew this ahead of time
we'd go well if I do this thing for that
guy for those nine people over there I'd
make it better for nine people what
about that weirdo well he's weird so if
you think you're getting clever and you
write intricate special looking code be
prayer for special treatment no I did
not say good treatment I said special
treatment
now let's take num and turn it into
something that is actually a
compile-time constant I'm going to say
it's one shifted to the place left 20
times and this is the result you get
okay well we learned before three
nanoseconds is about how long it takes
my laptop to return something and that
is in fact what happened here that is
all that is going on if we go and run
Java P and disassemble this we'll find
out that the actual bytecode of can cat
now is a return the string 1,048,576 and
that is it how that happened well even
Java C which is a deliberately dumb
compiler does about five optimizations
fortunately only needs to to pull this
off so we take one - the one is shift is
the left 20 times and we go well that's
a constant a binary operator and a
constant it's basically ARAG X I can
compute that okay great
num is really 1,048,576 all right now we
take num we go ah we have the constant
string empty and a constant number
1,048,576 let's again constant fold dos
and now we end up with 1,048,576 as a
string literal and that's it brilliant
produces code in just runs
instantaneously it'll actually run
instantaneously in the interpreter but
you can see that performance doesn't
entirely compose intuitively a seemingly
small change even not even changing
whether the variables static and final
but just changing its right-hand side
very dramatically change the performance
outcomes and now no matter how I wrote
the builder manually it's not going to
do as well as can cat so can cast is
always the right answer that's the
takeaway right No
no can't be that simple so now let's
take a string and we're just going to
add all the number concatenate all the
numbers up to 100 why I don't know I
felt like it and then I'll try it with
the Builder as well we're going to get
the Builder another chance and what
happens if we run that well the Builder
wins comes in at 918 a know seconds to
concatenates nearly 2700 so the Builder
is faster by a factor of three that's
pretty good oh it gets better
think about what happens when you do it
can't use a string builder or do a
concatenation what actually translates
is to is create a new string builder
which has the character rate inside of
it copy all the characters from the
string we started with into that
character array add a little bit on the
end now call to string which has a
string and another character array I'll
by the way copy all the characters again
so we create four objects for that
concatenation operation if you chain
them together you'll it'll do a little
bit better but nominally it's four
objects so I used another tool called
caliper from Google to actually measure
the object allocations not just the
performance so concatenation produces
four hundred objects and Aamir's 72,000
bytes of memory are used to produce a
less than 300 character string well done
the builder generates nine objects you
might think well shouldn't I just have
like a string builder
a character array a string and another
character array yeah but the character
array the string builder started with
was a little too small it was 16 and
then it grows exponentially to 32 and 64
and 128 and 256 and ultimately ends up a
develop 512 - Comedy are 300 characters
to create a extra five character raise
we don't really need but we only use
sixteen hundred and forty bytes of
memory that's not so bad but there is
something there I can actually tell a
stringbuilder how big I want it to be
and I know roughly how big I need it to
be 300 characters so if I did that
ah performance improves a little bit not
a lot but now we're down to actually
just four objects
and fourteen hundred and sixty-four
bytes okay that's looking pretty good
now admittedly for most of us this just
doesn't matter I told you one of the
things I used to work on with genomics
applications when you have character
strings that are 500,000 DNA nucleotides
this does actually start to matter and
in fact this is true for all of our
collections we could actually just size
them appropriately most of them start at
like 16 for hashes 10 for linked list
and vector string builder and string
buffer honestly the only ones I think
are probably a little too small because
they are slide 16 and we tend to produce
strings that are a bit bigger than that
but fortunately the concatenation
optimization when it works usually makes
that kind of a non-issue the rest of the
time we usually use small collections
and these sizes are okay there's one
here I really do actually hate vector
now you've probably thought it was
vector but you probably thought it was
because it's synchronized now I don't
care about that that's not the problem
with vector now the problem with vector
is it doesn't grow exponentially it
grows linearly so think about that for a
second if you're doing in a pin loop and
you haven't sized it appropriately with
most of these collections you thought
you wrote something that was order in
what you actually wrote was something
that was in login every login steps
through we have to do an order in copy
of the backing array with vector
now it grows from ten to twenty to
thirty to forty so what you thought was
order in is actually N squared
brilliant yeah that that will hurt but
hey you can actually set the growth size
so that's something and now for our last
java is dynamically dispatched there's
no getting around it by default it is
dynamically dispatched and we need to
optimize for that and so which you
think's better do you think if I have an
object invocation and there's one type
flowing through it it performs better or
there's an object invocation there are
two different types flowing through and
it performs better or there's an object
invocation with three different types
going through it and that performs
better you can probably guess what the
right and wrong answers to this are I've
made it kind of obvious by saying that
we say three types is mega morphic which
is basically we gave up but let's try to
demonstrate I can guarantee you there is
a cliff at three types but let's try to
demonstrate this the reason I say
there's a cliff there is because I know
what the machine code it produces is if
there is a function apply so to speak
and there are two types flowing through
it or less than two types it will expand
into say an if-elsif of if the type is
square well let's just inline the body
of square apply which is you know x
times X or if it's a cube let's inline
that and just do x times X times X else
we do something else versus if it's more
than two types I can tell you it just
calls function apply and it has to do a
dynamic look
okay let's try to let's try to actually
benchmark this set up whether this one's
a bit complicated I've created an array
of integers actually in the static block
of the benchmark and then I call set up
and set up has a morphism that is
parameterised like we saw with range
before and we go through and we warm up
the benchmark code with different types
of function objects it morphism is
greater than or equal to one we use
square and we warm up the benchmark code
if it's greater than equal to we also do
it with cube it's greater than equal
three we do it with a cleverly named
class called
another square and if it's greater
before we do yet another Square and then
at the end regardless of how many types
we've shown it during the set up I just
use a square so at the end of the day
we're just going to use one type it's
all about how many types did we show it
during the set up base and this should
actually change the code gen and we kind
of expect to produce a different
performance result but in fact it
doesn't three and four look just as good
as one and two but I thought like we in
lined the code if there were less than
two types and otherwise we did dynamic
call and isn't that slow no on a modern
x86 that isn't slow it will actually see
that dynamic call and because I'm only
using square at the end of my test it
will go oh well you're just always going
to square this is easy I can predict
where you're going to go before you even
know it and that's what it does it
prefetches what it needs based on
predictions just like with the if before
and so you can't see the difference on
an arm you could probably see the
difference by on x86 you can't or
you what happens if I put this inside of
a loop that works for concatenation
let's try it here - okay change my test
a great deal it is now this call loop
sum all the values of the calls over the
nums that's it the set up is otherwise
the same and this is what you get Oh
apparently one is actually the best you
probably guessed that correctly - not
bad I mean it goes from four thousand
nanoseconds over this array of like a
thousand ends to six thousand but three
and four yeah there's there's a cliff
there what actually happens is the call
is left when the call is left inside of
a loop it doesn't slow the call down per
se but what it does do is destroy the
loop optimizations and so you can't
evaluate performance in isolation very
easily
you can't go oh I wrote this benchmark
to see if the call optimizes and go ah
three four ten types look great to me
yeah that doesn't quite work and even if
you hadn't used just one type in the end
after you'd warmed it up with a bunch
the numbers for just varying it would
have looked a lot better than the
numbers for putting it inside of a loop
performance doesn't compose in obvious
ways you can take two things that are
fast and put them together and get
something that's slow and that makes
reasoning about performance quite
difficult
unfortunately in performance is just
full of surprises it's often not
intuitive it has all these intricacies
it has these sensitivities with these
weird cliffs like the huge method limit
it has it's not additive in a way that
makes any sense to us it's really really
hard to reason about if you don't live
in this ever
des and I'm guessing most of you
probably don't want to but I don't want
you to be too doom and gloom about this
what I actually want you to see is I
just want you to write clean code if you
write clean like tiny methods you write
code in the way that most people write
code we're probably going to optimize it
pretty well you know if you start
behaving like a crazy person and produce
methods that have 8,000 bytes of byte
code in them you've gotten what you
deserve
don't do that
we will generally take care of you yes
there's some weird cliffs like the two
types thing and that sounds terrible but
you know what we looked at it
90% of call sites even if they're in a
dynamic call you want to guess how many
types they use one another 5% use too
and that's why we give up at 3 it's just
like well we covered 95% of cases that
one in 20 guy and the hell's we did
actually add support for three types to
our VM so if you're one of those people
come to us it's really not so bad but
you do have to be careful you when you
are pursuing performance things it's
very easy to mislead yourself to write a
test in isolation that you think shows
you something but unfortunately it
doesn't oh and let's all agree this is
the best way to add the numbers from 1
to 1396 because this is the fastest
right okay we could have done this that
might be better
a little grade school arithmetic might
actually be a better result and yeah
that optimizes no matter how big your
number is ok I hope you found that fun I
hope you found it interesting if you
want to learn more I would welcome you
to look at these references alexey
shavelev
is the guy who created jmh he's done all
sorts of performance blogging I
and lots of footnotes - far more
detailed explanations of these things
from his blog absolutely go look at this
his stuff if you're interested in it the
Java specialist newsletter is also a
great reference psycho Banach lobotomy
saw is the blog of my former colleague
knits on work art and mechanical
sympathy is a blog that talks a lot
about Java performance but also
low-level hardware performance as the
name might imply so thank you all very
much for coming I hope you enjoy</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>