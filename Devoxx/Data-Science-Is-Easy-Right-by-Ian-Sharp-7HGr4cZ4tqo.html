<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Science Is Easy Right? by Ian Sharp | Coder Coacher - Coaching Coders</title><meta content="Data Science Is Easy Right? by Ian Sharp - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Science Is Easy Right? by Ian Sharp</b></h2><h5 class="post__date">2017-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7HGr4cZ4tqo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">where again you play KITT do it make
sure all right
apologies being slightly late I'll dive
straight into my name is Ian sharp I'm
data science lead at Oracle UK and the
theme for this session is as data
science is easy that's the title behind
what we're sort of talking about today
so obviously the answer is is not so you
can leave now see that's the giveaway so
but the background to this is that I've
been working in in the space both at
Oracle and previously with Microsoft for
a number of years and prior to that I
was in academia with SPSS and the really
I thought what we do say is just to
explore the opportunities and the
challenges that we sing right now with
the term data science and and how that's
playing out in the market I hope he
tries to try and separate the reality
with with sort of some of the myths that
are going on there so hopefully this is
a reality check but also lots of
excitement as well so past present and
future of what we're doing in this space
and reactor before we get too excited
about the whole thing is that reality is
that for a lot of organizations is
actually proving and we do it big data
or data fication or data side we really
don't think it matters a lot it's useful
as branding exercises about what we do
with our data we think there's a value
in there within this belief in that data
that we can such a Knauer organizations
or however we interact with the world
but regard to what we we talk about the
premise and I thought we were a super
interesting point in the market is that
what's the premise that we can extract
value from our data and operationalize
that into into the reward is still there
and then probably even more so the
realities that organizations are finding
a lot harder than maybe they initially
would so three or four years ago this
market first appeared as a nascent
technology from lots of Google and Yahoo
and the first generation of
distributions around machine learning
and an open source starts to emerge in a
commercial world certainly over here
then there was a lot of exciting about
units of standing up these or new
technologies and it was a lot of you
know messing around with some stuff and
a good opportunity to rebrand the
reality is the ones that was obviously
still hold
through organizations have struggled a
lot more than maybe they initially
thought and maybe we could have foreseen
this but but we'll explore that a little
bit as to why that's happening and the
reality is that it's hard in certain
ways that that perhaps aren't really
aren't really the sort of thing we want
to force would have foreseen so we
talked about the idea of good hard bad
hard and we'll talk about a little bit
of ugly hard later on but the kind of
the good hard is obviously your data
scientists your your knowledge workers
sweating your information assets finding
knowledge from them and then
operationalizing the value putting it
into the mainstream the bad heart is
actually getting to that position in the
first place and what we're seeing which
is really frustrating for people like me
a lot these parties are starting to fail
so a big retailer we're talking about it
going we partnered with cloud era in
this sort of space with a big retailer
we do a lot of digital marketing and
talk about a lot of these things that if
you laid off all their data scientists
which is hugely annoying because it's
too hard to get the data into the system
in the first place I'm working with a
big pharmaceutical company and they've
got lots of you know bright data
scientists they using scikit-learn and
pandas and all the things that we get
very excited about and we spit and talk
to them and we take what are you doing
with it this is what it's taken 12
months to get any reports out of the
system in the first place because it's
really hard to make a lot of this stuff
work together and similarly you know a
big tail go we work with they're
switching off all their open source come
on and this is not an anti open source
message at all it's very much the
opposite
but they'll ensure all this or build
your own open source on Amazon because
the CTO just sees huge costs people
spinning up instances and not really
seeing any value and so this is a real
massive problem so we are seeing sort of
opportunities around this because
actually there's a huge benefit to be
had from doing some stuff around this so
what we resore talk about is take away
those kind of bad hard which is actually
getting the things to work in the first
place getting all the stuff to connect
in the first place
getting it to scale all the boring hard
we kind of think there's value in doing
that you know that's the kind of stuff
that we come in and do in this sort of
space so leave your guys with the good
heart which is actually sweating your
information acid I know how many of you
would call these update scientists have
used packages like Jupiter and and the
things so smattering of you guys in
there okay so I mean I
I'm not is a package what anaconda which
is a framework for managing your your
Python packages for machine learning and
for tips and giggles I decided to
install and a controversion tree is
opposed anaconda version 2 on the Big
Data light image that we had not only
did all the applications that I wanted
to work with it all my or my Jupiter
notebooks and more my Python package
stopped working but everything that used
PI spark a spark in general stopped
working because there was a conflict
between my my spark Avro files there was
a version JA constraint that without a
sync not only that but I couldn't be
install anything because it kept on
leaving a whole bunch of residue around
system and that's basically everything
stopped working on my machine and I had
to rebuild so it's really easy to be
stupid like me and and get it wrong so
the whole thing that we're sort of
trying to do just put a bit of
pragmatism there and say well look we'll
take a clip we'll manage the installs
we'll make sure it all works together to
leave you the good stuff so that's
really sort of messages and you know
it's scary and what I thought I'd do is
I'd show a couple of pipelines and some
design patterns that we're seeing that
hopefully illustrate the different
approaches to doing this and but and
also share some stories because you've
just far more interesting hopefully than
hearing technical technology vendors
talking about themselves so I want to
share with some examples of things that
we think our customers doing are really
kind of cool
so the first one and who've gone through
this journey probably more importantly
so first one I want to raise as a forum
group I don't know how many you guys
come across so they are a big sports
digital media provider so in the US they
manage the sports franchises for for
American football for baseball for
basketball in Europe they're probably
best for known football so and rugby so
they manage the Optus statistics
effectively they have two streams of
data they have what's called event data
so they have a bunch of kids sitting
with Xbox controllers which is my son's
dream job watching football matches and
they sit in there tapping away and
they're capturing this information
real-time and it's everything from
really trivial stuff like a foul throw
or you know Venga remonstrating with the
ref right through to the goals and the
big deals in the matches okay and they
provide this through to the Tier one
broadcasts
sky ESPN BT BBC all consume this
information in game they provides the
broadsheets
The Times The Guardian et cetera et
cetera and they also provide it to the
club's themselves the Cabal clubs
ironically are becoming increasingly
statistically obsessed the extent that
the Golden Triangle if you heard about
this before so any any of you follow
football ok so the idea behind the
Golden Triangle is that the pitch gets
broken into into thirds and the
attacking third is the most dangerous
zone and there this concept of the
Golden Triangle which is the supporting
holding midfielder behind the to
attacking guys passing into a space
between these these two players and
their and they caught the Golden
Triangle because they know that the
successful passes between these two
angles
I wrote the resulting goals most most
often so it's a guy who plays a Man City
called David silver who somehow a genius
at Euclidian geometry because his
passing angles are better than than
anybody else in this sort of whole space
so it's kind of absurd you know they
talk about but it gives you an
indication to set how stats see sports
companies are becoming so a massive
opportunity around this whole area these
guys talk about the fact that last
season Leicester City did an awful lot
of analytics Aston Villa did very little
one of them won the championship one of
them got relegated I'll leave you to
join the dots as to which one was which
in case but nevertheless a huge
opportunity the other side of their
business which was probably even more
exciting is the idea of taking real time
Internet of Things data and that's what
we're going to be focusing on in our
demonstration as we go through so I've
got plenty of time and so this is
basically 23 high-def cameras capturing
25 playing per second information around
every single football match so La Liga
in Spain the Premiership and so on and
so forth and there's a massive
opportunity through this organization to
merge this information in real time to
drop it was millions and millions of
pounds to them so for example the
broadcaster's want to know when a girl
scores where every single player is on
the pitch
clubs are obsessed in terms of looking
at player under pressure for example so
which of those strikers is best under
pressure and they define pressures
having two opposing players when their
radius of six feet when the goal happens
but this is a really hard problem to
solve because all we've got we've got
next y-coordinate of every player we've
got an XYZ coordinate of where the ball
is that's all we've got
and that
all we know so when for example when
we're trying to match up these these two
events they set and this amazing little
guy here Messi does what's called a
nutmeg which is when he nudges the ball
through somebody's legs and he splits
around the other side for a moment in
time he's actually lost control of the
board as close to somebody else it is
quite hard for us to work out what's
going on here so we did a whole bunch of
sort of sohcahtoa type sequel so I logic
around that and some read ahead sequel
to make that happen
but actually there's a whole bunch of
error built into this so for example
cameras fail so some of these cameras
fail sometimes players warming up is a
nightmares all the pictures of different
sizes so you've got two guys hurtling up
and down in bibs we kind of know who
they are but to our feet it's just these
two little sort of dots are working
around their corners are a nightmare
because all players hurtle into the sort
of box at the same time so it's really
hard to pick out who these individuals
are so from one perspective it's a
massive opportunities worth millions to
them in real time if they get it right
if they get it wrong and they start
spewing garb garbage to sky then their
whole brand equity is completely
destroyed right because that's what
their base are that is all that quality
of data so the solution is actually
effectively a mix of some trigonometry
some machine learning so we've run some
single class support vector machine
anomaly detection with Gaussian kernels
and some cool little algorithms around
that in real time to sort of spot
outlier behavior and faults but actually
what it really is is is a real time risk
modeling solution that's happening in
this sort of context so it's when we
have these streams of data coming in
real time you know what is that what is
actually happening is it real it is what
we're seeing what's going on if it's not
in the probability of that being high
then we can intervene we can look at
that we can pull it from the stream etc
etc so although it's quite specific
sports media is quite specific what
we're really interested in doing and
what's cool about these guys is that we
can cross pollinate the ideas into other
sections the they've actually had every
bit of open source commodity stuff so we
do use open source for this so
somebody's using scarlet and some of
it's using so we're actually doing part
of that part of the thing which is quite
interesting is they do real-time gaming
so they have a bunch of data scientists
working in Poland they're building
scarlet algorithms in real
time to profile corner situation and
looking at the odds in terms of what's
going to happen for bookmakers so it's
pretty it's pretty leading edge as far
as I can tell in terms of the
applications we come across but the
reason they invested it in an Oracle
platform is because you know we needed
it all together set to get this thing up
and running by the start of the season
so really interesting company in terms
what they're doing the other thing oh my
god so the other thing the other thing
that's interesting about them from a
cultural point of view is they're all
the data scientists they have lots of
bright young data scientists coming out
of New York and they get poached by the
baseball teams out there when they bring
them over here they get poked by the
football teams because there's so much
money involved in in Academy so Chelsea
in Man City who I'm sure you all the way
if you don't follow football are
massively rich corporations interesting
enough what they what they're concerned
with is not the value of the players
that they're going to buy because they
really care about that they're okay
they're they're concerned with the
resale value of the Academy players who
will never play for those teams but
getting the sort of stamp of approval to
go through the systems so this huge
amount of money so it's hard for them to
sort of hang on to these sort of guys
from a data science perspective one of
the ways they can do it though is by is
by operating a self-service data science
paradigm whereby we can operationalize
the value of our in a very simple way
another one I wanted to talk about here
is CERN so it's solar gray and you know
it's real big data it's a petabyte of
data a second you know it's real base
science so that they're all double PhDs
and that they're going to leave every
six months anyway but the cool thing
about this is that actually it's quite
Universal and one of the dangers of data
science becomes and this is my pet pet
peeve about it it's big is it can become
massively steric and exclusive for all
the kind of wrong reasons so guys like
me can stand here and talk about
Jupiter's on so our building pandas on
Jupiter's and binarized and it's like it
and some people going is great and
ninety percent bedroom we go what on
earth are you talking about
and that's traditionally maybe a reason
why because of the advent of it's all
open source startup cultures that didn't
have tools that's why it was the case
what's really encouraging about these is
just some of the most brightest in it
most innovative data scientist is
actually using GUI tools for certain
aspects of that process so for you know
the the basic export tree data analysis
on the hive tables now that I write
binarize from scratch they're going to
use a UI for that and then they're going
to use and then going to pass that data
to the machine learning stuff from the
ensemble Oh modeling from scikit-learn
or tensorflow or whatever else but it's
much more pragmatic so you know the
danger is that actually people just
focus on writing code as being you know
a realm of data the realities absolutely
not there is no reason why that should
still be the case the other really
interesting thing about data science is
that people sort to work what a data
scientist and and there's a great
company in the UK who are who are ran in
in in mass way before data scientists so
you know I used I was originally a
mathematician before I grew a beard and
hope for a pay-rise budget coming a day
a scientist and still has them and these
guys come from that background and they
do a thing about you know what kind of
data scientist to you and you go into
their website they're called manga by
the way of looking up that they're great
and you enter a bunch of questions and
what it comes out this is one of those
spider webs of attributes and
interestingly enough there are five or
six different attributed to what dead of
science is sure there's the model of
stuff which is the stat C thing that
everybody thinks it is but it's also
about data wrangling it's almost like
coding it's also a visual izing it's
also about communicating and the reality
is that you know there the unicorn is
the guy that can do them all nobody can
do all that sort of stuff so really in
terms of the entrance point to data
science you know people that people come
from coding for visualization is much
more about the sort of spirit and if
that's not too too woolly the the
passion about data the passionate about
getting involved so we're looking to try
and make it much more inclusive and I
think they're a great poster child for
that and finally one before I get into
the demonstrations again I mean home
base here right guys I don't know much
about wargaming but this is really
encouraging because actually a lot of
the sort of use cases and this comes
down to the original change that we
talked about in terms why people are on
certain cases switching off data science
and big data and the problem is that as
it moves from innovation to mainstream
you're going to start to get the man
looking for
value how many know it's great messing
around with this kind of stuff but
where's the value in this okay where's
they with the value and as it moves into
mainstream it will get those sort of
challenges and so what would so it's
great to mess around this stuff but it's
also sort of good to kind of find a
metric around you know how much is this
worth to the organization in any ways so
this is really good because it's
actually quite traditional CRM it's just
a vast volumes of data so these guys
walk wargaming 300 billion events per
day so it's a pleasant ization kind of
problem you know but the bottom line is
that it's delivering a tailored
messaging and experience and an
interaction that's much more one-on-one
and there's a metric around this you
know one of the big areas what my first
project is anybody feeling from Amazon
ticket
so my first project when I join Oracle
remember a big friend of ours at the
time so their big reference for
warehousing back around the tender
century and you know there was first
petabyte they've introduced RAC the into
partitioning so so great in there and so
whenever a new bit of technology so you
would say who else is using ago Amazon
so it was really good then we introduced
them to our first generation of
personalization engine so this is when
we started to move beyond collaborative
filtering into the realms of statistical
functions like transaction naivebayes
and predictive association so it started
getting a little bit more stat scene
involved and yeah it went a little bit
sour but nevertheless the technology in
this whole space was really really
promising for personalization we could
actually start to build better models
because we could get more statistical
umph behind it and that was 15 years ago
and then we went into like the Discovery
Channel and we start to profile those
guys in terms of so people who watch the
Discovery Channel so they were kind of
interesting the idea that people were
interested in things other than serial
killers and sharks who watched the
surrogate who watched who was discovered
they found that they weren't actually it
was a kind of an awkward conversation
but nevertheless you know that whole
experience was was really interesting
and my personal bug bearing this whole
thing is actually I think
personalization hasn't really moved on
very much in the past 15 years and this
is the big sweet spot you know the idea
that we want content that is really down
to us and really sort of relates to us
as individuals so digital marketing age
you talked a lot about gray sheep if I'm
going to consider the idea of grain
crate is your taste a - outlier for our
algorithms our Cox proportional hazards
are machine learning albums to identify
which because reality is everybody's a
gray sheep right it's just that the
challenges the data is so vast the
albums are quite CPU intensive it's
pretty much most lots of rehearsal table
scans so effectually that's why we're
left with this you know equivalent of a
bandsaw which is saying the most idiotic
thing really really quickly that forms
the basis of most social social
interactions and personalization so
nevertheless as but the point here in
the Salvation here hopefully or your the
the guidance here is that as processing
becomes more powerful as the albums get
more sophisticated the premise is that
we can start to use more sort of beefy
techniques to find better inference and
relationships in that data and we're
just starting to explore the likes of
machine learning and deep learning
algorithms in this sort of space so
we're looking at people's preferences
for music in terms of in terms of
emotional response to music and and and
profiling that and seeing out what that
impact has in terms of content
personalization so the idea is that the
more we use data that you know we can
get we can get better at serving those
things and this is a great example
because it's a big metric behind it so
just some examples that we're sort of
playing in this sort of space but all
all organizations that have gone through
the curve of trying to build it
themselves trying to stand all the stuff
up themselves and and and struggling and
we think there's value in having an
end-to-end platform where we take away
that pain that leaves you with the
opportunity to really innovate and find
cool stuff in your data that's what we
care about okay any questions at this
later good right I'm going to sort of
shift gear at this point I'm just going
to take you to a demonstration here in
terms of in terms of sort of some things
that we're seeing really resonating at
the moment and some of the common areas
that we that we see a lot of interest in
and we have a what's called a series of
analytical design patterns so we create
these for four different methodologies
and four different work streams and so
this is one that
Universal at the moment so it's an
internet of things prediction problems
so the idea here is that we have
real-time Internet data it's based on
the financial services application we're
going to play a machine learning
algorithm to this in real time and we're
going to put a player into a stream okay
so we'll take you through that process
as a data science methodology and we'll
we'll compare the different ways to do
this
okay so uh yeah I should say so
typically my entrance point to the big
data would be in Hadoop environment
which which we would use and typically I
might start off with something alike
like like hi for example so hive is a
I'm sure many of you know it it's a way
of querying it against it's a way of
sort of construct metadata over and
above Hadoop Hadoop data sources so we
can query against this but it's not
necessarily the easiest way for a
non-specialists to work on and it's not
necessarily the easiest way to share
data so going back to the perform group
one of the lines of business that they
work on is providing wrote the effect
where they effectively build themselves
for gaming companies as the Netflix for
sport so they provide real-time
streaming to the William Hill to the lab
books he said to etc and they had paid a
lot of money for this kind of thing but
when Ladbrokes would come along to them
and say how many people subscribe to
this service it would take them 15 days
because there's one guy writing pig
latin scripts to do this whole thing
which is really kind of scary because if
that guy disappears they're running
their business on it so we kind of
searched again one of the sort of
constraints we sort of thing is try and
try and move this into much more of a
mainstream way of doing this so actually
rather than starting from pig latin
script or a hive environment wouldn't it
be nice to use what people like CERN are
doing and create a GUI tool for for
querying that data so
my starting point in this particular
case is something called big data
discovery it's a visualization tool to
go directly against my underlying oops
and lying dataset
I might go in a different way
let's try it here
so I bet my guys I'm going to just this
interfacing a different way
tunneling huh okay so okay so Euler this
technology's design and Sonja's an awful
lot of this is that it is designed to
write a very shareable environment for
our data scientists starts with query
data so rather than stab you code
everything from scratch they can come in
here they can explore there that they're
there five tables I can pull it into a
into a spark environment and query that
so it's a different project chef's
financial crime and yet we've got a
Formula one demo that does a similar
sort of thing so here for example yeah
I've got a fairly simple view of my data
I've got some sort of recency frequency
data I've got some latency so I've got
some stuff that's pulled from a
relational source some stuff that's
pulled from my sensors and I've got the
thing that I want to to model here which
is a sort of type of financial crime and
we're categorizing in a very very simple
ways it's a machine learning problem so
it's high risk low risk medium risk so
it's a very example in terms of how this
works but the idea is that from a data
science point of view rather than to
writing code at this perspective I can
start to query this I can drill into the
enemy's components I can look at the
relationships in my data so I can look
at the distributions the whisker plots
the box plots all the sort of things
that as a data scientist would be really
useful from an exploiting point of view
to understand what's going on my data
what the distributions look like how I
can query it and so on and so forth so I
can come in here and I can look at the
credit rate and I can slice and dice on
that so prior to but prior to these kind
of technologies existing it's very very
hard for us to sort of share and
collaborate on this kind of kind of work
in a data science arena and going back
to the point of view we said earlier on
about there being no single sort of
person that can do all the value of data
science very often sort of stayed in the
heads of individuals and didn't really
get to the mainstream so we're sort of
talking about a much more collaborative
way of doing this we can also apply a
whole slew of transformations on this
data we could publish as groovy scripts
so again it's much more about sort of
self-service it's much more about
making that whole process much easier
for a data scientist to use now
scrolling a bit this is all running on a
a VM we have so it's available for you
to play with but the point behind it is
that it's got an entire cloud air
distribution with all the sort of bits
configured for you and ready to go so
it's pretty self-contained
so in this case rather than having to
write code to restructure my data
it recognizes the sort of types of
fields I've got so if I've got a string
variable in here then it allowed me to
restructure my data right we took create
groovy scripts and so on and so forth in
a very sort of gooey way and we can
visualize stuff in here too so in this
case I might want to look at a
correlation chart so it's a bit of
exploring data analysis in here and I
might look at the correlations between
the different attributes I've got my
dataset and the thing I'm looking to
model which is a financial crime risk on
this particular transaction so we can
come in we can visualize those to just
cure a person okay I'll come back to
that in a second so we'd work through
this process now here's that here's the
nub here's the nub as we go through the
methodology I'm kind of little reaching
the end of what I can do with GUI tools
so sooner or later I'm going to have to
get my hands dirty
in terms of writing some code now the
coding world's changed massively ml
world's changed massively in the past
few years so five 10 years ago just as
in a relational data and Oracle and the
big vendors was the only way you could
manage data and we all know that's not
longer the case similarly there were
very few ways for us to do our analytics
so I didn't really exist Python disease
response
Scala certainly didn't exist so the
choice was to go to sort of the big kind
of network computing vendors and and and
that's hugely changed in the past of
five years and that's in massively
liberating four five ten years ago you'd
have an algorithm argument
so if you were talking to a media guy
and he was doing market attribution and
he and you getting into conversation
about an architecture and making things
fast and he goes what have you
 proportional hazard and you may or
may not if you didn't have it you're in
a not a very comfortable place now the
advent of open source you've got five or
six different flavors of that
proportional hazard you've got seven
different ways of doing it
so it's becomes much more about the
architecture and more importantly the
kind of people you've got in the
organization and how to make that
operational so we use so in this case we
use um we I use a Jupiter an awful lot
so Jupiter is an open open source Apache
notebook it comes from academia it's a
really good way for us to sort of
configure and share our analytics so let
me just
so this is Jupiter how many of you come
across Jupiter see you okay very very
good so yep so there's a couple of
flavors here so Jupiter Zeppelin is
another one but they're really cool and
it's just a really nice elegant way of
creating and running code so these comes
from these are really came from the
pipes and notebooks the AI notebooks and
the ideas that you have these context
cells you can run it remove your code
you can have markdown in there you can
have you know droughts to sort of
different sessions accept etc and
different contents of people creating
whole presentations with this framework
Jupiter's kind of similar to that except
it's been extended to cover code from
Jupiter j/s pison and an hour hence
hence Jupiter you'll see it in the name
but this is really quite as I use this
so in this particular case I did some
machine learning I've got some
predictive algorithms using a library
called scikit-learn which is open source
library love it it's really cool so if
any of you views come across things like
an epic price and it uses in Shambala
models which is like a superset of
classifying things and they've won it
for the past few years and it's always
in sorts of people contribute to their
as a here for example I'm using panda
packages and I'm doing some sort of
summary statistics so I can come in here
I can run correlation coefficients so
for example I'm looking so one of the
things I'd be interested in at this
particular point is what's called a
whole bunch of yeah exploratory data in
our analytical functions so for example
I'm using a pandas correlation method
here a Pearson method what this is
looking to do is identify correlations
between variable so that's really
important when I build my models later
on um and so I may know that my series
is monotonic so therefore I'm going to
use a Pearson but actually I may look at
my distribution then think it's linear
so I might want to go to a you know the
default Spearman's in there so I can
come in I can rewrite this that I can
simply recode that so I can take the
method back to default and automatically
all the code will change behind this so
it's a really cool way for us to be able
to sort of share the logic behind what
we're doing great but it's all coding
now but one of the coolest things that
we've managed to do in this whole
framework is also incredibly boring but
it's very very cool to the people that
care and this is called a bdd binding so
pison binding to this so this little
line here the Python execution shell
pretty sexy for data scientists how we
get pretty hot under God but that's
hugely useful because what that means is
that we can take all the output from our
BDD environment all the restructuring
all the groovy scripts we haven't had to
bother with binarize errs and writing
graphs to do this and we can apply them
as collection keys into this framework
here that's really really cool because
previously it's really hard to raise no
single tool for everything right we know
that but it's really really hard to move
things from one environment to another
so it's really kind of hard to move it
from a from a data prep and I'm an ETL
environment into your coding environment
into something else that's really hard
and so often people have op ended up
with doing things sub-optimally in one
tool
because it's just too bleedin hard to do
it all across the - so we've really been
focused in working with organizations to
try and make this this part of the
process and accelerate that whole
learning thing so use gooeys for the
good stuff at the front end and then
pass it into here so I can let then
build my machine learning models in this
context and you know this case you could
use Oracle's technologies I've used a
bunch of stuff in here so I've used them
you see I won't bore you with the
details of this so I use some in sambala
models with gradient boosting in there I
think and there's a whole bunch of
statistical fits on that and then I came
out and I tested those models and I've
got predictions so I run my sort of
standard code but the cool thing about
this is that because it's running on a
fairly beefy node so we tend to have big
beefy nodes on our Cloud area
distribution it means that we can sort
of trying to get around some of the
issues of scaling of multi-threading and
paralyzing Python from first first
principles so for example if you guys
you're looking at tends to flow great
and it's a tense flow this is the google
deepmind project for deep learning I'll
talk a little bit about that later on
but if you're using those webs great
fantastic just plug it in here and it's
just going to run quicker so
whole kind of propriety algorithm stuff
is kind of fading away we have a bunch
of them but we don't really care if you
use it on it's much more about plumbing
into this environment so it's a huge
sort of change and making that something
much more practical might do some
visualization at this particular point
et cetera et cetera and we deploy that
into the process so imagine a scenario
I've gone through a very very quick view
I've explored my data I've done some
summary data analysis I've done the
clever modeling to make predictions on
whether these transactions are are
potentially fraudulent or not that I'd
then like to sink it into a stream so
I'd learn like to sort of have that as
an operational data stream and IB
business outstand so I'm not going to
show it as a live demo today but the
final part of that process is that here
we can pull that real-time learning into
our into a streams analytics technology
so so that this is based on complex
event processing and here we've got
these what's called zones of interest or
polygons of interest in certain
locations and against this we can feed
in our real-time learning so he's got
all the sort of information around that
but a credit card number the things are
happening real-time and we've got a
real-time score based on the machine
learning now here's the point behind
this what we've really done is is taken
an ingress of streams data we've
augmented with some relational data we
put it into a hub we've made a real-time
REST API call to a machine learning
algorithm that makes a prediction and
then we've sunk it into a streams
analytics hub now here's the thing we
have choices and on Hage's by the way
you could do this both in our
infrastructure this is absolutely not
about saying who are you're going to do
this way because this is what critical
do but you have choices so what you
could do is you could use something like
a you could use something storm for
managing the hub you can have an
ingesting semantic language from lots of
Casca you can then take something else
from zookeeper to manage the resilience
you can then have something else manage
the whole process you could then maybe
use the machine learning a separate up
data this becomes really really hard
this becomes really hard to sort of
manage or in this case I've just used
sort of a couple of web services now
this is actually not to say that every
single thing can be done in this way but
it does illustrate one of the problems
is that is trying to get away from
making all this sort of stuff working in
the first place to getting the value is
a one of the sort of key message that
we're seeing is resonating in this or
space or you know a couple of I've just
used a couple of web services like cloud
services and a desktop tool for
visualizing that so what we try and tie
into is the idea of design patterns and
so these are pretty much the same as we
see across any sort of processes from a
data ingestion point you know there are
challenges around each of these things
it's kind of hard to get a lot of these
things to join together Pacific
particularly as we move into the
polyglot world and we're consuming new
sources of data and relational and
streams data becomes quite difficult we
have this process this methodology of
wrangling our data figure out what's
going on building the models iterating
through that and then we've got the
process of actually operationalizing the
value into the front-end so for example
I don't know if anybody who's tried to
talk to to deploy sort of scarlet
packages or piping packages or SPARC
packages and managing that whole kind of
kubernetes sort of dockwise frameworks
in the past that's really hard right
guys that's really tricky to weave
through we talked about 300 customers
last year 298 of them really saying this
is a big problem for us so there is help
on hand I mean I know that cloud era our
big data sites partner in the space have
have a solution coming up which is quite
exciting but 298 F 300 people said this
is a big problem this is a big challenge
for us moving forward I don't know about
the other two they're either geniuses or
they're mad though they were like but
nevertheless you know the point is
though that as we move from
experimentation which is really cool
really really exciting we're now
starting to get to the point of saying
people saying right come on guys let's
get serious about this where is the
value to this and looking at some of the
sort of more pragmatic things around
scalability around governance around
security around all those the boring
stuff actually does become really
important so I've got 10 minutes left I
just want to wrap up with a little bit
of a look we know we've explained where
we've come from in this analytic space
we will hopefully highlighted where
we're at what
current challenges I just want to talk a
little bit about the futures of this
it's kind of the present but it's also
the future really in terms of where
we're going with this and I am hugely
humbled to be Oracle's representative on
the data ethics committee for Oxford
University so this is a cross company
initiative so Microsoft is their Oracle
their Facebook Google IBM research et
cetera and front and center to every
single conversation for data ethics is
machine learning and cognitive services
okay so the whole advent of deep
learning and how that works I used to
work at Microsoft and my previous CEO
Satya Nadella talks about that you know
the democratization of AI and and the
advent of the the empathic robot and
certainly every single company is aware
of this so what I thought I'd do is just
sort of give an update in terms of sort
of the kind of thinking that's going on
there on in that space and also a
reality in terms of where we see the
technology at the moment because
although it's hugely exciting you know
some of the promise isn't quite there
yet we'll sort of highlight the good
stuff the real stuff and maybe the
sci-fi stuff because the reality is that
if we're looking at this as a journey
that's kind of neptune and a lot of us
are still trying to get our shed to get
to Mars maybe or the ends of garden so
it's important to know that but the
challenge with data science and all this
big data and it's a good and bad thing
right it's great that people talk about
this people who didn't you know Newsweek
and The Times didn't talk about big data
or data or didn't about bi or stats or
anything like that so this has grabbed
the attention of the press but with that
comes the challenge of actually managing
expectations so hopefully just a few
minutes in terms of sort of some of the
things that we're sort of seeing as
really really good use cases and and the
things that far brighter people than I
am a tox of you are saying about how we
sort of move forward with with the
ethics of these technologies okay I
always put this up like a couple of
things really um first thing to say is
that there's certain technology areas
that are really really good
so I think that video recognition is
some way behind and
facial-recognition is somewhere behind
sound recognition okay so that's the
first thing from some reason it's
certainly in the mainstream certainly in
the public area so the use of those
technologies kind of basic so one of the
things that but but but audioposting
seems to be much better so one of the
things that we're doing is we're working
with a big Formula One team and what
they're interested in doing is they have
open chat on the racist every so yakking
away and in their different languages
and what the different teams have is a
bunch of translators listing into these
open channels because they want to hear
what Ferrari are saying or Honda is
saying or Williams is saying because
they can use that for tactical advantage
and the process for that is really
manual they have guys sitting there
listening typing in transcribing it
through so wouldn't it be great if we
could identify the individual the person
who is actually speaking what they're
saying and feed that back in real time
as a stream there's quite a few
components to that the first one is
spectral frequency analysis so what we
do is we apply a spectral frequency
graph we strip the the voice from the
background noise we can do that this
there's really cool open source
technologies like the browser out there
that I play around with a lot it's a
really good for that we can then start
to identify the speaker so we a human
voice is as unique as a fingerprint if
you can get that back that recognition
so your thorax the the size of your echo
chamber your lips your etc etc combined
together can become as unique as your
your fingerprint or your heartbeat we
then combine that with actually what
that individual is saying and strip that
noise back and feed it back into real
time there's three or four different
components that's really quite difficult
to do especially as people sort of
talking at the same time so you've got
to be able to sort of strip out those
sort of frequency ranges but that's
pretty good and we're actually getting
there
to be fair to Microsoft as well at my
time one of Microsoft's applications is
drive through him in McDonald's and so
there's some kind it sounds like Detroit
and Saturday evenings really noisy I
listen to this wife I was like couldn't
barely hear what was going on never mind
what he was saying but what they do is
they take you know they take they
stripped voice in real time they feed it
into a human who's working on a tail and
he figured out what the order is and I
think that's the key in terms of the use
of these kind of technologies it's
things they're going to augment human
interactions so make human rashes better
rather than replace them they're kind of
ethical nobody's going to
sit there and say well I don't really
like that that's a bit creepy you know
the other side of the thing is is that
we see the rise of facial recognition in
an emotion recognition the problem with
a lot of that is it's really kind of you
know surveillance capitalism right so
you know you'll be walking into a shop
and whether you're frowning a banana
somebody will pop out from somebody and
say you don't like a banana people don't
really like that there's not really much
value to the individual because it's
about sort of the value corridor I feel
like I'm staying within the lines you
know not too much value to the customer
because otherwise we don't make any
money from it and not too much value to
the business because they're being
exploited so stay in that core is that
sort of cool thing in this type of all
sorts pace I'm there's a long way to go
with face recognition and emotion
recognition we've done some cool stuff
in China we're looking at smoke so we
thought if actually the way these things
work is that there's people talk about
deep learning you know that expression
people learning is really just a super
set of neural networks ok so they're
like clever multi-layer perceptrons
they're just working on different data
sets that's kind of really how they
operate so in this case arm you know
with face recognition and motion
recognition what it's going to do is
look at pixel densities around certain
areas in our face it's going to
recognize that a part of your your face
is your features it's then going to look
at distances between those using a
multi-layer perceptron and plot
similarities with other people so
Microsoft have this this application
called celeb or celebs like me calm in
fact you feed in your photograph I'm the
old guy on that on that on the right
there and it profiles you with the
celebrity that you most look like we're
doing a corn in China actually which is
looking at smoke so we pixel so we can
identify the RPG values of smoke and
then look for smoke in in public places
there's quite cool this is the person I
most look like according to Microsoft
from his albums I couldn't help but
feeling a little wounded by this but but
you can see how maybe how it works and
but you can also see that actually
what's really missing is context a lot
of this sort of stuff you know the
pepper project they talk about the idea
that there are I don't know in about ten
years time machines will probably be
able to make a billion neural
connections
you know and sciences the human brain
has a hundred thousand billion so
there's some way to go before these sort
of computers will take over the world I
had a I had a conversation with my
friend over to me go about station
machines and thinking robots and there's
obviously you know a lots of literature
around this and we started up going
isn't that terrible and then you know
three business at night we thought it
would be cool to be subjugated by robots
at the end of the day but nevertheless
there's some way to go yet guys but I
think the serious point in this whole
thing is there are some super really
interesting applications out there I
think I think voice is ahead of speech
at the moment it's coming we need to
provision for these things but
reasonable you know find the good cases
in the article cases printing out there
okay
oh yeah and then in interests I provide
a genuinely handsome movie star here and
this is who Leonardo DiCaprio looks like
I think it's sort of like you know
saying if you've gone on a mess binge
for the next five years that's what is a
warning from history anyway guys
hopefully that's useful apologies it's a
fairly quick quick run-through running
out just to sort of the real take away
if you're finding it hard don't worry
everybody else is but there is help on
hand there is help on and there are ways
forward with this and and you know what
it's a super exciting time right because
this is front and center and what else
are we going to be doing it's amazing
alright thanks guys
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>