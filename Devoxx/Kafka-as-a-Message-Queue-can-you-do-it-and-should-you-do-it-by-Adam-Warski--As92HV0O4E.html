<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kafka as a Message Queue: can you do it, and should you do it? by Adam Warski | Coder Coacher - Coaching Coders</title><meta content="Kafka as a Message Queue: can you do it, and should you do it? by Adam Warski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kafka as a Message Queue: can you do it, and should you do it? by Adam Warski</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-As92HV0O4E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I guess we can start hello and my
name is Adam Barsky thank you all for
coming
it's great to be here back at devoxx
actually one of my first conference
talks what on the box as well but that
was nine years ago so quite a lot of
time and so yeah so today I'd like to
talk about how a Kenyan youth SCAF
crisis is a message queue but before the
how I would also like to discuss should
you do it at all right maybe it's not
such a good idea or maybe it is um so
the rough plan for the next 50 minutes
or so is first we will make an overview
of how our acknowledgments work in plane
Kafka so what you get out of the box and
then we will a wonderful bit why would
you want a different model for of
acknowledgments that is selected
acknowledgments and we'll look at some
alternatives Kafka is certainly not
alone here there are there are other
message implementations which you might
also want to use finally where you will
look at the implementation of the
message count up of calf:cow just called
kmq and there will be a short demo so
hopefully that will work and I will also
talk about performance which is a quite
important in systems in big data systems
like like Africa and so if you would
have any questions if I would like if I
would skip over some important Kafka
detail that you don't understand please
ask a question I try to answer and okay
and and yeah and before we start also to
give you some context on who's actually
talking to you so I'm a software
engineer and co-founder of software amo
and it's a software consultancy and a
custom sort of custom software
development house in Poland and so we do
quite a lot of software main using Scala
and Java also quite often using Kafka
and that's why I'm talking about one of
the reasons I'm also doing some open
source so I once did a lot of Java so if
you are using hibernate maybe you've
seen hybrid
of us so I've been involved in that now
I'm doing more scholar things like SCTP
quick cleanse my coin and so on I have a
blog and Twitter so that's nothing very
original and okay so let's let's take a
look at how acknowledgments work and
plane Kafka okay so the central topic in
Kafka is a topic that's where you put
your messages your data right that's
like a logical unit where your data is
written to okay so when you create a
topic you specify how many partitions
through the topic have okay these
partitions can be placed on different
servers and it's up to Kafka to balance
which partitions go to which servers for
rep replication fault tolerance and so
on so you create the topic with a number
of partitions now when you write data to
a topic each message that you write to a
topic goes to a single partition okay
which partition that is is decided by
one of many algorithms so it can be
random it can be based on some property
and so on so that's one so when you
write a message the top it it goes to a
single partition now at some point we or
usual also wants to read data from a
topic right so then we create a consumer
or usually create a number of consumers
so these consumers form a consumer group
and we create a number of consumers for
two reasons one is performance so we
want to process messages in parallel and
the other is fault tolerance so if a
consumer dies another one takes over so
in a single consumer group many
consumers Matt Kafka assigns to each
consumer a set of partitions which will
be handled by that consumer right so
each each consumer gets a set of
partitions and all consumers in the
partition in the consumer group get all
the partitions like in total so the
partitions are divided among the in the
consumers so now let's say we have a
topic with three partitions and we have
a consumer this this consumer got
assigned a partition number two that
might be more but let's say it only got
assigned one and it's reading data from
the from the topic right so reading some
data processing so we are executing
business logic and to handle each of
this of this measure of these messages
and now we usually would like to have
some mechanism to save our progress
right when the consumer restarts or when
it dies and is taken over by some other
node or when the clusters rebalanced we
usually don't want to reprocess
everything from the beginning
right we want to actually start
processing where we lost a stopped right
so we need a way to save our progress so
Kafka offers such a mechanism it's
called the offset storage and so we have
the possibility to write it took half
car the offset so like the position in
the in the calf car in the car car topic
we can write the offset upto which we
have processed all messages okay so
using this we can implement at least
once or at most ones processing so if we
read the message save the offset and
then execute the business logic that
gives us at most once processing right
because the offset is always written
before processing the message so even if
something happens now if then if the
consumer is restarted it will start
reading after this saved of this set
offset if he wants to have at at least
one's processing and then we have to
commit the offset after the message is
processed right so it processes the
message once we know that the message is
successfully processed we store the
offset okay the important thing here is
that storing an offset acknowledges all
the messages up to that offset yes do
you have a question or do just waving it
to a friend okay that's fine but if you
would have a question I'm you know I'm
here and so yeah so the important thing
here is that we acknowledge the
processing of all messages up to the
given offset okay
we can't say that we want to only
acknowledge a given batch of messages
these are all messages up to that offset
so that's that's what you get out of the
box
when using when using Kafka okay so why
would we want a different model of
acknowledgments selective
acknowledgments so let's say that as
part of executing the business logic of
hey when you handle the message you want
for example to call an HTTP endpoint
right so for each message this message
translates somehow to an HTTP call right
or maybe wants to send an email or maybe
you want like in general integrate with
another external system right so I guess
that's quite a common scenario so here
they up to up to an offset and commit
mechanism isn't that useful because all
of these calls may fail individually
right it's quite common than an HTTP
call fails either because of client
error or server error and maybe some
business logic error all right so these
things happen but we don't want an
individual error in handling a single
message to stop the whole system right
maybe you want that single call to be
retried so maybe we have called an HTTP
endpoint and it was unavailable at that
time so we wanted to retry it in like
say 10 minutes or something like that
right and but that shouldn't stop us
from from from from processing the other
messages that are further down in the
topic okay so here they committing or
offsets up to up to a given point isn't
really that useful because well we if a
single message phase right we don't want
to retry a after a period of time we
don't want to retry all messages that
have been in that batch right because
that would lead to a lot of duplicate
messages being processed and we also
don't want to stop processing with the
cost that would delay processing of
subsequent messages so that's why we
would actually want to have a way of
acknowledging on a single message or a
batch of messages but not all messages
up to a given offset so what I've
described here is in fact the users
you use it scenario for a message queue
right and there's quite a lot of message
queue implementations out there there's
rabbitmq ActiveMQ Artemis which is like
in your ActiveMQ sqs and so on and so
there's nothing new and if you have that
available in your system you can simply
solve the problem that I've been
describing your thing is this these
systems so why would we even consider
using Kafka instead of a message queue
right well I think there are three main
reasons first of all a quite often
nowadays when we have a system and we
accept some data into the system we want
to be sure that once we say whoever
wrote that data to our system we say
that the data is written that data is
actually replicated across a cluster and
safely stored so Kafka offers us a
proven reliable clustering and
replication mechanisms which have been
tried in a lot of deployments and I
think it's something you can trust right
you can quite with a quite high degree
of confidence trust that Kafka won't
lose your data and secondly if Kafka's
were known for its performance and so if
we could well the system and message
queuing system on top of Kafka which
could at least partially have the
performance of the plain Kafka that
would be quite quite nice finally
there's the convenience of operational
complex right if we already have Kafka
in our system then adding for example a
RabbitMQ class or an active MQ cluster
that significantly increases the
operational complexity of your whole
system right so maybe if we can actually
implement the functionality that we need
without sacrificing of course
functionality then this did was that
would that would actually make our whole
system simpler right so if you'll
already have Kafka maybe you can use it
for that functionality as well and so I
would like to explore Amazon sqs in a
bit more detail because we will
to build something similar on top of
Kafka so Amazon exquis is a message
queueing as a service I suppose many of
you have come across it and it has a
very simple API right it works in the
Amazon Cloud so you just use it you
don't have to install anything and there
are four basic comments so there's a
create queue which not very surprisingly
creates a queue send message receive
message and delete message right send
message again is probably quite
self-explanatory so now when we receive
a message from sqs what happens is the
message is not deleted but it's put
aside for a given period of time that
period of time is called the visibility
timeout okay so now if we don't delete
the message within the visibility
timeout it will be put back into the
queue right
if when the message is put back into the
queue subsequent receive calls will
actually see the message and it will be
received again okay and using this
mechanism we can implement again at
least once delivery of messages so so
when we receive a message right we
execute some business logic for it once
the logic is done we delete the message
and everything is fine if the business
logic falls over for some reason I know
the the note dies or whatever and then
after the the visibility timeout passes
and the message will be again available
for receiving by a receive message call
okay so that's Amazon ex-us and this
quite often works great how are this of
course a couple of problems with it so
if you really have a lot of messages to
process it can become quite expensive at
at a large scale and second is also it
it has good performance
and but the latency is not it's not
always that great
well latency in terms of the time
between sending and receiving a message
okay so how how could we implement a
message queue on top of Kafka okay so we
will use two topics to two Kafka topics
and we'll have the queue topic which
will contain most of our data so then
that's where we actually
right the messages that's that's where
we we store the data to process and we
will also have a markers topic and where
the markers topic will start for each
message I start and marker how this
works I will explain in a second so now
it's important that both of these topics
have the same number of partitions ok so
each partition in the queue topic will
have a corresponding partition in the
markers topic kind as a one-to-one
correspondence so that's the setup on
the Kafka side and now we also have a
number of cue clients so the cue clients
is where the data is actually processed
ok so there would be typically a lot of
them maybe not a lot but a couple of
them again for performance and for
failover so that's where the business
logic is executed but because he wants
to have these selective acknowledgments
it will be a bit more complicated than
the regular Kafka consumer and finally
we have a number of redelivery tracker
components which will actually handle
which will monitor if a message to be
delivered or not ok so how the how does
the cue client work and so the first
thing it does it reads a message from
from the cue topic once we have read a
message we write a start marker into the
markers topic and the start marker
contains the offset of the read message
so the position of the message in the in
the in the queue topic ok and now what's
important is that we wait for that send
call to complete so now here I'm an
important notice that he I'm describing
the flow for a single message but we can
actually do it in batches to increase
performance so if we have waited for the
center to compete for each individual
message that would actually be quite
slow but if we bad things together it's
it's much better ok so we read the
message and we write the start marker
and then we commit the offset of so now
now we use the Kafka a mechanism of the
offset committing so we commit the the
offsets of
to cure so that when our consumer
restarts it will actually start reading
from the right place and not reprocess
too many messages we can do that because
the start marker has has already been
written so it is like in the game queue
system so then we can finally in step
four process the message so we execute
the business logic associated with the
message and when that's successfully
done we write the end marker to the
markers topic again
so then marker again contains the
positions of the the message offset so
the position of the message in the in
the queue topic okay so and showing this
a bit more graphically so let's say we
have our topic with a three partitions
right the queue topic and on the right
and we have the markers topic also with
three partitions so each partition in
the queue topic has a corresponding
partition in the markers topic okay so
what we do here is first we read and
read some messages from a topic so let's
say we have a client a consumer which
again reads messages from the second
partition and so we read some messages
from from from the topic and the first
thing we do is for each read message we
write a start marker right now once we
are certain that the start mark has been
written we can actually commit commit
the offset the read offsets back to
Kafka and now once the marker has been
written and the offsets committed we can
process the message so processing the
message can end in two ways so it can
either fail for whatever reason and if
it fails well we can't really do
anything right because the note might be
not non-existent anymore so we don't do
anything right or it may succeed if the
business logic succeeds then we write
the end marker for that for that message
into the markers topic okay so now
that's these five steps I was
implemented in the cue client component
now I mentioned this already
there's also another
and called the red delivery tracker now
the red delivery tracker streams all
those markers and it keeps an in-memory
priority queue of all the messages for
which there has been a start marker but
hasn't been an N marker okay so now
additionally there's a trigger which
fires every second or so and it checks
if there are any messages for which
there has been a start marker and there
hasn't been an end marker for echo for
the visibility timeout period so let's
say maybe in our case it can be 30
seconds right so if there hasn't been an
animal cure for 30 seconds then we
should probably redeliver the message so
to redeliver the message what the
redelivery tracker has to do is it looks
for that particular offset in the queue
topic and reach the message and writes
it back again to the topic okay so in
fact the message isn't we delivered per
se but a copy of it is being written a
dose up into the topic so it will be
reprocessed okay so as I said the reader
river tracker
it's a Kafka application and meaning
that it uses standard Kafka consumer
producer API so that's not really a lot
of magic there it's a consumer of the
markers topic you know there should be
multiple instances of the reader every
tracker for failover if one of them dies
the other that they will take or take
over and if you have multiple instances
and you have a so they all the Red River
trackers and form a consumer group and
so if you attach a if you attach the Red
River tracker to the markers topic they
will actually divide the partitions to
consume between themselves using Kafka's
Auto partition assignment and as I said
the delivery tracker holds an in-memory
priority queue now this may sound
dangerous if there's a lot of messages
but if you think about it like the most
messages that will ever be in the
in the priority queue is as much as as
the visibility timeouts right so it is
the visibility Tomatis 30 seconds right
we won't hold more than 30 seconds of
message offset sorry not not not
messages but only offsets right and so
even if you have a lot of messages 30
seconds of messages probably will fit in
your memory without any problems another
question about the red alert tracker is
isn't the Red River actually very slow
because we have when we see that a
message should be read Oliver's right we
have to do a seek in the queue topic and
then read that message and write it back
again ok so it sounds bad but it's
actually not that bad and the good thing
here is that we only do the seeks in the
redelivery tracker going forward in the
in the queue topic right so if the
message is unprocessed and if there are
no end markers for the messages name
then they will always be timing out in
order of their offsets right so so like
the consumer the we deliver consumer in
the tracker of the queue topic always
goes forward so it's like delayed by the
visibility by the visibility type a
timeout so it's delayed by 30 seconds or
so but it will always go forward
well the flopster is actually not not
that bad there won't be that much actual
6 so so so the disc should should be
fine okay so now let's see and how this
works in practice and if this works in
practice
yeah okay how do I guarantee that the
Red River tracker doesn't read deliver
the same message more than once okay so
if it crashes then I don't so if if
everything works fine then once the
once the redelivery happens and n marker
is written for this old message so the
reader will track our even upon restart
once try to read over again but if it
crashes in the middle of V delivery then
the message will be read their words
twice but so yeah so all that I'm
describing here is actually at least
once deliver anyway right because even
like if the message if the message is
processed successfully from the queue
right the node may crash just before
right right writing the end marker for
example so then you get deprecated in
the river anyway so you have to be
prepared for that so our goal is to
minimize of course the the duplicates
but we can't prevent them entirely okay
so let me first start a Kafka cluster so
it's Kafka 1.0 so first what we have to
do is we have to start zookeeper once
the zookeeper is up we will start Kafka
itself and we will have to create two
topics okay starting server and yeah
okay I guess binding to port is as good
as it gets and so now we start the the
Kafka server so that's like your typical
and Kafka startup okay zookeeper here is
fine
Kafka I guess as well okay oh yeah
no no it's now it started let's start up
complete okay that's good
so now we create a the topics so we have
to create two topics right so first of
all we'll create the to the queue topic
so here it is it's very unimaginative
Lee called queue but it can be any name
of course and in our example maybe I
will make this bigger sorry
still quite small yeah maybe it's better
now okay so so yeah so we create the
queue topic it will have five partitions
and a replication factor of one because
I have only one laptop here so yeah
let's create the queue topic and then we
will have to create the second topic
holding the markers and this will again
be called quite unimaginative Lea the
markers topic why is it taking so long
okay created topic queue now we create
the topic markers mm-hmm
I guess my computer's telling me it's
time for an upgrade okay and then when
we list the topics we actually should
see that we have the two topics created
okay so and that's that's the setup on
the Kafka side we have we have our two
two topics created so yeah why it will
finish eventually and okay so now let's
let's see the code for the actual a came
queue usage okay I guess that's big
enough and so first of all we have the
configuration and so it's an open source
project is you can take a look at it if
you if you'd like to it's all relatively
simple code yeah so to actually make it
work it requires two configuration
classes one is they kmq config so they
came to config class holds the name of
the topic that we are going to use for
the queue the name of the topic that we
are going to use for the markers data
right so these are the two names the
they need to correspond to whatever we
have just created in Africa and also
here we have the duration of the
visibility timeout so after how long
will the system try to redeliver
messages so 90 seconds probably too much
so let's make it or maybe even 10
seconds so that we don't have to wait
too much
actually 10 will be too fast let's make
it 15 okay and another one is Kafka
clients let's just a helper class to
create Kafka consumers and producers
using the given Kafka host but the
important thing here is that we have the
name of the queue topic the name of the
markers topic and the visibility timeout
okay so now let's look at the client
standalone processor ok so here we are
so here we are going to use the Gamecube
client class so that's that's the
important part the Gamecube client class
we need to passes the the configuration
and if you go and take a look at the
implementation over here you can see
that the numbers here a correspond to
the steps that I have described in the
diagram earlier right so what the acute
plant does it has a method called next
batch
so this reads the next batch of messages
from Kafka so what it does is first we
get messages from our queue topic right
so here we are using I'm not sure how
familiar you are with Kafka's API but
that's like regular Kafka consumer API
right that's what you get get out of the
box from Kafka so we get a batch of
messages then as I said for each message
that we have got that we got we write
the start marker so here again we are
using plain Kafka API to actually send
the start marker to the markers topic
right we do it in a batch and then after
all the markers for the batch are
written we actually wait for the sense
to complete right as I said we are doing
that in batches so that it's much faster
ok now that all the batches are actually
sent we commit the offsets and return
the records to the consumer ok so at
that point step number four would be
executing the business logic right so
now yeah we have
step number four somewhere on the client
side we'll see that in a second and when
the client is done processing a message
for each message the clients should call
the processed method so the process
method will actually write the end
marker and what is important here is
that the acknowledgment of each message
can be done out of order as
synchronously so in any thread so it
doesn't have to follow the the order in
which the messages have been returned to
the client right so it's totally it's
totally up to you in what order and when
do we actually acknowledge the
processing of a message all right so
they came to client implements these
four steps that I have mentioned again
here the fifth steps sending the end
marker it's using regular Kafka producer
api's and and then here's the usage of
the client so here's the client right so
what we do is we read the next batch of
messages from the client the next batch
will actually send the start markers
right and now we execute on our business
logic so here is just a demo business
logic and what it does is it uses an
executor to process each message in
parallel and so we have a process
message method here which will attract
them return true or false if it returns
true then the message is processed and
we actually call the fifth steps so
writing the end marker and if there is
and if the return value is false it will
drop the message so we simulate failure
right so we have some messages will be
dropped so if you look at the processed
method process message method you can
see that it roughly one in ten so ten
percent of messages should be dropped
right and the other there's some logging
just to see that we actually process the
messages and how many we have processed
so far so yeah so one in ten messages
two should be dropped and of course
after the fifteen seconds passes they
should be redeliver okay so that's
that's our client logic here and now we
also need two other things so
a way to actually send messages to the
topic so that's a very simple class it's
a bit it's a it's a bit of code but all
it does really is it creates a Kafka
producer over here so that's like a
completely ordinary Kafka producer of
messages where the key is a byte buffer
and the values by the buffer doesn't
matter it's you know just an example so
that's a Kafka producer and what we do
is we send a hundred of messages to a
topic and here this right and so it's
just sending messages so that we can see
that something is happening and this has
like no code coming from kmq it's just
pure pure pure Kafka for the demo so we
have we have a way to send message we
have a way to process messages but I
also also mentioned that we actually
need a way to track the reader liveries
okay and for that we have a third class
which is the redelivery tracker which
will actually consume the markers topic
and if a message is not acknowledged it
will redeliver it so here it's again
very simple that's coming from kmq
there's actually a delivery tracker
class which has a start method and all
we needs to do is call it passing in the
Kafka at the Kafka kind what the river
tracker does is as I said it maintains
the in memory cue okay so let's start
this so let's start the reader every
tracker first and yes so it starts up
when there are arrows
you do not available of course a number
of alive brokers is zero does not meet
the requires replication factor of one
why there are no alive brokers that
should not happen and there are some
curve super exceptions well mmm
the only reasonable thing that we can do
right now is restart everything
okay let's try running zookeeper again
I'm not sure why well it worked like an
an hour ago so and let me remove all
Kafka data okay so yeah so the zookeeper
works and Kafka doesn't want to shut
down even maybe I have too much maybe I
have something running in the background
okay well anyway I'll make it work just
after it will start working just after
this session completes so it's a pity
doesn't work though let me try cutting
all of it
why doesn't even want to close yeah
there's something hanging in the
background - knowing that does it yeah
okay so now let's try super again and so
yeah so what I wanted to do is I wanted
to start to redeliver trackers so they
would actually divide the partitions
which they consume between them
themselves and then we would see that
when we run the processor and when we so
we then can run the sender which
actually populates the topic with some
data and when we run the processor it
would consume the data quite fast
dropping about 10 percent of the
messages and then after 15 seconds the
remaining on average 10 messages would
be would be we delivered so again here
we will drop on average one message and
then this man one message would be read
over again okay so
let's try running Kafka again and but I
created the topics before that and it
worked so it must have stopped working
somewhere between creating the topics
and actually running the the code well
that's weird
startup complete okay so let's create
the queue topic sorry for that one thing
that is suspicious is that it takes a
long time
topic you already exist okay well I
won't be spending it much time actually
trying to make this work
but I guess you you you get the idea of
how this could work if it worked fine so
yeah so now about the performance of of
how this actually works and so we did
some tests on a three node tough cluster
and using reasonably large Amazon
servers so we use a service with m8 CPUs
and 32 gigabytes of RAM so nothing very
fancy but not not the smallest instance
as well and we are running the test in a
single availability zone using a hundred
byte messages they were sent in batches
of up to ten messages and we work we
used quite strict replication settings
so we wanted to be sure that once a send
message completes the the messages is
replicated so we use the replication
factor of three for the for the topic
and Kafka was configured with minimum in
sync replicas of two so if the majority
of nodes has to be up for the for
everything to work and when sending a
message we require an acknowledgment
from the majority of nodes
okay and yeah we were sending so there
was the MQ in the middle either plain
Kafka or play the Kafka ways with
acknowledgments
and we were and we had a number of
sender and receiver notes trying to to
send and receive messages and what we
found out is that in an online scenario
so when you are sending and receiving
messages at the same time and
performance is quite comparable so one
one thing to keep in mind when looking
at these graphs is that these are not
like numbers absolute numbers which you
can actually say that Kafka can process
up to 6 this is 6 TK messages per second
these are only that's only the
performance of Kafka in that specific
setup right it's not like high-end
service or anything like that and it's
also a very specific in terms of the
replication sending and so on so it's
only for comparison between plain Kafka
and Kafka with selective acknowledgments
okay so in plain Kafka week we got
16,000 messages per second at most in
kmq a bit less but nothing very
significant okay so why is that snake
came queue actually does quite a lot of
additional work right we send the marker
stop we send the start marker and then
we wait until it's being sent and then
we send the end marker and so on so I
think the explanation here is that the
operation that actually saturates the
cluster is sending a message using the
replication settings so it's actually
after when each message is sent we need
to wait for confirmation from the
majority of nodes and that's what
saturates the cluster not the process of
not so it's the sending side not the
receiving side which actually processes
the message with either batch or
selective acknowledgments and if you
actually run we also run a test where we
started a plain Kafka consumer on a
topic which contained messages so it's
not an online scenario but it's a
scenario where there's a given number of
messages waiting in the topic so when we
come when you when we run plain Kafka
the versus game queue chemica was
actually two times slower than playing
Kafka okay in this scenario where the
topic already contained messages and
that's what you would expect kind of
because chemic you kind of has to do
twice to work because of the markers
what's also important is the latency so
in our set up with plane Kafka we have
seen consistent latency is here's the
95th percentile of 50 milliseconds so
the latency between sending a message
and receiving a message so it came queue
it usually is if it usually was 15
minutes a millisecond as well however
when we got to the I think our highest
was using there's no nodes here I think
the highest was using 8 sender and the
receiver knows this goes up to 130
milliseconds again again that's
something you would expect because
chemic you has the Edit latency of
sending the start markers and markers
right for which so we don't actually
have to wait for the NMR curse for the
sense to complete we have to wait for
the start marker sense to complete right
so you kind of expect that the latency
would be 2 times as bad in the extreme
scenario so chem Q and summer it doesn't
really in fact throughput it impacts
latency another interesting test is what
if messages are dropped so here we have
again it's an online scenario test so we
have been dropping 50% of messages
consistently so in the beginning you can
see that it's a fairly stable throughput
rate right but because there's still
messages new messages being sent and
messages being produced received and so
on but when they send process completes
so when all the test messages are sent
and then you can see an exponential
declining curve which means because we
are dropping 50 percent of messages so
first 50% of messages have been dropped
right then the way they are retried so
on the next retry at 25% of these will
be dropped and so on and so on right
that's why you get the declining
exponential curve so that's like this
graph con confirms what you would expect
from this throughput graph when 50% of
the messages are actually dropped and
okay
as for the internals of how of how they
can queue is written and again as I said
it's quite simple code it's it's not a
lot of code I guess it's quite easy to
actually read it reasonably fast and
understand it it's implemented in Scala
however all the api's are using Java so
as I've shown you we have been using
plain Java it uses the reader every
tracker uses akka under the hood so
there's one actor but the markers topic
partition and one actor / qÃ¸ topic
partition so if the way if the reader of
a tracker is assigned multiple a marker
partitions they all handled concurrently
the redeliver is as well and as I said
it uses the Kafka out of balancing
mechanism they came cue client which I
which we have which we went through is a
single a Java class implementing these
five steps if you remember and plus
there's the value classes for the
markers and that's a and that's in
essence everything in income queue so
again it's quite a simple a code wise
it's quite a simple project so to
actually implement the individual
acknowledgments and Kafka all you need
is one calf complication to handle the
reader liveries plus a bit smarter
client to actually read messages from
the Kafka cluster okay so to summarize
we have implemented individual selective
message acknowledgments which are quite
similar in how they work to sqs with the
visibility timeout let's I think it's a
variable in alternative to
the upturn offset acknowledgements which
again out of the box and Kafka
especially if you integrate with
external systems as far as performance
is concerned in terms of in terms of
throughput we get comparable numbers the
latency can be worse when the cluster is
under stress and of course you also get
a bit of a storage overhead because you
need to have that additional metadata
topic right
however the metadata topic the markers
topic it can have a very short retention
policy it only needs in fact to keep the
markers of visibility timeout plus some
buffer for the reader will tracker to
actually catch up anything older than
the visibility timeout can be safely
discarded and a couple of links so the
project is open source on github if
you'd like to to take a look or maybe
use it there's also an introductory blog
where I explain more less what I've said
about here we also have perform a
comparison of performance between
various message queues so this benchmark
focuses on a queues which can replicate
data and so there you can see how a
rabbit activemq Kafka Mongo and some
others compare in terms of functionality
and performance there's some both a
comparison in terms of latency and and
throughput and yeah if you had if you
would have any questions after the this
talk and I'm everyone with our email and
here until Friday so yeah so I would be
happy to answer any questions if you
have some thank you
yeah
okay so the question is am i moving to
JMS so I suppose you if you wanted to
you could implement and I miss interface
on top of so like this and jameise
interface on top of sqs for example and
I I guess you could implement JMS on top
of that but I didn't do it
yet at least but it should be possible I
mean the model is quite similar yeah
sorry okay so current is the question is
when do you give up on a message that's
a very good question
so currently we never give up give up on
a message so what you would need here is
before we delivering the message you
would need to apply some transformation
on it to actually mark it as being
redeliver dice let's say five or six
times right so in increments every
delivery counter but that's that's like
very message specific so it's not
implemented in any generic way you would
have to apply the transformation before
the message is delivered so it's it's
quite easy to do but it's not done it
yeah
yes oh we we as a company are using this
not in production yet but getting close
okay thank you again and sorry for the
demo would you then for I'll fix it in a
couple of minutes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>