<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning as an API: easy ways to add intelligence to your web app by Sara Robinson | Coder Coacher - Coaching Coders</title><meta content="Machine Learning as an API: easy ways to add intelligence to your web app by Sara Robinson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning as an API: easy ways to add intelligence to your web app by Sara Robinson</b></h2><h5 class="post__date">2017-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8ZjQqyxghto" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Birgit you play kids do it thanks s
hello everyone
everyone here okay awesome thank you for
the great music by the way let's also
welcome to machine learning as an API
I'm going to tell you easy ways you can
add machine intelligence to your
application by accessing pre-trained
machine learning models with a single
API call a bit about myself my name is
Sarah Robinson you can find me on
Twitter at s Rob tweets I live in New
York I'm super excited to be here in
London today I'm a developer advocate on
the Google cloud platform team
what is developer advocate means is that
I get to do a combination of building
demos to teach other developers how to
use Google's different developer tools
give talks at events like this right
online content like blog posts
screencasts and technical walkthroughs
something I started by talking about
machine learning at a high level so at a
high level machine learning is teaching
computers to recognize patterns in the
same way that our brains do so if I show
a child a picture of a cat or a dog
they'll almost instantly be able to
identify which type of animal it is it's
much more difficult to teach a computer
to do the same thing the idea here is
that we want to write code that will
find these patterns for us and it will
improve over time as it said more
examples and given more experience so
this is what a typical deep neural
network looks like this particular
neural network is identifying a picture
as either a cat or a dog and we can see
here that the input to the network is
pixels in the image and the output is
going to be a prediction either a cat or
a dog and throughout the different
layers of the network it's looking for
features in the image so maybe it's
looking at on the type of hair the shape
of the ears the eyes and it takes all
that information is able to output a
prediction for us this is a pretty
typical neural network but I want to
take a step back for a moment and try
some human powered image detection so if
we didn't have neural networks if we
were to do this on our own how would we
go about writing the code to identify
the difference between this picture of
an apple or an orange you can shout it
out any ideas things you might look for
in your algorithm color color is a great
one
so we could look at the majority of the
pixels in the image if the majority of
them are red it's an apple if the
majority of them are orange it's an
orange
that would work great for this example
but what happens if we then have
grayscale images then we to start all
over again
writing new rules for this so now that
we have grayscale images what's a second
feature that we might want to look for
to texture texture is a good one so if
we look at the texture for these
different fruits we might be able to
output a correct prediction so what
happens if we add a third fruit we add a
mango it's going to get crazy we're
going to start again looking for the
texture and color of this specific type
of fruit all of these images look pretty
similar they're all types of fruits so
if we take two images that are
completely different this should be a
lot easier right so what if we have
we're trying to tell the difference
between a picture of a dog and a mob
right if you have nothing in common
the mob is not living or breathing it
doesn't have eyes a nose or a mouth
should be pretty easy to do it's
actually kind of difficult because so
here we have four pictures of dogs and
mops and it's kind of hard even for the
human eye to tell the difference between
these two they look pretty similar so
the idea here is that we don't want to
we don't want to write specific rules to
look for each of these individual things
in our application because what if we
have photos of everything right that's
more of a common use case so we might
have all sorts of different photos and
if we're just looking to identify the
difference between apples and oranges
our algorithm is not going to be very
useful for other types of applications
and in addition to photos you might have
all types of unstructured data like
video audio and text and this is what
machine learning can help us make sense
of but why would we want to add machine
learning functionality to our apps like
machine learning can tell us is the
picture of a cat so why do we care the
answer is it can help us automate things
that as developers we want to focus on
the user experience of our app we want
to add awesome functionality to our
applications and we might not want to
get hung up on you know building and
training our own model to be able to
identify things in our pictures there's
a lot of tools out there to help with
this so rather than reinventing the
wheel you can make use of these machine
learning api's to add interesting
functionality to your applications
there's essentially two ways to add
machine learning
to your apps so on the left side we have
some tools that can help you build and
train your own machine learning models
using your own custom data and this will
give you a lot more room for
customization there's a bunch of
open-source libraries that can help you
do this and then on the right hand side
we have machine learning as an API which
I like to call friendly machine learning
so this gives us access to pre trained
models with just one REST API request so
as long as you can send a rest request
you can make use of all of this machine
learning functionality so I'm most
familiar with the Google cloud API is
fitness so that's what I'm going to kind
of walk through today when I talk about
different machine learning problems but
there's lots of tools out there so I
encourage you to try out everything find
the best one for your use case the first
problem I want to talk about that you
can solve with an API is analyzing
images so using an API to do this you
can achieve pretty complex image
detection which is a single REST API
request and first before I get into how
this API works going to talk about some
problems that it can solve so one thing
that image analysis can help you do is
identify text in an image which is
something you might want to do across a
variety of applications and once you get
once you get that text you might want to
do something like translate it or
analyze it further you can use image
analysis to do this another thing you
might want to do is say is this image
appropriate or not let's say you have an
app with a lot of user generated content
and you don't want to have somebody
sitting there manually reviewing all the
images saying are they appropriate or
not you could add send do that with an
API call so you'd only have to review a
small subset of those images another
thing that can help you do is extract
emotions from images so let's say you
have a lot of pictures from an event and
you want to extract all the ones where
people are happy to create some sort of
flip book or maybe marketing materials
to tell a story about the event you can
use machine learning specifically image
analysis to extract people's emotions
from pictures and finally there may be
some use cases where you want to verify
that an image contains a certain object
maybe you have some sort of competition
people uploading photos that have to be
of a certain thing you can use machine
learning to verify that the photo does
contain that specific item
and so I want to talk about two
companies that are using Google's cloud
vision API which can help you accomplish
a lot of these things just so you can
get a sense of some production use cases
and companies that are using these api's
in the wild on the left-hand side
there's this is an example from Disney
and they used the cloud vision API to
promote a recent movie called Pete's
dragon and they had a game to promote
the movie where it was kind of a
scavenger hunt people were given a clue
and they had to go around and take a
picture of that item and if they got it
correct they would superimpose an image
of the dragon for each one that they got
correct so the problem was they need to
weigh to verify does this person's user
uploaded image contain the item that
they were supposed to take a picture of
and the vision API is label detection
was a good fit for that label to say yes
it contained the couch or a computer
they've solved this one correctly and
then on the right hand side realtor.com
use the vision API as OCR optical
character recognition to identify text
in for sale signs so when people were
going out and looking for houses to buy
they could take a picture of the sign
and it would bring up the relevant
listing in application so just some
examples of how it's being used in
production and looking specifically at
the vision API these are some of the
features that it provides so the most
basic one is label detection which is
what Disney used in the previous slide
which I was talking about and this will
this is essentially what is this a
picture of it'll give you for this image
it might say cheetah field animals
that's what label detection is face
detection will tell you how many faces
are in this image where are those faces
and what are the emotions in those image
in those faces are they happy are they
sad or they surprise or angry face
detection can tell you that OCR this
feature will pick out text in images so
in this sign for example it would be
able to tell us where the text was found
what language it's in and the actual
written text itself explicit content
detection pretty self-explanatory
touched on that before but it will tell
you is this image appropriate or not a
landmark detection can identify common
landmarks in a photo along with the
latitude is latitude and longitude
coordinates and a logo detection
tell you is there a common company logo
in this image so I just want to show you
what the JSON response looks like this
is a selfie I took in Jordan last year
with some teammates on a trip there and
this is the JSON response just for my
face it returns a JSON object for each
face found in an image and the one for
my face you can see it has a bunch of
data here it says head where likelihood
very unlikely which is true I'm not
wearing a hat but for my two teammates
it did return very likely for head where
it can tell you if you're surprised here
we see joy likelihood it's very likely
I'm smiling in the picture and it also
tells us where different features are in
the face so left eye right eye nose chin
in case we wanted to do anything with
that image like maybe draw a circle
around the face or a box and it gives
you other information like is it
underexposed is it blurred lots of
details on face detection the other
feature that I mentioned I wanted to
show you is landmark detection so does
everyone know what this is a picture of
I'm guessing you do so actually I
thought I did too but this is not the
Eiffel Tower this is the Paris Hotel and
Casino in Las Vegas kind of a fake
Eiffel Tower so I wanted to see what the
vision API be able to identify it and a
jig directly it was I able to identify
this as the Paris Hotel and Casino with
80% confidence so in this case the
vision API was smarter than me because
the human eye could be fooled by this
image which looks a lot like the Eiffel
Tower some new features that were
introduced in the API recently are
cropped ins which gives you suggested
crop dimensions for your photos Y by
notations is my favorite one I'm going
to show you a bit more about that on the
next slide this will let you search the
internet for more details on what your
images and then document text
annotations is just improved OCR so if
you've got an image with a lot of text
in it like a picture of a receipt or
business card or something like that it
will be able to do a better job picking
out all the words and paragraphs in that
text a little bit more detail on the web
annotations feature first thing I wanna
talk about is web entities so we can
find different entities in an image I'm
a big Harry Potter fan so this is an
image
of not just any cards the car in a
museum actually and it's the car from
the second Harry Potter movies so I
wanted to see what that what entities
the vision API was able to pull from
this image so the first thing was able
to tell me is that it is a Ford Anglia
so they will tell me the model of car
that it was which is true this is what
they used to fly to school in the second
movie and then was also able to tell me
that this is on display at the art
science museum which is a museum in
Singapore where the exhibit is currently
on display and finally was also able to
tell me that this is from Harry Potter
the literary series all this just from
sending the API the image getting a
bunch of data back so these are the web
entities in addition to the entities it
gives us a lot more data on what's an
image so it can tell us where all the
URLs where this exact image exists on
the Internet
which is useful for things like
copyright detection if you want to see
this user uploaded an image has it
already been published somewhere this
will tell you that partial matching
image images is kind of like a search by
image response so it can tell you where
are similar images on the web so maybe
it would show me a car that is the same
model but totally different background
things like that and then it will tell
you all the pages where the image was
found and you can all try it out in the
browser actually go to cloud.google.com
slash vision you can upload your images
before you write new code if you want to
see if it's right for your app I'll have
all the links on the last slide as well
but you can upload photos it'll tell
it'll give you the full JSON response
and you can see the response for all the
different features so that is image
analysis oh and in case you were
wondering how it performed on these dog
and mock photos this is the response for
the for the dog picture all the way on
the right it was 99 percent confident
that it is a dog and it actually was 77
percent confident that it's a komondor
which is the breed of dogs that this is
and maybe some out incorrectly for the
mops it was able to identify this one 70
percent confidence that it's a broom or
a tool I won't go through all the
responses but we can see the overall it
did pretty well this third one it didn't
come out with a labeled dog it said fur
so I wasn't sure if I was a hit or
and then on the third mop it's a textile
but not mop or broom but overall this is
a pre train model so we didn't add any
custom data to it we didn't build a
model to specifically identify dogs and
mops so considering that it did pretty
well getting about three out of four
images correct for each one so images is
one type of unstructured data we want to
analyze next I'm going to talk about
analyzing audio so is anybody used an
app on their phone that does speech to
text transcription voice direction app
looks like a few hands and so what the
API is that can help you with this can
basically expose that functionality to
developers so if you want to add any
sort of speech to text transcription in
your application you can use a machine
learning API to do this and so some
problems that speech transcription can
help your app solve is make apps more
accessible so maybe somebody that can't
type into their phone instead once these
voice directions this could be an app to
make make your application more
accessible for them so along those lines
it could help connects people that might
not otherwise be able to interact with
your application and facilitate
conversation over different mediums
audio text things like that so this is
an example of an app called as our that
is using Google speech API and it's a
chat application that has matched over
15 billion matches of people that can
chat with each other and they use a
speech API for all the audio snippets
that are sent over the app and they also
are using the translation API so anytime
so this is a good example of combining
multiple machine learning api's which is
something you might want to do in your
app so once they've transcribed the
speech let's say to the people that are
matched don't speak the same language
they can use a translation API to then
translate the text into the other
person's host language and the best way
to show the speech API is through a demo
so I built a bash script that calls the
speech API I'm using socks which is just
a command-line utility for understanding
audio so we're going to I'm going to
make a live recording I'm going to
create an API request send a JSON file
we're going to send it to the speech API
and then we're going to see its
response back
so I'm going to switch to the command
line over here make sure everyone can
see that and I'm going to call my script
with bash requests SH and since I press
Enter when you're ready to record so I'm
just going to record a five second audio
clip here we go
I built the demo of the speech API using
socks and I'm going to send that to the
speech API this is what our JSON request
looks like so we tell it the encoding
type the sample rate in Hertz and we can
see that did a pretty good job it got
the acronym API and obviously it didn't
get the tool that I use correctly
because that's the proper noun and I
didn't build it using my socks but
overall it did pretty well it gave me
eighty two point eight percent
confidence just to look at some of the
other things in their request so we tell
it the language code if you leave this
out it will default to English but it
supports over 80 languages so if you
have a different language you can just
put the language code in your request
and you'll be able to do that for you
the speech context is a parameter you
can use which I'm going to talk about
now related to this so let's say my
application has some proper nouns or
specific terms that the API may not know
how to transcribe correctly I can pass
it some suggestions here so if I go into
my script right here and let's say I
wanted to recognize this tool that I use
called socks I can put that in the
phrases parameter and it's going to look
out for that so now if I go back to my
script and record the same thing that is
looking for the proper noun socks I will
try it again
I built a speech API demo using socks
and we'll see how it did now it's
looking for that phrase I could have
also tried dev ox
I probably would have been a good one
too and it was able to get it correctly
with a little bit higher confidence and
so it's just an example of how you can
kind of add some of your own custom
parameters for the speech API -
specifically to look for things that are
specific to your application or use case
and also works in a bunch of different
languages and in this example I used
batch mode so I just
a complete audio file but you can also
use it also supports streaming so I
could send it a continuous audio stream
and it would be able to transcribe the
audio as the stream was coming in so I'm
going to go back to the slides
and so that was analyzing audio and one
thing you might want to do after you
analyze audio is you've got the text
transcription you might want to do some
further analysis on that text and
there's many different api's that help
you do this and what they'll help you do
is extract entities sentiment and syntax
in your text so you can further analyze
it what are some reasons you might want
to do this so let's say you have a lot
of people leaving reviews on your
application you might want to parse the
sentiment from those reviews so this is
just a TripAdvisor page that I grabbed
for a nearby restaurant and we can see
the first review had very positive
sentiment the second review they were
happy with the gin and tonics very happy
about those but they medium satisfaction
on the food and then the third one was a
comedic ly bad experience angry person
there and so instead of reading through
all these and finding out you know which
ones are good and bad if you have a lot
of reviews might be useful to have an
API that can do this for you another
thing you might want to do is find the
relationships between different words in
a sentence or extract the key phrases
from a sentence so in the sentence about
London I want to extract that it's about
London and the United Kingdom and I
might want to be able to tell you know
like what adjectives are being used to
describe a specific subject in my text
that's where these types of api's might
come in handy one customer that's using
Google's natural language API is called
woo trick and what they are is they're a
customer feedback platform so if you've
ever been on a page where you see a
little survey in the bottom it says how
is your experience with this application
and you give it a rating on a scale from
zero to ten this is what their
application helps our customers do and
they help them make sense of this
feedback so the first thing that you'll
do is you'll rate it from zero to ten
that's pretty easy to parse it's just a
number what is much more difficult to
make sense of is this open-ended
feedback that customers leave and
they're using the natural language API
in three different ways to make sense of
this feedback so for
they're using sentiment analysis to say
okay did the persons numbered rating
align with the feedback they gave so
maybe they said it was a 10 but they
actually had some negative feedback in
their open-ended feedback so they could
use the sentiment alysus to just kind of
gauge where that lined up and then
they're also using sentiment or entity
and syntax annotation to pull the
subjects from the feedback and route
that feedback appropriately so let's say
a customer wrote in and they were very
angry about usability they could then
route that by extracting that it was
about usability and send it to the
correct person to give them a response
rather than manually having to classify
that so that's an example of how you
might use this text analysis API so I
have I want to show you in a bit more
detail of three methods of this API that
you can use the first one is entity
extraction so I just took the sentence
about JK Rowling from her Wikipedia page
and I ran it through entity extraction
and it's able to find these five
entities in the text and what's
interesting about the first three is
they all actually point to the same
thing they're all different ways of
referencing JK Rowling the third one is
a pen names used for a different book
series and so we get in the JSON
response we get back all of these
pointing to the same thing so we get the
name of the entity the type of entity is
person and then we get some additional
metadata about it the MIT which is an ID
that refers to JK Rowling and Google's
knowledge graph so if you want to get
more information about it you could make
a call to the knowledge graph API using
this ID and we also get the Wikipedia
URL if we want to get any more
information on her and in the response
for the other terms look similar for
British we get a location which points
to the United Kingdom Wikipedia page
again so if you have a use case where
people are referring to the same entity
but in different ways you could use the
entity analysis method to kind of
normalize that and make sure you are
only counting the same entity once even
with different types of mentions of it
and then we get a similar response for
Harry Potter so that's the entity
analysis the second thing I can do is
analyze sentiment so in this in this
review the food is excellent I would
definitely
back we get two pieces of data on the
sentiment of this we get the first as a
score which tells you overall was this
positive or negative on a scale from
negative one to one how positive or
negative is this sentence and this one
is 0.8 so it's almost fully positive and
then the magnitude tells us regardless
of being positive or negative how strong
is the sentiment in this in this text
this ranges from zero to infinity this
sentence is relatively short just one
sentence so we get a pretty small number
for that here and then the last thing
this API can do is it can analyze syntax
in text so this part gets a little bit
more into the linguistic details of the
text and here for the sentence the
natural language API helps us understand
text we get a bunch of different
linguistic data on it
and this is just a visualization of the
JSON response we get back so the in
green we have arrows that are telling us
the relationships between the words in a
sentence this is called a dependency
parse tree and in orange we can see the
role of each word in the sentence so API
is a nominal subject helps is the root
verb so we get data there on let's say
we wanted to get extract all the
subjects from all of the sentences that
are coming in maybe it's from customer
support tickets something like that in
purple we have the lemma which is just
the canonical form of the word and so
the four helps it is help so if you
wanted to count how many times specific
word was being used
you probably don't to count helps and
help us to different ones so you can use
the lemma to get the root form of the
word in red we have the part of speech
noun verb adjective and then in blue we
have additional morphology details so is
it in first or second or third person is
it singular or plural is it a proper
noun things like that and there'll be
additional details for other languages
other than English here again this is
best shown with a demo so for the
natural language API
I built a demo where I use the Twitter
streaming API to stream all the tweets I
looked for I ran this last week for two
or three days and I stream tweets with
either London United Kingdom or Deb ox
in them
so I stream these tweets the streaming
API gives you just a small subset of all
the tweets about a particular topic that
you're filtering on and I sent them
through the natural language API in the
syntax annotation method so I wanted to
analyze the syntax of those tweets and
then I store them in bigquery which is
essentially Google clouds big data as a
service to LSU do an analysis on really
really large datasets so I saw them in
bigquery so that I could further analyze
you know what word adjectives people use
to talk about London what were the
subjects they were using things like
that so I'm going to switch to the demo
so here's my bigquery table and you can
see that over a couple days I collected
about 140,000 tweets that either had
London United Kingdom or Deb ox in them
so I can I can grab all the tweets with
Deb ox obviously with less tweets with
Deb ox and London or United Kingdom but
here here's all the data that I'm
collecting on the for each row in the
table so I've got the ID of the tweets
the text the timestamp it was created
how many followers that user has I've
got the hashtags that were in the tweet
that's returned from the Twitter
streaming API and then I have this giant
JSON string with the natural language
api's response and it tells us all the
part of speech data is it a subject is
it a noun or an adjective so what I
wanted to do was find out all the
adjectives people are using to talk
about London or Deb ox and you're
probably wondering how on earth I'm
going to parse this giant JSON string
with sequel there's a feature that we
can make use of called user-defined
functions which lets you passage a
custom JavaScript function that's going
to be run on all the rows in your table
so what I did here is I wrote a function
to look through this JSON string and
find all of the adjectives so if I go
ahead and run this again this is going
to run this JavaScript function on all
140,000 rows in my table took about five
seconds so I wasn't using cache results
and if we look here we can see the top
adjectives that were being used to
describe London or United Kingdom
probably related to
to some news that was happening at the
time and I can do something similar on
this tab I've written a query to extract
the subject from the tweet so let's say
I want to see what the subject was of
all of those tweets and find the most
common ones so the the JavaScript
function here looks similar and I'm
going to run this query on all 140,000
plus tweets and we'll see what subjects
we get took a little less than eight
seconds and we can see that a lot of
people reference the tweets maybe to
have an opinion about something so the
subject was I that one's not super
interesting but if we scroll here we can
see some other subjects that were used
in the tweet so that's just an example
of how you might use this syntax
annotation method I thought I would show
an example of that since it's a little
bit might be harder to imagine how you
can make use of that in your application
since it gets into very nitty-gritty
linguistic details but it can really
help you understand how people are
thinking about a specific topic one
other thing I want to show you with the
natural language API is that you can try
it out on the product page here and we
just added a new feature to it that lets
you do entity based sentiment so rather
than giving you a one sentiment score
for an entire block of text you can
understand what the sentiment was around
each entity so if I have a sentence like
I liked the sushi but the service was
terrible I might want it to extract
sentiment for each so here we get for
sushi gets a score of 0.7 which front
scale of negative one to one is pretty
positive and then service gets negative
0.9 which is almost all the way negative
so that's really useful in this case
where I have a couple different
sentiments it's throughout the sentence
so this is a case where I wouldn't want
to get just one sentiment score for the
entire thing so that is the natural
language API and I'm going to go back to
presentation mode
next thing I want to talk about is also
type of text analysis once you've got
this text one thing you might want to do
is translate it so you probably have
users all over the world you want to
make your application accessible to your
users wherever they may be
so the translation API can help you
translate text in over 100 languages or
detect the language with a single API
request why do we need translation has
anyone here ever used Google Translate
when they travel most people I find it
super useful and traveling always
especially in countries where people
don't speak a lot of English I'll use it
to talk to cab drivers or order food
last year I was on a trip to Japan and I
really wanted to order octopus I was in
a restaurant where nobody spoke English
so I showed the waiter Google Translate
found out the word for octopus was taco
this is a little confused by that but
when was it and was able to get some
delicious octopus but you probably want
to use it to translate more than just
the word for octopus I probably want to
translate lots of things in your
applications and one company that's
using Google Translation API to do this
is Airbnb and what you might not know is
that 60% of the bookings occur between
people that use the app in different
languages and they were able to use a
translation API to translate not only
their listings but reviews and
conversations to improve a guest
likelihood to book and they found that
using the API to do this greatly
improved people's experience throughout
the application just an example of how
you would call the translation API this
is a Python code snippet so you would
just import the Google Cloud module and
create a Translate client and then you
just call dog translate and you pass it
the text you'd like to translate and the
target language that you would like to
translate it to I noticed you don't need
to tell it what the source language is
that will be able to detect that for you
so if also all you want to do is detect
the language of attacks if you've got a
lot users inputting text in a bunch of
different languages you can just detect
the language without translating it and
then you can print the translated text
there one interesting thing that we
added to the API recently is neural
machine translation and this is a change
to the underlying model behind the
translation which greatly improves the
so before with first generation
translation the way it worked was it
would translate each word in the
sentence independently so this is much
the same as let's say if you were
traveling and how to travel dictionary
and you wanted to translate one sentence
you might do a word-for-word lookups you
might look up each word and come up with
a translation and your result would
probably be pretty accurate but it might
lose some of the context of the original
text and what the knurl machine
translation does is it looks at each
word or token in the context of the
surrounding sentence and it's able to
produce a much more accurate translation
than before there's a great New York
Times article about it if you go to this
bitly link if you want to learn more
about how neural machine translation
works under the hood and just to show
you some how it's an improvement of the
first generation translation there's a
lot of text on the slide I'll explain it
so what I did is I took a paragraph from
the original Spanish text of Harry
Potter they obviously didn't use an API
to translate Harry Potter they had
somebody doing it in each of the I
believe over 70 languages so this is
what the Spanish translator wrote and I
passed it through first-generation
translation and then compared it to the
same translation back to English for
neural machine translation and we can
see that the the improved translation
picks up a lot more of the nuances so it
changed from made change it to
manufactured which is a much more
accurate verb describing what mr.
Dursley did and then here if we look at
the bottom I bolded all the differences
but it changes sense of the gardens to
garden fence and then uses the correct
pronoun in the last example so small
improvements that they do make a big
difference in quality of the final
translation so I built a demo of the
translation API it's pretty
self-explanatory on its own because it's
translating text but what I wanted to
show you is that you can combine it with
other API s
and it can be pretty powerful so what I
did is I wrote a Python scripts that
either takes raw text as input or images
or audio from speech send those to the
natural language API and then translates
them into a different language so I'm
going to show you a quick demo of this
I'm going to go to the command line here
and I'm going to call my Python script
and it's going to ask me do I want to
enter text record text or send a photo
and I'm going to start by just entering
text I'm going to say London is one of
my favorite cities and here we can see
this is the JSON response for the first
token so just for London from the
natural language API so it's identified
as a noun
most of these things don't apply but is
singular it's the subject of the
sentence it ran a through sentiment
analysis oh that is a little hard to see
on the screen sorry about that
the colors so it tells us that the
sentiment seems happy and it's able to
find a few entities from this including
London so it can also pull out entities
that don't have an Associated Wikipedia
page so if I just wrote a sentence about
myself it would be able to pull out my
name as an entity obviously there would
be no metadata associated with it so I'm
going to translate this I will translate
it into French and then I've also
translated it didn't quite get that '
but I translate it back into English and
we can see that the translation is
pretty accurate here and one other thing
I wanted to show you this is how you can
combine this with an image analysis API
to extract text from an image and then
maybe you want to translate that text so
I have here just a picture of the mind
the gap sign from walking around in
London and let's see what happens when I
send that to the vision API and then to
the translation API so I'm going to send
a photo and it's going to ask me the
file path of my image underground JPEGs
and so we can see that it found that
mind the gap text an image it's able to
identify mine as a verb it's the root
verb of the sentence it's a pretty
neutral statement and now I'm going to
translate it into Japanese and I
translate it back to English because I
do not speak Japanese and it was able to
do a pretty good job locate the gap
still kind of gets the message across
not quite the same though so that's just
an example of how you can combine a few
different machine learning api's
to do different things with text
analysis in your application and the
final type of unstructured data analysis
I want to talk about today is analyzing
videos this is my favorite API to talk
about it's also the newest one that
we've really so video analysis api's can
help you understand your videos entities
at shot frame or video level and why
would you want to do this let's say you
have a large library of video content
and you want to make it searchable if
you were to do this manually would take
a lot of work of somebody actually going
through all of your videos and writing
down what was happening in every scene
so rather than doing it manually be nice
if there's an API that could tell you
what all of your videos are about so
then you could easily create a search
from that another thing you might want
to do is create a highlight reel so if
you have if your media company and have
lots of videos you might want to say
okay I only want my videos about this
certain specific topic and grab all the
scenes from those videos so similar to
what I was saying before you normally
you would have to have somebody manually
going through scrubbing the videos and
finding the exact clips of what you
wanted to put in a highlight reel and
then finally it'll simplify the process
of manually searching through videos to
make make a large library very
searchable so I'm going to talk about
Google's video intelligence API which is
relatively new we launched it about two
months ago and this is a comp Cantino
the company that is using it and what
they are is they're a media asset
management company so all their
customers have a lot of video content
they're uploading their videos into the
platform Kenta mo transcodes them and
helps them make sense of it and they're
using the video intelligence API to
search all of their video to improve the
users experience searching their video
content and this API is also best
experienced with a demo so I'm going to
jump to the browser and I have a video
here of a Superbowl commercial for
Google home I'm going to play the first
couple of seconds of the video I won't
play the whole thing but what we can see
going on here is there's a bunch of
scene changes throughout the video so
there's lots going on there's a dog
there's a garage if we were to manually
look at this we'd have to watch the
whole thing and write down what was
happening
each scene and then store this in a
database somewhere with the video API
were able to get a JSON response back
that tells us in each scene what label
does it find in the video so we can see
here that it finds a dog and it's able
to tell us exactly what seeing that dog
is in found a birthday cake over here at
towards the end of the video and if we
scroll down we can see all the different
labels that return or return so they've
been able to tell us that this is a -
and may pronounce that incorrectly so
it's able to tell us the specific type
of dog that it is and it's also able to
identify that mountain passed in the
beginning so this is great for one this
is what it does for one video but you
probably if you're using any sort of
video analysis tool you probably have a
whole library of videos that you want to
analyze and the video API can help with
that because if you can see that this is
the JSON response you get for one video
it makes it easy to make your entire
library of videos searchable so if I go
over back to my home screen I've got
quite a few videos here and let's say
that I am a media company sports media
company and I want to find all my videos
of baseball but I only want the baseball
ones not any other sports so I can
search my library for baseball videos
and it's able to tell me right away what
are all the videos that have baseball in
them not only is able to say this video
is about baseball it can tell me exactly
where in that video I can find a clip
relevant to baseball so this one is most
almost all baseball scenes this one on
the bottom actually only has one tiny
clip that's relevant to baseball and
this is really useful because if we had
somebody looking for that clip they'd
have to watch the entire thing find that
tiny moment they might even skip over it
if it's only last for maybe a second
this is the year in search video that
Google publishes at the end of every
year which gives you just the top
searches from that year so we can see if
we skip ahead here this is from 2016
when the Cubs won the World Series in
Chicago but it only lasts for that tiny
moment in the video so I'll do one more
search it's a little cold out today so
it would be nice to be on a beach right
now and while machine-learning
cannot take us there
you can show us all of our beach videos
so if we look here we can find all of
the different Beach clips in our videos
again even if it's just a tiny frame in
the video and so something that used to
be a manual task is now super easy with
a video intelligence API able to analyze
our video content for us with just a
single API call so I'm going to go back
to the deck for a bit of wrap-up a
little more detail on how this demo
works so all the videos are stored in a
Google Cloud storage bucket which is our
object storage tool and I use a tool
called cloud functions which is
event-driven micro-services so I have a
cloud function that's listening on my
storage bucket that's going to be
triggered anytime a new file is added to
that bucket so never new files added
this function is going to be called and
that function will check is this a video
file if it is it will send that video to
the video intelligence API for
processing and once it's done processing
in my API request I'm able to pass it a
URL of a storage of a file in Google
Cloud storage where I can write the
response so it's going to store all of
the json annotations in a separate
bucket and then the front-end is just an
ojs app and the front-end itself doesn't
call the video api it just pulls the
videos and their associated metadata and
it's able to display that and make it
searchable to the user that's a bit of
how the app works just a quick look at
what the label detection response looks
like for a video so here we have a video
that's just a tour of the White House
and at this particular scene we have a
bird's-eye view is able to identify this
label and then it tells us the start and
end time of when that label appears in
the video and in the confidence of how
confident is that it found that label in
this particular scene and then here we
have the scene of a portrait it's able
to identify that this is indeed a
portrait with 83 percent confidence
that's just what the API response looks
like and that's all I've got so these
are all the api's that I talked about
today if you want to learn more feel
free to head to these links and you can
actually try them out all
on the product page so what I showed you
at the natural language API is actually
available for all the different api's
you can upload your own content and try
them all out right there and if you're
interested in any of the demo code I
have that all in my personal github repo
so it's under ml talk demos for most of
them and in the video intelligence demo
is in its own repo and you can try them
all out in the browser as I mentioned
before and that's all I've got
thank you for coming looks like we have
a bit of time left if anybody has
questions I'll be around after yes the
question is how can it detect what
accent you're speaking and you're
talking about the speech API I'm
guessing right so you can pass it the
language code I know that it is a
specific language code for British
English for example so it can't detect
the language right now but if you if you
tell it to look out for a specific
accent then it will be able to identify
that otherwise it's okay picking up
accents without like if you if you have
a British English accent and just give
it the American English the trance the
transcription probably wouldn't be as
good so right now you just need to tell
it like the specific language code for
the dialect if there is one good
question then anything else
yes
it's how does it activate it on YouTube
like to support passing a YouTube URL
and so the question was does the video
API support YouTube URLs it doesn't you
need to pass it the file URL of your
file in cloud storage so that is good
feedback
the question was it for the speech API
if you say a word that has two spellings
is that the question how is it able to
transcribe which one I believe it looks
at the context of the surrounding
sentence but that's a tricky one so my
might have a lower confidence in that
case where there's a word that might be
what was your example vowel yeah that's
I actually don't know how it would how
would perform that but if you come up
after we can we can try to find out
socks yeah so that that one might be
tricky because once I gave it the proper
noun it's looking out for the proper
noun so it might not understand the
context if I was instead wanted to go
back to talking about my socks not the
command-line tool so that that might be
tricky based on context I'll be around
for a bit afters if you want to come up
and ask me questions happy to answer
them thank you all for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>