<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Harness the power of Spark SQL with the Data Source API by Stefano Baghino | Coder Coacher - Coaching Coders</title><meta content="Harness the power of Spark SQL with the Data Source API by Stefano Baghino - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Harness the power of Spark SQL with the Data Source API by Stefano Baghino</b></h2><h5 class="post__date">2018-03-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2JVcrGPFrIA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I think we can start thanks
everybody for coming to this talk my
name is Stefano Bettina I am a data
engineer at eBay and today I'm little
I'm gonna talk a little bit about this
data source API for spark sequel let's
start from the motivation for this talk
so basically I am a spark user I can see
that most of you are or are looking
forward to being the way so my role as a
data engineer basically boils down to
taking some data from a point a and
moving it to a point B and making it so
that this data is available to my mostly
internal customers in a way which is
performant and of course also convenient
to them so that the API is right for
them getting the API right can be wrong
I'm sorry can the API right can be hard
and there are several things that we
have to think about for example language
support I write things mostly in scholar
and in Java but I can expect analysts
and data scientists who be more familiar
with languages like R or Python also
when I'm designing an API will with
these withstand change your requirements
we we are also after engineers we all
have changing requirements so it's
difficult to get the API right in the
data driven world let's see there has
been these long things in traditional
that in sequel as the language that
interfaces your data systems that allows
your your your data system to be tapped
into by your analysts and by your data
scientists the good thing is that if
you're a spark user there is this thing
called spark sequel which enables you to
have the to to express things in a very
high-level language
which is usually either spark or
something maybe closer to the data frame
that you can see in Python think
Panthers and this solves some problems
because using leveraging sparks equal
means that we are automatically using
allowing users to use the language that
they want provided F of course this is
supported by spark I'm talking about
sequel Scala Java Python and evolving
the API it's no longer even IPI it's
just a tabular view on the data so you
just the the problem becomes scheme
evolution which is not a simple problem
from time to time but at least you have
some constraints that you can think
about the problem which is what drives
us to this talk is the limited number or
at least drives me the limited number of
naturally supported formats so you have
CSV of JSON you have or C or per K files
if you want some analytics oriented
format but these are more or less those
I mean what if we have our own customer
custom data format or what if we
actually just want to provide an
abstraction over exist in api's and we
want to use sequel as an ultra as an
obstruction for this the good thing is
that we have this data source again I'm
going in to introduce briefly today so
the aspiration is to have something like
you seen the second and third line in
this small snippet of code so you just
say okay spark just read with a custom
format you don't read JDBC you don't
read CSV you don't read or see you just
read a custom format that in this
example I'm I call just example sorry
I'm an engineer an autocrat and yes in
the same way and you can also provide
some options and you can see own as you
can see on the second line and also you
would probably find useful to also write
to this to this particular format that
you're defining so basically again to
reiterate the main point of the day is
that the data source API allows you to
extend the native capabilities of spark
sequel to create new data sources or if
I just read the slide but that's the
point we talk
let's take a step back I can imagine
most of you already being familiar with
Sparkle Sparkle I promise this is gonna
be a to slide the introduction to the
things so don't expect this to be very
interesting but if you already know the
thing but at least it gives you some
context and also being short means I
don't have to make a word count which
should be banned by by a big data talk
so sparking was in one slight spark
exposes a mostly functional API you have
your distributed collections and you
manipulate those like you would do with
Scala collections or if you're maybe
more familiar with Java like with the
JLS rate in Java streams API is
something like that to execute this code
in a distributed fashion it builds at a
regular cyclic graph of tasks based on
the transformation that you provide and
then it executes those tasks on top of
atomic units of concurrency which are
called partitions so you have your data
at this building across several
partitions on your cluster and your DG
is executed on top of that there is also
the problem of shuffling of course but
it's not in the scope for today yes and
of course interesting property may be
also a little bit out of the scope for
today but the idea is that this lineage
built for each partition can be really
cute as lazily and can be executed only
when needed
sparks equaling one slide again here we
are raising the level of destruction and
we are saying we are only exposing that
mostly declarative API so think about
sequel of course or maybe if you're
using this from Scala or Python think of
pandas so data frames what it does is it
takes your query it optimizes the query
plan both from a logical standpoint and
of course considering the physical
layout of the data underneath and it
produces the directly synchro graph for
you which is already optimized there so
it is an optimization that you do not
have to think about when you're using
these spirit sequel API so it's it's
really just runs on top of the Year
system and these are more or less the
basic concepts that you more or less
have to grab to do to get this talked
about in pretty sure that most of you
are already kind of familiar with spark
so let's get to the core of this talk
the colors do this talk of course is the
resource API we have a couple of
concepts that we want to introduce after
I introduce this small
concepts I will move on to showing you
some examples a very very small sample
to just give you a feeling of the API
and of course then the code is actually
also available on github so there will
be a link at the end of the talks but
right now let's focus on the on what we
want to implement what we want to
implement first is what is called a
relation well the relation is the core
of your data source and it defines what
gets extracted from from from your data
source and what's get written into it if
of course you want to do something which
is writable you also can do a read only
thing which from time to time that's a
good thing your implementation can
define implementing different traits
which is if you're coming from Java you
can think about of them as interfaces by
implementing different traits you can
express different capabilities that you
can build optimizations on top of your
existing data source so if you are the
first thing that you have to implement
of course is making sure that your data
source outputs the schema that it's
trying to read so what names and what
types what columns of this table or view
it's going to extract and then of course
if you want to read there are three
levels of optimization that you can do
on top of your data source first is they
just get all the data in the data source
which is the most basic thing that you
can do probably the data source itself
the storage engine is aware of things
about the layout of data that allows it
to make certain optimization like for
example column pruning which means that
you if you have 1,000 columns and you
only want to get three of those it's
beneficial if the underlying data source
can just load those columns exclusively
so this is something that you can do
with the source API and if you can even
have your data source being aware of how
to efficiently apply predicates so
filtering data on the data sources
itself you can also define how to push
down predicates
in the example we're going to see a
small example which highlights column
pruning not predicate push down but
maybe I'm not sure that the time is
enough let's say and when you're writing
of course you can define the insertion
if you want to create a new new relation
the place is somewhere else but we're
going to see this right now because here
is how you define the bulk of your
resource but you have to make these you
have to make spark aware of the
existence of this and in order to
support this dynamically at runtime you
have to implement the relation provider
the relation provider instantiates your
relation what spark does is whenever it
finds your the name that refers to your
data source so in our example before it
was this example string whenever they
defined this it tries to look up for the
data source provider the relation
provider that implements this that is
given for this string you can define
here our new relation gets created from
an existing data frames so you get a
date frame and you dump it to your D and
to your resource and of course you can
also define I mean it's you can even
either have it called by the full
qualified name of your or the class that
defines your relation provider or you
can have a short name like example as
you've seen before I'm pretty sure that
this is a lot so let's give a couple of
example let's give a couple of examples
to put this in a little bit important
perspective and give you a chance to
just see a little bit of code first
let's start from the very basic format
that we're going to expose with inspark
this is a very simple purely educational
text columnar data format so it begins
with the number of rows that you would
see in the file then for each column you
have the column data packed all by
itself for each column you see the name
of the column and then all the data for
that call
so for example here this file that you
see on the left gets translated in the
table that you can see on the right and
this is what we're going to implement
don't focus on the implementation this
is just to give you an idea of what
you're going to face when if you ever
going to use this API so here I define
my relation this is the example relation
it defines how you are you can read the
the the file that I wrote before as you
see it's it's it's very self contains
very very short this is actually the
full implementation and you can have a
look at it and you can get up but the
link will come later so the two things
that I would like you to focus on are
the fact that you are extending two
things base relation and prune scan base
relation you have to implement this and
when you implement base relation what
you do is what you have to do is you
have to provide a way to let sparc aware
of the schema for this relation so in
this case I just take all the column
names and I'm assuming that all the the
all the data is just string the type but
yeah again educational proposal it's not
supposed to be a serious format the
other thing that I'm standing as I
mentioned is prune scan which allows you
to define how to build a scan on your
data source for our restricted range of
columns so you see the build scan method
here where you have columns I shortened
it to see us just to fit into the slide
but bigger but yeah you have columns and
you have them defined as an array of
strings what I do here again do not
focus too much on the implementation but
I mean the idea is that this build scan
method allows you to define how you are
going to retrieve exclusively certain
certain columns one thing that this does
not do for example is what I mentioned
before so taking a predicate like for
example give me in this case all the
ages greater than 40 in this case of
course there is probably no way to
straight in in a straightforward way to
push down this filter but yeah
that's not relevant I mean it's it's
it's an another capability that you
cannot on top of this but this is just
meant to show you the API one thing that
I would like to mention also is that if
you define for example up not a prune
scan but a prune filters can which is
the thing that allows you to push down
predicates you do not have you do not
have to define both you just have to
define one of those bills kind of
methods but yeah let's keep through the
example for now here is the provider so
the provider for me for this relation
extends three things the third the first
thing is a relation provider the
relation provider allows you to define a
readable relation and this is taking
care of it the first create relation
method where you are passed by the first
by spark then through the the relation
provider then this is passed down to the
actual relation the sequel context and
all the options that you could have
passed originally are passed as a map of
strings and string if you go if we go
back to this example it would be the
contents in the second line of the
option method so some opt true will be
would be passed as this map of string to
string in that method okay so no sorry
yeah another thing that it implements is
the creatable relation provider that
allows you to find how you're gonna
write - are you gonna create a new file
in this case based on existing data
frame here the example is actually
shortened the implementation is a little
bit larger it's really not optimal but
that's not the idea behind this so again
and don't take this as a serious example
I mean this is just based on unlock of
files in your case you're probably
looking to have something distributed
not necessarily but that can be the case
okay
so we are moving on to the conclusions
we are gonna have then a few minutes for
a question if you want but conclusions
advantage over custom API is why just
not exposing something through a custom
API is something that
turns our data frame and the user just
gets that and uses that more or less
I'm always returning a different right
well the advantage here is that you can
write in Java or Scala which is if
you're the engineer or software engineer
it's probably a language that you're
much more likely to be familiar with but
you also enable the usage of your API
through Python sequel and our two year
users so you can define this you can
have for example have a plain notebook
and your user can load the data frame
that the data source that you define in
Scala which I think it's great also it
means that you can take advantage of the
native capabilities of a data source so
if you can push down predicates if you
can prune columns if you can do this
kind of Tamizh ation you can define them
directly on top of your of your existing
data source so this is another plus and
also this is something that I personally
found very useful in my use case in
abstract user from the underlying data
source which means what if we just want
to experiment with different data
sources what if we are not entirely sure
of how different access patterns will
affect the performance of the queries we
can try to swap in and out different
storage engines while keeping the API
for the users the same because the API
is just pure sequel or deferring and
also why not using different storage
back ends with the same defined as as
part of the same data source but perhaps
when you see one query defined in a
particular way with certain properties
you say oh no I want this to go to a key
value store or no I want this to go to a
relational datastore this is something
that came up was was very handily not
everything can be covered of course in
15 minutes this talk did not cover
catalyst integration catalyst is the
query optimizer for spark as I mentioned
when you see a query with a particular
with a particular shape you may want to
direct it to one or another data source
catalyst the catalyst integration in
this source API allows you to actually
navigate the old tree of the queries so
you can take a very interesting
decisions based on that or
you cannot even often as a query itself
we didn't cover streaming support if you
have Kafka you are maybe used to use
park streaming with kapha you can define
your own streaming data source with
streaming support also worth noting
which part dr. 22.3 which came came out
just a few days ago just to ruin my talk
introduces a new data source API which
by the way I would like to say it's
still not stable the describe API it's
still valid and supported if you just
have a look at the example that I'll
share it actually runs on top of
spectre-3 using the data source API if
you want what does the new APIs data
source API does better than the previous
one it's that it's easier to implement
data sources in Java right now it's all
written in scala and it's not very it
can be a little bit awkward to write
something that uses Scala code let's
color code from Java can be awkward from
time to time you know abstract away from
course part concepts so that it's it can
be more future-proof this API we didn't
discuss alternatives there are things
like drill or presto which more or less
cover the same space but of course this
was not the focus of the talk and with
this I have done and done I'm not sure
if we have time for questions we have
one minute so if you have any question
first of all thank you and
or maybe not I hope this means I have
been very clear okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>