<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Four Distributed Systems Architectural Patterns by Tim Berglund | Coder Coacher - Coaching Coders</title><meta content="Four Distributed Systems Architectural Patterns by Tim Berglund - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Four Distributed Systems Architectural Patterns by Tim Berglund</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tpspO9K28PM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">very good to be here my name is Tim do
work at confluence we are a company that
makes streaming data a platform based on
Apache Kafka and what I do what my team
does is we are responsible for training
and documentation and soon-to-be online
video and evangelism things like this so
this is what my team does really
anything developers need to be
successful using Kafka or using
confluent products and to feel awesome
doing it so thank you for being here and
letting me do my job this is the most
fun part of my jobs being here with you
all right we have until 2:50 and we're
going to cover some architectural
patterns they're not quite reference
architectures usually a reference
architecture is a very specific thing
like here's how you build you know an
example of a thing these are more
patterns or ways people build
distributed systems and these are most
of them are are you know from real life
and I'll I'll give examples of actual
companies that use these different
patterns and just kind of talk through
each one now if you were in my earlier
talk two hours ago you will see some
overlap so I want to warn you right now
if if seeing a few of the same slides
will make you angry you want to begin
processing that anger now ideally it
won't make you angry at all but just so
you know there's a little bit of overlap
otherwise they're they're fairly
different talks that that do different
things normally I would say if you have
a question ask it but we're in a little
bit of a large space here so if you're
really loud shoot a flare in the air and
then after your question otherwise just
see me later I'll be around all day
today and tomorrow alright let's look at
the architectures themselves we're going
to look at
or for different architectures for
building distributed systems look at the
pluses the minuses who uses them why and
I'm going to try to come up with all
rating of them when we're done all right
we'll begin with what I call the modern
three-tier architecture now when I was a
younger developer I entered the job the
world of Java in about 1999 or 2000
before that I had been writing firmware
so with my usual impeccable timing just
about the time the first dot-com bubble
was over I said hey let me get on board
of that internet thing became a Java
developer and people were talking about
three-tier architecture back then and it
like cutting out here oh right it looked
like this you had at your front the
presentation tier in the middle what we
called the business here I'm glad we
don't usually use that term much anymore
and then on the back that date it here
pardon me just a moment that that has a
terrible word on it that it shouldn't
have there was just before this talk a
terrible crisis in the formatting of
this entire slide deck and that crisis
is it's just about over now so back in
the day the presentation tier was JSP in
the middle we had terrible things that I
shouldn't even say in polite company and
on the backend another terrible thing
that I shouldn't even say in polite
company this is just how we built these
weren't really distributed systems as
such back then they didn't really act
like it and a few things went wrong with
that right and those things changed over
time and it felt like this you know we
talked like this went away and in the
middle of the 2000s you know there in
the java world there was struts and
spring MVC and then rails happened and
you know we were very focused on these
MVC frameworks and we stop talking about
tiers and if we look now at how you
might build a comparable system I'm
going to keep the diagram the same I'm
going to take away the screaming people
and I'm going to change the labels
might build this you might have a react
front-end in a node J s middle tier and
you might for purposes of scale have
Cassandra on the backend now that would
be a perfectly respectable system and
people do this and it's no different
than fifteen years ago
and almost twenty years ago the three
tier systems we were building really
really is the same thing so let's talk
about how to scale this now the front
end just now as then scales
automatically we have lots and lots of
clients lots of browsers out there
scaling the front end and we've gotten a
little smarter I think in that we are
pushing more and more application
functionality into that front end with
the 600 JavaScript frameworks that exist
for the front end a lot more of the
application lives out front than it used
to so that now is going to come into
some kind of a load balancer
that'll be elastic load balancer if you
have asked Amazon into your heart and
you do everything in Amazon or it'll be
some species of nginx it's not and then
we simply have lots of node instances so
this would be a typical way of doing it
again this is indistinguishable from
what we might have called three-tier in
the days of yore a long time ago on the
back end since the database was
initially very expensive and then when
open-source databases became popular the
database became a point point pain point
of scalability what do we do now what we
usually will just have a Cassandra
instance that's a good way to accomplish
that back end like that that database
problem if what you want is a system
with a database where you update things
in place that problem feels more or less
solved and we can just split that into
three pieces like so now for each one of
these architectures just to make this
worth our money I want to pick a
technology to explain a little bit of so
you have some idea of where the magic is
here you know without making it a node
talk or a react
talk I don't really want to get into
those I kind of want to talk about a
distributed systems technology in each
one so we're going to focus on Cassandra
I want to talk about Cassandra for just
a few minutes SuperDuper lightweight
introduction to Cassandra but as I said
if you're in my earlier talk you got
this same lightweight introduction to
Cassandra but if it was your first time
you probably need it twice now a
Cassandra database is not a terribly
difficult thing to understand the nice
thing about it and the thing that
originally six years ago when I first
started looking at Cassandra the thing
that won my heart was all of the nodes
are the same you'll need one shape and
one color to draw the diagram just make
a box and that's all you've got it's an
entirely peer-to-peer database there is
no central node of any kind and I really
really like that each node though has a
unique token and we use those tokens to
divide up the data that we're going to
write and read in this database the
problem is once I I scale my database
from being able to put all the data in
one place to now needing in this case
eight nodes in large clusters a few
hundred nodes I need to know if I'm
going to write something where do I
write it and so we assign each node a
unique token and we always by convention
we draw them in a circle we draw this as
a ring of these token IDs now this is
not a Cassandra talk we have about five
minutes talk about Cassandra here so I'm
going to skip over some details in
particular I'm not going to say anything
about the Cassandra data model it's not
a key value database but I'm about to
treat it like one because that's an easy
way to begin explaining it so with that
we're going to we're to act like Kiva
echo Sandra is a key value store and I'm
going to say hey I'm going to write a
key value pair into this in this case
the name of my favorite coffee and I
think I'm a really boring coffee drinker
this is true probably 90% of
Muffy's i order just an Americano I'm a
simple guy so Ken here is the key
americano is the value that's my
favorite coffee and if I want to write
that key value pair to this cluster what
we will do is we'll run the key through
a hash function and in this case I'm
using some of us this is the 16-bit
Arduino version of Cassandra because
those numbers fit on the slide better
rather than the real 64-bit numbers
those are a little unwieldy so we hashed
that and we say ok 9 f7 - I look around
my cluster and I say well that's bigger
than 8,000 less than a thousand that
means that a thousand node has got
responsibility for that token and if I'm
only writing a symbol rep single replica
which would be a little crazy in
production there you go I write that -
that token and I'm done now later on
when I go to read Tim's favorite coffee
as a client I'm ignorant of where that
is right nobody is going to remember
what node that got written to so what I
do if I want to read the Tim key is I'm
going to do the same thing hash that key
look around the cluster say what node in
this cluster has responsibility for that
token and that's going to be the range
of 8,000 thousand so I can look there
pull it out and I've read Tim's favorite
coffee we're ready to proceed with the
order in this web-scale coffee shop now
obviously I wouldn't want to write just
one copy right because one of the things
in a distributed system is here you're
using lots and lots of computers you
want lots and lots of uptime you're
always going to assume that some node is
going to be failing at some time that's
just that's just good sense so for
example in a simple case I might instead
of just writing to that node I'll just
keep walking around the cluster to the
next two nodes and write replicas there
now I've made three copies of the data
and I can lose nodes and still win now
this itself is seems elegant and simple
and it's kind of easy to wrap your mind
around where does the complexity emerge
here
once I have taken mutable data and
copied it in three places
bad things happen right that's just
that's a terrible idea because what if
when I go to write it one of those is is
having a long GC pause or something and
the write doesn't work one of those
nodes like today my favorite coffee is
an Americano tomorrow I might change my
mind to almond milk latte which is kind
of my 10% jam and the update tomorrow
maybe one of the nodes won't get written
I have to manage consistency now
cassandra has all kinds of elegant
answers to how that works
which we won't go into in detail here
because there's other things to talk
about but that's a typical pattern
anytime you're doing something that
would have been simple on one server
when you do it on many you you fix a
problem you create five others so that's
that's a complexity Cassandra's answers
to consistency are really fascinating I
love the way it handles those problems
and there's a cool website called data
stacks Academy if you're ever interested
in more of this Academy data stacks com
I don't work there anymore so I can plug
it as shamelessly as I want now the
really shameful part about that is there
are a lot of videos of me on there so
maybe this is just more self-promotion
and I just can't stop myself but for
more info and Cassandra lots of free
resources there I'm back okay I'm
cutting out occasionally so if I cut out
for long we'll fix it strength of the
modern three-tier architecture three
things lots of rich options on the front
end now you can make the front end very
responsive put lots of functionality out
there the more compute you can push into
the front debt the better you scale
right because you've got thousands maybe
if you're successful millions of
computers doing that work for you so any
compute you can push into the front end
still and still like the way your app is
built you have one you've distributed
that work to the front end and lots of
good options there the middle tier these
days not super hard to scale I remember
in the early days of Java frameworks
like struts and spring MVC people would
worry about oh no I've used the HTTP
session too much and and it's hard to
cluster Tomcats and all these things for
the most part we're better at that now
and the default way of scaling the
middle tier is easier so that's a good
thing
also the data tier with databases like
Cassandra now kind of being fairly well
battle-tested and and well availa
available and it's pretty easy to find
people who have used them that's getting
to be a safe choice you've got a very
scalable data tier as an example of
somebody who has productized part of
this Walmart Walmart gigantic us and I
guess global retailer has taken their
version of react and note J s and
packaged them at project called
electrode so if you remember those are
just the two examples I happen to pick a
presentation tier and middle tier well
large operations are using them and even
productizing frameworks based on them so
there you have it
now look sorry about that
what's bad about this any state that
exists in the middle tier is going to be
an impediment to scaling that middle
tier so you have to write those middle
tier servers in a way that's stateless
which means all they know is what they
get in the request and what they can
learn from the day to days and that is
of course harder than it sounds that's
an easier thing is easy thing to say but
actually doing that and delivering low
latency requests times is a challenge so
that it that can be a bit of a problem
let's rate this now for each of these
approaches I'm rating them on
scalability difficulty and flexibility
now I thought hypnotherapist I'm viewing
as inverses of one another and I picked
hypnose I'm going to say modern
three-tier is a 4 out of 5 on
scalability most of it as long as we're
responsible for middle tier it's
actually a pretty dang scalable
framework hip this we get two out of
five beards it's not especially cool but
it basically does work and difficulty
three out of five most of these pieces
are becoming pretty easy to deal with
Kassandra can be a little tough for new
people but clustering node and putting
react on the front end if those are the
tools you happen to choose you can fill
in those blanks anyway you want that
stuff's getting fairly easy and again
you can probably build anything with
this so this is not a bad approach not
about approachable and walmart.com like
I said that's that's a gigantic global
retail site built this way all right
let's talk about the next pattern the
sharded architecture a slightly
different approach we'll start with this
same thing that same basic diagram here
we've got our client on the front we've
got a database on the back and a piece
of complete application in the middle
and at low scale you build the system
like this this works there is some
number of clients there's one server
that's being the complete application
there's this one Postgres server on the
back end and you back it up in some way
that you're happy with and life is great
you're not building a distributed system
like everything is beautiful here and if
you can just live here if you take my
advice I'd just never leave this slide
right here your life is going to be so
much better as I said if you're in my
last talk I took some time to try to try
to talk everybody out of doing anything
with distributed systems it's just it's
terrible anytime you do everything goes
wrong so if you're here which means
you're you're not going to take my word
for that if would if you would just live
here life would be great but what's
going to happen at some point
maybe the application the site the
business whatever it is it's going to
have some success there'll be too many
clients for that one application and
that one database instance to handle so
what do we do fundamental insight of the
sharded pattern is to say we're going to
break the application up into pieces
such that each one behaves like a non
distributed system you remember my
advice don't build distributed systems
well the sharded pattern really tries to
live that way it says I'm just going to
have a complete application instance a
chunk of the whole application and just
repeat lots of those and in between my
clients and those chunks of application
I'm going to have some sort of router
that will direct traffic from the
clients to the correct shard and this
gets done slack is a good example of a
very successful modern service that's
built with shards it's essentially a
sharded application easy enough easy to
think about that router could be a
little bit of a trick but it's not all
that hard to do and if you're a service
like slack where the whole universe of
slack is broken up into these
organizations and most of the
organizations are not
that big for example my family slack
channel house of Bergland slack comm
unless you live in my house or one of my
children you can't come so I regret that
I can't in it but house of burglars
lacks calm very small slack org it's
going to fit nicely into a chard and
frankly it will be able to share server
resources with a number of other small
slack teams to see this really really
well now when does this go south let's
talk about when sharding goes wrong now
just thinking about the sharding of a
database for a moment not a whole
application but sharding of a database
time-tested pattern that is nevertheless
fairly bad will have when we begin
scaling the database we usually start
off with read replicas like you show
here so one big database there that's
the master all the rights go to that and
we replicate those through the read
slaves we can direct reads from the
applicant to those slaves rights to the
master and everything's okay usually
there's probably a little zookeeper down
there just kind of doing its thing
helping us coordinate something or other
but when we run out of gas with this
this is not charting yet this is just
read replication when I've got too much
right traffic for that that master to
handle well now I have to break up the
database into chunks or shards and
direct traffic to each one of those
shards based on some key now again if
your slack and there's always an
organization that you're connected to
this works great there's always a key to
start with and so you can conceivably
shard
so there are workloads like that that
are very shard about there are other
workloads that are not as char double so
this works sometimes sometimes it
doesn't but suppose it works suppose
we're slack and then this is this is
working out well let's look at what can
go wrong here's where sharding can
regarding a database at least can really
get ugly now that remember that
zookeeper down in the corner he's
helping do things like alexa master each
one of those show
has four servers in it one of them gets
to be the master only one of them gets
to the master at any time that's where
all the right traffic goes you can
load-balanced the reeds to the slaves
and life is good but bad things happen
in networks and some of these servers
might occasionally get cut off from the
other ones we call that a partition a
network partition is when one or more
the servers in your cluster can't talk
to the others this can be for very
simple reasons like I said a GC pause
some people define a long GC pause as a
partition sometimes networking
infrastructure can get miss configured
ports can break switches can break
anything can go wrong sharks can bite
cables whatever it is we can get a
partition and when that happens well
what's going to happen those guys are
going to go oh no the master is dead on
the bottom side of this partition
they're going to go talk to zookeeper
and say oh the master is dead and we
need to elect a new master and that's
fine and then later on the partition
will go away and what happens then well
this is I guess kind of like the
European history we have the Avignon
papacy where you had a Pope in France
and a Pope in Rome and both of them
claim to be Pope and they were
anathematizing each other and was very
very messy and it took a long time to
sort out that was not a partition
tolerant system it was hard to resolve
the partition so that happens in sharded
databases and in other kinds of sharded
systems that's a potential weakness also
typically in sharded systems you have to
decide upfront how many shards you're
going to have it can be difficult to
recharge later on now that's not
universally true of everything there are
like some databases that will help you
reach are dynamically but it's a thing
that requires a certain amount of
planning ahead and can have problems in
production so what is sharding good at
sharding again lets you pretend that
you're not building a distributed system
each chunk of application functionality
you you build conventionally using known
and ideally simple technologies it's
also easy to isolate one client from
another so if you've got depending on on
your physical networking topology
geographically where clients are served
can matter a lot there are for example I
don't know of all the countries in
Europe but I know German data privacy
laws don't allow German companies to
host customer data on US servers so if
you have infrastructure that's located
in territorial United States a sharded
system might let you locate some of that
other infrastructure in other areas
where you know data sovereignty laws are
compliant with whoever has the guns
locally so that's potentially a strength
there it's easy to isolate clients from
one another in terms of data and in
terms of where things are deployed so
where does this go wrong now it feels
like a good way to manage complexity and
it is in terms of the application and as
long as most of your effort is in the
application itself you've done a good
job keeping the distributed system bad
stuff away from you and just really
acting like you're building an
application for a single server however
little bits of complexity are going to
nose their way back into your system for
example monitoring and logging you
pretend that it's not a distributed
system but we all know that it is and if
you're running a large sharted system
you are going to need to know what's
going on on all of your shards and
you're going to need to know aggregate
performance data there's going to be
trends that are important to you so
monitoring and logging still distributed
concerns you are not getting away from
that altogether
also there's complexity in the router
right the router
is a new piece of infrastructure that
needs to be developed it's not that bad
but that is going to that's going to be
a little piece of things that you have
to do in terms of the data itself
I now have as many databases as I have
shards and that's potentially a lot and
if I want comprehensive analysis of all
of that data I'm going to have to do
something like ETL to take my six
thousand charted databases and squish
them into one place so I can do analysis
on that data so I've probably created
the need for some kind of ETL process
that I would otherwise like to avoid
over sized shards are also more or less
a fatal problem other problems like what
we saw with master election or as I call
it the Avignon papacy problem
those those we can we can figure out
ways to solve those but if one shard
becomes larger than I can handle on the
hardware that I deploy my approach has
failed and I will now need to introduce
more conventional distribution
techniques with inside that shard like
we saw with the modern three-tier a
shard becomes its own a shard its own a
shard suddenly becomes its own little
distributed system we're going to get
through this and and like I said that's
that's you need to be sure that you're
never going to have a shard that's
oversized that's a bad thing so what we
often end up doing to solve the data
problem is we might take that data on
the backend and aggregate it into a
large database for analysis or logging
or something like that so we're in other
words we've we pushed a lot of the
distributed system stuff away but it's
always going to creep back in that
abstraction is always going to leak you
never get to get away from it all
together all right
now let's rate this guy I'm going to
give it a three out of five for
scalability scales just fine as long as
your shards don't get too big it's not
very cool but that's ok sometimes not
cool
the best it has difficulty in surprising
ways because of what it forces you to do
for analytics in that it forces you to
deploy some kind of ETL solution and
other cross-cutting concerns are still
always a part of your life that's fairly
not fairly flexible it's not too bad all
right a lambda is not a general-purpose
architecture but it's more of a way of
dealing with analysis and it introduces
a few enter at one really interesting
topic and lets us introduce another
open-source project that's worth talking
about so I like bringing it up also it's
a great springboard into the fourth and
final architecture that I want to talk
about which is streaming so lambda kind
of tease us up nicely for that lambda
makes this distinction between what we
call streaming data and batch data now
that what's the difference to streaming
in batch the difference between good
we're back the difference in streaming
and batch is batch batch data processing
is what we do on data that lives
somewhere in a database it's in a file
it has come to rest in some place and we
address it by that place that it is in
either a file name and an offset or a
primary key in a table or something like
that it is in a place somewhere
streaming data is a log of events we
might also refer to these by the terms
bounded or unbounded bounded data is
batch data we sort of know how much
there is it's right here we can look
unbounded data is just a succession of
events that are happening and we presume
they'll never stop
last year I think was about a year ago
yeah coming up on a year ago there was a
great interview on software engineering
daily with Francis Perry now she's a
lead of the Apache beam project which is
one of the kind of streaming data
options out in the world right now and
if you're not a software engineering
daily listener you should be a terrific
podcasts incredible amount of work being
done there just to keep developers up to
date on on what's going on in the world
I'm constantly impressed with this thing
and Perry happens to be an extra good
guest I've been on some software
engineering daily and I have been
interviewed a number of times I learned
a lot just from her media presence she
did just a great job good on the content
good good on the delivery so I recommend
that podcast if you are interested in
the topic so the lambda architecture
assumes unbounded data there is a stream
of stuff happening and it assumes that
the data is immutable so these are
events and events are after all
immutable once something happens and I
have a record of the thing happening I
don't want to change that record events
are definitely immutable sometimes we're
painfully aware of that right you can't
go back and change something you did so
lambda is on board with that we have
unbounded immutable data and let's talk
about what happens we've got these
events in the world and on the one hand
we're going to write those events to a
database this term storage
and this is this is you know we could
say Cassandra we can say we're scaling a
relational database somehow but this is
for our bounded analysis we're going to
have some batch framework like a spark
or I'll just stop there hopefully it's
just spark and we'll do that that batch
analysis on that data at rest in that
database with relatively high latency
spark is a sir framework than Hadoop
MapReduce is but usually with a spark
job you're talking about a long time
seconds at the least min
ours are not impossible if there's a
huge amount of data you're doing
something complex this is high latency
analysis on lots and lots of data those
events also get written into some kind
of event processing system the lambda
architecture thinks of this as a
temporary cue some message queue of
unbounded events so they get written in
and then expired after a relatively
short period of time so we can do this
low latency enough down here and we're
going to take both of those our high
latency bounded analysis our low latency
unbounded analysis and write those
results to some kind of scalable data on
the back end so anything we need to know
now like Twitter for example the the
notification your two friends just liked
this tweet I assume some of you may get
those notifications on your phone I'm
not convinced to the value of them and
maybe I should turn them off for myself
but that's a that's a that's a real time
breaking news kind of thing that's going
to be in that event passed down at the
bottom longer-term analysis might be
like what business analysts are doing
about what what people respond to what
kind of ads and ad hoc query and that
kind of thing those would be batch jobs
running on that top top thing so the
event queue coming in usually people
think of that being Kafka will say
Cassandra and SPARC for the batch
analysis some kind of event processing
framework we'll talk about options in a
minute on the bottom and then those
things get written to a low latency
scalable database on the backend like
Cassandra so that's the idea of the the
lambda architecture the basic insight is
there's two things you want to do and
it's hard to do both at once we think
and so let's build two systems and
optimize one to doing the unbounded
analysis and optimize the other to do
the bounded analysis and just write all
the code twice we'll come back to that
this brings us to another open source
project we can explore a little bit and
that
is Apache Kafka since you've usually got
a Kafka in a lambda architecture after
we go through this we'll peel back the
layers on Kafka a little bit more and
see if we can simplify lambda and arrive
at our fourth and final pattern alright
in Kafka
we've got producers consumers and topics
topics are just named queues of messages
the topic has a name I've put messages
into it as a producer as take as a
consumer I take messages out of it a
topic lives on a broker a broker is just
a server running Kafka so it's a
computer running Kafka a topic
importantly can also be partitioned
because probably that one topic since
we're in distributed systems land is
going to get too big for one machine
either there's going to be too much data
or the data is going to be changing too
quickly I'm going to be reading messages
in and writing messages out at a rate
greater than one machine can handle and
if I do that many many times with lots
and lots of topics that get large I'm
going to need a cluster of brokers that
are all working together let me give you
an idea of how partitioning works in
Kafka now if I were leading a simple
life and using a non distributed message
queue I would probably still have topics
I would certainly still have messages
producers would put messages into topics
consumers would take messages out of
topics and I would have guaranteed
ordering anything I put in I would get
out in the right order and life would be
great my life is still pretty dang great
with Kafka but I have to give up one
thing and that's ordering so let's say
instead of just one partition I want to
partition into three brokers I want
three brokers to cooperate in handling
this topic so 0 1 &amp;amp; 2 are three separate
computers I've got my producers over
here on the right I've got my consumers
on the left and my producers will just
start to make messages now each message
we're going to look at some portion of
the message and hash that like maybe
look in and find an account ID or an IP
address or some sensor ID or something
like that and hash that thing and using
that hash I'll figure out which
partition I write to so different
messages that get produced will get
written to different topics and we see
already we've forgotten what the overall
order is I don't have overall order I
only have order within partitions and
that's a fundamental limitation once I
decide to be a distributed message queue
I'm not really going to be able to have
global ordering I'm only going to be
able to have ordering within each
partition and that's a bummer that's the
thing I have to give up on but I deal
with it and because the producer is able
to decide what part of the message gets
hashed if it wants to be thoughtful
about that then I can have ordering say
within user name or within IP or within
sensor ID or whatever my domain is I get
to preserve some ordering partitions are
ordered whole topics are not ordered so
then as I consume those consumers will
read messages out in order and of course
those consumers that doesn't show in the
slide but those consumers are running in
parallel all right that's just a quick
key into Kafka Kafka's usually involved
in lambda and it's definitely going to
be involved in streaming as we'll see
next
all right what are the strengths of
lambda well again it's basic insight is
to say look we want to do batch jobs and
we want to do stream analysis and it's
real hard to do both of those at once
they're just different kinds of systems
and and they deploy differently and you
optimize differently so let's just build
them both and optimize both that both
tasks that's that's its basic strength
is we got these two subsystems and let's
try to be good at both things separately
it's also good at unbounded data and the
previous two architectures are not good
at that the weakness
you have to write all the code twice
that's basically that the classical
approach to lambda this is becoming an
almost frowned upon thing because it's
hard people's experiences well I have to
do this stuff twice and that is not fun
to write basically the same code in two
places so let's rate this scalability
very scalable definitely only one beard
out of five at this point because it's a
five out of five on difficulty and it is
not really a general-purpose framework
this is for analysis purposes you
wouldn't build your whole site using
lambda you'd analyze events using lambda
but this brings us to streaming our
fourth and final architecture now in
streaming we have a few things that are
true usually people get into this
starting with integration as a gateway
drug
I have system a and system B and they
need to talk even if you're not building
a lambda architecture a lot of people
these days are starting with Kafka to
integrate systems
I've got data I need to load into Hadoop
or I got micro services and I want them
to be able to exchange messages with one
another good another fundamental insight
of the streaming pattern databases are
by their nature static things that means
where database means is I've got
something that's true and I'm putting it
here and I'm going to be able to address
it in this place that data just sits
there the business that you build around
that database it's fundamentally a
dynamic thing there are events happening
and things changing all the time so what
we do a lot of what we do frankly it's
depressing amano what we do as
developers is is we take forms of we
take form submissions and we do a little
processing and we write things to a
database then later on we read things
out of a database and do a little
processing and format it and send it
back to a client it's kind of your job
but it's important work don't get the
press the point is the the mapping of
these events onto the static data is a
little bit of an impedance
match and streaming says hey wait
because the world is dynamic maybe the
system should be to the Kafka approach
to streaming which we're going to see
keeps computation very close to keeps
stream computation very close to where
your application code is this is
absolutely key that doesn't make sense
yet but it will keep that in mind you
want to keep stream computation very
close to your application all right
let's look at these concerns one at a
time integration usually starts off with
hey I've got this database and I've got
this application I've got this other
application and they'll both talk to the
database and they'll occasionally write
stuff to my HDFS cluster and
everything's fine and then you build
another application and there's another
database and a elasticsearch and and
more services and everything is pretty
sure pretty soon it's you know it's bad
bad bad things have happened when these
applications are talking to one another
um that rat's nest in the middle we can
clean up if we do all of the services
will always just exchange messages with
one another these messages will go into
a common message bus and we'll leave it
at that
now if you've been around for a while
you recognize that pitch from about 15
years ago that sounds like the pitch for
an enterprise service bus you have all
these disparate services in your eyes in
your enterprise let's have them all read
and write XML messages into this
expensive service bus and all your
problems will be solved and oh by the
way let's put some message routing and
processing functionality up in that
service bus and pay an enterprise
software vendor lots of money and
everything will be fine and usually back
then the CIO bought the software on the
golf course and we just where the
benefit beneficiaries of the decision
make sure I don't sell you that here so
in the next few minutes I'll try to
explain why streaming is different from
that
well we end up with we
is rather than that rat's nest we'll
have a message bus at the middle and
we'll have a service here and a service
there and we'll have monitoring we'll
have an elastic search cluster will have
some analytics will have Hadoop all of
those things are fine and the the this
is just basically describing a micro
services implementation here all of the
services are just going to exchange
messages with one another rather than
necessarily write into a database one
service writes to a database and to
integrate through there the services
will simply read their inputs from
topics right they're processed outputs
back to topics which will allow other
services to consume those same things so
there isn't ever really a central
database where these things live there
are just topics we just stream data and
those connections there in the case of
Kafka those often end up being very
standard things that you want to do like
it's very typical to dump data into an
elastic search cluster or into Hadoop or
maybe you've got a legacy database out
there somewhere and you want to read
database you want to read data from that
database those lines can actually be
implemented in in modern Kafka modern
Kafka is much more than just a message
queue when you're building a streaming
system modern Costco's got a thing
called Kafka Connect that's an API for
doing those standard connections to
things on either side of the message bus
you don't always have to write that
producer and consumer code yourself
often if you're if you're connecting to
a standard thing it's a configurable
item and not a thing you write code for
the event stream I was kind of waving my
hands and saying here are these events
coming from out in the world and yeah
fine
you say Tim as long as you've got an
event-driven system that sounds great
but what if I've got a database what if
I've got you know a record of truth
somewhere well the the translation
between database and stream or table and
stream is a little easier than you might
think
updates that I make to that database if
I insert a new row well that's an event
if I change a row well that's an event
if I delete a row that's an event so I'm
able to take my basically create a
change log of what happens in my
database and make that a topic of event
databases do this internally all the
time anyway this is a standard thing so
if you've got a table you need to
translate that at the streaming world
it's okay and doing that starting to
rely on my message queue as a place to
store data isn't as crazy as it sounds
the retention policy in Kafka is seven
days by default the New York Times
stores all of its newspaper data going
back to the 1860s which is a long time
ago if you're an American in Kafka
topics so that retention policy is is
configurable you've also got great i/o
performance you've got very fast writes
anything you'd want in a distributed
database partitioning replication scale
those things are all there next insight
that streaming asks us the next change
streaming asks us to make remember this
we've got the three-tier architecture
we've got the presentation here the
business dear the data tier what we
normally do is we make a request to the
presentation tier and then we get a
response well in a streaming system an
event happens to the system gets
consumed and another event happens to
the system gets consumed maybe it gets
consumed by a service gets written to
elastic gets written to the analytics
system whatever has to happen in there
we're able to do that the events just
come in
each service is able to consume them the
way it wants then later on I can go to a
particular small-scale well-defined
service I can make a request of it I can
make a request of my elastic cluster and
and and get a response from it all of
which gets us finally to something that
this talk is not about but I strongly
recommend you check out the streams API
for Kafka this has been a core part of
Kafka since a year ago on streaming data
it lets me do aggregation filtering it
lets me join certain types of stream and
it doesn't require its own cluster this
is not like spark or flink or one of
those things where I stand up a stream
in cluster this is stream processing
that lives inside my service so
deploying this is super easy I already
know how to deploy my services so my
stream processing code just gets
deployed along with them
finally let's rate this guy overall
we'll give it a five out of five for
scalability because well it can scale
very well very hip this is this is the
leading edge architecture of these four
in terms of of mindshare it's kind of
hard to do because it requires a lot of
different thinking but you can build any
kind of system with it highly flexible
now with that we are just about that
time
can we really declare one winner we
cannot this is a little bit of a salad
bar kind of thing people are going to
pick what they want there are different
architectures that are suitable for
different purposes and that is about the
best we can say all of these decisions
are always contextual let me give you a
few things in closing in fact this is
the one you want to take a picture of
since I'm with these guys if you want to
know more about streaming and more about
Kafka
check this out you can find out about
our meetups join our slack community I
would love to see you there hope to see
you at a future meet up also summit in
San Francisco talk to me if you want a
discount code I could hook you up I'll
be hanging around later you're out of
here time's up guys thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>