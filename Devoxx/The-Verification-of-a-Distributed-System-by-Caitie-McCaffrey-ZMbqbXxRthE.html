<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Verification of a Distributed System by Caitie McCaffrey | Coder Coacher - Coaching Coders</title><meta content="The Verification of a Distributed System by Caitie McCaffrey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Verification of a Distributed System by Caitie McCaffrey</b></h2><h5 class="post__date">2017-04-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZMbqbXxRthE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to talk to you guys all
today about the verification of a
distributed system and what we're going
to focus on in this talk is you know
less about formal methods although we'll
talk about them briefly but more about
what you can take back in your
day-to-day jobs to help build more
resilient and reliable systems and
increase your confidence that the system
is doing the right thing so I am Katie
McCaffrey I am the distributed systems
engineer I work at Twitter and I've been
sort of all over the place in platform
and developer tools prior to that in a
previous life I worked in the games
industry on titles like years of war and
halo this is where you can find me on
the internet my DMS are open on Twitter
so if you have a question or want to
think of something afterwards feel free
to hit me up on that okay so let's just
dive in and get started so when I talk
about increasing the confidence of our
so that our system is doing the right
thing what we're really talking about is
exploring on the state space of all
inputs into our computer program so if
you just visualize the circle is all
inputs that are potentially into our
system how do we start exploring that
state space and then verifying that
certain combinations of inputs are doing
the thing that we think they are I also
wrote this article in the communications
of the ACM and then was also published
in ACM queue called the verification of
a distributed system that's sort of like
the more formal less fun version this is
the fun talk version where I get to tell
you more ridiculous stories and
anecdotes because you kind of to keep
your stuff buttoned up for the ACM so
but there's this is online if you want
to read it afterwards as well okay so
Leslie Lamport once famously said a
distributed system is one in which the
failure of a computer you didn't even
know existed could render your own
computer unusable so I guess back in the
day when we were originally exploring
the idea of distributed systems this was
okay but it's totally not tolerable now
to say hey some computer on the internet
went down sorry you can't tweet or sorry
you can't watch netflix or you know
whatever you're a business of choices
sorry like this thing is not available
so we have to figure out how to make our
systems more robust and we're sort of
all building distributed systems so when
I talk about this this should be
applicable
every one because it could be something
simple like a client and a stateless
service and a database which is your
canonical source of truth this is a
distributed system it could be something
like Twitter services where we have all
of our micro services that talk to each
other and form these sort of like
Deathstar architectures or it could be
something that you've inherited and
people duct-taped a bunch of things
together but you still have to maintain
it because it serves some business
function and so how do we verify these
systems this talk should help us address
all of these questions so we'll start
with a quick overview of formal
verification if you haven't seen it I
tried I'm trying to humanize it in this
talk a little bit it's really not as
scary as I think a lot of people make it
out to be and then we'll spend the bulk
of the time on testing in the wild and a
lot of different techniques and research
about how we increase our confidence in
system correctness and then I'll talk a
little bit about research because I
always love doing tech transfer between
academia and Industry and sort of
providing you a new hope and maybe where
I think we're going in the future there
will be a lot of information in this
talk I will tweet out a link to visit on
my github account there's a references
section that has links to like literally
every paper and paying reference in here
or if you want to dive deeper it'll all
be there I'll tweet that out and it'll
be on a bigger slide at the end so don't
worry about that okay so formal
verification so formal verification is
an academic term and we're sort of using
math and propositional logic usually to
see that a system is provably correct so
once we've done a formal verification of
a system we know that it's provably
correct it's mathematically correct like
there are no bugs in the systems that's
sort of the gold standard this is
typically done using a formal
specification language either like TL A+
is one that's super familiar or super
famous or coq is another one and so
basically what it's doing with formal
verification is you have all of the
inputs and we're exploring all of them
with formal verification so that we can
say that the system is provably correct
because we've literally analyzed every
single input and the way it's doing this
with a formal specification language is
you're sort of writing a proof and I'll
show you an example of that and it's
just a second and then we're either
using a proof assistant so like you know
actually proving it using math that it's
correct or we're using a model checker
and all a model checker does is it
explores all of the state space of all
inputs into your system this obviously
takes a long time depending
and how complex your system is but then
once the model checker runs and is
exhaustively explored the state space
you can say you know we're done
everything is correct if it passes so
this is an example of what a formal
specification looks like from TLA plus
if you're super interested in this
Leslie Lamport actually is a book online
called specifying systems that's for
free and you can just go read it but
this is what you how you would write a
TL A+ specification for a hour clock so
a clock that goes from 1 to 12 and then
like back again because we're not on
European time so this sort of looks like
code it's just propositional logic you
write this and then you would use a
proof assistant or a model checker but
what we're doing is we're saying we have
this variable our it extends natural
numbers so and then basically our can go
from 1 to 12 those the valid range is it
can be in some 1 2 3 all the way up to
12 in the natural number sub range and
then we're defining what the transition
is like so if our is 12 or not 12 then
do our +1 else set it to 1 and then
we're adding this concept of time and so
if you ran this you could have a
provably correct our clock that only
went from 1 to 12 and whenever somehow
set itself to some other number and
would increase you know in the correct
order so that is all a formal
specification looks like and the reason
we talk about it here is like it
actually has found its way into industry
Amazon used for most US offical a
published a white paper on it in 2014
and what they did is they use formal
methods to verify some of their Amazon
web servers that they have so this is
pretty cool they basically went through
if you read this they verified s3 so
their court what are their core storage
layers and ten other core pieces of
infrastructure they found two serious
bugs and then they increase their
confidence to make optimizations so
finding two serious bugs doesn't really
seem like maybe that was a lot of effort
it took a ton of time but what they talk
about in this technical report is that
the bugs they found would have never
been found otherwise they would have
only ever been found in production when
a catastrophic failure had happened so
that's pretty valuable because they've
just basically prevented two incidents
from potentially happening the other big
win they found is that increase their
comments to make these really aggressive
performance optimizations because
they still had a proof to say that this
was correct so that was really valuable
because they the developers weren't
afraid to go and try these performance
optimizations because they still had
something that said hey this system is
still provably correct you're not going
to introduce some bug that's gonna come
cause data loss or something else and
why I think Amazon invested the time and
they talked about how they were gonna
continue to invest the time doing this
and later bigger infrastructure projects
is because like this is how they make
money right legs they're selling you
something to deal with databases and
reliability and web services and so it's
like very core to their business model
to prove that it's correct and so and I
also think like even if you don't ever
use write a formal specification for a
real application I think learning about
it and thinking about it is really great
because it forces you to think in a very
rigorous way and so Leslie Lamport has
this quote where he says it's a good
idea to understand a system before
building it like that's why we have
design Docs and things but it's also a
good idea to write a specification
before implementing it because it makes
you think so rigorously about the system
that you're implementing all the edge
cases and failure cases that you're more
likely to just sort of find bugs just by
thinking through the problem and the
Amazon paper also reinforces this idea
they said a lot of the bugs were found
just by writing the system without even
having to run the model checker or the
execution piece the other big complaint
you'll sort of hear about formal methods
is that they deal with models of systems
not systems themselves so if you write
something in ela plus that's great and
you've it's provably correct but now you
have to go implement it in your language
of choice like say drop so that kind of
sucks because now you still have to go
and turn the specification into code and
obviously there can be bugs caused
through lack of transit or things are
lost in translation or people just make
mistakes or or whatever so you still
have to sort of test that system as well
the other specification language COK
actually will generate code but it can
only generate a subset of oh camel
because it's not you know we can prove
that things halt and so therefore it's
like not a turing-complete language but
what's super neat is you can actually do
quite a few things with writing
something in Cocke and then generating
the code so at a an academic conference
called poeple
which is about programming languages in
2016 there are two papers published one
is about formal verification where they
wrote the raft consensus algorithm and
they wrote it in Cocke and they prove
that it's you know correct and then they
generated the code and they ran it and
it worked so that's kind of cool and
then another one they actually wrote a
key value store this came out in 2016 as
well they wrote a key value store in Cox
Oh approval ii correct key value store
and then they you know generated the oh
camel and you can actually run it on a
real network and make requests and it's
a provably correct data store now is it
the most performant data store no but
like this is kind of hopeful that maybe
we have some tools that can help us
generate more correct systems for these
really hard distributed systems problems
and it's also a new area of research to
say hey if we can generate code that is
useful from formal specifications we can
also find ways to optimize that code
right that's all compilers do so this is
like an extension of things that I hope
we get to in the future
but we don't have this necessarily today
so let's go on to something maybe a
little more practical about distributed
systems testing in the wild as I like to
call it or we're going for like seems
pretty legit and like ship it uh-huh
so so the basic one of the basic ones
that we'll go through is unit testing
and I think everyone sort of knows about
this but the reason I bring it up in
this talk is I think it's incredibly
powerful and we'll go through some
research about why it's that powerful
and like that's proved it's even super
powerful in distributed systems
so just for clarity when I define it
unit test I mean something that can be
ran locally with your CI system its
environment
there's no environment it just needs the
CPU useful for basic functionality and
testing of error cases okay so this is
my favorite paper from 2014 it's called
simple testing and prevent most critical
failures so this is another nice
partnership between academia and
industry where a bunch of academics said
hey we want to understand the
distributed systems open-source
community better and they went and they
analyzed a bunch of dish 5 systems in
the distributed system open source space
they analyzed Cassandra HBase HDFS
MapReduce and Redis so these were all
open source with open source bug
trackers and they found they basically
took like 198 randomly sampled bugs from
these systems and then they took them in
and they reproduce them and they figure
it out like what caused it did some root
cause analysis and then and we're
looking at ways of how can we help
people better prevent these things from
happening in the first place when
they're originally writing the code or
designing the system one of the key
findings they found is that
seventy-seven percent of production
failures can be reproduced by a unit
test so this was pretty surprising to me
it's really cool too because it
basically means that it doesn't mean you
have to be like omniscient and like
maybe necessarily guess all of the unit
tests you need to write upfront but what
it does mean is when you find a really
critical bug you can most of the time
like three-quarters of the time write a
unit test and add this into your CI
suite to prevent it from ever happening
again so that's really hopeful because
it means you won't have these
regressions like once you solve
something it will just like stay fixed
which is nice and then they also found
that testing the error handling code
could have prevented 50% 58% of
catastrophic failures so a catastrophic
failure is something we're like livelock
in a distributed system data loss things
like that so basically what they're
saying here is that we need to spend
more time as engineers testing our error
handling cases we're actually really
good at testing the golden path they're
like though like this is what it's
supposed to happen happen right like
when everything's going fine what we're
not so great at is we sort of you know
say oh it's hard to test error handling
cases and so we don't spend a ton of
time on them but that's where all the
bugs and distributed systems lie or the
majority of them the other 35% of
catastrophic failures around error
handling so basically a lot of this is
also just what I call dev laziness the
error handling code is simply empty or
contains a log statement so I guess it's
in the log I guess that's okay
error handlers abort a cluster on an
overall a general exception so in Java
this is like when you say a catch
exception all it just like keep going
because it's kind of annoying to like
catch each one but like right this goes
back to code cleanliness and maybe
spending a little extra time there
because it is valuable
my other favorite one is error handlers
contain comments like fix me or to do
and then just like don't do anything so
it just swallows the error and then your
system keeps going so that's that's
problematic so what we should take away
from this this piece of research is that
we really need to test our error
handling code we can use things like
coverage tools to help us do that I use
this with my teams where we actually go
in and have code coverage that runs and
I'll go and sort of look on code reviews
about what error handling cases did I
miss when I said before I submit a code
review and make sure I have tests that
explicitly execute most of those because
that's where the bugs are probably gonna
be okay we're gonna have a brief
interlude and a quick rant efest oh I
like typed languages I actually much I
prefer writing in them away over untyped
in dynamic languages but types are not
testing so if you come to me and say oh
well I have a typed language I don't
need to write test cases I'm probably
gonna get really mad at you and so I
sort of want to tell you this like story
just to illustrate the point where I was
working with a younger developer and he
really really liked type languages and
type languages are super cool so I was
like yes they can help me do a lot of
really great things this is an active
area of research so they're gonna help
us do even more things in the future but
he was like you know I don't really need
to write test cases he submitted this
code review where he's like I don't need
to write test cases because the method
signature prevents this bug from ever
happening and I'm like okay so I keep
going through and I'm reviewing his code
and I find an infinite recursion bug I
think and so I'm like going through and
so I leave a nice comment I'm like hey I
think there's maybe an infinite
recursion bug here and he's like oh my
god you're totally right like and I'm
like maybe let's add a test case at
least proves that your code halts after
like so many executions and doesn't blow
up our stack and cause an outage because
like the JVM got really sad so you know
caution types can help you but they
obviously don't encode all the semantics
like this super dumb example does here
where the method is called add and I'm
actually multiplying them a simple unit
test case can help you go a long way
also TCP doesn't care at all about your
type system so like once you go over the
wire into distributed systems like all
bets are off like the types are not
going to help you there so we basically
have to go to our next topic which is
integration testing so integration
testing is basically the idea that we're
just going to test the integration of
all our models and verify combined
functionality this is super valuable
because you're usually just testing a
lot of boundaries between your
organization's as well
because that's sort of how the work gets
broken up and how things got specified
so when I was working on Halo 4 and we
were writing the Cystic service we stood
up the new one we were all proud of it
we were like we weren't in production
yet but we're stood up the new one we're
all proud of it we started integrating
it with a day-to-day like game testing
and play testing that went on and I
noticed this really weird thing where
the game was sending us everyone's stats
at the end of the game which is a 15k
blob so it's pretty big and pretty
expensive to process that three times
and I'm like why are you sending it to
me three times a hundred cent don't want
it three times I would like it once when
I tell you I finished processing it and
like don't send it to me again
and so this sparked like I think a three
day like battle flash like comedy of
error thing going on between me and the
main networking down where I'm like look
like we're setting you an HDTV hundred
back and he's like no like you're
sending us like a failure back and so
finally I'm like went spelunking through
the old codebase to figure out what's
happened and I found that the old
codebase happened to put something in
the payload of the response which was
done in all capital letters exclamation
point and so I was like wow if that's it
I'm gonna be really mad but I put it in
and then the game stopped sending us the
requests back every single time and I
was like cool okay so the spec was not
following HB 200 and we didn't fix that
in the original game but this is sort of
where things break down like the old the
game engine and the old services we're
looking for the word done in all caps
exclamation point to say like hey you
process it correctly and the and this is
not that weird of a story if you work
with a ton of different clients like
sometimes they don't support HTTP the
xbox360 didn't even have a formal HTTP
library until I don't even know like
2011 and then we weren't allowed to use
it cuz the memory constraints because
you don't get anything in games you just
like it's all graphics gets all of the
memory but like this was the thing so we
we ended up fixing it but like the
integration testing prevented us from
catching the helped us catch this early
and not cause additional load at launch
that would have certainly taken down the
system the other super interesting thing
I think that came from simple testing
can prevent most critical failures is
that they found that three nodes or less
could reproduce 90% of cache of failures
in these distributed systems remember
they looked at HBase and Cassandra and
HDFS and a bunch of
couple other in Redis and one more than
I'm forgetting off the top my head this
changed my perspective dramatically
because I was sort of under the
impression that like yeah there's no
data like broad data which is still true
but you also like in order to produce
these catastrophic failures you had to
have these really robust staging
environments to not just do the testing
in production and that was that's super
expensive right to like stand up you
know multiple hundred no clusters in
staging and in production and have them
running to do testing this changed my
perspective because now all we have to
have is a three node cluster I can run a
three node cluster on my laptop so I can
do really robust testing do a ton of
error handling and things like that we
can work these into integration tests we
can work the business in two different
release cycles because three nodes is
very very doable to test things on and
will prevent a lot of failures another
type of testing that I think is less
well known but as super awesome is
called property based testing so this is
sort of borrowing ideas from the model
checker space so model checkers remember
are this idea that we exhaustively list
all the inputs and then we sort of go
through them in some order and then once
they all pass were like cool our code is
good to go
the problem is that is it's super
expensive and takes forever to run and
as you have these really large systems
that you're running with CI and
continuous deployment and micro services
like this becomes untenable but what
property based testing does is it takes
this idea of doing random inputs and
it'll run them for a certain amount like
you can specify I want around 100 Hamid
Nations a thousand whatever and then it
will return you results that are
incorrect and it'll hopefully shrinks
some of the better ones will actually
shrink down even very large sets of
input like I took a thousand inputs to
get it to that the bad behavior it'll
shrink it down to the ones that actually
matter so this is neat this was invented
by John Hughes the original one is
called quick check it's written for
Haskell and Erlang so the idea here is
you define a specification instead of a
test case I will show you what one looks
like in Scala
its dot it basically it's code it looks
like writing a test case it's not even
as scary as formal specifications and
then you just sort of say you know the
tool will generate a bunch of inputs and
then shrink the state space down and
then we'll send you a bug reporter it'll
say like hey they all passed
so quick trick is super great because
they've found a ton of really really
interesting bugs in distributed systems
like react and bash bash o has a no
sequel database called react which they
use quick check on heavily heavily to
model their eventually consistent data
store and found a bunch of bugs John
Hughes went and ran quick check on Volvo
cars and found a bunch of bugs and
April's of 2015 and then in February of
2016 he ran it and found some actually
pretty big bugs inside of Dropbox so
this does take time but once again these
are bugs that are difficult to find and
often result in data loss or really bad
things happening to your users so that's
what we want to prevent um
the JVM has Scala check so that
obviously works with Scala and Java it's
super friendly and then if you use any
other language there's like a whole page
on Wikipedia that's linked in my
references section there's basically an
implementation in almost any language
you want to write in because people find
this super useful so here's a super
simple Scala check example the first one
is basically just super dumb to show you
the syntax it's saying we want to make
sure that the number we put in is always
between 0 and 100 so I define a property
which is for all small integer values
and must be you know bigger than or
equal to 0 and less than or equal to 100
if I find a case based on this generator
that only generates number from 0 to 100
then it'll you know blow up so if you
wanted this test to fail you could do 0
to like 101 and it'll find it or 0 to
whatever a more interesting example is
the one below where it is testing my
implementation of reverse on a list of
strings so Scala check will generate a
bunch of different lists of strings of
varying sizes and it will put this into
my function and I'll do the reverse of
the reversible list should equal the
list itself right so this is super nice
cuz it'll catch a ton of edge cases
right just think of all the things we're
like you know different sizes different
character sets all that kind of stuff so
it'll hopefully find them if you want
run this for a while if you have any
bugs in your code
I find this super useful I use property
based check testing with one of my teams
and even just personally it helps me
think more rigorously through a problem
right this is getting closer to the
propositional logic of formal
specifications and so you'll find stuff
just having to think through
do you have question so the question was
is why is property-based testing
different than a unit test so why it's
different is because Scala check is a
system that will run under the hood and
it'll actually generate like maybe a
thousand or ten thousand or however many
you specify instances of a list to do
the reverse of the reverse a unit test
you would have to like I mean maybe
you're already doing this you can
definitely do this on your own or you
generate a list then you run the same
unit test a hundred times um if it's
randomly generating a list this is just
a helper to do that so it makes it a lot
easier so that make sense so these
frameworks help you do this testing and
it literally once you start doing this
it's about like as much effort as
writing in a unit test but you explore
way more of the state space that gives
you more confidence that it's doing the
right thing okay and then find one of
the other big ones that is different for
de tributo systems testing is you have
to do fault injection testing I believe
this is super core
if you don't explicitly force your
system to fail it's unreasonable to have
any confidence that it will operate
correctly in failure modes so this has
become more popular Netflix obviously
has the simian army which they've been
doing for a long time so if chaos monkey
monkey which will kill an instance
that's the injection of a type a fault
which is a machine itself dies there's
latency monkey which will inject latency
into the network so that's a different
type of fault injection adding some
asynchrony to the network and then chaos
group gorilla takes down an entire data
center so that's another type of fault
injection test and they've obviously
built a lot of infrastructure and stuff
around this to support it another really
interesting version of fault injection
testing is this tool called Jepsen it's
open source Kyle Kingsbury wrote it and
actually like now as a business where
you can pay him to Jepsen your systems
he also writes this really great blog
post where he'll basically take a
distributed system and then he'll run it
through Jepsen and what Jepsen does is
it spins up local clusters on this
really beefy machine he has and then you
know injects faults like creates Network
partitions on the machine so that
certain instances of different nodes
can't talk to each other and then I'll
sort of verify that the claims that were
specified either in their marketing
documentation or whatever or held and so
what Kyle started finding when he first
started running this
that most of our distributed systems
kind of look like this and we pretend
everything is fine but the good news is
is like since he started doing this I
think it's really increased the notion
that we need to start a testing our
dependencies and be really like being a
little skeptical about the claims of
some of the open source software out
there because it's hard to write these
systems and then he'll also you know
file bugs and there'll be a lot of times
they get fixed MongoDB is one where he's
tested that a lot it's added to a lot of
fixes he's found a bug and you know
Kafka go oh that was pretty bad that got
fixed he's found all these bugs so he
has this great blog if you want to read
them and it also is a nice introduction
to sort of like all the different
consistency models and what's going on
and the open source ecosystem so I just
want to pause here and say you know once
again even if we implement all these
techniques that I just talked about in
your system and you're like all of the
tests pass even if a test database
passes at Jepson tests it doesn't mean
that there aren't any bugs it just means
that we haven't found one so we have
more confidence that our system is doing
the correct thing so visually what this
looks like is we started with our state
space and maybe we're exploring some of
it using unit tests we do some
integration tests and that's going to
explore a larger chunk of our state
space it may overlap with some of the
unit tests so we're doing some duplicate
work property based tests we'll cover
another portion of our state space and
give us more confidence since we're
exploring new areas and then fault
injection testing can uncover in
different space but you can sort of see
that there are gaps we haven't covered
everything and so this is where bugs
could still lie but we have more
confidence because you know the
probability of us hitting a space that
we haven't tested is lower another great
way to test your systems and I like
thinking that of this is testing your
DevOps or testing your your processes
with people is this idea of a game day
and this is just the operational part of
running a distributed system and
providing a service is just as critical
as writing the code so game days were
proposed by Jesse Robbins at Amazon and
in the 2000s he had this title of Master
of disaster which I thought was pretty
cool and so so what he would do is he'd
start saying you know tell engineers and
three to four months there's going to be
a major outage he would just tell people
like send out this email and then
and then you know people would sort of
forget about it and they'd try and shore
up their systems and things like that
and then you know someday down the line
they'd actually take out a data center
or mimic some really large failure like
one home you actually mimic a fire in
the data center and what results in
happening is a significant amount of
capacity is eliminated from a single
data center when they would do this and
so then they would watch and see what
would happen to their services
gamings are super critical because they
often uncover hidden dependencies for
instance they ran one where they took
out the style paolo data center and that
affected a Mexico instance of a day or
an instance of service for me in Mexico
so that was weird
but then they realized there was like a
hidden link that they didn't know about
and then they'll find single points of
failure like I think one they found
where they took out a data center and
that was the only one that happened to
be running the paging service so there
was like a major outage and no one got
paged which is bad so it sort of helps
you find this in a controlled
environment where you know the outages
is happening other than like you know if
you take out like something bad happened
in the data center all the paging went
out was out and no one got notified like
how long would it take you to figure
that out so if you want to run these in
your organization you can basically
notify your engineering teams that a
failure is coming so you're gonna do a
fault injection test of some magnitude
you will then go and induce the failure
like this is super critical to like tell
people you're doing this because they
will super hate you if you don't you're
gonna induce the failure and then you're
going to monitor the systems under test
so you basically have like a war room or
whatever of people like looking at the
systems and being super hawkish because
you are testing in production this has
the ability to impact your users but
it's better to kind of do it now than it
4:00 a.m. when people are asleep you
also should have a teen that is
observing only that's basically
observing the team monitoring the system
in their coveri process because when an
instant happens and like shit hits the
fan and even like something like it's a
planned incident it's still really
stressful and it's really hard to like
be objective about how the processes you
have in place to recover are working and
like all those kind of communications
things so you have another team that
isn't involved with bringing the system
back up that's just watching and taking
notes and seeing what happened and then
they're gonna go file bugs because it
might be that like hey like we should
stand up the paging service in another
day
or it might be something like hey there
was an outage here and these two teams
didn't know who was responsible for like
the service they dependent on there was
no way to get in contact with one
another and so that's a breakdown in
communication or it could just be like
hey we found a really gnarly bug in one
of our distributed services and we need
to go prioritize fixing that right now a
really good example that was sort of
low-cost because I like to show the the
spectrum of fault injection testing and
these game days they don't have to be
like you have to have chaos monkey and
things running they can be as simple as
running kill9 on a node in your data
center so stripe ran one this was back
in 2014 but I like this example still
because it produced a really interesting
result so they plan to do this and so
they test and they wrote a blog post
about it and basically what they did is
they ran kill9 on their primary node in
their Redis cluster and when it came
back up it lost all of the data in the
cluster which is kind of cool and
terrifying luckily they had backed up
all the data in case something it bad
had happened so they could restore and
did the responsible thing but what ended
up happening is basically and they used
some configuration setting that the
author of Redis basically said he didn't
expect people were gonna use it this way
and so it was good that they tested it
because when you start doing performance
hacks or like using things that are off
the mainline and especially an open
source software it's probably not tested
super well so what actually happened if
you're interesting from a disturbance
perspective is the primary node went
down
it hadn't persisted its state to disk
because of this config setting they used
they were using something where they
were flushing periodically for
performance and then when it came back
up it still thought it was the primary
node even the leader election had
happened and there was like another
primary runni in the system and so it
took the fact that it had no data on its
system and it's like oh this is what
everyone should have sir like sent the
messages to everyone else in the cluster
and said you all should have no data -
so all the all the secondary nodes were
like cool and they deleted all their
data so that's bad but luckily they
caught it they didn't have a huge
incident where they weren't prepared and
didn't have a backup of the data and so
like kudos to them for testing this and
it's really just as simple as running
kill9 on it no
you could do this an integration testing
where you have three nodes running on
your machine maybe in a docker cluster
or whatever and you just kill one of
them and see what happens this is super
super simple and low-cost and you don't
even have to do it in production so I'm
going to Segway onto some thoughts on
testing and production I think we talked
about testing in production a lot and
it's a little bit cowboy and it kind of
drives me a little bit crazy because it
totally works if you have the
infrastructure to support it like
Netflix or something like that if you do
not have that large scale of
infrastructure and you don't have a
baseline idea of where your reliability
and fault tolerance in your systems are
you were essentially testing on your
customers and that is bad there is
always risk about testing and production
so when you run these tests where you
say hey we're gonna kill a node in
production and we haven't done the due
diligence maybe of killing it in staging
first because that stripe bug could have
been reproduced ageyn we are risking our
customers experience on our product so
that's kind of scary I think so there's
always this tension it's obviously more
work and more cost to set it up in a
staging environment and you do want to
test in production right you do want to
force your production systems to fail to
ensure that the right things are
happening but do some due diligence
first and do the proper of not it's not
like a the only thing you should be
doing you should be doing some of these
other strategies that we talked about
finally a monitoring isn't testing
people say this to me all the time
so I was like the tech lead of
observability at Twitter and they're
like Oh margarines testing and I'm like
not really all it's doing is telling you
at your system that you chose implement
metrics on is currently doing it's super
crucial you 110 need monitoring to run a
system and do operations and effectively
run a distributed system in to check
errors and respond to things but like if
you just have monitoring that is not a
form of testing that's only reactive
operations it just helps you know that
something bad is happening it doesn't
prove or it doesn't give you confidence
in anything good is happening another
one that's like testing in production is
Canaries I think this is a super great
practice and you should hundred percent
use this if you can but once again it's
testing in production so do your due
diligence but first to have confidence
at that build you're gonna roll out is
not doing something weird
um so the idea of canary is if you if
you haven't heard of it is or it's
sometimes called like green blue builds
or things like that so you're just gonna
roll out a new build into production on
a small number of nodes usually start at
one and then sort of scale up if you
have really great infrastructure to
support this like maybe it'll just Auto
detect something's wrong and roll it
back if you don't you can have someone
like manually watch it and then look at
certain key metrics and then make a
decision on whether we should increase
the canary population in the cluster
until it overtakes the whole one this is
nice and then like if the canary went
wrong you could roll back this is nice
because it gives you some confidence as
you're rolling out maybe even to a
really large cluster that the right
thing is happening as you see your
metrics sort of not tanked and like
people are having a bad experience and
it also reduces the risk around
deployment that you catch something
faster but you still are testing in
production um I sort of also like this
visual to a serve explain all the
different techniques we've just talked
about so there's this idea and Ynez
Sombra has a really great talk that's
linked to in the reference section about
how you're building resilient systems
and so this is sort of an idea of how we
classify the faults that we find in
resilient systems there's like the
probability of the failure and sort of
the rank of how Bad's it is when it
happens so if you think of this area
under the curve as like all the bugs in
the system so there's a bunch that
classical engineering can catch there's
a bunch that you can catch with reactive
ops and then there's this unknown
unknown so this is the thing that like
there were always going to be unknown
unknowns in the system there are too
many things to enumerate and so these
are the catastrophic failures that
happens where you lose where you have a
really bad outage because you just no
one could think about what had happened
and then you learn from these right but
these will always be in the system so I
like to think of it like this if you use
unit and integration test that's a
normal classical engineering if you use
property based testing we can sort of
extend the region of classical
engineering because we're exploring more
the state space was sort of a simple
technique fault injection testing will
also extend this the space of bugs we
can catch with classical engineering and
then you add things like Canaries
gamedays monitoring and while this isn't
catching things you know before they go
to production we are catching things in
production and having a
increasing that space of like that we
can react to and we're reducing the
unknown unknowns here and so that's the
really critical part because we just
want to keep reducing that space because
those are the things we don't know that
are gonna happen those are the things
that are going to take down our system
and cause major outages so so this is
sort of like a nice handy graph when
you're trying to like make a selling
point of why you should do these
different things is because you
significantly get ticket to a decrease
the amount of ops work you have to do
and be decreasing amount of potentially
really bad things that could happen okay
so I want to talk a little bit about
some research on verifying distributed
systems that's ongoing that I think it's
super cool i Malay talked about one of
these just for time but there's links on
all of them in the references section if
you this sort of like tickles your fancy
I'm gonna talk about cuz I'm strong
enough this is a super cool paper that
also came out of pop was 2016 I really
like puffle 2016 if you can tell from
this talk there was a lot of really
interesting I think almost kind of
practical research from a programming
language comprehend times there isn't
from an industry perspective and so what
this tool that's called sizee or because
I'm strong enough did is it's the
premise is it's really hard for
developers to reason about what type of
consistency choices you should make in a
distributed system right because
obviously you want to choose weaker
forms of consistency because then you
have higher availability but sometimes
you need a really strong form of
consistency to enforce a certain
invariant that's like a business
decision like right like I must not lose
this data or like X has to happen before
Y or like finance there's like all these
different things we'll go through a real
example and so what this tool does is
it's trying to help users understand and
give them a tool to reason about how do
you know that you've picked a strong
enough consistency model and how do you
only use the minimal amount of
consistency right like when you're
designing your application because you
don't need super strong consistency for
every operation you might only need it
for one so what they did with size is
there's like this example like say we're
going to design a bank application a
super simple one and it's distributed so
the invariant that we must always
preserve is that a bank account must
always be greater than zero you want to
be able to deposit money into this
account and then you want to be able to
withdraw money from this account and
these can happen at any node on the
system right so size
works like this it's basically so this
is Java and all of these annotations are
sizing so what you do is you write your
code and Java you define your classes
and then you're going to define your
invariants using sizee annotation so
this top part says basically the balance
always has to be greater than zero this
is an invariant that must always hold
this box second part basically says what
are the invariants for the deposit
accounts so we're basically saying the
the amount you input must always be
greater than zero so it has to be a
positive number greater than or equal to
zero
the balance what it's going to do is say
the balance equals the balance plus the
amount and so then if you just ran
size.you without the debit part it would
be like cool you don't need any kind of
strong consistency here because a
positive number added to a zero or a
positive number is always a positive
number or zero so this doesn't violate
our invariant that our balance has to be
greater than or equal to zero at all
time so I can like literally add the
money at any node without having to do
some sort of like round of consensus in
between the final bit is on debit right
so deb it's a little different because
we have the amount that we added and the
balance and we're still saying the
amount has to be greater than or equal
to zero so sister you don't you like
deal with negative numbers and the
balance is now gonna be the balance
minus the amount so in order to force
the invariant that our balance is always
greater than zero we cannot perform this
operation if you didn't have there's a
there's a token in here there's
basically a consistency token in this
correct one but if you didn't have that
in there and you ran sizing it would
throw an error so it basically tell you
your design says you have to have strong
consistency here because I cannot
guarantee that the balance is always
greater than zero unless you talk to all
nodes get consensus on what the balance
actually is and then decide if you can
debit the amount so this is a nice way
so the thing that's cool about sizing is
that it basically is Java annotation
that sits on top of your code it doesn't
do anything to help you implement that
code but having the design next to the
code makes it easier for developers to
reason about and then you also sort of
have docs that live right with your code
so I think it's a little it's a little
nicer
this is actually open source you can go
play with it there's not a ton of
documentation around it but the
researchers are I think fairly
interested in hearing from folks so if
you want to try using it go ahead so
this is just Java JVM it's really neat I
think this is a really great example of
where we should be working towards in
the future where we start writing tools
that incorporate ideas from formal
verification into design and languages
that they can help us detect when we're
running into problems and we're going to
make bugs because it's just a lot to
think about there's just so many
different variables going on in your
head because that state space of what
the possible inputs are is huge ok so in
conclusion you know if you have
something like a really critical
component if you're an infrastructure
company maybe you want to investigate
using formal verification on those unit
tests and integration tests find a
multitude of errors and they're sort of
the baseline like you should 100% have
them I really don't feel like there's an
excuse for not having them and then you
can increase your confidence via
property testing and fault injection
testing in your systems these yield very
very high rewards and you can sort of
they're on a sliding scale of effort of
how much you want to invest into them
because I'm totally realistic about you
know not everyone can go build a Netflix
like infrastructure so I'm gonna close
with a quote from my friend Camille
Fournier enjoy the ride have fun and
test your freaking code thank you to all
of these lovely people who helps me with
the talk and the original article and
now we have some time for questions this
is the link to resources and I'll tweet
out the slides and the resources link
afterwards as well thanks
yes
yes so if I if I understand your
question correctly you're sort of asking
like if I'm building the infrastructure
and not using it how do I sort of test
that and really increase the confidence
that I'm putting out a good system for
people to use so I would recommend going
reading a little bit more of that simple
testing to prevent critical failures
paper because there's actually a really
fun of great info there but just to sort
of summarize like one of the big things
I didn't talk about in this but they
talked about in that paper a lot is a
lot of bugs like 25% of them come from
config changes and so it's like a place
where we don't do a great job of testing
our code and it's also because of like
the more configure options you add
obviously that increases the state space
that the system could be running in and
so limiting the amount of configures
options to what you can really test so
this becomes you know obviously it's
once again a balanced task between your
customers are asking for this and you
want to like provided to them but then
you're aren't exactly sure how that
interacts with another feature so being
really diligent and having especially
when you're starting to do something
like shipping sort of an MVP or a very
very smaller subset will produce a more
correct system the other thing that I
like to is write use a code coverage
tool the result of that papers they
basically make a code coverage tool for
Java that you can run I mean it's a tool
for Java that will run and detect code
coverage misses or it'll just tell you
that like hey we found these weird
things or you have an error case and you
don't do anything you don't really
handle the error and it'll sort of flag
all those and they did find a bunch of
bugs and stuff and report them to open
source communities but you can enforce
this with code review and basically use
a code coverage tool which is kind of
might be easier and like a little easier
to get set up and whatever custom thing
they implemented that may or may not
work how's the research project and then
like how mean right so they if you turn
around where the numbers off the top my
head they actually I mean they found a
fair number of really like good bugs
like the the false positive number was
acceptable basically because what they
did as they ran it on the 5 and then
they increased it to like 9 or 10 and so
they have more stats on there but they
were basically saying it was worth it
and most of the requests
that they submitted to these open-source
projects were greeted with like yeah
that's a bug we should probably fix it
and a lot of them were fixed yeah I mean
it depends on the language obviously so
I've been writing some go so I just I
mean like girl has one that's like built
in so I just have it set up and running
with Travis CI I don't like have
anything particular was anything super
magic about a code coverage tool other
than like it's easy to work with your
infrastructure tip so it's very company
dependent I like what can you run
consistently and give feedback to users
and maybe gate check-ins on or at least
expose it during the code review process
that like hey this is going to drop code
I'm not a super stickler like it has to
be 90 percent or whatever it's more of a
tool for you to make a judgment call to
say this is gonna drop coverage is that
okay hey look all the coverage places
that you don't have our error handling
cases so easiest is best in my opinion
yes
do your fault injection testing at
Twitter um so we we don't have anything
like super we don't anything like super
like robust as of Netflix but we do do
fault injection testing like so a like
nodes in our data centers died so it
just happens by you know sort of running
the thing but then also like on the
teams I've been a part of we definitely
will test in staging to ensure that if
we're implementing a new algorithm that
like it can handle that and then if a
node goes down like the right the
scheduler remix up the right thing
happens all those kinds of things so
it's more ad hoc I would say then at
Netflix where it's an infrastructure
that automates it for you but teams do
do it yes
yeah so to repeat the question the
question is like it's easier to do to
justify fault injection testing and
production if it's something that like
you can gracefully degrade away from but
it's harder to do for something like
yeah like Kelly as you keep her note is
really scary I don't know like many
places that like do that regularly it
usually results in a lot of outages when
it does happen it's a risk reward
trade-off right do the especially if
you're going to implement something
custom you know do it in staging first
maybe if it's gonna be something really
hard like like where you're gonna you
know take you only you don't have a ton
of extra capacity so that's a problem
but then like if it does happen like
you're kind of screwed anyway so it it
sort of is this you know thing where I'd
like us to do it more and more because
it means we're doing capacity planning
more correctly because we have to like
account for like we're gonna fail this
much and fault injection testing doesn't
mean I have to survive every fault it
means I say I'm gonna survive these
faults like these are the list of faults
I'm saying we're gonna survive look she
doesn't mean I'm gonna survive multiple
like maybe like for like your system it
might not make sense to say we're gonna
survive to Amazon data centers going
down or whatever right like that's a
business decision about how much money
you want to spend to like make your
system more reliable and how many
backups and stuff you want to have it's
the same thing where we replicate data
we replicate in triplicate usually
because that's good enough like the
probability of two things failing and
then like the third one failing before
we can replicate it is fine but like if
you really cared about a data that you
might replicate it in like sets of five
and things like that so I don't have a
good answer I can't this is a cop-out on
that question it's sort of you have to
talk to your business needs cool all
right thank you so much I'll be around
for a little bit of human effort</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>