<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Serverless datastream processing by Martin Görner | Coder Coacher - Coaching Coders</title><meta content="Serverless datastream processing by Martin Görner - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Serverless datastream processing by Martin Görner</b></h2><h5 class="post__date">2017-11-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dMMnKCtYwdo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello so welcome to this tool selection
section session I will try to show as
little slide as possible and to make it
all a demo which will of course fail
because this is a conference so I hope
you don't mind today I want to piece
together an entire application that does
streaming data at a high rate but I
don't want to be bothering with
launching machines or monitoring them at
work or whatever I want to do everything
in a server less environment so I will
be using various components for Google
cloud from Google cloud platform to run
this the business case imagine you have
a taxi business and you get you got
these little panels installed in the in
the backs of your taxis they are useful
for your customers they display where
the taxi is so they can follow follow
the ride and from time to time you allow
yourself to display little add because
monetization is good but you need an
infrastructure to handle all this data
your taxis they are equipped with a
connected GPS receiver they send you all
their data points in real time let's say
every two seconds you receive from your
taxis the position the number of
passengers the the taxi meter and also
information about the ads that are
displayed and if the user has clicked
clicked on them in one of your customers
calls and says I'm not seeing my ads
what's going on and you call your data
scientist to investigate so how can he
do that this data stream first is pushed
into pub sub pub sub is the first
element here and let me make this
slightly bigger
so I'm showing you rather than showing
your slides I'm showing you what this
looks like in the Google cloud platform
console pops up is a messaging service
fully managed messaging service that you
can write messages to it and you write
messages into topics and then clients
can subscribe to topics and pull the
messages out of it so it does queuing
for you basically it will accumulate
backlog if they're if you are not
pulling in the messages fast enough but
also it does fan out
you can have one topic and have multiple
subscribers as well as fan in you can
have one topic and have multiple writers
into that topic here all my taxis are
sending the data directly into pub/sub
as soon as they have an HTTP correction
it's REST API boom they send the data
into pub/sub so I have my data in pop
zone how do I D bug it it's a data
stream it's live data it's coming in
real-time
I need something to visualize it so I
will use Google Maps for that but the
problem is that this is a high rate data
stream will be receiving something along
this the lines of 10000 events per
second and if I just throw this into a
maps visualization which is in my
browser while I make my little laptop
here a part of my big data system that's
not going to work so I need to aggregate
this stream into a format that is more
amenable to visualization for that I
will use cloud dataflow dataflow here so
for people who wear in the we just did a
lab on cloud dataflow cloud dataflow
rather than showing you the interface
let me show you the code because we are
developers we like seeing code without
reading this extensively dataflow
understands Java you are writing your
code in Java
and dataflow is is the successor after
10 years of our md2 MapReduce which was
published the MapReduce paper was
published in 204 we are 1213 years later
now but the idea is still the same you
if you if you write your algorithm in a
in a programming model that has these
map and reduce steps well it's slightly
different in data flow of slightly more
generic but it's the same idea
if you write your algorithm in that
programming model then dataflow can
paralyze this workload automatically can
automatically without you doing anything
send this workload spread it across 20
or 100 machines and what is new in data
flow compared to all the previous
incarnations of the same idea is that
it's not just one map and one reuse
operation it can be a comp NAR betrayed
any complex graph and the second thing
is that dataflow now does both batch and
stream processing so here I can do
stream processing and actually the code
you're seeing here is that this I'm
reading my stream from Bob sub and then
I do various operations to down sample
it which you can see here there is a
down sampling operation here what you
are seeing in this UI is the logical
view of the pipeline you wrote it's
still your Java we have optimized it and
send it across 100 different machines
and this is a non-trivial operations all
those machines are specialized they run
different parts of this pipeline but you
wouldn't recognize your algorithm if we
showed you that so in the console here
we are showing you the way the pipeline
the way you wrote it so that you
recognize it all right so now I have my
let's
see if I have a visualization okay what
I did in my pipeline I downsampled my
stream and at the end I'm writing it to
pub/sub again and I configured this
little JavaScript app to pull from this
pop subtopic pops up has a REST API I
can call it from JavaScript I pull it
and visualize it in a Google Maps API
heat map API and now I see taxes and now
I see the lights this is the business
owner that was that was complaining that
he was not seeing his ads in his taxes
and this is where the taxis are which
are carrying this ad well there are a
couple around his store but most of them
are in South Manhattan obviously there
has been some miss configuration so now
I see it okay I can configure it better
and the result will be something like
this the correct behavior that I expect
let me put this store back on here
that's correct so but then I think well
I have my my ad server which I configure
manually
my my salespeople they know where to
place these ads in the taxis so that
they are relevant to the businesses who
are advertising fine but it's it's
error-prone they can do a fat-finger
mistake configure it for a wrong area
that's not very good what is my first
line on the of defense well first I want
to log everything because here I spotted
a problem in real-time but I want to
have all of this information on disk
somewhere so that I can browse through
it to find where the problem was even
yesterday or the week before and we have
a tool for that which is called bigquery
and in bigquery let me make this bigger
bigquery is data warehousing solution I
would say but it's a
fully surveillance you just pour data
into it and the nice thing is that the
data that is inside is is not siloed
it's super easy to access actually
bigquery has an API which is just a box
in which you type a query and the let me
let me launch this query so that you see
that this is in real time and whatever
the size of the data in bigquery I have
a guarantee that bigquery will spin up
enough compute resources to get me my
answer on my data in a relatively
interactive time so here in nineteen
second it went through almost three
terabytes of data which are all the logs
saved from this application and whatever
what was I trying to see I was trying to
answer a business question for my
business I was trying to say well people
are configuring those ads by hand that's
good but wouldn't there be rules that I
can find for making this configuration
automatic so let's try to sift through
the data to find you know some insights
about my business and and and write
those rules and one hunch I had was that
for my store maybe a lot of customers
are coming from airports and maybe I
should do a campaign specifically for
travelers coming from airports so I
wanted to see how many people drive in a
taxi past my store coming from an
airport the the sequel query is well
non-trivial and I run it on my entire
saved data set of logs from taxes and I
find that it's 8% well maybe it's worth
it maybe it's not that's what I approach
I can have let me show you the code of
how I save this data into bigquery I do
that in my data flow pipeline so it's
right here you see I read from pops up
on this side of the pipeline I down
sample and then blah blah blah I do
something else but immediately and
that's a really a best practice whatever
you do whatever processing you do in
data flow have a an operation that just
saves the entire stream to disk in
bigquery bigquery can ingest up to
100,000 rows per second so that should
be okay here I have what 10,000 elements
per second yeah and and this is a very
simple way in which I can persist my
data and still have it live and
accessible in the code itself this is
the code that does it data stream that
apply
bigquery IO right and I say that I
create the table if it's not there yet I
specify the schema my schema is up there
this is it just adjust each field and
and and and what type it was what else
do I say append I say I'm appending and
I say to which table I am writing here
and that's it let me show you a little
other demo in bigquery I want to run
this one this is really just for fun I'm
running here a query which will retrieve
the latest data points that bigquery has
ingested and all of that is running
right now I'm not showing you any fake
slides or anything all of this is
running of course I'm not getting the
information from real taxes
unfortunately I have a machine that is
pumping this data into B into R into
pub/sub but the data is realistic
because it's a data set that is based on
historical data so if you want to know
what you will be seeing on screen is
replay of taxi traffic in New York from
the first week of January to 15 so my
query is almost finished what I'm
querying here is just the latest pieces
of data I want to know it's like a tail
command on on on a log file it's almost
there
59 seconds 60 mmm should be quicker but
so let's leave it running okay welcome
to my book we'll come back to this later
maybe it will take two minutes but I I
don't want to go that far so now I have
let me stop this and let me go back to
the full visualization oops so now I'm
visualizing my real business and I had
initially my ad server manually
configured then I thought well maybe I
should I should replace it with
something with rules so that I don't
have to manually configure it but now
he's thinking there is this new thing
machine learning wouldn't that be useful
what if instead of outputting my rules
myself I could take all my data and
train a model to predict the best ad to
show at any given location do I have the
information to do that well actually I
do because I have in my logs I have the
position of the car the time of the day
where it was coming from where it was
going and I have which ad was displayed
and I have one little additional
information if the user clicked on the
ad and when he when he clicks it gives
him additional information that's a user
interaction that's all I'm interested in
I know that what I showed to the user
proved useful for example he was going
to south Manhattan and while while
driving past Broadway I showed him that
by the way this is Broadway and this is
what is playing tonight and he said oh
yeah maybe you have tickets it was
useful in that in that situation so how
do I
do that and let me go back to my
architecture slide this one here so
initially I had my taxi data into
pub/sub I used data flow to visualize it
I persisted it into bigquery now I have
all this data in bigquery I would like
to train a model on it but first so this
will be the full architecture but first
data is usually messy and yep this one
we have a new tool in cloud platform
which is called data prep and then let
me make this bigger unsupported zoom
level come on and edit the recipe most
of the time of a data scientist is spent
cleaning data and even more is spent
finding out that data is not clean so
this news tool which is a collaboration
between a Google cloud platform and
trifecta is the trifecta tool deployed
on cloud platform what I did here is
point it at my bigquery data just
pointed at the tables the table has four
terabytes of data it doesn't care it
samples the data and shows me a view of
the data with these histograms showing
how much how the data is is how the data
looks and for instance here on passenger
accounts I have this histogram and it's
telling me a passenger count of 10 there
is a significant number of taxis
carrying 10 passengers no I don't think
this is right this is probably a
fat-finger mistake for one passenger I
don't think it's right let us fix it so
I will replace this with something else
it when I click on the column it
automatically suggests a couple of
things to do it suggested to replace
this value with no that's not exactly
what I want I think this was a
fat-finger mistake for one so you
replace it with one and I add this
my script this is just the UI it's an
interface that allows me to produce a
data cleaning script but but in a way in
an interactive way where you see the
data you have suggestions of on how to
fix the data and once my script here is
finished I will click run job and this
will run this data cleaning job on my
entire table produce a different table
and it will run it on data flow but
automatically I don't have to write the
data flow pipeline it will automatically
convert this script into a data flow
pipeline and apply it
what else can I change here well I have
pickup location and destination location
they are compound columns latitude comma
longitude I don't like that I don't like
that I would like a proper latitude and
longitude column so usually it suggests
- well I will take something else no I
don't want couch
I won't split okay split no I don't want
to buy come on
I clicked something wrong sorry I told
you that this would not work immediately
suggestions let's go back here
No where was the new one set all right
sorry I'm confused in my own demo I told
you that something would not go well but
well there is a split function where I
can split this column into on the comma
and have two proper columns I run this
and now I have my data that is clean I
can start training on this data so I
will use tensorflow to build my model
and ml engine to train the data ml
engine is has two functions here ml
engine is is again a service platform it
is a tensorflow a platform designed to
edit you to run tensorflow jobs so once
i've write my model i have run it on
tensorflow i run into an ml engine for
draining the the added value you I could
also spin up an instance on GCP add a
GPU to it and train it on that that
works perfectly well but usually as a
data scientist I like to think of it my
parameters a lot until the model is good
and ml engine has this very very very
useful hyper parameter tuning function
where you can define your model leave
some parameters open and tell him L
engine to try various combinations of
parameters to work to see what works
best so that's what I did I train my
model on ML engine let me show you what
ml engine looks like so for instance
from here I can go back to my console
did this one finish oh yes
so the the query I was trying to show
you previously finished in something
like two minutes it parsed five
terabytes of data and what I wanted to
show you is here the lag so that in my
query I'm outputting the difference
between when the data
retrieved and when the data was ingested
and it's still being ingested as we
speak this demo is running and you see
the lag is a sub second which I found
quite extraordinary
that's the lag between a piece of data
being ingested and a piece of data being
visible in a user query so back to my
cloud console here and let me go to ml
Engine so ml engine has training jobs
here I see all my training jobs but once
a job training job is done my 10 through
flow model will have saved the trained
parameters to disk and I want to show
you how I deploy a new model online in
this model I simply simply click on
create version and I go to browse and
here I go to the file where the trained
parameters of my model happines have
been saved I click deploy and I have my
model deployed online behind the rest
API on a fully server less autoscale
infrastructure all I have to do now is
send traffic to it so that's what I will
do from my data flow pipeline I will be
calling my model and asking for well
what the model predicts which is the
best ad to show at this point in time
space and so on so let me go back to my
data flow pipeline reading from pub/sub
saving to persist everything as it is
then down sampling for the visualization
and also because it's it's no good
calling this this model every 2 seconds
then on this side I perform at it for
the visualization on this side I down
sample a little bit more and I do my ml
engine call which is since it's just the
rest API it's in my data flow code in my
java code in my java code
I call this rest endpoint and now I
should be able to show you
what it looks like right here so let's
say that what I want to have I here so
here I'm showing I picked one taxi and
I'm showing you where did this taxi is
going how many passengers and so on and
it's carrying this ad and it has been
predicted because this passenger is
going to LaGuardia Airport it has one
pass one passenger so maybe the
executive wine bar is something that he
might find useful in now something else
this taxi same destination but three
people and for three people the
algorithm things that maybe a sushi bar
is is a little bit more useful and so
this is what it looks like for all taxis
where they are going there are some
going to South Manhattan so Manhattan
breakfast maybe something they like and
let me go back to the full architecture
and here we have the entire application
entire ad serving application built with
a well I think we can call it an IOT
front-end because those cars are sending
data from from the fields ingested in
pub/sub processed in data flow persisted
in bigquery then I train by model in in
ml engine I deployed in on on ml engine
and everything that is in production
here is a server less component and I
want you to show you this to show you
that you can build a full end-to-end
data stream real-time processing
application using only a real-time
components because I don't like managing
instances that's it thank you
and we have four minutes for questions
yes yes good question how did I train
the model so first let me show you what
model it was I think I have a slide oh
yes it was a model called widen deep why
can I train it because in my data I have
all the ads that have been served and I
have user interaction I have user
engagement data okay I have the fact
that someone clicked on an ad so I know
when an ad is roughly useful and then I
used this wide and deep model we it's
open source in tensorflow intensive flow
you can instantiate it in just one
function call actually I think I have
this function call somewhere here ml
model wide what is it what comes No yeah
here it is here a DNN linear combined
classifier so I'm not writing my own
model I just instantiated this one it's
I think you can call it the workhorse of
most enterprise ml problems it's the
model we use in the Play Store to
predict which applications you're going
to like and it's a jointly trained model
between what is called a wide model so
something that learns a lot of
combinations just point by point
combinations that someone who is into I
don't know wargames might like this
specific game because now it's doing
well in war games and also a dense model
that produces what is called embeddings
so there is a vector representation of a
game in a space where gave our vectors
that are close represent games that are
close in in in in interest in people's
interest and this gives you care
best of both worlds because sometimes
you want to give to learn exactly what
people want and sometimes you want to
get a little bit outside of the rizona
of comfort because they want you to find
something new but you still need this
information about proximity and well
there is a whole blog post about it in a
scientific article that explains how to
train those in together and and obtain
something that works quite well I used
it here as a black box I just specify
the inputs trained it some hyper
parameters used it as a black box and it
seems to be working okay in this case
another question No one minute and 30
seconds for a question no it was all
clear
all right yes one more question there to
test it what do you mean to test what
exactly do you mean to test so yeah so
there is a lot to say about testing here
maybe there is one point that I want to
tell you about testing and that is about
data flow so what use what you are
seeing not here because this is Python
code for my model but here my data flow
code this one I send it to the online
data flow service for execution but this
is just Java okay
and the what is what I like very much
about data flow is that by changing one
parameter I can execute it locally
instead of executing this on this
optimized data flow server less
infrastructure and when I execute it
locally of course I will have to take
care of my inputs and Ana my outputs if
it is if I'm doing batch processing that
means that I will have reading I will be
reading from some local file or even
remote but usually a smaller file and if
I'm doing stream processing I will have
to configure a stream especially for
debugging apart from that
I can debug locally in my Java IDE put
breakpoints everywhere and make sure
that everything works correctly and
that's something that I find really very
very useful in data flow thank you for
attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>