<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices Data Patterns: CQRS &amp; Event Sourcing by Edson Yanaga | Coder Coacher - Coaching Coders</title><meta content="Microservices Data Patterns: CQRS &amp; Event Sourcing by Edson Yanaga - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices Data Patterns: CQRS &amp; Event Sourcing by Edson Yanaga</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eyf2Fs7GBo0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody good afternoon
of course again it's a great pleasure
for me to be here at devoxx and my name
is Atsushi Inaba I'm currently a
director of developer experience at Red
Hat's my - your hand Rosetti Onaga and
just in case you're wondering or if you
didn't know that yet yes
I'm Brazilian Japanese I know there are
other Brazilians at the conference but I
think I can tell that I'm the only
Brazilian Japanese here such a unique
combination okay when today I'm going to
talk about Microsoft data patterns
sakura-san event sourcing I introduced
it a bit this subject on my talk on
Monday but today we're gonna explore
even further this this topic so I start
with some stereo horizontal donation
again I also happen to be a Java
champion a Microsoft MVP which he kind
of showed that the world has changed a
lot and we're living in a very different
world and last year I had the
opportunity to give a talk devoxx
with the subject migrating to
Microsoft's databases this was the
result of the last two years of research
I've been talking a lot with different
teams and developers worldwide trying to
figure out how they solve the problem of
splitting and later integrating the data
in Microsoft's architecture so in this
book I cover the subject of how can we
split and integrate the data using
relational databases but after that I
received a lot of different requests I
talked to a lot of more people about the
subject again and I realized that some
pieces were missing because it's not all
about relational databases we have
different use cases we have different
scenarios different requirements lead us
to use different technologies to solve
different problems that's why I decided
to go even further on my research and
again in the past year at least I was
able to collect even more useful tips
for you if you're considering using
microservices architectures and just as
I said last year of course this is an
oversimplification saying that code is
easy
the state is hearts codes can be hard
too but no matter how hard your code is
I think that in most enterprise
information systems the really hard part
is your data because the data is
something I'm pretty sure that most of
the code that you write today you won't
be running ten years from now but I'm
pretty sure that most of us enterprise
developers we are dealing with legacy
data and this data might have been
written into a relational database like
30 years ago
and we still have to manipulate that it
might be we're dealing with legacy
schemas and we probably have to deal
with that in the upcoming upcoming years
too so data is a special point in any
architecture there is no system that
doesn't manipulate data
you always have behavior and you always
have data but I've seen that many people
when when discussing microcell
distributed systems and Microsoft's
architectures in general they only talk
about the general architecture of your
code and sometimes they forget about the
data that's why I decided to to pick
this topic to go even further and try to
explain to you what is happening in the
data world and how people are solving
the data problems in this kind of
architecture but before I dig into the
sakura-san different sourcing of today I
have to introduce a bit of context
because particularly I've been reading
some books and articles on internet and
I think some people have got some I want
I want to say wrong but not very clear
definitions of what is your class what
is different sourcing why are we using
this kind of things today so I think
it's it would be important for me to
introduce a bit of context so because
this patterns efficacy patterns they
were created more than 10 years ago
and we need to understand why they were
created at that time so I want to first
ask how has data management managed ten
years ago if you can remember which were
the tools which were the context that we
had ten years ago when we were
developing specifically Java software
you know if you take them back you
forget to take a time machine you'll go
back ten years what were you doing ten
years ago in your daily development
activities you would say that 10 years
ago you were pretty terrified about
entity beings because many of us had to
deal with entity beans for persisting
object States into relational databases
then we magically had something that
promises to solve all of the problems
that we have of entity beans we had
hibernate and just to give you some
numbers in 2005 we had hibernate 3.0 and
it took like many years to have like
another version of hibernate for Java 6
were released in 2006 so approximately
10 years ago 11 years ago and Java EE 5
and JPA 1.0 we release in May 11th 2006
so that's the word that we had 10 years
ago and you can bag that you can bet
that 2006 we had Java EE 5 and JPA 1.0
but it took like many years for us to be
able to use this technology in
production mainly because the release
sack of the java application service
it's not like java was released 2006 i
bet that most of those were able to
first use like JPA 1.0 in production in
2008-2009 or even 2010 so and maybe some
of us are still using like Java 5 inter
production so the release cycle for us
to be able to use new technologies
sometimes takes longer than we we wanted
to so giving this context we had
hibernate we had entity beings we wanted
to use pojos in our domain model and we
still add at that time we're still using
XML to map our our pojos to to our
relational database so we have the Java
5 we started to think about well maybe
we don't we don't need to keep using XML
for this kind of co-creation maybe you
can introduce some kind of metadata into
our code so we started to think about
annotations and some people thought that
it would be a good idea to have
annotations everywhere then we had
annotation hell just like today but but
at that point in time we thought we were
leaving XML hell so we thought that we
could get rid of everything XML
and get everything into annotations so
basically that's the context that I
would like to give back ten years ago so
we instead of using entity beans we were
creating pojos we what I call a dynamic
domain model right we had this pojos
most of them were just placed at content
holders were just like playing poachers
with just attributes and get our setters
we have no behavior mainly because of
the legacy that if we have the entity
beans we could an add behavior anyway so
the first step well now we want to use
pojos with hibernate and everything else
we just try to mirror the concept that
we had in past we're doing the same
thing as before but now we're all simple
Joe's and maybe using some annotations
that's the context of ten years ago then
ten years ago we are approximately we
created another kind of architecture or
I don't know if was really created but
the term was coined by them by their
even sourcing which is a very nice
architecture for some requirements event
sourcing basically means that the state
of your data is not stored anymore in a
single row in a database with a lot of
columns where you can read and write
that the state of your data is now
represented by a stream of events
usually in an append-only file system so
you're not recording anymore that like
your the money that you have your bank
account is 10 euros no you're just
recording like I start with 0 and I just
keep recording the depth and credit
transactions until I can reach the
current state of mind of my object or or
my entity my abstraction so you might
think that event sourcing if you look at
the theory in orphan else it's a very
nice architecture for you for writing
you have very fast writes but reading
but it's not that fast because every
time you want to know how much money you
have your bank account you have to start
from zero again and keep calculating all
the data back and forth ok so this is
the concept of insourcing so 10 years
ago we were starting to think about
pojos and then we realized wow event
sourcing is a nice thing but there was a
huge gap in mindset between the
technologies and tools that were
used to in this new architecture called
oven sourcing and even source is not
fine just because we can have like a
free auditing log and a free state
machine and very fast writes but I think
one of the key points of event sourcing
that we can benefit from today is that
if you decide to use a vent source in
your architecture events enables you to
think e the events that happen the
system instead of having to think about
the data structure of your systems and
think about it's one of the dangers all
that condition of thinking we are always
we're always trying to reproduce the way
that the the procedures that we had in
the past so if you if you talk to domain
experts or the system analysts or
architects that are like older than I
expect the average of us they used to
think about suco databases em for now so
every discussions about the domain model
always start with that well now we have
to model a bank account now we have to
model a customer and the next step is
always well customer what does this
customer have the customer has an ID has
no no maybe a date of birth maybe it has
the the day that were the where the row
was greatest it might have a phone
number you might have a social security
number you might have like a lot of
different addresses but if you have a
lot of addresses then I think you need
to at the foreign key then you're going
to create an association that's the
typical discussion especially if you
talk to other people because they're
they're used to think about the
structure of the information system
before thinking about the behavior of
the system so that's a typical use case
and many of us if not most of us are
also used to think about the structure
instead of the behavior so but when
you're thinking and modeling your system
as an event sourcing architecture you
have to think first about the events or
which means that you have to think about
the things that happen in your system
and just later you start thinking about
the structure of the information that is
flowing between your endpoints
the system I think this is an important
switch because today we're talking about
distributed systems distributed
architecture such as micro service
architectures we have to think first
about the things that happen in our
system we have to think about the
dynamic behavior and consequently the
events and later how we think about the
structure or that I think we were
switching the things that we think first
so that's important switching events
sourcing so 10 now imagine now we have
this kind of information but ten years
ago
pojos even sourcing it was with such a
huge gap and to try to fail to fill this
gap a bib it's Greg yang which is the
person that coined this the the term
secure is you know he used the term cqs
which means a different thing from souk
us work was created by roberto Mayer the
guy that created the info language and
in press poet meant commands curie
separation basically meant that in your
system you must have interfaces or
abstractions and this abstractions must
be split between writing operations and
reading operations so writing operations
that modify the states and reading
operations should only return the
current state we should never modify the
state so that that's the principle that
was applied and or in other words I like
this quote from better mayor to asking a
question should not change the answer
which means that if you ask the same
question of five times in your system
you should always receive the same
answer qe operations should never change
the state of your system okay and
improving all that grab young needed a
term for this new kind of architecture
or pattern that we was creating the time
so he decided to call that secure RS
command key responsibility segregation
which is a very fancy name for it I
think and now we consider simple
architecture and I think we had some
kind of confusion in the past too
because most people thought that seek us
and secure has meant the same thing we
can see that security improves on top of
seek us but they mean different things
at least as the
that we have today so if I wanted to
give you a brief explanation of what is
the security architecture or a
cigarettes pattern so in traditional
enterprise information systems the way
that you want to store and read your
data just as I show the bits in on
Monday we had if you want to if I want
to have a customer abstraction I creates
a customer class and this customer class
has all the dependencies address phone
number and everything else and I use
this customer class for writings so if
I'm using JPA I get my entity manager I
asked my entity manager to store and
just save to merge my customer instance
and when I want to read that I just
create a J pqe and I retrieve all of the
information from the customer into
memory I populate the POJO back again
and maybe I extract this information to
show to the user this is the typical
credit architecture but you might think
that it doesn't perform there well
because if my customer cleared my
abstraction has a lot of different
dependencies like if I have like my
customer class has a lot of fields and
maybe on the screen of course all the
fields won't fit on the screen if my
customer had like 20 different fields
and he had like I don't know 10 join
tables and I just wanted to show on the
screen the customer ID the name in the
phone number I would have to retrieve
all of the information from the database
populate all the poachers just show
three different fields on the screen so
it doesn't perform very well and the
more complex your entity is the world
you get even worse performance so when I
create secure as we created what we call
at that mine D Tio's data transfer
objects which are different from the
entity being ones and we call that data
transfer objects because they are just
placeholders for information they don't
have behavior they are truly anemic but
they're just supposed to mimic the
exactly same view that you would have on
your screen right so if you have web
systems you just give me the fields that
you want to show to the user so now you
created a security architecture because
you're writing still writing populating
a customer instance and writing that to
your database but for reading operations
we are steeped in another class which is
customer dto and you're showing that
your user on your screen that's the
simplest possible securest architecture
okay and of course you will have
different if you're using a repository
interface for example it's a good good
practice to to have one set of
interfaces interfaces in your repository
for writing and another set of
interfaces for reading because you might
even use different technologies for
reading and writing as I said the
simplest possible tetris just creates
separate classes domain classes but you
might be even reading and writing your
information from different data stores
implemented in different technologies so
that's why it's important this
segregation and if you think about
secure as that this very simple security
architecture of course then you start
thinking that you can improve on top of
that and today for distributed systems
the most common architecture that we
have for data distribution and
integration is secure s with separate
data stores you have different models
for reading and writing and your reading
and writing from separate data stores
when I say separate data stores it means
that you might have different sets of
tables and columns in the same databases
or you might be writing your information
in an event store you might even be
writing or your write operations in
something like MongoDB but reading from
a sequel database or you might be
writing to large like any and in the
memory data grid and reading in front of
information using like some stream
processing analytics cluster you might
be using Apache spark to to process some
real-time analytics so today we're
studying used different technologies for
the write and read operations so that's
an important thing that we have to have
in mind because again the most common
architecture for for secure as these
days in distributed systems is separate
writes and read data stores and secure a
sentiment sourcing it's very common for
us to be applying these architectures
together today I'll explore even 30 the
discussion but I'd like to say then when
Greg young
this disturbance occurs 10 years ago he
intended secure has to be an
intermediate step between traditional
like cruds information systems just like
using the same model for everything to
an event sorts a sector so if you
thought that well maybe if we introduced
secure acid as a step between we first
separate the reading the data models it
will be easier for people to grasp event
sourcing and then migrates which I
believe that holds to be true and I
talked a bit about this on Monday too
but I think that secure has proved to be
such an useful pattern that we're using
that in many different use cases days
not just for event sourcing I think
there are there are perfect fitting
secure s in the mid sourcing mainly
performance reasons but I'm going to
explore with you from now on why why you
should be using secure as in other use
cases and which are the technology that
should be using depending on the
requirements that you have in your data
so why secure s classical reason for
using seacrest
is performance if you have you you're
not allowed to just retrieve all of the
custom information in the POJO for your
QE or you might even want to perform
some some critical aggregation and you
don't want to perform this aggregation
in memory it's much more efficient for
you to be performing this aggregation on
the database using your your your
aggregation capabilities of your sequel
database so performance used to be the
first reason for using secrecy
architectures ten years ago but now that
we have distributed systems Microsoft's
architecture we're using security
architectures for things like
distribution availability integration
and analytics so distribution is that
now if if in a distributed architecture
for macro service if each one of the
macro service must own its own database
which means that the owner of the
Microsoft data is the macro service
itself so you have different pieces of
information that is scattered around
your architecture but if you have just
the pieces of deformations very hard to
you
for you to do something useful because
data usually needs to be correlated with
other pieces for you to process anything
in enterprise system so you need to
integrate and correlate the data to
generate any useful output and also so
distribution and integration but you
also if you have had distributed system
now you need to take care of the
availability of your system because now
if the customer information is only on
that micro service and that micro
service goes down I'm not allowed to
break my entire system just because I
don't have the customer information but
if you think about behavior many people
think about circuit breakers and their
final circuit breakers are a nice
solution for the behavior parts but if
you need the data that is on the
customer service and that thing is down
how you're going to finish any kind of
computation that needs that data if you
don't have it so you're also
distributing or your data and
integrating your data to improve the
availability of your system maybe you
need a local cache or mirror copy of the
information that is from the customer
micro service here on your monolith or
your other macro service because you
want to improve the general availability
of your system so availability is an
important part and analytics this is
becoming very well the the main theme of
the Vox this year it is artificial
intelligence and most of those I'm
thinking about using some process of
machine learning and to have machine
learning we've needed lots of data and
one of the easiest way for you to
generate the data that is flowing
through your system is through events
right so events that are being
distributed in your system are a very
important source for your analytics
process because we want to generate even
more knowledge from the information that
we have store today or that we're we
have in our system but since we're not
storing this the events are happening
anywhere we can take the benefits of of
every off of all the information that we
could have in the system ok so the
typical use case here for secure is is
that I have a canonical source of
information which can be your enterprise
system or can be
our new micro-service yeah again you
don't need to be creating multiple
microservices architectures to take the
benefit of secure s because you can use
your old legacy monolithic application
to generate events and create a secure s
that is tor for analytics for example
and and some people use this kind of
thing for data warehousing in the past -
so then we're housing you basically
creating a secure s model to perform
surgeries outside your your production
database so it's a same principle here
so you have your canonical source
information doesn't matter if it's micro
service or monolith then you need to
create your distributed secure s redo
the stores each one has one specific
purpose if i had a customer
micro-service maybe I'm creating here a
secure secure s read the disturbed
because I want to improve the
performance of this part of the system
with a local copy of the data or I want
to improve availability or in the other
case maybe I'm creating another security
read datastore just as I said before it
just for analytics right or another
reason for me to be doing that I might
have some sensitive information doing my
canonical source information that I
don't want to be hacked so if I have
like some medical records or social
security number information that
children is supposed to be public and I
don't want to be like threatened by a
node struts version that might not have
been patched in production then I don't
want to have like my production of
database being exposed to the public
with that part of the system maybe I
want to tie well
this public API we want to consume the
records that are supposed to be publicly
and the sensitive information will be
stored in the separate database so you
can create a secretor s read data store
with just information that you want to
make available to the public and serve
that in a different server right that's
another valid reason for you to be
creating a secure S with the restore but
since you're basically now creating rich
data stores which you can think about
cache it copies because they're just
supposed to be used for reading and the
writing always happens on the canonical
source information
and I have to open a parenthesis here
when I say canonical source information
I'm saying that this datastore is the
only one that there is allowed to have
rights it's different from canonical
model I think I made some confusion
Monday canonical model is that old
legacy thing that people thought that
well my entire entire enterprise needs
to agree on a common model you see
everybody has different requirements but
we must have a common model this is a
canonical model here I'm just saying is
a canonical source information meaning
that the true point if you want to see
if anything is really right you go to
the canonical source information because
all of the other ones they might be
wrong or updated right that's the that's
the other end points but if you have
these other end points you have your
canonical source information you need to
update this read their stars in serene
somehow and that's where events take
part on one architecture right the
easiest way for you to be updating your
read datastore is through events and
that's where you have your event source
in architecture like the flow of
information for you to be updating your
reads secure has read the restores is
through events and you have many
different techniques for generating
these events okay and since I've
explained this concept already long
let's go that different choices of
technology depending on the requirements
that you have on your data and your
application some considerations that you
need to have in mind when try to
consider an architecture or technology
to be solving your problem includes
latency when Emil and I see is that I
have the source of information I have my
read datastore latency me if can me I
can have different meanings depending on
the context latency can be how much time
does my data change gets to be
propagated from the writing the restored
to my read the restore okay how how how
much ladies to do I have with the
queuing network connections that I have
from here from here right am i writing
my readers
is that on a local network or I have a
geographically distributed datastore
some of Conservation's
other kind of ladies we can consume
consider that business latency
how long can my business application
wait to receive an update from the
canonical source information does my app
and so if a record get updated right now
if I if I wait like five seconds for
that information to be updated s we will
have a strange behavior in my system or
if I wait like 30 minutes you won't
matter because users can work with the
information that they have from 30
minutes ago that's a lot of
considerations that you need when
choosing technology size how big is that
data set that you must replicate and
again how big are the changes that you
need to transfer to transfer from one
endpoint to the other it also influences
that the technology that you choose to
solve your problem stay honest how
frequently your data gets updated on the
canonical source of information does it
get updated like five times per hour or
is it does it get updated like 10 times
per second or a hundred times per second
right these kind of requirements change
the technology that you choose to
implement this architecture to ownership
who owns the canonical source
information is that your own enterprise
system that is only the information is
that is that is that a database server
that is owned by another team inside
your own company or is that an external
service you just consuming your data
from an external provider so that's
other considerations that you must have
to security which is basically which
subset of the data is it sensitive as
the example that I gave before you do
how much of the data do you want to have
the reliability of exposing to to the
public so that's another conservation
and time I said type and not pretty sure
if type is the right name for these
requirements but say depending on the
type of information that you want to
share you must think about type is
important for me to be always
corrects to be always curious with the
delays the information do I need to
store the whole history of the events in
my cigarettes data store all my data is
simply ephemeral right is something like
traditional enterprise system
information or is something like trading
information where it doesn't matter if I
change you if I lose like a lot of
Records I only need the latest one or if
I'm if I have an IOT system a collecting
sensors that is matter for me that the
old values no you just you only need the
latest information okay so these are
some considerations that you need when
choosing your technology now we go into
the technology part which kind of
technology is suitable for each one of
these problems so the first technology
that I want to cover is in-memory data
grids remember that it reads of course
it's you're storing everything that you
have in memory and you might have the
benefits that you already know for
example most people using memory data
greets for caching and that's a very
simplistic how they say problem to solve
you can also use in memory that agrees
like for session externalization these
days since we're talking about 12 factor
applications and you need to standardize
the sessions to an external storage you
use the memory data grids for that you
have some many popular data grids in in
the markets most of them open source you
can use that readies you can use
memcache but what I want to tell you
today is about infinite span because I
believe that it has some very nice
capabilities not only caching an
external session storage but we can't
have other capabilities that help us a
lot so so in memory that it reads for
example they can have charting if you
have an infinite span cluster and your
and your
your business is geographically
distributed and you know well if you
think about maybe the data from the
Brazilian customers is better to be
stored in Brazil because that's where
you're going to use that so you might
have a distributed cluster and you just
shard information depending on the
geography that you are so if you have
you store just you European customers
here in Europe Brazilian cut lady in
America customers in Brazil and so on
also you might use this charting abuses
charting for regulation purposes maybe
your country has a specific requirement
that the data from you do your users
must be store inside the country
Sharleen that is another interesting
application and keep that in mind your
data must be shard because I'll use that
in another example if you really need
fast transactions if you have a lack of
massive volumes of transactions in
certain moments of the of the year or
the day for example if you have like if
you have an online ticketing
applications and you you you sell
tickets to music shows and you know that
everybody is waiting for that new show
that somebody famous might be releasing
and and yeah I don't have a nice a
second ago right now but then everybody
when you release the the sales everybody
wants to buy the same tickets at the
same moments and usually traditional
like relation to the base
they don't cope with the demands so you
need something very faster than that can
can give you this this kind of support
usually usually you use an e memory
degree for that if you need real-time
analytics you need to know if something
is happening right now and you're not
allowed to wait for a spark cluster or a
Hadoop cluster to give you then an X
that you that you want to show you know
you need really some fast data we call
it fast data if you need the results now
from the analytics maybe you need
something like in-memory data grids and
one of the things that I that I want to
discuss too is that some e memory data
reads it's the case on finis BAM they
have some features which you call
continuous Curie's so if you were able
to store the your source of information
in an in-memory data grid the
traditional way for you to know if your
data change it is you're storing your
data here you creates a QE yeah you
apply the QE to your
data sets then you compare what I had
before and what I have now maybe you
have a control role saying though this
one was changed after this this time
stamp or something else then you know
which of the records changed it after
the specific time step or which were
modified with a certain flag this is
traditional way of thinking but you you
might bet this is also called Pauline's
you have to keep balling with the QE to
know if anything changing at all
traditional way of to know if it data
set has changed it another way for you
to be achieving that is what we call in
the in memory data grid terminology
continuous queue is instead of indexing
the data and applying Kyrie's to that
you do the opposite you index the query
and apply the data to the QE so that's a
very interesting scenario that you can
use for that and and instead of using a
pull based approach where you need to
keep pulling the datasets for changes
you can use a push based architecture so
just simply receive the changes in a
reactive way or something so I have a
very and very short and quick a simple
example that I can show to you here if I
just catch some code I have an embedded
if you spend clustering in your
application I just created a class
called developer and I have different
languages and each developer has a name
you have different languages that you
can choose from Java JavaScript Python
Java of course we love Java JavaScript
if you want to be a masochist and dotnet
if you're in the wrong conference but if
I say here I just created a an
application I just instantiated that
agreed I create some hundred developers
already then every 100 milliseconds I'm
creating another oh sorry I weigh 200
milliseconds between creating a new
developer and putting that on my data
grades and what it creates here is what
I'll call continuous QE I just creates a
Curie it gets into accident in finis one
cluster I must say I want to filter all
of the Java developers that have an A in
the name so every time a new Java
developer which has a maiden name
added to the cluster I want to get non
fides of that and perform some operation
so what do I do
I creates a listener a continuous cured
listener and it's like it is a callback
every time it happens I get not fired
and in this example I'm just showing you
a simple C's out so if I execute this
you can see that in the beginning I
already had a lot of Java developers
populated and now every time that I
randomly generates a new developer which
has an A in the name it which is a Java
developer I get this information printed
here because that's my callback running
and it seems well take some stick
sometimes because we don't create a new
Java developer so fast and sometimes we
get an new Java developer with an A so
that's what I call continuous query and
I'm pretty sure that you can imagine
some nice applications because if you
are able to index your data set in your
memory data grids in fact if we need
able to index your key wrists you can
easily use your this listener to start
to push the event notifications through
the other secure as with the restorers
so if you need to reply very quickly to
changes in your data set probably any
memory that agree this is the easiest
way for you to generate your events so
that's some of the considerations and
also other considerations that you might
have infinite span for example has this
feature too is distributed processing so
again traditional way for you to process
your data you have your data there you
you have a relational database your
connect your applications connects to
the database using a network connection
you could you use a key to QE use track
you of your future us tracks all the
data from the database cell and server
you get all of this data for a network
connection you load that into memory you
process that you modify something and
maybe then you write and update back to
the base so traditional way for you to
process information enterprise system is
you move the data to where your code is
you process that and then you move the
data back it works because usually you
have a lot
latest in network and a high throughput
network between your application and
your database but if you have
geographically distributed data sets
just like the ones that I said mentioned
that you can kind of like sharding in
your in your information or if your data
set is very big and you have a huge
cluster and you don't want to replicate
all that information so you have like
different machines in your cluster they
have different parts of the data sets
you can use you what we call this
tributed processing which is the concept
which is not new but finally 2017 were
able to use that in production you
create your code and the code gets
serialize it to your cluster and - which
is where your that data is and your QE
your code is in the cuted in the cluster
which is very close to the data you eats
process there and you just stream the
results so this ability process means
you're not moving the data because
usually the data is the big part your
code usually is marked you move the code
it gets processed on the grid and you
get back the results and since you can
use sharding in any memory data grid the
operation the member that agreed is
supposed to be fast already but if you
have a huge data set that is charted it
gets even faster because you can you
your curie is running in parallel in
multiple different nodes in your system
you just can get back all the
distributed queries and aggregates like
I don't know it was ten instances you
get back ten subsets of your curie and
you can if you're computing the average
is very used for you to to be doing that
just with five like ten different
results you just consolidate the results
back again your application okay so and
distributed processing continues queries
is a push based model
distributed process is a pull based
because you're moving the codes and you
have to wait for it to come back so you
you need to is to initiate the action
another way for you to be creating your
secure is read data model is through
data virtualization yeah and you might
thinking I know it's a complain that
some people say to me when I want to
talk about it if you create virtual
databases as kind of a sort
view for your distributed read that a
storm is not really data distribution
because virtual database they are still
reading from the the right data store so
it doesn't allow for example real
distribution or failover
but certain data visualizations like
deeds can be tied for for example for
with a when an embeddable
or a local infinite span or the
technology in memory data grid
application so you can apply cash to
your virtual databases so when you apply
virtual data databases data utilization
with a local cache you are able to
achieve better performance because
you're not reading from the right to the
store and you also have a local cache
for data availability because even if
the production database goes down or
your network connection to the
production database goes down you still
have the local cache and your
application your remote application or
my remote micro service can keep running
on that and when would you use data
organization for that because it sounds
like something legacy you need a data
station platform because that's the
easiest way for you to split and
distribute and integrate your data if
you need advanced set reporting
capabilities in your remote endpoints so
if you have like complex qyz just like
the one we're using Enterprise Systems
you probably want a secure secure
database on your remote endpoint and the
easiest way for you to create that is
using like a something like a data
virtualization platform and of course
all the things that I'm saying showing
you today they are all open source you
can start using them for free tomorrow
should we use message brokers much very
much likely you'll be using a message
broker for distributing your events in a
daily ritual ization you don't in data
organization you don't need message
brokers in the book in the book that I
show you in the beginning you can use
all the technologies that don't need
message brokers like database views and
materialized views to be achieving these
architecture to but I'm not to discuss
that today you can use message brokers
for example you can use active me Q or
everybody these days
talking about Kafka for even sourcing
systems really depends on your
requirements again if you have a trading
system or you have like an IOT
architecture where we have like multiple
devices just streaming to you the values
of the current values of the sensors if
you're measuring temperature for example
it's sending to you a new information
every 10 seconds it doesn't matter if
you lose like free of the last ten reads
because you can even that even that way
you you can have the latest value and
you can calculate the average over the
day so it's not really important if you
lose any of the message the same of the
training system if the your stock is
rising or declining you only really
matter for the about the current value
so you can use something like a JMS
topics for that you don't you don't care
if you lose some of the message you
really want to know if you're receiving
the latest message so because you need
just the latest information but if you
need something if like most of the
enterprise systems if you need to
receive the events in the exactly same
order that they reproduce it if you not
allow it to lose any of the message
because it would lead to inconsistent
State on the other end points then maybe
you might be using something like Kafka
which is a distributed streaming
platform and the nice thing about Kafka
is that each one of the the consumers of
your bus can consume the message in
their own pace because it's a it's a
pool based broker so you you never
overwhelm your endpoint and since it's
persistent it's well it's durable it's
not forever for example but if you have
a big enough persistent storage on your
bus you can read the message from the
last week or last two days the depends
really really depends on your
requirements if you are endpoints allow
it to stay one week offline then you
probably you need more than one week of
persistence then that's some some of
things that you need to tune when using
a Kafka bus right and you wouldn't need
a Kafka bus if you if your events the
data that is being propagated on the
events are of CR DT conflict re conflict
free replicated
data types but again CR DT is a complex
discussion in distributed systems very
rare data types RC R DT so I'll skip the
discussion to and next okay you might
also be thinking about choosing a
reactive platform for implementing your
your your security architecture and let
me tell you why it's useful for you to
use a reactive platform for example if
you have like you are hiring some
service from Amazon or from a
third-party provider from Google or from
Microsoft's and usually these
third-party providers when they provide
a public API for you they charge you for
a certain number of requests so every
thousand requests you're going to pay a
certain amount which is completely fine
if you only have one at the point in
your system but now that we're scaling
and distributing multiple instance of
your system you're not allowed to have a
pole basis architecture and you have
like well I have like 15 different
micro-services my architecture all of
them need that information and each one
of my micro serves they have ten
instants so I have like 150 endpoints in
my system keyring that public API you
can tell that your Amazon bill will be
very high in the end of month you just
allow everybody to be connecting
directly balling your remote endpoint so
what can you do you can create like a
nother micro service and I suggest you
to do in a reactive way for you to be
able to scale very well and you don't in
this kind of application you know need
like relational database persistence or
anything else so you're pretty cool we
have a very small micro service for that
what you can do is you can create a
reactive point endpoint here you can
hold some internal state or if the
internal States too big you can use an
Emer that agreed to to store this this
ephemeral State so basically what you do
this endpoint will carry the remote API
so it will hold the akashic copied
information and this is the point you
can configure that depending on the
staleness requirements that you have if
we will get it to execute the poly of
the remote endpoint to check if the
information that you're receiving now is
different from the information that you
have cached here the ephemeral storage
if anything change it you can use vertex
for example to propagate the changes
through Kafka bars or or JMS topic for
the other clients to be consuming or you
can even use your vertex application to
create replicas of the public API so
your internal clients will be queuing
your internal endpoint and not the
public one it will save you the costs of
having like a thousand requests per
seconds coming to the public endpoint so
that's another very nice application so
you can have even though a public API is
public HTTP API is our traditionally
pull basis if you add vertex to to a
reactor platform that can stream to your
Kafka bus for example you can you can
create an almost push base it depending
on the stainless requirements that you
have on your public API yeah and the
last one change data capture it's very
interesting that I'm talking about CDC
right now because Gunnar which is the
lead architect of the division project
is giving a talk showing division at the
same time in another room but just to
explain to you what has changed that
capture its sourcing architecture too
but how do you generate the events of
the changes if you have a if you don't
want to mess with your current legacy
architecture but you have physical
access to your database server you can
plug a CVC to and it starts reading
directly from the database transaction
log and for each one of the insert
update leads and alter something that
happens inside your table
the CDC two can read the transaction log
and creates a message that can be
propagated for the message bus and in
particular the besom can read from my
sequel post Kazuko MongoDB and they have
work in progress for Oracle and second
server I think but you would have to
check with Gunnar
to be very precise so basically it reads
that it generates a JSON payload that
can be broadcasted through your Kafka
bus and any of the clients in your
system can use this push based
architecture to
this Jesu events and replicate the data
that you need you can see most events
sourcing architectures that you're
coding your type of events you probably
want to use domain level events which
means that well the which means that oh
I have a bank account I have a Lac
account overdrafted or I have like
account closets and not just please
simply like somebody added money
somebody withdraw money which was sought
you have domain level events if you're
coding the events but for certain types
of system which is the kind which is the
example that I want to give you now you
might use low-level events because
insert update delete and alter table are
very low-level events are much closer to
the to the database operations that you
have and this kind of events they don't
give you the semantic value of a domain
event but I want and you might say well
this is a big advantage I will say no
it's a very low level so it's very easy
for you to just filter the fields that
you wanted that you don't want to create
your secure as read the restore and
that's what I think the greatest
benefits of low level events that you
don't have the power of domain level
advanced but you also don't have the
coupling that domain level events give
to youth many faecal many people think
that well since we're using master genes
we have a low coupling between the
endpoints but people forget that if
you're using domain level events every
time you create a new event or you
change the type or the semantics of the
event you have to change all of the
endpoints of your system to be prepared
to handle this this change but if you
have done a low-level events such as
insert update delete there are such a
low level events you can't if will never
change use what was inserts updates and
elite events so it's much easier for you
to couple with changes in your
architecture and I'm already over time
that's what I wanted to discuss with you
briefly about that I'll be publishing
some articles of these different
strategies and I'm trying to also to
create a very nice demo integrated all
of these different technologies and I
hope you'll be able to check that later
on my blog or my Twitter and again thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>