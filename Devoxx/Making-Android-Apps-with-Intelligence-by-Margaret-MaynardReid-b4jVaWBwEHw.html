<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Making Android Apps with Intelligence by Margaret Maynard-Reid | Coder Coacher - Coaching Coders</title><meta content="Making Android Apps with Intelligence by Margaret Maynard-Reid - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Making Android Apps with Intelligence by Margaret Maynard-Reid</b></h2><h5 class="post__date">2017-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/b4jVaWBwEHw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">go ahead and get started
hey everyone welcome to my talk
making Android apps intelligence and a
little bit about me I'm a Android
developer and currently a consultant at
Microsoft AAA and research I'm also a
student of Udacity say I nanodegree am
organizer and women techmakers lead for
Google developer group Seattle and in
2015
I taught Android application development
certificate program at University of
Washington so during my talk if you want
to tweet about it you can use my twitter
handle at Margaret MZ which I included
on every slide going forward ok so
that's what I just said over there so
there's artificial intelligence
everywhere these days from Google home
to Amazon Alexa Microsoft Cortana pepper
the robot and self-driving cars so a is
everywhere I'd like to share with you a
personal story of how AI is impacting my
life in January this year my dad was
diagnosed with prostate cancer on March
6 he went through a surgery at u-dub
Medical Center the surgery was conducted
by the expert surgeon with the
assistance of a robotic arm it is so
precise with so little intrusion to the
body
my dad was discharged within 24 hours of
the surgery and 10 days later
last week he was sitting in the
restaurant celebrating with us for being
cancer free so I am even - today I'm
still so amazed by how technology is
changing our lives so we also hear a lot
about machine learning googles
tensorflow
IBM Watson uber uses Microsoft
of cognitive services to self selfie
check drivers we hear a lot about AI and
the machine learning interchangeably
these two words but machine learning is
actually a subset of artificial
intelligence it's the study of
algorithms is about learning from
examples and experience instead of
hard-coded programming rules for example
a very simple example of say telling the
difference between an apple and orange
using supervised - learning would be to
collect training data training a model
and then use that model to predict
instead of writing if-else rules
to tell the difference between an apple
and orange so this example is taken from
the Google developers machine learning
recipes on YouTube I include a link here
for you to take a look so the goals that
I have today to talk about making
Android apps with intelligence is to
help you get started with making apps
with intelligence and share with you
some of the resources available and this
is not a crash course on machine
learning as I mentioned earlier I'm an
Android developer I'm not an expert on
machine learning or AI that in fact you
know sums you know how to make a REST
API call you can already make apps with
intelligence and also even though my
title is called making apps android
absolute intelligence the content i'm
sharing is applicable for iOS and web as
well so as I said earlier machine
learning is not new but the exciting
news is that recent years there's a
break we hear a lot about it but because
in recent years there's a lot of
breakthrough in this technology as well
as the news of these services are pretty
much available to us app developers a
lot of it for free and I want to share
with you the resources the complexity
goes from easiest to partner right so I
want to talk about how as Android
developers we can just use the Google
Play services mobile vision API
that's one option another option is REST
API call as I mentioned earlier Google
Microsoft IBM HP offered these machine
learning services then later on I will
talk a little bit more complex example
about training some pre trained models
for example using the tensorflow and
microsoft language understanding
intelligence system and I will not in
this talk I will not talking about
coming up with your own model machine
learning models I will leave that to you
when you become an expert one day so the
first option is to use the mobile
fishing API as part of the Google Play
services so for those of you who are
Android developers it's like really easy
to use because it is part of the Google
Play services since the seven-point a
release from August of 2015 and it's
very easy to get started it can detect
face barcode and text I've included
links for these collapse for you to try
out later so the example on the face
detection is that it can detect one or
more faces here's an example of the
picture of my face if I feed it another
picture say with a cat it can detect
just a face not the cat's face but the
humans face and I want to talk about the
distinguishment
here it is that this API will do face
detection meaning it can detect the face
but it cannot recognize this is person a
versus person B so once your app is
detecting that face you come back again
you will not know that's this particular
person this detection can be done from
still image or live video and can be
detected from different angles
so you know by comparison with other IP
eyes I find this one works out pretty
well with different angles you can also
detect what your face of facial
activities for example if you're
blinking your eyes your smiling and the
land marks on your face such as your
nose your mouth where your eyes are
located
so here's how to use the face detector
really easy you just need to include
Google Play services and then you create
a detector object make sure its
operational first before you use that
object and then you just detect the
faces here and then once you detect that
face you can draw a rectangle on the
face that's the simplest example but you
can do other things as well I included
the face detection with vision API
collab for you to try it out later
here's an essence of the example the
source is from Google
they created a sample app called the
googly eyes basically the same as what I
described earlier except in the landmark
in this case are your eyes so it can
detect whether your eyes are open or
blinking so you can create you can see
how this app is working the mobile
vision API can also detect barcode it
can detect multiple barcodes at once as
well as barcode of various formats and
it can also detect text this has been
around for a while so it's nothing new
but the reason why I'm going through
this here is how easy it is by using the
Google Play services mobile vision API
that you can have these capabilities in
your Android apps okay so now I'm gonna
move on and talk about using machine
learning services via REST API calls so
that using Google Play services is
really easy right you just put it in
your build that Grado now let's look at
the REST API calls well I'm sure you
already know the REST API calls
typically when we use these machine
learning services we will send over
image video audio or text typical
example and that services in the cloud
will provide the servicing in terms of
the vision speech language that natural
language processing text recognition and
translation search etc and then you get
back a response and then you just use
that response in your app known as
because of the API call you need to have
internet connection too
to use these services to get started
with these services a typical example is
you know all these companies are
offering these services they will offer
you some free trials for a period of
time or a number of transactions after a
while they will start to ask you like
you need to pay some money but some
services are still free and the typical
process is you get your API key you
include that key in the Android app and
then you will download the SDK or
included dependency your build.gradle
some companies for some of those api's
they do not have SDKs that you can just
make your own REST API calls and send
over the image and the tax etc I get
back the response and processes a
response so I want to talk about
ingesting a few examples of using
pre-trained models so these so called
REST API calls there's no training of
the machine learning models at all these
are pre-trained you do not need to have
any machine learning knowledge you just
need to know how to make a riskless API
call and to check out these services you
can typically try out these aps directly
online from their website and I will
take a quick look at example of the
Google cloud and the Microsoft cognitive
services so let's take a look at the
Google cloud vision API
this is that website for this one to try
out the API you can drag and drop an
image
so I drag and drop an image of a cat and
the API correctly detected it is a cat
it has some other alternatives here as
well but the cat is listed in number one
and this is just online for you to try
it out you can try out their text
translation as well this is for you to
try it out before you actually use the
API in your app and I will also show you
this one this is Microsoft emotion API
as I mentioned earlier the Google's
vision API can detect whether there's a
face or not and it can detect the where
your landmark of the face as well so
Microsoft has actually division the
emotion API well both Microsoft and and
the Google's API can detect the multiple
faces here right and then if I click on
this person you will see that the
emotions actually is listed you can see
the anger I can't quite see very well
but you can see listed out all these
different emotions so when you are say
writing your app you get back this
response JSON response and you can just
choose the emotion that was the highest
score to determine what actually the
emotion is for that particular face and
then I will not go into details
this is HP's again it's similar you can
go through to look at what API they're
offering and how each API actually work
before you include that into your app so
I will talk about a little bit about the
Microsoft phishing API the computer
vision API and the emotion and the face
can actually detect gender age emotion
as well and I will show you this is the
code example let's see I'm trying to get
my screen to work here
so this is just a very simple example so
I will take a photo and I didn't try
this before so I will just try it I
don't know what it will happen okay I
did not try this before he might totally
come out to being accurate so what did
they say
indoor building table bench broom okay
dog there's no dog but there's a man so
more or less it detected that some of
these objects in the room and notice so
this is the sample app that I will this
again this demonstrate you have to call
some endpoint and it get back to JSON
and then it will I pass the JSON you I
put these captions there and notice that
this one will rely on the internet
connection because the one time I did a
demo and there was no internet
connection and this did not work and and
I'll explain to you later on why this is
important because you know you don't
always have internet connection okay so
here's another example of using the
computer and emotion and speech if this
is the mimicker alarm app that I worked
on it's kind of cute so let me see if I
can move this over and I'm gonna set an
alarm to be too
45 and the lamb will ring in less than a
minute also actually in here I said it
to be tongue twister
so okay I'm gonna set it to just three
so basically now the alarm is going off
basically I have to do something to
dismiss it that alarm there's no way to
dismiss this alarm
you can't snooze it if you try snooze it
I think it will just try and come back
oh hey I snoozed it it will come back
sorry let me try again I should not have
sneezed it so if the alarm will ring
again shortly less than a minute and I
have to perform an action action either
by finding a color or making an
expression or by making a tongue twister
so sorry this is kind of freezing up
that's the problem with live demo okay
so this is the right screen now and I'm
gonna try to dismiss this alarm and I
need to find something that's green well
that is too far
so I'm gonna try finding this I have a
green umbrella
so this should have dismissed alarm
maybe I should have done used to do the
now I'm stuck with this alarm I cannot
that I cannot dismiss but I can go back
and change it to be the tongue twister
or anyways I'm sorry I cannot go through
this example I was going to show you
whether we can use a tongue twister or
emotion in making a silly face but I
think I'll just move on from the example
now
okay so so I talked about the doing REST
API calls for machine learning now I
want to move on to talk about actually
training the pre-training model some of
the examples so some of the examples
will include that google's tensorflow
although transfer fluid is not limited
to just training the pre trained model
but in my talk i will only talk about
the training the free training model
microsoft's language understanding I've
included the Google cloud machine
learning and Amazon's machine learning
platform here although I will not talk
about the actual coming up with your own
models etc so tensorflow
you probably everyone has already heard
about tensorflow right yes raise your
hand if you have heard about tensorflow
really okay
then I would put I will play that video
because I was assuming everybody has
heard of it basically it's this open
source machine learning library released
by Google
I believe end of 2015 and shortly after
that became the most popular repo on
github and they especially used useful
also for deep learning if you heard of
deep learning alphago beat the human
player that's using the deep learning
tester for deep learning and it's for
both research and production recently in
February they just released their 1.0
1.0 and then 15 days ago they released
one point 0.1 so since only two of you
heard about it I will play this video
hopefully we have internet connection
let me check my internet connection
it's supposed to be playing yeah it
downloads when it doesn't have internet
connection it just starts to download
this isn't just not working out very
well I'm sorry I have to move this over
to go check out this video because I
really want to show you this video since
only two of you have heard about
tensorflow
all right so I think I give up on trying
to play this video I will just describe
to you what it is and when I share my
slides you can go look at it basically
it's a pretty cool tool like researchers
can use as well as developers could use
I will move on to talk about the tensor
flow for poet
kulap how many of you actually have
tried out this collab before no okay so
I will not walk through the collab but I
will point you to the collab it's
actually pretty straightforward and
basically you will go through these
steps for the collab it's not really
machine learning per se you will install
a wrong attention flow darker image with
everything already there and then you
will retrieve the images of different
categories of flowers for example
dandelion some flowers and and tulips
its et cetera and what you will do is
you will actually retrain they provided
a Python script here that's really
simple one of these steps right once you
return you retrieved images you will
retrain the final layer of Inception 3
was 500 to 4,000 images and the more
images you train you know the better the
accuracy the model actually does on the
prediction and the inception is a huge
image classification model with millions
of parameters that you can differentiate
a large number of kinds of images so I
highly encourage you to go through this
collab it's not about Android it's not
about app development it's just sort of
intro to give you an idea what is
supervised learning learning is
supervised machine learning is and then
once you train that model you you will
use this retrain the model to classify
the image like you will feed in an image
and the model will tell you oh it's a
sunflower it's a lily
flower is it tulip so it doesn't take
that long to go through this collab and
you can try it out later and tensorflow
can run on mobile Android and iOS so
they also have this if you click on this
link it will take you to some Android
sample apps with tensorflow and Google
has written these three sample apps of
tensorflow classify cancer fold detect
and stylize write and I have also
included the recent tensorflow dev
summit video peter worden actually
talked about the mobo embedded
tensorflow will give you greater details
of how to compile to run the samples or
you can just download the ready-made apk
to try it out so i will show you real
quick what these three apps are doing
hopefully this is still linked so
there's a you can try out the sample app
you can also just download the APK in
there and there's three three of these
and this tensor tends to flow classify
this one will classify the object so
let's see you can see my screen as i
point to get the keyboard it correctly
classify as a computer keyboard here and
the difference between this one that i
was showing you earlier the sample using
the microsoft vision one that one was
using rest api call so that one must
rely on the network and you must have
internet and this one you do not need to
have internet connection
and you can just you know see it
classifies this correctly as a computer
keyboard
one thing i want to point out though is
i think their model is not trained on
humans so if i you know if there's a lot
of human sitting here i pointed to a
human a will not be able to recognize as
a human actually
some other things I don't know stretch
anyway it's not recognizing as a human
and then I will skip the tensor flow
detected you know this one it can detect
like multiple objects out human objects
or the same kind of object and let's
take a look at the tensor flow stylize
this one so this one there's actually
another collab not so much of Android
but actually you will go use the tensor
flow to stylize with some styles but
this way so actually the new tensor flow
sample app included as part of the
Android sample app so for this one this
is the preview screen right the Android
camera preview screen as I move around
as you can see the picture I'm trying to
take is they're taking on that style of
this particular one and I can change it
I believe you can blend the style as
well and then you can just click on save
and then you can save that image so I
will not go into details about how the
tensor flow style style eyes actually
works under the hood but I'm just
showing you how this Android app is
working out and there's all kind of the
dev summit videos on tensor flow you can
check it out if you want to learn more
about tensor flow now I want to move on
to talk about Microsoft Lewis so when I
talk about the tensor flow I did not
drill down deep about exactly how the
tensor flew at work he said sure I
basically talk about some pre-trained
model you train the last layer to
recognize images or some sample app on
Android and this one what I'm talking
about the language understanding
intelligent service it's actually a user
interface web user interface and then
you will train a pre-built language
understand
model again you don't start out from the
beginning it's already trained and then
you will deploy the model to HDTV
endpoint to interpret human language
then you use that HTTP endpoint in your
app just like I talked about earlier
about REST API call so let me see if I
can log in if I can't log in I will do
it that more if I cannot I will just
describe to you what is going on
so let's see I try to get all of these
all working ahead of time but somehow is
it just got logged out okay so I'm
actually logged in I think this is the
weather app so I don't want to talk
about it from the beginning but you can
as you can see it's basically a web
interface here and I will just describe
to you in my slide what can happen to
that web interface is for example you
can create one called
weather app and then you can add like
two pre-built entities like geography
and date/time and then you will add an
intent in this case intent is kind of
like the action you want to perform in
this case it's to get the weather but
you can imagine sometimes you can't
beget flight or get news and then you
will just type you actually type through
that web interface what is the weather
like in Seattle today or what is the
weather like in San Jose today when
you're typing that sentence you are
actually training that machine learning
model but you're training it through a
web interface and you can create two
parameters like where and when and then
you will train it with a couple more
sentences like what is the weather in
San Francisco what is the weather in New
York tomorrow
and you try to give a bunch of examples
that can turning the geography of the
location as well as the day time and you
just give it a whole bunch of examples
and then at that point you will publish
the HTTP endpoint when you publish the
HTTP endpoint the web interface actually
will give you the JSON that will return
that you will see the where and the web
parameter that you can use in your
android or iOS app and then as I
mentioned earlier you use the SDK and
optionally you combine with speech to
text with the language understanding so
you can imagine you can create an app
and it doesn't sound that impressive but
the app you can just say what's the
weather like in
Seattle and then you can see it in the
app they actually will tell you you know
you have to connect your another service
to actually get the weather but you're
in your app it will display or say the
weather to you now what does that remind
you of like what I just described you
can go ask for the weather can you think
of some other things you are using
already that it's already doing what I
described yes I like that and the Google
home right you ask Alexa the weather you
know the news Google home as well but
this example I describe to you with
these painful steps of you know you can
train but at least you you see under the
hood you're actually training that
machine learning with the web interface
and in the end you have HTTP endpoint
that you can work with so it's much more
flexible to the scenario of your app
okay so I want to talk about some more
other examples I just showed you how to
use the Microsoft language understanding
to create a endpoint for your story of
say getting the weather right well
Amazon Alexa is a voice service and it's
pretty powerful how many of you have a
Alexa at home I have one at home I do
talk to my Alexa so I have tried out
building these new skills the nice thing
about Alexa is that Amazon should
provide you these templates to build
skills you know earlier when I talked
about the Microsoft Louise I talked
about intent I can't talk about like
entity etc right when you go try to
build a Alex a new skill the template
you will see some parallel there where
you also have to you know putting in
your intent and your entity etc
basically when we talk about
intelligence we want to create like
intelligent machines right but under the
hood you're actually just trying to
train the machines to perform particular
action I don't know if they can
today the machines are still not as like
intelligent as humans like we learn how
to read and we can see and different
things they're more like categorize to a
particular action that you can perform
so I have included here some links for
you to try out that likes our voice
service and you can also try it out and
see if you can build something for
Android as well I want to mention about
pepper the robot how many of you heard
about peppered the robot true ok
when I was growing up I grew up watching
Astro Boy and how many you heard about
Astro Boy the robot from the Japanese
anime anyway so I grew up watching that
and then I never dreamed of that one day
I will be programming robot so I did get
the chance to work with pepper the robot
and to program the robot you use the SDK
mostly in Polish Python and you can also
use the Android SDK as well as of last
year at the i/o it's announced so you
just download the android plug-in SDK
you create an android project and then
you need to enable the robot project
structure and then you are ready to
program the robots this is the
intelligence itself is powered by the
IBM Watson so you can carry on some
conversation with pepper
you know pepper can hear you etc so even
though it does sound like it's easy to
create an Android app or ready for
pepper without an actual robot just
relying on emulator it is kind of hard
to work with a robot but with the
emulator you can try out a few simple
things like making the robot making some
sound like an elephant etc I've included
some examples here for the Google AI
experiments if you have never heard of
it go ahead and try it out these AI
experiments are basically really fine
experiments with images and music and
game and code I think as you start
making apps with intelligence you
checking this out will give you some
inspiration on what kind of apps you
want to make by incorporating these AI a
machine learning services so I talked
about AI and machine learning and how to
incorporate these services in your app
well before you put all these
intelligence in your app you really
should think about whether or not your
app really need that intelligence
because as I mentioned earlier you know
there's all these fancy capabilities but
you don't just include it because it's
fun fun and showing off the technology
you have to include it for because of a
purpose so as you evaluate these
services you also need to think about
does that service offer SDK for you to
use and how easy it is for you to use do
they provide support or do they give you
some free trial and then they make you
pay a bunch of money or is it actually
free because of all these services when
you use these machine learning services
most likely you will get some a lot of
times you work with images and the live
stream of videos so think about getting
user permissions this particular has
something to do with Android if you work
with Android apps think about the user
permissions and then another thing is to
inform the user about data privacy
because these the machine learning
services if they take a picture of me
they can detect my age my gender you
know some other things they know exactly
what I'm doing where I am it cetera so
it's a big data privacy issue so you
need to be upfront about your user if
you collect a lot of images where are
these images going to be stored so be
mindful with your users data so what is
next I will say there's a paradigm
shifting software development
today I think every single developer
needs to know about know about machine
learning and AI you don't need to become
a data scientist you know this is one of
the reasons why I'm a student of the AI
nano degree of Udacity it's not like I
want to become a data scientist but I
feel as an app developer it's very
important for me to understand what's
the difference between a and a machine
learning how does this supervise the
machine learning works you know how does
that image recognition was natural
language processing works under the hood
so that when you I should work with
these data scientists you can speak
their language also today I just read a
blog from engineering saying that AI is
actually like electricity it's going to
transform every has impact on every
single industry health care finance and
you know every single industry so I
encourage you to try out the machine
learning APs
I included it in my talk I didn't I
didn't include too much of like sample
code I mostly included a lot of
resources for you to try it out I think
even if you just try out say a recipe I
call look at that JSON response coming
back and parse that JSON and try to put
in your app it will already teach you a
little bit about what does that mean
you know to do face detection right in
try out the tensor flow tutorials and
collapse tensor flow org has a lot of
beginner tutorials teaching you about
for example they eat this is a simplest
example of a handwriting how do you tell
if this is a 1 or 2 or 3 the digits from
1 to 9 and then check out the AI
experiments that will give you
inspiration on what kind of apps you
want to make with intelligence and just
study some basics and make apps with
intelligence so thank you very much
everyone I know there's just a couple of
people here but I really appreciate you
coming
here to hear my talk today after this
slide I also included a whole bunch of
slides here about some talks on machine
learning from last year's i/o and as
well as if you actually want to learn
about AI and the machine learning how to
go about doing so just my personal
experience in the learning of this I had
to brush up on linear algebra calculus
intro to statistics and you know
algorithm etc before I dive into
learning about machine learning and AI
so sometimes it's okay to just play with
some tutorials but there's a lot of math
involved so it's good to brush up on
those and so that's it
do you have any questions yes so far I
have not noticed battery drain using
that but of course I'm only using it for
for a demo
I think I'll point you to watch this
video this talk mobile and embedded
tensorflow he did a very good job of
actually embedding tensor flow on the
device itself and talking about how to
reduce your apk size I think he
mentioned the battery is a bit there as
well any other questions no great thanks
everybody
unplugged my</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>