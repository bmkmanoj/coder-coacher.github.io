<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google Cloud Spanner: Global consistency at scale by Robert Kubis | Coder Coacher - Coaching Coders</title><meta content="Google Cloud Spanner: Global consistency at scale by Robert Kubis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google Cloud Spanner: Global consistency at scale by Robert Kubis</b></h2><h5 class="post__date">2017-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iKQhPwbzzxU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hello everyone good evening
thank you very much for joining me for
the session here about cloud spanner
Google cloud Sparrow my name is Robert
Kubis I'm a developer advocate for the
Google cloud platform probably most of
you I now know what the Google developer
advocates - were just for the ones that
don't know yet we are understanding our
job as a bi-directional interface
between you who built on our platform
and the developers inside of Google that
built a platform so please please please
reach out to us if you have feedback
constructive feedback if you think some
things are just too complicated and
could be much easier if you have some
really cool project on Google cloud
platform that we might can get promoted
for you so please please reach out to me
personally you can reach out on Twitter
under the name Aussie Aussie alright
what do I gonna talk about today this is
the tools and action sessions I'll try
to concentrate mostly on code but I
still want to first and foremost want to
get everybody on the same page and also
frame a little bit around like why we
built spanner inside Google so I gonna
go into a little bit of a history go
into some architecture and then spend
hopefully the majority of my time on
some coding and demo of spanner all
right so to get the motivation why we
built better we have to go back a couple
of years on 10 12 years and back then
Google was a massively growing business
and our edge system which was our
revenue driver was backed on charted
MySQL system now this short in MySQL
system was charted by a custom id and we
had to rechart it several times now the
last sweetheart actually took us
multiple years and we definitely saw at
that point that this wasn't a viable
solution to go forward we needed a
system that can could scale with our
business
from like a 10 million to a billion
dollar company and we needed something a
backbone that could support that so what
were the requirements for that database
that we needed first and foremost we
needed a database that could scale and
that could scale horizontally that's
just not no way to scale something like
that really
second of all we were dealing with
budgets monies partners agencies
customers so we were dealing with
something where eventually consistency
was just not an option for us so we
needed something that supports asset
transactions and global consistency and
third Google is a 24/7 business we're
dealing with money all around the world
and with Google ad system this town not
only Google uses much more importantly
and much worse our customers losing
money right so we needed a system that
could actually be maintained and evolved
and we could innovate on it without any
downtime so now we built this system
inside and we have a run in for almost
10 years or now inside the company and
many many of our products are actually
backed by a cloud spanner and more and
more are coming that are backed by
clouds banner dream is one of them the
Play Store there are many many others at
system as one of the first customers now
to make that available to all of you we
needed to package that into basically
what we offer to our developer console
and through our API is under the Google
cloud platform umbrella so first we
wanted to have it fully managed in a way
that you could go either use our API or
the developer console and create an
instance and then create database schema
and load data second we wanted to tap
into the knowledge pool that is out
there like there are so many people out
there that understand what relational
database systems are and they understand
what SQL is and their fee ephemeral of
it if he comfortable with it and we
wanted to tap into that pool so we
didn't want to create an proprietary
interface to cloud spanner what were you
like really on very important design
decision that we
as mentioned we needed asset transaction
we need a global consistency with global
consistency basically the requirement
for an automatic and synchronous
application came in so loud spanner can
be globally replicated and we also
needed to integrate with other features
like I am like enterprise features so
identity and access management we were
focusing on building our client
libraries job off one of our prime
client client libraries but also others
like PI tango nodejs and and so on and
more are coming now what is it so it's
better in terms of like the constructs
that you have and so it's managed I want
to bring everybody on the same page so
you really can follow easier along when
I talk about these different concepts so
everything starts with an instance an
instance is basically your resource pool
you define like how many nodes you wanna
have and that basically gives you a
barrier of how much storage you can
manage with that with these resources
now if you are under that limit you can
always scale in and out but you have
basically that limits like per node you
can manage 2 terabytes of data if you
have more than 2 terabytes of data in
your database you cannot scale back to
one node you have to at least have two
nodes in your instance you have
databases you can create up to 100
databases in your instance and then you
have tables you can create up to 2048
tables in a database now these are terms
database and tables that everybody who
is familiar with relational database
system knows now we needed a concept
internally that enables us to actually
scale to horizontally scale so
internally we have a concept of splits
and a split is nothing else than we
basically take their table ordered by
primary key lexicographically and then
we shop it up in parts so you basically
if you have a table over thousand
columns every thousand rows or ten
thousand million then we take like we
define chops or parts of that table
splits say splits by a hundred or
splitsville thousand ropes and that able
enables us to distribute the
responsibility of these plates to
different
worker nodes and by that enabling
horizontal scalability so how does it
look like so we have a separation of
compute and storage so if you buy or
rent one of our cloud spanner instance
notes you get actually three in a
regional configuration and we have multi
regional coming up later this year and
that we have even more replica notes but
let's concentrate on the regional
instance right here so in this case what
you see here you have a three node
spanner instance and these three nodes
are replicated in three so it's on of
one region now they are independent from
storage but each in each soin has
basically a dedicated storage for for
the computer sources now why do we need
this distribution for distribution of
work is one reason of course but the
other thing is also I mentioned no
downtime so we need actually application
for making sure if any kind of like
worker node falls down or we have a
network issue or anything that we have
replicas of the data and still the
database can operate now there is one
challenge with this if you have multiple
replicas you need somehow keep them and
think and usually the way that you do
this is that you define somebody who is
responsible for the entire group of
replicas and you need to make sure that
there's on one hand only one leader of
that group and then we also wanted to
remove things like massive communication
now communication is really expensive
especially in a distributed way if you
need to use a lot of communication to
keep consistency your database is
basically slow and in normal databases
if you look on a traditional databases
you have like two pcs the two-phase
commit protocols three pace commit
protocols to keep consistency we use the
two but in a different context now we
could cut down a little bit on the
communication by using something that we
call true time so what is through time
one
step back a little bit to explain
through time if you would now look to
your neighbor and compare watches and
you would synchronize your watches and
then you go out and you come back after
a week or let's say a month and it's
very very highly likely that your clocks
have drifted it's actually certain that
your clocks have to if that now it is a
it's a challenge like especially if you
need something that you can trust
globally and we wanted to have so we
wanted to actually use a timestamp
Global Times then global walk lock time
that we could trust and if we can
actually trust the curb local time we
can reduce the communication because we
can use this to ensure society
realisability and the global consistency
now as mentioned you cannot take like a
couple of watches synchronize them and
then go out and basically distribute
them in in multiple data centers you
need something else to synchronize these
watches so what is through time so since
we cannot really synchronize these
watches like to like a timestamp
precision we needed something to
quantify like an uncertain see the drift
uncertainty so that's what rare true
time comes in true time is not a single
timestamp true time is actually an
interval you get an lower and an upper
part now the lower bound basically
defines a timestamp which by the time
the true time that now has finished has
definitely passed everywhere in the
world and the upper bound is two times
them that definitely has not passed
everyone in the world and now like the
difference of that we call epsilon which
is basically these are uncertainty and
using these in the wall we can actually
ensure that there's exactly only one
leader for a group of splits which we
choose and use Paxos protocols to choose
that leader but we can make sure with to
time that there's exactly only one
leader for group of splits and by that
we have one guarantee to do the global
consistency now how do we get this drift
down like you can get this
to learn down this uncertainty epsilon
it's really important to keep that as
low as possible because if it's large
your database is slow again so we need
to keep that as small as possible
and what we did is we equipped all our
data set data centers with Hardware on
one hand atomic clocks and on the other
hand GPS clocks and now we use GPS to
synchronize our atomic clocks across
data centers and keep the epsilon as
small as possibly keep it in a very low
single-digit millisecond range now so we
synchronize them every 30 seconds and
then if you want to have a to time times
then we basically go out and say like I
want to have a two x times then we take
a subset of our time masters of our GPS
time masters and atomic time masters and
then we look if there are any outliers
if there are any outliers in the subset
we just kick him out of the crew and say
like we don't trust you anymore and then
we use the rest to define our epsilon
now some might ask like okay what if in
one case like in a zone or even yeah in
a so in like all time masters fail all
my GPS and all my atomic clocks fear
what do we do is spam are coming to a
grinding halt
spinners not coming to a grinding halt
we can go to a different zone and get
the timestamp the true time interval
from a difference so on but the epsilon
will be larger because you have to
calculate in all the round-trip times
for this call so a database will
actually slow down but it will not come
to an halt now let's have a look at a
query like a lie at the life of agree in
in floured spanner so in this case here
I have a consistent read where I want to
select all the events or I could say
like all the talks where the event name
is like devoxx 2017 now let's assume my
client request comes in through a
follower of my access group now the
follower will basically ask the leader
because this is a strong consistent read
will ask them like with the time some
years I have this timestamp of the data
it's just
the latest data for what was requested
for me now a leader might talk back and
say like yeah that's the latest data and
the follower can just serve the data so
the only data that moves between the
follower and the leader is basically the
timestamp it's not actually the data
which increases scalability again
now there's another case where you ask
the follower and the leader talks back
and says like nope you don't have the
most recent data but the response will
come with the time stem and say like hey
like once you see this time stamp on
your data you have the latest data and
you can respond to your client requests
and that's what the follower does is
just wait at that time and then sends
the response to the client now there is
a feature in loud spanner which is
pretty powerful especially in the global
replicated case and we call that
consistent stale reads or time bounded
state reads now they are still swing
strong consistent with the time stamp
basically that we use like with too time
we have always a consistent state and we
can basically travel back in time with
like in a multi version database system
kind of manner so what you define
basically is say like okay how much this
DNS do you allow or you can give
actually a timestamp within an hour and
say like here I want to have the view of
the data at times them now in my example
here I send a request and say like hey I
accept data that's up to 15 seconds old
now this is the most most most most most
most cases the case that the follower
already has the data so you should like
the rule of thumb is to use about 10
seconds their nests if you want to have
like a very high probability that the
data is already at your followers and
then you basically asked like hey am i
up-to-date and if this is qualified then
the follower can just respond with the
data without even consulting the leader
now what you've seen in all these
scenarios for the reads there were no
locks we don't lock for we don't need
transactions now in a readwrite
transactions of course we have to
acquire locks so the requests
the client requests get directly routed
to the leader and the leader acquires
locks responds basically with the query
result so in this case I might have done
select from something that I want to
update then the client does some changes
some updates modification on the data
sense this motivate modify data to the
leader the leader receive writes sends
out the rights to the followers as soon
as a majority of these followers comes
back and says yes I have committed this
data then we can release the locks and
we can send back transaction success if
it's not successful we send back a
transaction on board and the client can
retry the tonight retry this transaction
now this is scenario for one split now
if you have multiple splits that are
involved in a read/write transaction
then you use two-phase commit protocol
to synchronize between the and
communicate basically between the
different splits all right with this I'm
gonna switch gears a little bit and show
how to get started with cloud spanner so
your first start is cloud to google.com
slash spanners is a fully managed
service so unfortunately there is no
open source version that you can use
right now you have to use our fully
person we have fully miniatures now you
find a multitude of information on this
like comparison to traditional databases
or no SQL databases I don't want to go
into detail in that if you can find me
afterwards if you're interested to have
a discussion about that now I want to
point out one thing
loud spanner is not a one-to-one like
she lifts and shift from traditional
relational database you have to put some
work in because it is a distributed
system now there is some an article
especially if you coming from shorted
MySQL system and you want to know how
somebody else has done the move the
migration you can check out the quiz LED
block which is a technique very
technical details lock about all the
decisions that they made when they
migrated from a shot at MySQL there's
also always the questions like okay how
does Lord spanner
to the cap theorem does it defeated yes
or no and I gonna make a note at the end
if I have enough time for that
alright so everything starts in the
Google cloud platform console to the way
to get to spinners like either you can
use the hamburger symbol here on the
left and look for spent on the databases
which you find here you can click here
in the search box or if you're a whim
user you just use beam search shortcut
slash and then type in it's better now i
go to my spanner i can go create
instance in this case I do spare that
box and then I select region and
currently we support for regions I
choose your best say I want to have
three nodes I gonna click create and
within seconds I have my instance at my
disposal now I can create a database in
the schema here in the in the console
but I gonna switch to our Java client to
show how you can do that with the Java
client now first things just to check do
a path shake who is using maven here in
the room okay Gradle ascended okay so
I'm using maven here as well it's a
love-hate relationship I think everybody
knows but to get started with cloud
spanner you basically need three
packages first of course the Google
clouds better package currently in Co 26
beta and then you need the guava and the
Google Earth library you ask to library
to get started now I've created already
a class here fresh class as you can see
there there is on the left side a backup
spanner playground so if I fail to code
correctly I gonna cheat all right first
thing that we need is we need to
authenticate against the database and
it's pretty easy and cool actually to do
this Weiss is read doesn't want me Oh
of course we need a class function first
all right let's start with this all
right so first of all I need an
authentication process context so it's
very easy if you have installed G Cloud
SDK or you're running your code on GC
instance you have a default
authentication come context now you can
get that very easy with Google Trends
get application default and then you
basically authenticate it against the
project that you will currently have
active 50 Cloud if you want to
authenticate to a different project if a
service account of course this is
possible as well I'm gonna skip that to
do this right now here now the next
thing that I want to do is I want to
have my locks been our partner
playground so that's you spend on a
playground all right cool next thing I
want to do is I want to get actually
some options
so that's create the right stuff all
right cool so you need to get some
options we can get some information from
the authentication context so the first
of all I use the spinner options builder
get a service from where I use an
instance ID we just created spin order
box and I use a database in my case here
at that mode database now the first
thing that I want to do is actually I
want to create a database and I gonna
just create a function to do this here
and let's say Frank and then I called
this create DB all right so I have a
create the be run our instance function
I have my background I do create EP and
give it all the information that needs
so it needs a project which is my
options check ID I need an instance and
I need my database all right cool
so the first thing that I need here is
actually I need to create a admin client
so if you
do any kind of like administrative work
on on spanner instance like creating a
database or changing a schema you have
to get an admin client now that so so I
get my admin client here we get database
admin client and then I create a
database where I just provide the
instance ID a database ID and then list
of my DDL statements so in this example
here I create an account table with
three columns and and an address table
so I want to point out a couple of
things like that practice is basically
to you so first of all you don't want to
use any like monotonically increasing or
decreasing IDs in your tables so if you
come from a world from MySQL and you
have a use of auto increment add bad bad
for spanner the reason for this is I can
show this really quick is this so what
happens if you use something that is
monotonically increasing I mentioned
that we split the table
lexicographically key on the primary ID
on the primary key now if you use
something that is monotonically
increasing or decreasing like an ID that
you generate like sequence or time
stamps for instance and all your changes
all your inserts and updates where you
change the the primary key to the latest
timestamp well use the latest time
semester primary primary key all that
ends up in one split and the split is
controlled by one computer source there
is no multiple choice I'm splitting
overtime of statistics but there's only
so much that you can do if you always
append to your table so basically you
end up in one split and it better that
you restrict your horizontal scalability
now there is an overcome that we can do
there are multiple things that you can
do one of them is you can use a shard ID
so you basically can use a short ID here
for instance and with that you can
basically distribute between your splits
another option that I use in my example
here is I use a UID that I generate
and for now I actually gonna run this
and create my database now the next
thing that I want to do is I want to
insert some sample data so I just gonna
copy this because I'm lazy I gonna put
it in here and to insert data and then
we create did that work actually let's
see if it worked we have our instance
here we have a demo database alright
cool
one thing to mention also is that you in
spanner you have like schema updates are
actually online in a transaction so you
can while you wire your data is working
you can actually create schema or
involve your schema and make changes to
it and it's all handled within a
transaction so you don't have to bring
down your system or anything like that
now in this case I want to insert some
data so spare insert accounts now this
is of course only mutation so I need
something else first so first I need a
database client
alright so database client or DB client
alright new hope was not what I wanted
this is what I want
so so use a database ID and now here I
want to have the project the instance
and the database are you alright cool
now I need to write the V client I want
to do right
I want to write some mutations now what
is a mutation so basically in in spanner
in the Java client mutation is basically
what you do in terms of like changing
any kind of data B it's single columns
in your table or like adding rows or
things like that now what is special
about mutations is that they are counted
in a very special way so if you do any
changes in a table or yeah in a table it
so for instance inserting data in the
table and you have five columns in this
table one insert row in this table is
five mutations now if you have an insert
defined on this table you actually have
like let's say you index two columns of
your table then you have another two
mutations on top of this now this is
important because for for cloudspinner
if you get n at any point in time the
error message that you have exceeded the
transaction with patients limit which is
20,000 mutations per transaction then
you know okay that's not like rows it is
actually the cells that you change in a
in a table and in your indexes so be
aware of you if you use this now in my
divided line I want to have a write and
then I do memo tations and I could like
I get a timestamp back but I keep that
here alright so we're gonna create
insert some data now the next thing that
I want to do is really quick and
since I have only two minutes left I got
a cheat a little bit to be a bit quicker
so I want to do two more things I want
to have a read-only transaction and I
want to do a read/write transaction and
there are some special things with these
two things now my read-only function is
here I copied this over and just
explained the specialties around this so
if you do a read or Newton's action you
basically get a single use we don't need
transaction context and then in this
context you can execute a query or can
use or read api's to query data you can
nest like common practice you can use
query parameter so in this case I use
like a quick parameter for my country
code and then you just run this really
quick see your on this and get some data
back from our database if it worked all
right so that's basically we don't in
transaction now there are something that
you can actually do in this with only
transaction to to get like this time
spent our time boundness like this stale
consistent rate so when you get the
transaction context you can add a time
bound spent and then they like 15
seconds here and if I do with that
and execute that I get the same data
because data sorry older than 15 seconds
but this can be responded much faster
especially if you were in a global klo
global environment now last but not
least I want to show really quick the
read white and I'm not gonna run it but
I gonna show it to you because there's
one specialty here in a readwrite
transaction you basically provide
transaction callable and you write
basically the run function of your
transaction in a function why are you
doing this or why are did we decide to
do this currently in our client library
is that this way we can actually rerun
your transaction in the in the client
with our
having to do the real Shylock to or
logic for transactions all right one
node to its cloud spanner and the cap
theorem now we get often asked tacit
defeated and what I want to mention true
for that is cap theorem with Eric Brewer
came up with this many many years ago
that's the guy on the left no on the
right in this picture and this basically
says in a distributed systems you have
to pick two of three qualities of your
system one see for consistency so that
you always see the same data from any
point where you ask you your data an a
availability that your data is always
available and P is for partition
tolerance so even in like in the case of
a split sprain where you have like data
separated from each other it still works
so now if you if anybody comes to you
and claims their this distributed
database system is a CA system get
really suspicious there's only one CA
system or like one group of CA system
that are out there and that's a single
host database so traditional database
systems that are living on one single
horse they are CA but if anybody claims
to have a CA distributed system don't
believe them now you have a couple of AP
systems which are basically in the phase
of being partitioned or having a split
brain they still respond with data but
the data will be not consistent and then
there's a group of database system that
are CP which basically they default to
consist in the in case of becoming
partitioned so if you have like majority
voting and one partition of the majority
voting still can get a majority and in
their voting then this part of the
database will still work but the other
part which doesn't have majority anymore
will not serve any data anymore because
cannot be sure that it has the latest
data and just to close out spanner is a
CP system in the case of that there is a
split brain you will have unavailable
system but I want to say one thing for
that is like if you look at the
infrastructure and the architecture
which is underlying spanner it's set up
in a way that this case is really
really really unlikely and we beg bet by
an SLA of four nines which is four
minutes and 38 seconds for our regional
instances and five nines which is 26
seconds for our global instances in
availability so we didn't have yet split
brain in all the 1012 years that we are
running spanner with this thank you very
much for attention and staying two
minutes more that I over run if you want
to give me some feedback I would be
really appreciate that and otherwise I
made sure really nice evening thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>