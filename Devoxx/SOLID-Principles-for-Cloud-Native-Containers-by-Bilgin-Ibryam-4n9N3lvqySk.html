<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>SOLID Principles for Cloud Native Containers by Bilgin Ibryam | Coder Coacher - Coaching Coders</title><meta content="SOLID Principles for Cloud Native Containers by Bilgin Ibryam - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>SOLID Principles for Cloud Native Containers by Bilgin Ibryam</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4n9N3lvqySk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone
thanks for coming to this talk my name
is Bill guinea Bram I'm a middleware
high tech 14 for Red Hat premier
primarily used Java on top of kubernetes
and call it cloud native basically in
this talk I want to talk about
principles and rules you can follow
while creating cloud native applications
very often the way cloud native board is
described is by looking at it from
infrastructure point of view basically
how virtualization started at 210 Amazon
make offering it as a service and
OpenStack offering that as an open
source solution then Eric who's
introducing paths into the picture and
cloud hundred making it open source and
the current kubernetes basically
redefining that and calling it what we
know today as cloud native but I more
like to look at it also from the
application development point of view
not just from its infrastructure point
of view how it all starts with XP and
agile early 2000 with DevOps and how
micro services are built on top of that
then these two movements basically
coming together and what we have today
is micro services and we know pretty
well what we have to do in order to to
create micro services at scale and you
can see here that could cloud native
platforms such as kubernetes really well
addresses most of the needs of cloud
native and these two things cloud cloud
native and micro service kinda enforced
each other and increase the popularity
if we have to define cloud native the
best definition is the one I like is
long from Joe Beda one of the kubernetes
founders who says cloud native is
structuring teams culture and technology
to utilize automation and architectures
to manage complexity and unlock the
lofty base to what I'm trying to show
you is that cloud native is not on
drink put or only kubernetes it touches
on teams culture and technology in all
these areas in this talk we'll look many
at the technology side so cloud native
applications typically these are
application which follow the principles
of micro services then you run in
containers on some kind of platform
usually on the cloud so if you are if
you are doing most of these things
probably you are doing cloud native and
not realizing it now in terms of
technology stack is cloud native
platforms they they have very similar
stack they do provisioning runtime
orchestration application definition and
there is a quite a bit of choice at the
moments throughout there but the good
thing is that there is a consolidation
so the number of the popular frameworks
is actually reducing core OS we decided
to join kubernetes docker swarm decide
to support kubernetes hopefully same for
cloud foundry and who knows maybe by
next year the box will have kubernetes
and amazon ECS only on this slide now if
you want to create cloud native
applications there isn't much theory out
that moment the most popular one is the
fill factor up which is created for
Heroku platform and it's over decade old
and these are really good principles and
they are still valid but I think there
are really two White's but they touches
on everything so starting from the code
base that you have to use a place to
store your code and how you should build
and release and run your software
touches on the administration side on
scalability so basically lots of areas
and not focusing primarily on the
designing and developing application so
what I want to do is come up with a set
of principles for cloud native
containers and the inspiration came from
solid principles for the object-oriented
world so solid principles from Uncle Bob
day you
things like classes objects inheritance
and it says a set of rules that if you
follow you are more likely to end up
with a better design similarly in the
cloud native world where we have
containers there are set of rules that
you can follow in order to combine these
containers in a different way to come up
with a better design okay so the cloud
native world the building blocks are
containers for runtimes whereas in the
object-oriented world is our classes and
objects I want to go quickly to these
principles so the first one is single
responsibility or single concern
principle it is inspired by single
responsibility in the object-oriented
world and the idea is that what you run
at runtime is the pot which is
consisting of multiple containers and
you should not put everything in one
container you should not put multiple
reasons for for a change in a one
container you can have containers for
your main application and other
containers to do something next to it
and as a whole the whole pot represents
your service and when this principle is
applied the example of examples patterns
such as the sidecar where two containers
collaborate in a in a port to provide of
functionality okay the second and a
build time principle is about self
containment and this principle the case
that everything a container needs should
be baked in at Build time except the
stuff you have to attach it from time
for the different environments so your
container should not rely on anything
else other than the Linux kernel when
it's about to run on on a platform
everything needs like Linux additional
Linux modules the JDK the application
rundown everything should be baked in at
Build time this way you can avoid things
such as the locomotive pattern
the idea of that's an anti-pattern but
there are cases where there is a
observer in a container and you have
another container with your application
and you mount them bass guitar chamois
to each other at runtime and this is how
you can run your application and that's
really a bad idea
instead you should just bake everything
into one container that doesn't mean you
should bake things like on your
application and database in one
container instead it says everything
that the application is should be in
that container and everything the
database needs should be its own own
container even things like the cloud
database client if you have to do or any
kind of debugging tool you may need has
to be based in there the third can build
time principle is about image
immutability and this says that when you
are creating one container it should be
targeted for all environments rather
than creating container per environment
because there are also cases where
people create put their application
container and per environment maybe
Onishi environment they have slightly
different monitoring tool or some
different agent so that a great
environment specific containers but this
principle says try to have one container
that's immutable and don't change it on
different environments don't ssh to
different environments and change the
container set of same principle is also
mentioned in octaves like their
production parity or even Martin Fowler
has an article about snowflakes and
phonic servers which is basically the
same rule now the first principles were
about build time when you are creating
your containers
next for our more about the runtime
behavior of your application inside the
container so the first one here talks
about high observability and what it
means is these containers are these
black boxes that we built and run
everywhere but these black boxes has to
provide api's for
form turn if you are running in
container just with docker run that's
fine but if you want your container to
be cloud native that means for it to be
fully automated on platforms like
kubernetes you your container has to
provide certain api's for example the
kubernetes will check the health of your
profit the process and if it's not
healthy it will restart it
but we know in Java world you may have
out of memory exception the process is
still up but the application is not
functioning so to catch these kind of
cases you must provide readiness and
liveness checks as a very minimum in
your application in Java world a lot of
libraries which provide this kind of
functionality and this probably will be
part of JDK at some point so it's such a
fundamental thing that an application
that will be running in container will
have this kind of endpoint additionally
you should have basically provide a way
to gather metrics that's inside your
application
Thrun edges or with a service mesh or a
side card container and get tracing
metrics log aggregation all that but
readiness and lightness checks are at a
very minimum if we think about the
previous principle it's providing api's
for the platform torrent to read from
this principle is telling you that your
application also should provide api's
for the platform almost like to write or
to tell it what to do and here the two
signals your application should listen
for is sick theremins it kill so when
your container will be shut down okay
when kubernetes is going to shut down
your container it will send a signal
signal and by default you if your
application doesn't shut down it will be
killed in 30 seconds in the cloud native
for all applications can be shut down
and started for many more reasons than
the traditional one they may be just
scaling up or down on moving from one
host to another so it's really important
to kinda listen to these signals and
and react basically to conform the
signals coming from the platform and
maybe in the future there will be other
signals so just using the the process
terminology thick term cycle might not
be enough maybe in the future there will
be API calls or signals to tell your
application to release some of the
resources like memory it's using because
there is no other way to tell your
application to reduce memory it's not
like shrinkable resource as the cpu
maybe there will be command to tell your
application get ready to be replicated
etc next principle is about
disposability and yet is that you or the
distinct is also known as cattle rather
than pet but the point is you're a
container can be killed for various
reasons at runtime and you should not
rely on any particular instance of your
container so if you have any kind of
state as a minimum should be either
replicated or externalized basically
don't expect that the containers will be
running for a long time and ideally make
your containers to start up quickly and
shut down quickly because if they don't
do that they may be killed again and the
last principle runtime principle is
runtime confinement and the idea is that
rather than seeing the container only as
a single dimensional plaque box you have
to think that it also has runtime
dimensions for example every container
will need at runtime CPU memory and also
other things like networking file
storage and you have to declare those
dimensions and pass those information to
the platform because that will affect
how kubernetes will be scheduling your
application on which VM it will dictate
it what order the containers are evicted
how they are out of scaled etcetera
etcetera and it also affects the quality
of service of the container so if you
declare your limit and resources your
container
we'll be the last one that's kicked out
on a when its run out of resources if
you don't declare any of those it will
be the first one that's kicked out so it
affects how the quality of service is
propagated to the containers it also
helps to do the capacity management if
you know how much resource every
container needs you can easily plan how
much resource you will need in total per
environment and easily can calculate how
many VMs you will need to run all that
if you want to get read about this topic
on a PDF rather than slide so this link
we can get and read about this paper
which is online and just want to add
that cloud native development there are
multiple areas basically there are cloud
native primitives things like containers
port services health check so that's
like a new distributed application
runtime and these new primitives
requires new set of principles and rules
that you should follow in order to come
up with a more standard and better
design so these are the principles I've
been talking right now and then the
design patterns which is how you can
combine these primitives to solve
recurring problems things like the
sidecar pattern the controllers the
Builder adapter pattern and lots of
practices best practices which are which
may change with the next version of
docker and kubernetes or the practices
change more often patterns may become
anti patterns as well but the principles
are like the more basic fundamental
rules you should write for and with that
I like to thank you and conclude
so hello Amano Costa I'm walking for ice
backseat and today we will try to
discuss together all to deploy an o2
scale application with the web of
continents and Amazon is yes so first
I'm a developer I'm working every day on
back-end project using Java 8 spring and
spring boot and even if I'm developer
off to ask myself all the way the same
set of questions so first auto installed
application because we should be ready
to will the guys instead ating and
running the application on daily basis
Oh to test the application in a
consistent manner in all environments so
if something is working in that it
should also work in prod what is in
principle and also we scale the
application in which and in which
conditions so for example what kind of
behavior we can expect doing Black
Friday
what kind of spike off user we should be
prepare for and so on also to provide
IRL BT because we cannot afford downtime
so once we have these questions we can
already used containers because
countenance is design as immutable
components with enable consistency so we
can run the same image in the f-test and
production and reduce the hit that
something is working in tests but not in
pod also if we have enough compute power
we can spin up new containers in seconds
there's days in the past so we are more
reactive so the cloud providers may also
help us on this topic because now most
of the magic reproduce several regions
across the world and each version has at
least two ability zones so for example
here we have the map of Amazon Web
Services so as we can see there almost
everywhere one interesting thing with
this kind of cloud provider is that if
we have a service the service with big
success we are new up and you want to
expand across the world we can easily
expand in US for example in Virginia
also the cloud providers proposed and
implements more and more of managed
services with redundancy auto scaling
auto backup or to bestow and so on so we
can take advantage of these services in
order to have a better application
probably a simpler application and also
to sleep better during the nights
and last but not least you should
implement the application the white way
so even if you are running your
application in a container and in the
cloud you should be ready to scale so
you can take a look at the 12 facto hub
if you want to design a cloud native
application but today just keep in mind
that you should design an application
ready to scale or isn't aligned so for
example if you are managing the web
session do not manage the web session
into application window application
server do it elsewhere like distributed
service if possible and manage service
like Redis managed by a module of course
it's easier to think about that from day
one because if you are already in
production almost in production it's for
sure a very risky maybe impossible and
very expensive to adapt your application
so now let's take a look at the lc2
container service so easy s so easy s is
a container orchestration service manage
and implement by Amazon so Amazon will
provide us ES and we understood that ECS
is open burning innovation across
religions and on top of this of this
service on this managed service we will
build over cluster our tasks and our
service currently SES is only compatible
with docker and we may have on docker
image coming from a selfish registry dr.
web off from ec2 container registry so
ECI is a no-no another managed service
of Amazon are we new to cater he pulls
it away with a liability and so on in
less than 20 seconds so it's very nice
and it's not free but it's not very
expensive at all so
ECS is well integrated with the
application web browser will sit in next
nine ECS is able to one task like a con
job a bad job but also services like web
application so it's a long rivet that is
yes it's free but east yes it's build on
top of
ec2 instances so you will pay for this
or see two instances but the
orchestration itself is free so how does
it work so here we are focusing on the
more complex part of Ischia so learning
services
on this year's with the help of an
application with bouncer so previously I
said that ECS is bid on top of a see two
instances so the see two instances are
providing the compute power to your
cluster in this context the city
Winston's is also known as a container
instance so each time ECS will start a
new container instance because you click
somewhere in the Amazon console or you
have the auto scaling feature enabled
ECS will start the countenance tones and
will ensure that the ICS agent is up and
running on this countenance tense this
ECI surgeon is responsible to
communicate with the ECS scheduler the
ICS manager to also gather information
about what is going on the continent
stance and of course to start stops the
Dockers on this continent stands one
interesting feature of SES and some of
the orchestration service like this yes
is that you able to run several Dockers
on the same continent stands for store
for example in this case we have two
tools services running on on the same
cotton instance so we are even able to
run two instances of the same service on
the same machine so we are able to take
full advantage of our compute power to
do so there is a very nice feature in
Amazon Web service it's the dynamic poor
mapping so in this case when it's
enabled east asian will start the docker
and will assign an FM a helper to this
new docker doing so you will avoid
conflicts and so on of course you can
manage those poor yourself but it's more
risky and you should be prepared to
tackle conflicts and for sure you will
have conflicts if you are running
auto-scaling feature so when ECS Asian
will start a new service EGF surgeon
will register this service to the
application load balancer doing so the
application will balancer knows exactly
which service is up and running on which
continent stands and which poll which
FML PO should be used to reach this
service on the on the countenance tones
so the application of bouncer proposals
so some very nice
shows like else check so you can
configure dwell check and the
application along bouncer will ensure
that the application the application is
up and running correctly some bass bass
routing and so on and with this this
solution in fact the the application of
bouncer is proposing also a service
discovery in fact a server-side service
discovering so it is very nice and easy
to put in place so now let's take the
steps
let's check the steps to one and
application on ECS so first you have to
dock your eyes your application if it's
not yet the case then you have to build
and pull the docker image somewhere so
in this case I propose to use easier
because it's easier then you have to
define your easiest clusters or the
compute power in your cluster and so on
get the task definition so which took of
image the CPU tune in memory tuning and
so on then you have to create an
application load balancer and a target
group you know to manage running
services and access to this service you
have to get in easiest service and
deploy this service on the cluster and
then when the application is up and
running you can care about the skinny so
first let's check let's check what kind
of expected pho we will have with this
step so we have one cluster running on
to obey these ones so we have a set of
c2 instances providing the compute power
we may also scale them with the help of
the well-known scanning scanning group
feature we will get keep the docker
image from is here and we will manage
the access to a service with the help of
the application one balancer and target
good feature so first you have to dock
your eyes your application so in nothing
special in this example I'm running a
spring boot application so I will just
attach the spring Buju to the docker and
I will expose the default ports or the
default 8080 power and nothing special
about this yes we do not define any
agent or an if specific thing for yes in
the Tokyo then we have to build and put
the docker image so if we are using
easier as I said you can create a new
repository in less than 20 seconds once
created Amazon winner should
easier is available across the richest
ones and will also provide you the steps
you know to build the docker image with
the white name and push it at the white
place
so now let's get a nice years clusters
one mTEC s cluster so first you have to
create a security group so as it's a set
of Alouette unbanned outbound traffic's
and all the ec2 instances already know
compute power will run in this security
group you have them to create the empty
cluster attach the security group by
selecting the ec2 instance type so just
keep in mind that you cannot mix a
valence instance type so if you are
running T to me ho you cannot run or
thought it was moon in the same cluster
you have to choose a different number of
instances we can of course auto scale
this number you can also choose SSH key
panel to connect to the seat Winston's
and debug what is going on for example
with docker PS you have to select the V
PC and the subnet where the cluster is
enable then we have to create a task
definition so here we are referring to
the docker image we build and push to
start before so the torque service
repository with the latest version we
can tune the CPU reservation and the
memory reservation we can also pass some
add limit in this case if we exceed the
limit dude okay so I mean the docker
will be killed we have to configure the
power mapping so yeah we only
configuring the container pass or 80 80
which is in line with what we expose in
the docker file but nothing about those
because we will let Amazon manage it for
us we can also pass some comments some
environment variables so it's fine to
use environment variables to manage
configurations but avoid to use
environment variables to pass sequence
then you have to create an application
load balancer target whoops oh yeah I
propose to create an application load
balancer is turning on PO 80 even if the
application is running on po 80 80 we
set and Harpo and so on we don't care
because Amazon will manage it for us we
have to enable this application load
balancer in the V PC and several subnets
in nine weeks over ability zones then we
have to create a security group for this
application advanced also in this case
are proposed to allow traffic coming
from everywhere on po 80 and then a very
important step useful
traffic from the application advance of
security group to the cluster security
group otherwise the traffic will not
reach your cluster and you will not
understand what is going on in fact then
you have to create a target group for
your service you can take the chance to
configure dwell check so spring boot
actual actuator provide else endpoints
or we are just using this else endpoint
in the wealth check now if you want to
manage several health services and
several applications with your custom
it's fine you just need to define
several target groups so now we will put
all these things together store the word
across layer the top definition and the
application balancer in the ICS service
we will also define the default easy
weight count so for example in this case
when we will put this decision the this
definition on on Amazon Amazon will
start four instances and we'll spread
these instances on the see two instances
are caused the reaches wounds now we
have an application up and running in
our cluster so we can care about the
skinning so when you are scanning
something in in ECS you have to care
about two variables so further the
number of we see two instances so then
the compute power you provide to your
cluster so there you can use the usual
CPU utilization and memorization code
watch alarm available by default Amazon
you know to scale out of scaling the
number of instances or something custom
as you want then you have to define
your scaling strategy for the number of
running times the number of running
services on top of this cluster here
also you can use the CPU utilization in
an organization in this case its compute
based on the CPU reservation a memory
reservation we define in you all you can
at least in my opinion be a bit more
pragmatic and use statistics coming from
the application load balancer because
now if let's imagine we have a big load
and the number of running services is
too small so the application load
balance on will start to queue
some requests and we can hit hit these
statistics in cloud watch so you can
design a code watch alarm on these
statistics and
scale or the number of running services
get on the move the number of winning
tasks when you are queuing the request
during a certain amount of time and then
you can of course
kaeleen if you are not queueing anymore
and so on so today with the help of
containers and a good design so keep in
mind that you should really take a look
to the 12 factor up with a good cloud
provider proposing I already and and
managed service like ECS we were able to
improve the consistency the scalability
and ability of our application now if
you want to play with this at home and
you want to check all the details all
the steps to create a cluster from A to
Z just take a look take a look to my
blog post on this topic in this blog
post also took the chance to explain all
we can take advantage of ECS no two
paths to run Bluegreen deployment and
take full advantage of the on-demand
instances of Amazon and also o to
interrogate easily
ECS with cloud watch locks you know to
put the logs in code watch logs and
allocate them at a single place so
that's all for me if you have any
questions to catch me in the corridor
send me a message on Twitter thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>