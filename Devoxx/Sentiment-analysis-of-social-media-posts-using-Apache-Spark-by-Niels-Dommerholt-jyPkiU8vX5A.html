<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sentiment analysis of social media posts using Apache Spark by Niels Dommerholt | Coder Coacher - Coaching Coders</title><meta content="Sentiment analysis of social media posts using Apache Spark by Niels Dommerholt - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sentiment analysis of social media posts using Apache Spark by Niels Dommerholt</b></h2><h5 class="post__date">2016-11-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jyPkiU8vX5A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good evening everyone
I hope you had a wonderful day today
thank you for all being here it is quite
light our already welcome to my talk
about sentiment analysis with Apache
spark since it's my first time here at
deaf folks I'm going to very quickly
introduce myself my name is Niels I work
and live in the Netherlands I live in a
Marsha small town close to Utrecht
together with my daughter's my
girlfriend and I work for a smelly very
small 30 people a very cool consulting
company called Jade riffin I like the
name sort of implies we specialize in
Java consulting work together with
really fun and smart guys if you're
looking for a job he's come to us
because we need more people what do I do
my daily day-to-day job is working at a
small fin tech startup @anjghie the
biggest Dutch bank as you can see we
sort of check all the hipster boxes in
in the when it comes to our stack
especially spring good micro-services
backed by cassandra we do all kinds of
cool data science stuff that runs on
Apache spark and I've been working there
for about a year now and this is also
where I basically you ran into a patchy
spark and started finding all kinds of
cool stuff to do with it like important
genetic algorithms on to spark and stuff
like that so let's start with sentiment
analysis what is sentiment analysis it's
a it's quite a big word for I'd say a
very small thing it's not not complex
but in essence what you're doing is
trying to figure out if you have pieces
of text and that can be documents or
tweet tweets or anything else and try to
figure out if it's a positive or
negative
texts so why do you want to do this
well tweets are actually a great way if
you know if they are positive or
negative to see if yeah how you
customers respond to changes in your
product and there's a lot of vendors who
do something like that especially for
for tweets so are they doing basically
is figuring out if pieces of text that
are sort of like fixed lists like a word
like happy that's a positive word in
most cases or this product is horrible
horrible it's in most cases a negative
word and try to extract the meaning from
from that so I'm going to first show you
a type of thing that you can do with it
and later on what I'm going to explain
how it works so I need to find something
that has a lot of text and has people
kind of divided on the subject so oh and
it's gone and there it is again
so this is the I hope you can it's going
too fast to read actually this is the
Twitter feed so this is a small spring
boots surface with an angler front end
you can see my incredible CSS skills
here so what it does is analyze all the
tweets at the comic pass that have the
word election in it now we're very close
to the American elections coming to a
close so as you can imagine the the
sentiments are quite divided on it
the yellow ones are neutral comments the
green ones with the appliques are the
ones that have more positive worse than
by a negative words at a negative ones
are the inverse so how does this
sentiment analysis actually work I opted
for a very simple approach this is a
word less that is expected from a number
of different sources you have a very
cool sentiment analysis
this course on Coursera there's also
something that's cool to the center net
word list also pretty well known and it
basically it's a long list of positive
and negative words and what it does is
count them and sum them and you end up
with a score of course this doesn't take
into account inversions like not I mean
I'm not happy he's probably not positive
but for actually stuff like tweets this
this works better than trying to do
actual natural language processing
because natural language processing is
actually very very hard and there's all
kinds of inversions and inversions that
are a certain distance from the bird
you're trying to figure out so I opted
for a very simple approach which works
quite well for tweets and also for the
next bit that I'm I'm going to show so
the next bit of course is a Betty spark
so who if you have worked with spark
very few I'm really happy to see that so
that I'm actually probably tell you
something new so spark is well framework
library from the Apache foundation and
what it does for you is make it's easier
to paralyze well number-crunching does
anyone of you use Java 8 in daily in
day-to-day production ok really cool to
see you guys use the streams API the new
streams API ok cool and if you use the
streams API this will look really really
familiar to you this is also sort of the
goal of a spark is to make it quite easy
to get started with it giving you bit
more flexibility than the old method use
I'm not saying that program is part is
easy because there's a lot of God share
some pitfalls and I think and talk after
me it's going to go a lot deeper into
that but it's really easy to get to get
started with so we want to do something
with spark but and yeah of course a
little bit of data
not very interesting we want a lot of
data so I don't does anyone of you use
reddit yeah does do people waste way too
much time on it I mean don't you yeah
okay cool I'm not the only one awesome
a while ago somebody downloaded all of
Reddit it's a huge data set it's a
terabyte compressed of text basically so
I downloaded the entire data sets on my
server and I figured well let's let's
get started with one small problem and
this laptop is a really awesome Apple it
has an SSD but it's will never fit one
terabyte and so I had to take a little
extract so I used just one month of
adjacent data it's still 5 gigabytes
compressed so you can imagine how busy
gets cited and did my analysis and with
that so I'm going to actually do some
live programming it's my first time so I
don't hope it's all comes crashing down
oh that's for this often next one
alrighty let's do this one okay so spark
basically if you want to get started
with spark you add a maven dependency
and then you have a spark library and
you can immediately start using it's in
your development environment and this is
what I really like about projects and if
any one of you makes software please do
this for me as a developer have me test
it in my ETA that's so much more awesome
than having to create something like a
zookeeper cluster just to be able to run
your software for example ok I start
with a spark context this is basically
sort of the configuration and because
I'm telling it here connected to master
local the master is will actually wear
the your work is pushed and if you say
local and in this case I told it to use
one fret if you want to know why come to
me after
it actually spins up a local spark
cluster for you that you can immediately
use and connect connect against and
deploy your code so this is actually the
big one the big file so I start with
spark context dot text file and this is
actually what allows you to read a text
file you see here that's actually be
zipped file but spark can handle text
files directories be set files etc for
you it's really awesome
easy to get started with so let's take
just the first 25 because this is the 5
gigabyte one and do and this actually
returns a list so it's this is where you
actually pull data from your Cassandra
cluster into a list in your local driver
and for each system yes ok so now it
spins up actually the cluster I disabled
most of the logging output because it's
a lot and it would drown out the output
from my thing okay so you used C the
first 25 comments of February 2015 on
reddit so well that's not very useful
because now we just have strings so
let's first map them to a JSON object of
course I created this in advance so it's
little utility function function to
comment which basically just uses
jackson the mapping librarians map it to
a podium basically and i can do the same
thing take 35 and
the downside of doing it locally is that
you actually have to spin up the entire
instance so if you don't want this delay
you can actually download the docker
image or pull a docker image and just
run it on your machine and then you can
connect from your codes to that machine
and it will work exactly the same okay
so this is basically exactly the same
thing but what you're seeing is not the
JSON but it's the two string of the
object that I created so it has a time
and it's actually sorry it's actually
January 2015
it's the subreddit I hope it's sort of a
readable the knowledge that it doesn't
have a score yet and then we have the
text so let's actually also do the
scoring bit well there are we first have
to clean up this mess a bit because
there's a lot of deleted comments and
deleted comments don't have text and
without text you can't analyze it so
that's we don't want the deleted ones
and again map it and I use a map
function to go through the analyzer I
have to create something first and it's
this is basically just a hash map I'm
not going to go into the code because it
actually just loads that word list which
push it into a hash map and every words
that comes in gets checked against that
detachment rights
so this is a map so basically what we
did here we first met it to an object an
object filtered out deleted ones ran the
analysis which should Excel assign a
score and then again take 25 and print
them so let's see if this works
it does so instead of the know that you
saw before you now see that it's
assigned a score of a score object to
all of the comments that were passed
through the in the function the cool
thing is here and you don't actually see
it but what happens is that this whole
bit gets C relized by a spark and gets
sent to worker where it's actually
executed so one of the pitfalls is that
all the pieces of both the classes that
are code and the classes that are just
transferring data they have to be Cheryl
icicle because that's the mechanism that
spark uses to transfer your work to the
total worker we also really cool if I
replace this for this take 25 for
collected will grab everything just
gives me some time to show you a pretty
cool feature that spark has included
should be up now yes there it is so what
it's doing here is when you start spark
it also spins up a console but you can
keep an eye on only processes and this
will take a long time and it hasn't even
properly started yet later on you'll
yeah I'll not go too deep too into it
because I have only 15 minutes left but
you can sort of explore the complete
graph that's spark bills for you and
here and also if you have a cluster of
course you can have multiple jobs
running at the same time and you can
expect them see how yeah how long
they've been running except it's a trap
all right but instead of collecting I'm
going to save project file and then we
can move on to the next step the object
file is basically sort of a cache that
you can sort of reuse and I'm doing this
year because icons analyze this file
year in parallel
I don't know if it's a buck I think it
is but I can't use more than one core at
the same time so I prepared this and
also prepare that with a smaller object
file that's sort of like I believe 100
of these data sets to be able to
actually have it complete in time so
moving on to the next bit I'm starting
here with instead of reading a text file
I'm reading basically object file that I
created in a previous example and I have
the same analyzed comments here again
and I know that they're clean
so what I want to go and do here is
present a list of all the top thousands
separates and counts the total the
negative and the positive comments and
also you can make percentages of those
subreddits and show some neat statistics
so first let's start with this comments
RDD so it's a resilient disabuse the
data set is basically sort of sparks
version of a stream so a sort of Java
RTD if you put spin they're basically
sort of have the same thing in Java and
so I'm going to do a pretty basic basic
I guess a MapReduce here so I'm first
going to map the comments to a tuple of
subreddits key and the count one so this
is the comments
- I can't type anymore
come on and get separated
I'm going to reduce it by key so we have
to we get tuples which are separated for
example yes and one and you do a sort of
typical Map Reduce function so you first
you then combine the results into a
single pair and this because we're just
counting the totals it's a relatively
simple sum so spark gives us only the
value here and I can just a plus B so
this sums up every a unique Fe to papaki
and also want to filter it to remove the
ones that are the small subreddits we
have a very long tail of very
infrequently used once I want to remove
that as soon as possible to speed up the
process a bit so you see here that once
all four tuples get just one and two so
this is Java bear RDD okay so this one
this is just the total so we have to
total some of the other kinds of
comments in these these separators but
we also want of course the positive ones
I'm going to do basically the exact same
thing I'm just going to copy pasted it
but
and first filter only the positive ones
it's a sentiment positive so this will
give me only the positive ones and also
once the sensor negative ones negative
okay but as you can see here we now have
three completely separate streams and of
course if we want to have like a
positive percentage you need to combine
them this is what joined us so when you
have separate steam streams of a pair
rdd's our spark can for you based on the
key of that pair and combine them
basically into a key with a tuple of
tuples and we do that twice so we have a
key and a tuple of tuples inside a tuple
with just a value and I'm not going to
I'll start with so sorry I push the
first join so basically in that we now
end up with one RDD one stream with all
the information combined cool but of
course we am going to cheat a bit
because this part isn't fun this is the
oneness of tuples of tuples inside
tuples and yeah it's a it's a bit of
sort of the horrible horrible bits of of
having to work with them so what's I
think I missed a few
yeah you know what I'm going to just
grab the entire stuff to make sure I
don't make any mistakes
fix this one and I'll explain it what is
actually happening okay so I have joins
the negative and positive ones but I
want to have the top thousand I don't
want to have the entire list I just want
to talk to effort of thousand so I have
to I want to sort them and I want to
actually use Park for this and on my
local machine to sort them because these
bases can be pretty big but the way if
you want to sort something spark you
actually have to sort them on the key so
what we have to do here is refers to the
key because you might remember we first
have to separate it and then the count
we're going to swap them sort them by
key and this false is just that it's
descending and then we're basically
going to map them back again to topple
fear which before which has subreddits
totals positive negative counts take 25
and then prints and then print them
again okay and I'm going to run this and
I hope it works
okay closer local closer I started so
this should be okay so what you see here
is that when you actually only when you
do the stuff the thing that bit where
you draw the data back to your local
machine this is what take or collect
does then it actually goes and deploys
your task to the cluster and executed
and you see here all the different
stages that is has has to go through to
to can do all the calculations it's a
small set which you still see it as to
reach through a ton of data I only have
six minutes left so I'm not going to
wait for it because it takes two more
half minutes
unfortunately so I'm going to continue
the presentation to show you the actual
results when I did this with with the 5
gigabytes February or January February
2015 data set I'm going to stop it
because it's responding very slow
because this actually does take 8 cores
and you probably can hear my laptop
lifting off should cool down a bit now
so to resume sort of what we did in the
previous example we have the the analyze
comments we have three different streams
the positive the neutral and the
negative ones do the MapReduce we filter
them to take down a data size a bit and
then we join sort and take in this case
what I did the first 2000 so the results
of that 5 gigabyte data set us credit is
by far the busiest one it has over 4
million comments in one month and
there's a bunch of other very big ones
if we look at the most positive subs so
basically surface' subreddit or subject
make a prediction is an incredibly
positive sub so if you want to have a
good day then that's this is the place
to be there's also some porn there also
very positive and of course Pokemon we
doesn't who doesn't love Pokemon most
negative subjects news world news these
kinds of Serbs can be quite negative
also Europe is quite a negative sub so
people who have used
reddit's probably see some familiar
names in here so this is all fun this is
it took me a bit of time I did also did
some other stuff but what I did not do
yet is use all the data and so it's
something I still want to do but I will
have to rent an ABS or a Google compute
engine instance to run all the data
there's a lot of fun stuff I still want
to do like positively over time how
positivity relates to the voting and
people do in different types of shops
and stuff like that so still a ton of
stuff I can do well for me the
conclusion I'm a Java developer I ran
into spark like a year ago and I'm
having a ton of fun with it and I'll see
all kinds of datasets that I want to
analyze with it maybe maybe just me but
I think it's fun it's easy to sort of
not use get started with there's quite a
few gotchas and so of course if you have
a lot of data it's a great fit if you
have not a lot of data it's probably
easier to just do it on your local
machine and when it comes to sentiment
analysis I think that's it's very
underrated there's a couple of vendors
we do really cool stuff with sentiment
analysis on Twitter for example but also
Facebook and I think it's a very good
tool to use if you launch products and
it's going gone past your better
customers into the world and then you
don't know what happens anymore so I
think it's a great tool to look into it
ok that concludes my presentation anyone
have any questions here at front first
now it's now it's it's filtered on the
reduced data set so when you start out
with are all the individual comments and
then you end up you map them to those
pairs and then you reduce them and you
don't have the individual comments
anymore so you don't have the individual
negativity or positivity anymore either
you just have a list of or basically
account for certain separate what else
can I give my opinion I think it's I
hope it's it's Hillary the tweets are
coming too fast for me to to view all of
them I'm afraid anyone else I have
almost one minute left so I have some I
have been playing a bit with Scala
myself just bad projects but I have not
used
Scala in combination with Sparky yes I
think the answer will be yes because
there's a lot of Scala stuff you see in
there like the tuples and I'm on a
percent sure that will be issued to use
in Ghana anyone all righty okay thank
you very much there was a lot of fun if
you have any questions or want to play
around with it the the projects is up on
github if you have any questions just
poke me on Twitter or on get up or
whatever so thank you very much for
having me here and it was a lot of fun
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>