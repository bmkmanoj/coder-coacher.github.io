<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Graphics Something Android Something Performance by Romain Guy and Chet Haase | Coder Coacher - Coaching Coders</title><meta content="Graphics Something Android Something Performance by Romain Guy and Chet Haase - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Graphics Something Android Something Performance by Romain Guy and Chet Haase</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R6kqZQusv1U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the talk when we submitted
something a while ago we submitted
literally something something Android
something something which I thought was
amusing and also true we didn't actually
know until last Friday what we were
going to do two weeks ago we had a
really good idea and then we got a
better weekend and we realized we're
never going to finish that in time so we
switched that's why I like submitting
titles like this so it turns out that
our talk today will be on graphics and
performance I hope that suits everybody
otherwise there's lots of other talks
going on right now frame metrics is an
API that we introduced in n that you can
use to get way more detailed information
at the app level
turns out that this is the capability
they already had on the platform you
have seen it in action well at least I
hope you have if you run a DB shell dump
this GFX info you get a bunch of numbers
on the screen we'll show some of that
later
we now take that same information and
can push it into your app so instead of
actually needing to be connected to the
device and run it you know at Build time
or at development time to see what's
going on you can get this information
and in your app at runtime and aggregate
and do whatever you want with it so when
you're using the device and you want to
do profile GPU rendering you can click
on that helpful radio button on screen
as bars and then you get this nice
little colored thing there and if I had
done my homework completely I would have
replaced this slide with a more helpful
screenshot which would describe what all
of these colors are we'll get to this
later because you can actually see in
the data what's going on there but
basically these are the different
portions that go into the rendering and
UI interaction in any given frame you
have processing of input processing the
animation actually constructing the draw
commands and then pushing them to the to
the GPU all of which make up the frame
and then you want to try to stay below
the magical green bar that represents 16
milliseconds or 60 frames per second so
we've had that one for men
mater releases I think you did that in
probably honeycomb or ICS i sicily and
then it improved over time I think it
originally was just text output that you
could get at the command line and then
Ramon wrote the thing probably in ICS to
to pop it up as an overlay which is
really weird when it happens when you're
not expecting it it's in like every
window on the screen and then you
realize there's many windows in parallel
all the time QA felt a lot of bugs about
this window so it was a knock defect of
the rendering system well it sort of is
so hopefully you've used that and you've
seen it and you've seen better
screenshots than that one here's what
happens when you run it at the command
line when you say dump this GFX info you
can run it overall and it'll tell you
the same information for all of the
active processes or if you run it on a
particular activity then it'll tell you
the information there so I can step
through this really quickly Roman can
probably give you even more detail it's
aggregating all of the data it's on a
per frame level it gets all the timing
information for each one of those things
that I talked about input animation draw
execution and it a great so into a
histogram so you can see this this looks
kind of nasty but you can sort of you
don't have to extrapolate too much to
say well I could just take the state and
I could dump it in somewhere and look at
it in an easier way but this basically
tells you how many samples are in each
one of these buckets that are aggregated
over whatever period of time it was
collecting data over it also gives you
helpful information like not just what
was your average frame time that's nice
but how many frames were way higher than
you expected and how many frames like
essentially if your jank and you know
one out of the thousand frames you know
either it's erroneous data because well
you happen to be you know during a
transition between activities or
whatever or it's a glitch in the data or
you know something got dropped there or
you know okay you were too high ones
that's that's fine but if you're
actually missing a lot of frames if you
have a high jank rate that is something
that you should be concerned about
yeah and the key takeaway here is that
we hear about frames per second a lot
when it comes to performance of you know
applications or games friends per second
are actually not that good
information because they are by
definition average over a certain long
period of time at least a second and you
could have dropped a few frames but
still get a good frame rate so we don't
like to use frames per second especially
on Android where we render only as
needed so you could be getting 10 frames
per second but it might just be because
you have an animation on frame on screen
that's only requiring 10 frames per
second which is why we used this
information instead so when you look at
this as this dump and if you want to
quit dashboard from it i really suggest
that you look at the percentiles
percentile numbers here so we can see in
this example that the application
whatever measured here sis anyway oh we
should tell someone about it that's bad
azam bad ha more than 50% of the frames
were above 19 milliseconds which is more
than the 60 millisecond threshold if you
want 60 frames per second so you look at
the percentile and you really want your
your 90th percentiles in higher to have
good numbers there those numbers are
really really bad so as I said a lot of
these capabilities have been in the
platform at least since ICS but there
have been improvements over time the
aggregation of the data into something
probably more useful into these buckets
this histogram
I think dates from n the other
interesting thing is we're always
collecting this data at a native level
very low level like we're always
recording that timing information I
believe when Rama first implemented it
we were only turning it on with you
pulled up the dialog in the device and
you said I would like some graphics info
and then you would do something for a
couple of seconds and then you would
spit out the data but we're doing it
continuously on your behalf which means
that at anytime you can query the system
and see what's going on and we'll have
the data for you know that period of
time preceding it right and we're
expending these kind of capabilities in
oh we're adding the in OpenGL extension
that would let you query timing
information outside of your application
so there's those values are everything
that your app is doing but then there's
the work that the GPU and surface
fingers you must do after your app is
done and we cannot query them for
everything yet especially if you have a
custom surface you or you custom GL
drawing or you know a short game or
camera app so now thanks to that OpenGL
extension you'll be able to query
surface finger from
information that you can use to you know
make pre - boards so this is the new
presentation of the data the old and
still existing presentation of the data
is the sort of per frame duration so
when Rama first did this I think this
data would comprise of like three or
four different metrics it was total
duration of the frame and then the
larger chunks of actually creating the
drawing commands and then issuing the
drawing commands to the GPU that was
probably the core three that we started
with now there are a lot more there's a
lot of really confusing numbers there
don't even try to parse this by eye that
would be stupid
instead you can imagine these are comma
separated values put it into a
spreadsheet there's going to be some
header information about what the timing
values are that we're actually spitting
out here then you can start to analyze
your data right so you get this on the
command line you can basically say dumps
this gfx info of a particular package
frame stats you give it the frame stats
command and it's going to give you all
of these
excuse-me durations for the frames and
it's going to give you a whole lot I put
some helpful little T's down at the
bottom those timestamps of the durations
for instance okay they are time step
stamps
there's a really big number boy hour
duration suck yes time stamps all right
so that's what we're doing that's what
we're tracking at the native level and
then we aggregated into the helpful
histograms but there was no way for you
to get this outside of a DB or outside
of looking at the colored bars and the
devices until n now we have an API you
need to add a callback to a window and
your callback is going to be called back
on every frame and then you're going to
listen to for these framing for these
frame time events and do whatever you
want with them we're going to tell you
these timestamps or actually duration
values and then you can collect them
over time aggregate them put them into
histogram buckets
you know tease out the ones that you
want analyze them however you want you
can upload them to your own data cache
somewhere analyze them offline across
your user base or some select user base
and so the basic API works like this you
get the window you add a listener to it
along with a handler your listener looks
essentially like this on frame metrics
available you will be called on every
frame frame metrics is the
see data structure that you care about
drop count doesn't refer to drop frames
but rather drop data elements this
happens really really frequently so if
you waste any time in this call we're
going to drop some frames because we
would rather drop the data then wait for
you to come back and completely screw up
the data so we're going to tell you
anytime we call you here's how many
here's how many frames that we had to
drop in order to get you the new data
and then inside of there do whatever you
want the suggestion is you quickly copy
that data and then chuck it off thread
to be analyzed aggregated composed
whatever you want to do with it so you
can quickly return because it's waiting
for you to return and is dropping data
on the floor while you are not returning
the data that you can tease out there is
whatever you want out of that selection
that we went over before you know input
animation total duration obviously total
duration some of you're going to care
about but if you care about others some
of the other sub elements in each frame
that you can grab those as well all
right I love this math if you render at
20 milliseconds of frame you can still
get 60 frames a second we actually
changed the way the math worked in n was
this mr1 or this is how it was in ml
women Bluth mr1 so neat little
performance hack I was going to quickly
walk through and try to explain this
it's a bit tricky so here's a
complicated graph we will step through
incrementally you can imagine these
vertical sink pulses coming through
every 16 or so milliseconds so each one
of those is going to hit and I'm going
to show you the sort of the the two
models that we've worked in
traditionally over the last several
ESA's that's someone we referred to with
previously and then hardware composer
too so we would have touch events
processed on a vertical sink we would
also have UI happen on the vertical sink
so basically everything that sort of
starts in concert we start doing this
work we also process swaps so surface
flinger on a vertical sink would say
okay now we need to actually display
this thing on the screen so we have the
swap happening on every one of those
frames which means that's the window
where the user can actually see the
effect of stuff that we had to deal with
in order to create the new visuals on
the screen so we have
that happened way over there in the
first vertical sink on the left and then
the user is going to see the result at
one of two times either 32 milliseconds
later or 48 milliseconds later and the
reason is they touch the screen the
touch events get aggregated sent up to
the Java layer and then we process those
events we make any changes in the UI
that we need to we do layout we do
rendering we tossed up onto the render
thread etc etc eventually if we're
double buffered we will flip the buffer
on doing a mimosa so we'll flip to the
back buffer at this point and then
surface flinger is going to display the
results here so in the best-case
scenario we render within 16
milliseconds and the user sees the
result 32 milliseconds after the touch
events got aggregated right in a typical
scenario in a typical application
occasionally you're going to drop a
frame and when you do we will go into
triple buffering mode which increases
the latency for touch events but it
makes things smoother because we're no
longer like waiting for buffers to be
free if they're going to take a long
time to render but it means there's an
extra 16 milliseconds built into any
interaction scenario which means they're
going to touch the screen here or the
touch events are going to be aggregated
there and then 48 milliseconds later
they're going to see the results so we
go into hardware composer 2 and now
we've shifted things a little bit so
touch and UI stuff now starts getting
processed for milliseconds in this is
kind of a mystery to me to understand
but essentially what we're trying to do
is reduce the latency between when the
events are actually getting processed so
we're given a little bit more time on
them on the front end for touching
events to actually be aggregated before
we start doing anything with them so we
give that an extra 4 milliseconds and
then 4 milliseconds after that we can do
a buffer flip with surface flinger but
it's still doing the swap on the vsync
because that's actually when the pixels
get displayed and now what we have is
the user seeing the results that's still
the same frame over here and that is a
more typical scenario so what's actually
going on with the magical 20
milliseconds is now we have four extra
Mel
the seconds for them to touch the screen
and then for us to react to it and then
we have an extra 4 milliseconds built-in
before we have to get the buffer from
surface flinger right so now we have the
16 milliseconds in between these events
plus the extra 4 millisecond Delta
between the touch event and the the
sinks on which we're actually doing the
buffer flip so you effectively have 20
milliseconds to process that information
and then the user is going to then see
that result there and probably not get
triple buffered because instead of your
application having to stick within 16
milliseconds we now gave you an extra 4
milliseconds of slop so it's more and
more likely for the application to have
enough time to stick within the frame
boundary and for us to not flip into a
triple buffering mode so now instead of
seeing the result 32 to 48 milliseconds
later the users going to see the result
of touch input interaction 28
milliseconds later so yay all right a
sink layout inflator that's a new class
that was introduced in super February 24
it's in the visual the new package and
the name pretty much says it all it
inflated you in an asynchronous matter
it spawns a new thread that has huge
ethical thread number 2 when you look at
providers which are real difficult to
find but that's where it tries to
inflate views so this is what the code
looks like it's fairly simple you create
your letter inflator you have to give it
a context he I'm just passing the
activity which is probably a bad idea
because it's going to keep the the
reference in that context for for quite
a while and then you call the insight
method so it looks very similar to the
Leavitt inflator the regular loyalty
layer the big difference so you still
pass the resource ID you still pass a
parent but you don't have that Doyen at
the end and we'll see that one of the
limitations of the new Asics inflator
what you pass instead is a callback and
here I use the lambda notations which
really confused chat so the the listener
the callback gives you back the view
that was inflated the resource ID that
was inflated and the parents that you
wanted to inflate into the the async
level inflator will never add the
inflated view for you
to the parents you have to do it
yourself so here what I did in this
example is I'm actually playing
animation in the UI and while the
animation is running
I'm in flaming the view so I just have a
bit of logic to say you know if we're
already done with the animation then I
can add the view and if the animation is
still running I want to keep the
reference inflated view to add it to the
UI once the animation is done running so
it's a very easy way for you in your
application as if you know that you're
going to have periods of idle periods
for the user in the app that you can use
that time to prepared in the usual
natural for instance you have a
viewpager you might want to inflate the
the pages on the sides ahead of time in
the background now this is a really
useful but there are a lot of
limitations to this API the first one is
that the inflator must be created on the
UI thread so so far that's you know very
similar to a lot of the API that you
have in the framework inflate views
cannot create their own handler they
also cannot call looper that my looper
that's that will crash so if you try for
instance to inflate a webview
using async Levitan slider into a
tempting because web users usually very
expensive to create you're just going to
crash when it comes to thread safety the
UI toolkit never made any promise we
never word the code with this kind of
use case in mine so who knows what's
going to happen it might work it might
not work
we don't know it will the behaviors will
probably change in the future if you
just remember of course in the device no
rights condition threat alert the
reasons that we encourage application
developers to test their applications
you can see that as embarking on the
interesting journey it's an adventure
with us like I mentioned the new layout
inflator does not add the view
automatically to the parent you let
yourself for these reasons you cannot
use that inflator that secretary I think
very few applications probably rely on
that fragments are not supported I'm
sure Jake is really burned right now and
another fun tidbit is you know it tries
to be helpful so if for some reason
you're inflating a view that is not
thread safe and it happens to throw an
exception you're actually lucky because
most of the time you know threading
issues do not lead to exceptions but if
it does happen to throw an exception it
try again it will try the inflation
again but on the UI thread this time and
maybe this time it's going to work most
of the time it probably won't it's a
real exception so it's just going to
it's just doing the work twice and just
delaying the time when you're going to
get the exception in your logs
so given this many caveats it's a wonder
that we're talking about this at all
I think the real thing to be aware of is
if you have a custom view that's when
these kick in if you're using one of the
standard especially simple views we've
tried it with that like the thread
safety is not an issue we're doing
things in the right order and okay I
feel worse that's why the Moldy
detrimental yeah
all right still worth testing but for
the most part it should work with
standard views but in custom views we
have no idea what you're doing in your
code so then you'll have to see whether
the caveat supply which leads us to
systrace we've talked about this tool
many times in the past and just out of
curiosity how many of you have ever use
systrace okay so the rate that's the
typical answer either code from
tomorrow's our you know profiling /
performance expert on Android and this
is what he has to say according to him
there is no other profiling tool that
exists on Android and it's kind of right
I've heard that there are some new
profiling tools to check out but this is
a really good one so this trace is very
easy to use it's part of the SDK it's in
the platform - tools directory to invoke
this choice you have to make sure that
ADB is in your path in the path of your
shell it's just a Python script so you
call systrace racers py and then you
have to specify the tags the different
categories of events that you want to
walk if you don't pass any event
depending on the version of Android
we're going to log like a different
default
different set by default what I like to
use is those tags so frac shows you the
frequency of each CPU it's extremely
useful because what I don't like about
some of the performance tools out there
for instance they will show you the CPUs
edge they will say oh you're using 100%
of the CPU and you can you know it leads
you to believe that you're doing
something wrong in the applications
using too much CPU but if the CPU is
running at 200 megahertz it's fine your
applications is running really good
now if using 100% of CPU h2g at
two gigahertz that's a different problem
but that's why you want to see the
frequency of the CPU and also once you
start looking at distresses you'll
notice very often you may notice Jenks
in your application that could be tied
to the CPU frequency changing so we try
to you know those are times that we need
to fix on our side usually and we're
constantly constantly changing the
scheduler and the way we boost the
frequencies and the way we go back to
idle frequencies but it's a very
important piece of information to have
before you make any decision about
optimizing your code scared is for
scheduler so that's very interesting is
going to show you bad processes and what
thread are running on what CPUs and it
can be very useful to to develop
different profiling issues very often
tied to processes of threads running on
one another gfx will show you what
happens with the drawing subsystem view
will show you everything that's ready to
view so measurement layouts inflation
where is everything related to resources
so if you load Ruggles stuff like that
they're gonna show up in the traces and
WN so window manager will see extra
information it's between surface finger
to give you a better idea of what frames
are scheduled when and how much time
they take and when to go to the hardware
just to be clear like the things you're
specifying there are basically the
different modules in which we have added
traces so the reason you're getting any
information at all is that we've gone
through and manually added tracing code
to different things during you know the
processing of Android code so we have
tracing for animations and for view
processing and the stuff that you listed
here there are many more modules that
you could choose to include but
basically this turns those on and says
okay now spit out the information from
tracing in the following modules of code
and there's another module called app
that's for applications so if use
Android OS dot trace it's a class that
only has I think two static methods that
you can use to are tracing to your
application you have to add the app
category to see those traces appear the
nice thing about trace is that it
they're really lightweight so there they
won't impact the timings of your of your
of the measurements you know don't call
it a thousand times per frame like you
know at some point it does add up but
most of the time it will be noticeable
so the output of systrace is a HTML file
which is nice because you can just open
it
Chrome and you can just attach it to bug
reports or whatever so this one is a
trace that I took in the emulator last
night with the async Leavitt insulator
which is why I have a memory device with
12 CPUs it happens to be my Mac Pro a
partly when you emulate the Nexus 5x on
the Mac Pro you get 12 CPUs which is not
quite what the Nexus 5 has anyway so
this is what this dress looks like so
here we have all the CPUs and what they
were doing so the best way to navigate
this trace is to use the WASD keys so if
you've ever played doom or any
first-person shooter on a computer
you'll be familiar with that so you can
zoom in and out and scroll so if we zoom
in on one of the CPUs and you have to
zoom in pretty far you can see the name
of every thread that was scheduled when
it was running for how long what process
it belongs to and you can see
effectively what every CPU is doing and
you can see a lot of bubbles here which
means you know we didn't did not have
enough threads to to feed the CPUs and
that's important when you look at other
events so then for every process you're
going to see a bunch of different
categories some of them are threads some
of them are counters so the one that
were interested in here is my
application so there is just the demo
app I wrote for the HCL inflator all
those campers here come from the GFX GFX
tag so hwu why that's the name of our
OpenGL renderer if you click on them
you're going to see values here they are
used for internal usage so for instance
texture counts just tells us how many
textures are in memory at any time so
right now we have 99 textures and as
your app is running you'll see that
number go up and down that can be useful
you know we feel doing a lot of bitmap
installations the pub that are more
interesting you can ignore the hwi tasks
so we here we have the render thread is
where all the rendering happened every
little F is the signal the frame that's
when an actual frame started so you can
zoom in on it if you want you can see
how much time the actual drawing or your
app of your app took and when we look at
an event on the CPU
every so every block here as a name
that's the name that you specify in the
trace so if use that API Android that
always a trace you can put
then you want but just above is that
real thin line you see green here and
those are very interesting because green
means the thread was actually running it
was scheduled on the CPU white means
that the thread was sleeping and there's
also onions are zero sleep and blue
which means that we're waiting on
something so those can be very useful
because for instance here we can see
that we took let's see 15 milliseconds
to go through the frame but we spent the
bulk of our time sleeping
we were probably waiting on the hardware
or another process to do some allocation
and we could investigate by matching if
you press the M key for instance you can
easily see that what the rest of the
system was doing and you can get an idea
of what was happening if you want to
know the for kids just price question
mark into the ton of useful shortcuts
you can use just going to talk about the
tips in the frame bubbles yes and out
there's like a yellow or red ones there
is a red one yeah that's a good point
when you click on constants here there's
a red bubble for frames you click on it
you get a detailed explanation of what
was happening and why that's bad and
you're going to see some of these
explanations in a lot of places like
when you click so the you guys spread
here for instance when you click on
choreographer that do frame it tells you
what it's doing this link to the
documentation so we you can learn what
all those things are in the system
anyway so what's interesting if you look
at the UI thread we can see that the app
was starting and the app was doing a lot
of random stuff like it was trying to
schedule the first friend was fired when
you start up animation layout all that
kind of stuff
and here there's this vertical thread
too if you click on it we can see
there's an inside tag so that's the
async layout since layer doing its work
in parallel with the UI thread while we
are trying to render animation and shows
you that because the UI thread was green
the UI thread was busy doing things so
we successfully paralyzed about 19
milliseconds worth of work on a worker
thread without blocking the UI thread
which is excellent in our case and you
can get an idea of why do i thread ways
busy because here you can see those
other tags called animator :
those are special tags that then cross
multiple events they're automatically
generated
the animation system and they tell you
that we were animating the scale the
translation in the elevation of one or
more views and you can see for how long
so if we click on all of them so we we
had a four wonderful 4 15 music for 15
minutes or 15 seconds yeah we were
animating a scale that there was the the
code added to make sure that we could
see all the stresses and that's pretty
much all there is to see stress a lot of
it you know learning it takes some
effort I really encourage you to try it
and zoom in and try to correlate a
different events across processes and to
see what were scheduled in the CPU you
should be able to find a lot of very
interesting information about your
application and also because it is so
lightweight it's not going to affect the
timings too much compared to a trace
view for instance when you certain
sampling mode and the numbers are what's
more trustworthy one of the things
that's worth doing is if you're seeing a
lot if you're using frame metrics and
you're seeing a lot of Jencks or missed
frames then you can use systrace to find
out when those are happening and drill
down into hopefully what is actually
causing them there's information in
there about when the vertical syncs
happen so you can correlate those with
the operations on the UI and the render
thread and see those frames where the
render thread was taking a little bit
too long and pushing into the next frame
and then you can try to figure out what
I do in layout during an animation am i
doing too much work at the room so just
for you why you should do if you do
anything with multiple threads and see
all your thread here so you can make
sure that your scheduling or your worker
thread for your for your back-end
correctly you can enable the i/o tags
the network texture see you know how
you're talking to your servers so it's a
very very useful tool take a look at it
so I think one of the things is obvious
in the field of software engineering to
everybody that's been working in it for
a while is that all the new stuff that
we're doing all of the time isn't new at
all and we just keep redoing the same
things over and over and over again in
new languages and platforms and
frameworks and libraries and it's it's
fantastic it's job security for this
keven's universe so an example of that
is in Android one row Matt wrote what oh
yeah guard achill or you know so
everything that
which is going to talk about was
actually the first public demo I
published for Android so for Android one
no we had some time left after we should
the Android two to the carrier so I
wrote this little application that would
manage a library of books so you know
the books you could see like beautiful
book covers that were stored on the SD
card and back then you know those
devices were really slow so spend a lot
of time writing an adapter for ListView
that was trying to be smart in it was
guessing those call directions it was
looking at how much time your finger was
was was on screen every time you
interacted with the list whether it was
during his call or not to try to guess
what list item should be prefetched so
it was trying to load like all those
book covers ahead of time as you were
scoring really quickly it was also you
know scheduling cancelling doing all
that stuff it was not easy to write and
I think the issue ads have been doing
that since then and in recyclerview now
we have a system that's a lot easier to
use because you don't have to do it all
by yourself so fast forward about ten
years and one of the people on the UI
toolkit team Chris Craig did some work
to look at what was actually happening
in systrace to make recycle review so
darn expensive like recyclerview just
like wisp u is one of those core
components that so many apps depend on
and a common thing that users are going
to do is scroll or fling that list and
you get these janky frames as new
content comes onto the screen so we took
a look at a lower level about what was
happening when and what we could do
about it
so first of all let's talk about how
rendering actually happens during each
frame these are sort of the big blocks
of functionality so first we have that
input as relates to stuff we've said
before because again there is no new
information in software so we process
input first we figure out what we need
to do about that we process animation
we're going to change some properties
that's going to cause us to potentially
invalidate and redraw stuff we handle
layout if anything needs to be actually
measured in laid outs we actually draw
this stuff we create OpenGL commands or
lower-level displaylist commands out of
the stuff that we need to actually
render onto the screen as a result of
all the other processing that we just
did and then we sync up with the render
thread and then the render thread down
below takes care of actually displaying
the stuff into the buffer by calling
gl so that's what happens on every frame
at infinitum so this is a difference and
more complicated way of looking at this
this is several frames pretend that
there's a recycler view that is well
behaving there's no new content coming
on the screen you're just doing a scroll
or a fling and it's doing the same work
on every single frame and it's doing it
very well and very quickly and it's
doing it inside the important part
inside the frame boundaries right so we
have input we have animation we're doing
layout maybe a little bit of work there
and we're recording the drawing commands
we sync up the render thread does its
thing and it's all happening inside the
16 millisecond boundary so life is good
time goes on and then we hit a situation
where all of a sudden we have to do more
work on input the user is moving that
recyclerview and it crosses one of those
item boundaries and then we need to
reach out and actually create and bind a
view right and that means that input
could take longer which is going to push
out all of the other phases it's going
to make it the time period where we sync
to the render thread it's going to be
pushed later in the frame and the render
thread could easily be pushed so far
over that it's going to take longer than
that frame boundary to finish its work
which means that we've just missed a
frame and the user is going to see jank
on the screen so what is going on inside
of inputs in general yeah that is
basically what I just explained in
general we're doing creation of views
and we're doing binding of views and
these tend to be very expensive depends
on what you're doing and we would
encourage developers to be as optimal as
possible because this really is the
bottleneck for for that exact action
there the faster that is the less all of
this is is a general problem but we
understand there are complicated
situations sometimes that takes longer
than it should and that's what causes
the problem here so wouldn't it be nice
if we didn't have to stop in the middle
of this input action to go ahead and do
all this other stuff so that we could
you know not push everything out the way
that we just did wouldn't it be nice if
we could do that before and that's the
premise of what Chris did so based on
the scroll direction the information in
these heuristics that we know about this
recyclerview like well we're moving down
in the y-direction
we know that there is an upcoming item
there why don't we go ahead and eagerly
do what we need to the frame before so
that that information is just ready to
go so we do this and we do the create
and we do the binds in a free space in
the previous frame now one of the
interesting things about render thread
is I mean in general it's you might
think well you're just doing the same
thing it's just you're using more cores
to do it right we have to do all the
same information then we hand it over
then somebody else does their work there
it's not like we can paralyze render
thread with the UI thread because the
render thread information in general is
dependent upon the information we
generated on the UI thread it could he
has some ideas about that but this
particular render third action needs the
information generated in the draw step
in that frame right we can't actually do
that render thread in parallel no matter
what he says oh but what we could do is
we could use the down time in the UI
thread to do something else and in this
case what we're doing is we're using the
information that we know about the
recycler view to say well we think we're
going to need this information so why
don't we go ahead and generate it here
and then later in the next frame we've
already done that work and generated the
information that we need so that all the
drawing phases are shorter and we can
then fit that future render threads
rendering action into the frame boundary
again and life is good so there are a
couple of caveats what if the new item
is not needed so what if we said yeah
we're going to need that new item really
soon now and we go through all that
effort and it turns out the user stopped
at that exact moment or something else
happened such that we didn't actually
need it on the next code so we wasted
that effort okay so we wasted a little
bit of effort but in general that's not
the case right if we're actually moving
most of the time we're going to keep
moving and actually need that next to
information we are talking about frame
boundaries we're not saying a second and
a half from now 70 frames later we're
going to need that or like no no we're
just about to need it so in most of the
cases we actually are going to need that
information there are also situations
where there is no render thread render
thread came along in lollipop and
therefore if you're running on a KitKat
device
you're not going to have a render thread
so that stuff that we're doing earlier
isn't really going to buy you anything
because everything happens serially
anyway but it's work where we're moving
when that work happens but it's work
that has to happen anyway so it's it's
not going to give you an advantage on
earlier releases but you're not going to
suffer because of it there's an article
that we posted on medium.com if you want
all the details in a few more words than
I've used here
it's available in support library v25 so
it came out I think in late summer and
then there were more enhancements that
were made in 25.1 in the fall we built
it into all the default layout managers
for recyclerview so if you just use the
default then you get it for free if you
have a custom layout manager then you
should override collect adjacent
prefetch not prefect positions this code
will not compile on this slide
then you need to override this to do
something intelligent in the default
layout manager it is stubbed out we do
enable prefetch by default and you can
toggle that you can set that boolean
value as you want but you will want to
override this I know well I want to talk
about bending so I'm sure you've all
seen that that prime on your phones
at various times over the years so try
to recreate the effect on the site I'm
not sure is visible but you should be
seeing like more bends in the background
I did that in Photoshop and I want to
talk about why this happens and if it
does happen like so what Android is
doing to fight bending and what can you
do about it so what does it come from
the point is it's a quantization of a
signal so I'm going to show you a
practical example let's say that you
have a vertical gradient that goes from
white to black and you put that gradient
as the background of your application so
it's going to cover the whole screen
let's say you're on a pixel excel the
screen is about fourteen hundred and
forty pixels high so what's happening is
that we have 11 bits of data for the
positions to cover the 1440 pixels but
we only have colors that can use a bit
so we're trying to spread eight bits of
data over eleven bits of there
so we
woman actually I have a question about
that on at least on earlier devices we
have theoretically eight bits of data
per color channel but the displays
themselves would actually have the
display themselves who had less like the
very early Android devices they used to
have like maybe five x bits per channel
so it's on worse but maybe that's not a
yeah grainy but today's it's about eight
bits of data so here's a pretty graph
that explains exactly what's happening
so imagine that the x-axis is the the
pixel position so you know zero is the
first pixel at the bottom of the screen
one's the next row of pixels and so on
if we had the perfect gradient going
from white to black or black to white
there will be the green line so we have
that would be an analog signal we have
perfect the perfect value for every
pixel screen now when quantization
happens because we only have 256 shades
of color we get the red curve instead
and what you can see is that every
possible color in our gradient occupies
multiple position on screen and this is
where the bending is coming from and and
it's something that you know if you ever
see bending and I have the discussion
with UX many many times and I try to
make them understand that if you only
have 250 colors you cannot render them
properly on more than 256 pixels because
it's just not enough data so there's a
solution to that point that's called
gathering you might have heard of it we
have a couple of ApS on Android that
that lets you enable detering or turn
off the drawing but before we talk about
it I just want to to talk about where D
learning comes from because I found the
story when I was preparing the slides
and I think it's pretty interesting so
it came from World War two planes and
boats used to have mechanical computers
that were used to compute the trajectory
or you know bombs missiles that kind of
stuff you know and stuff and engineers
were working on his computers they
realized that when the computers were on
the ground being worked on you know
repaired or tweaked or whatever they
were not as accurate as they were when
they were in the air or at sea and you
realized that it was because of the
vibrations of the boats or the planes
themselves so what was happening is that
because they because were mechanical
computers there were four gears and
gears of those teeth and every number
and so we tend to like get rusty or get
stuck because of the grease or whatever
and the vibrations of the engines of the
the plane of the boat
we're just jittering the the the girl
enough to get them unstuck so the
solution they came up with who has to
had to add vibrators to their computers
so they added motors inside the
computers to this vibrate then to
increase precision and this is exactly
what we need to do to fix bending you
can imagine that you know we have those
right bands now if we were to disrupt
that signal and just generate at every
position we would on average have
something that's closer to the green
line and the word deterring I think came
from the German translation of the
jitter or something like that so let me
show you a demo to show you what the
ring looks like so this is a real shader
I wrote where at the top I have a
gradient of multiple gradients and I'm
quantizing them you know where more than
they actually would look like on your
screen but it shows the effect and here
vertically every row is a different way
of deterring the pattern and to make it
smoother so to be clear like the
gradient you see at the top is the same
gradient you see in every row is just
added some noise to every row to make
the gradient look better and what's
really impressive with jittering
you know it works pretty well already
especially if you squint so turns out
that in graphics for a lot of things
that we do we have something called the
screen test squint test we just squint
at the screen and she looks good when we
screen that means we're on the right
track so if you screen the gradients
would look great and I can go through
even two colors so if I cuentas all the
way to two colors and you squint a
little bit you'll see that the the
gradients look like gradients even
though we start from only two solid
colors which shows you like how powerful
these are in candy and if you know if
you were playing games a long time ago
if you had an Apple 2 or Camaro 64 one
of those you probably have just polished
in this pattern or something very
similar and we'll talk about its pattern
and if you had a Mac in the 80s you saw
the gray pattern yeah yeah
Britain will go back to prison
glad I have slide Jaeger alright so this
is what before Android oh this is what
we used to do for deterring we use
something called order deterring so
that's the Apple 2 version it uses a
four by four Bayer matrix so bigger
matrices are used in number of
situations for so they're used in camera
sensors that's how we get our vividly
colors out of the sensors and you can
see so that that's what the pattern
looks like and this is an example of a
gradient once we apply the pattern so if
we zoom in on it it looks pretty
disgusting and you can even see the
pattern so we said that this pattern is
isotropic because we can see the strong
vertical and horizontal line we can see
the shapes it's a it's a tile tile
pattern and we can see the repetition so
it works pretty well but it's not it
doesn't always work there are some
gradients where it looks horrible the
technical term is that you look like

so in OH a translation from the original
German word actually it's quite likely
so here's what I did in a instead
instead of using this fix pattern that
was basically a texture that we'd apply
the kwid overlay on top of the of the
gradients we generate on the GPU a
uniform noise it's randomized it depends
on the position of the pixels and we do
like some fancy triangle reshaping to
make it look better it's still
monochromatic that means every color
channel RD&amp;amp;D we apply the same noise
value we could do better but it is
anisotropic so if you look at the
picture here it looks more noisy than
before but you don't see this regular
patterns anymore so once it's on the
high resolution high density display the
fill is much more natural it looks a lot
better it also works on more gradients
one since the previous pattern works
great when you when your gradient is
horizontal or vertical because it
follows the shape basically of the
better but on radial gradients on
circles it didn't work at all this works
in much more
many more situations we also now
applying the desire into the Alpha
Channel which helps a lot if you're
going to blind several layers on top of
each other so we could do a lot better
everything experimenting with other
techniques so we could do temporal
deterring where on every frame we change
the seed of the randomize of the random
generate a random number generator you
effectively get a form of 3d terrain it
looks fantastic the plan on Android is
that we don't render all the time so we
can't really do that to try maybe to a
look sign out right Chet oh pardon we
could do chromatic detering where we
apply a different noise to every channel
to further you know where things we
could use something called blue noise
that looks very organic it's by far the
best noise we could use for deterring
it's a little annoying because 64 by 64
Bru noise texture takes about two hours
to generate on a MacBook Pro those areas
genic yeah so we totally do that at one
time we'd have to pre generate the
texture and we will recreate a pattern
which is pretty good at being tile but I
don't know if we're going to do that and
we could also deter in many more places
so you know if you looked at the release
notes for doing color management so
we're going to start using tended and
16-bit bitmaps or Windows so that means
that at the end when we go to the ad
display we could use deterring there too
to preserve the colors that we had the
high bit depth that we I can also do
your own spatial data in when you look
at your phone just shake it a little bit
sort of a natural smoothing function you
just but it turns out that there are two
things that are also helping us a lot on
modern devices pentile displays which
absolutely hate but by their very nature
of not having all the RGB component per
pixel
they are from a littering that helps on
the high-density displays because our
eyes also help by blurring everything
like the final details get better
together
so deterring that's even better with
hard work it's a wetware acceleration
thanks to eyes so what you can do in
your applications so fast is five six
five bit mats are tempting because they
use have the memory of regular RGB a
eight eight eight eight bitmaps now it's
fine if you jerry the five six
I did map from bitmapfactory so you call
bitmapfactory you want to decode the
JPEG or PNG and you said please give me
a five six I did not what we're going to
do is we load the bitmap in 80 days
apply littering pattern and you're going
to get something that looks pretty good
you've seen that with the gradients we
can do like a very good job at deterring
and especially on those hydrants in this
place it's going to be fine do not
render into fast excite bitmaps unless
you really really have to but if you do
what's going to happen that everything
you're rendering to the bitmap will
increase the error because we're going
to quantize even more every time your
render into it and if you turn on the
drain using painters editor you're going
to overlay detering on top of the ring
on top of dithering and you're going to
multiply the error is going to look
horrible should go back to Android one
with an emulator you're going to see
those nasty effects all over the place
and we cite six five because we have
more precision in the green Channel we
tend to we tend to get green bands that
look absolutely disgusting
also if you gradients and use alpha
gradients so you use the Alpha Channel D
you know it's not just two solid colors
that you have in your gradient you are
making bending worse worse because
what's going to happen is that we're
going to do the quantization first to
direct the gradient then we're going to
blend on-screen what we want to do is
first do the blending at high precision
then the quantization
so I have an example here I don't know
if it's very visible on screen but on
the left I manipulate the image on in
Photoshop so it doesn't look normally as
bad on on on an actual phone but yeah
you can change you can sorry you can
probably kind of guess can you see the
bend you should be able to see like
circular patterns here everybody's dead
anyway so you can see you can still a
lot of bends here and that's enough a
gradient from you know a bright color to
whatever to completely transparent on
top of this dark background now if
instead of using an alpha gradient we
bake the background color into the
gradient itself we still get bending but
a lot less bending you can see that here
the comparison we still see the bends
here and they're a lot less visible in
this case
it also added benefit that and there's
no way you can see that you can see it
so the design pattern will be will be
everywhere whereas with the alpha
gradient the design pattern will not
show up on the background so you see a
cut off the viewing pattern starts with
the gradient stops and it looks a bit
wheel so if you can if you render a
gradient on top of a solid color
background just don't bother with a
sidecar background first of all you're
going to reduce other draw it's going to
be more efficient but you're also going
to increase quality and whatever you do
that try to avoid doing another gradient
on top now for gradient and another
gradient because you're just going to
make things worse indoors and still not
hang about two minutes right so we can
do try to now filtering really quickly
it's been around for several years on
Android I just want to talk about it
again in case you're not aware of it so
let's say we have a source image we want
to render it at half of its size we can
do that really well on Android and this
is what happens so to render one pixel
in the final image we take four pixels
in the source destination so 2 by 2
square we do the average and it's great
because you know we devalue the size in
half and that's exactly how we do the
blending but what if we want to draw the
image at less than half of its size what
happens that we're still going to use
the same 2x2 pattern so we're going to
start dropping information and we'll get
a lower the quality so to combat that
there's a technique called trilinear
filtering it's also called mid mapping
and this is how it works so you shop
with an image then you divide the size
of that image by 2 then you take that
image you divided size by two you take
that image and so on and so on until you
reach a one by one image and you keep
all those images then what we do is when
we want to render one pixel in the
destination of the FEL image we take the
two images of that that what's called a
pyramid we take the two images that are
the closest to the side the file size
you want in each one of those images we
take 4 pixels with with the average and
then we average the two results and that
preserves all the information from your
enjoyment so I have an example here if
you look at this image it should look
here on the diagonal over here
I hate your slides you can see a lot of
aliasing artifacts that's because we've
dropped information we went from a very
large image to a museum in the Atkins
room you can see the DNA's in here and
here with writing our filtering because
we can preserve all the information we
can keep smooth edges so it's very
powerful especially if you're getting
you know large photos from from a server
or something like that it's also pretty
cheap because each level that pyramid is
a quarter of the size in memory at the
previous level so the total cost is 30
percent of the original image in terms
of memory so it's not that much and just
you know another story time
so I said that if this is a technical
mid matting and the the mid part of the
word I discovered comes from the Latin
multi mean bowel which means merging
little because for 30% more memory you
can get much better results net mapping
was actually invented and rum and it was
not in the air 50 PC it's also very easy
to use if you only use it you Cosette
has made map through on a bitmap and
you're done if you go through the
hardware accelerated code paths you're
going to get the higher quality also the
extra memory usage is going to be in
video RAM so it's not good at counter
against your Java heap so you're not
going to get an out of memory exception
because of it and if you have a drawable
you can also set the mid map attribute
to true XML and that's it we don't even
have actual insight we don't which is
fortune because we are way out of time
thank you for coming we hope you learned
something something</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>