<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sherlock, a Watson Cognitive Computing Use Case by Sandhya Kapoor, James Weaver and Daniel De Luca | Coder Coacher - Coaching Coders</title><meta content="Sherlock, a Watson Cognitive Computing Use Case by Sandhya Kapoor, James Weaver and Daniel De Luca - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sherlock, a Watson Cognitive Computing Use Case by Sandhya Kapoor, James Weaver and Daniel De Luca</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/39IrbA8lSc4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi hello everyone my name is Sandhya and
I work on Watson I'm really excited to
be here at devoxx and you know all my
colleagues Stefan Daniel gym so we are
going to take through take you through a
hasta box prototype that all of us have
been working on for quite some time and
the order of the presentation would be I
would like to go through natural
language processing and natural language
classification using Watson and then
Stefan so I'll show you the conversation
and retrieve and rank configuration that
we've used for the asked evokes demo
that's going to be an interactive slide
well browser session yeah my side I will
just show you the integration of pepper
to the rest services that we have also
developed with with James so did and
I'll show you a an orchestration service
created with the spring boot that then
is the kind of a server that then
interacts with allows pepper and an
Amazon Alexa and other clients to
interact with the cognitive computing
back-end and also specifically I'll show
you some things about Alexa and how to
do some programming with Alexa and that
we're probably depending on time do some
Q&amp;amp;A so if still have questions and can
make it interactive yeah let's make it
an interactive session because we have
had fun developing this prototype and
would like you to join us in exploring
further so let's begin natural language
processing you all have been attending
many awesome sessions during their box
so far it is
a very neat subject and there are lots
of underneath research that goes on
using different kinds of neural networks
namely convolutional neural networks
with several layers of dependency trees
using recurrent neural networks with
attention layer and several patterns of
chunking the representations both at
word level sentence level but why the
main thing we want computers to
understand is what humans are talking
and phrasing in their words and
sentences so it's very important that
the computer can understand our
questions in human language and that's
what we will go into and then we also
want computers to extract answers from
the structured as well as unstructured
data which is just in tremendous
quantities available all around us and
it is growing just at a very fast rate
so to curate that content and to glean
the answers from that unstructured
content is very important for AI and
then most of all what is important is
that computers should do reasoning and
it's not that they should get word by
word something from a document or news
feeds they should be able to reason out
and produce an abstractive summarization
that may include knowledge that was not
will include knowledge and also words
that were not in the document at all so
the computer would be phrasing and
producing summaries that are intelligent
and are based on reasoning so let's see
how that happens so I would like to take
an example to relate to your development
expertise
using conversation neural networks and
conversational api's so for that let's
take a look at a conversational bot
which are essentially neural Nets
trained with intense and intense are the
actions you want to take so you would
want your bought to do certain things
based on what you ask and then you would
want to qualify those actions with
entities that are nouns as to how the
action should be modified based on the
input that is coming in so let's see how
that happens and then comes in dialogue
where you have asked a question to the
bot how does the bot know what is the
context of your question and how it
answers so we will see how dialogue
nodes are designed so before I go into
the demo just wanted to mention that as
of now in conversational BOTS using
Watson we are supporting these languages
and by next month many more languages
are going to be available and we also
for ease of use have designed a virtual
agent that has predefined intense
because we would like to make it even
easier and faster to production with the
predefined intense so um and this are
some of the industries where the
predefined intents are available so
let's first take a look at a bot alright
so um just just to exemplify the
features I took a very simple example
and quite trivial one that is ordering
pizza which I have to order quite often
for children at home so let's just see
how this is designed
there are several intense i have added
here and with user examples so what is
the need to add examples in here you
will see that these are just random
phrases but i am just trying to Train
Watson to understand what this intent is
all about what do I want to do I want to
order a pizza I may want to change my
order how do I want to change it by
changing size or toppings so it's just
some examples of what I would like to do
and that's why I've added these phrases
and these phrases are taken by Watson to
the NLP layer and it forms a model that
when these words come in combined with
anything else the human might say what
action it should take so that's what
intense are now just to simplify this
example there are only two entities here
because those two are relevant while
ordering a pizza that is size and
toppings so I have given the size values
and the topping values now these values
are important because Watson takes it
into its neural network and says when
this is asked I'm going to take an
action in terms of fulfilling the pizza
order so these values with its synonyms
are important for example one could say
mushroom or one could say mushrooms but
I have to enter those synonyms so that
the network knows what that entity means
so lastly let's go into dialogue so
there is always a conversation start for
a bot and I have just said hello welcome
then I have added certain greetings in
here just to exemplify just to be
cordial and let's take a quick look at
this response it's just simply saying
when I say hello it's being nice to me
what's your name and I enter that as an
input text that is saved in a context
variable called username
and that context variable can carry on
in multi-turn conversations so that's a
powerful concept to keep the context in
our conversation so that the human
asking the bot does not have to repeat
himself or herself over and over so
that's what is going on so let's now
test this bot out right here it's let's
say hello can I get some help now these
words were not entered in or given to
Watson but it can infer what it should
be doing it maps those words to the
intent greetings correctly even though
the intent hat did not have these words
so that's where NLP and NL c kicks in so
it says may I know your name yeah sure
so that's what I am enter and it
recognizes my user input and fits it
into the response so um let's order a
small pizza with say cheese pepperoni um
bacon right so it says yeah great I
mapped your input to this particular
intent and these are the entities you
have given me well then I say Oh a guest
walk in can I change to medium right I
didn't say size or any such thing
alright so yeah I can change that now i
will give you a medium one with the same
toppings so it sees what input i'm
giving and accordingly chooses the
dialogue node that should we should go
to and that's that's happening on the
fly so that I mean even though it may
seem very simple example but all the
principles of NLP are being worked
underneath so if I say a change
the toppings to something else let's say
spinach olives let's just make it gross
and mushrooms right okay so no problem
it will keep my older size that I was
medium-sized with it it'll remembers
that context and changes the toppings so
that's how this UI tooling is available
to us to design very smart bots in this
but we can also make a REST API call to
some third party aap eyes for example
getting the session information at
devoxx or to be booking flights or
anything else imagine that whatever your
industry involves so that's about
conversation any questions sorry I did
yeah okay sure oh thanks thanks for
catching that so see that's how the
intents are very important i misspelled
and it could not understand that what
entity I meant so i can say um well i
right now it is not adding i can do an
append in my notes so currently it is
simply replacing the toppings and that's
how if you look at okay so this is
blocking the view all right so um i
right now the whatever toppings come in
it just includes that as the ultimate
last update and it is not doing a append
here if i were to do an append it would
it would be adding the topping so yes
since i misspelled i can definitely add
give it a new list of toppings yeah yes
mushrooms
you could also give a synonym and have
like different misspelled mushroom yes
and they'd pick it up absolutely thanks
Stefan so yeah here so for example I was
being very particular and did not add
synonyms that would be on misspelled but
obviously i'm not good at typing so
whatever i did maybe HS right so if this
is a synonym then it would understand
and as soon as i do something like this
watson does start training because the
entities and the intents are very
important for the NLP layer underneath
any other questions this question over
there yeah so repeat a question yeah I
if I understood you're saying can we ask
Watson to generate the synonyms
currently it's not possible but we
that's where we are going is to make the
tooling even easier so right now the
human has who's whosoever is designing
the conversation bot has to enter the
synonyms and the values that that he or
she cares and I'll just stand next to
you and don't stand so close to the
Micra to the video its go to scare
people and in a good way oh that came
out wrong sorry no it's just like and
but no but what you can do there are
some websites where you can enter a name
and it gives you synonym words so what
you do is just copy those words and you
include it into this if you I so you can
really do that very rapidly not only for
sitting synonyms but also for misspelled
words so you can really this is where
you really build up d NLP and feed it
definitely with pizza and it's very easy
to do but it's a manual task for now
yeah but we are making this easier so
our questions yes
so if I madness yeah does it also react
on invalid inputs how does it behave
yeah so I mean there are intense their
system in tents like off-topic out of
scope or even true so if nothing else
nor the condition is satisfied it will
fall into the true bucket which is a
built-in intent and then you can take
action on it whether give some in output
to the human and say please give me this
this this input and loop back so that's
what I have done here let me show you
and what's also really beautiful is
there because of you have that dialogue
you can immediately start testing and
see how it actually identifies the
intense and the entities so it is really
like then you give it to us just to your
son or your daughter just type some some
sentence and see how it behaves and you
just really need to add more synonyms
misspelled words etc and it becomes
smarter and smarter while you're adding
more information so you need to really
build it up and drain it so to speak
yeah that's in the code so the question
is that if you say medium it's then a
different context right so it depends on
the context of conversational context
that you can then interpret it in
different ways so it depends on on the
context right so I mean your question is
that context plays a very big role the
question was like medium size whereas
medium steak so the context are totally
different you can define a different
entity to build in the context around
steak and how it should be done medium
rare medium something else so I caught
at accordingly you can have your
variables which will be passed from one
conversation tune the next so all the
nodes which are sitting to the right of
your dialogue node they are all taken as
a sibling nodes and they are you go into
those maintaining the context that you
have set so I'll give you an example
so first of all to answer your first
question what happens when you are
making a mistake and you are not really
providing to the bot what it needs so
one small example is you have a
condition system condition true and you
can simply say sorry I didn't understand
please give me the size and toppings you
would like to order because this happens
to be a pizza bot so what happens then
continue from this particular input
where which is our starting point so it
is continued from the user input as you
will see in here this thing oops so it
takes me two loops me back but continued
from user input so that loops me back
and I don't have to repeat myself so
your bot can be smart in taking actions
now the next question was if we have to
continue the context we can have these
sibling nodes and they will carry the
context forward that you have set up
here so my context is the size in here I
will carry it forward to the next node
that has the toppings so in the next
node okay here so in the next node
because I knew the size already I've
used dollar size so that's the context
variable and this one came in to me by
the user so i combined the two so and
then again i save the toppings as a as
an array and that can be used by the
application so this is just to show you
how your applications can take action
and when Stefan explains he will go over
as to how as devoxx application is using
context variables you still have 10
minutes sure okay so um so yeah these
were predefined intense and let's go
keep going so conversation BOTS as you
already know can be used in various
scenarios
you have front ends and then you can the
input through any of these front ends
will come to conversation you can
combine it with many sir other api's
like speech to text and tone analyzer
and so on and also take advantage of the
backend to give the answer back so
thinks input comes to conversation
conversation can go to a back-end system
that may be a database it may be a
corpus that is sitting in Watson but and
then get the answer and come back okay
so it is very simple to use conversation
API all you have is one single method
that's called a service message and you
give the workspace identifier and your
message and that's about it okay here's
a curl command what I want to do and
this is one single curl command at the
emphasize the context which includes
your conversation ID so in one instance
you just have single ID where you are
conversing back and forth you have a
system context that keeps the dialogue
stack in the turn counter so every time
you're going through a node the stack
will move from root to node 1 node 2
node 3 as you go along in the
conversation and your turn counter will
intrument okay so then I mean we don't
have it's not just conversation watson
also a giv gives you an alchemy language
api REST API calls to extract entities
and keywords from any any unstructured
data so entities keywords relationship
between the entities the concepts and so
on you don't have to build the
algorithms which are implemented for you
and taking away the complexity so just
so that I can quickly demo to you here
is alchemy language and the beauty is
that you don't really have to use the
public model you can also use a custom
model train on your domain and then
extract the entities and build
relationships so this
is very powerful for doing many kinds of
cognitive applications so here Stefan
had done a keynote in 2415 so I took
that and I ran alchemy language api on
it and it definitely give certain
entities to me so each each of each
method that you see on the left hand
side is a REST API call and it gives a
JSON output and Stefan was talking about
professor Lawrence Krauss and that came
out as a person and the relevance was
quite high and then devoxx Belgium and
so on so these are entities and there
are keywords that are mentioned in here
keywords in Stefan's note last year were
definitely about professor Lawrence
Krauss but also that he is a keynote
speaker and then he had is a very good
author so and he's from Arizona he's
working at Arizona State so all these
key words came out so this is natural
language processing for you and if you
want a custom model what will happen is
them this this is I mean I can put in a
bit custom model I can put in a
different text and that text could be
lets say about the automobile industry
and since since this particular custom
model is built on traffic incident
report it will use entities which are
relevant to cars and traffic so that way
it will be able to better give you the
relationship between entities and give
you the concepts that are relevant to
automobile industry so this is a very
powerful concept that you can have a
custom model when you are doing a NLP
and underneath the neural network is
just focused on those entities and that
particular model any questions okay so
um then
and this is the custom model that I was
talking about you have your own
annotators and this is an example where
as I was saying about the automobile
industry you will see lots of entities
which are relevant to automobiles so
that's how it is you can have it for
particular industries this is alchemy
data API so you can take like tons and
tons of data from news feeds blogs and
you can have targeted search and trend
analysis on those and so that's what
alchemy data news is doing every day
taking 300 k of English language news
and blogs every day and building the
search so if we take a quick look at
that there is a demo here on music app
is just called news Explorer and I
thought we would want to know if at all
more about Donald Trump so yeah there
are lots of articles what is happening
is these articles are linked to other
people they are linked to locations so
if I this is a visual representation of
those and if we go into each article
then that will be highlighted on the
network that has been built but
essentially what it is is that a person
is being linked to many articles those
articles are linked to companies to
locations to other people and so that's
how the network is built and we can have
different examples here we could talk
about an organization and like NASA we
could talk about Apple Google come as
companies and so on so and these
locations are also entity so if I click
on a location then I would have a
different view of what is going on in
the from the news feeds what is going on
for that particular location so that's
how alchemy data news works is that X
and news feeds and then does all the
entity and relationship mapping
so these are the pictorial
representations and then there is a
query builder example as well so then
moving on we have a unique approach in
interpreting data you can go into
emotions that are much deeper than just
saying positive or negative sentiment
you can have emotions focused on anger
fear joy sadness and that's what we are
showing in the next demo that I wanted
to take you through that's a tone
analyzer and so it's here I just give
this is a a customer chatting with an
agent and so what is the tone and this
is a very real world example we get on
the phone sometimes we are upset with
the agent on the other end and our voice
level goes up and down and so do our
words so if we analyze the tone for this
conversation the anger is quite high
point 86 the language style is
analytical the person on the both ends
are being analytical and not just being
illogical and so the social tendencies
are well they are trying to agreeing to
solve a problem so these are the scores
for the social tendencies and here's a
JSON that comes back as always from all
the rest api calls and then at a
sentence level we can go sentence by
sentence and see for anger who had a
strong emotion so the customer said
somebody created an account using my
email and I'm really angry about it and
so and how can i delete this so the
really big red means he is expressing
anger in strong tones whereas the agent
of course is that being milder so same
way if you go into let's say joy like
there's not much joy in this
conversation for sure so it is just
light yellow so
that's that's how the tone analyzer work
so when we do a natural language
processing we are going into emotions in
the in the unstructured data and then
really fine-tuning it so that's what I
wanted to discuss and then we do have
personality as well so we use some big 5
matrix and then take your social data
your texts emails Twitter feeds and then
just analyze how you are in terms of
your personality and that is used in
many many many use cases I would like to
a demo so here is that I have my yes so
I mean this is kind of silly a way of
depicting it but I give in my Twitter
fear handle and it was matched with
several other celebrities in the world
and somehow on in terms of personality
I'm coming out 85 similar percent
similar to this person and 86 in terms
of needs so it needs are this so this is
Big Five personality matrix and so just
how the mapping can happen this is a
trivial example to show how personality
insights can be gleaned using natural
language processing okay any questions
so we also have along with NLP we also
have NLC natural language classifier
that uses machine learning algorithms to
create natural language interfaces that
you can add to your applications it will
interpret what is your intent behind the
text you are giving to the natural
language classifier and it will return
back with a classification like a
typical classification problem in AI and
it will give you our confidence levels
and then based on the values returned
you can take corresponding action so
here is a natural language
classifier demo that I wanted to give if
I can only go on to it yeah so here's
natural language classifier that is
ready to classify my input and since it
is trained on whether you know I asked a
question I hope it doesn't rain today
because I really wanted it to be a nice
sunny day so when I asked that question
how is it classified so it says that it
is ninety-nine percent confident that
the question being asked is about
conditions so it was classifying into
two buckets whether you're talking about
the weather as such I mean there were
two conditions yup as to the weather
conditions or you're asking about the
temperature and symbol since I said is
it going to rain today I hope it doesn't
it is about conditions so it put it into
that bucket so this is a small example
to show that natural language
classification is very important when we
are trying to do reading comprehension
and trying to do abstractive
summarization and answer questions so
and then we have language translator
which is very handy and useful and it
uses the recurrent neural networks
underneath to do the translation from
one language to another and so it will
dynamically translate all different
kinds of documents news patterns etc and
it will publish your documents in many
languages for example somebody in French
could be sending emails instantly in
English so in it this is a applicable in
news domain from and these are the
languages that are supported and also in
conversational domain so then is speech
to text and I think Stefan has to take
over okay so our CC text you could use
be using rest calls or web sockets a
very unique and a useful feature is
keyword spotting and the keyword
thresholds when your speech is being
converted
to text and also offering word
alternatives as the audio is being
streamed with the different time stamps
so we will see an example there are they
also the word confidence level is very
important in bringing out the final
transcript and we have a very helpful a
new feature added that is customizing
that which model and that has given us
awesome results for example if you are
using speech to text in healthcare your
lingo your language is so much different
the vocabulary is very much focused on
the medical terminology and so custom
language model comes in handy another is
you may have multiple speakers in a
conversation how do you mark this
speaker is the one speaking now and not
the other one so speaker labels that are
coming very soon are really useful in
making out the speech signals so here
let's look at a speech-to-text demo so
um I can take an audio file as well but
let me play the sample first in a severe
Gale like this while the ship is but a
tossed shuttlecock to the blast it is by
no means uncommon to see the needles in
the compasses at intervals go round and
round at almost every shock the helmsman
had not failed to notice the whirling
velocity with which they revolved upon
the cards it is a site that hardly
anyone can behold without some sort of
unwanted emotion so you see that we had
different word alternatives come out
with their confidence level but what
happened is that I was playing with
another audio file and had given the
keyword modules haha so it did not spot
the keywords which are in this
particular sample so let's select
quickly an audio file that is Mark
Reynolds keynote given last year right
Stefan so let
do that and then here before I I really
want the keyboard model good morning
welcome java 9 is primarily going to be
about modules this is probably not news
to most of you here in the room but as
part let's uh because i started the
audio much before i typed at the key
word so we are going to start over okay
all right good morning welcome java 9 is
primarily going to be about modules this
is probably not news to most of you here
in the room but as part of the bigger
picture well I had so let me start
transcribing so you see he was a ham in
mark is talking about modules and not
models but I did want to make sure that
that key word is spotted and a speech to
text told me that that keyword happened
at 13 second and ended here and
confidence level was quite low but
combining this keyword spotting with the
word alternatives here that I can go
back to thirteen point six seconds
easily I can take the alternatives at
that point and then combining the two my
transcript can be much more coherent and
high fidelity so that's that and so then
there is text to speech very quickly I
will go over that you can do
customization in terms of pronunciation
so our pronunciation varies depending on
where we are living how we pronounce
also you can make it expressive and
transform able so are quickly yeah yeah
we skipped up demo because we only have
22 minutes okay sure we won't have
enough time for all the other ones yeah
yes you're on the back yes I'll wrap it
up I mean where we can go if there are
questions we can talk after the session
great thank you cool because I do you
want to focus also on our part and we
give her to say she goes you know she's
so enthusiastic about this that she
could speak for days but we need to
switch can you help me with the slides
so what I want to show is the part that
I was involved with which was really the
boring part because all the other ones
wanted to develop including Jim and
Danielle but we had to provide the
Watson with data and so I played the
victim and on a rainy friday and weekend
i actually provided all the different
events that we need and let me see if
I'm sharing my screen here correctly is
it this one I think you're showing the
wrong one great we're gonna lose all our
time on this place remember it think
we'll okay yeah there is that's fine so
so what I did is create intense for the
focus we wanted to do was about DevOps
us we were doing really a marketing
around devoxx us and that was our
conversational context so basically like
Sandhya did is that you have a you feed
Watson with different examples of
questions that people might ask so you
you misspelled it sometimes you
associate yeah or you just write it in
full words like United States instead of
us etc and that's the first thing you
need to do it's really the more
questions the more related questions you
you can give it the more confidence
Watson will learn your your intense is
its flickering or
yeah don't don't touch it I need to
touch my keyboard then the other one is
the entities and so what we wanted to do
is create different synonyms so we could
actually say like okay who's organizing
it all where etc so what I wanted to do
was what who where when and those were
basically the entities I wanted to
define and you then try to declare that
in different synonyms and misspelled
words etc so again you're making it
smarter as it builds it up this is very
annoying so what I then did is create a
dialogue so you can actually create and
dialogue within the system so where we
wanted to start with was what is devoxx
us you see it on the right and so then
it actually does recognize your intent
and your entity because it now knows
that it is in the Box us and the entity
is about the conference and what it's
nice with the dialogue is actually it
opens up the box where you are within
the dialog scheme so you can really
follow the sequence and so what you can
see now is that there's two paths you
can do when if you see my screen or when
so when as a continuation of the
conversation or went as a new entry of a
conversation so if i do just when it
will actually continue the conversation
that it knows it was already talking
about the fox us so if I my type when it
just continues that flow okay new cables
this is the last session of the vehicle
like exhausted anyway then when and then
then you could say where and it just
basically continues the flow once you
see my screen and if i type in who there
it is and it's still free click through
flicking it will just continue to flow
but you get the basic idea right so
that's
basically the simple conversation now
that's where you can do the
conversational but but if you want to do
some more advanced stuff like what is a
unit test for example then this is wait
this is not advanced enough to actually
cover that so Watson has something
called retrieve and rank but you can do
there and let me just switch to that
hardly touching the screen so it doesn't
flicker too much is that you can upload
in the retrieve and rank you I all
documents again you're learning the
system so you need to first provide it
and feed it with documents so what I did
is that I took all the Fox articles I
converted or I enabled in a MP in which
is like a mobile plan to condense and
only get the content from your page and
then I fetch that that article within
the retrieve and rank so you will see
these different articles as you can see
it here will actually be split into
paragraphs or in chunks off of
information which allows you then to
link a question to that specific
paragraph so once you have fed that you
actually need to provide related
question study so in the article
sections collections Thank You Sandhya
and you then and that's the boring part
I had to actually invent all sorts of
questions which were then related to the
the paragraphs and you basically train
it so you give it a question and then
you say which paragraphs actually match
that question yeah and that was like i
spend like really a full week and just
doing that because the really hard part
is that you can only start using
retrieve and rank once you have what is
it 150 questions soho christian so they
were going where can we tested it cuz I
said man I'm need to provide questions
and then you can at least try it so you
you have funds then a ranker which based
on that information is able to give you
answers so if I say what is a unit test
already is j unit what is j unit let's
try that one it will then try to
identify
I all the paragraphs from Vox but also
other ones based on that training
because it continues to train if you
provide it more details to make that
link so that's just a quick demo of
conversation retriever rank and what we
then did in front was to basically
define a micro service with just the
orchestration it first does that gets
the question it sends the question to
the conversation APR if it doesn't get
any response it then goes actually to
retrieve a rank and try to find a
response there so that's how we did that
orchestration so that's my part I think
Jim Daniel still got 15 minutes so M it
has come down to 50 questions 50 yes 50
cillessen now she tells me that thank
you very much some people have all the
luck okay so let's Jim's I start oh ok
so here I'm gonna show you let me switch
screen each possible I'm using the same
connector as Stephan hope I won't have
an issue
okay let me try to reconnect to the
robot okay here I'm just restarting the
software just in case of so here in this
case we just use all the DeMarco
services that James developed initially
and then I upgraded it in order to use
her several speech to text conversation
from and the conversation system that
was already there for the IBM Watson I
by just using the the pepper robot as an
interface so in this case the pepper
robot is just an interface calling the
REST API as a client yes hi in this case
it just recorded the robot I'm just
using it as a audio recorder because the
robot by itself cannot understand any
words that I many sentences that I'm
saying to it it record the audio send
the file to the macro services spring
boot application that we've developed
and use the behind that that macro
services use the speech-to-text sister
API of Watson in order to get the
sentence and also get the key words and
then the key words are used in the
conversation API in order to start the
conversation and then the Watson returns
to the macro services the conversation
ID back to the robot and I use that
conversation ID to continue to
conversations with with the robot the
human and the robot interaction so let's
key basically it so it's in this case
it's a python-based service risk life
calling so recording the song here and
asking the devoxx to ask devoxx using
the other bus rest api call in order to
send the files and get the results so
it's basically doing rescue lip api from
from python directly here why I did this
in here in choreograph which is the tool
with the boxes and the links is because
I'm planning to use this primary for the
kids the box for kids
also to try to use what's on and to show
them AI systems with with the children
so they can program this and it's also
easy for me for playing having movie
movement with the robot so let's give it
a try simply of this world book good
afternoon again hello good afternoon
photo after all okay two seconds and
again studying and sometimes the robot
is a little bit shy not reach I'm it ok
move on not such a tease
good afternoon pero column can you
please explain me what is the vox united
states okay but to answer your question
i need the help of my friend IBM Watson
hold on a second Derek success is a
conference from developers for
developers priced affordably so that all
developers can enjoy this unique
experience I see I see anything else you
want to know yes when is it happening ok
let me ask it on the question to my
friend IBM Watson
this case it doesn't work so yeah
sometimes is the the connectivity with
the robot and sending the file to the
zoo to the Internet is quite difficult
here let me just try again two seconds
as a last test oh yeah good afternoon
can you please explain me what is the
Vox United States okay but to answer
your question I need the help of my
friend IBM Watson hold on a second so
here is sending the front tool Watson so
that all developers can enjoy this
unique experience I see I see anything
else you want to know yes in which city
does it take place okay let me ask again
the question to my friend IBM Watson
that Express takes place in the san jose
convention center thank you thank you
okay so you see also below all the
answer is received by the robot the rest
the jesus on the responses received by
our sprint boot applications so that's
basically it just having a rest client
different sort of rest client here at
the robot querying watson basically
through or business applications thank
you James see Danielle
actually two son dias screen so what I'm
going to talk about then is the is
another client Daniel just showed the
pepper client I'm going to show an
amazon tap or alexa client and so if we
can see it on the screen yes so the the
architecture then is we have the over on
your left we have the Amazon echo and
then we have a couple of pieces that
I'll talk about very quickly one is the
Alexis skills kit and another is AWS
lambda which both run in the cloud and
then they're hitting that that
application that orchestrates different
inputs and asks the Watson API service
so so to use the skills kit to create a
skills you go to the Alexa site the
Amazon Alexa site and there's a skills
kit icon but then you hit and then you
can see your skills the ones that you've
created or that you own in this
particular case I created one called ask
deb ox and so if we go into that we can
see then the skill information we have
different tabs on the left and so we
define a skill we give it information we
give it a name so that arrow says ask
dev ox and then we also give it a
invocation name and it's called devix so
when I say ask dev ox to the Amazon
Alexa then it will recognize that as a
skill and then we'll proceed with that
skill each skill then has what's called
an interaction model so similar to the
way IBM Watson has intense and entities
the Alexa skills have intense that that
we can use to then kind of parse out and
understand what the intent of the
speaker is in this case I just have one
intent called one shot
mand and that's going to take whatever I
say and use that as the intent into the
the web service that that we showed you
so then on the AWS lambda side we need
to create a lambda which is in this case
is a java application that then responds
to the different life cycle commands
that that Alexa has that are that our
client has so for example if you know on
start or on a particular intent the
particular lifecycle method would be
called but then we would then use to
then call the the service that then
makes use of the IBM Watson skills and
so then each lambda then has a name it
has a handler and so you just give it
different parameters for your lambda and
then the code that that is behind this
if you want to write your own amazon
alexa skill and lambda you can see the
code in this github repository it's it's
my github repository gem of expert and
it's alexa ask debauch so now i'll just
go ahead and demo this really quickly
it's going to use some capabilities that
Sundy had talked about and that Stefan
did where we use the the conversation
API that that was created with dialogue
but then we also use retrieve and rank
if there's no longer any any path in the
dialogue to answer that particular
question it will then go to retrieve and
rank and try to do something so the
first question I'll ask will go to
dialogue and the second one will go to
retrieve and rank but first we'll wake
it up and because with the ask debauch
skill so here we go ask deb ox
welcome to ask devoxx what is your
question what's new in java java 9 is
under development and will contain
features such as modularity and enhanced
to Lee what is modularity the Java
module system specifies a distribution
format for collections of Java code and
associated resources it also specifies a
repository for storing these collections
or modules and identifies how they can
be discovered loaded and checked for
integrity project jigsaw modularity for
java goodbye goodbye don't be a stranger
so that's amazon alexa and that
interface now i think we do have
actually maybe a couple minutes for
questions so i mean just to wrap up as
well as well first of all I would like
to thank Jim Daniel encendio for making
this happen it is a prototype y we
really focused on specific questions as
well and it's really a matter of
providing more data and more entities
and intense and so on and then uploading
on retrieve a rank that you can make it
smarter etc yes different I would like
also to thank a soft brown robotics a
Europe for walks yeah we all the robots
thank you for that thanks and so any
last questions we still got like two
minutes yeah yes so repeat a question
yeah so the question is can we have
different answers for the same question
yes so we can and the way is that we
have a property called random and we so
we have a property called selection
policy we can make that random or
sequential and give many many choices
and it will just choose from there yes
good question Thanks any other questions
yeah sir so can you include an emotion
in the reply is that you yeah you can
distill emotion from the phrase question
can you reply on the emotion so you
probably want to have different replies
based on the emotion if the person's
angry you want to answer differently
right so the question is that based on
the emotions that are in the question
are being asked can be reply differently
answer is yes you could call out in the
conversation response you could call out
to tone analyzer API get the analysis of
the tone whether it's angry or it's
happy and based on that a variable value
you can then put your response so it
would basically extend our microservice
rest interface there that's where you do
the switching you first then check the
emotions and then you go to conversation
X Y or Z that's why we had that best
interface in the first place to really
like orchestrates which services we want
to use within what yeah I think our time
is up so thank you very much for
attending don't forget to rate and if
you have any other questions will be
here later today and tomorrow as well
thank you thank you thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>