<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reactive Machine Learning On and Beyond the JVM by Jeff Smith | Coder Coacher - Coaching Coders</title><meta content="Reactive Machine Learning On and Beyond the JVM by Jeff Smith - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reactive Machine Learning On and Beyond the JVM by Jeff Smith</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/akPLphTykwI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is jeff smith and
i'm here today to talk to you about
reactive machine learning and today
we're going to talk both on the JVM but
then more off the JVM sort of exploring
the wider ecosystem of techniques for
building reactive machine learning
systems with the perspective of the
curious JVM programmer the person trying
to explore the full range of
possibilities a quick enter on me so
what I spend most of my time working on
is with data scientists and engineers
trying to build large-scale machine
learning systems right now I do this
primarily in Scala a text of a I at
x-ray I we're an artificial intelligence
startup building a personal assistant
who operates over emails to schedule
meetings for you just see seer on your
emails the same as you would a human
personal assistant and she'll take care
of all of the negotiations around
location and schedule and things like
that it's just X dot a I of course after
the events of yesterday's election I'm
also planning an exit from the United
States so if anyone knows the place to
find work in the EU that would be great
all right let's begin our technical
discussion and so one of the first
topics I need to introduce this group to
is reactive and so I imagine that some
folks here have some familiarity with
what their concept of reactive is the
reactive in different context means
different things I'm going to define my
work a little bit here as I would define
it reactive based on the reactor
manifesto consists of these properties
of a system that we want to have these
ideals that we try to design into
applications they are these for first
that it is responsive that it will
return values to the user in a
consistent time bounded manner next that
it's resilient it will get back up when
it gets knocked down it can handle
errors and recover it is elastic it can
scale up or down in response to user
demand in need and that its message
driven that we're not coupling all
this together on a single node but in
fact we're having some methodology for
passing messages across our systems so
this is the sort of traits of reactive
systems but these are just ideals to
hold to within the original reactant
manifesto they outlined three possible
strategies that you could use to
implement reactive systems these are
replication having more than one copy of
your data and this refers to both data
at rest as well as data in motion as
we'll see in some common applications
the next is containment this is keeping
your errors bounded within a sort of
scope having some ability to know what
are the limits of the possible error
conditions and what systems they will
affect and lastly supervision this is
building hierarchies this is knowing
that this component is responsible for
the failure of another component and
will make decisions based on it so those
are the three reactive strategies now
what I would like to discuss here today
is reacting machine learning which is
basically applying the techniques of
reactive to machine learning so just as
I don't want to assume that all of you
are super familiar with my personal
definition of reactive I'm going to
presume that not all of you are familiar
with machine learning actually I want to
take a quick poll on this one uh who all
is working on machine learning
artificial intelligence data science
applications okay so I see a decent
chunk of folks and i'm guessing the rest
of you are just interested in the topic
on some level um so let's begin with
what we're we're going to talk about a
couple of different applications of
different sorts of machine learning
techniques my personal context is in
artificial intelligence that's what I
happen to care about and that's what I
happen to work on one of the models that
we sometimes use to describe what is
artificial intelligence is the agent
model this is a simplified diagram of
what an agent might consist of an agent
is is a thing which can sense and act
upon some environment it has knowledge
and it's able to learn from those
experiences where it senses things where
it receives percepts from its sensors
records those in a knowledge base and
then learns from those actions and then
makes decisions that function that you
see there is what sometimes called the
agent function that is the portion of
the agent that makes decisions with in
machine learning we're actually only
concerned with a small subset of what we
have to do with in all of artificial
intelligence specifically we're
concerned with this learning process
whereby we receive in percept and then
have to reason about them and improve
our knowledge of the world as I broken
it down in some of the work that I've
done I think of machine learning as
going in this sort of cycle where we
collect data we generate out features we
learn models we evaluate those models we
publish them out for for use and then we
act upon those models these are just
sort of rough guidelines of the sorts of
steps we might do here I'm going to show
you a couple of pieces of this life
cycle but we're not going to work
through the whole picture of things all
right so that's my view on what we're
going to call machine learning for the
purpose of this talk so let's talk about
reacting machine learning reactive
machine learning is an approach I've
been trying to take to understand if
reactive is so powerful and so useful
for web applications for mobile
applications how could we apply it to a
problem as complex as machine learning
aren't there going to be more obvious
consequences things that we need to care
about within the context of a reactive
approach machine learning there are two
things that jumped out immediately to me
when I looked at this a problem first is
that the product in terms of the
properties of the data first is that the
data is effectively infinite in size
that we deal with arbitrary scales of
data within machine learning context
because we produce produce derived
amounts of data we just create new data
arbitrarily second data within a machine
learning system is intrinsically and
pervasively uncertain there are all
sorts of things we will never know
definitively in a machine learning
system because the sorts of questions we
want to ask are statistical in nature so
that will only get an eighty percent
confidence of truth or something like
that from those two properties of the
data certain techniques fall out certain
strategies coming from laziness higher
order
functions immutable facts possible
worlds these are techniques that are
used in other contexts and these are
just several of the many possible ways
that we can use these sort of principles
to guide us in a more actionable way in
terms of what what are the ways we want
to attack how to build machine learning
systems in a reactive manner alright so
that's reactive machine learning so
let's talk about the jvm and why we're
talking about the jvm in this context to
me what I care about and the jvm are are
a few things one of them is that I can
use it in lots of sorts of applications
I can use it in all sorts of different
infrastructural environments there are
certain aspects of the runtime that also
are quite powerful in terms of what it
enables for the performance and the
semantics of other languages and
technologies built on top of the JVM
particular things like garbage
collection makes it easier to do things
like object-oriented programming jit
compilation gives us the ability to do
highly repetitive calculations like we
do in machine learning very simply and
very efficiently and that there's a sort
of multi-threading model here and that's
that's an area that I'm going to dig
into a little bit more on this talk as
to how this affects how we encode the
semantics of reactive within our systems
I really like that we can use the JVM
kind of everywhere now even if the
original view of what the JDM was all
about the whole ecosystem you know
focused on single back-end servers now I
spend most of my life reasoning about
large-scale clusters and you can see in
projects like spark but also dupe mezzos
marathon all sorts of large-scale
cluster technologies they work very well
within the JVM because of some of the
properties of the system but of course
we're also still seeing the expansion of
the JDM to to mobile and embedded
contacts as well which is really
exciting for people who want to build
artificial intelligence systems maybe
someday we can give them arms and legs
and eyes and have them interact in the
real world unlike I think the majority
of folks in this conference I don't
actually write any Java I haven't
written any Java in many years actually
a quick poll Java developers all right
yeah ok so this might be a little bit a
little bit odd because i'm not going to
show a
a line of Java in this entire talk
because I'm interested in in in Java as
an ecosystem as a as a runtime as a
place in which we can build up
abstractions which span beyond one
single language in particular while Java
design has done good work with
developing out facilities for generics
other languages have been able to build
yet more complex type systems on top of
that the JDM in particular has has
recognized the the sort of innovations
that have come in through functional
programming as well as just different
traditions of programming such as
dynamically typed program so that we see
things like invoke dynamic high order
functions being added to Java but then a
wide range of other languages being
developed on top of the JVM things like
Scala closure groovy Kotlin I'm
primarily a scala developer and prior to
that I was primarily a closure developer
and so that's kind of the perspective I
have is a functional programmer working
within this ecosystem some of the cool
technologies we've been able to build on
top of the JVM that are really important
to building reactive systems are some of
these higher-order concepts right that
thing's some of these come out of a more
sophisticated view of the world then we
had at this sort of dawn of Java right
these are things like futures promises
tasks everything from like closures
approach to software transactional
memory hazel cast data grids akkas actor
systems and sparks r dds these are
really powerful tools that I think are
really core to the argument of why the
JVM is so central to the approach that a
lot of people are proposing for building
reactive systems a lot of the properties
that you want are enabled by reactive
programming idioms that are made easier
by having support for these sorts of
concurrency distribution parallel ISM
techniques all right let's get into a
more detailed example I'm going to show
just one example actually on the JVM and
then I'm going to launch off into other
runtimes and talk about how these
connect the first example here is a is
kangaroo capital kangaroo capital is the
largest credit card organization within
Australia they serve the marsupial
community there the problem going to
work with is there
fraud detection model so fraud detection
is a sort of classic data problem that
you can use machine learning to
accomplish in this case we're trying to
build up a fraud detection model that
helps us decide given a transaction does
the model believe that this transaction
was fraudulent or not so in the diagram
shown above if we have in fact predicted
fraud and we are correct everything is
fine we're a model has done its purpose
it served its role if we detected fraud
and we're incorrect in fact we've
angered our customers there and then
we're going to get a lot of negative
feedback from customers were going to
lose money as we drop customers or if we
fail to detect fraud we're going to lose
money as well because we're going to
allow transactions which are not
legitimate to pass the right system and
incur further costs we show you how to
build a basic model that will allow you
to to build a sort of fraud model here
for example in this case we're going to
use spark folks here using spark anyone
all right that's decent uptake so spark
is implemented in Scala for those folks
who don't know it has api's now
available for Scala Java Python are but
a very powerful JDM technology that
scales up to really some pretty
impressive performance feats on cluster
scale computing here's some basic
boilerplate that we must set up to say
the context in which we're executing all
of these things this is just basic setup
about what is our application on this
cluster we're going to load in our data
here here's just some example data and
then we need to partition our data into
training and test data this is a common
machine learning technique which we want
to use some data to load our model and
in a later set of data called the test
set or the validation set depending on
how you use it to allow us to make
statements about the performance of our
model and then we're going to
instantiate a new learning algorithm and
we're going to train a model on top of
that training set now we want to
evaluate this model right we want to be
able to deploy it in production based on
concrete knowledge of its performance
characteristics so to do that I need to
introduce a concept called an ROC curve
this is an example of a good ROC curve
an ROC curve balances true positive rate
in the false positive rate this is a and
so a good ROC curve has this sort of
shape we see here where it's it's above
the diagonal so the area under the ROC
curve should be greater than point five
if for example we had a random model in
a binary classification they just said
true-false true-false true-false
completely randomly then the area under
the curve would be precisely point five
on an aggregate basis if for some reason
we had a perverse model a model that was
actively making the wrong predictions
with intelligence then we would have an
area under the curve of less than point
five the results of our model training
process before ever deploying it to
scale on real traffic in particular we
can know that if a model has an area
under the curve of point five or less
there's not the slightest possibility
that this model has intelligence encoded
into it it cannot possibly be used on
real traffic it's complete garbage
here's Anna here's a skylit
implementation of how we can do that
basically what we're doing is we're
taking in a spark logistic regression
model and we're retrieving using the
facilities provided by ml libs sparks
machine learning library a summary of
the performance of the of the classifier
and then we're just going to compare
that to ensure that we are above point
five so checking back in on on what this
spark example of fraud detection means
in reactive terms a one thing to that
may not be obvious for many of that is
that spark is the canonical reactive
system it has all of the properties of
reactive systems it in spades it is
highly responsive resilient elastic you
can you can run this on arbitrary size
clusters and you can see enormous
performance statistics from from the
folks that data bricks an amp lab about
just how powerful spark is and it does
this using just the techniques we've
talked about that our data is replicated
across the cluster errors are contained
so that we can use message passing to
communicate and that and then we have a
cluster master which supervises all of
this stuff so it is
obviously ideal for working with
infinite arbitrary scales of data
alright so that was a quick spark
example of how machine learning works in
a given tool chain I want to jump over
into the boundary of the JDM so let's
presume that we still want to build the
majority of our system within a JDM
language of some type but we have some
need to step off for some purpose this
is actually a pretty common scenario
within the machine learning community a
large amount of development it happens
in languages that aren't on the JVM in
particular for most data science machine
learning research right now Python is
the lingua franca of huge amounts of
research right now so we'll look first
it how can we just use Python in a
better way you know in a way that allows
us to still maintain some of the nice
properties of our of our JDM based
solutions while still having access to
what is perhaps less reliable must
perform encode the context here is
timber timber is a dating app for bears
you swipe left with your par right with
your paw depending on if you like the
look at the bear you want and and so
what tinder hopefully timber is a
hopefully able to do is uh is made up
vegetarian pandas and and and
carnivorous brown bears and help them
find love through the power of modern
technology one of the projects that work
that they're working on currently is an
approach to artistic style I'm not sure
if anyone's seen the work on a neural
algorithm of artistic style a more
commonly known on Twitter as hashtag
style net in this case we're going to
try to apply this technique to the
problem of producing profile pictures we
want bears to see the best in each other
by having their pictures look like they
were painted by Monet or something to
give them show off their true inner
beauty to do this we're going to use an
implementation from tensorflow
tensorflow is an open source project
released by google the actual
implementation of the vast majority of
the functionality is in c++ but the only
bindings
provided for that functionality by
google in the original open-sourcing are
python bindings so people have been
built various algorithms using neural
networks on top of the tensor flow
framework in Python the one that we're
going to use here is called neural art
TF it's just a random mediocre
implementation that I grabbed off of
github and the main thing I want to
point out here is is a way that we can
use this sort of highly unreliable very
foreign code in a way that allows us to
simply made up with with the system that
hopefully we have stronger guarantees
around so the top we see a standard
invocation at the command line of this
application it's it's sort of Python
runtime made available here and it takes
various command-line runtime parameters
this Maps down to a Python function
shown below just the signature where we
pass in certain parameters out to
produce the arts there that where we
invoke the deep learning model to to
learn the artistic style of the images
provided to us the approach I'm going to
take to make all of this a lot easier
and more reliable is to expose this as a
service still in Python here we're going
to define out a server which exposes an
endpoint that allows us to produce our
art and return out successful value as a
true boolean so we're just wrapping up
the function and then Stan's you having
a server instance now to be able to talk
across the Python runtime here and back
to the JVM we're going to use a library
called pyro light this is one of many
possible solutions to this problem in
this case what this allows us to do is
use a Java library and a Python library
which would know about each other and
have common expectations for how to
communicate across in this case what
we're doing is we're instantiating a
pyro for demon we're creating a Ning
server and in this case we're going to
register this Python application as a
service and make it available for
service discovery just by name in this
case we've defined out our service as
neural server so all this does is it
instantiates that object that we saw and
and makes it available to other pyro
services let's jump over into scholar
lamb now in this case we're going to
start to build up our implementation
using common scala idioms in this case
we want to do an enumeration we want to
have some sort of static guarantees of
well-formed values for our configuration
so these are the names of the types of
the pre learned deep learning models
that are used here at the v GG and the I
to V and then we can start to build up
our job configuration these are just are
sort of static expectations this
actually mirrors very closely what we
saw in the Python but now we're able to
we want to put this within our Scala
code because we want to have our sort of
knowledge of what consists of a good and
well-formed job invocation exists within
our core application excuse me and then
this is fairly straightforward to be
able to make that connection from our
Scala code using the pyro light library
to to talk to our name server to prove
to simply call out to the neural server
that's our location discovery mechanism
by name they're just a neural server and
then we're going to find out how to make
that call then now since we're back in
Scala land since we're in a
multi-threaded runtime with
sophisticated higher-order functionality
defined around how we deal with
asynchronous e a synchronicity we can
use things like futures and so in this
case we're actually defining a timeout
here using Scala futures we've got
static guarantees of all of our
parameters matching up to the interface
that we've defined and so that we have a
lot of confidence that we're going to
get back our result or if not we can use
various reactive idioms to fail
gracefully in the case of in case of a
timeout for example what this allows us
to do is to take some mares and make
them look like they were painted by paul
klee or something like that checking
been on the principles that we have here
so again this is a sort of approach to
getting a more elastic and resilient
approach to things so in the event of a
failure or in the event of of things
simply taking too long we've encoded
within our scholar application using the
sort of multi-threaded semantics there
the possibility of failure and what to
do in the case of something goes wrong
of course that is a way of containing
things right so on this case the trivial
code here didn't actually specify where
the Python was deployed but we can now
deploy that Python separately so for
example tensorflow often functions
better in the presence of a GPU so you
might have a GPU enabled instance which
performs all of that model learning
separate from the computer optimized
instance running the Scala application
gives us a nice supervisory mechanism
there um right and so of course this
allows us to to take advantage of the
the arbitrary scaling properties within
being able to make those indications
across a large number of services from
ours or Scala pipeline all right next
example we're going to go further off of
the JVM and in this case I'm going to
show you examples that are purely from
an entirely different language as a way
of getting inspiration for what what
could possibly be in our future things
that we can learn from in the JVM
community I'm going to show an example
within a lick sir uh I'm not sure how
many folks has anyone here worked with
elixir use elixir at all okay a very
small number of hands how about Erlang
the beam okay more hands for Erlang okay
so it's been around for a bit longer uh
elixir is a is an erlang vm language
also known as a beam language it's a
more recently developed language than
Erlang just within the past several
years but it's a similar functional
language with a nice homo iconic syntax
like like a lisp one of the key features
that I want to explore in using the beam
technologies is the concurrency
orientation built deeply within the
runtime itself how does this inform how
we build systems and what it makes easy
checking in an hour what are we going to
build here I want to talk about the the
problem of maintaining our knowledge so
within a given agent in an artificial
intelligence design we presume that
there is a knowledge base that is an
immutable record of facts which we must
maintain so there's a particular example
here
I want to discuss which has to do with
the possibility of having a distributed
database which has somewhat of a leaky
abstraction in this case we're trying to
record out information about the
interactions of users so that we can
learn from them but we're recording that
information using the user ID as the
primary key this is all fine except for
that in this particular case of a
database not to be named by using this
as our primary key we've we've allowed
the abstraction to leak so ID 123 is
always going to be routed to this
particular set of servers within our
distributed database an ID 4 5 6 is
always going to be routed to this
particular set of servers this is all
fine unless you see an enormous spike in
volume on a given user for example if
you see a spam or botnet attack of some
sort in that case we have a serious
problem because at that point our
database is going to start failing it's
unable to keep up in this in this
arbitrary example here so in those cases
we can't perform those updates for those
users and so we have a a critical
challenge ahead of us which is how do we
continue to collect data how do we
continue to accumulate out our FAQ
database without having the problems
with ID 456 contaminate all of the other
user data acquisition activities going
on and so this is a this is sort of
knowledge maintenance problem here so to
begin we want to perform updates here
this is this is simply just recording
out that we've seen something about a
user happen of some sort and so in this
case this is our top level set up we're
going to do things like verify that we
have a fuse in place we're going to
write it to the database and parse out
our response the technique that we're
going to do here is is a technique
that's commonly used in reactive
programming idioms that originates from
for many years ago it's called a circuit
breaker technique you may have seen
physical circuit breakers in this case
we're using an erlang library called
fuse to allow us to have a very simple
way of defining a circuit breaker
within our fuse verification here we're
simply checking to see if we have a fuse
created yet for this for this given user
we're creating a fuse per user if not
then we'll create one and if
everything's ok then we'll proceed
otherwise if the fuse has been blown
then we need to send back a message and
say that we're in a failed state for
this user and we can no longer handle
updates for user four five six the
installation of the fuse here is is is
purely declarative we're just saying how
many times in this case we want to say
three times within a minute and then how
long do we want to leave the fuse open
for so this is every time we see that
error we're not necessarily going to
blow up in the fuse but in this case we
say if we see an error three times
within a short period we want it we want
to blow up in the fuse we want to leave
that fuse open and not dedicate all of
our resources to handling the updates
for this given user because it's not
possible there's there some sort of
error state corresponding this user and
we're going to simply have to give up
this knowledge in this case where we're
mocking out our our badly behaved
database using enum random and then
we're we're parsing our response in
particular if there is a so you can see
here this is a form of polymorphism on
the top if everything's okay then we
just simply send back the ok value but
in the event that there is some sort of
error returned out by our database in
this case we're going to melt the fuse
that is signaling to the circuit breaker
mechanism that we have failed and we're
in an error above state going back up to
our top level then now you can see how
these pieces compose in particular we're
going to check that our fuse exists then
we're going to attempt to write out to
the database and then we're going to see
if we got a good result back if in the
event that we did not get the result
back that we wanted we're going to we're
going to message that back to to a
higher level system to allow that to
handle on not all the aspects of the
runtime may be clear here but this is
actually a fairly reactive solution here
particular is very focused on on the
resilience case how do we encode the
possibility of failure within our system
and this
case we've explicitly said that failure
can occur at the user level and we've
we've declaratively set the parameters
for response to failure of course we've
contained that failure quite tightly and
that we've relied upon a supervisory
mechanism here behind the scenes this is
a full actor system right that's what
the Erlang vm was created to to do we
have the ability to to to supervise the
possibility of these errors cascading up
from our database right and this this is
a sort of way of uncoated encoding the
uncertainty of what we have within our
application in particular we don't know
if we're in an arid state in advance and
so we explicitly build that logic in and
allow for ourselves to to detect that
dynamically on the fly all right another
artificial intelligence example here in
this case one of the challenges we often
have to deal with in the construction of
artificial intelligence is actually
understanding what we've built at all
this is not a unique problem to
artificial intelligence but it's very
common and it's very challenging in
particular once we've constructed an
artificial intelligence typically
speaking the behaviors this state space
of all possible choices that the
intelligence can take is vastly larger
than then our ability to statically
reason about it so we need ways of
characterizing what is the behavior of
the intelligence after we've already
constructed one so we need we need good
tools to be able to scan over what what
is our implementation what are its
properties what can I expect of it the
tool I'm going to reduce here is
dialyzer dialyzer is another Erlang
technology in this case it's started out
life as a pure linter it was only
concerned with things like a basic code
formatting since that time the Erlang
community and the developers of dialyzer
have a lot it so that now dialyzer is
the effective way of applying success
typing the type system of the Erlang VM
and so this is an optional step that you
can apply to compiled Erlang or elixir
code and it will tell you certain prop
these of your program in particular
success typing is a form of optimistic
outer bounds typing so it's it's very
different than the sort of expectations
that we would have from statically typed
languages like Java or Scala so I'll
show you a little bit about how this
works within the context of the problem
of building an ensemble model for those
of you who are unfamiliar with the
technique of ensembles basically all
we're saying with an ensemble of models
is that we're using multiple models in
combination as a way of making a single
decision so in this case we're going to
take in some raw input we're going to
generate out features we're going to
then make different predictions across
all of our different models and then
we're going to combine them together in
such a way that we produce a combined
decision as a result of the composition
of each individual model first we're
going to generate features for anyone
who is unfamiliar with this terminology
a feature is a is a semantically
meaningful derived representation of our
raw data and so to generate features is
to produce features from raw data in
this case we're actually using that spec
annotation there that comes from the use
of dialyzer we're going to say what our
signature is here so this is something
we don't have to do generally speaking
within an elixir program we don't have
to say what our type expectations are
but we're going to do so here because we
want to get some help for ourselves and
so we're going to annotate that on here
and say that our generate features
function should take a number and should
receive a number next rule to find out
how we apply all of our models each
individual model may have different
properties so and it's quite common to
use many models in combination so this
is what you see here on the screen here
could be thought of as a sort of model
library this is the full set of
resources we have available to perform
predictions and so I'm not sure if it's
entirely clear here but there's there's
a bug on this slide does anyone see it
if you look at the type signatures we
and we know that we we want to take a
number and we want to return an
and these all look fine except for model
p there's something wrong with model be
all right now to use all these functions
in combination we're going to want to be
able to operate on them as a collection
of functions so this is an idiom I call
parallel function mapping not having
found a better word basically what we're
doing here is we're defining out a a
sort of pipeline of tasks in the first
one we're going to map over our
collection of functions and start out an
asynchronous task that is a unit of work
to be computed in the background when it
finishes it will then be returned out
and invoked all right and so then
bringing that together at this point
we're pretty far into our whole process
we're down at the predictions level so
we've generated our features we've
defined out what are all of the models
that we want to be able to call and then
we've actually invoked each of those
models using that parallel mapping
functionality so each of them will
happen asynchronously in parallel and
then we just need to once on balam
together and again we will define out
our ensemble function with a type
signature here saying that we have a
collection of numbers which should
return and humble number in this case is
just a stub implementation of all these
things these are sort of trivial fits on
the slide sorts of implementations but
with this sort of information we can now
approximate the behavior of a more fully
implemented artificial intelligence
system we can reason about what are the
behaviors of all the models within the
system and what are the possible
outcomes of that in particular dialyzer
is able to analyze all that and give us
results in particular it can tell us
that we're not going to succeed after
we've compiled our code we optionally
run dialyzer on top and what it tells us
is that we can't perform a prediction
because there's a type error within the
invocation of model be in particular we
cannot call a string bell up case we we
have it we have an error within the
implementation we cannot uppercase the
number three that's that's not a valid
type and of course the actual top-level
ensemble function will never be called
in this case so this is after
compilation we've determined certain
behaviors of our system that are
undesirable the fix here is trivial
we can simply remove model B from our
library or we can fix the implementation
depends on what is most appropriate for
our application I showed you that
example while it at the absolute
function level is is somewhat trivial to
give you a sort of introduction to
different approaches to how we can work
with type systems because I think this
is really interesting state space to
explore especially as as as JVM
developers and especially the more
polyglot you like to approach your
development so I wanted to find out this
spectrum between dynamic and static
typing and that's really all I mean by
this so at the dynamic end of the
spectrum we're talking about there being
very few guarantees we get about our
code before we directly invoke it all of
the all of the typing guarantees being
done on the fly during execution at the
very static end of the spectrum we're
saying that there are all sorts of
things we can do to our code before ever
executing it using tools like compilers
or letters and things like that so on
that spectrum I would personally place
Scala pretty close to the static static
end of the spectrum you get a very
extreme guarantees about the composition
of our of your application when using
this the Scala type system it's it's
pretty extreme and it's much like when
working in Haskell you spend in a normal
amount of your time making sure that
you've satisfied all of the type
guarantees before you can do anything
useful at all there's there's no
possibility of executing your program
with a type error in there so that is
good news and bad news depending on your
application but it's a very extreme
guarantee and i would say compared to
java it's a it's a more extreme
guarantee java imposes less as a result
of its type system if you just simply
look at the sort of assertions that are
held within the Java type system versus
the Scarlet type system or if you look
at how much time it takes to evaluate
them that the Java Sea is much much
faster than Scala see for example as a
result of scalzi having to do much more
work so that we we effectively are
passing off more guarantees to runtime
behavior within java than we would
typically be doing with in scala where i
think this gets interesting is not we're
particularly you fall on the spectrum
but how does tooling enable us to make
choices that are appropriate for our
applique
and allow us to adjust based on our
preferences particular i showed you an
example of elixir Erlang code here which
can have all sorts of errors and still
compile and still run you can still do
all sorts of things with and rely upon
the type system to be invoked at runtime
but as we saw with dialyzer there's the
possibility of getting more right that
you can get greater type guarantees
using success typing which is still a
far weaker type system then then then
Javas but you can get more when you
choose to do so you through optional
invocation of the typing system which is
a really exciting possibility once you
get a chance to work with it if you're
not working on an elixir Erlang
environment this is this might be kind
of a strange tool but I would point out
that if you're if you're working in
closure closure has a sort of similar
approach in some respects a closure is
another JVM language in this case it's a
JVM Lisp and it has a fairly dynamic
approach to types it's not as loose as a
lick sir Erlang in the in that respect
that there is a there's a tighter
relationship to the Java type system in
there but there's a lot of dynamism
involved them but using tools like core
tight or prismatic schema we have the
ability to much like we did with in the
elixir example optionally encode what
are the type expectations of our
application within our closure program
which is really powerful to be able to
do so because it gives us the choice
around which are the important areas of
our application in which we need the
sort of guarantee and which ones are not
so kind of recapping that sort of type
level overview of what we have going on
within elixir an erlang first that we
see here that we have an ability to
reason about resilience here that's
that's really powerful rent that we we
can decide when exactly do we care about
handling errors do we want to know
before we compile do we want to know
after we compile do we want to know it
when we run it this gives us strategies
for supervision in particular if we want
to delay all of these things we
and we can use our build tooling as a
form of supervision for example if
dialyzer is invoked after you've
committed your code but in your
continuous integration server you have
the ability to to then elevate the error
handling logic up to that system and so
that prevent something like a continuous
deploy right and these are these are
again ways of dealing with uncertainty
again and then we saw several examples
of have how we use higher order
functions to encode all of that so I
introduced all of that material to end
with a sort of perspective on what this
means for the future of how we build
reactive machine learning systems on the
JVM because this is a JVM conference and
I very much tried to take the
perspective of the average attendee here
who's very interested in and how does
this affect me and how does this affect
the areas in which I work in and the
tooling that I use how does this help
first I would say that the JVM needs to
continue to lead to be an excellent
place for the development of some very
specific techniques and tools we saw
like the long list of concurrency
distribution techniques that are going
on within the JVM ecosystem uh if you
look in the in the book that I've
written on unreacted machine learning
you'll see things like predictive
microservices being defined in acha HTTP
we have all of these great tools
available to us if you've never built a
large-scale distributed data processing
pipeline and spark it will blow your
mind how much simpler it is then the
solutions that have come before the JDM
is leading in this area and then I think
that's incredible and that's something
that we should be very proud about and
really work to to push the envelope even
further another thing that really care
about it the JDM is that it's a polyglot
run time that we've we've created this
runtime where we can have this this sort
of a weird functional programming code
growing on top of abstractions that were
never meant to support them in in places
like closure or Nam in Scala and I think
that that allows us to to use the JVM as
a testbed for experimentation and growth
and allows us to have more incremental
approaches to how we add language
features to any given language due to
the powerful interoperation be
tween various JVM languages we have the
approach of saying well we we think all
of this functional programming
higher-order function stuff is
interesting you know maybe yeah we do
want to try that acha approach to
supervision and asynchronous handling
but only in ten percent of our
application we can do that within the
JVM and this gives us a powerful way of
exploring incrementally what are the
possibilities and what are the benefits
for a machine learning system using
these using these technologies the other
thing that I would say about the future
of react machine learning particularly
on the JVM is that there's still the
possibility to learn and that is that we
we can learn from other ecosystems and
other other innovations with in other
communities particular as I mentioned
before the vast majority of deep
learning research right now is going on
in native code with python bindings or
pipe native accelerated python in some
form and this is huge and important to
the development of machine learning
artificial intelligence deep learning
techniques are tremendously powerful and
that we need to have a way of
interacting with them and so there there
is a project out there to to implement a
lot of deep learning functionality
within the JVM it's a deep learning 4j
for anyone who's interested but we need
to accept that we need ways of
interoperating with the innovation that
occurs on other runtimes using
techniques like like services RPC
mechanisms like I demonstrated the other
thing I think we could learn more is
about simple concurrency there so the
elixir examples that I showed there are
backed by a runtime at least as
sophisticated as the JVM that has all
sorts of opinionation' about how
concurrency should be handled using an
actor system within within the Erlang VM
and that was simple because it because
so much in my opinion ation was in place
we didn't have to specify anything more
than precisely the semantics that we
cared about how should the circuit
breaker open when should it clothes etc
if you've seen the equivalent code for
akka right now it's horrendously
difficult and that's a challenge for
folks trying to build their applications
in a reactive manner who know that they
should really care about
out the behavior of their system at
scale and use message passing to safely
separate out components that's just not
easy to do today and that doesn't matter
if you're a scholar developer java
developer there's still a lot of
complexity and boilerplate in dealing
with any form of multi-threading
concurrency within the JVM and i think
that there's better ways that we can
choose to build abstractions to make
that simpler and easier the last point I
want to make there is around modularity
and so I showed you a lot of pick and
choose approaches here and I think
that's really the powerful opportunity
that stands ahead of the JVM ecosystem
we saw with an incremental type system
excuse me the success typing system
being applied it incrementally in the
case of elixir and Erlang we got to
decide precisely win would we like type
guarantees on which compiled artifacts
you can't really do that within the the
JVM as well as I'd like you to be able
to do today there's forms of that as I
mentioned with enclosure but I think
that we could make a lot of progress by
being able to allow people to opt in and
get more of those guarantees on an ad
hoc functionality so being able to build
up technologies that allow us to say
that we this component is dynamic we
want our sort of guarantees here that's
that's challenging right now within a
JVM application it's easiest with
enclosure but is still pretty hard if
you if you try to use something like
Scala for example but I think there's
great power in being able to use the
sort of opt-in functionality that you
can and say like a polyglot code base to
allow you to encode in your semantics in
a very detailed and tactical manner that
allows you to apply reactive principles
and strategies when you so choose I have
a few things to pin you about for later
if you want to check them out first you
know you certainly welcome to interact
with an artificial intelligence yourself
on your own time you can hire an
artificial intelligence personal
assistant who will schedule all your
meetings for you and I would love for
folks check it out we have a free tier
you're welcome to ping me about your
experience in working with Amy Ingram
the other is that I've written a book on
this topic and you're welcome to check
that out as well you can use the code
there CT CT w de vb
al for forty percent off of my book or
any other book from Manning and without
that's all I have so thank you very much
for listening today I love to take any
requests does anyone have questions I
can barely see you soon
alrighty well thank you all very much
have a great day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>