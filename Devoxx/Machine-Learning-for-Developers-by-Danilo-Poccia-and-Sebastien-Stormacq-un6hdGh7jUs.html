<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning for Developers by Danilo Poccia and Sebastien Stormacq | Coder Coacher - Coaching Coders</title><meta content="Machine Learning for Developers by Danilo Poccia and Sebastien Stormacq - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning for Developers by Danilo Poccia and Sebastien Stormacq</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/un6hdGh7jUs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon thank you for being here
my name is Danilo and I'm Technical
Evangelist in Amazon Web Services if you
have any question that I'm not able to
answer here today you can find me on
twitter as dan elope and Danny look is
also my github account and I'm Sebastian
stomach I'm also working for Amazon but
for Amazon Web Services anymore I'm
working for Amazon Alexa I'm solution
architect I'm helping customer to
develop alex has skills and as your my
accent and your accent doesn't make it
obvious we are both days in in London in
UK ok so as you may have realized there
was a change in the agenda so the
speaker that was supposed to speak at
this time slot here to go to the doctor
so unfortunately for you there's the two
of us speaking and what better topic
after lunch then machine learning so the
idea today is to give you this is really
a session for developers to give you an
idea of how machine learning works what
is the terminology and then we will
together build a model using some public
data sets and then we will move on to
add predictive capabilities to your app
and and we will finish to show you how
to integrate machine learning and how to
integrate prediction with voice based
interface how you can talk to a machine
learning system how you can talk and ask
prediction to a machine learning system
using Alexa and the echo devices I
personally love this picture anything
here anybody here in the audience knows
what these people are what these people
is doing these people are counting
tickets from the London Underground this
is March 1939 and they were counting for
something about four million tickets to
understand how the tube the Underground
was used and to plan for the next
extensions of the underground so when I
see this heroic effort
I think that today really have no excuse
to use data to improve our life to
improve the way we work and the way we
live at home what we see is that our
customer starts with approach dissimilar
to the one that we saw previously but of
course everything is automatic so they
start analyzing data in batch so maybe
they have they built a daily report a
monthly report a weekly report and in
this way you can look at what's
happening in your business in your town
but you look at the past the second step
is usually to start looking at what is
happening right now introducing
real-time monitoring to your platform so
that you can receive immediately alerts
if something is not going as you were
planning and the third step is moving in
looking to the future
so use the data that you have to build a
model and create predictions actually
predictions is probably the main topic
that we will move around during this
session and to build prediction you need
data you need some data that it's the
leverage on top of which you can
understand how things are working and
extract some pattern and to do that you
build a model so the model is the way to
obstruct the behavior of data in a more
general approach so that you can create
predictions and how can we build models
so this is really one of the most
promising fields of these years it's not
a new field it's part of the artificial
intelligence umbrella and machine
learning is the capacity to let machine
learns and the idea is to extract
patterns from data to find patterns in
data so that the machine can understand
not just the numbers but the behavior of
the numbers for example and machine
learning is probably going to really
change the way we are going we're living
in the next 10 20 years
there's people suggesting that probably
machine learning will have an impact on
our society comparable to the Industrial
Revolution a couple of centuries ago so
when we talk about machine learning
there are two ways that you can learn
from data the two main ways are using
supervised learning and unsupervised
learning with supervised learning what
we mean is that you have data that is
labeled by level that I mean that you
have all the information of what
happened in the past including the
variable that you want to predict
unsupervised learning is a more general
approach when you want to infer a model
from data that is completely unrelated
so for example you have 1,000 or 1
million customers in your company and
you want to understand if this customer
have something that links them together
there's another category that sometimes
is considered at apart from supervised
and unsupervised learning
it's called reinforced learning and it's
similar to the way that we we learn so
the idea is that you don't have a our
brain that don't learn like in in a
period of time and then we stop learning
and we only apply what we learn we
continuously reinforce what we learn
with using the feedback that we receive
from our senses so reinforced learning
is really useful when you have a dynamic
environment let's assess for
self-driving cars or even for games so
if you want to teach a machine to play
chess you can use reinforced learning so
the machine can start playing at random
and based on the feedback it can improve
and learn how to behave better so these
are probably two of the most common
examples for unsupervised learning the
idea of clustering is really that the
most common example so as I said before
consider a company usually you have lots
of customers you can't know each
customer in the tail so you want to
usually create clusters of customers to
understand maybe the new customers
customers that use some products in a
different way
customers that are at risk of leaving
your company going to to use services
from another company and what you need
to do is similar to what you see
visually in this slide so you have
points and these points are like a
visual representation of the different
variables that you know from your
customers like how much they spend on
they could spend how much how many
tickets they open on your customer
support and you want to find the best
way to group these points together
so probably visually here using just our
eyes we can say that probably three
cluster is the best set up but this is
really easy because we are working only
in two dimensions think of this with
tens hundreds or thousands of dimensions
and of course you need a machine to
really do proper clustering with
supervised learning so when you have
level data usually these those are the
three targets that you want to reach so
one is to do regression so you want to
predict a numeric variable like you want
to predict the traffic in a town so
probably if you want to predict the
traffic in a town you want to predict
the average speed in different streets
in kilometers per hour or meters per
second so it's a number usually for the
machine is a floating-point number then
maybe you can round up to an integer if
if you want to predict something that is
an integer and then there is
classification binary classification is
classification among two different
classes the very common one that you
probably use almost every day is email
spamming so you receive emails and you
want to classify this email as spam or
not spam another extension of that is
that when you have more than two classes
a multi-class classification for example
you you can classify the movies by
general and you can have comedies
trailers historical movies and so on so
you have multiple classes when you start
working and playing with machine
learning a very very important aspect of
your experience is validation because
it's very easy to create very complex
models very complex procedures but if
you don't have a way to validate your
model then you really don't know if
everything that you did make sense at
all the best way to do validation is to
take all your data so that this is the
data from the past and split the data
into different sets one set
you use it for training for creating the
model the second set
you always keep it aside from the
training of the model so that it never
touches the model and when the model is
ready you apply the model on the
validation set and you can really see if
the model is capable of extracting what
it has learned on the training set on a
different set of data it's very
important that you don't use the same
date for the same data for training and
for validation otherwise is easy to get
a bias model that works only on those
data and doesn't work when then it gets
real war data usually training a model
means that you have some sort of error
function that defines how far you are
from your predictions to the value that
you want to reach and this becomes a
mathematical problem so you are an
optimization problem you want to
minimize there a function that you
define on top of all the parameters of
your model one one issue that it's quite
common with machine learning models is
overfitting so for example let's look at
these points here maybe they predict the
speed on a street over time so on the
x-axis we have time on the y axis we
have the average speed of cars and we
can say okay I want to use my most
complex model to understand and predict
how this the speed works and you can end
up with this very perfect fitting this
is a curve that goes exactly across all
the points that is in your data set that
are in your data sets and it's fitting
perfectly and if you use this curve to
predict new data you get this prediction
so you probably say okay if I go back in
time before my first point probably the
speed was faster if I go in the future
the speed would be almost the same of my
last point actually data from the real
world is always dirty you never get
clean data so it's this is a much better
fitting for that values it's so a
simpler model usually gives you a much
better prediction and the prediction as
you
we are completely different from those
that you were getting from one oversee
that function so this is another risk
and one way to reduce this risk is to
use of course validation so because
validation is using data that is outside
from the training data you can check if
there was overfitting usually the same
model doesn't work on different data
another way is to use regularization the
idea is to add to your error function as
some sort of a cost that grows together
with the sides of your parameters so the
more parameters then the bigger the
parameters are in your model this cost
grows and there are two ways of using
regularization one is called l1 that
means that you use the absolute value of
the sum of the sum of the absolute
values of the of the parameters and then
you have some sort of param variable to
reduce this when you use it in together
with your error function l2 means that
you use the squares usually l2 is the
most common one and works very well so
most machine learning pre-configure
platforms use l2 regularization to avoid
overfitting on top of the validation so
now we have the data data that we get
from the test we want to get our model
we need some tool to create this one
tool that is very common recently for
machine learning is a Apache spark that
you can for example run on AWS using our
elastic MapReduce platform because
Apache spark as a very powerful library
it's called M Lib it's an open source
library that you can use and includes a
lot of machine learning models that you
can use and fit and validate and it also
works in memory so Apache spark compared
to the older MapReduce approach is
working in memory so data is working in
memory and this is usually much faster
for machine learning algorithms that
usually work and I iterate on the same
data again and again of course if you
used a platform like passive spark you
need to really know what you're doing so
you need to choose the algorithm and you
need to
treat the data into validation training
test different parameters so if you need
some skill and in Amazon we found that
you need a data scientist if you really
want to work on machine learning at this
level the problem is that for us was
impossible to have a data scientist in
all of our project teams usually data
scientists are a quite a precious
resource and you want to use them deploy
them only when you really need and so
the idea was how can we overcome this
limit in Amazon we use machine learning
a lot for recommendation for predictions
the idea was let's create a platform
internally that is easy to use for
developers that need to know only how
this works they don't need to be dire
scientist and then this platform can be
provided via API to all the developers
inside Amazon to add some smartness to
the to their applications and that's how
I'm not so much in learning started
internally to Amazon we use it for a few
years and then one year now ago two
years we made it available for everybody
to use as part of Amazon Web Services
and with Amazon machine learning you can
build models that can create batch
predictions so you can have a set of
data to predict and you get all the
predictions or and this is what we will
use in the demo afterwards you can do
real-time prediction so when you have
your model up and running you can query
the model and get predictions in almost
real-time the matter of milliseconds
those are the the kind of optimization
techniques that we support in Amazon
machine learning so those are the three
of supervised learning that was also a
regression binary classification and
multi-class classification there are
some details here I'm not really entered
into the taste but for people coming
from a mathematical background they know
that gradient descent is one way to find
the minimum of a function so the idea is
to use gradient descent to find the
minimum of the error function and the
idea of using stochastic gradient
descent means that since you have
possibly hundreds or thousands
variables it's impossible to really
solve analytically the problem so you
iterate on different variables using the
gradient on different variables one at a
time one at a time so let's start with
the first demo we will have quite a few
demos during this session to see how we
can create a machine learning platform
and then we will use the same model that
we create in the demo with a voice
interaction in the final part as an
example I wanted to use something that
maybe can be useful in a town and I
wanted to start with a bike sharing so
bike sharing is one of those things that
usually people appreciate in towns they
make us live better and reduce traffic
one way was to one problem and usually
bike sharing companies have is to
understand where to deploy the bikes and
among across the different among
different stations because you don't
want to run out of bikes in a specific
session so having a way to understand
how many bikes will be used a specific
moment in time can be a very first step
and I will see helped by this website
anybody here knows cuddle so cuddle is a
public website you can subscribe for
free and they have lots of public data
sets that you can use to build machine
learning model and they also run
competitions on top of these data sets
so you can participate and see if you
find a better solution than ordering in
creating a machine learning platform in
this case I used a specific data set
that is from the town of Washington in
the USA so this is data that they made
publicly available in their open data
project it's data from a few years ago
but the idea was really to show things
works it's not important that it's the
most updated data and the same approach
can be using with the different
applications when we started back
sharing you can you can start to
understand how bike sharing works why
I'm saying that because usually the
first step of building a machine
learning model is to understand your
data you can't really use machine
learning as a complete black box you
need a little bit
to understand how the data how the the
the model that you will going are going
to build will work in case of
bike-sharing this is a first way to
visualize data for our brains for our
brains is much easier to understand that
if you visualize it than if you just see
a list of numbers in this case I'm
drawing the number of bytes that are
used in the different days of the week
and in during the different hours of the
day and you can see that there is a
pattern so there is a lots of bikes that
during the the week are taken in the
early morning and in the late afternoon
and you can imagine that this is for
commuting and then during the weekend
there are some more spread usage of the
bikes because people use them probably
to just move around the town but the
user of this bike sharing service can
have a permanent subscription so they
can be registered users or they can be
casual users that just rent the bike
occasionally so if we split these
graphics into these two domains so on
one we see only the casual user on the
other one we see the registered users we
see that the behavior is much more
easier to visualize because the casual
users they used bikes mostly in the
weekend and the registered users they
use the bikes only mostly in the during
the week for commuting so in this case
you can try to create a single machine
learning model for all your users but
it's very clear that there are two
different platforms here so it's
probably much easier quicker and faster
to build two different models and then
add them up then to try to create a more
generic model and that's what we are
going to do in our demo too many
computers so this is the this is the
data so first of all let's let's have a
look at the data this is the original
data that I got from kaga this is a
public data set public data sets are
really useful in machine learning
because since they are always the same
people can play around test new things
and compare their results with the
results that other people
from the same data so this is the best
way to compare different algorithms and
different approach so in this case I
have this CSV file with the date and the
time of the day the season the season is
one two three four four the four systems
a flag a boolean variables that tells me
if this day is a holiday of course I
already will change the pattern in our
bikes are used so it's important to edit
another flag will tells me if this is a
working day or not whether it's easy to
understand that weather is important
because under every rain you will have
less rentals than if it's a sunny day
the temperature the perceived
temperature the humidity the speed of
the wind then for this with all this
information so I know at the time all
the other variables I know how many
casual users rented a bike how many
registered users rented the bike and
then I have the the the sum of all the
users that rented a bike at that time
and this is all the information for from
2011 there's quite a lot of Records here
so the first thing that I did was to
split the data into registred and casual
and this is also important for another
reason so if I let's say that I want to
predict the final count define the total
number of users and I give this data to
a machine learning platform the first
thing that the machine learning platform
will do is to learn that the take the
total count is the sum of three casual
and the registered users and probably
the model with just settle saying okay I
can discard all the other variables I
just can use these two variables so it's
really important to not put a machine
learning platform in in a machine
learning platform in a situation like
this where you can just add two
variables to get to the third so I split
the data into registered and casual so
it's the same as before but here I have
only the casual users and if I go on the
register I only have the registered
users and then I I still started I was
still thinking have I all the data that
I need
so usually in machine learning the
different columns that that compound
your data are called features and one
important step before creating a model
is called features engineering so see if
you can create or find other features
that can enrich your data before
creating the model so if you look at
this data you have the day of as a date
and you have the you have the time but
you don't have the day of the week so
you don't know if generally the first
2011 was a Sun it was another day that's
that's for sure but in any case you
don't know if it's a Monday a Fri Friday
and we notice when we will slice the
data that the day of the week has an
important impact on the result so what
you can do in this case is to enrich
your data so I created that with in this
case I think I use some Python script
something very basic to add another
column that I called day of the week
that gives me the name of the day of the
week for each of those dates and usually
when you do Fisher engineering the best
approach is to iterate on the model so
maybe you created the model with the
data that you have then you try to do
some engineering change the features add
more data and then you build another
model and then you compare with the
validation which model behaves better in
this is what I did so I of course this
was a useful information so now I have a
list of registered users and the list of
casual users with the day the time and
in the end the day of the week and I can
use those to create my model so this is
the this is the a tablets console in the
analytics section I have the machine
learning platform and what I'm going to
do is to create a new model and first I
need to create a data source and then I
can create the machine learning model so
I will go quite fast here because since
we are replacing someone in this session
but we still will be here at the final
session at 5:50
the idea is all the things that we will
need to go fast during this demo during
this talk we can have more time to do a
dive deep session at 5:50 so if you are
interested you will not be bored at the
end of this session please come so here
I have the the bucket and what I can do
is I will just show the the first part
this is the way you can store data on
AWS I have my files I have for example
the casual users I can call this the Box
casual bike-sharing users I can verify
that I can read the file and what the
platform will do will automatically scan
the file the first line is the column
names so I can use that here and the
platform will try to understand each
field what kind of data contains like
the temperature is a numeric field the
date/time is a text field for a for the
platform and you can check if everything
works for example here you see the
season is is a numeric field but I know
that there are only four seasons the
problem is that I use numbers to
represent those seasons 1 2 3 4 so it's
much better to set this as a category
than a number so that the platform knows
that this is a closet set of values is
not some number otherwise you can expect
that there's a season 5 like for a TV
series now I can go on and the platform
will ask ok of all this platform which
is the value that you want to predict
and the this is the casual user so I
want to predict the the casual variable
so this will put these variables as the
target of the model and all the other
variable as the input for the prediction
of the model I can now review and
continue there is a step that I was very
fast where you can actually fine tune
the creation of the model so if you are
more expert you can enter into our data
is represented
so I I'm saying that for more advanced
users this is the step so I'm using the
default recommended the recipe and
everything otherwise you can start and
customize how that is manipulated by the
platform
my recommendation is start with the
default and then iterate and try
customization to see if they improved
the results compared to the first model
and now you can create the machine
learning model the platform will import
the data created the data source and
creates the model this is not very big
so usually this takes something like 10
minutes but I already created the model
and if we go into machine learning
models we see that we have the creation
of the current model and we have also
the this is the creation sorry here
model you can see that there are models
this is the model that I'm creating
lease pending and these are the models
that are ready created and for this
model I also enable real time
predictions but we can use the
application that we'll use with a voice
interface to interact with those bottles
and create real time predictions of how
the bikes are used okay with this we can
go back to the slides and just as close
up on this first theoretical part of the
presentation what about deep learning so
in the last year's deep learning is a
hot topic in machine learning and the
idea is that for those that doesn't know
deep learning is pushing forward an
older concept these neural networks with
neural networks we create a simulation
of how our brain works we create a
simulation of our neurons and this
simulation is of course a simplification
is called a perceptron and then we
create a network of layers of
perceptrons there are behaves similarly
to our neurons and then we use this
network to give predictions each
perceptron in this network is actually
summing the input of the that he
receives from outside
or from other perceptrons and it's
something deaths that those values with
some weights and the w1 w2 w3 and so on
these weights are the parameters of the
neural network the one that you need to
mean find to optimize the neural network
for our specific behavior and the output
of the perceptron is usually something
like this so if all the sum of these
values is below some threshold the
output will be zero if it's above some
threshold it will fire like our neurons
in the brain does do it will fire some
output to other perceptrons that are
connected with this one for mathematical
reasons who usually don't use really a
squared function but you use something
that you can derivate so it's a little
bit easier to use mathematically but it
you get almost the same output as the
square function neural networks were
pretty old but in the past with the
computing power that was available the
experience of the computer scientist was
that if there were too many neurons too
many perceptrons it was really hard to
train a neural network especially if
there were a lot of layers what so in
the first end of the 90s first year of
the 2000s neural networks were not used
a lot but then people started to
understand it with the new power that
was available especially with graphical
processing units we could train we have
so many so much computing power that we
could train neural networks even if
there were a lot of narrow of
perceptrons of neurons and people
started also to create different
topology different architectures of
neural networks and those let's say that
the use of so much computing power
together to create in different
architectures gave birth to what we now
call deep learning where we have
specific architecture of neural networks
that can work for image recognition or
voice recognition and use a lot of
neurons that were not it was not
possible to train before
oh no on AWS if you want to play with
deep learning we have a number two
machine imager so this is a an image of
a virtual machine that you can start and
you find the most common deep learning
frameworks already pre-installed so the
SCAF tensor flow tunnel torch there's
other tools that you can use to analyze
and work with data such as anaconda it's
a platform built on top of Python and
you can just use this on an easy-to
instance and we have specific ec2
instances with lots of GPU power so
graphical processing using power like
sixteen graphical cars from Nvidia that
you can use to speed up your training to
give you an idea of the cost if you use
the the smallest p2 instance that is
quite powerful you are in the range of
90 US dollars cents per hour so you can
train easy model with just a few bucks
so lots of talking about machine
learning why it's important how it works
but what happens if you want to use your
application in a more intuitive way
maybe by voice Thank You Daniel
as a developer I don't know about you
but me as a developer I'm not a magician
mathematician I'm not a data scientist
so I'm totally excited to be able to
include machine learning and prediction
in my application in a very easy way
that's great for us developer but how
can I expose that to consumer to
end-user
to my mom and my dad the barely can use
of smartphone so how can how can they
interact with machine learning and what
is better than voice voice is something
we learn since we are baby most of the
people on this planet even if they
cannot read and write they can at least
talk so at Amazon we truly believe the
future of human to computer interaction
is about voice and we have design device
for that and we are selling a device in
a bit more than two years it's the
Amazon echo so what is an Amazon echo
what can you do if you play music answer
questions get the news and weather
create to-do lists and even read books
for you so that's the Amazon echo you
can talk
I have two echos here I have a big one
this one is not connected it's just to
show it to you that's the white version
and I have a smaller one the dot which
is a bit more difficult maybe for you to
see I'm using this one for the demo
because it has a jack oh it put so you
can hear what Alexa will actually answer
to that so let's try to speak to the
echo and see if it works because of the
speaker of this room the sound
configuration is a bit different and in
your living room or your bedroom so
let's let's see if it works with the
speaker Alexa good morning Alexa good
morning good morning on this day of 1983
Bill Gates introduced the world to
Windows 1.0 it was regarded as a flop by
critics at the time partly due to its
reliance on this newfangled Mouse
contraption that people were talking
about Alexa where are you I'm here in my
head is in the cloud there is a bit of
latency this is because of the network
here and the Wi-Fi the Wi-Fi is really
good but I can tell you that at home it
answer must faster than it does here so
you can ask terms of question - Alex a
general question like Alexa Alexa
where is Antwerp
sorry I didn't can't stand okay let's
dim the speaker you can ask her to
control your lights when I'm coming back
home I can say I'll exit on the light
zone or increase the temperature in the
living room we Philips you with nest
thermostat you can play news it Alexa
play Michael Jackson on Spotify playing
Michael Jackson from Spotify Alexa stop
because I want to continue to talk to
you it it can also answer very real
question like Alexa Alexa how much wood
would a woodchuck chuck if a woodchuck
would chuck wood a woodchuck would chuck
all the wood if you could chuck if a
woodchuck could chuck wood so that's the
type of thing you can do you can ask her
to tell her joke my kids are playing
rock paper scissor every day with we
fell exam so how does that work for you
developer Alexa lives in the cloud she
it was one of her answer so Alexa is
running on the AWS cloud and this is
where we are doing the half things for
you we are doing the speech recognition
and we are doing the natural language
understanding the speech recognition is
to transform a voice into text
when I say Alex I play Michael Jackson
on Spotify that's the son of my voice
that we transform in world play Michael
Jackson the second part of the analysis
is to transform that text into an intent
to understand what I mean when I say
play Michael Jackson plays the verb
Michael Jackson is the artist and
specifies the source of my music library
that's the second part of the analysis
the natural language understanding and
all that happen in a real time when
you're talking to Alex up on the left
side of the screen you have the device
that can talk to Alexa the echo the dots
but also the fire TV from Amazon
that can integrate with Alexa the good
news is that we have open the Alexa API
it's called Alex's voice service so any
device that you are building yourself
any device with a microphone a speaker
and a network card can actually tap into
the exact same API so you will find
easily on github and YouTube some
tutorials to explain you how to create a
raspberry pi that actually talks to
Alexa as long as the device is
registered to you Amazon account you
have the exact same capabilities as the
one we have in the echo or in the dot so
this is for device manufacturer hobbyist
on the absent on the left side of the
screen we are talking with car
manufacturer home appliance imagine all
the things you can do in home if you
have Alex I integrated inside your
electronic device on the other side of
the slide we have what we call the Alex
a skill kits you as a developer can add
capabilities to Alexa we have roughly
two different types of capabilities to
Alex are the one that are built in into
the platform the one that we Amazon are
building the one that I just demo in
front of you but you as a developer you
can add skills you can add like
application to to Alexa and we have an
API for that as well we have an SDK and
this is what we are going to use for the
demo today to create a skill for you
that integrates with the machine
learning prediction system called the
prediction system that they need or just
talked about it so the flows goes like
that you speak to Alexa Alexa listens to
you as soon as the blue light is on it
means that it streams your voice to the
cloud it's record your sound and sends
that to the cloud and in the cloud we
are doing the half for you by the way we
are also using machine learning inside
Alexa to train the voice recognition
model
alex is continuously learning with
trainer with real prediction data so
from one week to the other from one
month to the other you can get new
capabilities or higher accuracy in terms
of speech recognition once we detect
that the intent is actually for your
skill we will send you a message we will
send you a JSON document to your code
and then it's up to your code to
actually do something with that so when
you implement a skill you need to
right code that's the things you and I
are doing for living we are in right
place to talk about that and all what
you need to do is to receive a JSON
document that we are going to send you
so the output of the Alexa cloud is a
JSON document that specified the intent
what the user wants to do and maybe some
parameter if I'm asking Alexa to order
margarita pizza order pizza is the
intent and the margarita is the type of
pizza it's a parameter it's like a
variable inside voice model so you will
receive that JSON code that JSON
document you do something called backend
API on your side call third party API
and you give us another JSON document as
the answer the JSON document will
contain the text to speech so the text
that Alex a must answer we will generate
the voice of Alexa in the cloud and
stream the sound back to the device so
the device are really really simple
there is minimal software on the device
just microphone speaker and network
interface everything happens in the
cloud so you have to write clothes you
have to write code sorry yes write code
so where are you going to all that code
actually you can all sit anywhere as
long as you can speak HTTP receive a
JSON document and send back a JSON
document that's good for us I have some
demo that runs from my laptop but if you
are going to go into production you need
to ask your code somewhere else then on
your laptop it might be on-premises
inside your data center but we are
Amazon and we love AWS so one very good
candidate to host your code is on ec2
instances virtual machines
Linux windows but then you have to set
up this virtual machine you have to
patch a new Dispatch Windows install
runtime libraries and things like that
and you have to be sure that your code
is highly available because you might
have hundreds of customers using Alexa
and talking to your skill at the same
time so you must be available to ensure
that you will answer when whenever you
receive a request from Alexa and you
must be able to scale of maybe from one
or two instance to many instance so if
you want to be a bit more professional
in terms of deployment you
end up with something like that with
load balancer web tier internal load
balance or application tier you might
have database as well to keep track of
profile so that database needs to be
highly available and six like that and
of course you don't want to manage such
an infrastructure just to deploy your
skill for alexa we have some tools at
AWS that can help you to do that elastic
beanstalk is one of them it will
generate that but at the end is still
the same infrastructure it's load
balancer auto scaling to scale your
virtual machines internal load balancer
and other very low level infrastructure
that will not make your skill better
this is just plumping this is low level
infrastructure so we have another
service that can help you to deploy your
code it's called AWS lambda lambda it's
a container
it's a runtime environment for your code
as a developer as I will show you during
the demo you just upload your code to
lambda and we Amazon take care of all
the rest it means that you don't have to
install virtual machine to install an
operating system to install and to
maintain a runtime environment we do
that for you
whenever something invoke your code like
Alexa for example invoke your code your
code will be run inside lambda and you
don't have to care about scaling about
high availability because we will do
that for you another good news about
lambda is the price of lambda it's 20 US
dollars cents 20 cents per millions of
invocation and the first million of
invocation is free for you every month
so as a developer it's very unlikely
that you generate more than 1 million
invocation per month so as a developer
lambda is basically a free service for
you so developing your code is one side
of developing a skill and hosting that
code somewhere is the first part the
second part is to directly interact with
the alexis skilled kit and you have to
tell us what will be the voice model
what will be the intent what will be the
function that your skill will perform if
I'm creating a skill to order food and
we have probably an order intent get
status intense maybe a pay intent and
you have to tell us this intent an
intent roughly match to a function your
developer so you know what is fun
an intent it's a voice function so you
have to define these and we have a very
small JSON document that you need to
give us that defines the list of intent
that your skill will support an intent
might be backup by many different
utterances so an utterance is the phrase
that your customer might pronounce to
interact with your skill for hello world
intents my interest my sample utterance
would be hello say hi say hello to all
the food my sample it runs might be
orderly a pizza please order a pizza
order a pizza please so the more sample
u-turns you provide the better the
accuracy of the voice model of the voice
recognition will be so we encourage you
to generate a lot of sample returns
these are basically all the different
ways that your customer will be able to
interact with your skill and you need to
give that it's very common to have
several hundreds several thousand sample
returns in in that file once again the
more you provide us the better the
speech recognition will and then you
might have noticed that I have something
like first name in this hello world
first name it's what we call slots slot
it's a parameter to your function if I'm
writing skill to order pizza the the
type of the pizza marker attack or the
type of pizza that's the parameter so
it's the same intent but you have
multiple values that can be passed as a
parameter once again your developer you
know exactly what a parameter to a
function is a slot is exactly that so
you can create your own type of
parameter your list of values that you
support but we also provide you a couple
of predefined slots for dates phone
number for cities countries US states
and things like that and with dates for
example you can expect Alexa to do some
fancy things for you so as a debt
parameter the customer can say tomorrow
or in one week and we will transform
that into the cloud and give to your
code in the JSON document the actual
date and not what the customer
say to to an exam and the last part is
the code this is just a sample with
typescript in in JavaScript I have to
intent here help and hello world I
extend a bit a type script to add
annotation to correctly route intent to
a function but you can write code with
anything as long as you were able to
receive a JSON document and to send back
a JSON document it might be no des
Python Java of course anything we don't
have a specific preference so in the
session later this afternoon at 6:00 we
are going to create a skill together you
will see the detailed process of that I
will quickly browse into the console for
the time left one if your skill is ready
you can publish your skill we will
verify it to ensure that there is no
illegal stuff and it follow a couple of
policies that we have in terms of
publishing skills and the skills will be
available on the skill store where
customer can select for them can command
can provide feedback can enable them for
the specific account so let's see how it
works with machine learning on the right
side of the the architecture slide we
have the machine learning prediction
system that Danilo just built
remember that Danilo enable real-time
prediction so that we have an API that I
can call on his model to get a
prediction about the number of dice that
will be used and you saw maybe when we
feed the model with data that the number
of diox use depends on the weather of
course if it is good if we have a good
weather or bad weather the bike usage
will be quite different so we need oh
and myself create a skill that will be
invoked by Alexa and when I say get me
the prediction or how many bags will be
used for that specific time slot the
skill will call a weather API to get the
weather prediction for that moment in
time and once I have the the weather
data I will create a request and send
that to the machine running service to
the prediction cells then the prediction
service will use the model will do
whatever in the
wants to do but will retrieve me and the
value that actually I'm waiting for
which is the number of user the number
of bags that will be used and all that
happens in real time two hundred three
hundred milliseconds including the code
to the weather forecast API Alexa open
bike-sharing welcome to by sharing
prediction you can ask me how many bikes
will be used on a date how can I help
you how many bags will be used tomorrow
afternoon it will be a Friday and a
holiday it's Veterans Day the weather
will be clear so I expect that 242 bikes
will be used 22 from casual users and
220 from registered users so in theory
we can also do one shot invocation like
give the name of the skill and the
intent Alexa asked bike sharing what
will happen tonight at 8:00 p.m. it
would be a Thursday the weather will be
clear so I expect that 171 bikes will be
used 48 from casual users and 123 from
registered users so how does that work
behind the scene I have a lambda
function that we created this one is in
Python and okay there is a lot of
low-level things to manage the early
days and the season and things like that
but the interesting part is this one
zoom yes I first called the weather API
to get some weather forecast and then
I'll just prepare an object with the day
of the week the fact is it a holiday or
not holiday what day of the week is it
what will be the weather or the minimum
temperature the humidity the winds
these are all the parameter that the
machine learning system expects to make
the prediction and after that calling
the machine learning system is as easy
as this
I'm calling the machine learning system
first for registered user giving the
prediction params as input and I receive
the prediction which is just a number
for registered user and then I make a
student API call for the casual user and
I received a number all the magic all
the complex mathematical stuff happens
at the time where you build a model and
happens within the Amazon machine
learning system when we see this
parameter and we have to find the the
answer or the prediction for that
question and the rest is just assembling
this result creating the voice output
once again at 6:00 p.m. this afternoon
we are going to dive deeper into that
and show you end to end how to create a
skill when you create a lambda function
like that of course you don't want that
lambda function to be call 'evil by
anyone on this planet so you can have a
couple of three girl and the trigger
here is Alex's skill so only Alexa
service the Amazon Alexa cell is
configure that lambda function and us
basic monitoring information available
directly from the lambda counts also the
number of invocation the duration of
this invocation and you have direct
access to the cloud watch lock where you
can see all the data that were generated
by the logging of the lambda function at
runtime you can see that as plain text
I'm looking a lot because it's a demo
it's the debugging stuff so I'm logging
the requests that we receive that's an
example of a request sent by alex a
service to my skill and you see that the
date has been transformed as dates at
the time when I say tonight at 8:00 p.m.
and a bit later
we should find we should see the the
result
register rental casual rentals and the
answer that is generated by the skill
with the text-to-speech
so this is the text that my skill is
giving back to the Alex Avila service in
the
and based on that will generate the
text-to-speech and give the voice that
you heard from from Alexa
that's the lambda part and the voice
model part it's pretty easy as well I
need to switch console it's the Amazon
developer console no going to the Alexa
skill kit you see all my skills and this
one is for the Vox the first part first
screen you define the language today we
do support three different language we
do support traditional English UK
simplified English anyways and we also
we also support German and of course
supporting many different customer and
different language is very important for
us so I know that's one of the question
I will receive after this talk when are
you going to add French Spanish Italian
we are working on that we know it's
important we have full results working
on adding multiple language to to an
exam you need to build your interaction
model as well so the list of intents we
have a bag sharing prediction intent
that received two different slots a day
and a time we've helped intend to stop
intent help intent is actually mandatory
if you want to publish your skin on the
skill store you must provide help intent
so you must as a customer I should be
able to say ask by sharing for help and
to receive a meaningful answer from that
the sample utterance we don't have them
too much here we have just 52 once again
it's a demo in real life you will have
probably hundreds of different way to
invoke that bike sharing prediction
intent we have very small one so we can
say Alexa has by sharing prediction for
tomorrow afternoon and that would be
deaths and pollute arounds which map to
that intent it's the best practice to
also include things that are not
grammatically correct because you don't
know how your customer will actually
speak to to Alexa and the last part not
the to last part sorry the configuration
part in configuration you just give the
endpoint of your Amazon
the function so internally at Amazon we
have Amazon resource name is out this
fancy number here that's the reference
to my lambda function you can host that
in North America or in Europe you can
also use an HTTP endpoint so if you want
to host on the Google cloud on Microsoft
edge upload on promises inside your data
center we thought I find with that
lambda is just there as convenience a
very cost effective convenience and you
can test that directly from the Alexa
console so for example you can ask for
predictions tomorrow afternoon so you
simulate what the user would say this is
the the request that we are going to
send to your skill so it's very handy to
copy paste that and to do some some some
scripting if you want to automate your
example test case of testing exactly and
this is the service response so this is
what we receive sorry this is what we
receive back from from your skill so you
can see if the Alex opposed this did
receive something from it most of the
time the first time you receive or you
are testing that you will see an error
message they are saying hey I cannot
talk to your skill all the response is
invalid then you have to go back to the
lambda function and to understand why
the response was invalid and you can
play that as well okay anyway so you can
hear the response that actually Excel
will send to the device if you don't
ever really device because this one are
currently available only on UK US and
Germany there is also a tool called echo
sim that IO that you can connect to you
need to authenticate with your Amazon
account and then you can speak to your
browser so the speaker and the
microphone depends on the quality you
have on your laptop it's not the same as
on the real device but if you don't have
a device it's quite handy tool to test
your your you
interaction only available on the US
language for now will add support for UK
and remand very soon probably first
quarter next year that's pretty much it
I talk about watch about about the
skills and so we can go back to this one
and that's the demo we wanted to show
you using a machine learning prediction
some super advanced stuff that I don't
understand
I read stochastic somewhere on one of
the needle slide I still have gray hair
from my time at university because of
stochastic I don't want to understand
all that I'm just a skin developer and I
integrate with that with just a couple
of lines of code to create a voice
experience on top of Amazon machine
learning yeah so I hope it was
interesting if you want to dive deeper
into the components that we used to
build this demo we will have the session
of five fifty and the only suggest is
suggestion is if you want to build an
application focus on the unique idea
that you have so use the providers of
services don't think of servers and just
focus on what you want to build thank
you if you want sorry I forgot if you
want to start these are the two links
that we use so developers dot amazon.com
is the Amazon portal for creating the
skill and AWS amazon.com slash free is
the free tier as we said for lambda you
can start using everything for free
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>