<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices Evolution: How to break your monolithic database by Edson Yanaga | Coder Coacher - Coaching Coders</title><meta content="Microservices Evolution: How to break your monolithic database by Edson Yanaga - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices Evolution: How to break your monolithic database by Edson Yanaga</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6dfBd-2Oq1M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody good morning my name is
set at Sanya Naga I'm a director of
developer experience at Red Hat my
twitter handle rates at yo Naga and it's
a very great pleasure to be here at
devoxx for me for the first time has
been a great week so far great sessions
great conference great food great drinks
and the most important thing great
people has been awesome to be here this
week with you and today I'm going to
talk about micro surfs evolution
breaking your monolithic data base
because you know everybody's talking
about micro services these days but most
of us we're dealing with enterprise
software and we have a relational
database to deal with and the number one
question that I usually have when I'm
talking about micro services is I have a
legacy monolithic database I want to
split my application my system into
micro services how do I deal with that
because each micro service is supposed
to have a separate database how do I
split the strings so I've been studying
this subject quite a lot in the past few
months or years and I've collected a few
practices that I hope will be able to
help you with breaking your monolithic
database I'm also a Java champion and a
Microsoft MVP which is a kind of weird
to have both these both titles these
days but just to say just to prove my
point that the world is changing
sometimes for the better sometimes for
the worse we had some strange news this
week and but I always like to start my
sessions with this quote from which is
not from me but from Forbes now every
company is a software company because we
know software is changing the world and
that you believe the software can change
people's lives for the better and for
the worse and some of the proof some of
some of very good examples of companies
that are changing our words change our
lives and some of the day exams so that
are collected is that the largest car
transportation company in the world
owns no cars which is uber the largest
lodging company in the world owns no
real stage which is Airbnb the largest
online retailer in the world has no
stock which is Alibaba and the largest
content
network in the world produces no content
which is Facebook all of these companies
have something in common is that they're
all enabled and they're made possible
through software so clearly software is
changing the world and we know that
software can change people's lives for a
better and some years ago I made a
choice and that's why I also like to
introduce myself as a software craftsman
and one of the best definitions that I
have about the software craftsman is
someone that cares about what he does
and so I truly believe that software can
change the world so I can I'm constantly
trying to improve the software that I
deliver and I decide that I would be
would be able to change the world for
the better to try to help developers
worldwide trying to achieve better
software so I hope that you will all be
able to produce better software with
some of the things that we're learning
with this this week in this amazing
conference and of course when we're
talking about DevOps and micro services
we have many DevOps micro service we
have cultural change we have process
change we have different tools to be
achieving that but for me the most
important concept about DevOps and micro
services certainly is the feedback loop
because we as humans we need to iterate
constantly to know if you're doing the
right thing right and the best way to do
that is to be fast in what we do so if
we can quickly access and evaluate our
hypothesis and validate them into
production the better the software that
we deliver so we know the better the
best in the fast faster feedback loop
that we have if we're able to just type
the things and quickly validate what
we're doing just we've stacked static
typing or with automated unit tests or
integration tests continuous
integrations if you think about it most
of the software process evolutions that
we had in the past 20 years they are all
trying to improve the feedback loop that
we have when trying to deliver software
into production so I think that main
figure that we need to improve when we
believe in software through DevOps and
micro services is the thing that usually
we're doing many different things at the
same moments and when we try to deploy
software into production we're deploying
just a lot of things at the same time we
would deploy like hundreds or maybe
thousands of lines
we have changed in the past week or even
months into production and when
something goes wrong it's very rare for
us to know very quickly what was the
thing that we did in the past weeks or
months that's breaking things into
production so thinking about that maybe
if you could try to reduce the amount of
things that we're trying to deliver into
production at the beginning oment and
even if you could trying to an ideal
moment to try to commit or deliver just
one finger just a smoke a small code
change into production at a given moment
then if it's something if something
breaks we would be able to quickly
access if we're what we did wrong with
the code that we deploying to production
and imagine also about business cases
sometimes business people asked for us
to develop some new feature and we take
like six months to deliver that
everything that they thought about when
they asked about the feature is gone
they don't remember why they asked that
so when we deliver that into production
we don't know why we're doing that
things so I imagine it for business
people if we're quick we if we're able
to just quickly oh my have added this
weird idea maybe change the color of our
buttons to orange we're going to sell
more and if you could test that
hypothesis quickly just like in a couple
of minutes or maybe a couple of hours we
could be able to measure and monitor
these things so we could improve our
software we couldn't prefer company and
improve event and revenue and most
likely improve people's lives too so
what we could this concept that we have
in DevOps and micro services called a
bad-size so the most important thing for
me when we're talking about DevOps and
micro services is try to reduce the back
size or the amount of things that you
delivery into production into any given
moment and it doesn't matter if you took
your bed size to today is like six
months or it's free months or two months
I think the most important thing for us
is to constantly try to improve our bad
sides so we always try to reduce until
we have an ideal moment if we will be
able to just take one commit and deliver
that in straight into production we've
continuous deployment
but if you think about that not
everybody needs to be Amazon or Netflix
or Google to be able to be doing
continuous deployments for many
different enterprise companies like
delivering once a week will be more than
enough for us to validate because we
won't be able to validate some business
things within within them paired lesser
than a week so we should try to find our
perfect bed size but for most of us we
should always try to improve try to
reduce the our bed size every week but
even if we try to reduce the bed size
every week we are software developers we
have to deal with the problem that we
have when deploying to production which
I call the maintenance window
traditionally we we need some time of
downtime in our systems to be able to
update our things and that's especially
true when we have like relational
database we're trying to deploy things
into production we have a schema for one
version and we have another database
schema for the next version and since
the both of them are incompatible we
need some downtime to apply the new
release into production and probably to
run some Seco scripts into production to
modify our database schema so we need a
clear maintenance we demand depend on
the Ops guys on your company it can be
like once a month
once a week once a day or twice a day
depends many other use case that your
company has because you're not allowed
to just interrupt the things that users
are doing daily you have to specify oh
we're going to have a software update
this this this night at 2:00 a.m. 3 p.m.
of course nobody of none of us like to
be like in spending our nights at the
point in releasing software just to
figure out that we broke something else
so if you want to reduce the maintenance
window we continue to to improve our bed
size so we can deliver a lesser things
at producing web production at a given
moment we need some strategy to be able
to do some zero downtime deployments and
the single most simple deployment
strategy to be able to achieve zero down
times what we call this days Bluegreen
deployment so quickly pass-through
blooming deployments but traditionally
this Bluegreen deployment architecture
we have some clients issuing some
requests you traditionally only have one
deployment then we have to change your
architecture we have to add some one
components in architecture which we can
call a proxy a load balancer or anything
else we just add another component in
our in architecture sir can just we can
pass requests for this kind of proxy and
then for Bluegreen deployment instead of
just having one single deployments at a
given moment in production we're going
to have at least two that's why we call
bloom in deployment and we have
Bluegreen deployments we have the
ability to we can scale because now we
have two production environments to
equal production environments and then
we can we have the opportunity to be
able to like take green out of
production update the green release now
we have a new version running to green
the green deployments then when the
green comes up again when it's warm it
up again and it's ready to receive new
requests then we can do the opposite
thing we can take blue out of production
we can update it and when blue comes up
again we can put it back into our
cluster so everything is updated right
now so we still have to environmental
production so we can do that for high
availability too but for now we're doing
that for to be able to achieve zero
downtime deployments but when we see
zero down plant with zero dollar green
deployment architectures in DevOps it's
kind of easy because we're always
thinking about code oh we have two
different versions or applications
running to production maybe it's not
that hard
but I always like to say that and when
we're talking about software
developments code is always easy State
is always very hard and we're talking
about States like that the persistent
state that our application have because
even when we're talking about session
stated which leaves aside our
application service session states
usually some something like that it is
ephemeral if we use this ephemeral State
the worst the worst case may be the user
has to log in again and just redo the
last step that he was doing but and when
we're talking about persistent States
the state is very hard because this
state usually is usually storing in a
relation
database and we need some way to make
like I have a different schema more
version or maybe different data
different data formats in one version
and this data is not are compatible with
the forwards version that we were blind
so how can we achieve some kind of which
techniques can we apply to be able to do
zero downtime deployments regarding to
persistent state which is usually stored
in relational database which is the the
scope of visits talk so number of
question is what about my relation to
their basis and the easy question is
that when you also need zero downtime
migrations and I don't know if any of
you is not a familiar with the concept
of migrations but migrations is the
technical term for the things that you
change in your schema from one version
to the other so over change when you
using like some alter table or some
update statement that's changing the
format of your data in your database
from one version to the order we call
that database migration and we have some
pretty popular database migrations tools
in the Java world which is not Java
specific but it's very common in the
Java world the two most popular tools
are Flyway DB and liquid base I think
both of them are like feature complete
by by this moment because they're I
consider them very mature but my
particular preference is Flyway but it's
just a matter of choice I know that many
of you use leaky base and it's just fine
for that and I used to have some
considerations because use usually when
we were using these tools like fly or
liquid base we try to tie this the
migration step of these tools into the
into the startup phase of our
applications and maybe when we're
talking about Bluegreen deployments and
not only Bluegreen deployments because
now you can weave higher veil ability
with a container clustering now in wipin
we might have like three four ten
instances of applications running or
starting up in a given moment so maybe
it's a better way to do that instead of
relying on on the startup phase of your
deployment process to run your
migrations maybe it's better to get your
Jenkins at an additional step run your
migrations on that step sequence to do
to make sure that your migrations are
only applied once instead of having try
of trying to apply multiple concurrent
migrations that are given moments
because you know you might have multiple
containers might have multiple instance
of our application being started up at a
given moment but at least I think in fly
way they solve this problem of
concurrent migrations being applied to
the database
I don't know if we'll requires a
specific database a functionalist for
that to work properly so I still
recommend people you should be applying
or your migrations on a separate step on
your continuous delivery pipeline and
the easy answer to how do I create zero
downtime migration is that migrations
must be back and forward compatible and
why we're talking about zero dontoh
migrations maintenance windows we used
to have just a monolith into production
so it's easy to just warn our users oh
we're going to have downtime this week
of this year or even if it wasn't easy
it was like come on to the debt but I
thought we were having micro services
and we have many different moving parts
it's very important for us to minimize
even more the maintenance window and the
downtime because you know if I have like
50 artifacts or 10 artifacts into
production the downtime that you have
into production you have to multiply the
downtime by by the number of artifacts
that you have so it's very important for
even more important now for us to have
zero time deployments into each one of
our micro services and with databases of
course we need into code they must be
back and forward compatible and our
schema migrations they also must be back
and forward compatible and to achieve
that of course again we need to reduce
our bat size and for database migrations
it means that we need again to do babe
steps which means that we need a
smallest possible bat size and so our
migrations so if you use it to create in
a sequel script we like 10 lines of C
code to do your database migration then
you need to split probably your database
migration script into ten different
scripts one each one of these five is
going to have a single statement and
maybe you need to get to just one single
sequel statement and break into much
more because we're going
I'm going to show you some techniques to
apply these your own time aggressions
also we have an issue when you try to to
update your data to a new you have a
column you want to check do to change
the format of the data that you're
storing maybe you're changing the format
of your time zone or you're decided to
change the value that you're storing
your enemies so sometimes you're you
have too many rows in your table and if
you try to add two to execute an update
statement of the whole table is going to
take a long time and it implies also in
locking locking this in this case a
particular case implies in downtime
we're not allowed to the debt so the
obvious reasons for avoiding too many
rows and long locks in our update
statements eetu shard or updates and
shard is a very interesting word we're
just trying to split our data like we
were not updating the world table at a
given moment in just a single statement
we're going to shard like I want to
update just the first thousand rows then
we're going to update the next thousand
rows and keep going until we have like
the whole table being updated so we need
to measure it depends on your specific
table your specific data how many time
your your update statements are going to
run but maybe hundreds or thousands is a
good number you have to test and of
course I can't emphasize enough where
we're doing migrations the best thing to
assure that things are going to aren't
going to break into production is to
rehearse a lot your migrations and for
that you should have a continuous
delivery testing environment you should
be able to be testing your migration
multiple times on your notebook and your
testing environment and your
pre-production environment and so on so
when you apply our migration to
production you have rehearse that that
migration multiple times to to make sure
that's good you're not going to break or
you know you're not going to have long
locks into production so how can we do
that maybe I'm trying to modify my
schema I want to issue a single alter
tape customers I want to rename a column
into production to another to another
name because I just think that this name
is better right now but now instead of
just issuing one single alter table
statement I'm going to have
to have many so this single one just for
you to take a to to have some notion or
how many statements are required now to
apply a zero don't immigration that
single statement or outer table renamed
column it should be method now for at
least like four or five steps into
production to allow your releases to be
zeroed on time so instead of just
renaming a column I'm going to add a
column I'm going to shard my updates to
avoid long locks I'm going to do some
code updates to be able to do that in a
forward and backward compatible way and
later add into any given moment I'm
going to delete that column later so
this one this is one of the reflectors
that we were able to apply with zero
downtime aggressions so there's a basic
concept here is we're never gonna do
destructive statements into our database
it must always be possible to recover
the previous state from our database in
any given release that we deliver into
production so I've collected some common
scenarios from many different users and
companies of how to apply zero data
migrations and I know that at the first
moment in Maxine seems like a lot of
work while all of the companies that
I've visited they can testify that after
they learned how to create and apply
zero data migrations their deployment
process just improve it a lot because
now they're are not worried anymore oh
my god do I need to worry about how many
time does it take to recover the backup
engine production no because you're
never destroying not an information into
production you always have multiple
compatible versions of your code and
your schema into production so you can
move forward and backward easily without
losing anything so the most common
scenarios that ever collected is to add
a column rename a column change the type
or format of a column and to delete a
column and if you think about a table as
a collection of columns then you can
easily think and apply the scenarios to
tables to to relationships and
everything else so the first scenario is
at a column this is a pretty easiest one
you just first step you're going to
alter table at a column
and depending on of data that you won in
the column you have to apply a per day
to know values to to assign a default
value which can be issued in the author
term they had a column or not or maybe
this new column you want to compute the
value based on existing values from
other columns on nor the tabular systems
then you have to issue your update
statement of course using sharding into
a second statements and then in the
third release so each one of these steps
one two and three is going to be a
different release into production
different versions version one two and
three of your of your deployment
artifacts into production and later and
step three when you have the column and
you have the value then you apply the
release into production where you use
the column okay this is the simplest one
next one which is kind of the most
complicated one is rename column rename
a column we have to first add a column
copied data using small shards multiple
update statements uses more charts you
have to trick tricky or worse statement
for doing that first then you're going
to create a release that reads from the
old column and writes to both column the
next version you're going to create
another release which reads from the new
column and writes to both again so you
always have multiple compatible versions
back and forward then version release
five your code is going to read and
write from the new column then step six
you're going to delete the old column
you don't have to do that because delete
is a destructive statement
most of the companies they just take the
do clean up like once a month or once
two months they usually they mark the
columns that they want to delete later
so after one month of your release you
did like hundreds of release into
production already so one month later
maybe that column really is not needed
anymore so you always delegated the
delete statements to a maintenance thing
which are going to do little lightly
later like one month later two weeks
later or two months later and you see
that's the pattern that you use when you
want to rename a column when you want to
the next scenario is change the type and
format of a column and you see if you
get to see both lines there's some kind
of similarity okay
so it's always the same pattern when you
can want to change the type or format of
a column it's always the same steps so
it's not that hard to know how to apply
zero downtime migration to production so
if you were able to split your bed size
so that each one of these statements
it's easy into a different separate
release into production you're going to
be able to be able to to move forward
and backward into your single releases
without losing data and without downtime
because all of these versions are
compatible with the next one and from
the previous one okay and the last one
is the leader column simply don't do
that
that's a destructive statement if that
column is not know maybe you want to
keep your application writing some value
to that column until you do you're sure
that you won't be needing any more so in
your application platform delete a
column your application should stop
using the read value from that column
but should keep writing to the column or
else your you won't have that data when
you roll back your version of your
application to the previous version and
later again in your sanity phase of your
database cleaning which can be again a
month or week later you can delete the
column ok so delete the column is just
destructive thing you don't want to do
that next question is okay I know how to
do 0 Delta migrations with columns and
tables maybe but what about my
referential integrity constraints
because we lack foreign keys it's et
cetera
if you think about it foreign keys are
just a safety net so we won't be
inserting wrong data into your columns
but it's strictly thinking about the the
data that we're adding referential
integrity constraints it doesn't have
any kind of business value to your
application it's just a safety net so
you can see safely remove them so the
quick answer is just drop your
constraints and recreate them your all
of our migrations are done so be able to
to check if everything is working and of
course you shouldn't again it's just a
safe if net just because you have a
safety net you should be relying on that
for your production code you know you
should be assuring that you're writing
the right things on the right columns so
constraints are just the safety nets so
now that we're talking about how to do 0
no time migrations in
multiple different databases into
multiple different micro-service to
minimize the doctor the overall downtime
that we have in our system I get to talk
to you about how can I extract my
micro-service database from my
monolithic legacy database I have this
big relational database into production
it consists of hundreds or maybe
thousands I've seen some cases like that
of tables and columns and relationships
between them first there's a question
that I'm not be able to answer today
which is how do I choose which tables to
extract usually the answer is you have
to apply some domain-driven design and
bounded context but the most simple
answer is experience you know which
which data on your systems changing to
get changed which which which amount of
data in your system to change changes
together which data changes apart so you
need some domain experts to be able to
tell which data will you're going to
split so it's not in the scope of this
talk but since you have chosen which
tables are going to migrate you are
microservice then enough we need to
create one database by macro service
that should be each microservice needs
its own data store it doesn't have to be
a relational database but since we're
extracting some data from a legacy a
relational database maybe the best and
simplest possible option for me to
distract the database is create another
database into my current database into
another schema or you can even create
tables in the same a schema that you
have just to be able to move them later
so we're just trying to strike the
information two separate tables it
doesn't matter where the stables are
going to be stored it's just important
for being to be in a relational database
again so be able to to to to craft small
baby steps into the distraction so I
have a monolithic database I want to
stretch this Microsoft database but and
before we are able to do that I need to
explain some concepts about architecture
because many people are usually confused
about crud and secure s and I have to be
honest
I've read a lot of different definitions
on the Internet
there are grades but it's very hard for
us to figure out what does scrud and
secure s means into codes into
production into databases so I try to
give you a very simplistic explanation
or what is screwed in what is secure is
regarding to database tables and code so
if you have creds there you have the
four operations create retrieve update
and delete if you're thinking about data
starts when you have a cloud
architecture which is great for many
different use case for many different
use case credits the best approach that
you that you can have to solve your
problem you're going to issue the right
operations of your application and the
read operations of your application into
the same model which can mean you're
going to use the same model classes in
your code and you're going to store all
of these operations into the same tables
into the same database so if you're
thinking about data stores crud means
we're storing everything in the same
tables in the same database and if we're
thinking about security architecture
which is a fancy name for command key
responsibilities in segregation
basically when we are talking about code
and data stores means that we're
reaching the right operations we want
one model which usually translates to a
set of tables in the database and we're
using the read operations into a
different modern possibly in your code
and into different tables in our
database so when we try to show the
difference between crud and secure is
regarding to data stores crud
you're always reading and write from the
same thing secure RS you're writing and
reading from different tables on
different schemas possibly or different
database okay so that's the basic
difference very simplistic definition of
crud and secure RS recording two data
stores so if you're thinking now I have
a monolithic database I want to stretch
this deformation to another to a
Microsoft database I need I don't want
to do that in a Big Bang step you know
when you do many things in to to just a
single release many things can break so
we want to reduce the bed size so it's
reasonable to believe that for for a
certain amount
of time both micro-services are going to
read from the same data store or maybe
they're not going to read from the
exactly same data store but the data
data is going to be generated by the
single same source so how can we achieve
this migration steps using the
techniques that we have today I've
collected a few strategies that I want
to share with you right now
these scenarios are view materialized
view mirror table using trigger mirror
table using transactional code the
mirror table using ETL to stand even
sourcing that's the scenarios that were
collected for you to be able to stretch
your data from your monolithic
application monolith database into a
Microsoft database so you'll be able to
do that using baby steps so first
scenario you have a you have your
melodic database you can create your
Microsoft data source using a view which
has the benefits of being the easiest
one you have the largest support between
database vendors you have some possible
performance issues depending on how you
create your your view you have a strong
consistency another thing that I have to
explain to you difference between strong
consistency and eventual consistency are
tracks is to explain again in a very
simplistic way a strong consistency
means that into any given moment of time
if I issue a query on my system all of
my clients at a given moment tea are
going to receive the same exactly value
so I have got strong consistency where
everybody receives the same read value
and eventual consistency means that at
into any given moment tea if I have
different clients issuing Kyrie's to the
same value to my datastore they might
have different values which is not
inconsistent values but maybe one of
these values is a like outdated value
but eventually all of the clients into
egg into into the future will be able to
read the same consistent value okay so
that's the difference between and
virtual consistency and strong
consistency and many people believe that
in your systems we must design for
strong consistency but it's not the case
in for banks because most of our
applications the
I can leave and survive without strong
consistency and we can have some kind of
eventual consistency in your systems so
even a strong consistency depending on
the database how to save visibility
configurations that you have you might
even have a some kind of a measure
consistency even if you're using
transactions so you have to think very
well if you you won't really need the
strong consistency or you can leave
without Inventure consistency and if
you're leaving with we the natural
consistencies you have to think about
how much delay is acceptable when you're
dealing with your eventual consistency
is my delay of updating all clients like
one second five seconds half an hour one
hour people that use like BI tools left
for a weave ETL some people like the
they can survive we can generate if they
receive a report of the data from
yesterday so in this case the eventual
consistency can be 24 hours so it really
depends on the use case that you're
applying your your your code in your
migrations so for now views have a
strong consistency
well the downside is one database must
be reachable by order depending on the
database that you use you can use a DB
link and views depending on the database
and how you create your your view you
can also be updatable so you're not just
restricted to creating a read-only micro
service in a separate database depending
on how you create your view you will be
able to create read and write micro
service using this this view you can
also use a materialized view
materialized views usually they are
implemented into the database as real
tables so it usually also has better
performance materialized views can have
strong or eventual consistency again
depends on how you configure your your
your materialized view again one
database must be reachable by the other
use some database you need you can use a
DB link and it also can be obtainable
the plenum depending on the database
that you use you can also use mirror
table using trigger triggers depends on
database support you
since the trigger is going to is going
to run this
transaction as your as your update
statement you have a strong consistency
again one database must be reachable by
the other and the pending or the
database you will be able to create a DB
link next scenario mirror table using
transactional code when I say
transactional code I mean any codes it
can be a store procedure code it can be
a distributed transaction code that
really depends on the technology that
you using the important thing is we're
going to have a transaction it doesn't
matter if it's a single transaction in a
single database application or we're
going to have a distributed transaction
using two-phase commit and XA and
everything else so we can have store
procedure to stream a transactions we
might have some cohesion and coupling
issues because now we're talking about
specifically about distributed
transactions we might have different
models and everything else so it really
depends on code right now and depending
on how you craft your your two different
applications you might have updatable
your tables or not you see in this case
you can do anything your code can be
updated or not next scenario is you can
create a mirror table using ETL codes
you can use bi codes many use some
people can use some kind of dashboard
tools we can use click view we can use
I forgot open source one talent okay not
that one Penta Hall okay you can use ATL
toast but you have a lot of different
tools that can use the structures from a
load into a separate database to create
a mirror table so have a lot of
different tools available usually these
tools requires an external trigger so
usually this triggers crumb basis though
somebody has to go into the interface
and trigger the update this update
usually can take a long time that's
where eventual consistency triggers can
aggregate from multiple data sources you
are not relegated to just queuing just a
single data source because that's an ETL
- and since its using ETL it's really
only you cannot update the mirror table
that you're using when using ETL expect
your original day
to be updated to so that's one of the
downside but it's also feasible
depending on your use case and lastly
maybe that's the kind of strategy that
we want implemented most of the use case
we're talking about distributed systems
is also the most complicated one
it's event sourcing because we're
talking about in many of the talks and
books when we're talking about micro
serves they're going to say oh now we
have distributed database you must do
even sourcing event source is not that
easy to implement when you have a legacy
application it's okay for greenfield
projects when we have a legacy events or
Selene usually implies that you have to
change the model that you're storing
your database and it certainly is not an
easy question thing because in a legacy
the application you have hundreds of
lines or thousand lines of codes issuing
Curie's to that columns and tables so
you won't be able to easily change the
your data model but we can say that this
is the ideal scenario not everybody will
be able to use event sourcing the good
thing about the event sourcing is that
the state of the data is a stream is a
stream of events instead of just a
single column or a single table in your
database it eases auditing when you're
talking about the event or consistency
event sourcing usually you mean also
eventual consistency that's a
requirement for event sourcing you
usually also need a distributed stream
through a message bus talking about
codes
one good example event sourcing I always
give the bank account example because we
don't have a bank account we just set
the current amount using set current
amount this or set current amount that
most systems the model bank accounts
using using secure areas and event
sourcing because you have the bank
account but the state or the data that
you have in your amount because assuming
that each one of the your bank accounts
start with a zero amount you just add
transactions and I say I create a
transaction class and this transaction
it has a number and add or subtract
operation so each one of the transaction
that I'm applying to made to my bank
account I just adding or subtracting
some value so if I get if I get to think
about that I have
I had a state machine it starts with
zero and if I successfully apply all of
the transactions in the same sequence at
a given moment of time I'm always going
to have the same result that's what we
call even sourcing so we recall about
even sourcing the state of the amount of
that you have in your bank account is
represented by the transactions and not
by a single column in your bank account
but you can have an amount column into
the bank account table just to cache
your information because you're not
you're not going to be able to always
when you want to know the amount of the
money that you have in your account to
apply all of the transactions from ten
years ago from now to compute the
current value so you must have some cash
advance reasons but basically the true
data is on the stream of events the
students transactions and the amount
that you store in the column is just for
performance issues you can also apply it
snapshotting or other kind of techniques
to ease your auditing but event sourcing
means that secure is an event sourcing
you you splitted your data if you're
reading from here and writing from here
and your true data the true information
source information if the string is the
stream of events that you're creating
weave it in sourcing so but for that's
think if we did a model application
using bank account and the stream of
transactions it will be very hard later
like ten years later five years later or
even six months later to change your
application code in our tables to create
this transaction abstraction so how can
we do that using legacy applications so
one of the cool things that I want to
show you today is a very nice open
source project which is kind of recent a
DB zoom the Biggio it's the origin of
the name is like they want a DB and
anything that you put in a periodic
table chemical periodical table you have
you had this young sound so they tried
DB zoom to sound like a chemical
elements so that's why they're called
that division so division it's a very
cool project as that this was created
for this purpose I have a legacy
monolithic application
I want to apply event sourcing on that
legacy database without changing my
application code and model my legacy
application code and model so be able to
extract these events from my database
and apply event sourcing into other
services which can be micro-services
right so you can think about different
kind of applications I can plug this
division into a current database it's
going to generate a stream of events
from the changes that are issued to this
database I can read these events and I
can do that well with that whatever I
want to populate a seperate microservice
database but instead of just talking I
think this is much more more cool for me
to show and then to explain further
concepts so right now I want to show you
here I have it running already my DBZ on
demo okay I think that I'm going to
crease the font again
I'm going to increase the font so that
you'll be able to see what's happening
but it's just infrastructure okay this
is busy running and here's my watcher
okay so I have some things running some
requirements for division division users
uses you zookeeper to share global
coordinated data he also uses Kafka for
the streaming so cut the interesting
thing about Kafka is that it's very it's
like a real-time stream its persistence
it's very light wave and each one of the
clients can have its own pace for
consuming the messages and each one of
the clients they also keep track of
which was the last message that they
receive it from the message bus so Kafka
doesn't have to keep track of that each
one of the clients can can handle it so
that's why it's very lightweight and
it's very important because when the
vision is reading the event long from
the database if somebody might like
overwhelm that the event logs issuing a
lot of different different
transformation multiple update
statements insert the leads and alter
tables and everything else so it might
be hard for the clients to keep the pace
or this clients might even go offline so
I can't assume that all of my micro
service will be online at the given
moment when I distributing the events
with the message bus so later when a
Microsoft comebacks online it needs to
be able to recover from the exactly same
point that it's soft but when it got
down so division takes care of that
thanks to Kafka
so I have here I have a my sequel
database running I also have a my sequel
client so I can do the update statements
and see what's happened happening I have
the B's iam running on this terminal and
here I have a watch just to show you
what's happening underneath which are
the messages in the events event stream
that the business generates for you the
cool thing that the bezel has support
for MongoDB my Seco post greed sequel
and Oracle so you'll be able to plug the
vision in all of these databases Gus
just going to connect you tell the
vision which are the tables that you
want to monitor and from that moment on
the bezel is gonna generate a stream of
events from each one of the DDL DML
statements that you issued to your
database so if you alter a table if you
update something if you insert something
or if you delete something the besom
will generate one event for each one of
these cases okay it uses the binary log
of each one of this database just you
for you to know how it works in the
background but the cool thing is I have
this database oops
Tory I have some tables the bean zone is
monitoring each one of these its tables
but I'm gonna the customers so here I
have my my table customer it has an ID
first and last name an email if I select
the information from customers you see I
have some information here if I am now
going to use select statement they don't
on the dough modify the state of the
database so now I'm going to get some
I'm going to update the first name of my
one of the customers and tell that now
the first name is Ann Marie instead of
just end and what what has happened in
the
background division generated and events
which is a JSON format of the event
which since its compacted here it's very
hard to see what happens in the
background but hopefully I'll be able to
show that event to you so if I get the
formatted event here now let's see what
our information that the business
generated for me so if I get to the top
of the events yeah see if the business
generates also tells me the source of
the event tells me the type of the event
the payload from ID a thousand four he
also gives me this trucks which is the
scheme at the current schema of the
table when the event was generated so I
can check the structure of the of the
table so it generates per meter
structure it also has the type
information here the payload the
important thing for an update statements
the payload here it's a before the
record was n crash man I don't know how
to run of that sorry and after the first
name is anne-marie so the bezel tell me
the this row used to be this and now is
that so you can consume this J's
information to your application and you
can like now I'm going I'm going to have
a cash into my separate macro service
and I'm going to use this information to
update my current cash so one of the
things that I've did in the past one of
the first micro services the the macro
service fing they didn't even happen the
best but we're trying to create a
separate system that was generating
reports of the transactions the sales
transactions of the users but the users
database used to be a single into a
different macro service when we're
talking about micro services integration
the first thing that comes to our minds
oh I need this application service to
consume the customer information here so
the easy thing to do is I'm going to
create a rest endpoint here to expose
the customers then I'm going to the
restaurants their ID here of the
customer then if I need the customer
information here to generate the report
I just gonna issue an HTTP rest request
to this service and it's going to return
the results
and I'll generate the report the bad
Fingaz works for a single kiri but when
you have multiple different users
generating reports for multiple
customers at the same time what happens
HTTP and the rest is incredibly slow you
won't handle the amount of requests so I
had users like waiting for half an hour
to get the report generated so it's not
a feasible amount of time so you have to
change that so the first finger I I
can't just rely on HTTP and remote
services to get information that I want
to execute my things on a micro service
I must have a must have this data in
stored in my micro service and I have
some different strategies to achieving
that I can catch the information that I
receive rest that's one of the things or
I can update the data that I have in my
micro service using event sourcing and
secure s so in this particular case if I
had the be zoom at this project I would
be able to plug the B zoom into my
customer database and create this
separate micro service so it's going oh
I need a folder for the customer I just
did the name and they may of the
customer I don't need the address the
phone number and everything else I just
needs informations so when I receive
Monday when I this application connects
to the besom and receives the change
event I'm going to take this JSON and Co
what is the new first name last name
email of the customer and I'm going to
update my local database so when I
generate my report I have everything
that I need local instead I have to
carry a remote service for that that's
the true beautiful true beauty of events
RC and that's the true beauty of the
bees iam we can do all of that using a
separate streams information okay so I
did that with updates I can also
generate other kinds of events I can get
here I can delete also from from my
database I deleted a customer ID 1004 so
let's see which kind of event DBS and
generated for me now I have this
generated two events for me so I'm again
going to get one
equal so I have this one basically the
pelo changed before the business is same
from it before before there the this row
used to be this and now it's no so you
don't have further information the
record was deleted maybe it's not a good
thing for you to delete your records
into your application because that's
that's a destructive thing but since
it's a legacy application you don't get
one you don't get to control that and
the division is very flexible for you to
receive these events and also when we
issued a delete statement the vision
generated two different kinds of events
to different events for just a single
delete statement so if I get this one in
this particular case the business
generates an event with schema and
payload no and why is that in this case
the business just is just using the the
compacting thing of Kafka because maybe
if you're able to read the all of the
events of the stream in a batch in
advance if you can read oh I'm inserting
updating updating updating a bidet 2mp
dating there the idea even for and then
I'm are going to do a delete if I can
batch all of these streams and I know
that the last one is the delete maybe I
don't need to insert update and update
update ability later delete so a
division sends like both events so to be
able to if you were able to batch your
stream of events you get to see all the
last one is a is a is that the lead so I
that I have to to applied in certain
update statement that comes before
because I'm not going to use the
information later anyway so that's one
of the things for the two division does
for optimization so I can also do one of
the things here division also it
wouldn't be valuable if just the
business not books
the division micro-service stopped so
nobody's receiving the changes that's
it's happening on the database right now
you wanna be user for the vision just
didn't monitor the changes so I'm just
going to stop the division connector so
now I can insert some values here or my
database and insert Sara
and then insert Kenneth so and if I get
to check here The Watcher I didn't have
any new events on the stream so nobody
is receiving the updates but when I turn
the division connect again I'm going to
run the BZ on connect again on this
terminal
so it's running again and Divisional odd
it looks at the bandar-log which is the
last event that I have in which are the
next one so it's going to generate the
events for me so if I get back to the
events here division just generate the
inserts in events that that happened
when it was offline so I forget there
get just one of these events I'm going
to show to you here it here's the can of
events so before I didn't have anything
so after that I have Kenneth Anderson if
you want to see the operation that
happened when you have an insert
statement you have the opie here is see
which is to create you have an update
statement they hope the operation here
is you so and if I have a delete there's
no data so there is no operation so
that's the things that you that you can
do with the B's in connect and if you
consider user case that I gave to you
separate micro service updating my cache
here my local data using event sourcing
Division is a very useful open source
project that you can use in your legacy
application without having to change
your tables and your model that you have
you just create new things connecting to
your legacy database I think this I know
about you but I think that division is
an absolutely amazing project that you
can use in your legacy monolithic
database so that's some of the things
that I wanted to show you today I want
to all of you to to join developers
redhead calm because all the information
they're presented here and much more
about micro services and breaking
monolithic databases are going to be
available I'm also writing a book by
O'Reilly about these subjects going to
be published I believe in the beginning
of next year and it's going to be
available at least for the first four
days for free at the developers Red Hat
COMSAT website so I strongly encourage
you to register and I also am collecting
the strategies for like more than a year
for from different companies different
users how can they split their
monolithic database and how can they
apply this zero downtime migrations so
if you're having any feedback any
downside any new scenarios that you want
me to cover I would be more than
happy to to address this the scenarios
because I really want your feedback you
can you can contact me at Twitter at
Gonzaga at my email
Anagha at redhead calm and thank you
very much I think we might have some
time for questions so if any of you have
any questions I'll be more than glad to
answer but I also be available at the Oh
at the hall or something how can I do we
have a microphone no no okay oh you have
to go there too okay
oh you can come here different tables
with different topics yeah the question
is is it possible to configure configure
division to listen to different tables
into different topics the answer is yes
you can configure in each division
instance to listen to I want this
division instance to connect to this
this databases the this database these
tables into publish into these topics I
have to check the documentation but I
also think that you can publish it to
multiple the topic is in the same
division instance but I just need to
check because because in my my trials
adjust adjust will connected so I want
the vision to connect to these tables
and I want all events that happen on
these tables you don't have to monitor
all the tables in the database you get
to theater but that's a good question I
need to address that thank you very much
anything else no oh here
yeah many - or framers or things like
that or shorting how we do the increment
updates okay the question is do we have
any tools or frameworks for sharding or
data so don't we don't have zero
downtime you said we don't have done
time where apply our migration
unfortunately I'm not aware of any
framework or tools because when you when
you're using sharding for your update
statements it means that you have a huge
amount of data maybe you don't have
indexes so and it's too big that's why
it's taking so long
anything else but usually the most
common strategy in fact the only stretch
that I've seen people applying is that
you just get if you want to partition by
ad maybe you can update all of the even
IDs in one update statement all the odd
in the other ones maybe that's too slow
because you have to rehearse your
migrations maybe you can des from one to
a thousand and we're going to apply this
one from a thousand one to two thousand
then in another update it really depends
on the application your tables your
database and also when we're talking
about the alter table statements I'm
assuming that the database that you're
using allows you to do alter table
statement specially add column once
we've zeroed on time without locking I
know that some I'm not going to say some
say the name of the vendors but as I
know that the most popular vendors in
the database space they allow you to
have zero downtime alter table
statements at least they add column one
okay so sorry you have to do that by
hand but of course you should be using
fire or leek base to be automating your
migrations okay thank you anything else
options for example to keep this events
if for example Division II was stop it
yes and I don't want to receive the
events that was generated during this
area so stop it mm-hmm the question is
there waved in division for me to just
ignore the events that happen when
division what stop it so I just consumed
the new ones okay well usually the good
thing about the vision that we started
at the moment that it stopped it's so
usually people consider that a feature
because the traditional way would you
come back oh I lost everything that
happened in the past two days now I'm
starting over again I would handle that
on the consumer side but I don't think
that answers your question because you
don't know when that happens
so yeah the answer for me right now is I
don't know because it was - because the
good thing about the vision is that it
never miss this is an event so if you
want to miss an event I have to talk to
the engineers about some user case
because that the first time I hear it
but I want our check your your answer if
you tweet or email me I can check the
dancer later so actually a consumer is
the is a calf care consumer so you can
just say give me the latest and ignore
what happened to the back okay I have
one of the engineers here so you can you
can tell them to ignore the the previous
advantage just start with the laces is
it's a Kafka feature right
oh yes that's a Casca connector
parameter yeah just start with the
latest invest instead of earliest for
example zero downtime when we have
stored procedures
so basically when we update our trade
application to this new version of this
feature
okay the question is zero downtime we
start procedures okay I hear this
question a lot too but the answer is I'm
not stored procedure experts so it's
kind of out of scope of my research
about this user a long time things but
again if any of you has any tips or
scenarios how to apply the zero downtime
things we use in store procedures I
would be more than happy to collect them
but currently I don't have any tips or
strategies for applying certain time
migrations using store procedures okay
anything else I think time is up but
I'll be able to answer questions here
and at the hall and at the conference
until tomorrow thank you very much again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>