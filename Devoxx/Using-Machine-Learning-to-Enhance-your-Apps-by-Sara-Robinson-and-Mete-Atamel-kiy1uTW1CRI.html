<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Machine Learning to Enhance your Apps by Sara Robinson and Mete Atamel | Coder Coacher - Coaching Coders</title><meta content="Using Machine Learning to Enhance your Apps by Sara Robinson and Mete Atamel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Machine Learning to Enhance your Apps by Sara Robinson and Mete Atamel</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kiy1uTW1CRI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
alright this is working can I run here
yes cool okay okay hi everyone my name
is meta and i'm here with sarah and
today we're going to talk about machine
learning for your apps before we start
our talk first it will be information
about ourselves so Sarah you sure my
name is sarah robinson I'm a developer
advocate on the Google cloud platform
team you can find me on Twitter at s Rob
tweets I live in New York but I'm super
excited to be here in Belgium today I'm
a swift fan both Taylor Swift and the
language Swift know swift code samples
today though I'm also a big fan of harry
potter and I meta I'm in the same team
as Sarah but I'm based in London I live
in this English town on the Prime
Meridian the zero point does anyone know
which time that might be Greenwich yes
it's a great place if you ever visit
London and you want to get out of London
craziness go to Greenwich there's a nice
park there's a planetary is really nice
but I'm originally from the island of
Aphrodite does anyone know where that is
yeah cyprus yes you know I feel like the
ton of Greenwich and the Cyprus tourism
organization they should pay me because
every time I give a talk I mention these
but both places are really nice to visit
if you are looking for places so that's
us so our agenda today is basically
first we're going to introduce machine
learning at Google because machine
learning it's a big thing at Google and
we have lots of products and lots of
different ways that we are using machine
learning at Google so Sarah will
introduce that and then after that I'm
going to talk about the machine running
spectrum because you can big your own
machine learning models and or you can
use what's already out there so we'll
talk about that spectrum that exists and
then i'll go through vision API which is
a complex image detection with a simple
rest request and then Sarah will cover
speech API analyt and let your language
api so that's kind of the agenda so now
Sarah will cover the machine learning at
Google cool thanks Metta so as not to
mention there's a lot of different
applications at Google that use machine
learning I'm sure many of you are
familiar with some of these examples i'm
going to talk about a few specifically
after the slide loads
let's see there we go so we're using
machine learning in many applications
across Google over 60 applications to be
specific and the use of deep learning
and google has grown significantly in
the past even three years so one example
is Google Music anyone here use the
Google Music app a couple people awesome
so Google Music uses machine learning to
create recommendations for you
recommending new artists or songs based
on what you've already listened to
another example is the speech
recognition in ok Google so when you say
something to ok Google it'll transcribe
it to text that's using a machine
learning model under the hood to be able
to understand different accents lots of
other things in different languages and
then another example is google translate
has anyone use google translate before
looks like most people it's very useful
i've used it on this trip actually so
this is using machine learning in two
ways one to recognize that there's
actually text in the picture on that
exit sign and where that text is in the
image and then once it finds the text it
is translating it in real time using
google translate under the hood and any
of you can try this out if you have goop
the Google Translate app just take a
picture and it'll translate in whatever
language you'd like to and then another
example is inbox so it'll it's using
machine learning to create these smart
replies based on the content of your
email something that you would respond
based on things you've responded in the
past I'm sure many of you have used
google image search so the way that this
uses machine learning is i typed
brussels and it creates these categories
of things it thinks I might want to
search for so for example I might be
looking for things to do in Brussels I
might also be looking for Brussels
sprouts I do like to eat Brussels
sprouts and so it creates all these
categories maybe I'm looking for a map
or people in Brussels and then we have
google photos and so Google photos will
automatically create all these
categories
a photos party of these are some
screenshots of mine so it has locations
San Francisco Berlin and I can also type
search terms in here so if i search for
cats it'll show me all my cat photos by
search for selfies it can show me all my
selfies this doesn't seem that hard
right like if I'm looking for pictures
of selfies they're all pictures with a
person in it that are probably taken
with the front-facing camera but machine
learning is actually pretty complex
problem to solve here we have pictures
of puppies or muffins and it's actually
kind of hard to tell which is which
right I mean to write the code to
identify the differences between a puppy
and a muffin would be very difficult as
web developers we wouldn't want to spend
our time doing that here's another
example this is sheepdogs and mops also
very hard to tell the difference and
here we have a picture of an apple and
an orange and for our brains it's super
easy to recognize the difference to this
we can identify almost immediately but
let's say we were to start writing an
algorithm to identify the difference
between pictures of apples and oranges
and any of you anybody have an idea of
where you would start what type of
differentiation you would be looking for
thoughts color color could be good but
then what if you have black and white
images then you can't look for color
someone said texture textures is a good
place to start also but then what if you
have a picture of a cross section of an
orange right it's going to be the same
classification that you want at the end
you you want to say orange but it has a
completely different texture than the
outside and then what if I add another
fruit here what if i add a picture of a
pair then we have to start all over
again and as developers you know would
be nice if we could focus on making our
apps awesome building cool features
rather than writing algorithms to
identify the difference between an apple
and an orange for example so what if I'm
not a machine learning expert yes and I
actually worked on machine learning for
the first time like in 2005 this was for
my master's thesis and I was trying to
do spam filtering back then Janelle was
new and you know all the email providers
they didn't really have great spam
filtering so for my message master's
thesis I thought might
I write a machine learning algorithm for
spam filtering so the first problem that
I have to overcome is what machine
learning algorithm do I need to use you
know you have you have naive Bayes you
have supervised learning unsupervised
learning you have some support vector
machines you have neural networks if
you're an expert then you know given
certain problems you can you can kind of
guess or you have some kind of intuition
about what algorithm to use but if you
are just a student like me or if you're
a developer and you don't really know
much about machine learning it is hard
because there's a lot of literature at
their own machine like that you have to
learn so that you can have that
intuition well thankfully my professor
he helped me to start with something
simple so i started with i think it was
it was called naive bayes algorithm its
supervised learning so you have this
training set or email emails with
different features and then you have the
label spam or not spam so you would use
those though and you in a naive based
algorithm you assume that each feature
is independent from each other which is
a simple assumption because if you think
about it for example in Apple's you have
the shape which is usually round and you
have the colors its usual red green or
yellow so the features are not really
independent but that's the assumption
that I made to make it simple so once
you pick your algorithm and then then
you have to pick your features what
features you look for I mean you you you
like machine learning learning the
features but sometimes you help it with
officials that you think are good the
other problem that you have is the data
you have lots of data it for me at least
for spam fielding I lots of data but
there weren't in a uniform form and some
of them were not that great so I had to
actually massage the data and get it
into a shape that I wanted to train and
test so as you can see there's lots of
lots of different things that you have
to figure out and in the end I built the
system and where you know the email
would come in and then it would split
into two one was like the regular email
chain and the other one was my machine
learning pipeline and I convinced my my
professor to use my
and filtering algorithm I think he did
it because he was nice and he wanted me
to that succeed and i'm not sure if he
continued using it afterwards but what I
realized is that I really enjoyed
building the system because you know at
heart I'm a software engineer and I like
to build things but I didn't really
enjoy so much about learning all the
math and all the statistics and all the
details of machine learning but I had to
build it myself because there wasn't
anything there was non layer based
algorithms out that I had to build it
myself so in the next part I want to
talk about the spectrum that we have a
google cloud for machine learning so if
you are a machine learning person and if
you know these algorithms and if you
like to you know apply your own learning
machine algorithm and build your own
model there's tensorflow tensorflow it's
an open source project it's it has in
Python API and all the basic machine
learning algorithms the naive Bayes and
and neural networks and support vector
machines they're already implemented so
you can just using the API you can you
have access to this algorithm so you
don't have to start from scratch but you
still need to know about machine
learning you still need to know about
which algorithm to use how to use it
there's still lots of different
parameters that you can change and in
machine learning one of the things that
I didn't like is you know there's lots
of parameters and and depending on how
you change them it affects results in
densely so you still need to help that
intuition and knowledge for tensorflow
but in some cases you need to build your
own algorithm and and then you know if
that's what what you have to use now one
of the challenges with tensorflow is
this scale you need to train your model
in a scalable way so for that there's
cloud machine learning so clap machine
learning it's part of google cloud it
basically enables you to write to train
your tends to flow models on google
cloud so it helps you basically to scale
your tensorflow so you don't have to run
you don't have to worry about the
infrastructure or running tensorflow now
on the right side we have what we call
friendly machine learning and these are
the guys that I really like and in
friendly machine learning we basically
have pre-trained models for you to use
so at Google with vu
lots of images to train our models we
use what so text to train our text model
so we basically have the models ready
for you to use and all you have to do is
just create a rest request to these
api's and get the results back and I and
we're going to go through these AAP as
in detail shortly so it's good to know
the spectrum because if you don't if you
don't really care about building your
own machine learning model you can use
what's already trained out there and
just make the rest request and get the
results or if you need to build your own
thing you have tensorflow you can do it
in-house or somewhere else or you can do
it in the cloud using cloud machine
learning so what do we so the rest of
the talk will be about this friendly
machine learning because we are focusing
on building your apps using the machine
learning that's already available to you
so that's what we're going to cover next
so in terms of the AP is we have we have
three ApS vision cloud speech and cloud
natural language and you're going to go
through this in order in detail so
vision API it's basically all about
image detection so you pass an image to
Google Cloud and Google Cloud responds
with all sorts of different information
so what kind of information so label
detection for example you can pass an
image of an animal and it will tell you
what that animal is whether it's a cat
or a dog it whether it's mammal or
something else so given the image it
basically tries to extract all the
labels about that image the second thing
is face detection so you can pass in a
picture of people and it will detect the
faces of people where they are in the
picture and it will also detect things
like whether the person is happy sad so
everything about the expression of the
person I one thing to note it doesn't
recognize people so you can't just say
you can say this is John or something
and we don't want to do that it just txt
the new faces and expressions the next
one is OCR so you can passing a text
sorry an image and it extracts the text
from the image it also extracts the
where the text is in the image so you
can you can tell you know there is no
parking at this bounded location the
next one is explicit contact detection
so you can pass
image and it will tell you whether it
has violence or it has adult content
content or medical content stuff like
that there's landmark detection so you
you can pass in the picture or Eiffel
Tower and it will tell you where it is
with lat and long and it also detects
logos and some famous logos that you
have like Disney or coca-cola or
something like that it will detect that
as well so to make a request it's it's a
simple JSON requests so you if you first
pass in the image you can pass in the
image in two ways you can pass image
directly using base64 encoding or you
can also save it to Google Cloud storage
this is an area of google cloud where
you can save binary files so you can
save it there and you can just point to
the image from the request and then once
you pass in the image you basically tell
the vision API what you are interested
in so in this case for example we are
interested in label detection and we're
also interested in face detection so you
can you can tell maybe if you're not
interested in face detection you don't
have to include that so it's up to you
to determine what kind of features you
are interested in from vision API you
can also say what max results you want
because vision API by default it will
try to extract as much information as
possible from the image but you can
limit that with with this max results
parameter and then to look at the
response for example in this image which
is kind of hard hard to know what it is
once you pass in the image this is what
you get from vision API am so we are
getting that it's a dessert and then
this data score so this course says this
is ninety percent dessert 92% benya and
maybe seventy four percent profit oral
and it even picked up the powdered sugar
on the dessert which is kind of amazing
for me at seven one percent one thing to
note there's also this ID it's an entity
ID so Google has this knowledge graph
with all sorts of entities so basically
basically everything about the world and
using this ID you can you can pass this
ID to to the knowledge graph and it will
tell you more information about this
entity and i'll show you in the next
slide
an example of that and this is a
response from landmark detection so here
we pass in this image and then it
detected that it's guilt houses and it
also tells us where it is in in the
image the bounding Pollack polygon and
then it also tells us the lat and long
of it which is really useful and as you
can see it also detects if there are two
things it will detect them separately so
it doesn't have to be just one thing in
the image and then the knowledge graph
that I mentioned so here you get a
entity ID right so you can pass this
entity to knowledge graph and get even
more information about about the thing
that you're looking at so yes yes yes
I'll get there actually we have a real
world use case that i'm going to show
you yes first i need to describe what it
is so they can talk about how you can
use it right so thanks so here as you
can see we pass in the ID and then we
get more information about this and how
is this used for example if you do if in
google search if you search for images
on there's on the sidebar you actually
get any information about that image
from wikipedia usually so this is how it
happens it uses a knowledge graph to
populate that basically and then the
next one is text detection so here we
are passing an image with some text and
it picked up the fact that this is yours
Coast Guard and then it also tells us
the bounding the bonding and polygons
for for each text and the next one is
face detection so here we are passing an
image these are my coworkers in some
somewhere in Jordan I think and it picks
up the fact that this is this person has
a head wear a hat and the person is
probably not surprised but the person is
probably joyful which which is good and
then in the next one there's no
air and then we're still joyful and then
in the same image again you can pick up
different things so here we are picking
up that the fact that this is there's
some pink in the picture there are
people there sunglasses they're smiling
and then in landmark detection it picked
up the fact that it's from Petra in
Jordan and with let's and long and then
we can even pass the image and see if
it's safe so in this image there's no
adult of violence or anything like that
and you can even try this in the browser
yourself so if we go here this is a
cloud google.com / vision and then in
and let's pick an image from our Twitter
so this is the Twitter for that box
Belgium so let's just save this image
I'll save it on the desktop that works
pay save and then if we go here and then
select the image
let me make this bigger so from here we
can tell that it detected the faces and
it's telling us that people are most
likely joyful phase 1 and phase 2 so
because there's two people and if you
look at labels it it's at fashion I
guess that makes sense for text it
picked up all the dev ox's the Box the
United States and other devoxx is here
and there and it detects the primary
colors the gray and shades of grey for
safe search it said that this is a
normal image with no medical or violence
or and stuff like that and you can even
look at the JSON response itself so this
is what you actually get and these are
the things that we pull from json and
just displayed here and then in the last
part i just want to show you this cool
app it's called claude cats so we have a
PM for node.js and App Engine Justin
back with he's based in Seattle Seattle
and he loves cats and he also likes to
read reddit so in reddit if we go if you
go to reddit there's a subreddit called
/ r / all so it shows like cute pictures
of cats and dogs and other animals so
what he wanted to do is you know can I
actually look at this page and see if
there are more cat pictures and or if
there are more dog pictures so that was
the question that he had so he built
this small app to use vision API to
determine that and this app is called
Clark cats so this is justin's calf cat
and it's a real cat living in seattle
and once i hit plus what it will do is
it will go to reddit pull those images
classify them using vision API and then
and then sorry and then and then you
will display dogs on the right and catch
on the left and it will count them and
then see which ones are more dogs or
cats so do you guys have any guesses on
which might be more
cats who says cats okay and who says
dogs all right we'll see we'll see so
let's start so now we are we're going to
pull the images if everything is working
it takes couple of seconds usually for
the first image there right yeah so cats
here we had the same image twice i guess
and it logs here and i think by default
it picks 50 images or something like
that and then in the end it will count
them so dogs are ahead i guess
cats are catching up it's very close
like yesterday but I won't mention that
it should be done soon I promise nope
I'll let it run and then maybe I'll show
you in the meantime the code for this so
by the way this cloud cats is open
source and you can you can take a look
at it if you want to and I it
under my github but i just want to show
you the vision API and I this is a
node.js application so it's in
JavaScript but if you look under Worker
envisioned so this is the file that
actually does the the that uses the
division API so all you'll be doing all
we doing here is that we are using the
g-cloud NPM package and then from
g-cloud you have access to all different
packages that you can use to talk to
google cloud so here we are accessing
vision API so this is the vision API
Omeros accessing this storage API this
is the this is where we're going to
store the images and then these three
lines of code is all it takes basically
to use this vision vision API so in this
code we are making a request to reddit
to get the URL so this is the URL of the
of the image and then from there we save
it to class storage and then from there
we call vision detect labels and passing
the file and this will actually detect
the the image and get the labels and
then this these labels are passed to the
client to do the classification so
that's all there is to it for that and
if you look here I still running I don't
know why well I'll I'll let Sarah speak
more and then maybe we'll come back to
this NC a you see if you find out who's
winning all right so that's that and
then just to show you the architecture
real quick we are basically in this case
we are you this is the browser where
you're using App Engine on google cloud
this is the front end and this is the
back end and the front and the back end
they are communicating using something
called pops up it's a messaging
form an onion Google Cloud the reason
why we use pops up in the middle is so
that they are loosely coupled so you can
change the back end in the front end and
and since there's pops up in the middle
you don't really care about each other
and then the back end it basically goes
and picks the images saves them into
cloud storage and then cause vision API
on it once it has a classification it
tells it to pops up and then pops up
tells it to a pension or a pageant
answer to the browser so let's that's
the desire collector and to your
question so this is cool there you know
kids are cool but then how how can I use
this in my apps so one example is from
sales worry it says where it's a
supermarket chain in the UK and they
wanted to basically make sure that they
put the correct labels on correct
products so they use vision API to
detect the labels and make sure that the
labels and the expiration dates they
match and make sure that people running
them they are pulling the correct label
so they pass in this so do people once
in a while they take a picture of the
labels and then this gets passed to a
vision API and the vision API picks up
the texts and the expression and it does
do it does a check basically to make
sure they match and also make sure that
the person at that station doing doing
the labeling is you're using the correct
labels so this is one example of course
depending on what you want to do you
will have different use cases but it
really depends on what you want to do
with your application so this covers up
the vision API now Sarah is going to
talk about the speech API and the
natural language api cool thanks Metta
so now I'm going to talk about the
speech API which lets you do speech to
text transcription in over 80 languages
has anybody tried it out yet one or two
people awesome so essentially what this
does is it exposes the machine learning
models that we use to power ok Google to
developers so you can build an app that
does something similar so quick overview
of the API it does speech to text
transcription in over 80 languages you
just need to tell it the language of the
audio file that you're passing it and
it'll be able to transcribe it it
supports two types of recognition it can
do streaming recognition or non
streaming so you can send it audio as
it's being recorded or you can send it a
audio file and it also filters
inappropriate content in audio similar
to how the vision API does that with
photos and this is a what requests will
look like our response looks like from
the speech API so it'll return
transcript so this is just an audio file
that says how old is the Brooklyn Bridge
and similarly to the vision API will
return a confidence value is it how sure
it is that this was an accurate
transcription and his final just
indicates that it's the final one you
would only see this on the streaming
recognition so the easiest way to
demonstrate the speech API is through
live demo so I wrote a little bash
script that does these four things so
first it'll make a recording using socks
which is just a command-line utility
that I installed with brew and then
it'll be 64 encode the recording will
just be a five second recording and then
we'll build our API request in a JSON
file and we'll just use curl to send the
request to the speech API so I'm going
to open up here's what the bash script
looks like pretty straightforward just
creates a file and then sends it to the
speech API and I can call it here by
just running bash request Sh so I'm
going to record something real quick
press Enter hello everyone I'm super
excited to be at devoxx Belgium today ok
so created the file it looks like this
it has the encoding type in it here
we're using the flak encoding type the
sample rate in Hertz and then the
language code if you don't supply the
language code it will default to us
English and so now I'm sending it to the
speech API using this curl command just
passing passing it to the sink recognize
and here you can see that did pretty
well ninety-four percent confident
everyone see that it said hello everyone
I'm super excited to be at the Box
Belgium today obviously the conference
is called dev ox not the box but dev ox
is not a widely recognized word so one
thing we can do is there's a parameter
that you can pass called speech context
and then we can pass this a phrases key
and we can give it a hint
that we're looking for the word dev ox
since it's not an English word so now
it'll look for the word dev box if I go
back and let's clear this out and I run
my script again hello everyone I'm super
excited to be at devoxx Belgium today
and let's see how it does with the new
hint sending it to the speech API so you
know that did not work all right let's
try it once more all right we'll try it
again live demos you know once more
hello everyone i am at devoxx belgium
today in antwerp all right let's try it
out all right it got DevOps thought I
said hello darlin that's pretty close
I'm an even got an twere so that's
pretty cool and it is ninety-one percent
confident now I mentioned that it does
speech to text transcription in over 80
languages so does anyone want to come up
and try it out with a non-english
language any volunteers come on you
wanna well met I speaks another language
because I can do it yeah okay what
language hungarian do you know the
language code ok I'll look it up we got
to get the language code which code it
is see it looks like hu qu you okay are
you ready so first I have to pass it the
code as a parameter so then when i press
enter just speak into here and you can
talk for five seconds ok ready go see
you dead box
because make each me up to okay so we
can see it used hu as the language code
this time and let's see how it does
thank you for volunteering real obvious
he was fallin told and how did it do I
don't speak Hungarian let me I'll make
it a little bigger okay okay so it got
part of it that's probably reflected in
the confidence score two is only 71
percent confident that time but thank
you for volunteering you should all try
it out in different languages and next
I'm going to go back to the slides and
now i'm going to talk about the natural
language api the natural language api is
the newest of our machine learning api's
and lets you perform a sentiment and
entity recognition on text has anybody
tried it out looks like nobody has yet
so what can you do with the natural
language api there's three methods that
it has i'll go into each and in detail
but here's a high-level overview so the
first is analyzing entities so if i pass
it this sentence Antwerp is a city in
belgium it'll return Antwerp and Belgium
as entities and then it can also do
sentiment analysis so if i say Antwerp
is my favorite place to visit it will
tell me if this is overall positive or
negative and how strong that sentiment
is in that sentence and then finally it
can do syntax analysis so if i pass it
this sentence and work aunt were posted
the 1920s summer olympics it will tell
me that hosted is the root verb of the
sentence I'm and I'll tell me how all
the words in a sentence relate to each
other so let's take a look at each
method so here's a sentence I took from
a new york times article about Antwerp I
send it to the entity analysis endpoint
and it was able to find these four
things as entities and this is what the
JSON looks like that's returns a little
hard to see here but there's different
types of entities it can identify
so this person whose name I'm not going
to try to pronounce as identified as a
person it also gives me their wikipedia
URL if they have one I menendez salience
number refers to how central the word is
to the sentence overall and it's a range
from 0 to 1 and it also tells you where
that is in the context of the entire
text that you pass it so we can see
Antwerp is a location entity type and
Royal Academy of Fine Arts is an
organization and we get the Wikipedia
URL for all of these if it identifies an
entity that doesn't have a Wikipedia URL
you'll still get it back there just
won't be any metadata in that key so the
next thing is analyzing sentiment so I
pass it a sentence and work is my
favorite place to visit it's going to
return to values for us the first is
polarity which is a negative 1 to 1
range in how positive or negative the
sentence is so this one is fully
positive so it gets a one and then
magnitude is going to be 0 to infinity
range of how strong the words in a
sentence are regardless of their
positive or negative so like the word
favorite probably contributes to this
response I mean it's normalized based on
the length of the text since this is
pretty short we get a pretty small
number point 9 and then finally we have
analyzing syntax so this will break
apart the different linguistic
components of the sentence so here I
have Antwoord posts to the 1920 summer
olympics and this visualization here is
called a dependency parse tree tells us
how all the words in a sentence relate
and you'll get a JSON response back that
basically has all this data in it so if
we look here on the top row it tells us
the role of each word in the sentence so
hosted is the root verb antwerp is the
nominal subjects here we have a number
punctuation and then what we have here
is called the lemma which is the
canonical root form of the word so if it
was hosting hosts hosted it would all
return a lemma of hosts this is useful
if you're counting how many times a
different word occurs in all the
requests you're passing to the API
different variations of the same word
will only be counted once and then on
the bottom row here we have just the
part of speech of the word so Antwerp is
a noun hosted is a verb etc so you might
be thinking that
it's all cool but it's pretty gets into
the technical linguistic detail how
would I actually use that in a nap in
the real world so I did something during
the World Series two weeks ago I don't
know if any of you watch it here
probably not that exciting in Belgium
but it's a baseball world series in the
US and this year it was the chicago cubs
vs. the cleveland indians and so i ran a
script to stream all the tweets that had
World Series or any mentions of the Cubs
or Indians in it and I wanted to I send
it to the natural language api to do
syntax and sentiment analysis to see if
tweets were positive or negative about a
certain team and then also track what
the most commonly mentioned subjects
were in each tweet and then i sent that
to Google bigquery does anybody here use
bigquery looks like just a few big craze
our big data analytics warehousing tool
and it lets you run sequel queries on
really really large data sets and it's
very fast and you can run the queries
directly in the browser which I'll show
you in a minute and bigquery has links
with a bunch of different business
intelligence data visualization tools so
in this example I use a tool called
exploratory which is just a desktop app
and I pass it a CSV file of my bigquery
export and was able to make some
interesting visualizations of the data I
have a blog post about it right here if
you want to learn more but I wanted to
show you some of the queries that I did
so I'm going to open up bigquery and
this is the bigquery web UI interface
let me get a little bit bigger so if we
look here we can see the schema of my
table so for every tweet I pass it the
ID of the tweet the tweet text I'm a
timestamp of when it was created how
many followers the user has that tweeted
it any hashtags which is returned from
the Twitter streaming API just gives me
a JSON object with all the hashtags in
the tweet and then I'm also saving a
giant JSON string of the tokens response
that I get back from the natural
language api along with the polarity
magnitude and the location of where the
tweet was from if there was one provided
this is what it looks like here's some
example tweets and you can see this
giant JSON string here of all the
natural language data that was returned
from it so the syntax
and finally if I go all the way to the
end I get the polarity and magnitude
values there so you're probably thinking
that's a giant json string how am I
going to parse that with sequel well Vic
where has a feature called user defined
functions that lets you write these
custom JavaScript functions so I'm going
to pull one up here that you can run on
columns in your table so what this is
doing is it's going to use the natural
language API returns end subs as the
nominal subject so for each sense it'll
have one subject so here I'm running
this custom JavaScript function to get
the subject and then the sentiment so
I'm getting the sentiment by multiplying
the polarity times the magnitude which
is a good gauge for sentiment if you
want a good way to measure it when you
get the response back from the natural
language api so i'm multiplying those
two to get the sentiment and then i'm
counting how many times each subject was
mentioned and then i'm going to order by
the subjects that were mentioned the
most and limit by a hundred so let's run
this query and see what we get it was a
pretty exciting world series game I
don't know if any one watch did anybody
watch cup one person okay well the Cubs
hadn't won in 108 years so it was a big
deal so here we can see the most
mentioned subject some of them aren't
that exciting they're just pronouns but
here we have Cubs got a sentiment
average sentiment per tweet with 0 point
25 Indians was a little bit lower Oh
point one four maybe because they lost
cub was this every 1.25 also game world
series Murray I'm guessing is a player
on one of the team's Chicago also again
some of the players from the different
teams we can see we get pretty varying
sentiment on all of these different
subjects and that ran really fast and
big gray took just under eight seconds
to run this custom JavaScript function
on hundreds of thousands of tweets that
I stored in bigquery so let me do one
more query so here I have one I wanted
to see what the top emojis were
people were using in their tweets about
the World Series because everyone loves
emojis or at least I do so here I'm just
looking for the specific character codes
that emoji start with so we can see not
surprisingly the baseball emoji was used
13,000 times it's pretty crazy it's a
lot of baseball emoji the laughing
crying it's always a popular one the
Bears for the Cubs the Cubs one of the
Cubs colors is blue so I'm guessing like
that's why that one was there clapping
heart eyes we can see all the different
popular emojis here this was I think the
Indians team emoji that they started
using so there we go and it took less
than 66 seconds to run in bigquery so
again I have a medium post on it if you
want to learn more and this is one of
the graphs I created using exploratory
zoom in so i was able to show which
subjects had them the highest sentiment
here crisp was one of the players his
sentiment average sentiment was all the
way at point eight six and then it goes
down from there and then i show the
queries there and here is an example of
average sentiment of all tweets over
time for the first hour of the game i
did that again with exploratory so i'm
going to go back to the slides briefly
I'm just a recap of the API as we
covered so the vision API if you want to
learn more cloud google com / vision
same with speech and natural language
and meta showed you the in browser demo
for the vision API we also have them on
the product pages for speech and natural
language so you can record an audio file
directly in the browser without having
to write any code in different languages
and test the speech API you can do the
same with natural language you can paste
the sentence into the browser and it'll
show you a dependency parse tree of all
the syntax and the text you can also see
the entities it extracts so definitely
recommend trying it out and thank you
for coming and let's see if the clock
gets finished though it's just a wedding
now I forgot about that Oh give a quote
I'll hear ya so they're still running
yeah night except something is wrong
usually it's done in like one minute but
yeah we will never know unfortunately so
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>