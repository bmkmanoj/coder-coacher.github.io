<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transactions and Concurrency Control Patterns by Vlad Mihalcea | Coder Coacher - Coaching Coders</title><meta content="Transactions and Concurrency Control Patterns by Vlad Mihalcea - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Transactions and Concurrency Control Patterns by Vlad Mihalcea</b></h2><h5 class="post__date">2017-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/onYjxRcToto" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming
my name is Robin Hodge and today I'm
going to talk to you about transactions
and concurrency control patterns and
first of all little things about a
little bit about myself I work for the
hibernate project and my developer
advocate and also I do a lot of
development on the core on my blog blog
nación can find over 100 blog post
about hibernate databases transactions
concurrence in control also if you use
twitter you can you can find my twitter
handle here and my github and this talk
as well is based on the research that
i've done for the first part of my book
high-performance java persistence and
some of some of the topics are going to
be covered in the second edition ok so
before i before i start with reaction
with the actual content i want to give
this quote this is from the google
spanner research birthday this is quite
new the google spanner better services
only release like weeks ago so think
about it nowadays Google is investing in
a distributed database a globally
distributed database that provides
transactions that provides a seed that
was you know like 10 years from like
when the no SQL movement as you March so
nowadays no if you take a look on this
code you will see that you know working
with transaction is not easy it's hard
but working without transaction is
actually even harder so in a way we
don't we we we don't we cannot escape it
so it's better to understand what
happens behind the scenes so when you're
working with a relational database you
have to understand that every time you
are going to use transactions even if
you don't declare them you are going to
still use transaction but it's just the
scope that changes if you don't declare
them every statement is going to execute
in its own transaction we are going to
work in a
to commit but usually that's bad because
you really want to set the boundaries
you really want to set when the
transactions should start and where it
should end either by comet or or
rollback so now I need a bit of history
initially this term was not acid because
if you like the paper this was written
by Jim Gray in 1990 and he defined
consistency he defined atomicity and
durability there was no isolation
because he assumed that you are going to
work with civilize ability so we're
going to discuss about those because a
very important concept first ethnicity
what does it mean it means that we have
a unit of word it means that if we don't
work in auto complete we can enroll
multiple statements in the unit of work
that either works and every statement
was executed successfully or the ninja
world fails and then we can roll back
every change
so basically atomicity means that we
need rolling back we need to roll we
need a cup we need this feature to be
able to roll back and to be able to roll
back we need at least to prevent dirty
right this is a very bad anomaly
unfortunately every database has to has
to provide at least this because
otherwise it will compromise ethnicity
so very good what what would the dirty
write do to your application in this if
that could happen it would mean that two
transaction like Alex and Bob could both
modify an entry but then it's not really
clear what should happen if Alice rolls
back what would happen should we roll
back to the previous version then we we
would override Bob's changes but then
what would happen
Bob commits or what we happen if Bob
rolls back so there's some uncertainty
and because of that every database
provides at least this at least this
level of of integrity and this is a very
interesting property because it shows
you that you cannot have write write
conflicts and because of that you
see that you can use this especially
when used optimistic locking to too much
relies conflicts over different entities
we'll see at the end of the presentation
okay so that was something now
consistency consistency in our now
industry is a very misleading terms
because it means different things in
different context in the context of
relational databases and I asked it's
just about constraints it's it's about
according to the definitions about
moving the database from one consistent
state to the other but practically you
can better visualize it if you think
about constraints like you any
constraints like not Nuala like primary
key like foreign key those constraints
should be on when you commit a
transaction all of those should be valid
so even if you have one constant which
is not very it will just roll back your
transaction when are we much more
popular than as it is this consistency
in capturing because it's very very
popular it has nothing to do with that's
it that's just a recency quanti so in in
capturing consistency is actually it's
not very good a very good term actually
the best terms generalizability which
tells you that if you have a master
slave system a distributive system and
and you do a change in master and then
immediately after in time you read from
the slave
if the system is linearizable then you
will see the updated change if it's not
it will lag behind and you will read a
stale value that's consistency capturing
I just said it to clear this this is
understanding now about durability what
does it mean it means that once we
commit a transaction all the changes are
persisted no matter what even with free
unplug the cable from the relational
database all the changes should be
persisted but then you will think about
how well that shouldn't be really
complicated because you can just flush
to the disk of the changes hope why do
you why do I have to to state that as a
property here it's because it makes much
more sense when you went wings look
and understand how data based work
because of course you have table and you
have that on the tables you have data on
the indexes but those are on disk so the
database does something like that it
uses memory mapped files to load them in
memory it actually splits all that data
into pages like six pages like eight
kilobytes so you can mirror them in
memory because it's much more efficient
to work them with them in memory and
actually if you can fit to the working
set into memory is going to be very
efficient because ROM even even compared
to so state drive run is like orders of
magnitude faster to to work to work with
it so then during you need to change all
your changes are applied to the
in-memory pages like tables now or
indexes and then during during commit
you you don't flush
you don't always flush to disk why
because flushing to disk this thing
pages would imply a lot of random access
rights which are very slow so the
database does something interesting it
delays it delays the synchronization
between the memory and the DS and the
and that only happens during checkpoint
time and the like for example it can
happen every other minute but then what
happens if I unplug and the database
could not synchronize it didn't manage
to synchronize the in memory with the
disk and that will the redo log coming
comes into play when you do the commit
you write everything but you apply the
changes to the in-memory structures and
then you apply also you write all the
changes in the read-only lock which is
append only and it's sequential and it's
fast you can you can flush it to do this
thing after everything after every
change Action Committee and it will be
fast the undo log is used for rolling
back in some databases if they actually
has this name it's a concept it is for
rolling back and it's used for
automatically so that you apply changes
in memory you know how to get them back
if you want to roll back to the previous
state ok so now if we advanced a little
bit in time in SQL 92 the term changed
so right
we also have isolation so now we we
don't assume that every time we are
using Sarai's survivability we can lower
we can use weaker isolation levels as we
will see what what that implies first of
all sir eyes ability the definition of
sterilize ability that I can allow
concurrency and statements can be
interleaved reasoning right can be
interleaved but that should happen in
such a way that it gives you the same
outcome as if the transaction would
execute say like in a serial execution
one after the other something like that
like Alice does her transaction she come
in and then bomb comes he does his
transaction and he commits and actually
there are systems that work that wave as
we will see now if you if you allow
concurrency if you want to allow Conn if
you want to allow multiple connections
and multiple user can connect to your
database and you don't do any conquest
control you can have conflicts all sorts
of conflicts like this one like for
example both both Alice and Bob are
going to read an account
Bob manages to update it and convince
and then Alice comes and updated as well
but she doesn't realize that Bob that
has has changed the row so the you
haven't lost updated just just one type
of conflict so we need some concurrency
control to deal with all those conflicts
and their teachers are to two ways to do
that you can either avoid concurrency
altogether for example volte be that
that is because it will get its working
in memory it's data it's a new SQL
database it works in memory and because
it works in memory it can it can work in
such a way that all the transaction can
be actually very very fast with very
very short response time so for this
particular reason you can just use a
single thread and every every
transactional basic read it one after
the other so of course you get the
reliability because actually get a
serial execution but then what those are
more like an exception
general rule in reality or relational
database or being even other databases
allow multiple connections to to end
multiple transaction to to rerun in the
same time concurrently so you need a way
to control and to deal with conflicts so
when dealing with conflict in conquest
in control there basically two two
things that you can do you can either
avoid them you know you can use
two-phase locking to avoid conflicts
organize or you can detect them and you
can that's that's what multi-version
concurrency control does no matter which
one of those you are going to to choose
there would always be some locks
it's just that two-phase locking is more
locking well MVCC uses tries to use less
lock it's not like it's not going to use
any lock at all
because as I explained before you still
have to provide prevention for dirty
right so it must imply that some locks
are going to be acquired once you modify
them so basically even if you modify
want a single row you're going to
acquire one lock on that roll no matter
if you are using two-phase locking or or
MVCC and typically we have two type of
log this is pretty much the same like if
you ever worked with a Java util
concurrent read write lock you know that
you have read locks which are shared
locks and exclusive lock and which are
also called write locks the Sherlock it
was like that once you acquire acquired
it anyone can still read that row but
changes rights are prohibited and if you
have an exclusive lock write log is then
prevents both read and and write and
databases need to have those look not
just on row level but on table or schema
level because that's what they need to
apply the lock when you for example to a
DDL when you change the structure of the
table now a little bit of what what's
two phase locking two phase locking it's
very easy to comprehend because just as
I said if you if you worked with Java it
will come
does the same things every read acquires
a shared vlog every write acquires an
exclusive log typically you just
acquired a lottery in a transaction
that's an acquisition phase and then you
release them all at once during the
point or during after the commitment or
after the rollback so it's not very
difficult to figure out how how this two
phase locking concurrency control system
works what the problem is Aidid is that
it was used like by every vendor
previously like in the 80s but a young
it is a problem the locking always has a
cost you know and that cost is key when
a property you know this law arm does
law but actually I'm just like just a
generalization in reality you know a
much better approximation of what
happens in terms like concurrency
locking and it's given by this this
theorem called Universal scalability law
which tells you that if you increase
concurrency you will going through
you're going to be hit right by the
locking and by the by contention so it
will it will affect the way you scale
and up to a point where for example if
you increase the concurrency and you do
and you don't do anything you don't
increase or reduce response time
actually it will get even worse so yeah
so there's a cost locking so the
researcher had to do something to change
because if you take a look back in time
when two phase locking was used this is
from 1980 1981 the largest system in the
world was using one hundred active
transaction nowadays even some small
internet internet application you
actually deliver a better scruple than
that so of course you you cannot use it
on any system and provides realizability
via two-phase locking so researchers had
come to new new ways to study new ways
how we can how can deliver how can
prevent anomalies but then do do is to
do something else so so basically MVCC
tries to to do it has this basic concept
so that readers would not block writers
and writers not block readers only the
only thing that blocks is right to right
so let's see how it works actually MVCC
it's not standardized two-phase locking
is pretty much the same no matter where
what database you are going to use with
mvcc's different is not a standard you
can each database implements it one way
or the other
I chose both rescue L in this in this
presentation because it's actually very
very you can even run the queries and
you will see how it worked so every
every row in Postgres SQL you don't have
that undo log like in Oracle or MySQL
you have actually everything in it is
actually in the row itself if you do a
select all and you also select those two
additional columns like x-min and x-max
you will see that every row has two
additional actually has more than that
but those are very important to our
network to this talk so the x mean is
going to use during inserts and xmax is
going to be used during delete so
exiting what does it tell for instance
in this particular case alex is going to
insert one row so when it inserts the X
min will take the transaction ID of the
transaction of ID of the user that
inserted that that particular row and
only after the user who means other
transaction will be able to comparing if
their transaction ID is higher the NICS
mean they will be able to read it
otherwise they will not be able to read
it that's why the first select of both
price the commit could only return an
empty result set well after Alice has
committed it reached arataura it
returned on the right value delete
delete' is actually similar instead of
using X min you're going to a six-month
so this time Bob is trying to delete it
is deleting the row so it he writes the
x max value until he commits are still
is she's able to to read the row because
they're always not addicted the row is
still there it only has one column that
was set and because her transaction
because that row was not committed the x
max is not taken into consideration only
after both commits that the IDS
Lexmark is going to take into
consideration and then all is not going
to see there the row but there is still
there is and it will be removed by
vacuum which is actually like a garbage
collector it works exactly the same so
you create objects which are then like
the reference for them Rd allocated they
are not no longer in use so that's why
the vacuum needs to run so that it
clears up those all the entries and it
also does something else important it
also allows you to make sure that the
transaction ID which is only on 32-bit
will not we not overflow okay so now the
update and the update is easy to
understand if you understand insert or
delete because the update actually does
I mean uh delete and then in an insect
so actually you will have that slice
multi version because in the same time
you can run multi versions of the same
rope so when you do an update you
actually set the xmax so that the role
the previous version is considered
deleted and then you create a new one
with annex main and you mark your
transaction ID there so before commit
before box commit Alice is going to
still see the old value but then after
vodka means this is going to see the new
value so these two properties are going
to be used to define what you are able
to see based on on your isolation level
okay so now based based on this we know
that there are two we can we know that
there are two types of pretty cleo scope
you go two types of isolation that I can
I can come out out of MVCC because if I
if this snapshot that I create because
I'm working with snapshot because I have
multiple version so I can to your
transaction I can define what your
visibility in time do you see the row as
if it were before your query has started
or do you see the role like it was when
your transaction has started so if you
provide the first query level scope
visibility you will get just a read
committed isolation level
that's typically the same like in two
phase locking but if you provided as a
transactional level you will not get
reliability you will get one isolation
level which is called snapshot isolation
which is weaker than I weaker than
sterilized ability but it's still
stronger than read committed and somehow
it's hard to compare it to repeatable
read because it's in between in between
so it provides some annoy prevention
more than no more than a repeatable read
but then also some of them are not uh
not quote as we will see next so I want
thing to to keep that in mind especially
when using it with MVCC but also with
two-phase locking is that in OLTP you
have to watch out for all long-running
transaction because if you're using a
wrong long-running transaction then the
problem is going to be like that you
will delay the vacuum because the cycle
will not be able to clear those
versioning if you run one transaction
for thirty minutes you will have your
three of version is going to grow higher
and higher to consume more memory is
going to be much more difficult to
reconstruct data based on that so yeah
you know LTP system you always have to
watch for long-running transaction
because those are bad from for from a
performance perspective okay so now now
back to phenomena by basically before I
even I didn't mention
too much of our isolation leverage
because what's really important to
understand it's not the name of the
isolation level it's what what level of
protection what what conflicts does it
prevent and you know when you read the
SQL standard the manual is pretty simple
in reality the reality is much more
complicated than that because if you
read the manual you you'll learn about
dirty read about non repeatable read and
phantom read and it doesn't look very
very complicated in reality it's much
more complicated we already talked about
those rewrite but you can have reached Q
we can have writes Q you can have lost
updates and some of them even phantom
read sometimes it's a matter of
perspective if it's prevented or not
so let's just let's just talk about them
so dirty Reid doesn't it happens if you
are allowed to read a row
that has not been committed yet usually
this is pretty difficult to work with
such an environment the only reason why
would you want to have that is for
example if you're running some you know
some analytics quiz which you don't mind
too much about consistency because
you're providing some statistical
results or stuff like that and there are
some systems that still use SQL Server
and don't have these requirements so
they have to use they have to allow this
because otherwise they will not scale
properly another conflict that you could
that could happen
it's the normally Peter was read in this
particular conflict it goes like this
both reads the row and then Alice counts
modifies the row she comments it and
then Bob's read it again and she n he
sees different values so now that's
going to be a problem because it's
changed make maybe I did some changes
based on I I assumed that once I read it
it should stay the same until I commuted
the transactions there so there are all
sorts of things that can go wrong based
on this depending on your Dell given
that you want to implement now the
Phantom rate is actually a
generalization of normal P double rate
instead of having one row we actually
have a range so this time is practically
almost the same instead of reading one
really read a range using a predicate
and then Alice Kahn she inserts a new
row which conflict with the same
predicate so if you reread if you run
the query again you will see four rows
it's the same it's the same thing that
we discussed we discussed before but
then there are no that those are in SQL
92 okay so you read the documentation
get familiar with them and you think
that's the only thing that can go wrong
in your application but then that's not
enough
in reality you have more like for
example reads Q which some developers
are not really familiar with this
because is it it's not it's not so
useful you don't have you don't get to
see too much documentation and written
about it is this phenomena that can
happen so
in this particular conflict you have two
rows which need to be synchronized like
for exam I opposed and then I also want
to make sure that both details which is
a one-to-one relationship between that
marks the the person who who did the
change on the post so so now boy Alice
is trying to read those two entries but
you know because transaction happen
concurrently they are like threads it
can be interrupted and someone can just
Nick in and do do do his work get a shot
and for the CPU and for the memory so
Alice start she reads the post but then
Bob comes extremely fast he writes the
post and he writes of course the
positives and he commits and then that
is resumed she reads the post-it and now
she thinks that the post which develops
transaction was written by Bob but
that's not correct because that
particular combination rod has never
existed in this form so yeah that's a
conflict depending on your application
that can be a problem or not it's not a
universal Universal guys that you should
prevent it or not so I also has writes Q
that's a little bit more complicated
because now both Alice and Bob are
trying to are reading this
- but then Bob does something like that
he figured out that okay so the positive
is still Bob I'm not going to update
that so I'm just going to update the
post and marking is with my change and
Alice look at this both said okay
develops posix transaction is really
good I see that Bob updated it but then
I want to be marked that I'm the last
person who wanted this well to be like
that so I'm going to just update the
post details and now we got into into a
situation where the values to values are
not relevant and not consistent anymore
and our constraint has just brick broke
and the last update I already talked
about it about during the conflict
during that example it's very typical
everything lasts because it can happen
even if you have sterilized ability and
if you're using web application with
long with the optional transactions so
here is just I told you you read a row
and then you decide about journal entry
the row and then they decide to change
it and
one transaction managed to change it and
once you update it as well
you will overwrite one change without
being aware that that change happen
since you lost read the row okay so now
depending on the two-phase local nvcc
those anomalies are going to be
prevented differently like for example
when you take locks as I told you avoid
conflicts so in this particular cases is
Alice has has issued like four examiner
in my in some department because she
wants to get them to to tuition updates
to raise the salaries but she doesn't
want to to go above a budget so now she
read that and then in between the
harsher Bob from Russia tries to insert
a value that would change so it
increases the sum of salaries so if
you're the two-phase locking because
because Alice has acquired a lock then
she both will be blocked until Alice
releases the lock so that's how it works
in two-phase locking on nvcc it was
differently it allows the conflict to a
court you allows Bob to do the change
but then when Alice tries to commit it
will see that the assumptions have
changed so you have to rollback
so that's what is called conflict
detection they get an exception but then
got this you get this situation is this
a phantom read or not if you read the
documentation if you go by the standard
by by the book it will say that the this
is not this is not a phantom right
because you still provide a snapshot key
if I can rerun the query I can still
read from the snapshot but here I can
still insert the value and then when
update occurs at least this is imported
and you can meet both of transaction can
commit and some would argue that this is
not a functionally it is okay because I
can real reorganize if I organize
another way it will still be the same
but if you think about it if it if it
worked on two-phase locking and it
doesn't work in MVCC you have to think
why is not working then maybe the maybe
we don't provide when we only provide
hospital tection so yes it's just a
matter of perspective whether you can
whether you decide if this is a conflict
for you or not
okay so again back to lexical standards
pretty simple we only have like four
isolation levels and only three three
phenomenas that could happen so for
example in read uncommitted all of them
can happen in read committed only thirty
feet is not allowed and in repeatable
read only phantom rate is allowed and in
serviceable not know Spellman also could
happen but reality is different because
reality looks something like this
in Oracle you have read committed and
realizable you only have two them but if
you look here the read community is
pretty much the same only dirty read is
is prevent lis but survivable if you
take a locally it doesn't prevent writes
Q because on Oracle you don't have
survived you have some extra type
solution which is so which is a good
isolation which is not realizable you
have to do something more above that in
SQL Server is well you have a lot o you
have a lot of isolation levels because
the first for one according by the
standard those are using two-phase
locking and the other ones are using
MVCC and exactly the synapse and
isolation we have the proper name is the
same like in Oregon and then in Postgres
SQL the repeatable read is actually
symmetric isolation like in Oracle but
then sterilizable is sterilizable
snapshot isolation and it provides put
prevention detection for all of that but
then the Phantom read is still with a
question mark and in MySQL MySQL is is
still MVCC but it's also in civilize
abilities is locking so then it managed
to provide a survivable isolation level
so it shows that compared to Oracle
although it uses MVCC you can still
prevent the write skew and much more
than that against tim prevent many font
on the font the last font on bro phantom
read are normally that I prevent it that
I showed you previously
it's prevented in MySQL because it takes
locks on row level gaps so acid is
important because as you see there are
many conflict that could happen so
depending on your application that could
be either bad or probably you don't have
to worry about that so it's a matter of
context it you have to you have to read
based on your requirement so yeah as it
is is a good thing that is not
sufficient because we don't work like we
used toward like 30 years ago when we
used a system which was more like a
two-tier you just got to collect a
connection to the database which was
only your's
you did the reeds and the rights and at
the end you just close it but we don't
work like that anymore we use multi
request logical transactions you read
you go to a database transaction there's
a user thing time you decide what to
change and then you do the right but
that happens happens in other
transaction so can sterilizable
prevent a lost object in this situation
let's see so in this particular case
we're reading a product and then the
batch job decides that we don't have any
quantity we don't have any product in
the warehouse but I I see that there's
still five of them so I can read it so I
want to buy it I go and buy it and I
update the quantity and now it's minus
one so yes you're going to be survivable
but you are because you have two
different transactions it will not help
you
you still get a lost update but then I
realized that okay so yes that happened
that happened because I lost my state so
I I had to reread oh let's keep this
state so yeah this way well I'm going
from stateless to stateful it doesn't
have to be on the fabric you can also be
on the client but I need to preserve
some state between my two physical
transactions to the database so here I
read the product the bed calm and then
he updates the quantity and when I try
to buy it I just say okay so I I use my
last last value that I that I read and
and now I'll decide that I just decrease
the quantity and update and that would
happen if you're using for example JPI
that would be typical and you just saved
the object you know in the current
running session by HTTP session and you
just update it like that if you will
just overwrite the bridge the the batch
job update and then you'll still
use a different quantity and that that's
not where the scaler conflict is still
allows tablets so you have to use
something else you need more than that
so not only happy that is good but you
need if you're using application level
transaction then you need application
level
concretely control as well so one of the
best of them is this optimistic locking
which is a version column which usually
the best one of them just a number and
auto monotonic implemented number so
when you have that you will use that in
your we're close to say okay I want to
update if the version of the row is the
one that I read before if it's not you
will get an update count of zero and you
will get any section and then you will
understand that you you've been
prevented from doing that which makes a
lot of sense so if we go back to our use
case here we we preserve the state
between those two particular
transactions and now we can see that
also have this version and the batch is
going to update it it's going to work of
course because the version hasn't
changed but now when I want to buy the
product you know I decrease the quantity
and the version now that version that I
see is one and try to update and assume
that the version is going to be one but
it's not because it's two and I won't do
the update because my object cannot
happen because it doesn't the where
Clause prevents you from doing this
update so you will get an optimistic
locking exception so you prevent the
lost update using an application-level
conquest you control so that's why you
have you go beyond this because in
reality you can have more than that okay
but they're not all why if I have a
version and I have multiple columns
which are not overlapping like this one
title views and like they don't have
anything one in common with the other
and I have three years users Alice Bob
and Kelly and Carol and they all of them
read the value and then they want to
update it and then of course they want
to increment the version but only the
first one will succeed and the other one
will not be those are not over
overlapping so in this particular case
according to my acquirement those are
just false positives
so yeah I don't like that it prevents me
in it it prevents loss some date but
what about what can I do about that
because the most obvious change I just
remove the version is not working
properly but that's bad so you need to
do something different you can you can
design your schema not just based on
read the consideration based on right
considerations now I'm splitting my
tables I have only the post post views
and I have a post likes and then I use
composition and use references from post
to post views and both legs but but then
I have three tables and now the updates
can work fine because each update goes
to a separate table so now I don't have
a conflict I can only have a conflict
when I modify this thing the same the
same table like modifying the same the
same title so now I catch the last
update on a poor attribute basis but
they're not all you know you don't
always have to modify your schema
sometimes you can do it with a for
example hibernate provides this version
less optimistic locking you don't even
define a version it will take into
consideration the fields that you
changed so for example if I change the
title I use the previous version of the
title attribute the same goes relax and
for use and that works even if you don't
change your schema so it might be very
advantageous to retreat to use that in
fact hibernate provides a lot of
application level Conquest the control
mechanism fortunately that their
lesser-known and they're very very nice
is what if it's very difficult to
implement it yourself it doesn't even
make sense to do to do them but yet if
you have them just take advantage of
them because look here you can have a
lot of options like for example you can
use you can still use pessimistic read
or write to a choir or exclusive lock
but you have much more than that
you can have like optimistic force
increment or pessimistic force increment
to coordinate multiple versions to bump
up the version of an entity that you
didn't you didn't even change so you can
do you can prevent a lot of conflict
this way with an application-level
conquest control because
otherwise if an asset cannot cannot
provide you this this one T so for
example the last example I'm going to
give you is this where we want to
implement implement and invade a system
ax which works like like subversion
add-ons if you used subversion because
nowadays get is way more popular but in
subversion you have like a repository
and you do a commit which has some
changes and when you commit the version
of the repository is going to be
incremented every commute is going to
increment the version so you cannot it
is always must monotonically increase
otherwise I would get a conflict so then
I will do something like that
I try to emulate two transactions here
the first one will read a repository and
will say read it with the optimistic
force increment lock mode and then both
comes in and does something like that
okay I'm going to read it also in the
optimistic first increment because
that's our protocol that's what we do so
I'm going to issue a commit and I'm
going to apply some changes and I'm
going to persist them and my transaction
is going to be committed and then all
this transaction is going to is going to
resume so now watch does should try
solution you commit and do some other
changes and do the comment so what will
happen it will happen something like
that so both they both start from the
version 0 of the repository when they
read it nothing at SQL level happens
when Bob comes he does the coming
humours the commit changes and hit
update he doesn't issue the update that
happens by hibernate as that the JPA
provider that does that it does that
because you told he told hybrid you want
to use optimistic force increment you
want to force the version of the
repository to be increased at the end of
your transaction so increase the version
with one another repository version is
one which is exactly how subversion work
but now Alice tries to do the same so
she resumes
she tries to insert a new commit and
with different changes and at the end of
her transaction she also wants to update
the repository but
to use the version that you read which
is zero it's not one so the update will
not work and so she will get an
exception because yeah and and because
you get an exception around an exception
you force hibernate to rollback so the
transaction is going to be rolled back
that's exactly how it works it force you
to update like to check out and then to
to to do the conflict if they are
conflict and to retry again so in a way
transactions words are you can think
about this sometimes like for example in
version control
like for example version control systems
work ok so that's pretty much it now we
have some time to to answer to answer
your questions please I'll be more than
or Gumtree looking that's not going to
work very well and it is pretty good
well yes in the distributed systems you
cannot get distributed logs
that's how distributed databases work
it's very very difficult to provide it
that was you know no SQL databases not
like they didn't want you to provide it
because it's it's there are very good
it's very good to provide prevention for
this anomalous but you realize that in a
distributed system you have the network
which is unreliable
he has unreliable latencies you cannot
use distributed locks because they don't
scale so anything something else so you
have to two possibilities either you
don't use lock at all so you don't care
about that so some systems provide only
limited limited consistency model or you
can if you if you can control the
hardware if you can control everything
about the infrastructure like Google
does spanner you can have a globally
distributed database that's still a seed
and it doesn't use it doesn't use locks
it is something else which is actually
even more awesome it is conserve it uses
like pox offs or raft you know so that
you have you have to deciding that you
have to have a quarrel to decide if a
read or a write happened because some
nodes will see low
bell ringing the majority of the node to
define what what should be the latest
state of a row or what should when I
read I read from multiple notes and the
quorum must decide which one tells the
truth in which i don'tá-- so you yeah
it's still optimistic locking because
you allow things to go wrong but then
you go above them and you provide a
protocol which tries to solve it tries
to hide the fact that you don't use
locking to prevent it so it tries to
resolve colonies like a conflict
resolving mekinese like error detection
like how for example error detection
codes work something like that if you
think about it
yet if you really an important property
for in-memory database availability or
durability durability for how does the
vault Devidas source the durability it
has to yeah it also the involved it it's
it's not as difficult because they use
these voluminous distributed nodes so
then you don't write to a single node
before the transaction commit it must
write multiple node and it's also write
something to the disk I'm not I cannot
see for sure how it does but I know that
they use a combination of the disk also
they uses some flashing but not but
mostly they rely on the fact that use
multiple nodes so at least if you have
two nodes at you to write at least you
have at least one replica which still
has a Val
so if these node crashes you still can
connect to the other one which
automatically becomes the new master
because you can only have one the other
one are just for reliability you know
but you keep still read from them but
only one can write because only one
transaction can be written at a time but
those edges they are not typical use
cases and the database the Foley
business it's a very good database which
is not a general-purpose one like a
relational that you cannot take all the
B and solve and apply it to all projects
that that you want usually the in-memory
wants traded it provide very low
response times like for example when
you're doing gaming when you're doing
high frequency trading or stuff like
that you know so they they work in a
niche you know like like you know
in-memory data grids are very very close
to what DB but they don't provide SQL or
I see those store procedures which are
implemented in Java so they it has many
advantages over a simple key value
memory data grid you know
okay so thank you for coming to my talk
I hope you liked it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>