<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Patterns and Antipatterns in Docker Image Lifecycle by Jonathan Roquelaure | Coder Coacher - Coaching Coders</title><meta content="Patterns and Antipatterns in Docker Image Lifecycle by Jonathan Roquelaure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Patterns and Antipatterns in Docker Image Lifecycle by Jonathan Roquelaure</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/clD7H7_-ueE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone welcome to my talk I didn't
expect to have so much people so that's
cool first of all I would like to
introduce myself I'm Jonathan wroclaw
and I'm a social engineer at j frog so
as a social engineer I used to work with
our customer to help them to improve the
architecture to attempt to use our
product to fit the needs for our use
case and I'm also working a lot with the
community going on conference he is
making some terms webinars etc what's
frog so frogs are people walking in
Israel in the US or in France and it's
also a tiny animal which has a cool quad
to stick to always look forward it's
never goes back rub and so we are also
accompany a frog and we have four
projects first one is at factory maybe
you always you already know this product
we have also been to a our distribution
platform and Mission Control and also
the latest one x-ray from energies of
component for security license check etc
basically we're a company focused on
binary is to take care about your binary
Zeb you to distribute them to manage
them etc etc I would like to start with
a quick question so since about two
years we're all talking about dhoka we
heard a lot about dhoka dhoka dhoka
dhoka everywhere in every conferences
and today I'm here to talk about it so
first question would like I would like
to know and if you can raise your hand
it's you know we were already out about
dokie Oh everybody okay and who did the
tutorial doko ok whoo did some perks
playing with docker playing with dr.
Campos warm
doing some admin stuff okay steer alerts
and now who's running doe current
prediction well that's some people but
not so much and that's because that's
all thing we are lots with our customers
many customers are asking for dokie
registry support in at factories are
asking for how to manage to occur what
has the best practice etc etc are
working with docker since one year to
year but there are not a lot i'm doing
do current prediction which are running
the current prediction and what they
told us is they like the curse that like
the concepts they love it but they don't
trust it enough to put it in production
and so we try to figure out to help them
to trust to have more confidence in
dokur and so today's a torque will will
be about out you can be more trustee
with joker and i will use artifactory to
show you some demo and some concepts but
the main ID is not about that factory is
about managing binaries in the proper
way and how to manage dokie image as you
manage any binaries by building proper
pipeline so while we are good with daca
and why artifact we can be a good
project for dhaka its dhaka is no more
than another kind of binary at duke
energy's just binaries and we have
decade of experience with binary this is
our job also we did the first image
registry even before there was a doctor
registry and we provided the first
private doctor registry even before the
official do k 1 also we are doing
promotion pipelines for dokie as we are
doing promotion pipeline for all the all
other kind of technology so yeah another
interesting thing is about why you can
consider using at factory with docker is
that nobody is using docker and nothing
else dhokha is a container technology
and a container is to ship something you
will not use docker without anything
else you will use it to ship some java
application to build a micro services to
do things with other technology and so
it makes sense to have a solution for
all this technology so the goal for
today will be to develop and to
understand how you can create a
lifestyle kill for the containers and
now we have the goal we'll start with a
quick question do we add an existing
pattern for life cycle of batteries and
do we need to adapt it so I'll let you
think about this do you think that
docker is more like a doctor image do
continuous mode like a Debian package or
Java Wow archive so just as a quick
reminder a Debian package the net with a
decline of dependencies to resolve where
a debian java archive will be more a nap
with all the dependencies rapid wrapped
up in the binary and in fact this is
dhoka dhoka comes with everything all
the dependencies inside the image so
that's a good news because we are doing
pipelines forever while file since yes
so that's a good thing and we can so
start to look at how we are doing it for
java there is one important concept when
you are doing back line it's the concept
of quality gates and visibility so i
don't know if you're familiar with this
diagram but we will like it at j frog so
the idea is every step every stage
staging area is is related from the
other one by equator gates equator gates
is simply some condition you want to be
able to resolve
and to fit with to move to the next step
so the first quality gate is simply
should I commit my god what are the
condition I have to to fill to be able
to commit my god in general is running
the unit tests on my machine making some
some free test on it extras and I come
in my goods and I go to the stage the
development under non-central which she
will be like a junkie into CI server
whatever will be doing some other tests
some you need test but some code
coverage etc and you promotes your
binaries like this from stage to change
stage passing through all the quality
gates also a classic see icd pipeline
for software something like this you
have some developers using a build tool
with a dependency manager and they're
using it to resolve dependencies that go
through a repository manager so because
now I think everybody understand why you
need a repository manager and this repo
manager will go to the internet to get
so external dependencies it will be
bringing to the developer and after
testing on its own machine we commit a
change change will be taken by the CI
server like check-ins for example which
will run the same build with the same
build build tools and dependency manager
and so it will resolve dependencies
through the repository and at the end it
will publish into the repository some
artifacts generated artifacts and all
the building formation relative to the
build and from that point you can start
to use party tools like selenium jmeter
or a QA team number to get your artifact
from the repository test them qualify
them and contribute with metadata and
promote them to through the different
stages
and once you are at the last stage you
can choose to distribute your artifacts
with a solution so directly to your
production server without restricting
tool or you can directly distribute them
with for example bean tree which is our
distribution platform okay so that was
just for the main concepts now let's go
back to dokur and the first thing do you
know why this guy is a pea it's just run
his first dokie build and I don't know
for you but when I make my first doctor
will I was so amazed I was always so
simple I don't need any more ops guy I
can run my server my several apps etc
it's so easy and so when you start to
play with your first dhoka file to
create some things with with it you feel
like yeah you want to do some do kabil
everywhere you want to do it in every
environment it's so easy its fastest
cheap that's wonderful and so first
thing when you think about the diagram
before with the pipeline you can have
the temptation to to do something like
this say okay i will run my doctor bill
on my machine and my drinking server
will just take the Duca file and want to
talk a bit and then on the test
environment i will do it again etc so
you will just run docker will in each
environment it's fast cheap and this is
not always the way to go fast and cheap
wheels are not always the way to go and
that's why in fact if you look at a
docker file and you start for getting
your base image so it's an ubuntu when
you say from ubuntu you will have the
latest image is what it means and then
you're getting the latest beta version
with the apt get latest on GS etc all
are the latest version so on each
environment you will have the latest
version and for the base image for
example if there is a new version of
ubuntu
between the time you move from your
prediction mere endowment to your
production environments you won't have
the same docker image at the end on the
200 months which is not good so to fix
it you can specify which image ever
going to you want for example the
particular tag of the base image you
want to use so this can be an ID it
looks better but sometimes what happen
is there are some security patch on base
image and the gas the maintainer of the
image will upgrade the ubuntu 14 dot 0 4
and we will not notice it so you still
will have different images in your
different environments so you can fix it
again by using the ash of the base image
so the shadow of the base image and now
you sure that your ducket i will build
always with the same base E major foo
bhuntu that's good but i'll reduce our
by turns yep it'll get in style etc how
you can control that you're sure in each
and element you will have the same
version and you are what you are testing
the image you are testing is the same
you will put in prediction because this
is one of the reason why you don't trust
dhokha when you are doing this naive
approach it's if you are doing the cabin
on each environment at the end the
document won't be the same or at least
you will not be sure that you will have
the same docker image and so you will
test something which isn't which will
not be the one you particular and this
is really important what we want to have
is something like this a build at the
beginning of your belt line and to keep
the same binary and it's not only for
dhaka but keep the same binary for each
environment and improve this binary new
staging environment improve it in use
testing
in your prediction your environment to
be sure that what girls in prediction is
what you taste it okay another
interesting thing by this interesting
guy is the concept of immutable and
stable binaries so I don't know if
you're familiar with this concept I
guess yes but let's remind it so a
traditional server pattern is something
like this so you package a server image
you provision you seven stands and at
the moment you need to apply some
configuration changes and then you have
another changes to do and so you up like
configuration changes and again and
again and again and at the time it start
to be really ugly doesn't work really
well and so you have to block to break
everything to package a new server image
and start again and you eat rate like
that the concept of immutable server
pattern is different the idea is that
you package your server image provision
your server and if you need to do some
changes you will not apply changes on
your actual server you will start again
with a new package for your server and
provision again this is there is a real
great article from Martin for about this
so yeah so photochem its we want to
apply this so the idea is not to only
use container but to have the same
container of the world way and to be
able to eat to recreate the world's ever
really easily and fast so to have a
really fast and efficient pipeline so
for today what I will do is I will build
a simple application continued rise
application so just assign not I change
a bit as a demo for today you're like
yesterday to apply what
said so I decided the original talk was
about a war file inside the Duke image
containing the Tomcats and jeddah I
change it to make John Atwell as you say
then with the spring boot application so
we will use a base image with engine
eeks based on Jesse the latest official
image of ingenix will add some Jettas
this will be our base framework image
and then we will add inside it the
result of a gradual build so the spring
good application for the web services
and an NPM build for the UI so front-end
and back-end application inside it so
what we want to do some requirements
about this pipeline first the
application the Jaffa so the UI and the
backend must be tested and I've final
deployment in container we want to test
them in container and we want to deploy
them in a container then we want to know
that the framework we will use based
framework image is good but we want to
be able to address vulnerability so we
want to be able to upgrade it and also
of course we want to have is rated a
great updating process we want to be
able to update the base framework layer
or we want to be able to update the
application independently so to do that
we will have several pipelines the first
one will be the Gradle pipeline which
will build the web service applications
jar file then we will have of course the
UI pipeline I didn't rotate but will
have the framework build so the fray
mode wheel will take the base image the
engineer days image will add jetta and
run some security test and at the end we
will have an integration build so it
will take the base frame walk image it
we'll add it the Gradle build and the
result of the NPM pipeline so let's have
a look to the different pipeline so for
the Gradle build it's something really
simple I think we all doing this all the
time so we'll build the Gradle projects
run some unit tests deploy it to
artifactory and after testing it's in
containers we will promote it to release
if we have a look to repositories so
this is where artifact will come so we
will deploy at the end of the the build
and running unit test will deploy to a
development repository so the lips Dev
local which will be part of a virtual
repo leave dev and the promotion at the
end of this pipeline will be a simple
operation of moving the artifact
generated artifacts from the dev
repository to a release repository for
NPM it will be quite the same just the
name of the repo will will change but
it's exactly the same concept and now
for the framework build so for Tokyo
what we will do the first step will be
to pull the latest engine X image to add
in my to run the document by adding my
java inside my image and after that i
will test it by adding the latest stable
version of my griddle breathe and my npn
wield so the latest stable version of my
application inside my container to be
sure it works with the latest stable
version of my application and if it
works I will deploy this base image it's
a framework image to artifactory and
promoted to release so in terms of a
free PO I will start with the first step
to pull engine eeks run the document
and if it's it's a build well I will
directly deploy it to a dev repo because
I want to keep the trace of all my bills
all my development bills and then I will
take this image inject the latest stable
version of my griddle and NPM bills so i
will get them from the liberal is so the
artifacts that has been promoted during
the first pipelines the application
pipeline and i will run my tests etc so
let's start with the beginning so first
step i will build my projects so and i
will do it with a doctor for windows
because i love challenging and risk so I
will do all my demo on windows so first
thing I will show you else it will be
better okay so first thing I will enter
in my nokia framework workspace and here
is my dacha file you can see it's
something really simple it's simply from
the engine eclat 'used I get it from
artifactory and i'm adding your java and
restring java in the past and at the end
i'm just starting engine X nothing to to
complicate and so I'm going to build
this so first half I going to build this
image okay and I building it with the
tag one if it works please God of demo
mm-hmm that's what happened with when
you want to play with windows
okay yeah it's what I'm doing looks good
mmm okay and nothing go okay i will
start with a new portion okay what space
to curl ah cool thank you it was just
time to wake up the machine so now
copying the thigh and while it's uh it's
our building my image I will go back to
the slide just to explain the next step
so after building my image what I want
to do is to test it so if I want to do
it properly of course this one beam any
manual thing it will be done in my CI
server like Jenkins so the first job
would be to build the image and then it
will trigger another job which will be
the framework test so this is a job i
will do now so the idea is to take the
building age to inject the lasers table
application and to run or standard tests
so to do that I my image is built and so
what I'm gonna do first is I will push
this image to my docket of local so I'm
pushing to my to my artifactory okay and
in the same time i will show you a bit
of the structure of my repositories in
that factory so i will connect as an end
mean okay and
if I filter on docker yeah you will see
I have two virtual repositories in that
factory a dhaka dev virtual and occupied
virtual now I'm pushing to the dough
cadet virtual which contain several
repositories a remote repo which going
to do carell to be able to resolve my
basic management engine eeks base image
dr. dev local and dock your protocol in
case I want to resolve some released
images what is important is there is a
default deployment repository to allow
me to deploy to this virtual repository
and at the end the image will be
deployed inside my dev my development
local repository the second virtual
repository for dhoka dhoka prod it
contains to repo because I choose to
have a strong intuition between my
prediction repository for the base image
framework and the repository for the
Duca application the final image but
what is important on this one there is
no default repository for deployment
which means that I won't be able to push
an image to my doc yup road repository
because I don't want anybody to be able
to directly push something something in
my release repository I want to do that
through a promotion by copying from
development to production so so the push
has been done and I will just do another
quick thing here I will yeah so I will
first remove the curl alyas in poor
shell okay and run a curl command to
retag my my poker image in that factory
in fact the ID year
is just to to have a latest so i will
show you the retag file I'm just saying
to artifactory okay I have a Duggar
framework image with the tag one in art
factory now I want to make a copy and to
have a new tag latest on this image to
be able to consume it by running a ducal
pool dhaka framework latest and now i'm
gonna do a really quick test just to be
sure my image is walking and so i will
run this image so as you can see i'm
pulling the Duca framework latest so i
will consume it from art factory okay
and i will just make sure that if I as
I'm exposing the port 8000 if I go to my
local hosts on port 8000 ok welcome to
Angelique's it's ok it's the only
purpose of this base image there is also
java on its so this will be the
framework test job so let's go back now
it's time for the fragment test so i
will get my application and build a test
image to check if everything is working
fine to do that i will go back to my
poor shell and get my container ok and
now i will enter in the framework test
folder and build a new image so this
image will contain is best based on my
base image so my friend wat my my
framework dr. framework your latest
image and i'm copying inside it's my jar
file and all my UI projects all the
forms an application so it's now don't
and after that it will I will run the
container and make sure that everything
goes fine all together and if everything
worked fine together after testing I
will finally have the final step for
this pipeline so I will be able to
promote from my daughter of dev local to
Tokyo protocol it's a bit longer ok
folks fine um just to show you in that
factory so if i go back in my artifact
yeah into tokyo dev local just to make
sure yeah I have well my duka framework
with the tag one and the latest one so
that's good ok build is almost done so
it's adding the job oh yeah also I'm
adding some configuration for engineers
to reroute request to the web service
and now i have my image my test image
and i will run it so Ronnie it exposing
ports so i can see ingenix spring
starting and now if i go to my local
host app index where it is this is my
application it's a really simple
application showing our good friends
well that's good so now now i tested my
stable application with my with my
daughter framework and so what I want to
do is to promote my dakka framework to
to release so to do that I will use a
simple call to the rest api to the
artifact to rest api so let's do it
and okay provide my dice world and
that's it so now if i go back to add
factory and i go in dokur pro local to
yeah there it is I have my nokia tag one
and my tag latest i promoted both images
because i want to keep both in my
prediction our repository so that's cool
for the first occupied line that them
now the application delivery so the idea
it will be to consume the released
dhokha framework the dev griddle and dev
and PM bills so not the realist one but
the development one to manage them
together in a new doctor image to test
them and if everything is fine i will
promote everything my tokyo mhm my
griddle and my npn wheel to the final
release and if and after that i can do a
final step if i want to promote
everything to a distribution repository
which is a concept in that factory our
link to bin tray so by doing that you
will directly distribute your dokie i
made your application with vintry hope
you can start to consume them with some
orchestrating tools to deploy them to
prediction so in terms of free posit or
ease so this is i would work so i will
consume my just build dhaka framework
image from Stoker prod repository so it
will come from my dough Kapoor local to
I will consume my development
application from the lib zebra
call an NPM dev local i will build
everything put them in a docker
development repository take them from
from air build an image with build an
image test it and then if everything
goes fine i will promote everything so i
will in the dockyard of local i will
promote my doctor appt image to do co-op
or local so for the first one let's do
it the first job so okay first of all I
have to move out of this I will go in my
book CD and okay hot so here is my dacha
file nothing really complicate and so I
will run the bridge the first step first
step is to build a new image of my
doctor appt and of course it will take
some time to start but while it's
starting i will show you so after the
beam it's exactly the same concept
that's that's the first pipeline so
after the billowy push it to my
developer and get them from the repo
test it and promote it
hope can still not so slow hmm thank you
so it's starting it should be quite fast
because everything is already on my
local machine so that's cool and now I
can push my document before to test it
so I push it to my dev repository and
after that it's waiting oh okay
Sweeting after I will do the retag also
as I did before just to have the latest
image available so is it okay okay okay
still pushing takes a bit of time and so
I will befall to test my application I
will just retagit my image from 12
latest as I did from the Duca framework
and after I will jump to the test
application job so as I explained before
and quite slow so yeah
hmm
it should start to be here so in my
develop call if i refresh it I should
have so yeah it still in upload process
can see my layers coming into my hat
factory okay
what I'm gonna do in fact is simply
running the tests while it's pushing say
some time because on I think you got the
concept ok so let's go to my doctor aw
space Oh done so it was not so fast so
far so good walk so now I'm retaking
retaking my nokia image inside my devil
repository so it's the same i did before
ok provide my pass warned I can't open
reduction why ok
you know
we
okay an issue this file ok i will do
without it it's just a retag it's not
mandatory for mania for this and so so
now what I want to do is to test it so I
will not test the latest with latest a
guy will test with the tag one the image
I just push to artifactory and so I'm
just running the container and of course
there is a conflict because I already
have a framework test so i will remove
this container ok and I'm running it so
starting engine eeks in fact it's
exactly the same thing I did the only
difference is here i'm using the
development version of my application in
this case they are the same did the
latest development version of my
application my front end and my back end
are the same as my latest stable version
my release version because i didn't run
it now but this is the main 90 the main
difference between the first test i made
and this one is to use the development
version and so as my test will be the
same simply go to my application and
check if still running it is so i'm
satisfied with this i will promote my my
image to the production repository
okay and after that I will be ok so now
I should promote everything to release
and distribute my stuff but there is a
the main idea so what's doing next so
just another view of the world wide line
so we have once again there is no the UI
pipeline the front-end pipeline but it's
really similar from the Gradle build so
the idea is to have a different entry
point so if you commit something on the
Gradle project it will trigger a job
your Gradle build so you will compile
the Gradle project runs a unit test
deploy it to us factory and this will
trigger the framework tests pipeline
which is on the bottom in orange so this
will build the framework image with the
latest jar and latest ga.js GS
application run some tests and if
everything goes fine it will promote
everything we are the same if you don't
three point is a security patch for
example the maintainer of the framework
base image a new update on it so it will
trigger the Duca framework pipeline and
it will follow the blue pass so by
building the image deploying it to add
factory promoting testing it's promoting
it to release and building the doc your
app testing gates and promoting
everything one very important thing in
this pipeline is automated tests this is
everything if you don't provide all
automated tests this will not work you
will lose all the control on your
pipeline and you will not have control
on what you are doing so for the g mo i
didn't provide any integrate automated
test because it will took too much light
for me but it's really the key on this
kind of pipeline maybe some of you will
ask if if with such kind of pipeline
with for separated project dependent
which other etc we can reach a point
with some jam and something with where
you pipeline will be blocked in fact it
can some what can happen is for example
if you have a security fix on your base
image that with an on backward
compatibility with your application so
you have a security fix and your
application will not work because of
that so in this case you have just to
think a bit in advance to it and improve
your application changing the things to
be able to to make it compatible
compatible with your new image with
security fix test it with the old
environment because it should work or it
will still work with all environments
and so by testing it you will be able to
release the new application and restarts
the pipeline for the docker framework
image in very very rare rare case to be
honest I never seen that but I heard
about such case you can reach a moment
where your API becomes so insecure that
you have to build an entire new one in
this case you will have to stop all your
pipeline and to to do a manual first
step of building your application and
moving it from there to prediction just
to reinitiate the world pipeline but
this won't happen often for sure and at
the end of the day you will have with
such pipeline you will have something
which will run really smoothly and unto
my teat way with the front end
developers pushing the card into the VCS
and this will upgrade the wall
application in the same time it will be
the same for the back end developers for
the security guys maintaining the Duca
framework image etc so this kids full
automation and if you remind immutable
server pattern we are talking about I
think we did it it's I don't know what
do you think but yeah and the key here
is with the immutable set up a time is
to reach a point where you apps you have
an absolute truth in your software you
know exactly what you are moving two
production you test all the things and
you know that in every stage is under
control every part of your software is
under control and every part of your
software has been tested in the exact
same condition that will be your
prediction at the end so yes with it and
just to finish this talk i will show you
because it was a bit shitty because
i didn't I did only dukale push and pull
command on my machine from my poor shell
but I didn't show you any pipeline in
Jenkins so I don't have the full
pipeline in fact I don't have the
interaction between the different
pipeline but in a I have a Jenkins with
some pipeline integrated with the
artifactory Jenkins plug-in and the gsl
pipeline for Jenkins so for example for
daca framework
yeah can be an example for the pipeline
and I don't know if you will be able to
read it maybe it's too small but you can
see it's really it's something really
easy to do to manage this pipeline so
it's simply calling some object coming
from our plug-in to instantiate the
server to instantiate a building
formation object to be able to have a
full traceability about your build and
to link your bill image to a build
information to maintain the history of
all your artifacts from development to
production then you are you use a simple
duker object which come from our plug-in
and which allow us to have a trance
ability of all used dependencies like
the basin age and also of course the
image you pushed during the build and at
the end you simply publish build info
then you're doing some tests so I have
to do that and the promotion is just
like I did on my machine a simple
declaration on from what report what
people you want to copy your file which
will be if you want to attach some
properties etc and just call the promote
metered and at the end call I'm retaking
my image to be the latest one and of
course i have also the same for you have
a npm pipeline here doing my UI pipeline
and the Gradle pipeline also is here and
dock your app also so yeah and just so
yeah the conclusion really is faster day
this is a sentence we love but
I like to add release faster die but
don't lose control over what you release
that's all for you have any question yes
oh yeah okay that's the question so the
question is why I don't deviate of the
configuration the integration parts some
configuration to like prepare Tam
Sibbald yeah so this is because of the
immutable pattern this is the ID to
knotts reconfigure the things at the end
what you want to have is to have
something that you bill at the beginning
and this is the same exact image that
will move to all the stages so this is
exactly the subject of the Martin fuller
article i have in the link which rate
about this practice to reconfigure other
things in each stage instead of
rebuilding everything already configured
unpacked so that's why another question
Oh
okay so yeah if you have a so yeah this
druzy stopped was pretty simple because
the base image was only used for one
application but now what happen if you
have hundreds of application depending
on your base image and you want to have
security patches on daily basis it will
be exactly the same just you will have
more more pipeline but it's what you
have to understand it's really an
independent process with this kind of
pipeline the the base image upgrade is
is an us to be really independent from
the application of great so with this
kind of pipeline and this is why you
need this kind of pipeline it's because
as a security guy you will be able to do
as much security patches as you want you
will be sure but by the back lines you
automate it back line that it will work
with all the application and if it not
you will have a really quick feedback
also guys maintaining the application
will have a feedback and I will be able
to rewrite our refactor the application
to fit the new security patch welcome
yeah it's uh this is uh exactly the if
you remember yeah it's here the
immutable server pattern this is the
purpose you don't change the
configuration on an existing container
on an existing server you the idea is to
if you have to change something the
configuration rebuild it do it again
from the beginning because if you start
to change the configuration one day then
for sure you will do it again for the
next day and again and again and you
will reach a point where you will lose
the first thing you built and you will
have a stack of changes of stack of
configuration changes
yeah but I agree that's that's a common
idea to to to say that a server should
be up all the time this is a really
common and so a server should be about
all the time so you will just add
configuration and manage also history of
your configuration changes by adding
them but for sure by experience there
will be one day where you you will have
to rebuild your server it will crash
because of some reason for sure and so
you will have to apply all the history
of your configuration and this rebuild
of your server will take a lot more time
than to have a simple pipeline that is
building a new server each time you want
to change your configuration because
your pipeline will be more effective and
moreover this pipeline you will you will
have build it in advance so you will
have think about it in advance on out to
have an effective pipeline to distribute
fast a new server because you will do it
every day instead of waiting 10 years
for say oh I have to make it again and
it will take maybe one day two days one
week you don't know so thank you for
occasion but it's an interesting debate
so we're forming it left so you don't
have another question no thanks a lot
and see you soon
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>