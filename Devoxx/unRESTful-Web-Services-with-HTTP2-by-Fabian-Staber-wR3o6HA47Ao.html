<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>unRESTful Web Services with HTTP2 by Fabian Stäber | Coder Coacher - Coaching Coders</title><meta content="unRESTful Web Services with HTTP2 by Fabian Stäber - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>unRESTful Web Services with HTTP2 by Fabian Stäber</b></h2><h5 class="post__date">2015-11-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wR3o6HA47Ao" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello welcome to one of the last
talks of this year's stair walks I hope
you had a good time so far my name is
Fabian I work for a company called
console in Munich in Germany and I'm
going to talk about about unrestful web
services with HDTV - so what is HTTP -
it's the new version of our HTTP
protocol and it was created to make the
world wide web faster so the goal is you
have a web browser open a web page and
the web page should appear as fast as
possible and in order to do that the
HTTP protocol was optimized and a bunch
of new features was introduced I am
myself and not a web developer and I'm
not a webpage optimizer I'm a Java
back-end developer but as it is 2015 I
guess everybody knows that HTTP to HTTPS
no longer just used for viewing web
pages so when I have two server
applications and need to integrate them
and need some kind of interface for them
to exchange data then the most popular
choice today is basically to use rest
with its video which is an HTTP based
protocol so when I hear that there is
some new version of HTTP that somehow
helps the web developers to do fancy
things then I get curious and I ask
myself yeah maybe some of the features
are also good for back and rest services
and it turns out that actually some of
the features are really cool for for
rest services it's even possible to
create services with some kind of
functionality that's not possible to do
with traditional HTTP one that's why I
chose this title unrestful web services
so unrestful saying it's kind of a rest
service but it does things that would
not be possible if you tried the same
thing with HTTP 1 so what am I going to
talk about in the next hour first of all
I think there might be some people in
the audience who did not have a look in
- HTTP - yet so it start with a quick
like my first HTTP - requests
introduction so we run a simple get
request look at the traffic and so that
everybody has an idea of what HTTP 2 is
about and after that we go through the
main features the main features are
multiplexing server push stream
prioritization and flow control and we
will I will explain why they were
introduced and then we will have a look
if we create rest services if you do
server to server integration how can we
benefit from these features and there's
a wrap-up if you go home and think wow
cool features they want to use them
tomorrow in my project then it's not
only enough that there is some protocol
that supports that you need some
environment some tools to make sure that
things really work some automated
integration testing and they have a few
links to tools that you might probably
use if you start doing this in a
production environment so but let's
start with the like introduction for
people who did not look into HTTP 2 yet
the first thing I want to say is that
HTTP 2 is nothing that will be available
in five years or anything HTTP 2 is
something that's available now all
current versions of major web browsers
support HTTP 2 so Chrome Firefox even
Internet Explorer if you use this
version edge that's included in Windows
10 for Java developers there are a lot
of servlet containers that support HTTP
2 already there's jetty of course one of
the first then
nettie you might know it if you are into
this reactive programming thing undertow
maybe you don't know undertow by its
name but undertow is the servlet
container that's included in the wildfly
application server so if you use wildfly
starting with version 9 it supports HTTP
2 and also if you have some non Java
infrastructure around like an Apache
server there's a Mott h2 that you can
enable and also nginx announced that if
they support HTTP 2 now
and even some major web pages enable it
already so if you use one of the latest
browsers and visited twitter.com in the
last months then you have used HTTP too
so maybe without even noticing if you
just look at the timeline of HTTP
versions this sounds a bit strange
because like in the 90s the world wide
web was invented and there were a few
versions of HP PT HTTP then 16 years
nothing and then in May 2015 HTTP 2 was
standardized and now a few months later
it's supported major web pages use it
but seems to be not the whole story and
the missing thing here is this one in
2009 Google started with an experiment
called speedy and it was like an
experimental implementation of a new
HTTP protocol with the goal to make the
world wide net faster and HTTP 2 is
basically nothing but a standardized
version of the results that came out of
Google's speedy experiment and so all
the browsers and servlet containers and
everything that supported Google with
the speedy experiment they could just
reuse a lot of their code to now support
the standard and that's the reason why
it went so fast that it is now available
and ready to use before we look into a
first request I want to set up it the
like the frame the expectations what
HTTP HTTP 2 wants to do and more
importantly what it doesn't so what it
wants to do I said it already it wants
to make the world wide web faster and it
does a lot of things to do that less
protocol overhead less data to be
transferred and it has few features that
we will look into in more detail in a
few minutes multiplexing server push
whatever I believe that's the main topic
of the top you will see it soon but more
importantly I want to say what HTTP 2
does not and the main like the very most
important goal is HTTP 2 does not want
to break your existing web applications
so the one of the main goals is that you
can take any existing web application
as it is deployed on an HDTV to enabled
server and it should just work if this
wasn't the case
HTTP 2 would have no chance to be
accepted or because they cannot go ahead
and tell the world that all the web
applications need to be refactored now
so what does that mean compatibility
with HTTP 1 it doesn't mean that HTTP 1
client can talk to an HDTV 2 server it
means that like the things that the HTTP
1 protocol exposes to the application
like as the interface that an
application use this interface stays the
same
so what is this the first thing that an
application uses is it sees that HTTP is
a request response based protocol so
everybody whoever implemented a servlet
knows that there are requests coming in
responses going out right these requests
are these get put post delete whatever
and this stayed the same so if you have
a servlet and you implement it do get to
post to delete whatever method
it just works the same as HTTP 2 because
HTTP 2 has the same request response
based mechanism mmm the next thing an
application sees is that this request
response things are stateless and this
is something we all have kind of trouble
is because we want to have some state we
want to a user should be able to lock
into a web page then when the user comes
back with the next request the user
should still be locked in so we need
some kind of state and I think everybody
in the audience knows how we deal with
it we introduce cookie headers and
associate cookies with HTTP sessions on
the server side and that's what existing
applications do and so this thing didn't
change it's the same as before
everything underneath the raid like the
data is exchanged that all change it was
refactored really a lot and there are
also some new features that applications
might use but don't need to but like
everything that you have in this current
servlet api or everything that some web
application does see it's just the same
interface
so that's the the scope of http/2 so to
say but let's start with an example I
will show a quickly for show a demo but
first there's one more thing I need to
tell you in advance it is that HTTP 2
only works with SSL connections this is
kind of not hundred percent true because
the standard also allows some clear text
connections but they are supposed to be
more for debugging and some of the major
browsers only don't support it because
they want you to use HTTPS with HTTP 2
and also if you use clear text HTTP 2
you will have trouble with proxies and
firewalls and whatever so whenever you
use HTTP 2 you use it via SSL and
normally for a Java developer it
shouldn't really matter because you say
to your API give me an SSL connection
and you get one but there's one reason
why this SSL connection establishment
kind of effects Java developers today
and so I need to give a very quick
introduction what the problem is if you
really want to know what happens and
what kind of like packets are sent back
and forth and how the the latencies are
there's a really good book that I can
recommend I put it in this star on the
top right corner it's by ilya grigorik
who maybe you know him he's like wet
performance evangelist at Google and he
wrote a book called high performance
browser networking it's an O'Reilly book
but it's there for free under this URL
so that's the initial letters of high
performance browser networking dot Co if
you go there you find a lot of resources
a lot of these images and you really
understand what's going on on the wire
but what's important for us now if you
use SSL today with traditionally HTTP 1
what you do is you create a TCP
connection then on top of that you
create an encrypted channel with this
SSL handshake once you have that
encrypted channel and you want to use it
for some protocol like for example
WebSockets then you send an HTTP
great header and switch to WebSockets
and then you can start using the
connection for your actual application
data and as you see it takes a lot of
round trips to before the first
application data actually can be
exchanged and so with the introduction
of fhbp - this was optimized a bit and
the idea is to slightly modify the SSL
handshake there's this new thing called
a LPN it's the application layer
protocol negotiation and it means that
there's a small additional field that's
exchanged there and within this
additional field you agree on what
protocol you're going to use once the
encrypted channel is established which
means once the encrypted connection is
there
you agreed already that you want to use
it for HTTP - and you can start speaking
HTTP - right away without that you would
need an additional HTTP upgrade and tell
the server you want to switch to HTTP -
and we would have an additional round
trip so that's a bit more efficient but
what does it mean for Java developers it
means that the underlying SSL
implementation needs to support this
additional field this additional ALP and
mechanism and this is something that's a
bit tricky because SSL is implemented in
the Java Runtime it's some packaged Sun
security SSL there are open source jar
files that provide implementations of
this that support ALP and so you
basically just download it and use the
version from the jar file instead but
the the tricky thing is that yes that
you cannot just put a jar file in a
class path and then replace some classes
that are defined in the Java Runtime you
know the Java Runtime does not allow for
replacing its own classes with something
from the from the class path if you want
to do that you need to start the JVM
with an additional parameter it's it's
you see my mouse yeah it's this one here
boot class path and if you start your
JVM with this additional boot class path
parameter and specify this
Java that implements as is Elvis a LPN
then you're fine then your as
implementation can do it and everything
works and you will never have any
problems with it it just works it never
crashed for me and no problems at all
it's just that you need to remember to
use this and if you forget it or don't
use it you will not be able to create an
HTTP connect a HTTP to connection this
is only relevant for Java versions that
came out before HTTP two which is Java 7
and Java 8 the next Java version Java 9
comes out after HTTP 2 and it will
support alt n so once Java 9 is there
this workaround will not no longer be
necessary there's this jet 244 that aims
at implementing this new thing in the in
the core SSL implementation good so and
it was a quick introduction let's go to
a demo and view of first HTTP 2 request
I have a really like this is the source
for the most trivial thing you can do a
servlet that takes a get request and
response with hello world
I have implemented it with where is it
this embedded jetty so there's a jetty
version that you can start embedded like
with a regular main method and I put
this main method in an executable jar
file so I can just start it with Java -
jar the only thing I need to do as I
have said this additional boot class
path parameter here is the SSL
implementation so I started see if it
works if I go to http low close they put
it on port 80 443 probably very small
but it says good where is it
let's refresh although I don't see that
it should say hello Bert here I think I
saw it for a second I don't know but it
it basically works but the I want to
show you here yeah ok let's let's try it
just close this window
sigh you tapped localhost yeah there
there is this proof it hurts okay so but
what we want to do we want to see the
traffic you know we want to like see
like what's going over the wire and what
you do for that usually you use some
command-line tools so if you just use
the Chrome's developer tools
it displays the traffic the same way as
it displays HTTP one traffic so you
cannot really inspect HTTP 2 protocol
specific stuff there so you need some
command line tool and oh no that was the
wrong direction like this and now comes
the first like message that might I
don't know maybe not so nice for
everybody because HTTP 1 was an esky
based protocol so the UNIX hackers could
use standard tools like telnet or maybe
net cat or if it's an encrypted
connection like open SSL and then once
you like get the traffic you can just
dump the stuff that's going over the
wire to your terminal and you see what's
going on because HTTP 1 is human
readable ASCII format with HTTP 2 this
was changed HTTP 2 is a binary protocol
that has mainly two reasons the first
and I think most important reason is
that this enables HTTP 2 encoding to be
more efficient so there's less data to
be transferred and yeah that reduces the
latency when you view web pages and
there's another reason when you
implement in parser for HTTP 2 in code
then it's much easier to do then to
implement a parser for HTTP 1 because
the format is quite simple and quite
quite clear on the downside you now if
you want to see HTTP 2 traffic you need
some specialized tools that do that so
you cannot just use telnet anymore there
is a tool that everybody knows curl the
secure version that supports HTTP 2
actually but curl has a bit of a problem
why I didn't use it because what Qura
does it it runs some requests that you
specify
then it terminated again and what I
wanted to have is a little tool that
just starts a background process and
leaves a connection open for a longer
time because later I wanted to play with
push messages and in order to receive
push messages and must be some open
connection that the server can use to
push data and because this wasn't there
it I just did my own little thing so I
have the the github URL on the button of
my slice but it's only for use in HTTP
two demos so it's no other use case so
you start the tech brown process will
start so this is where all the traffic
goes through and then in another
terminal you can run requests the nice
thing is as here all the traffic goes
through this process it's most easy to
implement a dump parameter that
additionally just dumps all the traffic
to the to the console so if I type
connect to connect to the server port 8
443 Oh what come on localhost and then
get some path here he'll over it it
works again very real I reliably so
let's look at the traffic can I make
this larger so we don't want to go
through all the traffic here the main
point I want to talk about is that you
see that the HTTP to traffic is split
into small chunks and these chunks are
called frames and the frames have a
maximum size so in HTTP 1 you can have
potentially infinitely long messages so
to say and they will be all transferred
like until they are done in HTTP 2
that's not the case h-e-b 2 is chunked
it has single frames the frames have a
maximum size and like one frame after
another is transferred over the wire
that's important later for the
multiplexing because if as the whole
traffic is just a sequence of chunks you
can have like many requests in parallel
response messages coming in and so on if
you want to split HTTP
messages into chunks there's one thing
that comes really natural and that you
want to do an HTTP one message is
composed of a header part and a body
part or in many cases only a header car
part in the get request for example and
that's a natural place in HTTP 2 we have
a headers frame like this is an outgoing
headers frame for the get request you
see it's a get and this is the path so
you find everything again there and now
for the response this is the headers
frame again like with status code 200 ok
and this is the data frame so what used
to be the message body is now called
data frames and this data frame has 13
bytes payload so that's the length of
the string hello world if the content
would be longer like if it was a JPEG
image with 5 megabytes bytes or
something then there wouldn't be only
one data frame but a sequence of many
data frames right and here's a little
flag and stream that tells you if it's
the last one on or if there are more to
follow hmm good let's go back
so although I so it was a quick intro to
show you up show you how HTTP traffic
looks and I guess so now you have a like
rough idea what it does and know it's
time to jump into the cool new things so
to say and the cool new things are
multiplexing server pushed stream
polarization and flow control let's
start with multiplexing that's I
prepared already a bit because I told
you that the traffic is chunked into
frames and frames have a maximum size
and so it's possible to do many requests
in parallel I want one more thing in the
in this demo can I switch there what
maybe maybe importantly so when when
there are more than one requests going
on at the same time then somehow the
client and the server need to know which
frame belongs to which request and this
is done via this little number here this
is called the stream ID as there's only
one request in the demo there's just
stream ID one stream ID one stream ID
one or these belong to one request
response pair like a pair request
response pair is called stream in HTTP 2
if there were more than one request
going on the other requests would have
different numbers so that's how like
frames are related to the request
response pairs so multiplexing now
multiplexing now uses this to to like be
able to do many request responses in
parallel and the effect of this is very
well visualized with a small demo I've
created docker container with the demo
so I can just run it locally this demo
actually comes from the guys who did the
HTTP 2 implementation in the go
programming language and what it does no
larger scale in a bit
yeah what you see here looks like a big
picture but it's actually composed of a
lot of small pictures and the
our delays the delivery of each small
picture by 200 milliseconds and now I'm
loading it with HTTP 1 and what happens
is yeah it takes some time until the
whole web page is there so why is that
because the web browser sends a request
server delays the response by 200
milliseconds then the response comes in
and while the request is running the
connection is blocked there can be
nothing else done with the connection at
the same time the browser has to wait
until the image is there only then it
can use the the connection to issue in
the next request then it gets the next
response and so on and this is a very
like inefficient use of the connection
capacity because the connection is idle
most of the time but with HTTP 1 there's
nothing there is nothing the the browser
can do the browser can optimize it a bit
because the browser creates not only one
connection but maybe five or so I don't
know how many but if you have few
resources like a few more than five then
you see that it resides in a real delay
so if you load the same thing with HTTP
- yeah you see it just basically comes
immediately what happens there's only
one connection so that's one basic
principle with HTTP - you have only one
connection leave it open use it for
everything and over this one connection
all requests are sent at the same time
each request is delayed by 200
milliseconds still so the server side
didn't change but then after 200
milliseconds all the resources are sent
back at the same time like using
multiplexing and you see the effect it's
it's a lot faster obviously so what does
that mean for rest services have any in
rest services we basically have the same
problem if you have a rest server and
there's one request that takes some time
to be processed then the client
connection is basically blocked until
the server responds what can we do about
it we can basically do nothing about it
so we can either ignore it and then the
client is just
because when there's a long-running
request going on the connection is
blocked or we can be as clever as a web
browser and open not only one connection
but more than one connections in
parallel but yeah what does that mean I
mean that means either we limit the
number of connections somehow and create
a connection pool and then yeah we need
to size the connection pool and if the
load gets higher than all the
connections in the pool are used and yes
it's kind of difficult to do and it kind
of only delays the problem because if
the load becomes more then the
connection pool is used or we don't take
the effort to maintain connection pools
and just fire a new connection with each
request and then we have kind of a
danger in our system because it might
run well for a long time but if the load
is such that the the frequency with
which new connections are created is
higher than the frequency with which old
connections are cleaned up then like the
number of existing connections will pile
up and then at the end of the day they
will basically run into some limits you
know I don't know no no threats left or
no TCP ports or whatever and basically
this will kind of hold your application
or something weird will happen and like
whatever you do it's either very complex
or like you need to find some trade-off
and it's hard to do with HTTP - things
are easy you just create one connection
use multiplexing and yeah if you have a
long-running request you just
long-running but still you can use the
same connection for short requests in
the meantime so there's just not you
don't need to put much effort into doing
these things I have a few examples how
this would look like with current Java
implementations of HTTP to clients also
just to show you what Java clients are
out there the problem is here I mean
it's not really a problem but it's kind
of what you need to do if you want to
use multiplexing you need to implement
an asynchronous client so if your Java
code is synchronous and waits or if
they're each request until the response
is there then it doesn't help you that
the underlying protocol would
be a synchronous you know so and there
are few HTTP to clients out there that
support this one is okay HTTP it's I
think everybody who has to do with
Android development knows it and it's
also works pretty well on the server and
yeah it's like the typical asynchronous
way you do things you know you create a
request but instead of sending the crest
right away a request right away unq it
somewhere provide a callback and the
coil Beck has our own response method
and then this will be called once the
response is there hmm so in that way you
can just use a connection for multiple
requests in parallel another client is
Nettie
I mean Nettie is basically more known
for its non-blocking server but Nettie
also comes with an HTTP to client
library there's an example code in the
Nettie github repository it's pretty
verbis it's like for classes and showing
all kinds of things you can do but at
the end it all comes down to the same
thing so you have kind of a response
handler with a message received message
and this is called once the answer is
there so it's the same idea but a little
more code and then there is jetty jenny
has to actually I mean jetty is
basically an HTTP to and a servlet
container but jetty also provides an
HTTP to client library and the client
library provides two api's so one is
high level and it looks pretty much
similar to what we saw with okay HTTP
then there is a low level thing that
like allows you to really like when you
do a request that you create your
headers frame that you want to send and
create a new stream explicitly and then
have like methods the callbacks that are
called when frames come in and you can
just like view the results of the
individual with frames you receive so if
you want to play really low level with
HTTP two features that's the way you
could go and when it comes to standard
like standard clients you could use I
mean there's
actually in in java SE in the regular
Java there's just one HTTP client it is
called the HTTP URL connection and I
don't know how old it is but very very
old don't know Java one-one rustle and
actually as now this whole HTTP 2 thing
is going on there is an attempt in jet
110 to implement a more modern
alternative to that and that will
probably go into Java 9 and then in Java
9 you will also have a like a basic HTTP
to client where you can do basic things
without using an external library
what's with Java EE I actually didn't
don't really know this to be honest
good so this was the first new feature I
wanted to show you the first thing that
you cannot do that way with HTTP 1 I
think it's pretty cool so for me it's
actually my favorite feature because it
makes the clients much simpler much more
robust and everything much more
responsive but maybe you're not yet so
thrilled because basically just solves
like non-functional requirements there's
no real new things you can do it's just
that the things you do work better
somehow but now we come to server push
push which is really a cool new thing
and server push I need to explain a bit
what it does because when you just read
the name you maybe have some wrong
associations like that it's kind of a
new web sockets or so but server push is
actually it's very easy to explain it
because it does exactly one thing it's
exactly for one specific use case to
solve this and the use case is the most
common thing that happens in the world
wide web a browser accesses an HTML page
on the server and the server delivers
the HTML page to the browser now within
the HTML page there are probably links
to CSS files JavaScript files images and
so on so the server basically knows that
very likely this client will come back
in a few seconds and request the CSS
files images JavaScript and so on but
the server cannot do anything the server
just said
and waits and waits and waits and once
the other requests come the server says
yeah I knew that you probably need these
also right and at some point very HTTP
can be optimizing that HTTP or HTTPS
quite a bit because what happens with
HTTP 2 is when the server knows that the
client will probably come back and ask
for a receive SS file or JavaScript file
or image file then the server does not
need to wait for the actual requests
coming in the server can just create the
response and send it right away to the
client so what happens then the client
receives a response for something that
he has never requested and it takes the
response and just puts it in some cash
right and once the rep that browser is
done processing the HTML Dom structure
and comes to the link where the actual
like CSS file is linked for example then
the read browser issues a get request
but the HTTP to stick within the client
says oh this request I have the response
already there it sits in my cache and
then J the request is responded right
away without any additional network
traffic and I think this is a pretty
cool solution because it means from the
application point of view like the web
browser being an application now nothing
changed so the web browser makes a get
request gets the HTML page makes a get
request get the CSS file and so on so
it's all like compatible works the same
way as before but when you look at the
actual network traffic there's a whole
lot of optimization going on so the
traffic is like very differently than
you might think it is ii get requests
doesn't even trigger network traffic and
there's this push message being sent to
the to the client and so on so i think
it's a it's a really cool way to
optimize these things while still being
compatible with the HTTP one semantics
when we think of yeah maybe first thing
it's actually easy in java to create
these push messages so you build a
request in in your memory somewhere
because
doesn't come from the client and then
you call push and then this request just
goes through your servlet stack and
comes in this any other requests and
like the response is sent out and it
will be part of servlet for that oh so
this is now a like proprietary jetty
code but with the earth servlet for that
oh you don't even need any external
dependencies for that so let's think for
a moment of what we can do with this
with rest interfaces and rest interfaces
basically I like them very much because
they're very clean and very simple and
very nice and when you have nice
resources and verbs and so on you it's
really easy to understand what's going
on when you see a rest interface
definition but there's only always one
thing that doesn't work well with rest
and this is when the client needs to be
informed when a resource on the server
side changes so I don't know the server
like counts something or don't know and
when the count goes up it wants to kind
of inform the client so what can you do
in that case with your rest interface
basically you need to do polling so that
the client every second just triggers a
get request gets the state the the
status of the resource next second
triggers a get request again gets the
response and so on so that's the only
way you can do it when you want to stick
with the rest interface and it's also
the preferred way I mean if you kind of
can live with falling and can do it and
it has not too many drawbacks
then it's the way to go because it's
very robust it works everywhere and you
still have a rest interface however as
you all know this polling you need to
have you need to do some trade-off the
the more frequently you Paul the more
unnecessary Network load or like also
load on the server you create and if you
reduce the polling frequency then the
time when something really happens on
the server the delay until the clients
gets this information increases so you
have to find some trade-off you can live
with and in order to avoid this
trade-off yeah the people do kind of all
kind of crazy things like that
we create rest interfaces since they say
okay but for this specific use case we
don't really use rest we use WebSockets
instead or long polling or whatever kind
of non not really rest not really HTTP
stuff is going on there so what can you
do with this server push note that the
first thing you can do of course when
your server sees that a resource changes
and the client would be interested in
that the server can create a push
message that helps a little bit because
the client Paul's frequently where
frequently but when the client pulls the
next time the the actual response is
already there so like the polling
request goes a little bit faster when
actually something happens on the server
so that's a bit of an optimization but
not very thrilling but when you have a
client and you learn that whenever
something really happens with the
resource you get a push week push
message from the server then you might
modify the the loop that Paul's a little
bit because usually polling is triggered
by a time interval like every second or
so but you might modify this trigger you
might say okay maybe my polling loop
goes once a minute just to be sure but
in addition to that my polling loop is
triggered whenever I receive a push
message so what what do you end up with
Dan you kind of still have a rest
interface you still have this regular
polling semantics you still have a
client that uses get requests to query
the state or the the value of a resource
but you don't need to do this trade-off
anymore you know using like when you're
polling loop is triggered whenever a
push message is received then the delay
when something actually happens on the
server until the client receives it it's
really low so you have no no
disadvantage there and on the other hand
you don't need to switch to any like
WebSocket or long polling or
non-standard protocol you just use the
real rest semantics you really use HTTP
two as it is supposed to be
even if it fails like if for some reason
there's some network component that the
that causes that you cannot use HTTP to
you're just the only thing that that
changes is then you just revert back and
trigger your polling loop like every
second again
so it's know you're kind of without
having any major disadvantage you can
like use polling but get rid of this
mere trade-off that's associated with
this in HTTP 1 and that's something I
like really very much
that's one drawback a little bit in
order to do this you need to have a
client library that allows you to
provide a callback that is triggered
whenever a push message is received this
is easy if you if your client is a Java
client in some kind of back-end system
because all the the libraries ok HTTP
nettie jetty that I showed you there are
open source libraries and you can put
context wherever you want basically yeah
no problem if your client is some
JavaScript in a web browser then you
cannot do these kind of things because
the vet browser doesn't expose like any
low-level HTTP to features to the
JavaScript API so in in JavaScript in a
web browser you have no chance to like
provide a callback that is triggered
when a push message is received
so basically server to server
integrators back-end developers can do
more fancy stuff with HTTP 2 than the
actual from that people can do which is
nice good that was server push I think I
mean the most amazing feature probably
there are two left but they are really
like easy to explain it's a stream
prioritization and flow control stream
prioritization means that like they
originally in in in in the in the web
world you want that messages that
deliver content are processed earlier
than messages that deliver just some
fancy graphics stuff or something so
that in the goal is that the user can
start reading the web page right away
and some JavaScript that's
only painting the button green or
something can be delivered later and in
order to support that there was some
stream prioritization introduced it has
like two mechanisms one is some priority
field which is kind of an integer the
toilet CEO tells you how important this
response is like from between 0 and 255
and the other feature is some dependency
tree stirs you can mark a response and
say ok when this response depends on
another response and then the other
response should be processed earlier but
all this stream prioritization stuff is
just you know it's just additional
fields in the protocol so it depends on
the client if the client actually does
something with this information or if it
doesn't so if you just as a server put
some priority field somewhere and the
client doesn't respect them you don't
gain anything but if you have a use case
where you have kind of don't know some
normal messages and some really
business-critical messages and you
implement a client for that and you want
to have some kind of prior
prioritization then you don't need to
invent any additional application layer
stuff you can just use this these fields
that are available on the underlying
protocol and then there's flow control
flow control is also pretty simple but
for some use case is pretty powerful
because yeah what does it do
it's as I said HTTP 2 supports
multiplexing so there's a lot of
parallel requests potentially going on
within one single connection and what
you want to avoid is that if you have
one very large resource like few I don't
know you have a rest service and this is
called once a day and it gets your 2
gigabytes SQL dump right then you don't
want that the connection is that the
whole bandwidth is used by getting this
dump and other important messages don't
get through anymore and so what 8:52
allows you is to limit the number the
the available bandwidth for a specific
request or response so you can basically
say if you have
strange service that gives you the two
gigabyte thumbs once a day you limit its
available bandwidth to a certain number
of bytes per second and that way you
make sure that the there's still
bandwidth available for other stuff that
you would do in the meantime I mean
there are in most cases I think you will
not need it but once you have like last
message a large messages or want to
limit something it's really useful that
the underlying protocol supports it and
that's also something that's implemented
in the in the transport itself so once
you you you specify this there are some
like callbacks some flow control
strategy interfaces that you can
implement this example comes from from
jetty and once you implement this it
it's row that the underlying protocol
really respects that and uses the
bandwidth in in that way okay so that
was it basically for the main headline
features so to say multiplexing server
push stream prioritization and flow
control so if you are excited about
these new things and think okay I want
to start using it it sounds really cool
then basically you need some way to test
it regular unit tests won't help so if
you have your your service that uses
server push messages to trigger some
action on the client you can't have a
unit test to just figure out if the
right business logic is triggered or
something but you need a real test with
the real network interface to make sure
that this push message is handled
correctly on the client stick right and
one way to do it with the real network
connection is this our Killian cube
there was actually a talk about the
quillion cube at this devoxx and what it
does basically it starts a talker
container so you can define a predefined
docker container and within the docker
container you could like just put an
whitefly for example with HTTP to
support enabled and then a Killian cube
will start that container when your test
start and then you can use a Quillin to
deploy your web applications there and
run your tests against that
and said that way you have real like
network communication from your host to
to the container there's a another
alternative it's a bit more lightweight
than a pillion cube it's this docker
maven plug-in you can use it to start
doctor containers at the beginning of
your of your integration tests and stop
docker containers at the end of your
integration tests but during the tests
themselves you don't have this achillion
that you for each test deploy new war
files or something it's just you know if
you make want to create one container
and use it for your entire test run then
this is a bit more white light light
white than what you can do with
achillion cube docker is pretty cool for
testing it because you can just you know
you can create a docker image where
everything is configured in HTTP to
works and so on and you don't need to
care about it and once you have docker
you can just run your tests and they
will work the same everywhere but docker
cannot be used everywhere I mean first
of all you need to have admin rights on
your machine which is maybe not the case
sometimes second thing is yeah you need
to have some way to share your docker
images if you're working on an open
source project it's simple you should
just like push them to taco hut and lock
download them from there it's the
infrastructure for free so to say but if
you work on a closed source project you
basically need some kind of internal
docker repository in your company and
you might not have it if you don't have
docker you can there's the ok HTTP
project that implemented this client
that I showed you they also have a very
nice mock web server so it's it's really
simple you kind of script it you say ok
here first Rick ones response should
return this point a response body second
response this one third response this
one and so on
and then you start it and you can then
query it with real HTTP two requests it
works really really well and doesn't
require docker but as I told you in the
beginning of the talk if you one
to use HTTP - you need an ssl library
that supports HTTP - so that means when
you want to start that for testing HTTP
- you need to start it with this special
boot class path parameter that you need
to have HTTP to support how do you do
that usually you start integration tests
with maven surefire plugin and you can
just put in like a JVM parameter into
your maven configuration however the
version of this jar file depends on your
JVM version so if you have Java 8 visit
40 you might have an other Java file
here then if you have Java 730 or
something but so that means that when
you do this you need to be aware that it
probably works for you
but if somebody else checks out the same
project and starts to run it and this
other guy has some other Java version he
needs to basically go go ahead and edit
it and download the corresponding a LPN
jar file there so that's a bit of a
drawback that's better with docker
because in the docker container you can
just have your Java there have your a
LPN jar have everything predefined
but on the other hand once you have done
this it's just plain Java and you don't
need any other infrastructure ok so
basically that's it for my introduction
on HTTP 2 - and how you can do how you
can use it for creating rest services
that can do a little bit more than you
can do with HTTP 1 today I have a few
links there's other guys apart from me
doing HTTP 2 talks one is Adrian Cole
who committed a lot to this ok HTTP
project he was at Java so in this year
and they put their videos on v-mail then
there's this other HTTP to talk in the
other room
it's by Ximena ported and he also talks
a lot about HTTP - he's the guy who did
the HTTP - implementation in jetty so
basically I didn't when I created the
slide I didn't I thought
we still have this parlays thing but as
devoxx talks are now immediately
available on youtube you basically can
forget it and just watch his talk if you
want to learn a bit more and how JD does
it or whatever and then there's this
very highly recommended book by ilya
grigorik high performance browser
networking or write a book but available
for free as an online version so if you
if you want to dive into these topics
and see what what what HP we do actually
does and what kind of other optimized
agents it has it's really highly
recommended so thank you very much I
hope you enjoyed it a bit and you hope I
hope you got a bit curious what http/2
is and hope you will look into it
have a good trip home everything hope
you don't fly with Lufthansa they are on
strike yeah thank you
start question or if you have a few
minutes I don't see ya
but oh if there is any plan for having a
standard API for JavaScript for push
notifications in the browser as far as I
know there's no such plan but I'm a
back-end developer and no front end
developer so maybe my knowledge is not
so reliable there okay so I would say
have a good time um if you have any more
questions so just come down I'll be here
for a few minutes more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>