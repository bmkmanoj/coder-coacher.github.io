<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How Uber Audits Real time Infrastructure of Trillion+ Messages by Ankur Bansal | Coder Coacher - Coaching Coders</title><meta content="How Uber Audits Real time Infrastructure of Trillion+ Messages by Ankur Bansal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How Uber Audits Real time Infrastructure of Trillion+ Messages by Ankur Bansal</b></h2><h5 class="post__date">2017-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5NCiygjT2B4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hello everyone how are you all
doing good well I'm not so good I'm
still jet-lagged so if I'm stammering or
if I freeze you know just give me some
slack
anyways I'm Ankur I'm here today to talk
about the real-time infrastructure at
uber and how we audit it at a massive
scale between I like my presentations to
be interactive so you know there's a mic
there or if you have any questions if
I'm going too fast too slow raise your
hand and let me know alright here's the
agenda so we're gonna quickly look at
some of the use cases for streaming at
uber we're gonna talk about kafka
pipeline which is the heart of the
real-time infrastructure uber then we're
gonna talk quickly about what is
auditing why do we need auditing and
then we jump into the chaperone and open
source tool which came out from uber the
version 1 which is already open source
for over one year and there's a redesign
which which has been running in
production still not open source but I
will give you a little bit about that
and will probably open source at some
time in future and then we'll wrap it up
by talking about the future work on
Chopra alright let's look at some of the
use cases you guys might have already
heard about real time processes in
meaning a buzz word for last 3-4 years
everybody wants to be real-time right
like most of the companies they are
figuring out how how does it fit into
their picture how do they want to do it
but here at uber this is bread and
butter if it wasn't for real-time
processing we would not be able to scale
and make it an awesome service that uber
is let's look at the real basic use case
you know this is this is one of the
first thing that you do right as soon as
as a rider opens up an app they want to
request the cap the
the data point goes to the server and
now we had to figure out so many things
we had to figure out who the rider is
where he is what is the search
multiplier over there how many cars are
in the vicinity what is the traffic
looked like and all the other bunch of
things all of those real-time data bits
go into our stream processing processing
system through Kafka we do some
calculations and spit out the result and
say okay here's the driver and rider
match and you know this is the right
which you're going to go away we also
calculate ETA based on a bunch of things
all in real time within like couple of
seconds you see a ride coming your way
another big application who knows uber
eats here varies of Oh awesome yeah you
know it's a big thing in Amsterdam I
hope it's coming to your city as well
pretty soon and please try it
uber eats is another awesome product
from uber engineering where we deliver
food from your favorite restaurants very
quickly but as simple as it may sound
calculating eta for uber eats is much
more complicated even than right in ride
all you care about is what is the
distance and how much is the traffic on
the way while in uber eats you also have
to worry about which restaurant is this
order coming from what is the historical
ETA s from the stressed friend are they
slow the fast what kind of dish are you
ordering you know apart from the regular
distance and speed things so what
happens is all of those data points
along with historical data points go
into our system there's a machine
learning model which predicts a ETA and
it keeps on updating itself as the data
is coming and it generates estimated
time for you another big one is fraud
detection so like most of you most of
the other applications uber also has
fraud problem and one of the unique
thing with uber is if you take time to
figure
the fraud there's no way going back
because right has already happened and
you cannot do anything about it so we
need to figure out it's a fraudulent
request right before the right happens
which is a couple of seconds there more
use cases I can keep going on and on but
pretty much real-time processing is the
DNA of uber and we don't do it at a
small scale you know over years we have
grown to this enormous scale we're
processing one more than a trillion
messages a day and we process more than
a petabyte of data per day and we within
the kafka terminology we have tens of
thousands of topics I would have said we
are among the world's biggest data
producers as well as the real-time
sisters until I heard the presentation
this morning from CERN right they were
probably the biggest we had nowhere
close but given the real time given the
real-life applications we still have
enormous scale compared to you know many
other just think about it for a minute a
trillion is easy to say but it's a huge
scale we're talking about 11 million
transactions per second on average and
as you know not all of the data flows
with the average we have slows and ups
and our Peaks are much bigger so we have
to build up an infrastructure which can
take 10x peaks of this how do we do this
again here's the heart of all of the
real-time infrastructure Apache Kafka
this technology has been very good to us
how a Kafka by itself obviously cannot
scale to this humongous level we had to
do a lot of engineering around it to
make it scale up to this level but in
short I would say you know Kafka is like
a lifeline
for most of the data infrastructure and
ruber
and next question is what kind of data
you're talking about you are talking
about petabytes of data what exactly are
you you know what what is that data we
use Kafka for pretty much everything
it's used for journal pub serve between
the services it's used for stream
processing we already saw a couple of
examples it's used for ingestion into
SDF ss3 it's also used for database
changelog last but not least logging
which is probably which has probably
lion's share of data being produced and
this is important for debugging and
figuring out our systems everything
he'll be on our here's our our ecosystem
looks like if you see we have our Kafka
system right in the middle on the left
are all the sources of data which
includes Rider app driver I app all the
services databases and this is not like
you know all services produce the data
multiple times over on the right these
are the big consumers of our data for
example search would be looking at some
what sort of data points put it together
cover the search multiplier sends it to
mobile app similarly elk gets all the
logging data and gives the opportunity
for our developers to do real-time
debugging similarly Samsa and flink
based stream processing system power our
- dashboards real-time analytics
etcetera and then we have her do Peter
Lewis for a long time data storage as
well as analytics
we we do this enormous scale and a very
strict SLA guarantees we promise about
five millisecond it's not end-to-end
latency but it's five millisecond as
soon as klein gives us a message the the
acknowledgment goes back to him within
five milliseconds and we'll see how that
is possible we give about four nines of
uptime we are actually doing much better
than that
and then we have reliability guarantees
for most of our data up to four nines
some of the critical data like payments
finance all that is guaranteed 100% some
data like logging is obviously not that
durable because if we have to if we have
some system issues we drop messages and
we give 99% reliability for that most of
most of the data fits into four nines we
have multi diesel application and we
have multiple languages that we have to
support and obviously we need auditing
and we'll talk about auditing you do
here's a high level diagram of of our
infrastructure if you see right in the
middle this regional copper cluster a
regional Carrefour cluster is a Kaffir
cluster for a particular data center and
we have many of those let's hold on to
that thought for a minute but that's the
center point and that's where I want to
put messages but it a message to reach
that tougher cluster it has to go
through multiple stages we put rest
proxy in front of CAFTA so a
million-dollar question is why do we do
that because boss most of the other
organizations which you don't which door
which sorry which use Kafka they have no
receipt the biggest reason for us is we
have hundreds of thousands of client
connecting to Kafka and if you do any
kind of maintenance or Kafka or if
there's an issue going on you'd have
connection thrashing so think about
maintaining hundreds of thousands of
those connections and we can we have
multiple languages to
which means you have to have multiple
Kafka libraries which was not the case
when we started using Kafka and you know
there are a bunch of features which
Kafka doesn't provide for example
auditing throttling and you know
stopping any of the bad cluster bad
clients so all of that can actually be
done in this proxy so we put a layer in
between and which has been actually one
of the biggest contributing factor which
has made us scale up to this level then
we have our own client libraries which
are nothing but which does a post
request to respond see and each
application uses those we don't use the
default Kafka libraries and once the
message start from application it is
given to our proxy client it goes to
this proxy and from this box it is given
written to regional kaffir cluster from
there we have your applicator which is
our homegrown version of mirror maker it
takes data from least a cluster and it
replicates it to aggregate clusters all
in all the disease and all DC's have
aggregate cluster which give you a
complete picture of data from all the
data centers collected together so this
is the DC one shows you one complete
pipeline for message for message in a DC
the lower picture also shows you a
secondary cluster which is present in
all the data centers so this is a
fallback path in case of a region a
Kaffir clusters were to go down less
proxies because we loaded ourselves they
automatically detect the curve there's
an outage on restock of a cluster start
writing to the secondary and from there
the data is replicated back when the
outage is over we have similar thing as
local agent sitting on an all clients if
flash proxy were to go down data can be
written down and spooled locally and
when the outages recover data is sent
back again hi
so reason atlas sir is not just one
cluster hits bunch of clusters and they
depend on what kind of data is being
written to them why do we do that first
of all we want to reduce the blast
radius of any problem or an issue
because if one cluster is having in any
issue you don't want to impact
everything secondly each type of data
has very different requirements for
example our logging needs to be very
high throughput you know want to block
you want to quickly write a lot of data
but it's okay to lose some of the logs
however you know a high value data is
something which can which could be slow
but it's not okay to lose any data point
because that's dollars similarly time
sensitive data like push notifications
or search related data it is very time
sensitive if you are having some issues
and if you cannot process so everything
it's okay drop them but if they're late
they useless right so by having multiple
clusters it gives us an opportunity to
tune each cluster for specific specific
requirements while less proxy is more or
less same for everything then we use the
Kenda cluster for fallback we I already
talked about it and then we have
aggregate clusters which which collects
data from all the data centers and give
you a complete picture so that we have
everything replicated from all data
centers in one place all right
another interesting diagram let's just
let's look at the life cycle for every
data point which is written into Kafka
and this is the key of the scale that we
can manage so as soon as the application
writes the number 1 they get points that
as soon as the application writes our
data point to a rest client we get the
data in the rest line and we acknowledge
so what we did there is we we took the
data point and put it into local buffer
in the memory and we acknowledged this
way we can give a very low latency and
very fast response by without blocking
the application but the risk there is if
application were to shut down
ungracefully we're going to lose that
data point right then we we batch most
of the data together in from for the
same topic
we send it over to rest Roxy we do the
same thing address Roxy we take that
batch and we acknowledge right away even
though data hasn't made it to the calf
pasture and then we do it we do we split
the data we do another sort of batching
where we optimize it and depending on
how we're gonna write to different
clusters and different brokers then we
offline we write the data back to Kafka
and within Kafka same thing repeats we
don't replicate it we just write it once
as soon as kafir gets it into the memory
we just return so what we did here is we
up we did the aggressive batching
everywhere we never block the client and
you know we optimize it for high speed
by compromising on reliability aspects
and this is the secret behind high low
latency high speed high throughput
system that we have however it comes
with the disadvantage I already
mentioned which is anything and
everything goes down application goes
down we'll use data
less proxy node goes now we lose data
regional cluster node goes down all
already started we lose data and that's
exactly why we need auditing right so
the reason we started thinking what
auditing is we used to get this question
a lot think about it somebody provides
your service and says hey here's the
service which is very fast but you know
what there is a possibility of loosing
data what will happen every time you do
not see something you will point fingers
at them right
like oh you have a lossy system probably
you lost my data and that's what used to
happen with this quite a bit so first of
all we saw was speed and then we started
looking at reliability aspects and this
is the question which we would get every
day which is I wrote X messages but I
only got Y messages we are at the rest
of it well they could be multiple
explanations it is possible the client
only wrote Y the thing they wrote X but
they didn't it is also possible that
they hit their quota limits because they
were writing too much data very fast and
we actually throttled them so they
violated Thessaly it is also possible
that ran did a kill 9 on their
application and didn't give us chance to
flush all the data which means you know
it's again their fault but we need to
prove that apart from that it is also
totally possible that it is pipeline's
forward and we did lose data and if we
did lose data we want to find out where
the loss was right and that's why we
need hardening we also need auditing to
figure out what is the end-to-end
latency if we are meeting our
reliability goes or not and accounting
also helps us to say auditing also has
to do accounting as well as charts right
that's where that's where we started
thinking about developing a system which
can do this auditing for us and that's
Chevron right so when you think about
auditing so the first problem you want
to solve is counting how much messages
are going through each stage so
so you saw pipeline each message has to
go through multiple stages and they have
multiple touch point first touch point
being in the client libraries second
being in the Red Sox E and then calf
clusters and then aggregates so what if
we could count
the messages for a given time period but
if I could tell like 100 messages passed
through stage 1 between 1 and 110
similarly if I could count the messages
for this proxy similarly if I could call
count the messages at Kafka if I could
count the messenger each stage for a
given time period I can have a very good
idea of how many messages actually
started and how many actually made made
it through all the way so that's what we
did so that's where the first stage
comes and we we ship a counting
mechanism in the stage 1 and stage 2
here we created a simple algorithm which
can quickly look at all the messages
being written and count them right how
about the riester clusters like stage 1
and stage 2 was easy because they were
the code that we wrote and we could
quickly embed the counting code but
kafka it was not so easy to embed that
code because otherwise we will have to
rewrite we would have to do a massive e
right but again so Kafka has an
advantage it's a pub sub system so we
could have a service which listens to
all the messages and calculate do the
calculations as well and you know
publish that so that's what we did so he
created a chaperon service which can
consume pretty much everything coming
into the system so how this thing has
this thing still has a problem so a
message could start at 1 p.m. from the
application by would reach one second
later into the into the rest proxy it
may reach 3 seconds later into the cache
for question and 5 seconds later into
aggregates so if we were to do
calculations there is the time drift
issue right how do you solve that again
if you remember we control the we
control the library and we basically can
embed a timestamp within our messages
this way everything is being made on the
creation time instead of it being the
arrival time here's the here is a very
basic picture describing what Chevron
does so at each stage we divide the time
stamp which is which is really an epoch
and take a modulus by ten minute windows
right this gives us sorry we'll take a
divided by the ten minute windows this
gives us the bucket that particular
message goes to and within that bucket
we would have a map which would have
different topics mapping to aggregated
information so what then this diagram
shows here is within this bucket topic B
and C B C and B were the ones who were
to tend to so this becomes a key and
then the value is what tier this data is
being aggregated at start and end time
what is the count and then we have some
of the latency stacked latency starts
like p99 and average so we collect this
for all the time windows then we seal
those packets and create an odd
investors every ten to twenty minutes
and then we send it over right so this
diagram here explains how the ordered
messaging thing works so ordered library
here is doing the counting it so the
blue lines show the date of real data
path the yellow lines show the auditing
path so at first stage audit message
audit library does the counting it
creates the audit message an aggregated
one and puts it into Kafka as well so
take calculate count sensor to send it
down into the Kafka because that was
that's the easiest thing to do
similarly at CSU we do the same thing
similarly shopping service takes all
data does the counting
sends it back to Kafka same thing
happens in aggregate cluster and all of
those messages make it through aggregate
cluster that's where the chaperon
collector is a piece which look at the
special topic which is all racket
message all the ordered messages coming
in puts it all together calculated and
puts it into database so everything
clear so far all right
one of the key pieces or one of the
challenging pieces was creating the
chaperone service so chaperone service
as you can imagine it has to read all of
those tailor messages and do a
calculation how do we do that we created
a distributed system using Apache helix
where helix does the resource management
which is minimal people of those workers
and what helix does is helix is like a
brain which assigns the work to
different workers and the workers spin
up multiple threads depending on how
many topics they're managing and
depending on how many brokers they're
gonna talk to and then they do all the
calculations and do the same thing that
we saw in our libraries and put that
data back into Kafka so it's I'm making
its own little bit trivial it's this is
where meat of the work went in and you
can look at the code which is open
source but here is a result in the end
you know chaperones produce the results
which you can go to our UI and easily
query you can say this is my topic name
I want to search from this time to this
time I will tell you okay this many
messages were returned in rest Roxie
this many where it went into DC one data
center one calf a cluster data center
two or three for and total is this and
this money went to aggregate if you see
all these numbers match and says okay
100 percent of messages made it through
we also emit metrics which you can plot
and this is this is one of the very
helpful thing for us if you see this top
graph you see all the lines all four
lines match up which shows that all four
stages had same number of data going
through however when there is an issue
you see a graph like this where you see
that this proxy client is saying it
wrote 82,000 messages while all other
tears are reporting 53,000 messages now
this from this we can conclude that
there was a data loss between rest Boxey
client as well as rest itself and we can
go drill down into what exactly happened
who all lost data if you remember we
also were collecting data on latency so
sharpener also helps us figure out the
health of the system how much time does
it take on average from data to make it
from one end to another this becomes
very important for time sensitive use
cases so this first version of chaperon
was pretty good for the first first
iteration we could scale it up to 250
billion messages per day which in
Chaperone terms because it looks at
everything four times it's one trillion
messages per day and we have tens of the
deployment per data center and it can do
a reasonably good job of calculating
latencies however it was not good enough
right as uber continued to scale we
started feeling that we're not able to
operate chaperon with the ease first
problem was if you saw the first design
the auditing itself first depending on
Kafka so when you're monitoring a system
and you're depending on the same system
what does that mean whenever you have an
issue
you're gonna lose your auditing
capability as well right so whenever you
have an issue you don't even know how
much impact that issue cost so this was
the first thing we wanted to get away
from if you remember chaperone collector
is in the end which is collecting all
the audit messages and it puts it into a
database the way it was designed it was
a single point of failure because it had
a state and it had to get all the
messages together well there is an
argument we could have scaled it to not
become single point of failure but
anyways this was a design choice at that
time I was a single point of failure we
because our audit messages are also
going through Kafka pipeline this Lord
of latency which is being added to the
to the audit messages which was also a
drawback not so much not a big issue but
it was a little bit of an issue we
started running into scale related
issues because Apache Helix can only
manage up to hundreds of thousands of
resources and when we were managing many
many partitions per cluster we could not
scale it much which means we had to had
one ship on deployment but after cluster
and as a Kaffir clustered view it big
just week end number of data centers q
it became unmanageable and my sequel was
always a bottling so our collector can
only go as fast as it can write to my
sequel
there were other issues as well whenever
you restart a node in chaperone helix
had a lag in telling the state to all
other nodes which means there was there
could be a double counting so our
accuracy was not that great how do we
fix it like just like any other engineer
right we wrote rewrite the whole system
actually not so much so we used most of
the parts that were there so ordinate
libraries were functioning fine
the first easy improvement was instead
of it writing to Kafka let's just write
to database directly and
obviously you don't want hundreds of
thousands of clients writing to database
directly so you put a web service in
between and all it libraries can just
give that message to web service get an
acknowledgement and web service can in
sync right to database next point
pinpoint single point of failure in
collector we exactly we just got rid of
it because everybody is writing to
database so if everybody does the job
that collected also collector has no job
right we did the same thing Kafka
services because they were limited
number of nodes for shopping after
service we can go and directly write to
database next pinpoint was having my
sequel bottleneck we replace the sender
which can take much more throughput and
because Cassandra also has multi DC's
support we you know we got that for free
and then we don't have to replicate data
in different aggregate clusters we can
just let Cassandra do it for us so going
through the same points again Saudi
couple Kafka from our auditing so in
that case we do not have any side
effects of cough cashews we reduce the
delays of multiple staging because we
are directly going to database now we
have like audit libraries in app and web
service app address proxy can go through
the service to write to database and
shuffle sorry directly writing to
database
what rid off collector altogether so no
single point of failure next thing we
did is if you remember helix is was
another thing that I told you about she
got rid of helix so helix is good for
complicated complicated distributed
systems where it can serve as a brain
but we don't really need a brain our
problem was very simple
if we could do a simple hashing of all
the topics that we are reading from
that's all we needed
so what we did is we created a zookeeper
based locking system where if we need
ten workers we could create ten lakhs
and zookeeper and all the workers will
fight for it and whatever lock they can
secure
they'll hash all the topics and figure
out all the topics that they need to
read from and they start reading it this
was pretty awesome
this is simple and this could let us
grow a single cluster to monitor all of
the uber data
another thing is because we moved away
from writing to Kafka model because you
know all our Kafka service is actually
writing into database they can you know
if there is a contention if there's a
double read we can resolve that using by
looking at the database is already check
pointed user and see ok I'm double
reading I'm not gonna checkpoint this
I'll start from the next time again
right so now we have a single sharp on
deployment per data center instead of
tens of deployment per to the center it
just reduced to one which reduces the
operation overhead quite a bit and we
replaced my sequel with Cassandra which
can handle much lot more QPS and multi
DC's out of box we were able to get much
better latencies because now data goes
to like all the auditing data goes to
database much faster than it was
previously
right so what happens if Cassandra was
to go down or if the Shepherd web
service was to go down well that's still
a problem right but guess what we had
already written the code in Chapter one
to write to Kafka so you kept that code
so just in case anything goes down if
the web service were to
the audit libraries would figure out
that and they'll retry it couple of
times and then they'll write it to Kafka
right
same thing with respond see if database
were to go down chaperon web service
would still be up it would take all the
data from rest Roxie and client I would
write it to Kafka and same thing with
chapel service chapel service because it
is doing a it's asynchronous to read you
can just wait for it abyss to come up so
we reused everything that we had to
create it as a backup path in unlikely
scenario of anything going now that
brings me down to comparing version 1
and version 2 so version 1 was good for
what it was but it was not good enough
to scale us to a level further so we
were able to take version 1 to 250
billion messages per day which is
equivalent of 1 trillion Kafka events
oh sorry chaperone events per day and in
case of version 2 we are able to scale
it to 1 more than 1 trillion
messages per day for Kafka which is
equal to 5 trillion
chaperone events earlier we had one
deployment per cough clustered now we
have one deployment per data centers
latencies are much better and it's
easier to operate and we are saving lot
more resources it's much more efficient
so again to summarize its chaperone is
our in-house auditing system it has been
running in production for two years
version one has been in production for
two years version two has been in
production for about nine months
version one is already open source feel
free to try it out if you are using
Kafka we are we also have a blog which
which tells you more about details of
how it was developed and we will be
working on open sourcing version two
whenever we can get some time and again
some credit of this whole design goes to
LinkedIn engineering they published a
blog sent them back and this which
became the inspiration of our chaperone
so not done yet even though we can
handle this much scale we can take
sharpened a bit further and we already
have some prototypes working on this so
this was all about calculating if we had
data moving from one end to another end
how about the duplicates we still don't
handle duplicates very well working on
it we and I'll talk about it whenever
you know it's ready another thing that
we were working on is because we are
looking at each and every message we can
do much more than counting we can look
for interesting patterns like fraud
issues or sensitive data and grading
into where it should not be and we
already have a prototype for that as
well working high that's all I had for
chaperone
and I think I'm little bit early I this
leaves us 10 minutes for questions
yes sir hey so the question here is
again the time difference is between one
stage and another stage so that's why I
said so the way we do it is we embed the
time stamp within the message right and
then we can use that time stamp right
then we can like even though it arrived
a minute later we can still put it into
the old bucket where it belongs
any other question it's a stateless web
service which has multiple hundreds of
nodes actually spin out right and when
you write a message it it stores its
state for example which topic does it
need to write to in zookeeper so it
pulls that whenever a new web service
comes up right it has 18 tracks with
zookeeper figures out where all the
clusters are and then you keep writing
data it knows where your topic belongs
to and just writes over there so it's
still stateless
however it's not completely stateless
there is that state which is maintained
temporarily before it is being written
to Kafka so that's something obviously
that's something I called out yeah any
other questions all right thank you all
for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>