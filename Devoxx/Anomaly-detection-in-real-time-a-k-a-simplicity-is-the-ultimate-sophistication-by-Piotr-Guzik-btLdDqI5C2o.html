<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Anomaly detection in real time a k a  simplicity is the ultimate sophistication by Piotr Guzik | Coder Coacher - Coaching Coders</title><meta content="Anomaly detection in real time a k a  simplicity is the ultimate sophistication by Piotr Guzik - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Anomaly detection in real time a k a  simplicity is the ultimate sophistication by Piotr Guzik</b></h2><h5 class="post__date">2017-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/btLdDqI5C2o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome you all thank you all for coming
I know that this session is going to be
hard because you're just after the lunch
break so I'll make I'll do my best in
order to keep this session interactive
you will see some kind of data flows I
will ask you some kind of questions you
can answer them I wouldn't like to
answer them Maus myself because it's a
good good way of thinking for you how we
achieve this this this talk is going to
be a little bit different than the ones
that we've heard during this conference
mainly because this talk is about war
stories if you do like the stories from
production this is for you if not if you
would like to hear about some more
theoretical things this talk is probably
not so used so I won't be mad if you
leave them because there might be other
more interesting talks for you because
this is mainly about how we achieved our
deployments what kind of algorithm and
what were the problems so first of all
Who am I my name is Peter guzik I'm
working at Allegro which is quite a
fancy company you could go to our booth
today because we are sponsors also I
highly Fable you to do that I'm also a
trainer I perform trainings for the
getting data company this is my Twitter
and this talk is coming from me because
I do have some data science flavor I
always kind of like maths and I will
show you cameras how simple statistics
this is why simplicity is in the topic
of this talk
why simple statistics might work in
quite robust environment so first of all
if we want to start any kind of project
we have to answer the most difficult
question why are we doing it and I hope
that the answer is not that the
management wanted something but that we
had some kind of interest in what we're
doing so this is the first question for
you
nowadays we're living in a world of
applications which are being monitored
and it is rather not the case when the
application fails and the users are
notifying developers that their
application has fallen if you're in this
kind of situation I'm sorry for you but
this is probably not the case in many
companies but what about the data what
do you think do we really treat the data
as the first-class citizen who among you
thinks that the company can survive
without with losing all of its data what
should happen then what if the company
loses their data would it operate the
same probably no so as you can see lose
each of the data is crucial for the
companies but unfortunately not most of
the code very few companies think about
it this way right so that losing the
data is their problem but they believe
that losing the application and a
production leaving some SLA down it's a
huge problem right there are many talks
when people say that you have to be 100
percent uptime 99 and five nines percent
of uptime but not not many people that
will tell you that the data also have
some kind of a service level agreement
and that you should cover the data
monitor if you gather the data with a
good quality and whether you could do
something useful with those data so this
is the first point we believed that we
have to detect some kind of anomalies in
a clickstream because this talk will be
about clickstream do you know what is a
clickstream okay I will explain in a
second so a clickstream is in general
users behavior and users activity that
is being gathered on a web pages when
you visit the page and you buy something
or you look at the recommendations or
you want to change the address of where
the package is going to be sent this is
your activity and most of the companies
do track this kind of activity sorry for
that but we are gathering this kind of
insights as most companies and those are
the crucial data because this is your
behavior we can serve better apps for
you we can do recommendations for you we
can profile our
services better so that you know that
you're using this kind of service which
is personal for you and not for general
people right so in order to perform such
operations we have to treat data as a
first-class citizen and you as a
developer should be the first one to
know that something went wrong as I
explained nowadays developer do know
that the application is not working on
production but very few developers know
that they are not sending data somewhere
or the data is being lost and this is
where the first code come in that
nowadays we're in such a strange
situation because we're engineers don't
forget that and in 20th century
engineers did make a few mistakes if
they build a wall that it could not be
90 percent it could be 88 87 but those
mistakes were general
minor mistakes but in IT world nowadays
engineers make crucial mistakes I'm not
sure have you heard about get up there
drop out of production DB this kind of
mistake was not possible where you had
mechanical engineers build their
builders and other kind of this kind of
classical engineering there were no such
mistakes because everything was counted
measured and everything seems to work
fine
whereas in IT world there are
spectacular mistakes and we have to keep
up we have to protect ourselves from
some such a mistakes so what was our
motivation first of all we have to know
when we lose the data from our
production system so that we cannot
serve better apps recommendations and
other things well we thought that losing
data is somewhat similar to losing the
money because for example if you charge
some kind of companies for displaying
their apps and they are displaying the
ads for the people that don't want to
see them probably you're losing money
right because you do not do what you are
supposed to do and in from the technical
point of view we came to the idea that
this should be a monitoring system so
that the team who is responsible for
coding gathering data from components of
the webpage gets notified when those
data is being lost ok you can think of
it this way you've got the search box
and it is very crucial
what is being what do people search
right what's the most common typos in
searches so that you could fix them
because Polish language is quite
difficult and you could try to fix
something automatically you could think
what the users want from that
perspective so if the search box is
losing the data
we're losing the money and we should
notify the team that's being responsible
for search box immediately that ok your
search box is working it's working
really good but you do not gather the
data so this is not exactly what we want
ok although the service might work it
might not cover all of the data
especially in the micro services
environment where most companies now try
to be there but they do not know that
the data flows in micro services are
really really hard it's not that easy so
that you have one DB under the hood and
you could gather everything especially
when you do something like a click
stream so those are the questions that
we had to answer before starting the
project first of all how to gather the
data where should we gather the data
from and what should we look at them
should it really be a real-time that's
an anomaly detection is it ok if we have
one minute delay or maybe five minutes
delay are still ok because it's a
monitoring right we would like to call
someone on the Beauty from the technical
services that they are observing in
order to fix it so probably this should
be near real-time because you wouldn't
like to call someone in the night after
two minutes of distortion because it
might be a error in measurement but if
the Box is not performing let's say for
10 minutes it's ok you could the
automatic could pick a phone and call
him in order to fix it so near real-time
should be ok and what is the acceptable
delay right because from the moment that
you gather the data are there are late
events the events may occur make the
gate last summer in the network they
might get reconnected so you should also
think more or less about the delay for
how long could you wait for the data
before you check if it's ok and last but
not least the most important domain
question in here what actually is an
anomaly right because there are a lot
of papers a lot of white papers but none
of them really tells you in your kind of
domain what is an anomaly
so some technical point of view we
decided that we're going to use Jewett
IO
who amongst you have heard about druid
ok and today if we have enough time I'll
perform some kind of a demo of a druid
because I believe this kind of
technology will change the way we think
about gathering the data in the future
it's what this druids do it is an all up
engine do you believe that you could
perform all up queries like you know the
data warehousing and all of this heavy
stuff in real time who amongst you
believe that you could set all up
cubes built in real time ok so I hope
we'll manage to get to the demo today so
that I can show you that actually we can
nowadays get a data warehouse working in
a real time and this is what we need it
right because we want to aggregate data
in let's say every 15 minutes right so
that 15 minutes is still fair enough in
order to call someone on duty to fix
their issues right it's acceptable if
you do it eagerly probably there will be
more false positives and this is what we
do not want because we will be calling
up people in the night in order to fix
their systems so false positives are bad
in here we would love a rather walk
would like to be liberal and the way we
work with the other people because it's
people and not the machines and what
should be measured that's also the very
important question before you start
developing such a detector or machine
learning you have to measure some values
that would be evaluated what's the
answer that you want to get how much
it's ok how much is not ok
what's where are the problems what are
the differences that are still
acceptable ok and what you got so the
measure we thought about that we're not
data scientists we're simple engineers
so we want to create simple things not
some fancy algorithm in a minute so we
decided to go with the simple
counts of what we are measuring so what
is the core data in the system that
we're talking about if you could see
this cell Doe SQL code this is more or
less what we want to perform and for
example we're counting how many events
from the webpage were gathered in
specific category and actions of the
users let's sign in some kind of time
window operations here 15 minutes and
example where we're walking on a page
views so that you click new page go to
the another page there refers between
them the links so that we keep tracking
what the users are doing so for example
here in this query we would like to know
how many were how many were the search
boxes interactions and which of them led
to the show item if you do show items
generally the auction the bidding right
the way you go to the product so in
order that you could buy it so this is
the simplest counts that we could think
of and it's a classical OLAP cube query
because it's got a group by clause so
that we're not interested in single
events because single events are not of
interest we only measure some kind of
statistics in order to know if system is
performing okay and this is the first
look at our data and here the question
for you could you see any kind of
anomalies here this is the result of the
query that I showed you few minutes ago
are there any anomalies here there are
no anomaly okay any anybody sees any
kind of anomaly where first and second
day okay to be honest I can tell you
these are free anomalies in here the
first one is in the first day the speak
of the load this probably is some kind
of anomaly because as you can see during
the day time the load is as big as in
the night house in the evening this is
rather unusual because people are at
work okay so this shouldn't look like
this also the second day is a there are
two anomalies one of them is a peak in
the load and the second one
is a dropout not knowing why okay this
this people didn't interact with our
system for a short time a period and
there's a dropouts okay you could see it
here not sure if do you see this okay so
this is how help you human things okay
this is what we believe in when we look
at it and the rest of the curve of the
system looks perfectly fine right so now
that we know the data we have to look at
it once again and answer some simple
questions so please look at it and think
of it how could what is the what can you
get guess from this data is it okay to
compare day to day no why
yes perfectly fine that's the correct
answer you cannot compare day today okay
because people in general uses the
system and weekends
rather than during the working week so
the load is heavier so you cannot
compare Friday to Saturday or Saturday -
Wednesday because those are totally
different numbers what else can you see
here so what is the period is this data
more or less periodical what do you
think yes and what the period many
periods but the smallest unit of time
that is a period in here a week calling
us good intuition and do you see
anything else that might be useful while
building an augur it's anybody okay so I
will show you a few things what we can
guess clickstream is period rather a
periodical system a period is a week
definitely not a day you cannot compare
day to day and last think that you
should notice in here and I will show it
once again please look at the early
morning and the end of the day the curve
is getting its load is getting really
high and really low in a few minutes
right it's a huge drop out and there's a
huge increase in the morning so that you
should be aware that your system should
manage it somehow so that the rapid
growth of the data and the rapid loss of
a data is not a problem okay because it
may it may not be obvious but if you
look at the curve somewhere here so that
there's a drop out and here the data is
being covered rather in a bigger amounts
this should still be okay in our system
okay and it should not be treated as an
anomaly so that there are some big big
impacts in loads so now that we know
basics of the data this is the way that
we have
always worked with data and I encourage
you to do the same look at it try to get
something try to find some things that
are common so that your algorithm ight
be a bit smarter
if you obviously want to create a
handcraft algorithm so we came to the
idea that what should be the best
algorithm and the best code and the
answer is obvious the best code is the
one that you do not have to write so
that it's already there so as a good
engineers we wanted to check if there
are some libraries that could do such
operations as guessing if they if there
are anomalies or not could be taken from
github or somewhere else and obviously
there are some kind of frameworks we
tried the Twitter library which didn't
work out for us because there's a heavy
muff beneath the hood I will repeat it
once again we're not the data scientists
we wanted a simple solution so that the
Twitter library with this heavy map
wasn't okay because we didn't understand
the outputs we wanted a system that's a
monitoring system we want to show it to
the people so that they could look at it
and know in two minutes okay this is
fine this is not fine this is an anomaly
we didn't want it any kind of a
sophisticated system very difficult so
that people should look at and hmm the
system called me in the night I do not
know if it's an anomaly or not because
some kind of neural network system
decided to call me
we didn't wanted it because there's a
people-to-people interaction in there so
we decided that we need something
different and there are very interested
author it's called HTML grits their are
based on neural networks there are a lot
of increase in interest in github using
HTML grits bar they are way too hard
we tried them with for example we
changed the size of the vectors that are
used to train neural networks from 32
bytes to 64 bytes and the results
changed no matter why okay the results
were totally
friends it we didn't understand why so
it was no-go for us because if you do
not understand this system you cannot
maintain it and you cannot say that this
is your system ok so the decision was
that the solution must be easy the
easiest solutions are the best one
that's my belief so that we will create
a simple model and right now we know
some basic facts what the simple model
should be so a perfect model that should
work with the technique anomalies should
be simple this is the most important
part must be time aware because this is
a time series data so it's got to know
something about the time detection
should be performed rather in minutes
done in hours ok because if the system
is not working for an hour is too much
but a minute
might be still ok it should adapt to the
trends so that there are some popular
items not sure if you heard of fitted
spinners
this is a typical trends in anomaly
detection so that at the beginning will
people type it it's an anomaly but after
some time it's a trend ok it's just
trending some kind of apps good ads
makes this situation secure so that
there are some really good ad and people
click up some things and at the
beginning it should be an anomaly but
after a little longer period it's
perfectly ok so that the ad made an
interest and people are looking for
something so it should adapt to trends
it should not report too many false
positives do you understand why ok not
to call people cool that's that that was
our goal because there are our friends
from the company as you know it's kind
of a stupid idea to call your friend
from company in the middle of the night
just because some kind of assistant
decided to and we decided that the
simplest mathematical concepts that
could perform anomalies detection is
called confidence intervals I will show
you in a minute what was the scratch the
draft of
our algorithm and why we decided to go
with it by the way if you are interested
in developing some kind of algorithm
this is the most sophisticated tool that
you can use absolutely the best one we
tried a few but none of them worked as
well as the white board the classical
way of working and it was really good
because we had like hot discussions down
there like in a class we'll back it back
to the high school there was a hot
discussions under the hood and you could
draw something which is not that easy
when you think of coding something in a
system and this is the draft of the
model that we wanted to create so the
Green Line is an actual actual traffic
actual web traffic the yellow line is
the prediction what we expect how it
should behave and the orange line is the
lowest possible amount of traffic that
is still acceptable and blue or teal if
I can tell you because I check how it's
called in English it's a teal color this
one is tells you that it's the highest
amount of load that is being treated as
an acceptable okay though these are the
confidence intervals very simple to look
at if you present it to a typical
developer he may not call it orange and
teal but he'll get it what's the
decision of the algorithm whether it's
an anomaly or not okay so do you still
think that this is simple enough okay so
now we did our first coding I call it
first attempt in learning it's an
acronym it was the first code that was
the statistical model coded in our it
wasn't that bad at all here's the
results it's the red dots are saying
that this is not an anomaly whereas the
blue dots says that it is an anomaly and
it has the first algorithm is not that
bad it did
find out the anomalies on the September
of 26th but there were some issues do
you know where are the issues yes there
are a lot of false alarms in this
growing and rapid growth and rapid loss
of the traffic in the morning which is
the worst case scenario because you the
worst kind of idea is to call someone in
the morning and this algorithm would do
that so we looked at it and the answer
was that it led to overfitting the
biggest problem with machine learning
algorithm you let it when it led to
overfitting so that it is very narrow
measurement errors and it's it's an
overrated algorithm and this algorithm
led to overfitting the trend so that the
rapid growth and impacts led to false
positives but it was our first attempt
in learning and the experiment in
progress we were developing more and
more now we want more now we're sure
that trending is just a little bit
important its trend should not change
the way we think about the data because
there are only short times when trends
appear and stranding data and now
something obvious what about outliers we
thought come on it's our system there
are some kind of retries but there
shouldn't be too many outliers ok
because this is what outliers are
leading for when you come to the
statistics if you do have outliers in
your data you have such a big peaks of
those confidence intervals so that
nothing will get to the anomaly ok this
is very important to get rid of outliers
mainly to the standard deviation problem
if you think of it and reason and we
thankfully to that that we know now that
duplicates are a big problem we could
think why do we have duplicates then it
came obvious if you're working with the
Kafka and HDFS you have to dump data
from casket to HDFS somehow that's our
problem and most of the tools that are
available as a free software do it in a
way of at
least one so that they give you a
guarantee that you will never lose the
data but there might be some duplicates
okay it's something gets stuck you might
get duplicates on the HDFS which is our
storage for the historical data for this
system and this was our case okay so
another fault a great way to deal with
the outliers is to work with the
percentiles 99th percentile of the data
is a great duplication removal okay it's
one of the best things that we have gone
so that the outliers might be for say 2
percent or freedom 97th is still okay
and it will let you get rid of most of
the outliers so that you do not see such
things in your models so now that it
started to work a bit we came to a very
sad conclusion we've coated something in
our and we're not data scientists and
who amongst you know our language the
same was in our team not too many of us
and as we're working in Allegro we have
some tough decisions to make because we
cannot go to the production if we do not
monitor our system if we do not have the
automatic deployment and it was almost
impossible to achieve it with our so
because we didn't know our so well it
was very difficult to deploy it to
monitor it which is very important
monitoring is like one of the core
things that you should do with your
systems so a very sub decision we have
to rewrite what we started from scratch
because now we do have some knowledge
what should be the model and what should
water the data and we should do it in
some kind of a language that is being
acceptable for the engine JVM engineers
we decided to go with the Scala because
most of our code base is written in
Scala so this is the very simple input
that's coming up from druids which i
will show you in a minute do you see
this code or is it too small okay
perfect
so what is the model key okay what is
how the data gets aggregated because
it's important for you to understand our
decision if you look at it please tell
me what's the aggregation model for this
kind of data anyone
counts per day close close close if it's
per day there shouldn't be hour of the
day ins minute of an hour its counts per
minute okay its counts per minute and
these group by clause do you remember it
or shall I come to the slide once again
I will show it once again so that you
understand so the model that's being
coded in Scala is counts per minute
that's being shown in some kinds of
dimensions and dimensions is this
category action and a minute okay those
category and action here are dimensions
for example there will be multiple you
know the Cartesian thing from those
dimensions for example or searches and
show item these are the pairs of the
dimensions that we're talking about
clear right now more or less what's that
perfect so this is the very simple input
to our system and generally speaking
this is the whole model not too much as
you can think of it and now we came to I
will try to show you what the math
behind the hood do you know the email
function have you ever heard about
EEMA okay so now I will show you
something
do you heard have you heard about mean
what is the typical problem with mean
let's look at it because we started with
something very simple like median and
mean and we came to this subproblem can
you see what might go wrong if you use
very simple metrics over the time for
the time series data in such a situation
that this is what the data looked like
okay can you see something remember
this data is okay there are no anomalies
in here okay this data were perfectly
fine but can you guess what might go
wrong so the problem is that if your
algorithm learns how it should work in
this period of time and then it gets
operated in this period of time
everything will be treated as an anomaly
right the simplest map like mean or
median cannot cope with it but there are
some nice mathematical functions one of
them is EEMA which is very simple and I
will explain it in a second it's called
exponential mean average okay if you
would like to read about it there are
nice video tutorials on YouTube it's
what bankers love because it's the way
that it's calculating the mean in some
kind of a time window okay this window
is sliding over the data and it's got
the head up in the code
okay please look at the email function
it's take the current think that it's
calculated the newest value and how big
is the impact of the new value based the
given value okay so that it forgets the
most important things with functions
that are operating in a window is that
the fact that they must forget what
they've already learned because the data
might change over the time if you want
to perform streaming machine learning I
repeat once again streaming not
classical but machine learning where you
have training sets and other data sets
that streaming way you have to forget
what you have already learned in order
to perform
all the time okay and email function
takes the weight so that the newest
samples are important and the older
samples should be forgotten okay because
if we think from the business
perspective this is exactly what we want
it is more important for us how many
users are searching something on our
platform right now then how many users
were searching on our platform week ago
okay we should think how many users
we're there one week ago but for example
this should be 10 percent of our model
okay and how many users were looking for
something two weeks ago should be for
example 1% okay because this is a far
far history it shouldn't be the most
important thing in an algorithm so this
is the X potential means that the new
data are important are more and more
important where the data that comes from
the history are getting less and less
important okay have you got some kind of
idea what I've talked about ok cool
enough so we decided to go with the
email this is the answer why because we
want to roll ok we want to roll so that
if that load is getting heavier it will
get into this time window and when it's
there those samples will get important
ok those will get less so that for short
time it will be treated as an anomaly
but in longer periods it will look
perfectly fine as a trend ok
and we've coded it this is more or less
the whole algorithm for anomaly
detection we're getting the real value
we're creating a model will calculating
the prediction we're calculating the
lower and upper bounds okay you know
what is the lower upper bound these two
lines and then we got the mean of the
model so that we know how in the future
to cut this exponential mean average and
the decision is very simple an anomaly
is whether the data is below the lower
bound or higher than the upper bound so
it's really really simple out
but we didn't come up with the last
problem we deployed it to production to
the huge success but we missed something
and we couldn't invent it
that's Iowa that's why I am carry highly
encourage you to go to the production as
quickly as possible because only when
you are in a production you will see
some real behaviors not the one that you
only thought about and this is one of
them
there are deployments people do deploy
the new versions of their components and
one of the deployments made such impact
on the system that their loads as you
can see got bigger and it stayed that
way okay and our algorithm wasn't able
to cope with those anomalies because it
couldn't learn animal is okay if you
want to detect anomalies you cannot look
at them and learn that those anomalies
are in your data set you have to
eliminate them and mark them as
anomalies so our system wasn't able to
do that because we didn't think of it
that there might be some heavy
deployments that people will love and
use those components twice the same as
they used to okay but it happens in a
real time and we have to manage it so
here you've got the nice graphs under
below that this is not an anomaly no no
no no no and here we've got definitely
an anomaly okay this is what the
computer see and these tress there are
thresholds whether to call someone or
not so the last idea behind this
algorithm we should go with the
probability that this is the anomaly
because that's what the people do okay
you look at something and say this is an
anomaly and I can tell you that this is
an anomaly with 99% of sure we should
call someone then but if you look at it
this might be an anomaly but I'm only
thirty percent sure that this is an
anomaly you better leave it this way
okay and that's what our our algorithm
and what's important it will sign
because when people click more
they used to it's okay it's still an
anomaly and someone should look at it
whether there are duplicates and data or
something like that but it's okay but we
should be extremely cautious when it
comes to the drops in data because this
is rather important for monitoring
whether the data gets lost okay and if
you got the anomaly with the sign so
that minus one means that there are no
data it's better way to cope with it
because then you could have stricter
policies on what to do if you're losing
data and what to do if you gained
English okay this is the reason so we
fix that we decided that there should be
a longer window that's calculating for
how long and I know an anomaly that is
lasting is becoming something normal
very simple algorithm why because if the
team that is responsible for some kind
of Box gets notified and they are to
have the SLA for example for three hours
they have three hours in order to fix
something and if our algorithm for
example says that after five hours of
lasting anomaly this is becoming a norm
it's perfectly okay because they have
only three hours we're still liberal
we've given them five hours if they
cannot fix something in five hours we
decide that this is this is actually
what is happening right now and this is
the truth okay so our system is quite
intelligent in this way that it takes
how much the team got to fix something
and it's liberal so that if they do not
after long lasting can normally occurs
we're deciding that this is not an
anomaly anymore okay because that's what
people do if you look at it it's more or
less what people think right now there
are no anomalies here something has
started and it lasted for a long time no
one has fixed it so after some time you
should bump up you should come up to
this level and say okay from now on
someone has deployed something and this
is the new standard I should forget what
I've already learned this is the most
important takeaway from the streaming
machine learning now I'm forgetting what
they have already known
and I'm starting from this point once
again there are new levels okay it's can
you understand what standard thanks
still simple yes okay multiplying model
parents is a technical think in general
you forget what you have already learned
because now you know that the
measurements that were here were not
enough you have to multiply them by some
kind of constant for example because
this is a linear okay this this should
be simple as I told you so this is
linear ball multiplying you see that all
those lines bumped up to some kind of
level okay this is fair enough if you
think of simple counts because there's a
new level of something that shouldn't be
anamoly anymore okay and the deployments
we decided to go with a soft model so
that everyone could deploy the same code
base but with different configurations
okay so the code was written once but
multiple teams have different
configurations of letting them know if
something happens I will show you in a
minute how it's done it's more like this
so that there are a lot of parameters
but there are quite obvious now if you
think what I told you for example Mac's
anomaly periods what do you think what's
that
okay perfectly that's the correct
definition and there are other as you
can see lower bound some kind of a
mathematical exponent so that we can
calculate whether it's an anomaly or no
those there are not too many parameters
to tune to be honest there are five
parameters that people should know what
they are putting and to compare to the
Twitter library whereas there are 32
parameters I believe this is okay we
have five parameters the simplest
library that enables you to do so have
32 parameters so I believe we're a bit
better and we and if you look at the
query so that what the team is looking
at if you look at this board does it
call a bit ringabel somewhere have you
seen something like this today
anyone I have showed some kind of cell
to SQL and this query is more or less
coded here okay
nobody is coding SQL queries in this
system they are only declaratively
saying that they are interested in
service ID category induction especially
two categories listing and search and
actions some kind of listing items
possible show items and the metric that
is being used should be the event count
okay declarative simple way for
developers to decide what they should
monitor it's mapping 1 to 1 to the third
SQL I've shown you before and generally
speaking that's the basic concept behind
this audit that was more model machine
learning stuff without some very fancy
algorithm that the simple way that
should work I hope that you can
understand it in a while now well I am
obvious I like this slide because I
believe it's important say thank you for
people who helped me working with this
algorithm our staff that's I have an
opportunity so I do say thank for the
people who work with us helped us and
now I can guess I still have some time
so I can show you more technical stuff
how we did it from the
technical point of view from the Druids
what is the druid and after that there
will be a maybe a small demo if I have
time and Q&amp;amp;A okay so that's the end of
the algorithm I believe that the
algorithm was quite easy and it's
working for school and now the database
behind it okay
we decided to go with the druid io it's
a quite simple tool I loved the way that
it was developed because there's a
company in u.s. called Metamarkets and
they did a very pure business on how
them ads are performing for google and
other things and they did coded some
kind of an OLAP cube engine and they
decided that it's so good that they will
open source it although it's their
internal tool and they did it it's open
source for three years and it's a really
nice library it's bottled proven because
they were using it on a production for
the scale of the Google Apps so it was
really battle proven and for big queries
and for big data it was well suited and
what's cool it was open source just
because nobody thought that it's a sell
and forget issue for our company but
they wanted to share it and that's why I
put this slide because it's a nice thing
to do from the company what it is what
it's not and what are the pros and cons
of this tool it's lightning fast I will
show you it in a minute how it's really
blazingly fast in the million of records
and I will explain it why in a second
it's very stable
it's almost rock-solid it's working with
us for a half a year and it never
dropped so it's a rock-solid system I
can tell you it's a generic tool it will
accept all of the data as long as you
provide some JSON configuration about
what you want to read I will also show
it to you and it's got a lamp book built
in lambda architecture so that you could
combine real-time results with
historical results it's also cool
feature the only cons that is here it's
got well quite sophisticated and
difficult architecture but it's
rock-solid and the configuration of what
you want to ingest from your data must
be Noah
Proms you cannot decide in a run-time
what you want you have to put it in
adjacent piles before starting okay so
this is one of the cons of this system
and now some general concepts are you
familiar with something called you roll
up this is how it worked because we're
getting one millions of events per day
so as you can think of it if we were to
process one millions of events in our
anomaly detection service it probably
wouldn't work that fast but thanks to
the Druids and the way it's working with
a rollup we only operate let's say on
ten thousands of rows because it's
performing aggregation at the ingestion
time okay this is very important and
this is very smart so please look at it
here we've got the timestamps the red
columns are the dimensions so the
business metrics some business stuff
that you want to look at and the blue
ones are metrics so what you have
gathered for example those are the
clicks on the US zero indicates that
there was only the ad was shown that it
wasn't clicked whereas one indicates
that someone clicked on the app and what
is the revenue what's the price for the
click for clay okay and if you perform
this query so that you're grouping by
all of the dimensions and supposedly
count something glass impression is
everything zero maintenance impression
whereas clicks are only where there was
one and the revenue of this plank and
app is the sum of all prices with clicks
okay
quite simple then you got a rollup after
such group buy queries you do not have
one million of rows anymore you're
operating on an aggregate and this is
the way that drew it works so that now
you can see that during at this
timestamp this kind of business
combination values males in USA that
were advertised by Google had seen in
this one minute
180 to 1800
impressions there were only 25 clicks
and the website should get $25 for that
okay this is the business value that's
being aggregated from the row events
that's our situation because we're
gathering grow events from the click
stream and we're really interested in
some kind of business metrics so
aggregates so this is the roll it ring
rolling up the important thing once
again to understand that this lets you
have 100 maybe 1000 less number of
records okay this is cool stuff if you
really have big data so the rule of time
behind the druid and general systems big
data is cool but the smaller data the
foster system
if the data is small enough and covers
the some business value it's perfectly
fine so making data small again should
be one of your concerns if you really
want to perform operative in a quick way
responsive manner and that's what the
Druids really do under the hood so as I
mentioned the data gets aggregated at
the ingestion time you are losing CRO
records okay you it is impossible from
the druid database to read the row
records that this guy clicked it at this
timestamp you will never get it
only what you get is that the typical
user at this website click this that
this number of apps and it should be
paid this amount of money okay you get
the aggregate that's a different kind of
thinking and some kind of something
that's not really fine but it cannot
work any any differently you can only
query things that you have calculated
okay if you calculate some aggregate
then you only can calculate the
dimensions or measures that are already
there you cannot create something new
from the scratch and that's how you
query the data you got the group by
clause and you can group by as many
columns as you want and this is one
thing you can perform top end queries do
you know what the top end query is the
simplest group by by one dimension and
drew it is really smart
if you do have a group by query for
example from three columns it can
perform three top and queries and
combine the results so that you know one
big query with three columns in a group
by might take 20 seconds whereas free
top and queries one each of them might
take three seconds so you are two times
faster okay and if the round in parallel
you are even faster and faster than
typical group by query and the simplest
query is a time series query so that
you're not interested in anything that's
grouped but only the data that occurred
in specific time stamps and there are no
joints you cannot join the data inside
through it the only way to join the data
is to create an OLAP cube that has
already joined data from the external
systems okay that's why the queries are
so fast because there are no joints but
you could live with it because you have
to ingest the data somehow into the
Druids and the way to do it is probably
to create some kind of application it
has a complex architecture but this
architecture is rock solid you are
hearing more and more about
microservices and drew it has one thing
that's common with microservices that
each new type of node has single
responsibility okay that's how it looks
like the last slide with the theory and
a quick demo so it's a lumped
architecture you have the real-time
nodes and you have the data that are
stored somewhere and given by the
historical notes so there are two types
of notes real time the real time notes
and historical notes there is a metadata
coming around the cluster saying what
kind of data are being ingested what are
the dimensions what were the last query
for those data and this is kept in the
coordinator notes and the only notes
that the client sees are the broker
notes so this is a gateway to query the
data from the druid it is quite
complicated architecture but what's
really nice from it is that if some of
the notes comes down the cluster is
still operating okay
it happens for example real time notes
may fail
but if the real-time notes fail and drew
it then you won't be able to ingest data
from Kafka but it will be perfectly fine
to read the data that was already
ingested and to ingest data from HDFS
probably so this kind of design is quite
cool and right now I would like to show
you how you can work in a druid and
simplest fashion so I will start a druid
application right here as you can see
all of those nodes are being run at the
moment then you should run some kind of
a Kafka under the hood if you want to
load the data into the druid so that the
Kafka is for the real-time component
okay now it's starting now I was far
away now I will start some simple
application that I've written so that
you could see that I am loading the data
from the batch and from real-time ok the
app is running so right now we can go to
some systems this will be our UI this
will be the admin console for our
cluster this will be the second admin
console so that we know how the cluster
is operating and what it's doing at the
moment now I will execute my batch
application ok we have generated a batch
data and right now I will try to load
this batch data into the druid it will
take a while but it should work
perfectly fine as for the demo so what
I've already done
I sorry for that I can run a cluster so
this is starting the indexing of the
data from the batch from the file it
could be a local file it could be HDFS
file and as if you look at the console
right now this is cool it's got nice
monitoring the task is running in here
you could see what the status what are
the dimensions of your data that's being
loaded what are your metrics from where
do you want to load it so it's really
nice for monitoring purposes okay
it's finished it's successful now
probably we will have to wait about one
minute before it's loaded into the
cluster sorry for that but it's not that
good machine and it's not actually a
cluster because it's only my local
machine but after one minute it will
occur in here and you will see that then
we can add our data to some kind of UI
and the UI is blazingly fast okay if you
want to work with the Drouet you will
love this kind of UI because they are
working fast okay so now we have our
data source it's already in there you
could go in here see what are the
dimensions what are the metrics and
please look at it the file the segment
of the data inside the Drouet the
aggregates are more or less one megabyte
okay you can see it whereas if we go to
the file that I've generated it weighs
thirty three megabytes okay so the same
data after aggregation waits thirty
three times less cool I believe it's
fine making data small again remember so
now we've got the batch data now we can
load them into the UI you could see
something similar
okay and now these are the million
records under the hood this is how you
can say that let's look for example at
the last week of the data and this data
is about shopping so we can look how
different types of shops we're selling
things and we're a typical businessman
we're interested in total number of salt
items that calculates to the money and
for example you can look at the line
charts how it look it's really blazingly
fast for the million records under the
hood that being aggregated into
thousands of records okay it's live demo
now it's still working so cool and
what's even more if you do not you're
not interested in looking at the time
line then you might take for example
or let's say the category of the shops
and then once again shop names and you
can look how the different categories
are being sold inside the shop now
different types of shops okay
totally cool for me because what you
really want to look at is a heat map so
that for example if you look what
different stores are selling you have
the combinations right so that this kind
of category is being sold more there are
more salt from this in this type of
shops okay this is what typically
analysts wants to get from their SQL
queries and here you can simply click it
so last but not least I would like to
generate some real-time data this
endpoint is producing data to Kafka and
I will show you that after let's say a
few seconds you have real-time indexing
service in here the data is being right
now pulled by the druid from the Capcom
and if we look at the last hour of the
data for example let's split it by the
time that's the data from the CAPTCHA
that I've already produced okay if you
look at it at the last minute of the
data now they are here let's wait for
example for a second now I'm not
producing any data so we will have no
data at all
and now we can produce data once again
dump them to the Kafka might take for
example a few seconds but you should
have data somewhere in here if you look
at the biggest period okay they are
already being produced from the real
time from the Kafka so you could dump
the data inside two things you could
look at the data at different
perspectives and what's really really
cool and why this tool is so good you
could split different things for example
if you have the category of things that
being sold and the day of the week it's
obvious that some things are being sold
better at different days right if you're
interested in
and the hierarchy of things that the
category is the main and day of the week
is the seconds okay but you could swap
it and if you think of it that this swap
took two seconds okay it took no time
now you can look at it from a different
perspective and if you're interested how
the things are being solved in a weekend
it's cool
people buy alcohol during the weekend
it's perfectly fine you could see it
from one click you do not need to know
any fancy SQL and it's nicely to monitor
you could see what's being run there on
you could see that it's something yes
you could know what the cluster is doing
you know that your data sources are in
here how much of the memory do they take
it's really really nice software I hope
that you enjoy it and now I will be
honored if you have some questions I
will try to answer some of them so thank
you for the presentation and now it's
your time to ask me something
when source you ice and there are okay
the UI that I've provided you is a trial
it's not very expensive software but
it's druid this open-source platform and
there are open source you is this one is
based on a license that generally
speaking it has an open-source version
it's not a problem
PBOT P what the name is very good
do you know pivottables more or less I
think I did during the live demo was
people think something's and that's that
that's what most analysts do in
companies and it's really blazingly fast
from Airbnb and what's the name super
set yes we were using super set super
set is also an open source UI but the
super set is not that good at real time
super set is cool if you want to look at
the historical data because it's got
better visualizations of let's say bars
and splits but its pivot is better for
real-time data accessing and super set
is cool if you want to have static
dashboards typical reports for business
right so that this shares 20% of the
data this takes 40 it's cool and yes
we're using super set that's the second
UI
it's not open sourced yet it's very
simple algorithm it may go to open
source one day I can tell you when we
probably will try to do something about
it but unfortunately not yet but I hope
that you can agree with me that it's
quite simple yet working as I've showed
you there are no magic in there which is
a rare case in anomaly detection order
it's because most of them do have some
magic map under the hood or neural
networks but the second thing that's
very popular but not so easy any other
questions okay it's not a problem in
Druitt right we have two data centers at
Allegro it is very important to know
that the only problem with two data
centers using druid is when you think
about the real-time technology because
there are two ways to import data from
the real-time to the Druids one of them
is based on Kafka and Component constant
ility that was run by me and it supports
two data centers and the second one
which is faster and generally better
it's called Kafka indexing service and
the only flow that the test is does not
support multiple data centers so
depending on what you want you have two
options
we're using tranquility components it's
the older one more duplicated but the
killer feature for us is what you've
asked a very good question that if does
it supports multiple data centers yes it
does to be honest I can show you why if
you want to look at it here these are
the times and the periods of the
segments of the data as I drew it right
here and the tranquillity what it does
it merges from multiple Kafka clusters
one segment okay whereas Kafka indexing
service is
faster yet number because it will
replace them so if you have multiple
Kafka clusters it will replace one with
the another so it's not working general
in molesey bc but
tranquility works perfectly fine i
highly encourage you to read some druid
dogs it's very quite difficult to be
honest because it's not that popular
framework yet the dogs are not that bad
okay you could read reason about it it's
it's up it's already there if you're
interested more and more and its allies
comments are there people are committing
so someone is still using it and i
highly encourage you to give it a try if
you really really have thousands of
records and want to aggregate them in
order to perform faster any other
question
okay this is the more difficult inside
question to be honest the way to cope
with such things is first of all to
check how your component is behaving if
it's producing it duplicates we have
this kind of metrics if the data gets
duplicated because every row that we're
producing has some unique IDs so if you
calculate the counts and the unique
count of those values you will already
know if you do have a duplicates so for
example this is one way to go and the
second is to check obviously if your
system is alive because then you might
produce less and less data or if you do
if you have deployed the application
let's say and 16 nodes of a cluster
let's say to check how many of those
nodes are still alive because maybe load
balancing is not working perfectly fine
there are multiple ways to deal with it
first of all all check uniqueness then
check how many deployments are life from
the cluster and if the load balancing is
still okay that should do the trick any
other question
it looks like this our system works this
way we're gathering the data to Kafka
and it's where the data is but after
some time this data gets dumped into
HDFS and these are the historical data
there might be duplicates there but
those are more or less like that HDFS is
more like could be treated like a data
base okay so these are historical data
the one that will be loaded to the druid
as i did it from the file okay this is
it and the second part is the new data
that are not yet dumped the HDFS but are
only on Kafka
very good question the answer is that we
do and it's a killer feature of druids
that I haven't mentioned so that you
could replace the data from the real
time with the data from the HDFS it's
good because you could change for
example the query granularity yeah why
because if you analyze real time data
from the Kafka you might be interested
in what users do per second but for
example if the one week is away with one
week after that we could dump the same
data and replace them from the HDFS
loading them in such a manner that users
can only query it for example what users
it in 30 minutes you know with less
grana query granularity so that these
data weights less are fast loader faster
and are more aggregated
it's even simpler you've got asked as I
can show you if you do not even have to
check it because it's here you've got
the data source and for example as long
as you load something that is like ID
it's not a problem but a very good
question yes jus it enables you to work
with multiple query granularity so that
you could treat batch data in different
way that the real-time data okay it's
also fancy because as I mentioned you
could look at the data from the batch
processing for example in one-hour
granularity whereas if you really
interested what's going on in your
system per second you could use it in
real-time really cool feature but very
nice question I liked it a lot thank you
for that ok so if it's ok thank you very
much I hope nobody's left after lunch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>