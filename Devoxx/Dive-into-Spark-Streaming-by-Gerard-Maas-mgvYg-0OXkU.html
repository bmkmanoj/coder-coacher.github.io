<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dive into Spark Streaming by Gerard Maas | Coder Coacher - Coaching Coders</title><meta content="Dive into Spark Streaming by Gerard Maas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dive into Spark Streaming by Gerard Maas</b></h2><h5 class="post__date">2015-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mgvYg-0OXkU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right good afternoon I hope you all
had a good lunch and are ready and
prepared full of coffee for our session
of this afternoon on SPARC streaming
I'm Jared masse I work for ver data it's
a start-up where we are building up a
managed cloud service for the Internet
of Things so we connect devices to the
cloud and we can collect analyze and
deliver data from those devices at a
scale here they are in ver data I am the
team leader technical lead for the data
processing team and in your team we we
are building up those pipelines to take
the data out from the devices and
process it and create value from it in
we have been building up this on top of
Kafka a spark and spark streaming and
what I'm going to present you today is a
part of what is a spark streaming and
what are some of the tips and tricks
that we have learned along the road
that's about me I would like to first of
all learn a little bit about yourself if
you could take your mobile phones out
and tweet few words about you so what
I'm interested in is some hashtags like
that that's me for example a spark
scholar guy if you please start it up
with the stream box hashtag so that I
can collect that information and we are
going to jump into the first spark
streaming application so come on start
voting quickly so maybe I used put it
back a stream box very important
otherwise I cannot ask that from the
Twitter stream and a few hashtags about
what are your interests we have here a
Twitter stream running and capturing
that information trying to count what
are the please use hashtags so
spacewalker the hello world won't work
because does not have hashtags all right
so we have four big data
let's few of them more comp and I'm
going to
explain later on how this can be
achieved using spiked streaming so now
we are currently capturing the the
Twitter stream and I'm running Windows
on top of of the vote stream so I'm
calculating votes out of the inputs are
providing me and we're calculating the
results from that more on that later on
so we are on Big Data Java
okay and we have some Android and ball
that come to in promotion okay
so maybe we can get started so Scala
there on number one you can let them
come I will continue with my
presentation but we will come back with
to do this a bit later on all right
let's get started
streams why you should care and I'm
going to get started with giving you a
and let's say that the big picture first
and then we are going to zoom in very
quickly into into spark streaming what
is Big Data let's start with this
rhetorical question so it's a hunter
bites big data or it is five megabytes
big data it seems like a silly questions
of course and you will be biased by the
answer but then when the data starts
coming and flowing you will see that
your your thinking of what big data is
will change and analyzing one terabyte
of data is very different from analyzing
five megabytes of a hundred megabytes of
data at once so the streaming approach
will let you inspect the data as it
flows through the system as opposed to
as when is stored on the system and it
will provide you the means to react much
faster to make your businesses actually
gather data all in real time on whatever
is going on and then react to it
one typical application of this is to
collect important bits and pieces of the
data store the data and analyze it
afterwards
and so that you have like the best of
both worlds you can have quick small
triggers very important for your
business and
you can run large machine learning
applications or analyze analytics on top
of your data later on today I'm going to
focus on spikes streaming their several
streaming frameworks for big data
available out there like some storm and
spark swimming is one of them and I'm
going to give you insights on how it
works and why you should adopt it first
maybe hands up who was familiar with
spark all right like third of the
audience a bit less than the third I
will very quickly introduce spark to
level the ground for everybody and it's
very important to understand used to
elements of a spark in order to grass
but a spark streaming a spark stream a
source Apache spark a very successful
open source project that delivers a
framework for parallel computing it's a
very generic framework for parallel
computing it has a very functional API
as you can see there in the middle box
that lets you express transformations on
data using very familiar functional
paradigm so you can take take away your
large data set and map and filter it and
reduce it and implement like let's say
they also cool Map Reduce in very in a
very concise way and in a way that can
be taken to different places of the
cluster and executed over there it's a
very fast-growing ecosystem a spark has
a number of projects already going on
there have a increase in adoption like a
spar sequel and the data frames API on
top of it then we have a ml lipfird all
the machine learning algorithms you can
use out of the box and apply it to your
application and graphics that allows you
to do big data graph analytics analytics
some graphs on the large and then we
have spark streaming and spark streaming
reuses spark to enable a streaming
applications and used to give you this
bit of information of a spark because I
will be talking about early days
most of the time during my presentation
it's very important you have an idea
what what they are so resilient
distributed datasets it's a mouthful to
say this is the distributed collection
for those of you I saw a scholar very
high on the a post chart we have we have
let's say local collections like a list
and a set you can apply operations to D
you can do map flatmap and you get a
result they're a very natural evolution
of that was a parallel collection so you
can do a parallel application on a list
and then there will be on several ports
and to execute again parallel and give
you result faster release give you the
next step in that evolution it provides
you means to operate on a collection
that is distributed among the cluster so
early D is like that say the unit of
computation in a spark that enables this
distributed computing framework where
you have a pieces of data distributed a
lot among several nodes in a cluster and
you can express what you want to do with
that data with a very simple API then
spark will take care of taking those
functions you want to apply this map
flatmap filter and many other more
complex operations and send them to the
cluster operate there and give you back
a result one important aspect of LEDs is
that they are Anu table is once an LED
is created it it won't be changing
anymore so you can derive and reuse the
LEDs without being concerned with
problems like YouTube bility like
changes in the data under your feet and
the caching behavior is controllable
this is to say that instant in spark
world one of the let's say the highlight
is that it's very memory intensive so it
does not run on memory all the time but
it's very yeah it's very it has a high
affinity with memory so if you can and
have a large amount of memory in your
cluster you can make optimal use of it
and you can keep the data set let's say
on floating memory operate over it and
achieve a hype
for messenger operations very fast spark
streaming let's jump on it already and
it's a framework built on top of a spark
that takes streams of data so we have
there at the left side the providers we
have already out-of-the-box a consumers
for Kafka for flume for kinases for s3
buckets you can read from its DFS and
you can create your own there is an API
available as well where if you want to
to listen to a specific very let's say
in-house developed protocol and you can
plug into this and put it into spec
streaming I have heard for example
people hooking into replication cues
from databases in order to listen to the
replication between databases in the
cluster and get that information to
spark streaming for let's say a site
processing of that data and then you
will define operations on the data and
you will want to do something with it so
on the other side you have output for
databases you can write directly to HDFS
you can create like say websocket
servers and have an application listen
to that and get immediate responses
that's the case of my demo application
where the charts is just a web socket
listener on the spark streaming
application and you can publish to
streams to be like consumed again by an
order streaming application or by
another spark streaming application in
more particular case let's let's have a
look how it works
spark streaming is said to be a discrete
streaming processing framework what does
it mean we are creating discretized
stream we are doing micro batching we
are not taking every element that
incoming stream and passing it through
our probe by plane and getting something
out we are going to take the data as it
comes in and in time-based intervals
we're going to cut chunks of it and
that's what we call a micro batch and
every micro batch will be an RDD as I as
I explained before
these are very important to understand
to to follow the the spark streaming a
way of programming and execution on
those rdd's we can express
transformations like we can do in a
spark but those transformations will be
executed to egg every single micro batch
that comes into the system that's what
creates that streaming processing
framework and then actions will allow
you to get from those transformations
into a specific application as I
mentioned before you can have WebSocket
you can have the databases or you can
have an order order streams here
actually it's the choice is yours
you have the API to code whatever you
want on the transformation side we have
a number of let a built-in streaming
operations at transformations they take
one stream and create a new string and
then we have like the typical transform
my element transformations like map flat
matte and filter those are like the
bread and butter of every functional
streaming application you take the shape
of an element's and you change the shape
you remove some or you create more and
reduce those so there will be map
flatmap and filter but then we have
orders that are changing the internals
of that stream and what i'm representing
there are not elements but our
partitions and partitions are the pieces
of data distributed across in your
cluster it's a distributed framework so
all the data is distributed in the
cluster and not on one single point at
time so count reduce and convent value
and reduce very value will allow you to
take a view on all that data and create
a new stream that says oh there were so
many elements in your stream at that
point in time maybe if I go back here
you see the counts there and now or your
application went back to zero but the
count there was precisely account how
many votes came in in the meantime and
now that we are in presentation mode
amuse going
very quickly to stop this application
before it consumes all my resources all
right cool so Union join a KU group co
group sorry it takes two two strains and
creates a new stream out of it Union
just put them together join a operates
and key value pairs and so your stream
should be already prepared as a key
value pair of stream of key value pairs
elements and join will put those that
match together and will give you a new
stream of those elements matched and
cuoco group it will do something similar
to to join but instead of finding one
single element will find all the
elements that belong to the same key
from both streams for every interval so
what is important here to understand is
that those operations are operating on
the view of chunk of data of a micro
batch micro batches are typically on the
second and so you will have a one second
two second five second ten seconds
intervals the choice is yours and those
operations are executed on those
intervals every time non-stop transform
it's a very important operation I put it
separately because of that because
allows you to take an existing led so
from a spark you could for example read
some data in in this case I'm giving you
there a example where we have priorities
for given devices that we know we read
those priorities from Cassandra and we
join those priorities with a stream of
data so we are enriching the stream and
transforma let you during a static data
with the dynamic stream data coming in
through spark streaming very important
because it will increase the
possibilities for you to interpret the
data to apply certain things that you
have learned already and put it on your
streaming processing to let you apply
that on the on the data as it flows
through the system data in flight as I
call it I know the update state by key
is something that will let you do a
stateful stream processing so over
stream
you will be able to keep some
information about some key values so you
will have a key there that you can
update as data comes in let's say for
example you have a certain amount of
customers and you would like to track
the clicks of customers over time not
only on the limitations or on the on the
boundaries of a milk micro batch
interval so you can do this with updates
stay by key take some kind of complex
function would I even need to look it up
but it will let you implement those
kinds of functions then action so I was
telling that LEDs are immutable and one
important element is that also they are
lazy so all the operations that you
execute on LEDs that you execute on
these streams are completely lazy in the
case of a spark is when you call an
action on those on those transformations
you will get a result in the case of a
spark streaming it's a little bit
different actions are needed to
materialize the streams so you can say I
take my stream I do flat flat map map
filter and then count the stuff but if I
don't say for example I want to print
this out or I want to save it I want to
put it on a database or I want to do
this for each led operation nothing will
happen
they simply the transformations they
don't get scheduled so actions for these
streams are very important because tells
the SPARC scheduler please take whatever
is happening on this stream and schedule
it at regular intervals thus these
triggers actually the computation for
for the D strings for each RDD it
deserves a special mention in your
presentation because it's the door to
all order SPARC goodies that are out
there is for each RDD on a d stream give
you access to the under line LED that
otherwise is transparent for you and you
you say the stream dot map and that will
happen on the release underly underline
it but it you will not see it for H or
DD gives you their DD so you have your
lydiana closure and then you can apply
sparks equal you can register it as a
table you can here for example use
classification algorithm and say
those elements of my led a or me you can
use data frames to the apply operations
on it data frames are very powerful and
worth another session in Indy bucks and
our graphics they just store two
databases for each RTD is also tricky
because what happens on for each led
what looks like a very logic block of
code and very simple to go through and
understand is actually split out and
executed in different places on let's
say on a spark the first let's wrap you
stumble up is a sterilization on spike
streaming the first rock you will hit or
the wall you will face is where things
are running and here we have an
application that gets alternatives from
a rest server and use those alternatives
to filter my incoming led and create
records out of it it connects to a
database and then for each elements also
partitions are distributed on the
cluster for each partition there will
insert the elements on the database it
looks all very logical and it actually
is inspired in how do you say real-world
events what happens here is that all the
these stream operations are executed on
the driver had executed on one node in
the cluster the one that lets you a
command and execute what happens with
spark streaming on a cluster that
connection that that is establishing a
connection to a database is executed on
the driver but the insertion of elements
is happening is happening on a worker
they are happening on different
happening on different machines when
this will run nothing will happen you
will have no pointer exceptions or
something similar because that
connection information is not known at
the place where it's used so this is
very important I give it to you if you
want to try a spark streaming a thinking
where my code is running
is very important to have a successful
let's say proof concepts deployment and
production pipeline you will face it
sooner or later a regardless of your
mileage and in this case it's very
simple you need to move whatever is
creating something a stateful that needs
to be located on the same virtual
machine on the same VM the same executor
you need to put it on the for each
partition on the let's say the closures
that apply to LEDs as records that we
see is an LED derived from the from the
first 30 B of that this training one our
important function on streams are
windows and a spark streaming offers you
a sliding window functionality it's very
simple so we have a number of primitives
that lets you derive at this trim from
an order so with let's say we have a
these streamers coming every one second
and you want to view the data over there
let's say typical application of this is
a moving average I want to move in
average over 20 30 seconds of my data
that's coming every second so you will
define there a window of size and how
often you want to see that window in
that case we have a window size of six
and a slight interval of three so every
three time units in this case seconds we
will see a new a new bunch of data that
is grouped from the original of this
stream so we have their original de
stream in numerals 1 12 14 and you will
see the first window interval is 1 to 6
the second one is 4 to 9 and then the
third 27 to 12 and so on this will
provide you means to see the data on a
more coarse granularity one very trivial
application of this but very useful is
to increase the granularity of the
amount of data you see on these strings
you have for example are saved to this
function on spark streaming but if you
are going to save every stream every
second we are going to explode the
number of files that you're going to
create so you want when you want to move
things to this for example you want to
create larger
chunks of data that are not overlapping
so you will use a window that where the
window length is equal to the slight
interval I like we have here we have a
number of operations there as you saw
the operations before those are just
defined over windows or you will have
the same count reduce and reduce by key
operations a but now on those larger
views of data that's with more data
within the oddity a very curious
operation defined there is the reduced
by key and window that provides you an
inverse function and that inverse
function is there to optimize how you
deal with the operations on that data
every time let's say you have there may
be few megabytes of data and you are
adding just a little bit more every
second but you have a large window you
have maybe let's say a 60 minute window
so you have a large amount of data and
you only have a small date Delta coming
in and out every time so you can have a
function that adds the data but also a
function that removes data from from
that larger window of a food stream and
optimize it that way how you calculate
the result so instead of summing again
let's say 100,000 records you are going
to from that result that you already had
you are going to remove the records that
came in and you're going to add the
records that are I saw your add the
records that came in and remove the
records are going out like in this case
and we have again this this sixth
element window but the previous past
three are removed using the minus
operation and the other two elements are
used are added they're using the plus
operation okay so let's get ready for
for first demonstration on whole spark
streaming application looks like and for
that first demonstration what I want to
do is to go back to our Twitter audience
application now the one that we used at
the beginning I would like to take you
through the code for you to understand
what it does and how a spark streaming
application that we got this larger I'm
using the spark notebook which is a very
handy tool to play with spark and spark
streaming and all the other application
of libraries from spark as well
available on github just jumping over
those dependencies so this typically
what you will have on don't know on SBT
project or a maven we are just putting
it here in line those are the
dependencies that you need in the case
of Twitter we have there the Twitter
library that I've added in order to have
access to that Twitter D string and
those are not let's say in important
parts we need some credentials and I
also have a Cassandra they deployed on
the laptop I'm running locally to avoid
misery and heart attacks with Network
North world
not working correctly and I only have a
very small connection to the internet
world listening to Twitter and stuff so
we need the credentials we need to set
up so this is basic set up not really
very spark streaming and specific here
I'm setting up Cassandra I'm creating
key spaces and tables to store my data
and I have creation of a definition of a
definition here and here here's where
our business starts the spark streaming
job definition what do you need to
create your first spark streaming
application spark streaming as I mention
he works on top of spark spark streaming
relies on the spark as an execution
engine to process the data that is
passing through your Twitter stream so
the first thing that we need is a spark
context luckily for me the the spec
notebook provides an implicit despite
context so it's already there and what I
need to do next is to create a streaming
context so here I'm calling my streaming
context and I'm creating this every two
seconds so every two seconds interval
will be having data coming
and processing that and now I'm going to
create a Twitter stream maybe not not to
add some word in there so we have there
the stream box is the keyword that I
chose to isolate a bit the noise from
from the Internet in order to to hover
or interactive session and this is
justified as a filter on the very
specific case of the Twitter so the
Twitter our implementation for addy
stream so there we use that filter and
now I am going to define those windows
based on that in a starting stream so
what's going on is that I create a
Twitter stream it it logs into Twitter
will using my credentials
it's a subscribe to the to the Twitter
and so the firehose which is a filter
subscription and the data will start
flowing in and that data will be chunked
into those two-second intervals then we
have their or first up transformation is
a map so I will take those tweets
they're text-based and I will create a
vote out of the tweet and then I'm going
to count what what is the content of it
so I'm storing everything as it came in
but now I'm also going to break down
those hashtags and start counting
hashtags instead of incoming votes only
so I'm breaking off from that let's say
tweet flowing through the system into
the data that it tweet provided and if
anybody could prepare their phones we
can run this again and now I'm here
counting tops using a very large window
so that we have time to see the results
without having to resource to let's say
a secondary storage for that okay so
this actually keeping the results in
memory for those four hundred and eighty
seconds and next to that so I mentioned
when you have a stream processing you
have data flowing in and on the wire you
want to do something with it
in our case in the case of the small
application what I'm doing is persisting
those tweets to Cassandra
so that the tweet flow that with this
stream is safe to Cassandra it's very
important to maybe I like to for you to
have your attention in the fact that we
have here define three different streams
and we have the initial tweet stream
that contains let's say the whole data
that they did the handle the time stamp
only came in and the whole content of
the tweet the vote is a derivate of it
it only contains the hash tags the hash
tags filtered and removed a box and
stream box to have our hash tag keyword
removed from all my my statistics and
then I have at the end a accumulation of
values from it and the top count now
when I go to my Cassandra safe statement
I'm saying say if the tweet saves the
original information so I have my
original information derivates but I
still have access to the original
information it's not that it is not like
the stream is being transformed and
every time and you don't have like they
say a way to go back as I mentioned
these streams are immutable because LEDs
behind it are immutable that means they
vote this stream that I created a tweet
stream and I created is immutable and I
can access and get back to it every time
so here I have two derivates have a
window application let's say and put it
all back what I just explained to you
but then saving the original stream
because that's what I want to have for
later interpretation for later analysis
okay so this one we already executed and
then those are interactive reactive
widgets that we are going to use to
visualize the data
those are particular to the SPARC
notebook in let's say in an application
that you deploy to a cluster you will
have a separate let's say a play
application or JavaScript is sitting in
your browser
connecting maybe to some web sockets and
collecting information from your spark
streaming on the fly
process so we are here and I'm going to
print so here we are on therefore each
led that I also explained and what I'm
doing here is I'm accessing the data
that are contained in those these
streams and using it to populate my
widgets that I prepare beforehand so I
have three widgets one chart and two
text fields and as you have seen nothing
happens and everything returns
immediately nothing moves your spark
streaming application words will start
working at the moment that you say start
start will take the whole a structure
that you have created the lineage of
these streams derivations that you
created using your transformations will
take the actions and schedule those
actions those actions are attached to
all the lineage and SPARC Willis will
execute those on the cluster that will
effectively materialize your stream and
that happens when we just push starts on
here we can go again if somebody would
like to send a tweet with or with our
hashtag stream Vox
we can see data coming in thanks thanks
thanks so much for her live demo
interaction it's really great thanks a
lot what happens here I'm capturing data
continuously again we have a stream data
flows in in latina production
application you will have these
applications sitting on a cluster you
will not see it's actually producing
anything except for the flow that is
that is implementing and data coming
into your into your databases into your
file system etc you want to do something
with the data afterwards and here I'm
going to stop the the context again and
I'm going to analyze the data that we
just received
and it's also maybe
added value from a spark streaming is
that the API that you used to code spark
and Spike streaming is almost identical
and then you can do analytics on the
data on the spark streaming using spark
using the same knowledge that you have
captured so here I just loaded all the
data from the votes that you great
audience have provided me and I'm going
to see how many votes I have seen during
our session today and we have 52 votes
I can also visualize hold that voting
happened and we will have probably a
very large gap in between we have
probably more points than that let me
see let's put all the votes are
something like this well it's hard to
get to that okay so you get a picture
and they you have here the evolution of
devoting us when we started the session
maybe later on we have an order another
peak on that and we have we have a flat
line in between the two the two parts
and but I have something maybe a bit
more interesting it's back to the box
and last year I captured to spark all
the tweets come on they stopped jumping
all the tweets that were sent over the
three days of DevOps so I have a file
sitting here okay I have a file sitting
here that has all the tweets from last
year I'm going to load this and I'm
going to transform it come on stop with
that
let me see if I can jump out of this
okay there okay and let's see I have
some values there so there are the first
five votes from last year and I'm now
going to create from those votes sorry
from those tweets those are just tweets
I'm going to create something similar as
we had today
they reuse the same construct and there
I have 14,500 votes and 38 and from that
body I'm going to attract the Twitter's
who were the the handles tweeting over
their books last year and I'm going to
count how many tweets you produce at
that time so one of our Twitter's Stefan
we know him very well and he produced 39
tweets over Devil's last year now I will
have last year view and we have the data
that we have been collecting with spark
streaming over the session of today and
I'm going to reduce that the data in the
same way so I have the Twitter's that
have provided me data today we I'm going
to join that with the data of last year
why I'm going to do this I want to see
who of you have returned to the box back
to the Box future all right and I'm
going to take here the first five
persons okay final spy some killer
jun-chan just pretty and clever echo
welcome back from Twitter 2014 into our
session of a spark streaming I have a
gift for you so after the session please
come come by and see me so it's just to
say this is a this is a way of combining
data that you already had in your in
your data warehouse with data from the
stream and creating analytics out of it
all right so let's go back to the to the
presentation I hope now you're one top
with the spark streaming because we are
going to dive in how this thing really
works underneath and you will feel a
little bit like the guy fall in there
you have several deployment options and
the options for spark streaming are the
same as for spark you have the local
mode is what we are using today is very
useful for interactively
occations you can learn how to use a
spark you can learn how to use a spark
streaming on your laptop you don't need
a cliff ster on Amazon or sitting back
warming up your office you can do all
locally and the same code can be taken
to a cluster and process larger amount
of data then you have the options of
cluster one is a standalone cluster this
is when you deploy a spark
probably by hand with some script on a
number of machines they're called
workers in blue and you have a master
and there are also high availability
options for those but it goes too much
in detail for this presentation what I
want to give you is you can have a
standalone cluster if you deploy spark
streaming or you can have a managed
cluster those will be options like John
and messes that I provide you a service
on the cluster so you will think in
terms of resources I need so many CPUs
and it's a much memory for my
application and then you will have your
work your job deployed on that
infrastructure spike streaming there
will have a the receivers like the
caf-co receiver let the Twitter receiver
or a network receiver a TCP connect or
whatever and will be deployed on
elements of that cluster if this is very
important to understand because the
defined topology of your spark a
streaming deployment will affect the
performance of how it runs for example
in the case that I have illustrated here
to your right we have two receivers and
we have four executors what you see that
two of those executors are far enough
are sitting on different nodes in the
network that means I will have to get to
the data remotely they will have to add
an order a network hope to get to that
data that spark streaming has received
it will be in this in this very simple
example will be more beneficial to have
the executors sitting close together
with the receivers to achieve data
locality so in streaming you are not
talking about where is my data sitting
but where is my data arriving let me
talk a little bit
about scheduling I mentioned that we are
basing or streaming micro batches so I
have here split the timeline into into
three slots what happens is that our
receivers are getting the data those
those consumers are building up actually
data buffers that are submitted to spark
so in time zero nothing happens and time
zero your use collecting data and then
in time one the data that you collected
on time zero is submitted to spark to
the scheduler for processing and you
start collecting data for time one and
so your streaming application goes it
happens sometimes that the time it takes
to process data if you if you are too
optimistic with what you want to achieve
in your very limited amount of time on
processing exceeds the amount of time
you have allocated for your micro
batching and that will create scheduling
delays on your application this is like
when you start going more into real
proof of concepts or going toward your
first test in production you will need
to tune your application to achieve a
good balance between the size of your
micro batches and the amount of work
that you want to get done and sometimes
you will need to have compromises there
may be reduce the amount of work or
maybe increase the amount of all the
time allocated in order to get your job
done this will also play an important
role when you have a fluctuating input
and I will come back to that later on
with my little toy here in front hold
the streams become micro batches
I just gave you a glimpse on how the
scheduling works but there is more
underneath the surface we have a batch
interval is this time that it takes you
to collect that data but during that
time interval different receivers on
different places in your cluster will be
capturing data and that data is cut in
smaller elements as
chunks that are called blocks those
blocks are becoming partitions in spark
and partitions is what allows you to
paralyze some work I know this might be
getting a bit a low level but once you
understand this you will manage to tune
your spark streaming applications to
perfection to have them all the time
stable in production and there are a few
a few formulas you can apply very simple
algebra there where you have a number of
receivers and the number of partitions
that you will create from that so that
will be kind of your a parallelism level
once you have those those partitions
created and their didi is made out of it
and here I have have four elements and
those four partitions and they have all
little elements inside and those get
submitted to the cluster so the spark
starts crunching that data and once they
do that the results are done then the
spark releases the capacity on your
executor and it can can process more of
the partitions start actually the blocks
of data that are waiting for an
execution this is what's important when
you are defining the size of your
cluster that the size of the cluster is
aligned to the level of parallelism that
you want to achieve because if you have
in a case like this then you are waiting
to process they complete the complete
led until one of them is is done and
probably all your executors will be done
as well so they will be sitting waiting
for reduced one element to finish this
you can see back on the spark stream in
UI these kinds of scenarios where you
have a very empty white space before
something starts to execute it means
that you are lacking resources to
process all the partitions at the same
time in the org and your cluster and you
should turn and adjust your process to
that one a one simple way to do this
hint to help you on tuning this
is to change the block interval to
adjust you see you can put it larger or
smaller to adjust it to the number of
executors and that you have on your
cluster another tip to take home is that
caching is very important this Park
streaming it seems a bit unnatural
because things are mostly already on
memory but it's another memory space and
you need to explicitly say to spark
please use your cache memory for these
RDD during this time to execute a an
application operations on top of it it
will boost upper transformations in the
scenarios where you need to iterate over
the data several times like filter and
looking for different elements like an
algorithm a machine learning algorithm
you need to iterate over it it will be
very helpful in the cases where
everything is linear and you have just
one confirmation after the order this
will not make much difference there is
there are two models for the receiver
there is the one represented today is
the most generic model where you have
something sitting on your stream and
learn and then bring in the data from it
there is another model this is a
receiver less that has been created for
Kafka is an optimization actually on
spark streaming dedicated to Kafka where
we don't have a receiving anymore we
don't have a receiver anymore what we
have is an is a computation of offsets
as time passes I use count okay that
this this is the last time I saw you
this is a new time I go to Kafka I see
what offsets are there and then I submit
that to spark and in the process in
spark in the cluster it will go to to
the Kafka partitions and read those
blocks those offsets and process them it
is the simplified parallelism model so
you don't have to think too much about
how many receivers I have to put where I
have to put this in my cluster you take
whatever you did in Kafka and they let
say the parallelism level in Kafka or
for your hair for your topics
you you pass those partitions to spark
and so then the number of partitions is
in Kafka will be equal to the number of
partitions in a spark is very simplified
it's very efficient much faster than the
receiver based model and you can achieve
exactly once semantics if you have an
idempotent target like let's an insert
of a database that does not update for
example because you might see data twice
and you might see data twice when one of
those executors for example fails and
have to be retried on an order note so
exactly one semantics are not referred
at this consume inside dot at the
receiver side and let's say one no to
happy faces and you have less degrees of
freedom one example of that is in the
locality I mentioned about where the
data is processed now is lost every time
that you submit one of those cough card
IDs to do spark will be allocated in an
order executor in the cluster with
receivers you have an implicit data
locality some receivers are on some
nodes and you can apply for example
local clubs left
sorry local caching to improve some of
your operations we had for example in
our IOT applications we had a caches of
devices we had seen and they were
distributed in the cluster and one we
have seen them bloom filters will say oh
we already have this data you need to
store it anymore and that is not
possible anymore so you give some new
takes on and a very important new
feature on the SPARC streaming version
1.5 that came out with SPARC 1.5
hundreds 3 months ago is the
implementation of the reactive model for
SPARC streaming in the initial
implementation we don't have let's say
the full reactive streams well we have
used back pressure calculation support
and that's done using the PID model PID
controller which is a proportional
integrated develop derivative controller
for those who know anything about that
but actually what is doing is
calculating an error on the capacity
that you have the projected capacity and
they let's say they build up a queues
that you might have to process the data
in the
interval you have so you will regulate
actually the input stream how may have
much data that you are able to consume
based on how much time you use to
process that data and based on how much
backlog do you have it's very impressive
to see in action and there is a video on
the resources you can have a look in the
spike summit where we demonstrated that
I'm going to very quick my time starts
to run out I'm going to give you a
glimpse of what is to deal with sprite
streaming performance and how can you
have a look at it who can use to
understand it and for that I'm going to
jump back to the to the SPARC notebook I
have an order a very compact SPARC
streaming application in this
application I'm going to process sensor
data and that sensor data is provided by
in akka application so I'm going to
start my akka application okay and this
thing should light up as well let's see
painful seconds all right so I think we
are there so this is this is an akka
based a load producer it will create a
data in Kafka that will be provided that
will actually be pushed to to Kafka and
controlled by this dial here but I first
need to start my spark streaming
application before we can start playing
with it so let's start reset or devoxx
demo in this case I'm creating a job
that is made up of a combination of
linear transformations and some water so
there are of quadratic nature so I have
a number of transformation star just
taking data and filtering it but their
orders are trying to join data together
after it has been filtered so it's very
hard to let's say reason about the
performance of that job from from the
code itself you will need to see it
in action to understand what's going on
so here again I'm going I'm just
preparing my notebook output to be able
to to view the data and going to start
my streaming context a two-second
interval and now I am going to have a
calf consumer using let's say the
traditional a Kafka consumer that
consists on the receiver and then the
spark streaming and spark processing
connecting to zookeeper by the way all
this is is running on on docker on my
machine so there are all different
applications running and it will be
pretty similar on running on a cluster
do you need you have different IP
addresses and the only thing I don't
have is many instances of those okay I'm
going to define my job my job just to
give you a glimpse of what it is I don't
want to go into much detail
due to my clock ticking down but what
I'm what we are doing is we are
capturing the stream of data coming in
from that acha base producer we're going
to parse a stream we're going to take
different elements that were interested
in so we are splitting that stream into
temperature humidity humidity and
pressure and then I'm going to calculate
in some function over the temperature
and humidity to see some correlation
between those so I'm going to execute
this and at the end I want to take all
those separated streams of my that my
big stream coming in is now split into
things that I'm interested in and I'm
going to write it to the disk
very simple we are going to start
running the application and let me now
move this to the side let's go back to
the output and I need another window and
the window should be prepared here okay
here we go so let's see where is the
streaming tab oh sorry
now right so we have a streaming tab
there I'm going to start my automatic
refresh let's put it put it back
let's reduce the size because otherwise
we won't see everything that I want to
show you and I need to jump back to my
okay so now we can start playing a
little bit with the with the controller
if we could summon into it but what I'm
going to do I'm going to modulate the
input of data to to my sparks to Kafka
and therefore to my spark streaming
application so in few seconds if
everything works we should see data
flowing true let me see if that's
correct
a producer oh it's something wrong come
on it happens
serial connection going on alright I
think we are now good okay so we have a
receive rate and there we go okay so
this is just modulating the rate of
messages that I want to push a Kafka is
impressively
high-performance so we can start pushing
a lot of data into Kafka and that all
the data will be passed through a spark
streaming so we have here now I can
start putting this up and down the mile
dial and you can see what the reaction
of a spark streaming is important
elements to to observe here we have a
processing time and it has a barrier
there so I mentioned that those micro
batches are time-based
and at that time that of a micro batch
is also the time you have to process
that data and as I explained before that
is indicated on the spark streaming UI
with the
line over over the chart there so we
have a batch interval of two seconds and
then we have the time it takes to
process my data being plotted a real
time underneath nice this chart once we
break through that line then we are
start we will start to get in trouble
and it's let's see what happens so here
we have a rate of about 50,000 records
and I think it's the opposite direction
so let's make it higher and as my input
rate starts to rise I will see my input
race there on top rising up and then my
processing time starts rising and
scheduling delay that's the evil one
will also start to climb if you have
this on production you will crash
eventually so it's very important that
you understand the performance let's say
let's try to crush this stuff
we go on the red Sun and jumps it jumps
up you will see that underneath there we
have a number of cute events if your
peak happens temporally let's say you
have you have a peak of visitors at some
point in time and that that goes up and
goes down is relatively on a small
amount of time you can recover and the
the time that you have on processing
will be used to catch up for that a
fault for that data that has been queued
to be processed so at this point in time
it's a bit hard to scroll down but we
have recovered completely you see my
queue there below has clear has been
clean up so spark streaming will collect
that data will keep it in a queue until
it can process it but that queue cannot
grow forever you have choices of growing
that queue memory on memory and disk but
as you understand if is it unbounded and
bound then at some point you will crash
reactive streams and the backpressure
will help you because you will modulate
that that input rate and will start
limiting the amount of Records being
received to allowed you to only process
those that you can pearl
and then resume operations it will
prevent you from going to crush a in in
production and that's it for this
demonstration let's see go back to my
presentation one last word so we have
been from seeing the streams from far
away we jumped into it and now we try to
get back to the surface a few things I
want to take away with you streams are
important when you're dealing with big
data and you want to learn from your
data quickly and when you want to be
burden to run in very big batches over
largest amount of data it takes a lot of
resources when you could analyze and
interpret the data as it comes in and
sparks streaming provides you a model of
doing that when throughput is
self-importance and when your latency
requirements are not that high as you
have seen it takes this amount of time
to process one element of data so for
real let's say real time applications
that might not be your choice number one
but when you want to do analytics when
you want to do let's say on the fly
data mining then that will be your
framework to go and you can reuse all
the goodness that has been implemented
into SPARC and that's are some resources
there when you have the slides you can
look at them at your own pace and I
think we have like few minutes for
questions two minutes and running
questions
okay I guess everything was very clear
questions okay thank you very much and
if you want to work with the spark
streaming Scala akka and Kafka we are
hiring</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>