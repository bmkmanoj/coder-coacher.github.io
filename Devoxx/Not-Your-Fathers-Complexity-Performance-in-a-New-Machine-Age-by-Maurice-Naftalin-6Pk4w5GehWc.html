<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Not Your Father's Complexity: Performance in a New Machine Age by Maurice Naftalin | Coder Coacher - Coaching Coders</title><meta content="Not Your Father's Complexity: Performance in a New Machine Age by Maurice Naftalin - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Not Your Father's Complexity: Performance in a New Machine Age by Maurice Naftalin</b></h2><h5 class="post__date">2018-03-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6Pk4w5GehWc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thank you very much for coming to
to this talk I'd like to thank the
present the organizers of boxed as well
for for inviting me that's okay thank
you very much
so yeah good so I'm going to talk today
about complexity I've got a I've got an
introductory slide here which I'm proud
says the only slide I've ever had which
when I first put it up actually has in
the past really this is only the
introductory one driven people out of
the room because to see these formulas
and they think this is going to be
really awful well it's not going to be
that awful so it's useful for me to
start off by just saying maybe a word
about myself is this uh-huh
click it will work that it's decided not
to that's bad news
it's woken up I think good okay so so
just just a word about me because it's
relevant to the talk if you've heard of
me then it will be because I've written
a couple of books on Java one on Java
five and one on Java eight which were
the kind of big introductions to the big
innovations in Java and the nice thing
about writing books is that you get is
that you get a chance to talk about them
and the really nice thing about them is
you get a chance to talk about them
twice once you once you when you've
written them you say you go around you
said this is a great book and you should
read it because it's got all this useful
stuff in it and then a few years later
you go around and say I made these
mistake when I was writing the book and
this is the things you should instead
learn so what I'm going to talk about
today is stuff that I got wrong well not
so much got wrong but wasn't adequate in
particularly in the first book and
actually has got some implications for
the second one the first book is Java
generics and collections and I was
responsible mainly for the collections
part of it that's what I'm going to talk
about I'm going to talk about today
collections and their performance with
modern hardware which is a bit different
from what it was well what I thought it
was when they wrote in 2006 and so this
is the stuff about me but you would
think we can skip over
so I've titled the talk not your
father's complexity and and actually I
think this might be not a great idea for
a talk that I'm giving in Switzerland
because it's so it's a very colloquial
phrase in English and when you say not
your father's something what that means
is it's not the way we always learnt it
but I thought actually I could I could
sort of improvise on on this title a bit
and say well if it's not your father's
complexity what was your father's
complexed in particular who like what
were we thinking of what did we think
about when we thought about program
complexity well it's something that ever
most people have learnt if you did a
computing science course and it's
something you will have picked up even
if you didn't it's a mental model of
program performance or we use it as a
mental model of program performance
based on the number of instructions that
are executed in a in a by by an
algorithm and and it's worked pretty
well I mean at the time that it at times
it was invented it was a pretty good
model and I'm going to explain why that
has changed and and how we and how we
need to change it so this idea of you
have father's complexity led me to think
I'm gonna improvise a bit who is your
father well and I'm not talking
biologically I'm talking in terms in
terms of complexity so the guy who the
the the the man whose name is most
associated with with computational
complexity although he didn't actually
invent the concept is this man that's
Don Knuth who's one of the very greatest
of the greats in computing is creator of
tech a Turing Award laureate and the
author of this epic book the art of
computer programming which is
unfortunately not so much not not
unfortunately so much studies as it used
to be but but it should be because it's
about the basis of the way in which we
were in which we think about algorithms
so at the introductory slide which
didn't drive you away I'm glad to say
was actually was actually taken from a
page of that book because he spends a
lot of time in in in the art of computer
programming working out the precise
computational complexity of various
algorithms
although I always want to say that I
mean he's a really wise man it was
wisely enough to give up using email
after 15 years he used to think I've
been from 1975 to 1990 and at the end of
that time he said email is for people
who want to keep on top of things I'm
somebody who wants to get to the bottom
of things and if you want to communicate
with them now you've got to write him a
letter
remember letters now you don't remember
lettuce hardly anybody who remembers
them anymore so it's being a wise person
one of them one of the things one of the
things he said which really is worth
remembering when we think about
performance is that actually performance
is not really as important even though
everyone spends a lot of time thinking
about it the key thing that the key
thing is a line at the bottom of this
slide he says we should actually forget
about small optimizations about 90% of
the time premature optimization is the
root of all evil and you've probably
heard that phrase that he's easy
originator of that so I always start
every talk about performance by hit by
it saying you shouldn't care about it
and I give a lot of talks about
performance and generally speaking
everybody would people want to hear all
about performance we are we can't resist
it but most of the time we're not we
shouldn't be concerned about it and
really before you start worrying about
the performance of your algorithms you
should go through quite a long process
of analysis to ensure that your that
that the the thing that you're worrying
about the program steps you're worrying
about really are the ones that are
affecting the the performance of your
algorithm because generally speaking is
a whole lot of other stuff that I will
mention in passing on this talk that's
actually that's actually got more
influence and even even if it's even if
your the problem is with your algorithm
it's probably only with the relatively
small sections of code it's certainly
only with a small section of the code
anyway that's just as a kind of
diversion so let me let me get down to
the substance of the matter what is
computational complexity so the
definition I think this is from
Wikipedia says an algorithm executing an
F event steps with time complexity over
G of n will complete in less than some
constant times G of n for some C and
sufficiently large
so that's kind of hard to understand
when you first see it that's the formal
definition but in fact this graph shows
it a bit more clearly if you've got f f
of n is then is the actual number of
steps that programs going to require for
increasing data size along the bottom
axis and the vertical axis should say
that it's the number of the number of
steps then then the G of n provides an
upper bound on the number of steps that
you'll ever be required and it's going
to be a much simpler formula than the
than the F of n I mean tip f of n can be
pretty complicated for example here's
Knuth working out the number of steps
that are required for a multiple list
insertion sort the execution time is 3.5
n squared plus 25 24 point 5 n and being
the length of the lists being merged
plus 4 M for the for the number of lists
being merge plus some constant
now as that's a formula that it's not
particularly easy to work with but
actually is you don't really need to
work with it the only thing you really
care about is the N squared term because
that's going to as n becomes large
that's going to dominate all the others
and it's it's the one that really counts
so we call this order N squared and we
don't really care about any other about
it about the other steps and the reason
we don't is because generally what we're
interested in in knowing is what's going
to be the effect on the execution time
when we increase the size of the data so
computational complexity may not tell
may may not tell us that but it can tell
us the effect on the number of steps
that are executed so so the reason we're
interested in this is because it's the
traditional way of working out whether
an algorithm is any good because it'll
tell you what will happen if you for
example I've just chosen an instance
here double was double the size of the
data set so the ones in the red border
here are the ones that are the other
ones that we really care about and as
you can see the the order I mean
everyone knows it I guess a constant
time is the first line and then if you
double the size of the data set the
number of execution steps will be
unchanged
order log n the number of execution
steps will have a constant added to it
order n it'll be doubled and so on we
care about we care about these four that
are in the red that are in the red box
there because above that we the it felt
essentially algorithms are infeasible
they're not infeasible strictly speaking
that that's for exponential time
algorithms but in practice say something
of order N squared you can't work with
it because if you double the size of the
data then that then the the the effect
of then the effect on the execution
sepsis to quadruple them and so on and
and you if you double it a few times as
often happens in modern data sets then
you can you're going to have an
execution time which which can't be used
so so why what's the connection of that
with collections well I always knew
about this but when I came to write the
book on the half of the book on genetics
and collections I learned something
about how the collections framework was
was designed so here's another candidate
for for your father's complexity is this
guy Josh blah author of effective Java
now in its third edition live kind of
viable for Java programmers and also
author of the Java collections framework
I had some fantastically good luck when
I was when I was writing the book or
towards the end of writing then towards
the end of writing the book of having
Josh as a technical reviewer on the book
so this transfer if you're ever gonna
write anything I mean even if it's a
blog post always get somebody to review
it and it's a good idea to get somebody
pretty pretty rigorous to review it so
Josh was incredibly harsh so is his idea
of a kind of a relatively mild comment
on what I wrote would be this is totally
false and misleading so that would be
that was the middle Midway and in the
level of his comments but there were
extremely valuable and they made a huge
difference to the quality of the the
quality of the work so one of the things
that he told me and one of the things he
was very very harsh about was the fact
that I had diagrams like this in the
book so this diagram because you know I
and I'm worried about what am I going to
say and I thought well there's a little
if you look at the collections framework
hey there's a lot of these abstract
classes in there and these are there
start classes that are in the
collections framework had generally only
package visible and the and they're
they're really two as an assist as an
assistance to someone who's going to
develop a different implementation of
the interfaces so like it they're like a
starter implementation
he said don't talk to people about these
at all don't don't mention abstract
collection and abstract lists and so
forth because client programmers don't
care about that what what client
programmers care about is is the is the
interfaces and they care about this the
the what do you what do you what do you
explain to me was that the Java
collections framework is interface based
and you should care about and tell
people about the interfaces and once
they understand the functionality of the
interfaces then then they should choose
the implementation that they want on the
basis of on the basis of the the
scenario the execution scenario that
they've got for it so so if you've got
an implementation which is going to be
particularly good at some kinds of
operations but very poor at other kinds
then that's the one you choose but you
don't you choose that as a secondary
step from having chosen the
functionality of the of the top-level
interface well that works quite well for
the Java collections framework it's not
the this the collections framework has
deviated from that ideal by quite some
distances since then so different
implementations actually can be
functionally different like for example
queues the queues do behave quite
different the different queue
implementations do behave quite
differently but the general idea of this
that there's this notion that you're
going to choose you're going to choose
an implementation with the best
performance for your usage scenario is
really interesting and I use and I use
this a lot when I was writing the book
because as saying well how are we going
to choose what is the best performance
and the answer to that was well I'm
gonna say if I'm looking for example at
lists so I'm gonna say well what is the
yeah the computational complexity of the
different kinds of operations that each
one of them does we've got different
we've got different implementation so
list is actually a poor example because
we've only got ArrayList linked list and
copy-on-write ArrayList and
be quite rude about linked lists during
this talk and copy-on-write ArrayList is
kind of is it's kind of off on one side
because it's so poor for for for right
operations but all sense the the prince
the principle still holds so we've we've
got a so if we've got these three
implementations and I've just put in a
little little section of the
computational complexity table they've
gotten the chapter of the in the
appropriate chapter of the collections
book and then we think well what
operation that we wanting are we going
to be maximizing what operation is going
to be performed most frequently in our
in our in our program and let me see if
this thing's gonna work does it work
no doesn't work I've got this really a
but I bought this really fancy pointer
which is supposed to focus it's supposed
to give you a little thing on the screen
but I saw I saw it briefly never mind
look at the first column I can get this
back all right this isn't this is really
not gonna work I have to stand here so
if we look at the first column here
supposing supposing something that
that's not gonna work is it nice idea
look at the first column if you if your
programs going to be doing a lot of
random access to the to the to your list
then you're gonna care about the first
column that's that's that's forget at
some arbitrary index and you can see
that you've got a choice there between
ArrayList and copy-on-write ArrayList
you really don't want linked list
because that's all there any you've got
to step all the way through the list to
find it if you're going to be adding an
element onto the end of the list then
your choice is between ArrayList and
linked list because both of those are
constant time whereas a copy and write
ArrayList is it's very expensive but
that as the name implies it does
something special when you try to write
to it but if you want to remove the
first element or add the first element
to the list then in the case of a in
case of ArrayList you're going to have
to shuffle all the elements up or down
and in the case of copy and write
ArrayList then you're going to have to
completely rewrite the list
but Ling list is great for adding a net
for adding a cell just an element just
at the beginning of the list all you
need to do is add on the cell and do a
little bit of pointer swinging it's a
constant time operation so it looks like
these the this idea of choosing the
implementation for the scenario is looks
like it looks like it's really going to
work no I'm obviously being a bit
skeptical about it so I'm going to ask
did it ever make sense and it does make
sense if you assume that you can ignore
constant factors so for example that
although ArrayList is order n for for
adding and removing elements from the
beginning it actually is implemented it
really is proportional the time it takes
really is proportional to the length of
the list but the proportionality
constant is really small because the the
the array copy which uses to shuffle the
elements up is is intensified and it's
executing machine code so it's really
fast and it really matters a lot less
that it is order n so that was important
so the constant factors is important do
all instructions have the same duration
well of course they don't I actually
found that the last time I gave this
talk people were talking a lot about
cryptocurrencies and I just and I just
noticed this this slide this is from
it's probably a bit of a distraction but
it's from the way aetherium operates is
when you want to do an operation on the
etherium blockchain then the operation
is going to be performed by all of the
machines and the on the network that all
of them that are holding references to
the blockchain and so you have to pay
for the operation and you've got to pay
for it in terms of the number of and in
terms it's effectively of the number of
instructions that are going to be
executed but these instructions don't
all cost the same so you can see here
that although adding is relatively cheap
and doing exponentiation at the bottom
of the table is relatively expensive so
that so they have a realistic estimate
of the of the idea that the realistic
assessment of the fact that operations
do cost differently and they cost
differently on hardware is one standard
hardware as well not only on virtual
machine like aetherium but also I for
example modular arithmetic or or long
division it's very expensive and much
more expensive on all hard
than the simple addition a bit shifting
so the assumption that all instructions
have the same duration is they're very
much a simplifying instruction memory
doesn't matter that's that's an
assumption although there is actually
something that this is an idea of space
complexity that's similar but basically
this doesn't factor in memory and that's
going to be a really important element
of this of this talk and the instruction
execution dominates performance and
that's clearly not always not always a
realistic assumption at all so you just
have to you just have to laugh there's
all these other kinds of things that
that affected execution time and those
are things that actually don't have
anything to do with your program sort of
not like like access disk access and
network speeds and so on and how fast
the databases and so on and so on but
but but these things really have impact
on the initial quote that I gave you
from Donald Knuth saying most of the
time you don't want to be worrying about
this stuff so the question so the
question naturally arises what's
complexity study ever worth it and the
answer is yes it was modulo these modulo
these problems so I won't go into them
in a lot of detail one of the things
that one of the things I like to say is
again it's a commonplace but it's worth
emphasizing that even when you think
what you've done it's got rid of a
bottleneck in your pipe in what you
might think of as your pipeline of
instructions that that are getting
executed in the pipeline of data you can
get rid of that you can you can get rid
of that bottleneck and you think well
everything must go a lot better but the
increased but the increased load on a
later downstream bottleneck may actually
make things worse so it's not as that so
that this business of optimizing
performance of an entire system is quite
a complex business we're not going to
look at that as a whole in this talk
we're going to look simply at the
question of of how complexity should be
viewed particularly for collections so
what about this the question that's in
the title of the talk the new design of
hardware hardware used to be a really
simple thing it so it used to be when I
swear I learned about computers and ice
and a long time believing this was still
model right decades the the idea was
very straightforward that you had that
the your your processor would retrieve
instructions from memory then it would
then on the basis of the addresses in
those instructions
it would then retrieve data from memory
the the arithmetic logic unit would then
process those instructions and typically
you may you may well have had input and
output as well but the idea was very
straightforwardly that the the number of
that the the mechanism for executing
machine instructions were simply
retrieval from memory and processing in
the ALU so that was what you were
interested in I couldn't help when I was
looking at the when I was doing a bit of
research for this talk thinking about
old hardware I couldn't help putting in
some of the images that I found on the
on the web for hardware the way the way
it used to look so this is nobody nobody
else ever ever ever recognized this
picture but it's a picture from deep in
my memory they we used to have uses
called machine holes well these would be
I remember the one in the first place
that worked which maybe two or three
three times the area of this of this
lecture theatre and it was filled with
these cabinets that you see there and of
course the power of those machines were
all put together would be would be no
more than you've got on your phone in
your pocket now I guess and something
else I can't resist telling you so when
you look for the adverts for these
machines they have these things called
mini computers and the advertisements
for them always had the machines in the
garden I have no idea why and it also
always had an absolutely necessary
accessory this one this looked quite a
long time before I could find one that
was you know sufficiently you know
uncontroversial to be able to show you
because I love the listen even have any
clothes on and this was the way they
advertised computers in the 1970s anyway
anyway it's just I I could I couldn't
resist this so while so in those days
this is actually the time P 1980 when
the computer architecture was the way
that I showed it
this was the
Golden Age of what I've called the
Golden Age of complexity analysis or
rather rather we saw how that happened
at the beginning of the pre-1980 the the
cost of the cost of memory was hard to
believe a dollar a byte that's how much
that that's how much random access
memory costs in the 90s in the 1970s so
you see that the the cost of the of by
the top by 1980 we're talking about a
million dollars for a gigabyte up for a
gigabyte of memory and you can see this
is obviously a logarithmic graph and you
can see it went down it just went down
really steadily on a on a logarithmic
basis to 2010 so the effect of this on
the way that we thought about the
bottlenecks in programming was that this
at the left hand side memory was
fantastically expensive so you had these
things like external sorting algorithms
that was a big deal where where you'd
have a lot of data on disk that you
wanted to sort but you couldn't get it
into memory to do an in-memory sort all
the times you'd have taken a chunk of
the time sort that put it back out again
take another chunk sort that and then
find some way of sorting that sorting
the two so so really the cost of the the
paging costs the the the amount of time
it took to get the memory to get data
down from a disk on into the memory
dominated the performance completely in
the middle of this the the in the middle
of that graph memory was cheap enough to
execute algorithms in memory and so what
so the cost of executing an algorithm
now really was to do with the number of
operations and then something went wrong
so this was the the golden age was there
was where the golden writing it and
something went wrong so what happened
well the the performance of memory if we
look at the performance of memory it
looks like this would this actually
followed it follows a similar kind of
graft that's the that's the that's the
cost here's that here's the the
performance it's obviously not going up
quite so quite so well
the big the big the big deal though the
the long with the reduction in the cost
of memory the big deal over this period
was the increase in the performance of
the processes
so the performance of memory went up a
bit by a factor of tennyson forms of the
performance of the processes went up by
10 thousands of processors became
outstandingly the fastest part of a
system and bridging that gap was a is
really the big problem for computer
architects from the from the turn of the
century from before the turn of the
century onwards through the night even
even through the 1990s so the big deal
is how are we going to keep the COS hot
how I'm going to keep them running how
are we going to get enough data to them
because if memories as slow as all of
that getting instruction as a memory
getting instructions of data from memory
to the processes is now the the
overriding problem of how you're going
to manage to get them getting maximal
performance out of your machine so
keeping the cores running today means
looking means a structure like this
where you've got between the memory at
the bottom and the processes at the top
you've got a series of caches and these
caches are increasing in speed as you go
out as you go narrator to the to the to
the processors and they increase by the
way in cost as well and they decrease in
the decrease in size and the the notion
is that you've got to have a kind of
like a pipeline of data I'm mainly
interested in data in this talk where
you've got some memory loaded into the
last level cache and you've got you've
got less data loading than into the
level 2 cache you've got even less data
loaded into the level 1 cache but the
level 1 cache is really fast
it's almost as fast as the processors so
there's relatively provided you can make
sure that the processors are working on
data that's held in the level 1 cache
you're going to not pay an awful big
price for the for the for the for the
slow speed of memory access and a huge
amount of the of the real estate on
modern chips 80% sometimes it's given
over to these crashes they're really
they're really a big deal the the it's
often called the memory hierarchy and
actually everything's now positioned on
the memory hierarchy
between the between the processes at the
top which are which are really fast and
and the most expensive part of the the
chip right down to right down to virtual
memory you've got the the the the the
scales on the right hand side going up
it's its speed goes up power goes up and
cost goes up as well and roughly
speaking you've got about a four-fold
decrease in speed for each component as
you go down let's go down the level so
on my laptop here which is which is old
now it's mid mid 2014 that's a core i7
as well and the the it's three gigahertz
so there's so one cycle on this is about
a third of a nanosecond the level one
cache is about one nanosecond so it's
about three times slower level two
caches for nanoseconds four times slower
level three 16 four times slower again
main memory is a big jump is 100
nanoseconds it's actually more than 100
nanoseconds so you see that the the the
difference in how long it's going to
take to get something up from the from
that from the memory is it is a huge
difference and keeping the pipeline fed
that's making sure these caches that as
far as possible your processors are
accessing the data that's high up in
this diagram are really important so I
found I found trying to illustrate how
this my vision of how this pipeline
thing works I found I found this slide
which is Mongolian stamp from 1977 how
they put out fires in Mongolia or at
least how they used to do and my vision
of this is that if you think this stream
is main memory then what we're doing is
is we're feeding this pipeline going all
the way to the house that's on fire
which is actually a good description of
the processes so actually it's not it's
not entirely realistic if I would could
go back to the Mongolian artists who do
this I would say could we have more
people at the Riverside and slower and
then you know and fewer people and
faster near to the fire that would be
more like the cache but the key thing
what you see about this is if everybody
drops their buckets which is what will
happen is the data that you're that
you're taking in it turns out to be the
wrong data and you have to
start again refill that pipeline then
you can tell it's going to take a lot
longer for the water to get to the fire
then if you could just keep keep that
going so sometimes I like to use because
because there's this if I say that the
the you know that main memory is a
hundred nanoseconds you're thinking well
that's really fast you know how in the
earth can I be complaining about that so
sometimes I like to use this analogy
about thinking of the processor as being
a person boiling eggs at the stove I use
this one because everyone knows how long
it takes to boil an egg it takes about
three minutes to boil an egg so as long
as as long as somebody's gives me an egg
to boil I can I can I can keep busy and
executing an instruction is boiling an
egg and it takes and it takes three
minutes to do it so if I if I if I'm not
I'm being passed the eggs by a chain
along the chain of caches like this as
long as the pipeline is full if they
give me the wrong egg because there was
a misprediction about what data would be
required then it's then the whole
pipelines the pipeline spelled and
you've just got to start again so the
pipeline will be refilled for the main
memory that's 100 nanoseconds which is
about 300 times longer it's about 300
memory cycles so that's roughly speaking
300 times 3 minutes so instead of
instead of being able to boil an egg in
3 minutes I'm gonna have to wait for
about a thousand minutes which is like
15 hours so no that's no that's wrong
it's 300 times you know 500 times 15
it's a long time anyway it's a long time
to get it from the fridge my calculation
hasn't worked out this time but but but
it usually it's the way that I figure it
is it's going to be many hours for the
for the eggs to get from the fridge to
me didn't work quite well so so the the
cost of the cost of cache misses can
dominate can completely dominate your
program execution time and in fact they
frequently do so how do our architects
spend a lot of time thinking trying to
think their way through about how to
inch power to ensure that the won't be
cache misses it's a big measure of how
good a piece of hardware is that the
standard benchmarks the
it will that the the cache misses will
be relatively low so main memory
retrieval is two or three hundred times
to get it to get it from the to get data
from the main memory it's gonna be a
hundred times level one cache that would
be that would be five hours on my on my
analogy and two or three hundred times
the the register access so if it's a
register access would be ten hours
typical programs have very high hit
rates because the hardware designers
have thought really hard about how to do
this stuff but the other 5% makes a
really big difference because the costs
are so high so where these cache misses
come from and the answer the the number
of their own large number of reasons but
one possibility is that you've just not
got sufficient capacity in the cache
another one is you might have a failure
of prefetching so I'm going to look at
both of those in a minute let me look at
the second one first now in terms of
explaining it because you if you don't
know what prefetching is it wants a
moment's explanation the the incapacity
of the cache in sufficient capacity is
more obvious so the way that prefetching
works is the hardware predict where
you're going to be wanting to where
you're going to want the data from and
it loads it into the caches in advance
so the the idea of it's really really
quite straightforward the implementation
is not straightforward here we've got a
picture of the of the level 1 cache only
the level 1 cache shown on this these
are the the horizontal lines are are
what are called cache lines their units
of the level 1 cache store typically
they're 64 bytes a very important thing
to know about them is that they get
loaded as a lump you can't use you can't
say oh I just want a couple of bytes
could you load that into the level 1
cache anything that's going to be loaded
into level 1 cache will be 64 bytes at a
time so so I've drawn a rough kind of
correspondence different bits of the
level 1 cache correspond to different
bits of memory and now I imagine that
we're doing something like an array
access and we regularly stepping along
and elect an array like that and then it
then the hard work and they're quite
good algorithms in the hardware
observing other memory locations from
which data is being retrieved and say
hey I can tell where that's going to
come from next what's going to be wanted
next
and and it can and it can prefetch that
data in into the into the cars so so
that works fine for like for simple
array accesses but that's not what we're
doing most of the time in Java and this
is where collections are particularly
irrelevant because in an object-oriented
language what you're actually doing is
you've got this data dependent loads
you're following references all the time
or pointers
well it's like pointers but references
as we like to call them and so so you
you're leaping around in the in the in
the memory here and there's no the
hardware doesn't have a chance to to do
this prefetching it has no idea where
you're gonna want your next data from
and therefore the pipeline has to be
completely refreshed each time it's
actually even worse than that because
the memories memories divided into into
pages typically 4k and and because the
processor works with virtual addresses
that is with a with the page index and
the displacement within the page it
doesn't have absolute addresses in
memory and there are many reasons for
this but the strongest possible reason
for it is security it ensures that the
the no process can ever access a page
that doesn't belong to it so that's
that's that's kind of overriding reason
why most I think or all common hardware
uses virtual addressing so it's got to
translate this virtual addressing into
the virtual addresses into absolute
addresses it uses a table for that of
course the table of course is held in a
held in fast cash and then amidst us
does the translation and works out where
the read where the absolute addresses of
data that's to be retrieved
but the problem about this as you can
see is that the these this this this but
table which is called a translation
lookaside buffer for some reason or more
commonly just a translation buffer this
of course itself is in cache and of
course is the the translation buffer as
a whole has to be in has to be in memory
and so the bit that's loaded into the
page table has to be in memory so the
bit that's loaded into the translation
buffer is relatively small and that will
have to be refreshed so it may be that
for every memory access you're going to
have to have for in the case of data
dependent loads which are jumping about
between pages you're gonna have to low
not you're gonna have to refresh the
caches not once but twice this is very
expensive okay so let me let me go a bit
faster because some not really got to
the meat of the talk yet that's a bit
bad
how does caching play with complexity
well what we're going to what I'm going
to do is I'm going to look at a simple
case of traversing a list
it's obviously order N and I'm gonna
compare linked lists of the primitive
array sometimes I give this to people
say why are you talking about linked
lists the answer am the answer is
because linked list is particularly bad
but I'll show you why ArrayList is
actually not an awful lot better so
here's the result of doing some of doing
a jmh analysis of a simple benchmark
simply traversing a linked linked lists
with this number of elements so 1k that
gonna be gonna be 4 columns 1 K 7 K 63
came 511 K elements and the performance
in nanoseconds per operation is goes
down really dramatically right over that
so we've got reasonably good performance
7 nanoseconds for each operation and
operation is like moving a step down on
the list and it goes up for 511 for 511
K goes up to nearly 30 this this this
was run on a on a machine not unlike my
laptop but not actually my laptop so the
the CPI which is the clock ticks per
instruction is the way the hardware
engineers rate how well their hardware
is doing and that decrease is quite
radical quite radically over this that
this accounts for why the performance is
so poor clock ticks parameter per
instruction it's the inverse of
instructions per per cycle is the is the
the hardware measure the the fundamental
measure of how well your hardware is
doing so it can be more than one because
we have instruction level parallelism
and and we'd really sorry it can be a
lot less than one the number of
instructions executed per cycle can be a
lot more than one so we're going from a
lot more than one to two less than
here and if we look at the events per
operation then we're going to see really
what happens first of all the the number
of cycles number of machine cycles per
operation is going up the way the way
that the way that we would expect the
number of instructions being executed
stays the same so that this this line is
the one that is that defines why the
whole point of my talk here we've got
the this is an order end operation the
number of instructions is remaining the
same all the way through but what is
happening is if we if we go down and we
look some more we see the level 1 cache
misses is going up hugely it's doesn't
actually it's not very many but it's but
it but the small increase in level 1
cache misses because of what I said
earlier if because each miss is so
expensive the pipeline may have to be
completely refilled that they can
completely come to dominate the the
performance so there's some other
there's some other things here the last
level cache also the the misses and on
that go up as well in this in a similar
kind of way that's the bottom level
cache so we can see this this is not a
cache friendly program so mass link
that's linked list for you what's going
on here the linked list is a node size
is 24 bytes and here I'm adding together
the I'm adding together not only the the
size of the the cells in the linked list
but also the amount of the amount of
space that the data is taking up because
it's a linked list of integer and each
integer takes that takes up 4 bytes
sorry 16 bytes 16 bytes so so so the
node size is 24 the the the total
overhead of the of adding another
element to the list is 40 bytes and the
level 1 cache on on the machine that
this was run on had a capacity of 32 K
so so the level 1 cache was going to be
blown out really early on and you're
just going to need to keep on refilling
it and that's why the big list cut is so
much more expensive than the small one
and the reason why ArrayList is really
not an awful lot better than the link
list is because because the overhead of
adding an element
of course an ArrayList only takes the
the pointers and the in the actual array
of the ArrayList only take four bytes
the the integer objects the the integer
objects that it that it needs those take
another 16 so the Soviet so the total
overhead is 20 bytes so this is actually
not as bad as a linked list witches
which has 40 bytes for each element but
the fact is you're going to blow the
you're gonna blow the cash pretty early
on anyway compare this with an array of
primitive and you see something
completely different so the the number
of instructions again is constant the
level one cache misses hardly starts to
get off the ground for the 511 K array
and you see if you look at the top line
now the performance the performance is
pretty constant so so the the key thing
here is that we're not loading the
memory with all these big objects that
Java requires even to start even to
store an int so that's one of the
problems we may just run out of cash
here's another problem which is which is
the data locality so you may have heard
you may have heard of this term do we
man are we actually managing to match to
keep our data compactly in memory in a
way that matches what we're going to
need on the in the caches so it's quite
possible for example that you'll
intersperse your the data that you need
with data that you don't need and when
when you load of cash at 64 byte cache
line you're taking a lot of stuff you
don't need and so you're going to need
to refresh that line earlier I mean
another possibility is that that you may
not be you may be frustrating
prefetching and the way that in the way
that I taught so so I ran a little
experiment which which which I'll show
you briefly here where we populated a
linked list I populated linked lists in
what I call a natural way it's just it's
just my term and the idea of that is
that you just execute the code that
there is there that we have below there
and effectively the the the the cell
nodes and the data are interspersed and
they'll be required in a way that
actually that that if you if you
traverse the list you'll be you
be accessing the data in a fairly
natural way the alternative is what I
call populating the linked list randomly
so so for that first of all I created
and populating the ArrayList of integer
so the pattern looks like this so we've
got a long long array with some with
some data at the end of it and then I
created a linked list which I allocated
and I allocated the pointers to the
populated that with with data chosen
from random places in there in the in
the in the data that I created there so
so so there's going to be no that's
going to frustrate prefetching
completely and now I've got a little
demo which which I used to try to run
jmh programs as part of the it was part
of presentations but you can't
realistically I'll do and I can run this
you can't really know I'm you know
you're not gonna see it that's so
frustrating
well it's certainly shortened to talk a
bit one last try know how frustrating is
that sorry well okay so it's just a jmh
program anyway and the and thus be the
the difference in the performance of
those two it's running on here so did it
run is it running you going looks like
it wasn't running it wasn't really on my
display and that was stuck how
frustrating no good not moving no it's
gone
the the difference in performance was
great so that's unfortunate is that I
made a film I made a video of the of the
chair much demonstration because you
can't do you can't ruin a realistic
experiment fast enough in a
demonstration so usually that usually
that demonstration works but not in this
occasion obviously the demo gods weren't
with me but the the different the
difference the difference was is very
great it's like a factor of five factor
five difference just because of the way
that the date they're doing
the same operation both order em just
traversing a linked list there's no
difference in functional in the
functional behavior there's a huge
difference in the performance I can't
help saying you know just in case you're
thinking well I'm really banging on a
linked list that it's not really nobody
really does linked list very much it
doesn't get a very good press but in but
and the reason the reason is I mean I
never used to understand this the reason
that nobody really likes it very much is
because it's doubly linked and and
therefore each element takes a lot of
memory and it's this memory that's so
expensive it's not expensive because
memory is really cheap and people think
oh I don't care about that but if the
bility elements of the data structure
take up a lot of memory then it's going
to those going to it then it's going to
push your it's good it's going to
frustrate the the cache coherence and
that's really going to impact on your
time so I'll skip the this stuff to do
with the hash map but I will just I'll
just say that the hash the what people
think is the problem with hashmaps is
the is overflow because because if you
get if you get collisions in a hash map
then you get over foot on the bucket
chain and then effectively you're
reduced to some linked data structure
the where it used to be the din hash map
overflow would lead to a linked list
structure in nowadays in modern hash
maps and Java 8 onwards at least to a
binary tree but the that the but it's
still there linked structures and link
structures are the thing that really
causes problems with hashes the
immutable collections that have arrived
in in Java 9 are an improvement on this
and actually one of the things I'm
suggesting is that you might be able to
you might actually be able to do better
with with a say a map or a set that
you're going to be reading a lot more
frequently than you're writing is that
once you've assembled it is actually
moving it into an immutable collection
because immutable the way that immutable
collections are structured means that
means that they will typically be much
more they're populated at the same time
if you do it like this
they're populate at the same time as
there as I created and
the data will be stored all in the same
place so reducing memory footprint is
obviously a way of improving this
problem about cache go about cache
friendliness there's these third-party
collections frameworks which I don't
really have time to talk about in any
detail all of them attempt to reduce
memory footprint it don't generally
concentrate on this issue of cache
friendliness where they have primitive
collections it's a different matter so
you can improve data locality run an
improving memory footprint with
primitive collections like I showed you
with that array of that array of integer
so there are better options with that
there's the any-anything that many of
these many of these sections frameworks
do improve do support primitive
collections and therefore do improve
data locality people that have focused
on this in particular are people who for
example are working with bit sets rather
than bitmaps rather than rather than
objects so this roaring bitmaps have
some very clever algorithms for making
for getting very good data locality
object layout as a as a third party
library which developed actually by
martin thompson and gilt na who people
are so centrally involved in thinking
about this and that and and that
provides some C type structures
effectively very good for data locality
but unfortunately then out of the
function of collections at all so a lot
of the answer will come with project
Valhalla which will appear in Java
something where the idea of val is to
have value objects which will
essentially have the which will
essentially have the property they'll
behave like objects but they'll be
organized in memory like primitives so
be implemented like primitives so that's
a really difficult that's a really
difficult goal it's very ambitious and
they're making good progress on that now
they're addressing this problem with
project Valhalla but it's probably some
time away so the conclusion my
conclusion sorts as well of course
performance that mostly doesn't matter
well it does matter it really does every
performance improvement represent
to trade off of course you know that
algorithm complexity is still important
but so are all these other things
database network and database
performance GC resource contention and
caching so in fact it is your father's
complexity it's just got a lot more
complex thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>