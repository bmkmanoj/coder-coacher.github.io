<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Terracotta’s OffHeap Explained by Chris Dennis | Coder Coacher - Coaching Coders</title><meta content="Terracotta’s OffHeap Explained by Chris Dennis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Terracotta’s OffHeap Explained by Chris Dennis</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xCojSZMp_Zw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon or evening or whatever
you want to call this at this point my
name is Chris Dennis says on the slide I
work for Terra Cotta otherwise known as
software AG I work on many things one of
them is a thing on this slide so I'm
going to try and be about 45 minutes
give or take ever done this talk before
so who knows whatever you want to ask at
the end open season I wrote most of this
library so I should know the answer and
if I don't know the answer then I'm a
bad developer so Who am I I was trained
as a physicist after the end of this
talk it might be clear that I wasn't
trained as a computer scientist um we're
gonna tend to do some really elementary
computer science like 101 first year
computer science stuff hopefully it's
really familiar to everyone and so it'll
be a nice easy talk I spent four years
doing unnatural things with byte code in
academia I in a previous existence wrote
an x86 emulator in Java that did x86 to
bytecode translation and I like doing
weird things and I then spent three
years doing more weird things for Terra
Cotta um who has up if you've heard of
Terra Cotta by the way cool I can see
some hands and I can see a really bright
projector ball three so terracotta was
famous for a protocol DSi which tried
and sometimes succeeded in clustering
JVMs and I did weird bytecode stuff for
them I then spent four years doing
unnatural things with byte buffers and
which is what we're going to talk about
yeah that was originally terracotta as
part of the H cache hands off if you've
heard of the H cache it's about the same
amout of hands I think cool yes so
that's what we're gonna talk about
in total I've spent 11 years doing Java
development I still work for Terra Cotta
as part of software AG and I call myself
an engineer not really an engineer I
guess I'm a developer maybe hacker
probably so I mentioned this in my in my
abstract so I figured I should um I
should put in my slides I get to play
with big toys if you can do the math on
that kilobyte number that's a big number
that's six that's what many info looks
like on a machine with six terabytes of
RAM and there's not much free as you can
see and that's because all of it's being
used by the library it's also 120 cause
that's an Intel Ivy Bridge VT 4890 as it
says that machine had four CPUs it's 15
core CPU and the hyper-threaded so it's
120 cores in table that shit would have
cost you six and a half thousand dollars
when it was released last year and we
got to play with it because this was on
loan for Intel six and a half thousand
dollars sounds like a lot of money but
this machine had somewhere between a
quarter and half a million dollars worth
of RAM instilled in it so yeah the CPUs
cheap the RAM is expensive and I do not
recommend that you buy a six terabyte
machine unless you enjoy burning money
even Intel didn't have the six terabytes
of RAM they kept having a FedEx it
around between different labs because
they couldn't afford to buy more than
once so that's the end of me bragging
about the big things I play with some
people might get the reference sorry
that was really off-color a bit of
history so I started working on what's
called within Terra Cotta a feed store
library in 2010 and it was originally
intended as a caching here within eh -
so the idea would be you have heap of
heap disk in 2011 so the time circle is
using Oracle bdb anyone heard of Oracle
no hands of synergies head of Oracle
cashing frontier in front of the Oracle
BBB otherwise noon sleepy cap and we
stop this thing in front of it is a
caching here making Terra Cotta server
work better so in 2013 we had some legal
issues with oracle surprise surprise and
so we had to push the offered store
insert service it became the primary
storage of a basic for your place PDB
for the Terra Cotta server and so when
we tested on the 6th terabyte machine
that was for a that was for a terracotta
server instance 2015 we finally got to
open source it go to github go look at
it fork it play with it tell me how the
things that are broken when you look at
it you'll see it looks like code that
was not developed open source it's very
a little idiosyncratic at times I
apologize if you want to propose a pull
request to break all the things that I
did wrong and fix the things I did wrong
sorry
then I won't be offended so that's the
end of history that's where it comes
from
I've been wanting to do this talk for
five years I finally get to do it it's
great so problem statement so obviously
we were writing a library and we didn't
write a library for the head of it
first of all map I assume everyone knows
what a map is but just in case you
forgot a map is a collection of key
value pairs and and and I just think
that a cache is a map with bells on it
does some extra staff but in essence
it's just the map caching is good
caching is good I will give you a few
seconds to read it them it this xkcd
some of you might remember it it's a
little bit jokey but it's serious if we
didn't have caching good chunks of what
we do wouldn't work caching allows us to
have large stores that operate with the
latency of small stores because of the
wonders of the pareto distribution so
more problem statement more problems so
lots of caching is good caching is good
let's have lots of caching we've got big
data let's have big caching problem is
lots of caching
cause these lots of heat big heaps yeah
fun
no that's lots of work for the garbage
collector and unfortunately that leads
to a lot of juicy pausing and overhead
please don't tell my employer like it
says please don't tell my employer I
said that I'll make sure they never
watch the video
the situation is markedly better now
that when I started in 2010 there was no
g1 out the limit on what you could
really do in a heap without stunned
where it hit pause this particular with
CMS was probably eight gig
depending on your workload you could
push harder but but there sitting was
around there nowadays you can push tens
of gig probably pushing 50 gig with g1
and hundreds of gig if you've got zing
or Zulu or whatever you want to call up
Xul's offering and Shannon doe is
obviously in the pipe
that doesn't replace the need for
libraries like this it just pushes our
the tipping point out that although so
so we have this problem we have a lot of
QC overhead so so let's think map cache
best practices hands up if you think I'm
gonna say this very carefully hands up
if you think mutable keys are a good
idea in a map mutable mu double do you
think you should mutate to the keys in
your map yes please do this if you don't
this and we've had customers do this
very weird things happen and the
customer gets very confused and you get
stuck in awkward support calls where you
can't call the customer the word you
want to call them but you really really
want to say it immutable values who
thinks that mutable values are a good
idea in a map suite you guys are smart
please have immutable values really yeah
people are waving their hands around if
you're going to have mutable values you
are going to be doing multi-threaded
programming if you can avoid doing multi
through
programming avoid doing multi-threaded
programming in particular if in the case
of like marshalling things on disk and
stuff you obviously can't mutate because
then the question if that's done
asynchronously what exactly are you
putting in the map you don't know
because you don't know when the mutation
happens only when the marshaling
happened rather so so we got
immutability everywhere because we're
good people but if we have immutability
who really cares about object identity I
can just copy the object freely give you
a copy of it as long as the states the
same you don't care so if there's no
object identity do I really need a heap
I don't need the object identity I get
from it I don't need to always use the
same reference I can copy it all over
the place
no heap cool no garbage collector
brilliant no garbage collector no
overhead so the solution is obvious
we're going to replace all of our heavy
by which we mean large map cache usage
with an outside of the heap but inside
the process implementation please don't
do this with the small stuff you are
taking a hit if it's too big to fit in
the heap move it if not leave it where
it lies so this is benefits of two
scales at moderate scale which when we
started was probably in a sort of 2030
gig scale or maybe even smaller
potentially you're going to reduce the
don't offload the GC and that's going to
reduce overheads at large scale we can
still function you not do that that is
not going to function on any VM out
there I don't think even as all is going
to handle that so there are some caveats
obviously this is not free lunch the
marshalling and unmarshal thing you're
going to end up doing is going to cost
you time and obviously latency is very
important to most people nowadays and it
also you also cost you CPU there's gonna
be overhead it's going to cause
everything else to run slower but what
you're doing here is you're doing your
trade-off most people's err salaries are
not expressed in average latency
particularly in modern OLTP applications
what you're doing is your essay is based
on a 99th percentile or a 99.9 percent
I'll or something like that
so you care more about the tale Layton
sees the Layton sees when you go GC
pause the Layton sees when you're stuck
in this guy oh then you do about the
latency in the good case
so you're trading away average latency
in order to control the tale so we want
to do an off heat map what do we need we
need a map so we need to replace
java.util map be that hash map hash set
whatever you want to think of it as and
we need a heap we don't have a heap
anymore so we need something to stand in
for the heap and we're going to need
something to stand in for the job of the
garbage collector we need something to
clean up the things that we removed from
the map we need to make sure that we've
free the off heap we need something for
class a logic effectively the
marshalling and eventually we're going
to need a concurrent hash map so this is
where we get technical now we're going
to start talking about how we replace
each of these things so let's start with
the map so JDK hash map let's talk about
how it works
buckets will call these things buckets
so a regular hash map this one has eight
buckets and the way it works is we say
we want to do a put and so we hash the
click take the hash code of the key we
probably spread the hash code to give us
a nice even more even distribution in
case somebody's got a bad hash code
function it has to be a function that
constrains itself to one portion of the
integer space spread the bits around
then we're going to basically do a
modulo division or a bit mask and figure
out where it goes boom
so hashes to the third bucket cool stick
the key value there do another put sweet
another value
to a third perp oh it's hash to the same
bucket a conventional JK hash map at
this point is going to do that it's
gonna form so in print prior to 1/8 it
would form a linked list which is why
I'm representing their 1/8 and beyond it
actually forms a tree if the keys are
comparable which gives you better
performance if you've got bad hash codes
it degrades as a login rather no n but
but the essence is that the collision is
controlled or handled by forming a data
structure within the bucket that is not
how the off heat map works so we do puts
all this looks familiar it's doing all
the same things
threatt everything's going fine make
sure I don't go too far
cool we're about to put the thing is we
want to try and minimize the number of
data structures we have to create here
the more code we have to write them will
transfer your write bugs so what are
week used to do we choose to build a
different kind of map rather than form
your data structure off of that entry
that we put in we reap rope and we say
right that's lots not there we can't use
that slot it's taken so that's reap robe
to a new slot and put the value there
cool what we just did is what's called a
linear repro the Newbery probe is
usually done by reproving for the next
slot your principle could be Pope to the
fifth slot and under tents float on the
15th slot here we're going to do first
thought 2nd slot thirds 4 and so on
I'm ukee pre probing up to a certain
point what's called the repo limit at
that point you say if you can't find an
empty slot you go oh crap no space right
rehash make the map bigger try again
what that's put a linear repro the
reason we pick that is it's cache
friendly it's it's simple its cache
friendly because obviously the machine
may well affect the same slot the next
slot in the same cache line
it's more prone to clustering effects if
you have bad hash codes you can get
clustering of the slots in the slots and
at that point then you hit the reaper
blim it early before you've got too many
mappings in and you end up with enough
inefficient map that takes too much
space you can also do quadratic read
probe where you repro by 1 then 2 then 4
then 8 which tends to smear things out
and you get better behavior you can also
do a hashing repo where you take a
secondary hash function one quick point
here we're starting to hit problems
where the 32-bit hash code we get in
Java isn't big enough because we're
starting to get two maps with pushing up
against four billion entries and
obviously at that point it doesn't
matter how many time or how many times
you anymore peasy trumpeting they're
always colliding because there's no
space for more spread hash codes so yeah
so we're probably gonna have to move to
long coding soon there's actually a
branch in my own Fork of the library
that looks at doing that and you can go
and check that if you want it's don't
know if it actually works yet both being
played with cool so this is a hash map
the style of what we just talked about
where we re pro is called open
addressing and it's a linear repro one
slot cool so so what are these k kv
things so in a jdk hash map it's that
it's a java object it stores the hash
which allows you to rapidly check the
hash which it does is a precursor to do
a coin equals because it's faster and it
caches yet rather nopsi having a cool
hash code I mean it obviously saves the
key
it saves the value and it saves this
next thing which is the reference so so
we got primitive that's nice and easy to
store an off it we have this thing it's
closed addressing specific we don't care
don't need that oh those are heat
references that's a problem so what do
we do okay so this is your feed map it
is a struct but not in the C sense Wow
yes in the C sense but it's not a C
struct
it's a struct we'll call it a slot and
it contains an integer which is the
status which
tells us because obviously these things
always exist the status tells us
basically as a bitmask tells us if it's
present we can also mark remove slots we
won't talk about removing and also I can
got 30 extra bits to do naughty things
with if I want to store random boolean
method data it stores the hash just like
des decays and map does and it stores
along encoding so we got primitives cool
and this encoded key value pair so how
is that encoded oh well it's just done
with an abstraction you have a storage
engine and it's got all the obvious
methods you can write a mapping it takes
a key in a value and it returns along it
returns a bigger long because it can
fail and then it returns null the
metadata is the extra stuff you stick in
a slot in the status integer you can
free the mappings you can read them and
you can check them for equality which is
obviously potentially more efficient
than unmarshal in them this is slightly
edited because reality is always worse
than you want it to be there are some
other obscure methods in here to do with
caching Marshall representations in case
you have to run around multiple times
and then invalidating the caches and all
sorts of other weird gunk if you want to
look at it and ask me why the hell I did
things go ahead and I'll try and explain
it so we've got 64 bits to store our key
value we can obviously use all 64 for a
combined pointer we can store a 32-bit
key pointer and a 32-bit value pointer
but then we're up against a four gig
limit so we're going to run into
problems having really big maps we can
store an integer if the keys an integer
we just write straight down and then we
have a 32-bit point over the value and
obviously we do long keys directly
because of the contract of Long's hash
code just think XOR and do the math in
the EDD and obviously anything else you
like you can implement that interface
and do whatever you want your bike keys
short keys car keys you want to know
store zip codes something weird you've
got 64 bits to play with
so we have a map nice and simple but um
what are our pointers pointing to we are
going to need a replacement for that
Java heap where we store all the objects
so a native heap we want a block of
memory that we can write to and I've put
logical address space in brackets anyone
who's worked on the x86 done any kind of
low-level really local program in x86
that term should look familiar so should
that term it's exactly like this we form
an address space after pages our pages
are also not necessarily evenly sized we
allow them to vary you can specify a
fixed page size or you can specify them
in the maps and we grow them as you grow
the area and which means if you don't
know how big it's going to be you can be
far more efficient rather than wasting
dead space at the end of a page but but
what's what's inside a page it's a slice
of a byte buffer this is all just
straight and i/o it's a slice of a byte
buffer whether the slices come from they
come from back
it's an allocate direct and that's
effectively our physical address space
in reality the slice is obviously can
potentially come from different source
byte buffers and so what we actually do
here is we're going to allocate when we
start up and you say I want a 32 gig of
heap area we're just going to malloc 32
gig of space and then there's a thing
called a page source that's responsible
for carving those allocated byte buffers
up into pages as they're asked for
hopefully and that makes sense so it's
exactly like the be kind of like a small
portion of the memory model of an x86
and the a problem is this is flap we've
just got a byte addressable memory block
that we can write into but no control so
how do you manage that heap
we need a garbage collector I am going
to show the next slide our heap
allocator on native heap-allocated is
hands up if you've heard of Doug Lee
cool we use this thing called DL mallet
and DL malloc stanford ugly and anyone
who's ever looked at the the jess i one
six six staff is going to recognize that
URL so back in early 90s maybe even late
80s and ugly rotor an implantation of
malloc free in c and it was very widely
used it was i think it's still the
standard malloc free and lots of Linux's
used by lots of popular software
projects so what did we do we said well
we have a big byte addressable area and
we need to do malloc free well we don't
need to write malloc free it's been
written a million times let's just take
one so we took Doug Lee's malloc free
and we ported it now one thing to
remember this is free H cache where the
user throws us keys and values and we
have no idea what size they're gonna be
so we are the perfect use case for a
general-purpose malloc free if you were
taking our library and forking it or
you're embedding it you might want to
think about replacing this if you know
that your keys and values are going to
be fixed sizes because there's a lot of
overhead here with handling and making
sure you don't fragment when you are
dealing with random sizes thrown at you
if anybody wants to know how this works
they can either go to that website or
you can come and ask me at the end and I
will give you my really poor man's
explanation of how Doug Lee's night free
works that's it like it's not my code it
was just a poor it's a bit of awkward
porting C code to Java but it's not in
principle complicated
so we're doing pretty well we have a map
we have a heap we have a garbage
collector marshalling so we need to know
how to take a POJO and stick it in the
offi hands up if you think that's true
cool why is that true does anybody want
to voice an opinion or it's slow that is
indeed true I like to think of why Java
stylisation sucks you're right it is
slow and there are ways it could be made
better but many of the reasons that it
sucks is because you don't need it it's
self describing a civilization stream
contains a complete description of the
types that are within the string it
supports object identity if you have an
object graph and you reference the same
object multiple times the serialization
stream understands that and when you
deserialize you will only construct one
object and it will get all the
references right
it supports cycles in the object graph
most people don't have cycles in their
object graphs but this will handle it
and so therefore it tracks every single
object that goes in and make sure it
doesn't sterilize and twice it also
supports complex versioning most people
don't use this but it does it this thing
is heavy weight it's really kind of nuts
it's trying to be the all-encompassing
serialization mechanism for any
arbitrary use case that's to me that's
why it sucks it could be made slightly
better maybe without breaking its
feature set but the reality is most
people think it sucks because they don't
want it they want something with a
smaller feature set that's lighter
weight our problem is we have short
streams because we have a stream with at
most a key and a value and then you have
to stop the marshalling and save it
right it's the default sorrows ation
mechanism available in each cache 2x
this is why we have no choice we have to
get a handle on average
pojo that wants to be serialized in eh
cash 3x that we're working on now the
civilization scheme is completely
pluggable it obviously defaults to Java
Scala zation but into X you can't even
change it without doing hackery in the
source code so I'm going to serialize an
integer do you think that's it is it
more than that less than that hands up
if you think that's all I need cool what
about that is that enough more about
that or that or that
oh look there's two a what do we think -
a is - a is 42 that is 81 bytes for a
four byte structure if you were being
really mean you could say it's a one
byte value this sucks
enough heap we have a form of Gerardus
realization that is exactly the same
feature set wise but results in 23 bytes
it's this byte stream I think that's a
lot better it's still not ideal and this
is why in 3 I would recommend you
customize if you're dealing with
stupidly tiny keys and you know that you
can control them and right there in
serialize er
you've obviously plug in cryo or
protocol buffers or whatever you want as
well but again it requires upfront
knowledge the typeset so this is what a
realization stream looks like thematics
two bytes the versions two bytes and
there's byte flags everywhere there's
full-blown serialization new IDs it
encodes
the name of every field its type which
are luckily in this case the fields of
primitives but if the fields of classes
you get the fully qualified name of
class it's nuts so
that's what it looks like an off heap
it's structurally the same but what did
we do you kind of tell from there it's
where the 59 bytes okay
how many types are in my map all the
keys being the same type is extremely
common like probably 90 plus percent of
all use cases all values being the same
type fairly common probably one type
eighty percent plus two all that two
types or maybe three types ninety
percent plus so we've got a huge amount
of redundant data because most of that
realization stream was the object stream
class it's the thing that describes what
the class looks like so stick those in a
local side structure so it's just two
maps we have a map from the integer from
an interview to an object stream class
for reading we have a map from oh how is
that sorry I'm almost cursed get in
trouble what the hell is that thing
it's a serializable day Turkey because
helpfully enough object stream class
doesn't have an equals contract so
basically what we then do is we say on
read you look through the stream you see
the interview and you say what types
that I looked up in the map it returns
you up the extreme class that's the
object stream class that sterilizes - I
read the damn thing and I'm right I go
to write it and if there's a
serializable data key that matches my
object string class then I can pull the
integer and write it to the string
there's a whole bunch of even though
this is a tiny piece of code there's a
huge amount of logic around this because
you have to handle like class for
evolution you have to make sure that you
weakly reference everything and it
accidentally strongly reference a class
otherwise you could cause leaks all over
the place writing general-purpose
libraries isn't no fun so serialization
is pretty malleable this is what we're
using right faster script sorry class
the script you just subclass those two
classes and JDK use those everything's
done there's a huge amount more
customization you can do in Java
sterilization you can pretty much make
it do whatever it wants there's
obviously a bunch of overhead you can't
eliminate and
like why are always going to be better
but um but for us this was a this was a
brilliant result if the civilization
still sucks if you're using us directly
you can just implement that interface
you just say here's the object give me a
byte buffer that represents the object
I'll write up
give me the byte buffer here's a byte
buffer please give me the object it
represents and there's again there's an
equals method in case you can do that
more efficiently somehow obviously that
that would potentially evolve because
this thing is going to be called in a
safe context you can read out the buck
by buffer direct and compare if you know
where the fields are you can read direct
so am I on time yeah doing a little fast
we have more time for questions sorry so
we have everything we need except it's
not concurrent so what is the concurrent
map provide I've got time so I'm gonna
ask the audience what does the
concurrent map provide anyone what does
it give you that a map doesn't that give
you cool that's not the first thing my
slide
I wish keynote could do nonlinear
presentations by the way so I could go
you said that right show that one don't
show the first one dad don't tell me my
remote is run out of battery at this
point that's nuts right cool what does
it concurrent map provide what happens
before relationship to me this is the
most fundamental thing the next one is
what you said so the happens before
relationship says if I actions in a
thread prior to placing an object into a
concurrent map as a key or value happens
before actions if you haven't read the
Java memory model this might not make a
lot of sense to you basically it
guarantees if you stick something in I'm
going to wave my hands you will
immediately see it in other threads that
is some hand waving and you will also
see anything the writer thread did
before it put it in go read the Java
memory model
wait for your brain to calm down again
and then don't knowmaybe reread that
sentence it also does atomic operations
so you can say putting put if absent to
agra move to other place three agra
place those kind of things actually
that's all of them
but anyway thing is that's nice and
that's all the contract says we want
something more we probably want that we
want concurrent access we want to be
able to have multiple readers in the map
and multiple writers and potentially
multiple writers at the same time which
you can do with JDK eights concurrent
hash map here's the kicker you don't
actually have to have that to be a
concurrent mapper contract so it's kind
of we wanted but we don't actually have
to deliver it so happens before
relationships we need a happens before
relationship there's a bunch of
different ways of getting that we can do
volatile writes and reads those habits
before lationship there but you can't do
that on Norfolk memory location
please don't say unsafe we get into it
synchronized sweet I can get it with a
synchronization block some exclusion too
which is annoying but that needs a heap
object crap I don't have those and I can
do it with a bunch of other Java util
concurrent classes I can have I can have
locks I can have atomic things you know
barriers I can do lock support Park I
can do bunches of weird stuff but all of
that again needs a heap object
I need something to share the state
through or I need something to lock on
or whatever there is no way within the
JDK to enforce happens before
relationships between writes and reads
of an off people occasion without
resorting to some heap objects to help
you I said within the JDK so a bunch of
other people use unsafe to do off it
stuff I've never seen it to be a
bottleneck in our usages in our usages
is important maybe other people have
tried some performance requirements
maybe they maybe we've got bottlenecks
that I don't know about I've never seen
it to be a problem
the key thing after that is that
unnecessary complexity costs money we're
a business you have to support it which
is complicated particularly if I'd know
Oracle decide they want to remove access
to it and did you get all panicked
because you suddenly realized you build
your business hinging upon it you have
to maintain the solution which means if
unsafe changes its contract which you
won't be told about then you have a
problem it causes bugs but if there are
bugs in unsafe then you had suddenly
have to plead we are uncle to fix bugs
in an API they never promise to support
you can fix your car like this you will
successfully fix your car nine times out
of ten you will successfully kill
yourself on the tenth attempt so so we
don't use unsafe personally couldn't
speak for other people so simple
solution we have this we have an offbeat
map we have a peak memory area there are
dual Malek and we have a sterilizer you
just do that the wrapper and a heap
object we have a read/write lock we wrap
the whole thing we have a concurrent off
heat map that just says let's protect
all the RTP staff with a read/write lock
we write lock on mister things we read
lock on non-use to things and everything
magically works so we have a concurrent
map it has it happens before
relationship because the lock gives it
to us we get atomic operations because
the lot on mutation provides exclusion
so you want to output if absent lock
that map do the get see what's there if
there's nothing there do the foot
release the lock we don't really have
concurrent access though we have
concurrent readers because we have
multiple people take the read lock
we don't have concurrent writers because
there's exclusion so we need more right
concurrency so we just do that we say oh
we want two concurrent writers right
let's have two maps one answer I can
write to one map once they come right to
the other so keep looking on the
presence demoting getting confused so
you don't put what do you do you do the
PERT you take the hash code of the key
you send it into some striping logic
which is doing exactly the same thing
that the striping that the hash code
logic does when it's accessing the map
to find a bucket it's saying let's take
some of the bits while stir the bits up
find some bits and assign us to one of
the maps and as long as that assignments
table everything's good as long as you
don't you take the key don't you take
the key so we get Q value mapping and
that's cool there is one thing to watch
out for here and we actually got bitten
by this you have to be careful that the
striping logic in the concurrent map
isn't using the same bits to decide
which segment to go to or which piece of
the map to go to as you are then using
inside the map to decide which bucket to
go to otherwise you artificially cause
clustering there was an early version of
this for years and years ago that
actually did that we didn't find it out
until we borrowed at that time a 768 gig
UCS box from Cisco and Ram think massive
scale and suddenly there was a massive
degradation in performance and it was
because I had screwed up in the striping
logic and was sharing common bits and
everything fell apart you end up with
things going sort of not o N in the map
but at least Oh some fraction of N and
performance drops through the floor and
you wonder what the hell went on and
your manager says what did you do and
then you go I'm an idiot and I fixed it
so obviously a second perp is just going
to do that and a third / just going to
do that
so we have more right concurrency where
she have complete solution we have a map
like an current map we have a
replacement for the Java heap we have a
replacement for the garbage collector
and we have a replacement for class that
logic so I'm actually slightly ahead of
where I want it to be we're gonna have
20 minutes of questions by putting dirty
fast I apologize you can always watch it
back on like half speed on YouTube
simple engineering simple engineering is
simpler to support and maintain do not
over engineer your solutions even though
when you get into this kind of thing
it's really tempting like you start
playing with this stuff and you get
going i'ma like code ninja I'm gonna
write the best off EP implementation
you've ever seen it's gonna use unsafe
it's gonna do everything magic it's
gonna be insane but if you don't need
that insanity if you don't need all that
fancy stuff if you don't need that
performance if your testing doesn't show
that you're suffering from not using
unsafe don't do it simple engineering is
simpler to support and simpler to
maintain and at the end of the day
there's the whole thing about assume
that the guy that maintains your code is
a psychopath who knows where you live in
actual fact it's UVU which might be the
same thing you are not going to like
your former self if you write an overly
complicated version and going off heap
doesn't require unsafe unless ultimate
performance is your primary concern if
you really do need and insanely
performance off heap solution then by
all means go unsafe just realize it
comes with significant caveats so that's
kind of the end but there's a bunch of
other stuff going on in the library that
I didn't talk about so we didn't
actually talk about how we cache the
whole thing supports weekly consistent
iterate is like you see in concurrent
hash map and doing that's not easy when
you have to deal with the fact that Matt
might be resizing underneath you we do
this thing called cross segment eviction
I'm gonna describe all these because I
have time and you're stuck here well not
really you can leave but also in case
you want ask questions so cross segment
eviction is if you wanted to put a
really big value in the map and you've
got a multi segmented map but there's
not enough space allocated to your
segment to fit it and then you need to
kick things out of other things in your
cache to first base in that one page
d-link helps support cross sigma fiction
is to do with stealing pages from other
places using page allocation stuff we
have to be able to compact the native
heap to try and prevent fragmentation in
certain bad scenarios we have to be a
rehash to grow and shrink which is
something most maps don't do there's an
entire off memory solution involves SSDs
there's an entire another layer that I
can maybe talk about although it's still
prepared for you at the moment that does
proper like acid light durability so if
you crash the machine everything comes
back there's this hideous thing called
entry level pinning which allows you to
prevent certain cations from being
evicted and I'm not sure I ever want to
talk about it again I mean there's
probably other stuff that I forgot about
here and yeah so if you want to ask
whether animate in a second then we turn
if you have any questions about why I
showed you can ask those if you have any
other questions short and questions by
the way we're hiring we're doing a bunch
of fun stuff not just well with off heap
but moving on from this and doing
fancier feed data structures and we may
even start using unsafe at some point
who knows if you're interested in very
low-level Java development distributed
systems or heap solutions data modeling
Java 8 streams lambdas or anything like
that
people are looking puzzled then come and
talk to me that is the github URL again
the drone that you might have seen it
asks bar booth is being raffled it's
7:00 p.m. so that's in about 15 minutes
or so in front of our booth
if you want it and you've entered then
you're gonna have to be there I promise
I will end the QA before because I'll
probably have to be there I'll get in
trouble that is the end of my
presentation does anybody have a
question questions I can't see so if
you're holding your hand up yes native
heat compressions so one of the things
we run into is the when you're running
it as a cache so if you feel if you have
one of these right and you fill it you
smoothly fill the native heap so you've
got this big block of contiguous
allocated memory right and then what
happens is and you if you start evicting
because it's a cache you end up
producing random holes everywhere but
wouldn't normally be a problem because
new puts coming in can refill the holes
in it becomes a problem when you're
dealing with a multi segment a multi
segment cache where I suddenly so
suppose I've got like two segments right
200 Meg because we'll deal with small
numbers because they're easier I fill
the whole thing it's been a victim for
ages so what I've got is 200 Meg but
with holes all over the place and then I
try and stick a 120 mega in the problem
is that it needs to kick things out of
both segments and the way it does that
is by stealing pages out from underneath
the segment you're not assigned to but
in order to steal a page it needs to
free everything and in order to do that
it needs to compact all the values so
that so that there's a page it's
completely unused there's more detail
than that but it's basically the in DL
malloc the way it works you actually end
up with a with a we'd be able to
navigate the assigned pointers and so
the way it works is we just find the
last thing take it try and reallocate
and move it down and just do that
consecutively until we've
cleared up as much the fragmentation as
we need it's a pretty rare occurrence
the problem with writing and
general-purpose library where you don't
control the use cases is there's almost
no eventually you end up having to cover
every single edge case because
eventually because there's a large
number of users it's not like you
control the usages eventually every
single edge case gets found and every
single edge case has to be fixed I won't
say it's bug free but it's probably the
most bug free piece of software I'd ever
written just because it's had five years
of being bashed out by thousands of
people and every bug that's found we've
had to fix because we don't we can't say
Oh we'll just won't tickle that one
because some customers going to come
along and be awkward that was not a
great explanation there are not great
explanations for some of these things I
apologize
so okay so the question was why did you
did why did I decide to go with Malcolm
free rather than just directly
allocating I assume you mean just
directly allocating at the RET byte
buffer when I need to store something so
there's two problems with that one is
you now have a direct byte buffer that
you have to keep a reference to that can
only be kept in the heap and so suddenly
your sit your basically your heap usage
is linear in the number of entries in
the map which is obviously better
because you're not you're only storing
the reference but the problem is the
thing that tends to cause GC overhead
isn't the size of the heap it's the
amount of walking it ends up having to
do how long it takes to walk the heap
and if you have a direct byte buffer for
every entry you have a reference for
every entry and the heap and the GC ends
up having to walk it the other thing
that's sort of that's the main reason
there is another reason which is that
whenever you allocate a when you have
equal allocate direct it not only does
it malloc but Java for security reasons
because of the sandbox will zero out the
memory malloc some math and potatoes
will do that too some of them will do it
more efficiently like they'll background
free stuff and background dearest of all
things I can get away with not doing
that and being cheeky because
everything's inside the same map so what
I actually do when I'm doing like free
is just malloc it and if it happens to
be dirty it doesn't matter because I'm
about to write through it all again so
it saves me basically one I end up only
writing half the bytes I would otherwise
have written cool if anyone else is
holding their hand up and I can't see
them because the projector is blinding
me just talk cool no more questions
thank you very much please come down if
you're too shy to ask a question I will
try and remember to look at the website
where you can ask offline questions my
twitter handle is behind me maybe you
can tweet me questions and I will
endeavour to get to those this evening
I'm running a buff tonight at 9:00
if you feel like staying up in the venue
until nine o'clock you can come to my
boss it's nothing to do with off
and if you don't hate me now you'll
probably hate me off my boss so so
please come to my boss and hate me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>