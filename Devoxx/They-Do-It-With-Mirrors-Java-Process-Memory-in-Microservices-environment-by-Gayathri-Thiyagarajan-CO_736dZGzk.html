<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>They Do It With Mirrors – Java Process Memory in Microservices environment by Gayathri Thiyagarajan | Coder Coacher - Coaching Coders</title><meta content="They Do It With Mirrors – Java Process Memory in Microservices environment by Gayathri Thiyagarajan - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>They Do It With Mirrors – Java Process Memory in Microservices environment by Gayathri Thiyagarajan</b></h2><h5 class="post__date">2017-05-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CO_736dZGzk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're against inflation do it make sure
I didn't even mention the first part but
that's a clue to the theme of the stock
I hope you find it useful but me I'm guy
takea Rajan I'm a senior software
engineer in captive and iek engineering
team I've been the java g3 developer for
over 12 years now currently I'm working
for a major global parcels operator in
the UK and part of the integration team
building Microsoft's architecture the
last year I co presented a talk at
devoxx about harnessing domain-driven
design principles in distributed systems
and this year i'm hoping to talk about
some of the pain points we hits recently
in our micro services environment
especially around resource management I
like to begin by highlighting with some
of the well-known micro services benefit
there is a reason for this slide we'll
see that later we know a micro services
or loosely coupled services separated by
bounded context they're usually built
around business capabilities using
cross-functional teams they are highly
independent they are independently
deployable so they are highly scalable
as well they're generally polyglot which
means within the same application you
can have multiple micro services using
completely different technology stack if
you wish or if you're crazy but in the
context of this talk I'm going to use
the term polyglot really loosely to
mention within same applications it can
have different micro services running
different versions of technology and
frameworks such as Java and finally the
size of micro services they are
bite-sized or human sized
which means a developer starting work on
a micro service or project can check out
the code for just micro services without
worrying about running the whole
application is local
but microservices or lady silver bullet
north or a term from some human golden
hammer they come up with they come with
complexity of their own Martin Fowler
famously said you have to be dis told to
do microservices some of the things he
lists out or being able to rapidly
provision rapid change deployment how
basic monitoring alerting and tracing
mechanism but in his blog on micro
services trade-offs he mentions
operational complexity distribution and
technology diversity as some of the key
challenges in adopting Microsoft's
architecture I think there is one more
challenge that is massively overlooked
which is the societal evasion aspect of
it but why should we care about
resources in particularly memory because
it is hard memory has always been a
challenge in computer programs we must
have all spent ages trying to get to the
bottom of memory leaks and out of memory
errors
I think traditional monolithic
architecture tended to focus more on
this aspect of application deployment
than microservice architecture does
because the focus rightly tends to go to
setting up operations maintenance and
monitoring around microservices but it
is important because memory is a key
aspect of computer programming Ram has
come a long way since it was just
transistors and capacitors but these
days with lot of in memory processing
and faster read and write response times
expected from computer systems memory is
more important than it has ever been for
faster scalable and responsive systems
but also it is our job as software
engineers it is our job to make sure
that system resources are consumed and
used efficiently and responsibly and
finally there is a cost to everything
average Pyatt price per gigabyte of ram
has come down from $100,000 in 1990 the
under $5 in 2015
but we'll see later on that even though
memory has become cheaper because of
time it doesn't come for free first
let's look at some basics so in the
context of this talk when I say memory
can mean two things first one is native
memory which is the memory allocated by
operating system to any running process
next one is our JVM memories our heap
non heap if we look at native memory in
details not too much detail it refers to
operating system in our case Linux the
memory allocated by Linux to a Java
Runtime process Java Runtime itself
consumes native resources including
native memory which makes it quite
difficult to get a overall picture of
how much memory is consumed by a Java
process this is even more difficult in
case of micro service architecture
because every micro service is Java
process so you can imagine how difficult
it would be for getting how much memory
is consumed by an application as a whole
but if we take a step back ram is the
physical memory where the process stores
command and data to implement a piece of
logic but there is a physical limitation
to the amount of RAM we have so most
operating systems use virtual memory to
make efficient use of this available Ram
so that multiple processes can access it
and use efficiently but it is possible
to allocate enough memory to run out of
the available physical RAM in which case
the operating system will be forced to
take extreme steps we'll see about that
a bit late selects is a vast subject and
the next memories or subject as well so
I'm not going to go into the gory
details of it I just want to call to
your attention a couple of key
terminologies which is relevant to this
talk
the first one is resident head size or
RSS portion of memory so this is the
portion of memory allocated to a process
in our case Java process in the main ram
so it doesn't include the swap pages
hence it's a key metric to your
microservices memory footprint the next
one is virtual size memory or we have
said it includes all the memory that the
process can access that means it
concludes your swap pages as well a
simple top command would give you
process by process view of how much RSS
and we have shared memory is consumed
let's move on to JVM memory this picture
gives you a good idea of what JVM
memories in the context of host
operating system as you can see JVM
memory is a combination of heat pump
Genesis Pregel
on Java stack plus a portion of memory
which is occupied by class loaders
garbage collectors and other internal
JVM data structures just code cache see
the older heap is the place where Java
objects are stowed and this notion of
memory is allocated upfront in the Java
process data and it's also maintained by
garbage collectors real object
references and perm journalists place
where class bytecode is stored and we
have to keep in mind that it's not just
our application process that goes in
here there's also your third-party
library classes application server
classes plus Java Runtime itself needs
to load classes dynamically and during
runtime finally Java stack so every
thread in the Java process needs the
storage stack which is your local
variables and ignis maintain state when
calling functions on top of this it
needs to store thread local information
as well
so with Java 8 this means some change to
the memory model as you can see or not
see very clearly Farmington has been
replaced by meta space to use native
memory directly so we have covered our
basics let's see how all this can
graveled in our micro service
environment in the first place so this
has I think marks of typical murder
mystery the kind you'd read in a crime
novel in fact I'm a great Ithaca city
fan so that's the clue in the title so
I'm going to draw paddles from some of
her book titles to explain our case I'm
not jerking so that is an actual killer
and everything so setting the scene a
bit so we have what 30 micro services
run in the collection all looking calm
and quite nicely
working together three or four
brownfield project which means our micro
services are running on existing
infrastructure
we're not deployed these containers no
use cloud but so we started our
developing our micro service in 2014
so we've hourly microservices ease Java
7 the our packages spring boots at yours
and use Apache cameras our integration
framework on some of our early micro
services run the own tomcat spring Roo
template Tomcats use maven as our
automation tool but when Java 8 came out
we started using Java 8 for our new
micro services but at the same time we
made some changes to our version of
spring boot and Campbell that may be
agree they're using with latest versions
of course and we also switched from
tomcat jetty and finally don't ask me
why where we fished from maven to grade
level so not a fan to really but events
where a second motion much before it
came to our attention so at that time we
had about two applications about 10
micro services running on the newly
migrated
stack in correction but things started
to come to our attention when we
deployed our third application versus
just one micro service into our dev
environment we started noticing that
some of our micro services were just
randomly stopped and by stopped I don't
mean gracious shut down but just
randomly killed so we thought it was
just random accrue someone being really
naughty and killing off the processes by
mistake but as time went by we started
noticing that this was happening at all
times over weekends and also this is
happening more frequently at one point
we couldn't predict which micro services
would be running so it took away the
dependability on our environment and we
were constantly worried that this would
happen in the middle of client demo war
u18 so what do we know up to this point
so we know something is killing our
micro services seemingly randomly it's
not a gracious shutdown
it's like an epochal and finally there
is nothing in the application logs to
indicate why this was happening but
looking elsewhere we found one entry and
the messages dot log where Linux log
system related messages simple googling
let us see out of memory killer at all
mr. Killer so as your killer so computer
programs normally tend to allocate
memory when they start up and they don't
normally use all the allocated memory so
most operating systems use what's called
overcometh model to use the main RAM or
the physical memory efficiently and
between processes so if a process uses
all the allocated memory then the
columns will obviously provide those
resources to the process but when too
many processes are to use all the
allocated memory then this overcometh
model can be really problematic on the
curl must start killing processes in
order to stay operational so under such
desperately low memory conditions the
out of memory killer
a step in and pick a process to kill
based on heuristics that has a world
over time so it's done basically based
on set of rules particularly any process
or its child process which takes up a
lot of memory or a minimum number of
courses that needs to be killed to gain
enough memory or find high score in
terms of badness the root or kernel
process or assign low score obviously so
as you can see microservices which are
Java processes internally make ideal
victim for out of memory killer there
ways it can disable the OM threat but
you certainly don't want to do it
because it's there for a purpose which
is to safeguard a service even though
the killer thread is the reason for
Microsoft's dying what we realized was
it was not the cause but just an effect
as you can see even though our micro
services are loosely coupled in terms of
functionality in terms of the underlying
host resources they are tightly coupled
I know what you are all thinking
containers say it is true that if we had
run our micro services on containers it
wouldn't have made our operating system
to kill off random the other micro
services so even though containers offer
certain amount of bulk bulk heading and
isolation I think the problem of
resource utilization and tuning it still
exists because it could be the micro
services running within containers could
be running out of memory make up causing
the container to stop and restart
automatically without coming to your
attention for a very long time but
secondly but most importantly JVM
processes if you don't set explicitly
the memory limit on them they tend to
ask the host operating system for how
much native memory is left so by default
if the limit is not set on the process
they'll allocate a portion of native
memory
so it should mean that they would ignore
the container imposed memory limit there
is an experimental support added to Java
9 which would force JVM process running
in container groups to actually owner
this limit as the maximum available
memory like if you're using Java 8 or
Java 7 it's still a problem so you have
to set memory limit on your process
so obviously this hasn't happened yet
unlocking for us in now protection
environment but we knew it was only a
matter of time because we could already
see our - codes that our servers or
really running in full capacity in terms
of memory so we did the first obvious
that we did was throw a lot of memory at
our servers but to our surprise all of
those newly added memory was immediately
taken up by the micro services when they
were restarted so after that fails
attempt and as the next immediate step
we split our micro services under on
them on reduce nodes now this was ok for
our node even our peak load but it took
away the scalability aspect of it
because if there was a sudden surge of
load to our applications this without
being a real problem for us so as you
can easily guess this is real problem
for us at the point so we had to get to
the bottom of this so we had a few
suspects first one being Tomcat
having memory leaks totally or it could
be non Java process we had Redis and
Haiti proxy running on the same boxes or
it could be the fat John's being really
fat we also suspect a Java 7 and family
which we realized later just because
it's not latest but we knew we were on
to something when we started looking at
the size of our micro services and by
size here I mean the size of our package
jars so we tweaked few parameters around
this and the result was actually really
rising the irony of microservices
running as five jars was lost on a lot
of people say we read about a lot of
jerks around that so recall this
exercise of spitting micro-services on a
diet so all three relates with each
microservice is unique just like we are
so which means we have to pay attention
to the dependencies we were bringing in
in terms of bill five so for example if
you had spring boot start web in your
belt bill file it would by default
bringing you tomcat embedded tomcat if
you don't explicitly exclude it so we
used Gradle dependency plugin and may
those dependency graphs to see which
ones they're obviously irrelevant so if
you obviously pruned it so if you for
example so this sample order service
which simply takes a request saves order
information to the database and then
sends back response initially without
any exclusions added it's about 100
megabytes which two cups is just
Activity Monitor gone on my local as you
can see the first one which takes log
1.7 gig they trimmed on even though the
size on event my event down by 20
megabytes you can see it saves about
point 5 gigabytes of memory so the
lessons learnt wrong dependencies pay
attention to what's being brought in by
your Bell file and also check for any
patches on these dependencies as some of
them might have fixes or memory leaks so
apologies for the blood of names
client confidentiality and everything
another revealing aspect of this
exercise was guarding the clarification
jars micro-services or famously
self-contained services with little or
no dependency on anything else
so what we tended to do with sticks of
the shared corridors shared
functionalities packaged them as shared
libraries on
we used it in micro services across the
platform so this is one such adapter
which wraps soap request to downstream
services so we save some in terms of
latency but what we follows the we are
packaging it as fat jars since he
doesn't have to run on its own we could
have actually packaged it as regular jar
saving a lot in terms of size of jar and
then subsequently the memory also
modularize you shared code into multiple
libraries I would even go so far to say
that duplicate code is it's just one
class or just couple of methods that you
need to reuse instead of bringing the
whole shared library so I mentioned
earlier that we suspected Java 7 I did
say I'm Charlie because when we actually
looked into the memory conceived by Java
7 based microservices and compared it
with Java 8 based microservices they
actually found that Java 8 these micro
services were consuming significantly
more memory than Java 7 based
microservices so in Jolla aid came up we
couldn't wait to use it for all the
functional programming aspects of it but
what we didn't know was the memory
changes it brought in so going back to
our previous slide I said Tom James
being replaced by meta space which means
you no longer need to you income gen
anymore and you wouldn't get the out of
memory error income gen runs out of
space but now meta space directly uses
native memory so by default there is no
size set for meta space and what's most
by default it dynamically expands with
the needs of application so this is your
previously advanced sample order service
with no memory limits set with the right
memory limit set this is now um
consuming significantly less memory than
it was before so it's fair to say that
the polyglot feature of micro service
has turned into an and pattern in case
of on so now that we understood the
cause of our issue which is a Java 8
memory changes let's look at some of the
JVM parameters key ones that will
actually instruct JVM to make proper use
of the memory and distribute it across
microservices first one is basic he
unknown heap xmx ping maximum heap and
excellence the minimum this is a change
in Java 8 so instead of saying perm
general size you're saying meta space
size to the size of allocated meta space
to your java process and by default the
default value depends on the platform
and available later when we have next
one is max meta space size so this is by
far this has had the maximum impact for
us this allows you to limit the amount
of meta space used by Java process as I
mentioned by default there is no limit
set so it dynamically expands in some
cases you might generally want this
medics pace to grow but you can control
the rate at which this grows by setting
this value if you set 0 then it makes
the meta space to grow less aggressively
but again it depends on the needs for
each micro service you have so people
need just like a service so what we have
also observed is certain design and
implementation decisions seem to make
difference in terms of memory consumed
so for example if you compare a micro
service like the last one which simply
takes a request and passes it on a soap
request converted into a rest request
and the passes it on to your downstream
micro service know maybe a little bit of
transformation but lower operations
whatever so that takes significantly
less memory compared to the one in the
middle which is
the order service naturally does some
database operations but what seem to
take most space is first one which does
batch processing so naturally there is a
lot more data there but what's useful
from this is it lets you plan how you're
allocating memory or distributing memory
across micro-services depending on what
they do and also you can distribute them
across nodes depending on the load you
have so how does the situation compare
in case of moonlit my two things on this
are with microcircuits there are a lot
of things that one needs that needs
one's attention like typically worried
about how do i split my processes were
my boundary context what's my aggregate
and not mention the whole distributed
system complexity that comes with there
but with monolithic application you
tended to focus more on resource to your
name Viviane tuning there also with
micro services like I said each one is
unique so it's not just one or two
processes that need you need te you'll
be hand cranking in our case two dozen
micro services they're not saying no
little better than micro services but
overlooking this aspect which is the
resource utilization actually takes the
rhythm of the perceived benefits of
micro services and in some cases turns
it into anti-pattern also is Maalik
better you run it in your local
environment or on the remote server you
always get the whole picture but with
micro services this is as with any
distributed system this is challenging
at best importantly other times it's
also very difficult to observe issues
such as memory because you're only
running slice of application at any time
in your local so for we saw what was
causing the problem in terms of memory
in our environment and soft ways we can
actually tell JVM to efficiently use the
allocated memory but
I'd like to briefly touch on some of the
tools and commands we used during this
exercise so these are traditional in X
or JVM commands nothing to do with
micro-services while if we can use more
sophisticated profiling tools but these
did more than job for us in terms of
getting information to metaphysical
ization the garbage collection
information and how much native memory
is left on our servers sitter
the first one is active Activity Monitor
which comes with Mac I guess in terms of
them days you can use tools furniture it
gives you quite similar to your top
command of a new light on the next box
it gives you like how much are said vias
adds concluded by process
so next useful tool is J console which
comes with your JDK it gives more
graphical representation of how much
heat meta space and also some
information about garbage collection and
you can trace it over time as well the
next one is chased at this is a built in
instrumentation on your JVM this provide
information or performance starts about
JVM processes especially our meta space
utilization and garbage collection so
this one was run on a Java 8 process and
the first one if you run it with JDK 7
you can see there's nothing under poem
punishing capacity utilization but for
the same process you can see there is a
matter space capacity and utilization
that's in terms of kilobytes it rides
with gcutil
you get same information but now it's
meta space utilization in percentage of
the total matter space capacity and you
also get number of full GC events
triggered since process started running
so to many would indicate a memory leak
this is another useful option that can
see the reason why the last GC was drawn
to Z
Sherly allocation sailors means to run
out of space they need to recollect some
of the memory next one is JC MD which is
quite useful to send diagnostic command
requests plus some other we'll see in a
bit - running Java process so it prints
performance counters for starters and
it's just one third of what it actually
prints on screen next one is heap dump
which is quite handy to detect memory
leaks there are three ways we can
generate a heap dump one is generated
automatically and you can specify
through JVM parameters value want to
keep them to be generated the next one
is through J map and let's another use
of gcmg you can use JC enrages I think
the preferred way of generating heat
dump or running Java process you don't
have to actually wait for the process to
run out of memory so these commands
output has profile which then said to
your memory and licensors eclipse heat
memory analyzer will without leak
suspect report actually this is a
screenshot of leaks respectable for one
of our micro cells is actually useful
because as you can see it shows a
suspected cash values histogram what we
found was one of the Netflix history
library was so this class within that
library was pre loading hundreds of
instances of HDR histogram class most of
it was not used and each instance with
was 41 kilobytes in terms of memory
consumption so we went and looked for a
later version so this goes back to
checking dependencies I'm looking for
memory patches so we found a later
version which fixes this memory leak so
I am aware that so far I've been talking
about JVM memory and how we can get
information such as metastasis little
ization but all this started when we
started running out of native memory but
just in mind that RSS is a combination
the RSS memory we saw earlier
is a combination of heap non heap plus
Metis based so it is a key memory
footprint so some of the useful commands
I find to get the bottom of this are
free and top the free digi a quick
overview of how much memory is available
on server how much is utilized but the
top command as we saw earlier gives you
process by process view of how much RSS
and reasserts been conceived again RSS
is the key stat we are looking for the
process mem infer is even more useful
because it actually gives you the jar
associated with your Java process which
means you know which micro-service is
concealing what amount of RSS memory the
last one is native memory tracking
systems again a new feature in java 8
there are three steps to get native
memory tracking information
the first one is enable nmt you can set
it as summary or detail levels so that's
done through JVM parameter let's
establish a baseline as an SD Java
process starts up and finally you
continue to monitor memory changes so
you can do that through j CMD command
say automatically compares to the
baseline and lets see how much memory is
being consumed but if you have a really
small memory leak it might take a while
for you to get to the bottom of it but
if the leak is significant then you can
spot it almost immediately
so on top of the performance on top of
plasmons implication to all of this
there is also a cost implication if
you're deploying on cloud while cloud
offers distinct advantages in terms of
rapid change deployment rapid
provisioning and managing your research
answering monitoring your resources
managing the resources in efficient
possible way is totally up to us there's
only two pricing models to cloud
computing first one is elastic or pay as
you use did this actually brings jake'd
focus the cost of using or utilizing
your system
resources which means more you consume
the more you pay and then the other one
is the fixed cost or subscription-based
model where the consumer pays for a ream
of particular spec on a monthly basis
but on both these pricing models you can
see the cost is directly proportional to
the consumption of system resources so
it's actually more important when you're
deployed on cloud to make sure you're
managing your applications memory
footprint so this table gives you a
quick snapshot of how the prices compare
across different I as providers as you
can see the more RAM whether you use a
fixed more e or an elastic model the
more you pay in terms of CPU cycles or
Ram so key takeaways so always remember
Daniel Bryan's first of seven deadly
micro services sense which is don't rush
into using new technologies until you
understand what's going on under the
hood
especially around resources and memory
management one key stack change at the
time I would even suggest that you run a
scalability test not just when
functionality changes also when you
change your stack or using your version
of library it also not just at
application level run the scalability
test at server level think about memory
at every stage beat operations design
implementation maintenance or monitoring
finally memory is cheap not free before
we go I'd like to quote a couplet from a
comer literature I'm not going to read
it aloud and I'll get back to the
translation in a bit the intended
meaning of this verse is to emphasize on
knowing one's capacity and strength and
operating within that capacity
now this was written almost 2500 years
ago but I think it is still 11 today
particularly to our talk because if you
replace peacock feathers with
micro-services and the cartridge server
there's your link thank you so much for
listening look at the link from last
year but I checked it still works
question right
of the the captain mouths and territory
first question on what percentage of
meta space it is safe in the end because
it grows dynamically but we did see
significant change in terms of meta
space like I showed you in the previous
slide it actually came down from being
in gigabytes to megabytes so this'll be
but it changes with environment that was
my local changes when you write on the
next boxes so
hi you mentioned used a CMD to get to
the bottom and do your baseline
comparison for native memory leaks yes
how about getting the root cause when
you get the opal from James Jessie MD
sorry how about getting the root cause
for that so it was it was more like
walking third so I tried using native
memory tacking when it actually became
evident that it was meta space which was
actually ultimate course not
understanding what the bamboo changes
where so I just simply play around with
native memory tracking so we didn't have
any significant memory leaks but it's
more useful many how memory leaks and
then you actually want because the stats
that it prints out in a dilemma
attacking it's too much
so they actually used that to get to the
bottom of a shoe in the first place but
if you have an idea of what's going on
overall and you want to keep a close eye
on your battery footprint and that is a
useful tool to have
I guess no more questions Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>