<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Netflix Keystone - Real-time Stream Processing Cloud Service by Monal Daxini | Coder Coacher - Coaching Coders</title><meta content="Netflix Keystone - Real-time Stream Processing Cloud Service by Monal Daxini - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Netflix Keystone - Real-time Stream Processing Cloud Service by Monal Daxini</b></h2><h5 class="post__date">2017-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YrENL4J-fGk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'll be talking about our
Keystone pipeline that you built up
Netflix and the stream processing is a
service platform as you all may know
Netflix is a data-driven company and we
generate over three petabytes worth of
events every single day so that we can
derive business insight and give our
members a really good experience when it
comes to watching your content and what
kind of content we can offer our
customers so typically to process these
real-time events has meant that majority
of the time is spent in building out the
infrastructure and handling the data and
prepping the data and very small time
gets left to actually do the data
analysis so we're trying to reverse that
equation so that our engineers and data
engineers can spend more time doing the
data analysis and less time having to
worry about the infrastructure they
built so to that effort we are building
a self-serve multi-tenant and
fault-tolerance stream processing as a
service platform so that all the
engineers and Netflix can leverage it
I'm when all that she and I lead the
stream processing infrastructure team at
Netflix and I'll share our experience
today about building such a platform and
the lessons we've learned along the way
so we look at in this pipeline to look
at what are a stream processing service
as a service looks like how we evolved
it in run time while keeping it up 24/7
and we'll pepper it with the lessons
learned along the way so we have over 93
million members in over 190 countries
and our service gets accessed from over
a thousand different kinds of devices
and each member not remember but overall
we have about 125 million hours of video
is watched every single day and this
leads to us generating over a trillion
events a day to learn more about what's
happening with their service how our
users are interacting with their service
and we can learn more about
how our service is doing so how many
Netflix members in this room oh that's
great you what you're one of those 93
million so in this pipe lines when you
have a lot of these events going through
our system having robust ingest
pipelines is very critical because that
enables you to do additional
functionality on top if you're not able
to use successfully and reliably in just
these events then all bets are off
because you don't have enough data to
gain insights into it so at a very high
level we have all the events that are
being produced at the devices they end
up in our head services and from there
they make it their way into our Keystone
pipeline that's our ingest pipeline and
that's built on our stream processing as
a service platform so it's a product
that we've built internally that we
offer to our internal data engineers who
are our customers and other engineers
and we do a bunch of processing whether
its user specified in terms of a DSL or
point including can ism or they're at
custom code and the data ends up as a
results in some kind of a sync it's a
pretty high-level view of how the data
flows so our stream processing service
has three ways we offer people to write
jobs to solve progressively difficult
problems
someone has point-and-click pipelines
this is the core infrastructure where
you realized an engineer realizes that
they want to create a new event stream
and there are some interesting events to
be captured so traditionally you'd have
to you know set up some kind of a buffer
in the front so you can hold your events
you'll have to worry about the
infrastructure you'll have to worry
about where you send the data so to
simplify that what we've done is we've
created point-and-click pipelines so
user comes in the provision a stream and
then we provision for it
the second way a user can add
functionality so that they can derive
insights is by specifying
during in projection through a DSL that
you provide this is an active area of
development we have we're looking into
enhancing this quite a bit but today it
just supports basic filtering and
projection and in the third way is user
can specify and write custom code this
is built on the infrastructure we have
there it makes it very easy for them to
get the base code starter they have
reusable components they can use to
build their services and the CI
pipelines for them is already set up so
they can do continuous deployment and we
have integration to docker so they can
easily run what they build even locally
on their machine so the first section
will look at the point-and-click
pipelines and the DSL so user usually
sees this when they land on our page to
create a new stream so the provision of
brand-new stream here we see that the
streams already provisioned it's an
example of a stream here what this ends
up doing is at this layer where do you
see where receipt says Keystone we
create a brand new Kafka topic called
seal event once the user clicks on a
create stream event we ask them to enter
a bunch of config information one of
them is what's the name what's the
approximate rate that you expect to send
events into what's the bitrate basically
now it does not have to be exact it's an
initial number we automatically adjust
to it as the traffic changes at a later
point in time but we want an approximate
guess so that we can provision the right
kind of resources for it and then the
user configures where they want their
events to be routed to do they want it
to go to hive do they want it to go to
elasticsearch and what you see here
recall key do is just under Kafka
cluster so that we can route it for
further processing and analysis
every path that you see here from the
producer to hive or producer to
elasticsearch and producer to k2 is
isolated at this processing layer so
that if you have slowdowns
in any one of your syncs it does not
impact the processing of events that are
flowing to the other system the first
point of entry into the system is Kafka
so we have ways to buffer your data so
that even if your producer goes down we
don't lose data and the next thing in
the configuration UI that they can do is
specify filters now this is basic
filtering that they may want to do based
on the data that's going in and here's
here's an example so we support an expat
style it's our own internal parser that
we built and this is an area that we
want to make more enhancements over so
here it's a very simple example where
based on a certain field the event type
it gets routed to a certain cluster or
it doesn't get routed so the stream that
we looked at here going to hive is a
subset of what ends up in elasticsearch
in Kafka and we can multicon have
multiple routings to the same sink and
each one can have slightly different
parameters or slightly different filters
and and projections as well and what we
call sinks is also called outputs here
where your results end up at so in
addition to you provisioning a Kafka
topic launching a bunch of stream
processing jobs with the filters you've
specified and scale that appropriately
make it one 24/7 we also generate a
bunch of dashboards and alert
configurations so we generate dashboards
for tracking how fast the streams doing
are there any errors in the stream where
the filter set up correctly is the
difference between what's filtered and
what's coming in the right so we know
there's no data loss there is it is the
stream lagging behind so when you're
processing a stream of events you want
to make sure you're able to keep up
because your buffer is only so large in
the front if you're not able to keep up
and you're not able to process as fast
you may actually lose those events so we
keep track of how slow or faster
processing is of this event so the lags
don't build up in the system and it also
impacts the latencies downstream if
further astray processing needs to
happen and for alerts reduce set up
alert configurations for each of these
mini pipelines or we like to call them
data streams and if they're having
errors if they're lagging behind we do
get alerted and then we take appropriate
actions we have automations to deal with
it so for example if you have a certain
spike in input requests and if it lasts
for a certain period of time then we
scale up our infrastructure so that the
number of jobs the number of instances
for a jobs that are running will
increase so we can keep up the traffic
and after a certain period of time if
the traffic dies down we can scale down
and we can also handle failures across
region right what happens if we run an
AWS or what happens if one region has
issues and our traffic gets evacuated to
a different region now the other region
because all the users are being routed
to a different region we could to get a
lot more events that we were getting
before so we scaled that region up to
deal with that additional traffic or
events that are flowing into the system
so let's dig a little bit deeper and see
what's happening inside the
infrastructure that we call the Keystone
pipeline so the redline boundaries here
everything in between is an
infrastructure we do have a couple of
components for making it easier for our
producers to produce events so we have a
library that's a wrapper on top of Kafka
and the reason we do that is we want it
to be better integrated into our metric
system into our ecosystem into our
platform that makes it easier to write
applications and it also lets us do
something called routing so we can
change a map here which tells us which
kafka topic to go to or which cough a
cluster to go to so what that means is
we can dynamically change that in in
case of a failure and I'll go over it in
a bit later under this presentation the
other option we have is for a non Java
based or third-party apps that can only
talk to you like an HTTP proxy so if you
have a way for them to send events
through proxy all those events land up
on a whole bunch of fronting Kafka
clusters we run over 50 50 to Kafka
clusters and around close to 4,500
brokers in three regions so it's a
pretty large fleet that is dealing with
you know trillions of events a day and
if you notice here there's a Kafka here
and then you see a bunch of Kafka
clusters down here the reason for this
isolation is we want really high
availability of these Kafka cluster so
that we can provide at least once
guarantee to our customers and we don't
want anybody other than our
infrastructure consuming from this
because we don't know what their access
parents are if you cannot control the
fan out and we cannot control how and
what kind of load they're applying on
this cluster so because this is the
first line of defense wherever you're
getting all our events and buffering it
we want really really high availability
for it so anybody who wants to do a
processing where they don't know their
predictable load or what kind of load
they're gonna put on the tasks of
cluster or the fan-out then we just
route those events to this cluster and
other stream consumers it would be spark
streaming it we think it could be our
in-house mantis engine that could take
those events in processor so the event
produces produces the event they land up
in the front in Kafka clusters and the
router is the piece that builds the
indus pipeline and it's a bunch of jobs
that are built on a stream
rustling platform framework which is
running on docker containers it's
currently doing flame for use link as a
stream processing engine and all this
gets setup declaratively automatically
once a user clicks on provision in the
UI we would end up creating a new topic
here and provision it correctly and on
the right target cluster for the for the
kind of bitrates that they've asked us
to provision it for we launched a whole
bunch of jobs here and then the data
lands up in any of these things so we
have to offer at least once processing
semantics because we have a lot of
interesting data flowing through it and
we want good business insights we have
strict SLA zuv in our on 1% we cannot
lose more than 1% data for any given
stream for a day because if it's more
than that and we violated relays when an
event enters our system we automatically
inject a few metadata fields we inject
it good so that it's very easy for us to
differentiate between retries of an
event versus a brand-new event being
generated so we've seen cases where the
event producers themselves are creating
the same event over and over again maybe
because of a bug or the way they've
written it and we want to differentiate
that versus our infrastructure library
retrying the same even multiple times so
the first scenario it would generate
multiple events with different words but
the same payload in the second scenario
you would have the same page same event
being sent downstream but with the same
word so it helps us deduplicate and
understand the duplicate rates we insert
the timestamp this could be used for
even time based processing downstream
host an app for traceability and we also
implemented a custom bar protocol which
wraps the whole payload so you have the
Kafka protocol as the data flows into
Kafka and then we wrapped that with our
custom wire protocol the reason we added
the customer protocol there several we
wanted to be able to control the forward
and backwards compatibility so for
example if you move different between
different versions of Kafka as we
evolved it from version you know a to
version B as new versions are available
the changes in the underlying protocol
should not impact our system that we
have to upgrade everything at the same
time or not updated at all
so having our own custom where our
protocol helps us offer that backwards
compatibility so that we can seamlessly
upgrade our infrastructure and keep it
running 24/7 the other reason is we can
support different kind of civilization
formats and offer reusable components to
automatically serialize and deserialize
them whether the users having to worry
about it we can also use it to inject
additional metadata so that we can add
traceability from end to end and know
what's happening to the event as it
flows through the different parts of the
system we have also invested in creating
smart deployments so some services or
downstream customers are latency
sensitive and some are duplicate
sensitive so based on that we leverage
how we do redeployments are for
infrastructure and redeployments with
off there changes in the filters of
projection if there is a failure in one
region and all the users are being
evacuated the other region we
automatically respond to those events
from our internal automation and will
scale up our fleet in one region to
handle the events and we scale based on
current and historical traffic and we
automatically pre scale to a certain
level so that we can handle that load
and there's an external process that's
monitoring these metrics about how much
we are processing what we are lagging
and what we need to scale up to and we
look at the incoming traffic we look at
the historical patterns of how fast we
have been able to process what our
current provisioning is and then we use
that to kind of project what we need to
yet and then we do a redeployment at
that point in time and expand the
cluster the other one is what happens if
you're fronting Kafka cholesterol goes
down right what if there's an issue with
it so to deal with it we've built
tooling to manage the failover and we
also do something called Kafka counts
every week we intentionally turn kind of
one cluster off to simulate a failure
and then route it to a different cluster
and then we try and recover it now go a
little more detail into it as to how we
exactly do that so this whole pipeline
actually processes over 1.3 trillion
events every single day we've about
three petabytes going in and as you saw
an example where you can have multiple
fanouts out of the system and different
filters applied through different paths
so if you have about nine petabytes out
of worth of fan-out so it's a three X
fan out of the data we get in and we
keep the system up and running we have
about four nines of availability now
this does mean that sometimes we may
have added latencies when you have
downtime but we don't lose data and the
system still stays available so
resiliency is really key in achieving
this four nines of availability and we
go to great lengths to make sure that
our systems are resilient and so we take
this whole thought into too hard and
you've actually built something called
Kafka Kong so a person who's on call
every week hits a button and we go
through this exercise so what exactly
happens we have event producers
producing events into this fronting
Kafka cluster let's say this cluster
goes down our system detects that it's
down and then we have automation where
we launch a brand-new cluster we
recreate all the topics that were here
on to this cluster and then we change
the map here on the producer saying you
no longer are going to produce a to this
cluster the topics created here so you
need to send all the events here at the
same time we launched our routers which
are based on off link infrastructure
to start processing the new data we also
leave the old routers running so that we
can drain if possible anything we can
drain out of this cluster so we try and
try our best to get whatever we can and
salvage out of this cluster sorry and we
have a mechanism to even switch back so
let's say we are able to recover this
cluster again then we have automation
which says rollback get rid of this
cluster go back to using the original
cluster the benefit of doing that is
this cluster we launched it a much
smaller capacity smaller kind of
instances than this one so we can
control costs and it's you know it's
temporary however if you do find that
this is absolutely unrecoverable then we
just throw it away and launch a new one
because sometimes the environment in the
cloud is such that that it's not worth
our time to spend a lot of time
debugging exactly what happened and we
may not have all the data so we just
launched a new cluster and we leave this
aside we quarantine it and then we come
back to it later on to debug as to what
happened so this is our automation you
can see some offer kafka clusters as
each of the region our engineers can
pick any one of those clusters and then
just hit a button saying failover now
and then we have automation that does
everything that I just talked about and
once the failover is completely can even
say divert the cluster so you know tear
down the temporary cluster bringing the
traffic back and it does everything so
it's a couple buttons worth of work but
there's a lot of automation that been
behind it to make this happen
I didn't have this initially to start
off with and due to a bug and zookeeper
outage or kapha clusters went down and
there was no way for us to recover it
so we learnt the lessons like a year and
a half ago and then we built this
automation and since then we are able to
keep really high availability of our
service let's get to the next stage of
how engineers can write
sorry stream processing jobs by
specifying custom code we look at that
in context of two use cases the first
one is when user starts clicking on the
website on the UI and start to play
movies they want to enhance those events
with additional data from other micro
services and this is one such stream
processing use case where they want to
attach additional discovery attributes
about what the user is doing on the site
and the additional services in our
back-end system have the data about
other events that are generated so we
want to enrich these events flowing
through the clicks that our users are
generating on the different devices and
these in the order of about 100 billion
events per day and the challenge here is
in a stream processing job we have to
connect to live services and deal with
the failures with those live services
how do we get to fail backs how do we
join that information into the stream
that's flowing in and make it work
really well within our ecosystem and
metrics the second use case is complex
socialization of user events so as a
user looks at on Netflix landing page if
you're familiar the little Netflix
landing page there are rows and rows of
videos right each row has a bunch of
movies that's customized based on your
personalization and how you interact
with that ROS which videos do you look
at so those are the impressions you may
just scroll through the video and look
at which videos are available to play
you may click on certain videos it
should be the click impressions and we
want to analyze these for your sessions
and these sessions can last between 8 to
24 hours depending on the
personalization and customization and
each session that we look at I mean
normally when you look at web sessions
it's pretty clear there's a certain time
on so you have a bunch of activity that
a user does and there's a gap and when
you hit that gap you mark that the
sessions ended
here the NEET requirements very
different the beginning of a session and
the end of a session is marked by what's
in the data what's in the payload so the
payload drives the start in the end of
the session and that's what we called as
you know punctuated session session
ization and the events that we receive
could be out of order so for example you
could click on you could scroll them
view the movie and when you know the
right move you want to play you can play
click on it now those two events could
arrive out of order and you could view
multiple videos before you actually
click on one to play or scroll through
multiple videos before you click on one
so when you scroll through multiple ones
all those are part of a session and then
you end up clicking one and doing
something with it there would be kind of
like the end of the session for that
particular duration so we have a larger
web session if you will where you don't
get locked out for a certain number of
time and that would be in 8 to 24 hours
and then within that bigger session
window we also have smaller activity
sessions and those could last from 2 to
24 hours and we want to make sure that
the events that we get even if they're
out of order right we're able to start
and end correctly so the second example
you may scroll 10 movies you click on
the 11th one and then you start playing
it now the movies that you've actually
scrolled through you want to know which
movies you scroll through that caught
your interest and the one that you
clicked on those 10 events could come
out of water so let's say we get even 10
first then you'd start the session at 10
now when we get the event first we have
to expand our session to include all
those events that happen so that's the
challenge here I mean how do we create
these sessions automatically and expand
and contract them based on the data
that's flowing through and these are
about for this specific use case is
about 10 billion events per day added to
this there are challenges for how do we
you know make this more scalable because
we don't want our data engineers to
worry about the infrastructure of
building these
so the first in this so for the two use
cases that we looked at here the list of
challenges that we have to address from
infrastructure perspective first one is
the complex punctuated session when
doing and we're working on making this
reusable so if you have use cases
similar to this the users can use our
library you can programmatically
configure it and it'll do the right
thing for identifying the start and end
sessions and applying the processing on
it the accessing of data from other
services was the challenge for use case
one and for the second use case we also
have very large state because you're
talking about you know millions of users
tens of billions of events and sessions
at last 2 to 24 hours and then we also
have to worry about accuracy fault
tolerance and dinner parity let's say
you are moving from virgin one to
version two how do you know the version
two is good it's not introduced any
regressions and especially in stateful
systems there we are flushing state down
it becomes they weren't important
because how do you redact everything
that you've produced downstream and once
you have errors in your stateful in your
state that you're sending downstream
it's very hard to correct that compared
to a straight full stateless processing
so having a system to be able to compare
two versions and do a daily priority
analysis is very important and it also
helps you do canoeing your new versions
of your code base and developer tooling
is key you should be able to run this
locally on your laptop be able to debug
through it and then easily deploy it
onto our entire cluster form and what we
found is these two use cases address a
lot of the stream processing
functionality that's expected our
platform and a majority of the use cases
can be solved if your platform can
support the functionality that's needed
for these two use cases so that's why we
chose those two use cases to begin with
so we built a multi-tenant stream
processing as a service
how that looks is the user creates the
job with its point-and-click DSL of the
custom code and they submit to our
social service UI or they can submit a
DSO to our self-service UI and that ends
up launching the jobs using our
deployment orchestration cool called
spinnaker which is open source and it
ends up running on our tightest
container runtime environment and to
also deal with the challenges for stream
processing you're working with flink
right now as a stream processing engine
we had Sam sub before but that is a
stopgap for us it did the routing piece
of it really well but now we have two
large set of use cases one is the
massively parallel routing use case
which actually supports our injust
pipelines the second is to enable all
these complex use cases where you need
really complex stream processing to be
able to get this data to deal with out
of order events what do you do when you
have later having data how do you do
complex fin during sliding windows even
time-based windows so for that
functionality we chose wink so this is
how a deployment looks like on our
container runtime and so our internal
container runtime is called Titus it's
kind of similar to how Borg works if
you're familiar with it
so Flinx architecture is similar to
support streaming they have a
coordinator layer so every job you
launch has a coordinator which is to
call the job manager here here we are
showing the high availability mode where
if you have two job managers running so
we have two coordinators if one
coordinator fails the other one can take
over and then they're a bunch of worker
hosts called the task managers so when
you have a job graph that you're
submitting to flank it figures out how
to either chain your operators how to
distribute it across your cluster that
you've specified and do the operations
so here the coordinator manages the
distribution of workload across this
task man
and it also does something interesting
it takes check points of your processing
so you could say check point what's
happening in my job every 10 minutes
let's say so every 10 minutes the
coordinator is going to send a signal
they're just called checkpoint barriers
and when those chip 1 barriers reach
each of these nodes each of them
independently take us take a checkpoint
or a snapshot of their data set so it's
a distributed snapshot where each task
manager is taking a shaft snapshot of
the operation that's running inside it
and saves it out to an external data
source that source that's configured now
when all of them have successfully
completed checkpointing for the
checkpoint barrier so let's say the
checkpoint barrier ID is 5 as soon as
everybody's checkpoint at 5 the
coordinator marks and writes to its own
datastore the metadata saying checkpoint
barrier 5 is successfully checked
pointed now it's okay to go ahead and
start the next pic pointing barrier so
the really nice functionality of this is
let's say you have a failure when this
cluster comes back up it reads the
metadata the coordinator reads the
metadata and knows that there were extra
mer of tasks manners that were running
it knows the location of each state
where it left off it sends that metadata
to these task managers who pull in
parallel their corresponding snapshots
that they've taken and they actually
begin where they left off so this is the
exactly ones processing semantics it's
not exactly once a delivery but it's
exactly once processing the semantics
what that means is let's say you're
adding accounts and you have two plus
four plus five so you have eleven clicks
that a user has done if you don't have
this check pointing mechanism then
either you may end up less than 11 or
more than 11 so in this scenario let's
say you added 2 and 4 and you're at 6
and then you have 5 being processed here
but you have a failure now when this job
gets resurrected the state that gets
resurrected is 5
sorry 6
and then it'll start processing the
click with the count 5 and it will add
11 so now we have pretty fault-tolerant
functionality to begin where you left
off there's another feature it's called
safe warning which is user user
initiated checkpointing so you could
look at the complex state of your job
and take a save point just like a
database save point and you could start
a completely brand new job in parallel
which starts at that same point so you
could do something very interesting like
get where you can branch off at
different points of your state and then
either use it for a/b testing or data
parity or validation of how your new job
is doing and the same pointing in can
ISM actually is built in to flink and we
leverage that so this is how our
container runtime and the complex stream
processing job that we're talking about
actually runs right here so there's a
lot of machinery that goes into making
it multi tenant we had our own container
runtime before we had this general
functionality available within Netflix
and now we have this complex
functionality available it leverages
maysa our fenzel scheduler which is our
open source project Cassandra internally
and we do use docker heavily for it so
every job that gets deployed unto this
infrastructure is a docker image and our
stream processing infrastructure
actually provides the functionality to
build these jobs so what does our
infrastructure actually look like at the
lowest level as the AWS ec2 runtime then
we looked at titus runtime which is our
container runtime that's shown here on
top of this layer we built something
else past core which is based on the
flink runtime engine and to this we add
integrations into our ecosystem how do
we talk to you keys how do we talk to
Kafka how do we talk to elasticsearch
how do you route events to hive and we
make some of the reusable components
available
so that when a user wants to process
events like the custom windowing is
available for them to reuse and leverage
what we've built they can reuse the
sinks so we have functionality to index
data into elasticsearch and they can
configure how they won they're in events
to be indexed what's the ID field how do
they want to rotate their indexes is a
daily monthly yearly or don't rotate the
indexes at all so all that functionality
comes out of the box we also know how to
discover these things and how to talk to
them automatically
so all that functionality gets leveraged
as a reusable component for people who
are writing jobs on this framework the
whole Keystone pipeline that you looked
at that process over 1.3 trillion events
is actually built on this functionality
so this is kind of a product that the
offer and if the product that we offer
if it's not enough to solve the complex
use cases then engineers can write their
own complex application leveraging this
whole stack and we also have metrics and
monitoring along the stack that we offer
out-of-the-box so how many events are
getting into your car pipeline how many
events are your is your stream
processing job processing are you
keeping up with the inflow of events are
you doing regular checkpoints what's
your checkpoint size are there any
anomalies in that so all that
functionality that's needed to keep
these jobs operational and running 24/7
is offered out-of-the-box and we have a
self-service management UI I think
that's a very critical piece of our
component which allows us to scale
without adding a lot of people onto the
team otherwise it's really operationally
burdensome if you had to react to every
single request that came in without
having this management UI and at UI and
service so in the past couple years
we've evolved the system two times you
had a legacy system that we replace with
Kafka and sansa and now we are replacing
it with our new tightest runtime
and flink so and while we did this we
kept our service up and running without
downtime are losing events and our SLA
was to not lose more than 0.1% of the
data while we do these huge migrations
the first violation was a complete
rewrite and a brand new system and we
still have to process a trillion event
and the new system we have the Kafka
framework that's constant but we have
replaced all our stream processing
engine and how we do self serve and all
that stuff so during this whole
migration we cannot have data that's
different so we have to do we have to
compare every single event that's going
out in a day so we compare about 2.6
petabytes worth of data every single day
to see if there are differences in count
if the SHA of those messages are
different to see if there's any
difference in data itself and what kind
of duplicates are we generating every
group generating excess duplicates or
are we losing data so the way we do that
is by building extensive tooling and
also building self feeling self
recovering infrastructure and having a
lot of metrics and monitoring along
different points to tell us what's
happening so for the example of our in
this type line in the second round we
replace this whole big layer and this is
hundreds of jobs and it's thousands of
instances in upwards of four five
thousand instances that are running all
these jobs and these are running inside
a new container runtime environment and
we were the first large-scale service on
that container runtime environment so
there were a lot of moving pieces that
we had to replace and we did that in
about six months and we built some
really interesting tooling to make that
happen so the first thing that we built
was when you are switching between
different implementations of the router
we want to make sure that we leave off
where we we start where we left off
reading from a Kafka stream so if you
have this old router here and let's say
it was reading at offset 10 and cough
the red message 10 and we decided that
now's the time to migrated so we
terminated this and we launched this new
router now this one needs to know that
the last message that was successfully
processed by the old router was 10 and
the next thing it needs to do is start
reading from 11 so what we did is when
this migration happened for the first
time this router was checkpointing these
message markers onto a different Kafka
cluster so this would write offset 10
into a different Kafka cluster when we
did this automatic deployment it would
take this offset number 10 and copied
over onto a different cluster where the
new router is actually checkpointing so
then the new router starts resuming
processing of events in the Kafka stream
from read the old one left off so the
new router would start processing from
offset 11 or regression or you have an
issue with your new router and you want
to actually roll back so you can roll
back we do the same thing in Reverse we
copy the offsets back from the old from
the new router checkpointing system to
the old router checkpointing system and
then we can recover the other thing we
did is doing smarter deployments so
there are some downstream customers who
expect really low latency because
they're processing this events in real
time for other customers the data is
landing in elasticsearch our hive and
the latency is they're fine accepting
the latencies so for low latency jobs
what we do is we start the new job first
and once it's working correctly we
terminate the old one right for the low
duplicate scenario we completely turn
off the job that's running right now and
then we launch the new job so that
doesn't reduce latency but it's
guaranteed not to have duplicates
because we know exactly where to start
off once our jobs terminated so with an
always-on stateful stream processing
system
it's really imperative to have the
following features or tool sets in your
infrastructure you need to have ways to
do data parity validation so that you
can evolve your system with confidence
you need to have a way to run a couple
of jobs in parallel so that you can see
how they're processing together and then
you need some way of doing smart
deployments that you can address
requirements for low latency or low
duplicates kind of scenarios and then
need to have scenarios where they need
both low latency and low duplicates and
that's where we do something interesting
like fling what we do is we start up a
new fling job we put it in a bar state
so it's not processing anything but it's
warmed up it's up and running all the
code bases up then we stop the old one
so it stops takes a checkpoint and
gracefully shuts down then we issue a
request to the new ones saying begin
where the old one left off at that
checkpoint and so the downtime is very
very small and it starts up almost
immediately so to wrap up you know
typically 80% of the effort in building
these real-time stream crossing
pipelines is spent on creating the
infrastructure and a lot of the times
this onus is on data engineers who don't
have the expertise of distributed
systems to build them in such a way that
their fault tolerant and keep them
always running and that leaves them very
little time to actually spend on data
analysis so we are reversing that
equation by building these
infrastructure component and
functionality using do clocks you've
come a long way since we started with
this effort but we are just beginning to
scratch the surface we have a lot lot
more long ways to go in building our DSL
so that it's very easy for daily
engineers to launch a job like do
windowing over the fast past five
minutes in a sliding window way and give
me how many clicks the user made write
something like that today if you're
doing sparks I mean you'll have to
launch your own cluster right the cord
submit the job and get it running
versus we want to make it so easy that
you have a sql-like dialect if you're
familiar with the magic outside or
something similar to that so that just
writing that submitting and clicking a
button they have a job deployed it's
running fully they have all the
infrastructure available for them to
evolve it to operate it to run it
there's metrics and insights available
to you for them to look inside and see
what's happening something that's all I
have for today if you have any questions
I'm happy to take them thank you so the
question is the rapper library is it
internal to Netflix or is it open source
so we have an open sources library
however the enhancements that we've made
we kind of contribute back to the Kafka
community and we talked to the Kafka
cognate us to roll those enhancements in
so a couple of enhancements that we did
there was how do we do our stickiness
when you're producing so what happens is
when you're producing events to Casca
partitions if a partition is down then
you cannot produce to it so you have to
go to the next partition what happens to
the messages that are in your buffer
currently like do you drain them how
long do you wait so that's one the next
one is how do you effectively batch them
so that you have better efficiencies in
batching events and you can have much
you can support much more higher scale
so those kinds of announcements you work
with the community to make them better
but then there are a whole bunch of
other integrations with our integral
internal systems like our service
discovery system our metric system and
those integrations are not really useful
for the community because they're
specific to our environment and the way
we switch our routing Maps using dynamic
properties so that's those kinds of
integrations are hard to open-source so
the things that we can contribute back
to the community we push it upstream
into the main Kafka builder tool
right so we have a bunch of clusters and
the way we looked at them as we have
something all dedicated clusters and we
have general-purpose clusters the users
unaware of this so if their bitrate
provisioning bitrate is below a certain
limit and the way we go about is every
partition is about half a megabyte worth
of data flow for us so up to 12
partitions
if anybody provisions a topic that can
fit in twelve partitions we put them on
this cluster a if it's much larger and
more than like 100 megabytes bitrate
then we find the part we find the
cluster where there is space available
and then we kind of automatically create
the topics there if you realize that the
provisioning request is really large and
doesn't fit any four clusters then the
provision it's slightly differently that
provisioning takes a little longer so
either will move the topics around to
different clusters and view the same
Kafka cog mechanism and we move top
topics over a little more topic to a new
cluster will have our automation route
the events for that topic to a new
cluster and then we can kind of move it
back and forth that way and then we will
create a new cluster if you need to and
if you need to expand so there are three
stages by default the land up on a
cluster over there space and it's
commonly used if it's larger than that
then they go into a cluster where we
know they're spaced and it's dedicated
for those larger scale topics and if all
that doesn't work and we still need
extra capacity then we'll just create a
brand new cluster and that'll take a
little little time we do have automated
pipelines through spinnaker our tool to
launch the new cluster so it's very easy
for us to launch them and set them up
I'm gonna be around for for some more
time here so if you have any questions
or if you have ideas or if you want to
collaborate and what you guys are doing
I'd be happy to chat more thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>