<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Thorough Look into Spatial Mapping with HoloLens by Rafał Legiędź | Coder Coacher - Coaching Coders</title><meta content="A Thorough Look into Spatial Mapping with HoloLens by Rafał Legiędź - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>A Thorough Look into Spatial Mapping with HoloLens by Rafał Legiędź</b></h2><h5 class="post__date">2018-04-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vycKsVJ1eAA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello nice to see you here thank
you for being at my session thank you
for choosing it between the other ones
and thank you organizers for getting me
for this conference it's my first time
in Bucharest actually so it's even more
exciting for me so I have several
Holland's and mixed reality talks and
organizers chose to ask me to do one of
them which is not the very basic and
introduction everyone and not the life
coding one but the the one where I do a
little bit of introduction and then
focus on more on one particular in my
opinion most exciting feature of whole
lines which is spatial mapping and
that's what we were going to focus today
but let me first before we continue with
the technology let's first introduce
myself I'm Rafael again I come from
Poland you can find me on twitter at
traffic and I'm a software engineer I
work for a company called solid brain
and for last year I'm mainly focused on
mixed reality augmented reality hololens
and i'm exploring the technology i'm
talking with different businesses
creating different proof of concepts for
different industries to see how it fits
how possibilities of this platform I can
solve different problems across many
industries and there's a lot of interest
in it actually and also I'm a conference
organiser I organize a conference in
Krakow Poland called DEFCON we're doing
it since 2011 so it's eight years now
it's in September so if you'd like to
come it's definitely odd PL with the
tickets are on sale already and also I
try to be a conference speaker so
whenever I think I have something
interesting to say I apply to
conferences and for example hands I'm
here and I talked so much about Holland
says for the last year that Microsoft
because that's the Microsoft's device
Holland says they decided to give me MVP
because I was loud about this subject
and they're giving awards for that so
yeah
as of February 1st I'm a Microsoft and
beef
houses okay so that's enough about me
let's first at the beginning let's talk
about those abbreviations that are very
common nowadays and I want to make sure
that we're on the same page that
everyone understand what we are and IR
is that there is an understanding around
this problem the topic so VR stands for
visual reality which is well the
experience that you can get with kind of
an occluded device it might be a device
like oculus rift or HTC vive or a mobile
phone that you put into a headset and
you are totally immersed into some kind
of artificial environment so I don't
know how many of you have tried HTC vive
oculus rift or any kind of mobile VR
thing not many of you so so those who
already tried already know that but
those who those who didn't probably
don't know that VR kind of move moved
the person using it in two totally
different environments so that actually
the mind is being tricked into thinking
that the real world doesn't exist that
everything that is real is actually this
artificial synthetic environment that is
created by the application within that
headset and we are immune for like
external external signals that come to
our body because we think we are in a
different world which is fine for
entertainment industry for example and
in some other cases and on the other K
on the other hand we have augmented
reality which is what abbreviation AR
stands for and this is the most common
way they are easily able right now is on
our mobile devices like mobile phones or
tablets where the application they are
application being ran on the on the
device uses the camera lenses and
whatever the camera lens is see is being
displayed on the screen and then on top
of that picture some other digital
content is being rendered right I mean
the best example of augmented reality
application is pokemon gold this is the
most common one right I mean that's why
a yard term
became so common in popular actually
everyone knows what Pocoyo is and there
are some other devices are devices that
come in the form of glasses or goggles
and there are all sorts of them from the
very simple one to very advanced one and
ha lenses is well one of them actually
the the head-mounted display which is
very advanced and this is what what the
topic is all about and you might also
see on the internet on or different
articles or how Microsoft is referring
to it that there is also a third term
which is called M R which stands for
mixed reality and there's lots of
confusion around it I mean at least it
used to be because we had like a R VR
NMR so what's mr then right so this
picture put it clear that mixed reality
is basically the whole spectrum of
experiences between physical reality and
digital reality so depending on device
that we were using or the software that
is running on this device we can get
different experiences I would say we can
have some mixtures of augmented reality
and virtual reality and I'm not going to
go here into details but there is a
whole spectrum of those experiences and
most of our mental reality experiences
are very near physical reality because
users are usually fully aware that they
are still in physical reality because
whenever we are using augmented reality
on our phones we still like perfectly
know that we are here and we only see
only a small fraction of our real world
augmented with digital stuff on the
other hand all the virtual virtual
reality stuff is on the other hand where
we are basically in a fully digital
world sometimes we know where is the
floor in those most advanced just like
HTC vive or oculus rift but those those
devices those experiences lives on those
two extremes and mixed reality is the
whole spectrum so augmented reality is
mixed reality and virtual reality is
mixed reality right so I think the
confusion disappears here and we have
whole answers
how many of you have seen this device or
heard about it okay so like half of you
which is good and what I want to say is
that this device is not very new it's by
Microsoft Microsoft is a vendor of this
device it runs Windows 10 so it's
nothing like anything special embedded
operational system there but it's like
regular PC with Windows 10 and it was
announced in 2015 so it's it's three
years now and until it was announced and
it was released in mid 2016 the first
wave of devices was sent to developers
in June 2016 so it's almost 2 years that
it's out there and actually many many
companies started developing for it for
this platform and many businesses
started to leverage the stack and of
Technology and enhancing their processes
and their business with it what else I
can say about this this is the device
that doesn't include our vision so we
put it in our hand ahead and we actually
see whatever is around us because it's
the displays transparent and we have
some special screens there so that they
display additional stuff in front of our
eyes
also as I as I mentioned it's a regular
PC so we do not need any connection to
any external computer or or a unit that
gives us computing power we can but if
you would like some extra power to for
our device but it's not necessary and
most of the experiences it doesn't
actually use it and also so it's
untethered it's Windows 10 and it costs
for the developer kit it cost three
thousand euros writes it's not very
cheap it's five K for a commercial
version but my main thing is that it's
not consumer targeted device it's mainly
for businesses so this price can be
comparable compared to to run on mobile
phones for example right and that's
where all day and most of the clients
are right now those clients are usually
big companies or smaller ones that can
afford that so after we put this device
in our hand
we can fire up the application regular
Windows 10 application and then we can
have all sorts of stuff in front of our
eyes I mean I'm not going to show you
how to how to code for it during this
session but basically we're recording
here in unity or dialectics so we're
using unity and c-sharp or C++ and
DirectX and if you're familiar with this
technology or at least you know what
what it stands for those are basically
the game engines that that are behind
many titles game gaming titles nowadays
so you can imagine here that whatever we
can do in those 3d engines can be done
here so we can basically visualize
anything in front of our eyes well given
what kind of constraint we have with
this device but well there's no limits
what you can do how we can augment them
the word so how is this even possible so
this is a pretty sophisticated piece of
engineering that's why probably cost
some so much so it wouldn't be all
possible without all kind of sensors
that are mounted here so long story
short it could be said that it's a it's
an enhanced Kinect being mounted on your
forehead because that's partly true
it's some-some next iteration of Kinect
but it has something more so on the on
the sides they have tolerance has for
environmental understanding cameras
which are for those are those things
which are for for scanning the
environment and this is what what is
this session all about so I'm going to
elaborate on that a little bit later but
basically those cameras work in a way
that they detect where are the surfaces
around the user and they built internal
model of the location of the user so
that developer can use it within an
application also has microphones
it has algebra camera it has deaf sensor
it has ambient light sensor it has mixed
reality captured sensor which is for
streaming whatever the users see into
the computer so that someone else can
see what what the user sees and now we
see I will show you that feature and
well that's pretty much it for the for
the sensors the other thing is that
makes it all possible are those optical
lenses I'm not going to go here into
very details those are two waveguides
powered by two HD engines so we have
seventy seven twenty P per eye and they
can render Holograms in front of our
eyes
I mean 3d objects and they can so that
the user can feel if the object is
nearer to the user or further etc the
the disadvantage of this technology is
that it doesn't allow for a big field of
view so it's not like 110 degrees field
of view it's rather something around 40
degrees
so as we as we can see everything around
us because it's not opak we can see
Holograms rendered only in kind of a
frame in front of the user so that's a
little disadvantage but that probably
will change in the future and this is
the disadvantage because of the
technology that is used for the lenses
and last but not least there is a
special processing unit which is called
HP holographic processing unit and this
unit is for processing all the sensory
input so whatever comes from the sensor
sensors is being processed by this unit
and then passed to the other
CPU that is that I mean the main CPU of
the device for the other hardware inside
it has regular CPU but 32-bit not 64 it
has 2 gigs of ram it has 60 gigs of
flash storage it has bluetooth Wi-Fi and
batteries of course and they allow for
up to 3 hours of active use so that's
pretty much it as it comes for the
device itself so how do we use it how do
we well we put it in our head we don't
have keyboard we don't have any mouse so
how do we issue commands to this device
right so there is this GG V input
paradigm which stands for gaze gestures
and voice so gaze it's very similar to
what we have in VR headsets so basically
we can imagine a laser that comes from
our forehead and wherever it hits
something it leaves a dot that's our
cursor so we can see a cursor in front
of our eyes and it moves with our head
movement so we have a dot that comes
with our a head movement not ice
movement head movement this is very
important so that that's why whenever
you see a person using Collinses they
usually just move their head a lot
so that might change because pupil
tracking is really a hot topic right now
so it might be integrated maybe in some
some new future so whenever we look at
something we can issue comment to to
operate on some objects while depending
on the application we're using of course
so it's usually combination of pointing
something with our gaze and then is
showing comment and we can issue
comments with gestures and voice and we
have basically one gesture available for
the user and for the developers out of
the box which is a tap gesture which is
like this so we look at something and we
just issued this kind of gesture that's
as simple as that we also have a system
gesture which is called bloom which is
like this and this is for opening start
menu or closing the application or
closing the start menu so this is the
system just so we can't use it within
our application I mean we can't program
it we can use it in our application to
close it but we have only this gesture
but based on this gesture we have some
other gestures derived like hold which
is half of the air tap and navigation or
manipulation which is hold and then
movements so that we can track the hand
movement and for example resize things
or move things around so and there is a
voice command support so we can program
against it and we can also have the
functionality so that we can look at
something and then say something and
then something will happen
the only supported language is English
right now so it might be a downside
might maybe not but it works really
really well and as I mentioned before it
has four microphones mounted here and
there are
like directional microphones and the
device it's it makes sure that the
comment actually comes from users mouth
and not the person looks like standing
next to the user so it works really
really well so to give you some examples
and to give you idea and to prove that
it actually is a piece of technology
that is being used out there it's not
it's not like some science fiction thing
and some kind of prototype there are
actually some real use cases for it and
I'm I'm just only showing you here like
four most popular ones but if you google
hololens plus i don't know any name of
the industry like Colin's Manufacturing
Colin's healthcare hole and construction
engineering whatever automotive whatever
you will find lots of lots of different
use cases and real applications that
actually are being used out there by the
companies I mean big companies like
Volvo BMW or GE or Schneider Electric
for example so it's a real thing it's
happening it's it's a really good time
actually to put your hands on this kind
of technology I mean they are in general
so let's just go through through for
this cases so the first one on the left
top is Citibank it they created a
trading application for for the workers
training workers so it's it's a AR AR
application for banking right so I mean
you you might you might thought that you
you wouldn't be able to fit a are into
banking but well you can so we can
extend the desktop of the user actually
to to not to be limited to regular
screens but it can basically render
whatever 3d objects you might want to
write so we can visualize all the data
that are flowing in and you can give
user the possibility to operate on this
data like bringing stuff here and there
and click through some graphs etc and
there's in this case there is also a
sharing sketch scenario so that there is
another person working on the similar
application on
his computer and he actually sees
whatever this guy is doing to the data
so it's it there are a lot of different
possibilities unlocked here the other
one is by Ford they recently announced
that they're working on the whole and
supplication for the designing process
so that they can review designs of their
new cars with Hall and says which is
also pretty cool because you have you
can visualize the new concept with your
3d glasses instead of looking at the
monitor right and here is also showing
the scenario that two guys are wearing
Collins's and it's one of the cool
features as well about how lenses and I
have like a whole separate session about
it a multi-user scenarios because with
all lances we can run the same instance
of the application on several headsets
like up to I don't know maybe 20 or
something
even more and we can well we can connect
them as we we are connecting in
multiplayer games because that's
basically the same mechanism and we can
align whatever the one user series with
the whatever the other user series and
they can basically can see the very same
thing and operate on very same things so
they can look at the very same set of
holograms which is pretty cool because
that's that unlocks lots of different
collaborative scenarios as well in this
case the other one Schneider Electric is
one of my favorite it's actually
application for service workers that
they do maintenance work on some kind of
pieces machinery by Schneider Electric
and this application basically overlays
steps to perform on the physical machine
so the the the the user approaches the
Machine and all of the sudden I mean
when he or she fired the application he
will she he will see steps over light on
that machine like unscrew this bolt or
or open days or use this wrench to
unscrew something and step-by-step you
can basically lead the user to perform
tasks and well you can clearly see the
benefit here actually right the other
one is testing group the company that's
does elevators and they also have this
applique
this is the most common one I think I
mean the most popular one and they did a
pretty good job with that
also the the application for Service
Worker so that the whenever the guy is
maintaining a elevator he or she can
have all the contextual information in
front of the eyes so that the key can
have like manuals video manuals or like
set of tools that he or she needs or the
3d model of the thing that he's going to
operate on etc etc I mean we can have
anything whatever we can render can be
there and as I said you can find lots
lots of different scenarios out there so
just out of curiosity Google for them
and you will see for every each of them
that I've shown here there are actually
YouTube videos so that there is the good
so that you can get a good understanding
how it all works but coming into the
thing that we are going to talk here
about spatial mapping and the
environment our understanding cameras
that are here there are some other
scenarios unlocked for Hollis is
basically the hololens can scan the user
environment and developers can put them
they put the data into use so that we
can detect within application where is
the floor where our walls where our
chairs etc I mean we can't scan out of
the box like really precisely so we
can't for example detect if the bottle
is here or not we will get like a rough
shape maybe but we can detect overall
feeling of the of the location and based
on that there are three good
applications that were created at the
very beginning where when the whole
answers were released the dish that they
show how you can use those special data
in your application into a pretty good
extent for example there's the
Unconquered game where you have this
this red guy running on your floor
jumping on your table and shooting to
other targets that are running in your
in your room there is a game called
fragments that's my favorite one
actually your room is being transformed
into a crime scene and you look for key
clues
for you're trying to solve a crime you
there are clues hidden in your room and
there are the holographic people walking
around and sitting on your sofa for
example which is pretty amazing pretty
much immersive gaming you have to try it
if you have if you have time you have to
try this game it takes time but it's
really really cool and the other one
Rober right it's very cool as well it
there's an alien invasion so that aliens
are attacking you and they drill through
your walls so you see like things coming
out from your walls and then some aliens
come out from there and you just shoot
them and if you miss shoot them you will
make like well the holes in your wall
that the experience there is also pretty
cool I have this game anyone who'd like
to try the contrite after after the
session so this is all possible because
of spatial mapping capabilities and
those wonderful cameras and because of
this holographic processing unit so this
is the thing called spatial mapping so
from the moment that the hope that the
device is turned on and we can't stop it
from the moment when it's turned on it
starts scanning I mean whether there is
an application running or not it scans
all the time so if I would turn it on
right now turn it on I don't have to
fire any other application it's just
Windows 10 running right now and it
scans it just grabs the information
whatever it says I'm up to three meters
and whatever there is he grabs it I mean
it grabs it it doesn't take pictures it
just the text where are the surfaces so
that it can build up the model it uses
different combinations of things I guess
I mean colors maybe some few special
features of the of the surfaces to build
up the model and to know what the device
is looking at but yeah basically it's
kinda the area of the user all the time
and it stores all the data within
operational system so the common
misconception is here is if we would
have if we have to handle special
mapping like grabbing the data within
our application and no we don't have to
it's happening by itself by that by the
device we can use those data later but
all the grabbing of data is being done
by the raid operational system so for
example right now it's running it's
grabbing data and in this room and it
probably created like a separate bucket
within the system on our hard drive
which is called space and it puts all
the data that it grabs right now into
this space right and that's it it just
it just saves it there we can we can see
in Windows 10 we can go to settings and
spaces we can see what spaces the the
device not knows about but we don't have
much information there we only can see
like space four space three space one
whatever that means
and how how how big is the this bucket I
would say so
not many informations here available and
actually we can't view from this point
how those data look like it's some kind
of a black box so we can't see well I
can't go there and see how much data of
my apartment is there right now on this
device like just can't but as a
developers and well maybe not as a
developers there is a there's one
feature on hololens that allows us to
view current location so if i would
connect to Hollins right now and I will
do it in a second I can display whatever
the currently scanned and environment
looks like so let me let me do this so
that you will get an understanding what
I'm talking about I hope it works
let me check the connection okay I just
need to connect to my hotspot sorry for
that because it disconnected because I
need to what I'm doing right now to
connect a whole lesson to be on the same
network and I just put IP address of
whole dances into a web browser and then
the something which is called device
portal will pop out and I can see stuff
there which I will present in a second I
just need to connect to my hotspot okay
so this is this device portal thing we
have all sorts of features here like we
can sell see statistics we can upload
applications but what I'm going to show
you right now is 3d View tab where we
have this this model so we can see
whatever the device already know about
this location let me zoom it you can see
my my head position here whoops okay
okay so I'm standing here
you can say I'm looking to my left and
then I'm looking straight and I can just
walk around so that it will grab more
and more information see so that's
that's the model I can I have in my
device and I can make use of it in
within my application what's more I can
actually even export it here I can save
it and it will be exported into regular
3d object like a regular one
so that can import it in my unity
project and just use it so that I can
simulate my application without the
headset but in the in the in unity
project so that I don't have to deploy
the application all over to do my device
so that's how it looks like what
happened again okay but but you have an
idea how it looks like so that's what I
must wanted to show you so
where is that curse means okay so this
is the data that is gathered on our
device and how to put it into use so how
is it developers can we access those
data and what we can do with it what
kind of how hard it is or how is it how
easy it is or what's the overall
workflow of using those data so within
unity because well most of the people
most of the community they just use
unity for Cowen's development so we have
in in unity API scripting API we have
spatial mapping classes and we can use
objects from there for accessing special
spatial mapping data so the main object
main class there is called surface
observer and that's basically we need to
have at least one for for the for our
applications so that we have an entity
that observes any changes in the
environment and by this I mean it
observes whatever is being stored in
that internal spaces bucket so that the
the gathering of the data is being done
by the operational system and then our
our application looks into those buckets
and the changes that come to our
application came from those bucket we do
not directly access the sensor and scan
our environment so we create surveys
observer and then based on that we
define a spatial volume so we define the
area around the user within which we are
interested in updates so for example to
give you an example the whole ants they
scan the environment all the time and
they can see up to three meters so we
can scan this whole row for example we
can have the model of this room but for
our application we might define that we
only need to get updates from our
operational system from like two meters
around the you
right so we can define like body locked
box spatial volume so that would be
something like invisible box around the
user 2 meter by 2 meters for example or
3 meters by 5 meters and we would we
would be getting all the updates only
within that box and we can create like
spheres or frustum based special volumes
so it all depends on the scenario so for
example for a standing scenario so that
we do not require a user to go a lot
with our application being run we can
define word lock box spatial volume big
one for example like 5 meters by 5
meters so that we only get we are only
interested in those information in this
box so we won't be getting any updates
from anything that is happening outside
of this box and that will be for like
very station or stationary scenario and
on the other hand we can have a scenario
where we require the user to walk a lot
like the warehouse application where the
user would go from one Ally to the other
so that we would probably want to get we
will probably want to get updates from a
first-term
that is be in front of the user so that
it is field of view thing so that we
will only get updates from there so
whenever the user looks and goes we will
get updates from there so it's it all
depends on the scenario it can be body
lock there so this first one would be
body lock to users head and we will get
up updates from from there and based on
that we can register for surface changes
and we just wait for updates and
whatever we get an update about the
surface that is that was detected or it
was detected that the surface was
deleted we can define what we're doing
with that so we process the surface
changes so if the surface is added we
can render the surface or do something
with those data updated we can update it
deleted we can cache it or just delete
it and then we can access we can we can
request the mesh of this data and now we
visualize that in a second but based on
this day
that we got from them from this surface
observer we can decide what we were
doing with it and we can literally
request for a mash so if we got a piece
of surface from our surface observer a
piece of our wall we can say okay give
me the match for that so that I can
visualize it for the user and I can also
decide if I would like to have a
Collider with it which I'll explain that
in the second and then I can render it
or not
so after this whole process and this is
usually the ongoing process I mean
depending on listener but usually it
just scans and builds the environment
all the time
we can stop it but that's not what I'm
going to talk about right now based on
that we might get this I know that
doesn't say much but if you think about
it it's a mesh of the room it's a model
of the room right I don't know if you
can see it from from from this
perspective but it's a it's a room so
after some time and when the device has
enough information about the environment
we can request all the meshes we've got
about of surfaces around the user and we
can literally build the whole room
within our application and this is not
exciting because well I can display it
and I could build this simple
application for this room and whatever I
would see well black would be
transparent on Hollins so I would see
the actual like physical thing instead
of black here so this is if I would run
in this in this room I would basically
see this white mesh
whenever the real surface is so I would
have this white mesh lined with the
surfaces you usually don't do that
because it's it's well it's not useful
right it's only for visualization of
what we detected but what's super
important here is that with with those
meshes we also might get colliders and
colliders are the mechanism in our 3d
programming engine for the engine the
colliders are those entities that
unlocks all the physical interactions
for us
so whatever 3d object we render and we
assign a kowai
to it and we would have canaveral
Collider somewhere else on some other 3d
object which is for example our room
model we can do all the interactions so
you can imagine if we would have all the
colliders whenever we have real surfaces
like floor and the nicely lined Collider
with the floor we can basically drop a
virtual bowling in our application onto
the floor and this ball were actually
bouncing on the floor or just stays
there right
so that unlocks the scenario where we
can literally blend physical world with
the virtual world within our
applications right I will show pictures
for that so don't worry it's not perfect
there are a lot of errors during the
scanning process so it's not like we can
scan everything and everything will be
perfect no holes perfectly aligned
surfaces it all depends on the
environment so we have to be sure we're
doing a lot of post-processing of the
mesh if we want to make sure it's it's
it's pretty good or we have to make sure
that we have a good environment for
scanning so the most common affecting
factors for scanning for a room scanning
our user motion so for example if I
would like fire up the hololens and just
walk around here real quick obviously I
wouldn't grab every possible data here
right I would have some regions which
wouldn't be scanned properly also
surface materials it's good when
surfaces have some kind of patterns on
them if we'd go into like completely
white room like completely everything
would be wide right floors ceilings and
and and walls the device wouldn't have
any particular feature to start start
attaching attention I would say right
and start grabbing information based on
that point so surface materials are also
very important and seen motion so well
we if we would have a lot of moving
things around the user it would affect
all the scanned data so if we have
person in front of if I would have a
person in front of me when I would be
scanning the environment I would scan
this person and if that person would
move
to bid I would have the model of the
shape of this person in front of me and
then it would disappear after a while
and it would appear where that person
will stay so if I would have like lots
of this kind of moving objects I would
have lots of recalled hallucinations
around us so we have to deal with that
in our post-processing scenarios and
then also lightning interference if it's
too light or too dark we have lots of
different errors and the surfaces aren't
scanned properly especially when it's
dark so we have to then come closer to
the area to scan it better or just put
some more light into it and based on
that we have some errors and also well
there's one other thing that mirrors
mirrors are tricky because when we are
looking in the mirror actually Collins
thinks that there's another room there
and it creates a model of a room behind
the mirror which is pretty fun errors
that are derived from those affecting
factors are holes so if we don't grab
every single possible surface in our
environment we can have holes and that
may end up being the thing that imagine
we have mesh of our room and we have one
spot on our floor which is Kant and
daresay there's a hole there
this beautiful hole and we have our
virtual objects that will play within
our application and it would be dropped
where that hole is so it that basically
that object would disappear into the
void right so we have to make sure we
don't have holes because we don't have
Collider there then there as well so we
have to make sure we post process those
holes house nations already mentioned
that those are the things that exist
that the special mesh exists of these
things but those things doesn't exist in
a physical world anymore and also by us
if we don't look at the surface from
different angles but only on from one
angle it happens that the physical
surface is not aligned perfectly with
whatever virtual representation of this
surface we might have so we might have
Collider is slightly above our floor for
example so we have to make sure we walk
a lot with hollow lenses before we run
some application so to avoid all those
errors or at least most of them errors
there's usually a step in every
application that uses special data which
is called like room scanning experience
or anything like that so even if Hall
answers I mean the holistic scan the
environment all the time whenever
they're turned on but as developers we
are not sure where the user will be
running the application if the user will
be running the application in a room
that he or she already been to or not so
the application might be run in a
completely fresh environment that the
Hollins isn't aware of or in the
environment that has been run like
several times so as developers we have
to make sure that the user actually will
fire the application I will have some
time to walk around and scan the
environment grab some data whether we
have those data or not because we can't
we don't know that so that's usually
what is called room scanning experience
and you can see that in several
applications and that's usually
something which is just visualized with
some kind of a shader or animation on on
whatever surface is detected so whenever
we have a surface detected we can just
visualize it with some nice colors and
stuff like that so the user get an
understanding of how much of the
environment is actually scanned and if
he or she can start working and based on
those data we can do all sorts of cool
stuff I will show a quick demo in a
second there is this application
evaluable
along with the library which wraps some
unity API around special mapping and
there are some ready functions and
methods and objects that we can use for
different things like rule scanning you
can see it properly here but basically
we can have some statistics about our
scant room despite in front of our eyes
we can issue topology queries so we can
based on this this library using this
library we can just ask
the spatial mapping library had give me
like the largest wall right so we would
get the reference as developers of the
largest wall and then we can I don't
know just render something there
depending on the scenario we can detect
different shapes so for example you can
detect like suitable areas or platforms
so that we can place things there
depending on the scenario and also
there's a placement solver which is
pretty cool so that you can issue based
on the spatial mapping data that we have
we can issue some queries so for example
we can issue a query like give me mmm
three boxes one by one by one near me on
the floor so it will run the query based
on the data that that it knows and you
do give you the the coordinates of those
those shapes that it found because this
is the sample application so would
visualize what's possible but we can use
that in our application and just render
regular things so I wanted to show you
then let's see okay it seems it's
connected so I will again go to this
device portal thing okay yeah and I
mentioned before that we have something
which is called mixed reality capture
sensor which is something that allows
for this so you will see whatever I'm
saying which is pretty cool right there
is a slight delay here like 2 or 3
seconds so it's not like whatever I'm
seeing right now you can see but you can
expect some delay so this is the start
start menu of our Windows Windows 10 we
can fire the application I will run
basically this demo that I've just shown
you and by the way I have the device
with me after this session whoever is
interested in to trying it in trying it
you can just approach me in during the
break and you can try it so I'm running
the application hopefully to run
it will scan the this room I mean it
will use the data that we know and it
will gather some more and then it will
visualize whatever the the surface we
already know and based on that we'll run
all those queries that I've just
mentioned so you will probably start
seeing green green agreed can see it
because I can't see the screen is it
visible there yeah okay so as you can
see it's being built up and it somehow
aligns with whatever we see sometimes
it's good sometimes it's it's worse so
you can see how how accurate it is but
if you look closer if I'm looking longer
at some platform or shape it will get
better right so for example here is the
wall I have to approach the wall when
it's not lighten enough and you can and
you can see it builds up I know a lot
already so here what is implemented is
that based on the data I know I
processed I can run some interpolation
algorithm and fill the old avoid we have
some data that is interpolated based on
what I know right so I can I'll tap here
and it will it will finalize the scan
and as you will see it should build up
the wall around walls around me because
it just interpolated all the data it
assumed that there are some things where
there's the wall where the hole is so I
have a virtual room around me right now
it's not perfect because I haven't
scanned everything and I can run queries
here for example I can well I can query
for a largest wall and you will see the
largest wall that it found was here so
within my application I could get the
coordinates of this wall and then I can
do something with that
I can apply some texture for example I
can detect see table areas and you will
see that it will detect places that are
good for sitting you know so if you
would have a virtual agent that we would
program and you would like to have and
coordinates of the chair that this agent
can sit on we can show him no this is
those are the coordinates just sit here
and the game fragments that I already
mentioned it makes use of it pretty
heavily so that you actually have like a
holographic person that goes into your
room and sit on your sofa for example
which is pretty amazing large empty
surface for example let's see what it
will give gives us okay so there is a
large empty surface for example that's
it and also whenever I am looking at
something with my cursor I'm getting
information if something is a wall or
platform or floor so those are all
information that are being there so so
we can make use of them what as you can
see our object placement solver that's
pretty awesome so we can detect on floor
objects which are near me right those
are some assumed parameters there those
are or parameter as a parameterised
function so that we can do lots of so
lots of things with that but as you can
see here it found some spots near me of
different sizes right so depending on
the scenario there's a game Smurfs for
example it renders a Smurfs village
so it renders some huts around the user
so that that's for example the moment
that you can make use of those kind of
feature well on floor let's see what it
will find on the floor
so you can see it found me some spots
for big objects on the floor right what
is interesting on floor and ceiling for
example so those are the columns so that
it kind of it founds the spots that
connects an empty space on the floor and
on the ceiling so that we can have some
columns for example right and this is
only the example we can do all sorts of
stuff here so that you probably now get
an idea of how it works okay that's it I
guess here let's continue okay
and yes based on what I've just shown
you we can have all sorts of scenarios
so I will show you what can we do for
example based on the special mapping
that we have we can create all the nav
nav Maps Navigation so if we have some
agents in our game or program we can
program how they walk around so that
they do not hit those physical walls or
they do not walk in in the air but they
walk on regular I mean on physical
things and we can have occlusion so the
virtual staff occludes whatever physical
stuff is in there and vice versa so for
example if this cup would drop behind
this table this table would occluded so
we would know that this cap is behind
the table this is pretty cool effect as
well also we have all physics so as I've
described previously so if we would have
colliders everywhere we can do all the
physics interactions with that so we can
have for example balls that are hitting
our wall walls or floors for example and
placement so we can just place things
around so as I've shown you with that
application seconds ago we can find
spots and place things here and there
and so that we can feel that though they
are part of our environment so that we
can create those really really immersive
experiences for the user and there's one
cool features of cool feature with
holders as well that we can we can point
a place in space and we can attach
something just called spatial anchor
there so that the hololens will put some
extra work into tracking this point and
that means that if you would place an
special anchor for this object assuming
that that's our application we would
like to a user put this object here and
you'd like this object to appear here on
our next application run so that the
application kind of remember where this
object is we can attach a special anchor
here close the application and fire up
the application again and the hole is
when they will identify that there are
in this room and they can do that pretty
good in a pretty good way they will find
the special anchor and they will place
that hologram in the very same place
which is pretty awesome so well yeah
based on all of this as you saw we can
create all sorts of scenarios that are
using the spatial mapping data and
that's time for questions if you have
any and if you would like to try how
last as I'm here or I will be just
outside and you can try any questions
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>