<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tensorflow and deep learning - without a PhD by Martin Görner | Coder Coacher - Coaching Coders</title><meta content="Tensorflow and deep learning - without a PhD by Martin Görner - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Tensorflow and deep learning - without a PhD by Martin Görner</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qyvlt7kiQoI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome again I see lots of people like
machine learning thank you for coming
back so this is the last session of the
day well almost
thank you Stefan for programming a
session with the hard mathematics in the
last slot not for me you will be the one
suffering so I have to I have to wake
you up a little now don't worry
don't worry the math is not so hard so
today I want to work I want to go with
you through the construction of a neural
network start to finish including
optimizations we will be looking at
these numbers handwritten digits trying
to recognize those digits and I actually
want to take you all the way through
from the simplest possible model all the
way to 99% accuracy on this on these
images so I'm not claiming that I will
teach you machine learning and neural
networks all of it in 45 minutes but
what I hope is that we can go through
one example together and and you will at
least know how to do this one thing
really well so let's go what is the
simplest possible model we can imagine
just ten neurons so we have our images
28 by 28 pixel images we first we take
those pixels and all of those pixels we
just flatten them as one big vector
that's what we will use as our input and
now each one of those ten neurons does
the exact same thing it computes a
weighted sum of all of the pixels adds a
constant which we call the bias that's
just an additional degree of freedom and
it it feeds this sum through an
activation function that produces a
result so what kind of activation
function we'll see several today the one
thing they have in common is that they
are non-linear here for a classification
problem well the scientists that studied
this told us that
this activation function called softmax
was a good activation function when you
wanted to obtain categories and that's
all if you want
we have those 10 neurons and we picked
10 neurons because we have 10 categories
0 1 2 3 and until 9 so what we are
hoping here is that if in the system the
weights and biases the degrees of
freedom are chosen correctly then if you
put in an eighth the eighth neuron will
actually light up very strongly and tell
us I have recognized an 8 so how does
the softmax work well very simply it's
just an exponential so each neuron takes
the exponential of the sum it had as an
input and then once you have taken your
10 Exponential's you normalize this
vector of ten values which norm
whichever you want
pick your favorite l1 l2 Euclidean norm
usually here we take l1 so that whatever
so what we obtain as an output is ten
values normalize meaning it between 0
and 1 so we can interpret them as
probabilities so this net neural network
will be telling us with an image in the
input what is the what are the
probabilities of these being a 0 a 1 and
so on
so let us formalize this with the matrix
multiplication but first of all let me
complicate a little this is too simple
we will do this not for one image but I
want to formalize this immediately for a
batch of images so here in this matrix
on the bottom side I have 100 images one
image per line all the pixels flattened
in in one line and taking this is my
weights matrix on the top so look at the
first column of weights I use I start my
matrix multiply and I use this first
column of weights to compute I first
weighted sum for the first neuron then I
continue the matrix multiplication and
using the
second column of weights I again compute
the weighted sum of all the pixels of
the first image and so on until I have
ten different weighted sums
corresponding to my ten neurons for the
first image I still have my biases to
add so there is one bias per neuron it's
just a constant I add one bias to each
of those weighted sums and now if I
continue my matrix multiply I will do
the same for the second image for the
third image for the fourth image and so
on until the hundredth image so here I
have all the weighted sums for all my
100 images for and the ten neurons I
would like to represent this with a very
easy formula so x times W that's the
matrix multiply I just explicit it that
works I have a problem with the Plus
over there you see x times W is this
result that's 100 lines and ten columns
B is just ten values I can't add those
two together the shapes don't match well
the standard solution in Python and
numpy which is the scientific
computation library in Python is to
simply a really fine plus we want to
write this as a plus let's redefine plus
it's called a broadcasting plus it's
it's not it's not a hack it's really how
plus is defined in numpy it's called a
broadcasting plus and the way it works
is if you are trying to add two things
and the sizes don't match don't give up
try to replicate the small thing as well
as many times as possible to make this
size as much it just so happens that is
exactly what we want to do those ten
biases we want them added on each line
because each line represents the
weighted sums for my ten neurons and I
want the weighted sums plus bias on each
line so it actually works I can use this
formula as the central formula of a
one-layer neural network and this is how
a one-layer neural network is
represented so let's go through this
again we have images and x100 images one
image per line all the pixels flattened
in one big 784 pixels vector multiplying
it by the weights matrix computes all
the weighted sums for my ten neurons and
my 100 images I have the biases and then
I use the softmax function I apply the
softmax function line by line so line by
line i elevate my weighted sums to the
exponential and normalize this vector of
ten elements
next line I take the exponential of what
I have and normalize these ten elements
in the outputs I obtain for each image
ten values which are the probabilities
of this image being a 1 0 and 9 and so
on those are my predictions so let's see
how this is written in tensorflow
I hope this is not too surprising
tensorflow
is so that it's a Python library it has
this useful and n of library which
stands for neural networks it has all
kinds of neural network related
functions such as softmax and apart from
that it does basically matrix
multiplications so we have predictions
and now while this will work if the
weights and biases in the system are
good what does it mean good well give
good means giving us good predictions
how do we know the predictions are good
well we will train the system for the
predictions to be good actually what
training means is that we will be giving
known data known images to the system it
will be telling us what it thinks this
image is and we will be telling it what
it should think that is training so - -
we need a measure of how good the the
training is at one given point and
that's fairly easy to calculate
our model is giving us ten numbers the
ten outputs of the 1010 neurons which
are probabilities of one of the input
image being something during training we
use known images so we know what these
images we simply encode the known
information it's called one hot encoding
very simple encoding to encode is six
you take ten values all zeros but you
have a one in the sixth position in that
format it's very similar we can now
compute a distance between those two
vectors and again any distance here will
work
you like the Euclidean distance the sum
of squared differences that's fine I'm
be happy with it but the scientific
community has a consensus that for
classification problems this distance is
slightly better so this distance call
the cross entropy how does it work well
element-wise you will be multiplying the
elements of the top vector by the
logarithm of the bottom vector element
by element and then you sum all of this
across the vector you add the minus sign
obviously because all of those values
are less than 1 so logarithm of that is
negative UNITA minus value and you have
a distance between what the network
sorry what the neural network thinks and
what you know to be true now the
training is simply to minimize this
distance so let's go let's try to train
a neural network let's see how this
works so here I run it on the top I have
my training images those are my 100
training images I send in each training
iteration on a white background
I put them on a white background when
they have they are already correctly
recognized on a red background if the
system still misses them so that's what
I use for training now to assess the
quality of the training you can't do
that on the training images ok you need
to do that on a set of images that the
system has never seen
before so I have also 10,000 test images
here you only see in the bottom in the
right bottom corner you only see 1000 of
them so you have to imagine that there
are 9 more screens of test images down
there but again thankfully all the badly
recognized one are sorted on the top on
a red background so you see with the
scale here that this extremely
simplistic model just ten neurons
trained across 2000 iterations is
already recognizing 92 percent of my
test images not so bad for such a simple
model on the top middle there I have my
lost function the cross-entropy
ok computed both on the training images
and on the test images well it's going
down that was my goal I was minimizing
the cross entropy I'll tell you in a
second how of course but here I just see
is going down and what I call accuracy
that is simply the percentage of
correctly recognized images so that one
is going up good time it seems the
training seems to be doing something
useful and and those last two diagrams
are all the weights and all the biases
all the degrees of freedom of my system
what you see here are percentiles so
just I mean percentiles are kind of hard
to understand but basically the full
extent of the colored region is where
100% of the weights or all the biases
are so basically what you see is that
the weights they started at all zero and
they evolved and now they are between
minus one and one the biases started at
zero evolve and are somewhere between
minus 20 that's just useful to see that
the weights and biases are changing
which is the goal of the training to to
find the good weights and biases and you
can keep an eye on this to make sure
that they are not shooting you know in
the in the hundreds or thousands which
would mean that your system is not
converging so this is how training works
you throw known images and known labels
at the system it tells you what it
thinks this this number is you tell it
what it should think you compute a
difference a distance between those two
and use the distance to modify the
weights and biases in a direction that
makes this distance smaller so let's now
write this in tensor flow to start in
tensor flow you need placeholders and
variables a variable is any degree of
freedom of the system so anything you
want tensor flow to determine for you in
our case it's our weights and biases we
will start with them initialized as zero
here slightly later as small random
values and we ask the training to
determine them for us
for that we need training data so for
all the things all the data that we will
be inputting during training we need in
the in tensor flow a placeholder here
I'm defining a placeholder for X which
is which are my input images let's look
at this the shape of this placeholder
it's non 28:28 one so first of all it's
a tensor the tensor is a
multi-dimensional matrix that's why it
has dimensions across well four
dimensions inch none will be the number
of images in this training batch not
known right now I will provide a certain
number of images during training I don't
know yet what it is 20 by 28 that's the
size of my images in pixels in one that
was not necessary here but this is the
number of values per pixel it would be
three if I was handling color images but
I'm handling black and white images I
just put it there to show you where the
three would be for color images so
placeholders variables now I have
everything to write my model so look at
the first line you should recognize it
that is the one liner representation of
one layer of a
neural network well we have a neural
network with just one layer so that's
all there is to our model just at one
line what you don't recognize is this
reshape operation that's simply because
our images our images were 28 by 28
pixel images and I told you we need them
as one big vector all the pixels
flattened so that is what the reshape
does and the minus one in the shape
simply means there is only one solution
figure it out so if you skip the reshape
what we have is a matrix multiply
between X 2 images W the weights plus
the bias fed through the softmax
activation function that's exactly the
one-liner
we have seen before so this produces the
predictions from our model I define a
second place holder for the known labels
for my images during training I know my
images are a 1 or 2 or 3 and so on so I
need to input that information as well
and I am ready to compute my loss
function the cross-entropy
what is it it's an element-wise multiply
the star is element wise between my
known labels and my predictions some
across the vector reduce some does a sum
across the vector and everything on the
bottom the three lines on the bottom
that is just the computation I'll skip
that that is just the computation of the
percentage of correctly recognized
images which I need for display to be to
- to put it on the curves all right now
we are ready to train and this is where
tensorflow really brings you a ton of
added value you take your loss function
you pick an optimizer here I chose the
brilliant design optimizer that the
simple that is the simplest one in the
library there is a whole series of them
and I you ask this optimizer to minimize
your loss function your cross-entropy so
let's see this cross entropy what does
it depend on you see the cross entropy
depends of course on the
no label labels and images so the
training data but it also depends
through the model on W and B the weights
and biases of the system that's exactly
what we want to modify so what this is
going to do is tensorflow here we'll
compute the partial derivatives of this
loss function relatively to all the
weights and all the biases in the system
first of all thank you ten through four
for doing this for us
how many weights and biases do we have
you remember here W variable of shape
784 by ten so almost 8,000 so this
vector of partial derivatives is a
vector of approximately 8,000 different
values Thank You tensorflow for
computing this for us secondly I would
want like to point out what tensorflow
does here is a formal derivation it's
not a numerical derivation it's actually
a formal derivation which is kind of
neat and this vector so mathematically
it's not as it's known as the gradient
and who know the gradient is a vector an
arrow
who knows where does the gradient point
yes
exactly almost exactly the the the
gradient points up we add a minus sign
because we want to go down and we have
an arrow that goes down suddenly we know
we're down is let's remember in which
space are we we are in the space of all
the weights and all the biases so if we
take a step in this space it means we
are modifying our weights and biases we
want to modify them so that the error
function becomes smaller well fantastic
we just computed an arrow that shows us
in which direction this loss function
will be smaller so that is the training
loop in the training you compute one
gradient on one batch of images and
enlighten known labels you add this
Union you use this gradient to modify
the weights and biases and suddenly
you're closer to where you want to be in
terms of loss and you start again last
thing the learning rate so if I was
adding a full gradient to my weights and
biases at each iteration I would be
going in the right direction I would
never arrive why imagine you're in the
mountains
you're on a mountaintop you have a
gradient for gravity we have senses for
that you know where down is and you want
to reach the bottom of the valley
imagine also you have seven-league boots
and you make seven-league strides you
would be jumping from one side of the
valley to the other you would never get
to the bottom so to actually get to the
bottom of the bath valley by following
the gradient you have to go slow so
that's what we do we compute this
gradient multiplied by this very small
number called the learning rate that
gives us small Delta's to be added to
our weights and biases we modify our
weights and biases and restart on the
next batch of training images and labels
here is the training loop before we dive
into this I have one more thing to tell
you about tensorflow tencel has a
deferred execution model so all the TF
taught some things that we've been
running until now they just build an
execute
on graphing memory they don't actually
produce values that's useful because we
want to do this formal derivation so we
need the graph for that it's a very
useful thing for distribution as well to
actually compute values in tensorflow
you have to define a session and then do
a session run on some node of your
previously defined computation graph and
when you do that you also have to
provide all the data for which you
define placeholders because now you need
it so the syntax for that is this feed
dictionary and if you like look at the
feed dictionary how this define feed
dictionary equals train data
train data equals blah blah blah and in
this dictionary the keys x and y
underscore are exactly what you define
previously as placeholders so this is
the syntax for feeding your placeholders
basically what we do here in this
training loop is that if we use a
utility to load the next batch of 100
images training images and training
labels and we run a training step what
is the training step that's what we got
here when we asked our optimizer to
minimize our cross-entropy
so this training step is what actually
computes the gradient applies a fraction
of the gradient to modify the weights
and biases so this is how the weights
and biases actually evolve during
training and you loop on this all the
stuff on the bottom forget it that's
just for display so I'm computing the
accuracy and cross entropy on my
training data on my test data
so that I can put it on this nice little
graph as the blue and red curves not
useful for training at all this is just
to follow on my graph
anywhere done this is the full code so
let's recap placeholders and variables
variables for all the degrees of freedom
of your system the weights and the
biases placeholders for
the training data then you define your
model in our case just a one-liner which
gives you predictions you load the known
labels you can now compute a distance
between those two the distance is called
the cross entropy last two line simply
the computation of the percentage of
correctly recognized images and then the
core of what tensorflow does you pick an
optimizer and you ask this optimizer to
minimize your loss function that gives
you a training step in this training
step is what you execute in a training
loop so in the training group you load
100 images and labels execute the
training step and iterate again that's
all there is to training a neural
network so using this very simple 10
neuron neural network and using some of
these ingredients in our in our recipe
we obtained an accuracy of 92% 92%
correctly recognized images who thinks
this is good who thinks this is bad yeah
this is good offal come on imagine you
do the post office you want to use this
on on zip code recognition it misses
eight numbers out of every batch of 100
it's unusable we have to do better than
this answer since a deep learning is all
the range let's go deep actually it's
very easy to go deep with the neural
networks
you just have to stack layers look the
first layer each neuron does a weighted
sum of all the pixels of the image well
if you stack a second layer on the
second layer each neuron will be doing a
weighted sum of all the outputs of the
previous layer very easy you can stack
them the only little wrinkle is that I'm
keeping my softmax activation function
on the last layer because it's useful
produces nicely normalized probabilities
that's what I want in the intermediate
layers the usual
activation function is this one it's
called the sigmoid it's simply a
function that is nonlinear that's very
very important but simply goes
continuously from zero to one so let's
try with this and actually so how do we
do with this in tensor flow this time
you will need a weight matrix and a bias
vector per layer okay so instead of
having one pair I have five pairs it's
also good practice to initialize them to
at least the weights to small random
values otherwise the risk is that your
course your system never even starts
converging so here truncated normal is
just a complicated way of saying random
so I have one weights matrix and if
someone volunteers to to explain what a
what a normal distribution is and how
you truncate it for me that's random now
so I have one way it's matrix and one
bias vector per layer and this is what
my model becomes so you recognize the
first line we've seen that before it's
just that we are using the Sigma it has
the activation function now and yes
exactly I see a gentleman doing this yes
we pipe the output of the first line as
the input on the second line in the
output of the second line se as input of
the third line and so on that's how you
chain them until the last line which the
only difference is this last line which
has only ten neurons because we still
have only ten categories to recognize
uses this softmax activation function to
produce probabilities that is all that I
have changed in my model so let's run it
before we run it I have to confess I
lied to you the sigmoid is not anymore
the most popular activation function
actually it has many problems especially
if you add many many many layers and
here you could see it
just five layers you could train with
the sigmoid function and you could see
that you know at least in the initial
few iterations the the accuracy goes up
but not very not very fast but if you
had a lot more layers you will start
having real troubles with the sigmoid so
a new activation function was invented
actually it's even simpler I don't know
I don't understand have this works it's
just zero for all negative values and
identity for all positive values you use
this instead of the sigmoid and suddenly
you can build deep neural networks why
honest answer mathematical answer honest
answer nobody knows we tried it it works
yeah success that's the hottest answer
I'm sorry slightly more mathematical
answer well you see the sigmoid function
is kind of flat on both sides so if you
end up on those sides there the gradient
is tending towards zero use the gradient
to go forward so it's a problem if your
gradient is is getting too low low to is
having very low values the the rayleigh
function at least on one side it doesn't
have this problem so that's what he
white solves it that was super
mathematical but actually I'm being
honest when I'm still telling you that
if you find a method math paper
explaining why this works better it is
an after-the-fact justification people
first tried it it worked better and then
that's it
and actually the reason they tried this
comes from biology people used to think
biologists used to think that the
neurons we have in our head they were
using the the sigmoid activation
function actually now the consensus is
more like this they think that a
biological neuron when it receives
simulations on its actions it produces
nothing until the threshold
and after the threshold it reduces an
activation that is proportional to the
amount of inputs which is a lot more
like this rather than the sigmoid
function but well so the inspiration was
biological but then people just tried it
works better that's fantastic and
actually if we simply replace our
sigmoids with rail use we get a much
faster start this is only 300 iterations
so just to start it starts faster okay
we're happy let's let's push this to
twenty to ten thousand iterations first
of all yeah ninety-eight percent
accuracy we are suddenly by adding a few
more layers recognizing 98% of our
images this is huge we we jumped from 92
to 98 but I mean those curves are kind
of messy you see all these noise and and
look at the accuracy computed on test
data it jumping up and down by a full
percent this doesn't work this actually
means that you're going too fast you're
jumping from one side of the valley to
the other you're not reaching the bottom
so you have to slow down but the clever
way to slow down is not just to use a
lower learning rate because that would
slow you down completely and multiply
your your your training time by as much
the cleverest thing is to start fast in
decay the learning rate to end slow okay
that sounds really as a very yep that
sounds really really simplistic but
actually the result of that little
changes is spectacular this is what we
had with a fixed learning rate of this
0.003 by starting at that level and
during training decaying the learning
rate exponentially towards 0.0001 look
at this all the noise is gone it's
spectacular all the noise is gone
now suddenly the accuracy the test
accuracy is stable above ninety-eight
percent it's not jumping up and down
anymore it's stable and look at the
training accuracy the blue curve their
age across the last 2000 iteration it's
stuck at 100% so here for the first time
we built a neural network that can
recognize our training images with 100%
accuracy it can recognize all of them
I'm not saying that it has 100% accuracy
on test images and at least on the
training ones for the first time it got
them all all correct
so we're quite happy with that but now
let's look at the lost curves first of
all the blue one the training loss well
that's what we are minimizing so it goes
down nice we like like that it works
then but actually we don't care about
the training loss we want to recognize
real-world handwritten images we don't
care about the training data the only
thing we care about is the test loss is
how well this is performing on test data
which the system has never seen during
training and look the test loss
initially the red curve on the right
initially it goes down and then there is
a disconnect so this disconnect is to be
expected okay because we are working
only on the training loss the all the
optimization algorithm is minimizing the
training loss on training data the
minimization algorithm never sees the
test data so it is kind of expected that
initially both of those losses will come
down but at one point there will be a
disconnect because we are only working
on the training data and that has a
positive effect on the test performance
as well until a certain point but what
is worrying here is that this gap
actually increases when you see this on
curves
you look at this in a in a neural
network manual they tell you this is
overfitting you need regularization the
good regularization function I love this
one is called dropout and it involves
shooting the badly behaving neurons
that's a methodology so let's see how
this works actually you do shoot your
neurons during training you pick a
probability here P keep equals 75% which
is at each iteration the probability of
keeping a neuron in play which means
that at each iteration of your training
loop you roll the dice and you shoot 25%
of your neurons when I see shoot I mean
you physically remove them from the
system their weight and biases will not
be updated on the next iteration you put
them back roller by the roll the dice
again and shoot another 25% of your
neurons that is how dropout works now of
course when you evaluate when you test
this neural network you don't test with
a half brain-dead network you put all
the neurons back ok so you need to set P
keep at 1 probability 1 of keeping the
neurons to have this during testing I
put there the code that is that you use
in tensor flow to implement this
actually the dropout function is simply
something you call after your activation
function and all it does is in this why
F vector so the outputs of the previous
layer it will replace 25% of the values
by zeroes and also boost the remaining
values by a compensatory factor to make
sure the average stays the same that
would shift the activations or otherwise
but that's a technical detail basically
it zeros out 25% of your outputs now
what is the consequence of I mean this
is brutal this does this really work so
let's let's try but we will recap all
the things that we have done so far
initially we had our
five layer neural network using sigmoids
we trained it for 10,000 iterations this
is what we get ninety seven point nine
percent accuracy we replaced the
sigmoids with railers suddenly it trains
faster and we actually gained a couple
of tenth of a percent of accuracy by
just doing that still very noisy curves
so let's decay our learning rate that
cleans up those curves nice stable we're
stable above 98% recognition accuracy
and for the first time we are
recognizing all of our test data
perfectly but we have this disconnect
between the loss curves so let us add
dropout first of all some noise creeps
back that is unsurprising given how
brutal this technique is you see how it
works you should you're shooting your
neurons so some noise is expected then
look at the test loss curve so the red
curve on the right well yeah it has been
brought under control somewhat there is
still a disconnect between the do to
test curves but it's not growing anymore
so dropout worked did it help with
accuracy well first of all what I find
amazing is that it didn't destroy
accuracy the accuracy stayed constant
here but in this case you can't win
every time it didn't help with the
accuracy I'm kind of stuck at 98%
accuracy I don't seem to be able to do
anything and I tried a lot of tricks
here to get beyond that so we have to
revisit this concept of overfitting
let's ask the experts what do they have
to say about overfitting so
fundamentally what overfitting is is
when you give too many degrees of
freedom to a neural network too many
neurons you imagine that you give to a
neural network so many neurons that in
the weights and biases of those neurons
it can actually store in some way all
the training data it will create some
kind of internal
or a for each example of a one that you
have given him instead of the one as a
concept that would work for training
data but that would be terrible in
real-world recognition because it will
it will basically learn by heart to
recognize the 10,000 different ones that
you have in your training that is data
set and would still be completely unable
to recognize one that I just draw now so
the takeaway here is that a neural
network you have to constrain it you
have to constrain its degrees of freedom
to force it to generalize from the
training data to your real-world test
data the opposite of that is if you
don't have enough data even if your
network is constrained it has a really
reasonable amount of neurons if you have
only a little data you can still store
all of that somewhere in the weights and
biases you'll get the same problem so
anything you do with neural networks you
need tons of data and then if we did
everything right here like here why is
the world so unfair we we actually we
have a five layer network I promise you
I try to remove layers doesn't help so
I'm still stuck at 98%
I have tons of data so I'm not
overfilling because I have too many
neurons I have tons of data this data
set has 60,000 handwritten digits for
training and 10,000 for testing so that
is not the problem I tried the
mathematical tricks like adding
regularization it helps somewhat but I'm
still stuck at 98% so the last
conclusion is that for some reason the
network the neural network will we are
using is actually not capable of
extracting more information from this
data set does anyone know why we need
something Freddie stupid at the
beginning like really stupid
any idea exactly exactly yes we had a
nice image 2d with shapes you know and
and handwritten digits this just shapes
like curves and pictures and curves and
and circles and straight lines and all
of that and we took all of that and
destroyed the 2d information we just
flattened all the pixels in one big
vector well it's no wonder that this
system doesn't know how to retrieve this
information and it's hard so we have to
build another neural network this time
designed to take advantage of the 2d
information in images and this is a
convolutional network this is how it
works so I'm going back to the generic
case of a color image so that's why I
have three channels of information per
pixel red green blue and this time a
neuron will still be doing a weighted
sum but only a weighted sum of a little
patch of pixels above him here four by
four pixels he does a weighted sum of
just that and then I shift one pixel to
the right and here again I do awaited
some of the 4x4 pixels right above me
and second big difference I'm reusing
the same weight okay so those two
weighted sums have been computed using
the same weights previously in dense
networks each neuron had its its own
weight
here we are reusing the same way it's
basically we are scanning this image
will do this in both directions with a
little patch of weights so if we scan it
in both directions and of course with
some proper padding on the sides we are
going to obtain as many output values as
we had image as we had pixels in this
initial image and for that how many
weights do we need well as many weights
as elements in that highlighted cube so
that's four by four by three which is 48
I'm not mistaken so 48 weights in the
system how many did we have in our
simplest 10 neuron neural network
something like 8,000 and here 48 that's
not going to work not enough degrees of
freedom we need more and it's very
simple to do to add more well let's do
this again we take a second set of
weights and rescan the whole image using
this second set of weights which
produces another here let's call it the
channel channel of output values and we
can do this as many times as we want and
since we are using tensors we can now
write those two weights matrices as one
by adding a dimension and we have the
shape of a weights tensor for a
convolutional layer of a neural network
so let's go through this again four by
four in this shape is the size of the
patch for using patches of four by four
pixels three is the number of channels
in the input so we here we had a color
image so that three values per pixel and
we did this twice applying two patches
of weights which means we obtained the
output twice let's call these two
channels of output values that is the
last value in the shape of this of this
matrix now the last problem we face is
that well written like this with a
number of input channels or output
channels you see what's coming we will
be able to stack them but we still have
one problem we need to distill the
information down at the very end we
still want ten neurons for our ten
categories so the traditional way of
doing this was to sample those answers
from those patches this is important to
understand because it gives you an idea
of what's going on even if it's not what
we're doing anymore but what was done is
that in the output values you would take
squares of two by two values and simply
retain the biggest
and that's what you caused to the lower
layers what's the justification for that
well during training those weights will
evolve into some kind of you know shape
recognizers some of those patches of
weights will become very sensitive to
little circles some of them will become
very sensitive to oblique lines so an
output of one of those patches is
actually something something saying well
I've seen a circle right here with a
good intensity so the down sampling
which is to say on four by four pixels I
take only only the sorry four by four
output values I take only the one that
is maximal that's actually saying well
here I've seen something so I will keep
keeping this here I've seen nothing here
it's the nothing here and I've seen
nothing so I'll be dropping those down
sampling but actually there is another
way of simply condensing the information
that is to play with the stride of this
convolution if I am plying my patches
not pixel by pixel or every two pixels
mechanically I will obtain twice less
values in both directions in the outputs
and that's also a way of condensing the
information so actually today if you see
convolutional layers you will mostly see
them with only convolutional layers and
no subsampling layers I'm not saying it
works better it's just simpler to
explain so this is the convolutional
network that I want to build with you it
starts with the image going back to a
grayscale image and a first
convolutional layer so let's look at w1
the weights 5x5 is the size of the
patches one is means it's reading in one
channel of information because we have a
grayscale image it's only one channel of
information for the last number is
because we are applying full of those
patches producing the output four times
let's call this four channels of
information now we go to the next layer
w2
now we use 4x4 patches reading in four
channels of information because we
produced four channels previously and
this time we use eight of those patches
producing eight channels of information
stride too so this time those planes of
results instead of being twenty by
twenty eight they are only 14 by 14
because when we do the scanning we jump
two by two pixels in both directions and
that is how we condense our information
third layer patches of four by four the
first two numbers reading in eight
channels of information because that is
what we produced in the previous layer
and we do this 12 times again stride of
two so now our planes of results are
only 7 by 7 going down from 14 by 14 to
7 by 7 and now we apply a fully
connected layer so a fully connected
layer is what we have seen in the first
part in a fully connected layer one
neuron does a weighted sum of all the
values in this cube of 7 by 7 by 12
value in the next neuron right next to
it will be doing a weighted sum of all
those values using its own weight this
time no sharing of weights because it's
a fully connected layer last layer our
10 neurons using softmax as an
activation function to produce our
predictions so let's see how to write
this in tensor flow again you need one
weights matrix and one bias vector per
layer the only difference is that for
the convolutional layers the shapes of
those weights matrices are slightly
different it's what we have seen they
have this shape and once you have that
you can rewrite your model so this is
what my model becomes tensorflow has
this useful comp 2d function which
actually does simply the double loop on
the image so you give it the image you
give it the weights it will do the
double loop don't mind the slightly
complex syntax for strides
to be honest I don't know why they are
for numbers I just highlighted in red
the numbers that need to be 1 or 2 to
get a stride of 1 or 2 I will read the
documentation next time and do this talk
to explain the other values so now we
have and of course once I have done this
I feed the result through by activation
function which is the reading so I have
my three convolutional layers then I
need to reshape the the outputs to get
into my fully connected layer you see my
cube of seven by seven by twelve values
I need to put it in one linear vector
that is what reshape does and then I add
two normal layers I hope that by this
time you recognize these these formulas
one fully connected one using Rayleigh
and another one using softmax
this time with only ten neurons in it
that's all I changed the model in the W
the weights and biases the definition of
the weights and biases let's run this
and I hope that you know I promised you
we will going to beat 99% accuracy
together so let's see if we get there
let's go so it's slightly slower because
yeah you know we are asking the system
to be do a lot more computations but you
see even at only 100 iterations the
accuracy is shooting up pretty pretty
strongly actually let me zoom because we
can't see anything here let's go well
I'm already at 96 percent I'll do more I
think I will be getting to 98 pretty
soon 97 98 and so on this is going to
take maybe slightly longer than you have
patience for so I prepare the video for
you which is slightly faster
let's go shooting up I'll zoom all the
99 is not far away we're getting there
we're getting there we're getting there
come on we're getting you
in the end we didn't get there 98.9
debit all right we have to go above 99
so actually I'm going to give you a
trick which is kind of a methodology for
designing the the ideal neural network
for a given problem first you start by
restraining your neural network a little
bit until it hurts here I know I can do
better than this so I know that I have
not given it enough degrees of freedom
once you once you are there you give it
a little bit more degrees of freedom and
you add a regularization to make sure
that those additional degrees of freedom
do not result in our overfitting so
that's what we will do here look I take
slightly bigger patches 6 by 6 in the
first layer instance instead of 5 by 5
but more importantly I use a lot more of
them instead of 4 8 and 12 patches in
each layer respectively now I use 6 12
and 24 more degrees of freedom and to
make sure I'm not over fitting I add a
drop out in the fully connected layer so
why not in the others my my feeling is
that we don't have enough degrees of
freedom in these in these convolutional
layers not enough neurons to start
shooting them the fully connected layer
is where most of the neurons are so
they're drop out will work alright let's
try this
and actually this time the accuracy is
shooting up pretty fast I will have to
zoom to see something let's zoom 99% is
not far and we are above yes thank you
actually we got to 99.3% accuracy and if
you go to the amnesty website where by
the way you will find 20 years of rng in
papers on this problem yes some people
had a lot of fun with this problem they
get the maximum they get today is 99.7%
so here all of us together in a now in a
one-hour session we got not so far from
what the maximum is and to finish I just
want to say too to show you what dropout
did you remember last time we apply the
dropout we were a bit disappointed here
is what we had with this bigger Network
but no dropout just a bigger Network we
already shoot above 99% accuracy so we
like that but we have this this wild
disconnect between the test and love
test and training losses so let's add
dropout on the fully connected layer you
see the test loss is very very nicely
brought under control and just with this
one little you know shooting trick we
won to tenth of a percent of accuracy
which which is a huge we're fighting for
the last percent here so getting to
Darrell there are only 10 steps to go
and we get with it two of them just by
adding drop out here it works really
really nicely alright so you here we
have the CliffsNotes for the the entire
session and I hope you enjoy this this
journey through the building of a neural
network I've tried to show you how we
proceed what we follow on the curves
which solutions we apply and also we've
seen today together two very important
neural network architectures which are
fully kinetic networks and convolutional
networks if you want to follow up and
have the full picture the one you're
really missing or recurrent networks so
I actually did a session on recurrent
networks in a university session on
Tuesday there is a video for it
you have this presentation as a link you
have my Twitter handle in case you want
to say something and so this was all
about tensorflow
we also have a managed service called
cloud ml for running your tensor flow
graphs in a distributed environment so
if you don't have a cluster of ten
machines under your desk you can do your
training and your serving as well as by
the way on cloud ml and if you don't
want to train your role models we
actually have a couple of three train
models we have plenty of data to train
them on so you can try and try those
there is a cloud vision API the speech
recognition API the natural language
processing API and and Google Translate
thank you and we have two minutes for
our four questions if there are any
questions yes
the number of neurons so the question
was we have used plenty of hyper
parameters so like the activation
functions the number of neurons in a
layer would it be possible to train a
neural network to come up with those
numbers actually we just release the
paper doing something even better
you see those the the full shape of a
neural network what is it
convolutional numbers convolutional
numbers convolutional numbers fully
connected numbers fully connected
numbers that's a sequence of symbols so
we we wrote a neural network that from
for a given problem description
generates a sequence of symbols that
actually represent a neural network
architecture a succession of layers of
this shape - shape and so on that solve
this problem you know in an ideal way so
I don't know if this will put all the
neural scientist out of business
immediately they have already put all
the statisticians out of business but I
think it's it's a very interesting
approach realistically speaking this is
hardcore research realistically speaking
hyper parameter tuning is something you
do yourself you know with the knowledge
of the system you can do like an
exhaustive search of these parameters
it's actually something you can do in
cloud ml you can you can say please try
to fit the best values for all these
parameters it will do a grid search the
only problem with the grid search is
that well these big networks sometimes
they already take a day or two to Train
so that plus a grid grid search well you
can throw lots of machines on it and
that's what cloud ml does exactly
other question yes the rayleigh function
the question was the rayleigh function
isn't a smooth function how do you do
the gradients who your math teacher
wouldn't be happy smooth what's that you
have continuous and you have this VD
foreign chable what we need is
differentiable it's differentiable
everywhere but in one point what is one
point in a computer it doesn't exist one
point is you know has no ignore physical
existence so in practical terms this
function is differentiable that's all we
need all that we need is that the
derivative of this function is defined
we can we can have the value for every
point that's the only thing we need yeah
II but I I don't think so we are we are
talking here the function itself is
continuous okay it's not jumping the
function itself the value is continuous
there is no jump the jump is in the
derivative of this function so when you
compute the the gradient well yes this
gradient might have very different
values for closeby values of of the
input parameters but that doesn't matter
the only thing that matters is that this
gradient has a diff point values you can
compute what it is and when you compute
this across 100 images which is here
it's called stochastic gradient descent
that's what we you that's why we use
mini batching well those you know
differences of opinion between the
gradients in closeby regions they kind
of averaged out and you still have a
pretty good direction that works pretty
well for those 100 images
I think we would have to go really
deeper into the equations to see if the
discontinuity in the derivative of the
rail you could potentially lead to
conversions problems I don't think so
but well we would have to look at the
equations ready to see if if the risk
exists in practical terms I've never
seen it it converges other question that
was a nice mathematical question thank
you yes yes there is most of the time
you need lots of data but let's take one
example where you can use less it's for
people who are in the previous version
it's is this example here this is a very
big neural network that we use for image
recognition well then the question was
sorry I had you can repeat it can we
devise strategies for for training the
neural networks on less data so usually
no you need lots of data but here there
is a strategy so this is a 40 layer
Network for recognizing images like here
a wheel and out of comes out comes the
fact that it's a car wheel this has been
trained on millions and millions of
images it's quite good it's what we use
in google photos and so on now imagine
what you need is not a car wheels not
the Eiffel Tower not to the Cathedral of
Antwerp and you need to recognize
flowers and yeah it doesn't do such a
good job on flowers so with what you can
do is actually take this fully trained
network with all of the weights and
biases already trained and simply chop
off the last couple of layers and you
will use this as a fixed encoding
functions for your images
the idea here is that all of these are
basically convolutional layers so all of
these have evolved into some kind of
shape recognizers and most of those
shapes are generic so you will still use
them
you just used it you don't know retrain
all of these layers you just use them as
a fixed including function to give it an
image it gives you a vector of you
choose the number of values one hundred
ten thousand values to represent this
image and then you train a small neural
network maybe just two three four five
fully connected layers to recognize from
those values your categories which are
types of flowers for instance in there
well you are training not all of those
parameters your training just a couple
of parameters in a couple of layers so
it's less more difficult CPU wise and
you need less less images to be
successful there this is a very useful
technique for retraining neural network
so that's one approach there is no not a
generic approach but case-by-case there
have been approaches last question may
be yes yes yes definitely there is also
an approach because mostly what the
network needs is is noise plus plus
information and to be able to pick up
the noise from the information yes that
is a very good approach if you know what
the information is you put it on a noisy
background and boom you have ten images
out of one real example that's
completely applicable and useful the
question was can you generate test data
and there are much more I mean even more
clever ways of of jinnah of using a
neural network to create your test data
those are adversarial generative
networks it's quite fun but I'm I won't
go into that all right
thank you very much I'm staying here for
five more minutes so if you want to ask
more questions please feel free to do so
thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>