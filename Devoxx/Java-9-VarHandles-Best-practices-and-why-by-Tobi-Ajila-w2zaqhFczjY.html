<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java 9 VarHandles   Best practices, and why? by Tobi Ajila | Coder Coacher - Coaching Coders</title><meta content="Java 9 VarHandles   Best practices, and why? by Tobi Ajila - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java 9 VarHandles   Best practices, and why? by Tobi Ajila</b></h2><h5 class="post__date">2017-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w2zaqhFczjY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right
so let's start so hi my name is Toby
today I'll be talking to you about Java
9 far handles
it's just disclaimer so a bit about
myself
I'm a developer on the open j19 in the
past I've worked on VM support for
lambda expressions I've also worked on a
Java multi sanity incubator project and
I currently participate in the Panama
expert group so here's the agenda for
today we're gonna start off with the
Java memory model then we'll take a look
at the of our handles API and then we'll
go into more detail with access modes
and fences and after we look at
performance goals so the purpose of this
talk is to motivate the need for a
memory model for access modes you know
why do we need these things
then we'll briefly look at some of the
older solutions in pre jdk 9 and then
we'll look at the new solution and jdk 9
so far headless and we'll go over those
features so if you're someone that does
concurrent programming maybe you do
lock-free
algorithms on java then hopefully this
will show you an alternative to the
older solutions and if you're not
someone who does that then hopefully
this is a nice introduction to these
kinds of things so let's begin Java is
the first mainstream language that came
out with memory model back in around 95
this was done to clearly define the
behavior of programs in a multi-threaded
system so we'll start with an example
here we have two variables x and y both
initialized to 0 and we have 2 threads
thread 1 and thread to sew thread 1
writes 1 to X and then after it writes
to Y
threats who will write will read why
ante and then read X n CB so what are
the possible values of a and B after
both threads execute so start with first
scenario thread 1 execute first so it
writes 1 to X and then 2 to y and threat
to execute after and it reads why is a
and X into B and the result is a is
equal to 2 and B is equal to 1 the
opposite occurs threat to starts first
so it reads Y and reads X and then
thread 2 runs writes
X writes why the result is 0 0 customer
to wasn't able to read the updated
values if thread 1 runs slightly ahead
of thread to sew thread 1 rights to X
then thread - reads why certain rights
why thread to reads X then the answer is
a is equal to 0 and B is equal to 1
now if the opposite happens thread 2
runs slightly ahead of thread 1 so
threat to reads Y thread one writes X 3
2 reads X thread one writes Y and the
answer is the same and if threads who
runs in between thread 1 so thread 1
will write x thread she will read Y and
X and then thread 1 will write Y the
result is also the same there is one
more scenario that's not here and that's
basically the inverse of the last case
where 3 runs first and then thread 1
runs in between and the result is the
same in that case as well so there are
only three possibilities right well in
reality that's not the case compilers
may decide and often do you decide to
reorder operations for performance
improvements they might decide to do
stores earlier and loads later to free
up registers for crucial calculations on
some platforms they might decide to
spread out stores so you don't carry
penalty if the right buffer is full
just to get consistent latency so and
thread once code the original code one
is rinse X and then Y is reinstitute the
compiler may decide to reverse those two
and if this happens we'll look at the
case where three two runs be in between
thread one Y will be written to you
first and then Y will be read at on
thread 2 and then X will be read and
then back on thread 1 X will be written
to and in this case then a will be equal
to 2 and B will be equal to 0 so there
are really four possibilities so let's
look at another example this time we'll
guarantee that there's no compiler
reordering so we'll just use machine
instructions no compiler in this case so
we have two memory locations x and y
both initialize to 0 thread 1 will write
1 to X and then read Y in our 1 thread 2
will write once Y and read X in our - so
what are the possible values of r1 and
r2 so we'll do the same exercise look at
all the cases if thread 1 executes first
it will write 2x read from Y and then
threat - all right so I read from X and
the result will be r1 is equal 0 and r2
is equal to 1 if the opposite occurs
then thread - you will rights Y and then
read from X and then thread one will
write to X read from Y and the result
will be r1 is equal to 1 and R 2 is
equal to 0 if thread 1 runs slightly
ahead a threat to the thread 1 right 1
to X and threats will write once Y and
then thread one will read Y and when
threat - will read X so in this case
we'll have 1 + 1 and the rest of the
scenarios are the same so the only three
possibilities but as you may have
guessed there is a fourth possibility
postures today can be characterized as
having either a weak or strong memory
model so clearly summarize so that means
strong memory models
the memory instructions occur in order
and memory instructions will be observed
by the other Hardware threads so in a
strong memory model when in thread one
when one is written to X the other
Hardware threads will notice that change
and the writes X will not be rewarded
after the read an example of a processor
with strong memory model is installed
x86 however in the weak memory model
memory operations can be reordered and
their effects may not be immediately
seen by other Hardware threads an
example of this kind of architecture is
PowerPC or arm so in this example if we
were on PowerPC for example the write to
X and thread 1 may occur but threats you
may not notice the change immediately so
this were to happen then threat to will
run it will write so while on and it
will read from X into r2 but not notice
the updated change so in this case it
would be possible to have both r1 as NR
2 as 0 so going back to the memory model
the previous two examples showed how
reorderings can change the behavior of a
program instead of three cases that we
expected we had four and you can think
of the fourth case as a potential bug
the previous two examples also showed
that reorderings can come from the
compiler or they can come from the
hardware and that different processors
may behave differently so if you want to
write a multi-threaded application that
has consistent behavior on all
architectures with all sorts of
compilers compile optimizations which is
important for Java by the way because
you know the model write once run
everywhere so if you want this
consistent behavior then you need to
have clear rules about which behaviors
are acceptable in multi-threaded code
you need to define rules between the
relationship between the variables in
the program and
the low-level details of storing data
and memory retrieving data for memory to
registers you also need to define these
rules in such way that they can be
implemented on a variety of
architectures and with the right variety
of compiler optimizations so to
summarize the Java memory model
describes how threads may interact
through memory so I'm gonna go through a
few specific things in the German model
this is not an exhaustive list so that
you actually have to read the memory
model but these are just some key points
that will be relevant to the
presentation so rule number one within
thread as if serial what this means is
that the runtime is free to perform any
optimizations to the code as long as the
result of the thread in isolation is the
same as what it would have been had it
been executed in program order program
order is order which the programmer
wrote it basically so that that just
means that the compiler is free to
optimize as it wishes as long as the
result is the same so in this example we
have thread 1 writes to y then writes to
X then writes to Y again and then writes
the value of Y to Zed so we see that the
compiler in this case eliminated the
first right so why because it's it's
never read in between and then reordered
the right X instead of it being second
its last and there are many other
optimizations you can call if you do and
you want the compiler to optimize your
code because you want your quotes run as
fast as possible so even with these
changes the program is still correct
because the eventual result is identical
the major downside to within thread as
if serial is that different threads can
have different views of the same data as
we saw in the first two examples so the
next rule gives you tools to kind of
combat this issue
so Java includes several language
constructs the two that are of interest
right now are volatile and synchronized
these tools are intended to help the
program are described their concurrency
requirements to the runtime volatile in
synchronized blocks at synchronization
points this means that what happens
before the synchronization point is
ordered with respect what happens after
of course synchronized the smaller those
locking but for for this presentation I
just want to highlight that relationship
so we'll look at an example we have two
fields x and y x is a volatile and y is
just a plain int so instead one will
write to the Y and then write one to X
and then thread two will read X if it's
equal to one then we'll read Y in this
case so in the previous examples we saw
how reorderings could occur and if in
thread one access written two before why
was it in two it's possible that in our
one in threat to Y would not be two but
in this case because we use volatile for
X what this means is that the runtime
will enforce that happens before
relationship between the right to two
and the right the right to wide and the
right to X so in this case the program
order is conserved the the runtime jet
will not reorder those two operations
and it might add more instructions to
ensure that the processor also maintains
this relationship now this doesn't come
for free enforcing it happens before a
relationship limits the possible
optimizations that J is a lot to do so
adding volatile everywhere is not the
answer
now for the last rule atomicity fields
that are 32 bits or less so atomic
operations operations that are
indivisible they're done all at once so
the camp instructor can't be paused in
this example we have two fields X&amp;amp;Y X is
an integer so 32-bit field and Y is a
long 64-bit field and then so X is we're
gonna write a 32-bit literal to X and
64-bit literal so Y so on the Left where
we see that the rights X is done all at
once it's done atomically in one chunk
however the rights of y is allowed to be
done in two separate writes and when you
do this this is called feel tearing what
happens is when you tear a right to a
field it is possible to observe the
field in between the rights which can be
a problem because this can lead sub bugs
in practice this doesn't usually happen
on especially on 64-bit platforms
because many registers that can write
that size easily and on 32-bit platforms
this there are some platforms that offer
registers I can also write this but the
memory model allows you to tear feels
larger than 64 bit 64 bits so it's
important to know that so next I'll show
you a brief code it says some
programmers over synchronized code which
can make programs slow some programmers
under synchronize code which can make
programs wrong so this highlights the
fact that over synchronization prohibits
performance optimizations but understand
Crenn ization can be worse in that it
can it can make programs wrong it can
introduce bugs that are very difficult
to find so ideally we want
tools that let programmers define what
kind of synchronization they need while
leaving room for optimization
opportunities so tools that are flexible
enough that you can dial up the
constraints or reduce them depending on
the specific scenario so
now we'll look at some of the existing
preacher JDK 9 options so I said before
we have synchronizing volatile which
will give you the secret and the
synchronization guarantees that you need
however there are some cases where they
might be too coarse there are some cases
where you could use a more relaxed
access mode and still get the same
synchronization guarantees but with but
leaving more room for the compiler to
optimize there's java.util atomic type
so atomic and atomic long but it's only
limited to specific types it long and
reference there are also reference
references so like you have a reference
to the feel that you're actually
interested in so if you have an atomic
site on each of your instances as
additional bloat and then there is some
misconceive which as you know is unsafe
and is likely not to be supported in the
future and there's talk of you know
requiring command-line options to use it
in next release so these tools are good
but we need tools that can do more and
that can do more safely so what we need
is the VAR handle API so this is a new
API recently released in JDK 9 of our
handle is basically a reference so
variable that supports a wide variety of
accesses so it's a variable handle hence
for handles there are a signature of
polymorphic so you don't have to worry
about boxing or packing arcs they're
also in the Java invoke package so final
is trusted which helps performance
optimizations and it's currently used in
Java so concurrent so you know it works
so there are five different kinds of var
handles for accesses to instance and
Static feels there's instance fields var
handle and side few of our handle so
these are probably the more common cases
these could be primitive
these could be references and potential
in the future they could be value types
of course feels are not their own kinds
of variables you can have arrays as well
so for this there's the array element
for a handle so this is pretty cool
because you can treat an array element
as volatile and then there are the view
of our handle kinds for by arrays and by
buffers this could be useful if you are
reading some data format for example
let's say it's backed by byte array and
you need to extract the first three 64
bit values because that's the special
meaning you could use a byte array view
over the data and extract it as Long's
because this API also supports byte
buffers it means that you can also view
off heap data as well which is a nice
feature to have the array and byte
buffer bar handles also do bounce checks
so there's added safety to that so here
are the memory ordering modes that are
as part of our handle API there so
there's plain opaque release acquire and
volatile they're listed from weakest to
strongest and we're stronger most just
adds further constraints on the weaker
mode and I'll go over these later
they're also atomic operations so get in
set get inside with bitwise arithmetic
require we compare and set and compare
an exchange so how do we get a var
handle first we have to do a lookup so
here's an example we have a class called
example and we have a field called X and
then with this declarative our handle
called capital X so if the look of is
similar to what is done with method
handles in the static initializer we get
the lookup objects and then we call find
var handle we give it the name of the
class
namely field on the type and then we
gave our handle so the different types
of lookups you can do in the previous
case we did find our handle that's for
instance fields you can do a fine static
VAR handle for static fields and if you
have a Java like reflect field object
you can unreflective our handle to get
the right handle for that field so how
do we use them and using them is pretty
simple
you take care of our handle and you just
call operations on it so in this case we
have an instance field that we and we
want to assign it without it too so
we'll do Excel picked this and too so
the important thing to notice is that we
don't have to pull offsets in here as
you would have to do with unsafe the for
handle knows where how to find the field
so you don't have to do any explicit off
offset it if this was a static call they
would only take one parameter so it
would just be set OPIC to and this is
not the only way to do it you can also
use methanol invoke so you would do a
look up on the VAR handle class you look
for the operation that you're interested
in in this case settle Paik then you
give it access mode and then it returns
a method handle then you bind the method
handle to the VAR handle and you just
call method handle invoke you could do
some similar thing with invoke exact so
this example in the previous example are
identical there are additional faction
methods for creating var handles for
array elements the previous stuff we saw
was just for instance field and static
fields but these factory methods can be
used for arrays for byte buffer views
and bio reviews so now I'm just going to
show you some tips that you can use when
coding for handles sotomayor as you may
have noticed the VAR handle was declared
as a private static final static is
important
so that you only have one copy for the
class as you know so the problem with
using as previously probably using Java
util concurrent atomic integer is that
you always have a reference but with VAR
handles you can have one reference to
the variable that you can use for every
instance of the pass final is important
because in for message it that values
constant so this will help it in
reducing the VAR handle call down to the
minimal instructions required for the
operation and adding privates just good
practice you don't want to leak access
to an internal field to another class
that doesn't have access to the field
also it's a advice to always declare to
feel value that you'll use with our
handles as volatile this means that you
have to opt into a weaker mode and it
makes the mistakes that's harmful so for
example if you were doing set release
and get acquired with X and you forgot
to do a release you just did a regular
write at least this way you pay for your
mistakes with worst performance rather
than a bug so wait if you forget to do
the proper mode then you default to the
strongest mode which is better than
defaulting to the weakest mode which can
introduce more bugs and then one other
minor thing is you'll notice when you're
using var handles you don't actually see
the name of the field that you're
accessing so it's a nice convention to
name the for handle with a similar name
or in this case I'm just using capitals
but it's good to name the VAR handle
with a similar name so the field is
referring to so that when you're
actually reading the code you know
exactly what you're doing access is on
so now we'll take a look at memory
ordering loads so as I briefly
previously said there are for this plain
opaque lease acquire and volatile
there's not gonna be a very detailed
description but just enough to give you
an idea of what it is and what it's for
so we'll start with a plain
so this is probably the one that you're
most familiar with these are regular
reads and writes basically what we saw
in the early examples they only have two
restrictions within thread as if serial
so they can be reordered they can be
eliminated and they are bitwise atomic
for fields that 32 bits or less so they
they have no ordering constraints
respect to other threads the compiler is
free to treat them in the isolated case
so let's look at another example it's
probably a familiar one so we have X a
field X initialize to 0 and we're doing
a plane get on X so if X is equal to if
it set to 1 then we will proceed and
continue but until then we'll continue
spinning and in thread 2 at some point
in time X is going to be set to 1 so in
plane mode thread 1 is not required to
notice subsequent writes if it wasn't
seen in the first encounter so you know
on many cases if you try this it won't
progress the register is free to put so
the compilers free to put X in the
register and continue reading from that
register so that means you just spin
forever and this doesn't break the rules
of plain access so the takeaway is use
plain mode if data races can't occur or
if they do occur they don't affect the
correctness of your program if we wanted
to fix this example we would have to use
a stronger memory ordering mode which
we'll see next
so opaque mode as additional constraints
on top of plane so it has everything
that plane had plus more this provides
awareness that the variable may be
accessed by multiple threads also it
ensures that updates the same variable
in the same context that ordered respect
each other so rights to later ease and
read so later writes ensures that writes
are eventually visible and enforces
bitwise atomicity for all types so types
that are
for exhibits but it doesn't impose any
ordering constraints with respect to
other variables so let's go back to the
previous example if we were to do the
same example except with I get opaque in
this case it would work
when 32 writes once X third one will
notice that change and we won't be able
to progress so now we'll look at a
stronger mode release acquire so this
mode adds a causality constraint on top
of impact mode so if an access precedes
a release right and in program order
then the axis will precede the release
right in the thread execution order
likewise if get acquired precedes an
access in program order it will proceed
the access in execution order here's
just an illustration of what that means
so set release ensures that what happens
before the set release stays States
before it's at least we're not program
runs and similarly you get acquire a
mixture that you know there's no
reordering after the grip get acquire in
program execution anything in the middle
of Suites agree ordered so similar to
the previous volatile example in this
case we have like we have a non volatile
so a plain Y that is set to two and then
we do a set release on X and then threat
to will read from X and if it's equal to
one then we'll we'll read from Y so
because we use set release we enforce
the happens before relationship with
writes at Y so writes Y will always
occur before the set release and that
means that in thread 2 if X is 1 then Y
will always be 2 so the benefit of the
release acquire is that it's not as
strong as volatile this means that the
compiler is free to do more
optimizations so this could be
potentially a more proof
solution than using volatile so this is
a case where using a weaker mode would
keep your concurrency constraints but
also let you potentially improve
performance so now we'll look at the
final mode volatile so this is the
strongest mode
besides further constraints on top of
lease acquire so in this case all
accesses are ordered so all reads and
writes are ordered respect to the
following operations
so using volatile with our handles is
equivalent to using the VAR handle
keyword on in in regular Java some
things to know JVM implementations are
free to use stronger accesses in place
of weaker ones and this because adding
more constraints does not break any of
the rules right on the other hand if the
JVM knows that some of the stronger
constraints cannot be broken
using a weaker mode and might downgrade
a weaker have a stronger mode with a
weak one for example if you had used a
volatile field on thread private
variable if you use the volatile access
on the thread private variable the
compiler is free to treat that as a
plain access because there are no
concurrency it can't break any
concurrency constraints because it's
thread private so now we'll look at
another synchronization tool just like
memory earning modes fences also enforce
memory organ constraints while
operations that happen before and after
mother named fences are is barriers the
benefit of this is that lets you be more
explicit about your synchronized
synchronization it's also useful in
cases where your synchronization or your
order control is not tied to a specific
variable and you can and they can be
used to substitute stronger accesses
so example you could use release vents
and plane right but as opposed to a set
release so here are the options so the
VAR hand will API provides five types of
fences
there's the load load fence the store
store fence the acquire fence release
fence and full fence using it as pretty
simple you just call the static
functions on the VAR handle class of our
handle and then name of the fence the
load load fence on the top left so a
little offense ensures that what loads
that happened before the fans can't be
reordered with lows that happen after
the fence and the store store fans is
basically the same stores before the
fence can't be reordered with stores
after the fence a release fence on the
top left and shows that Lowe's and
stores before the fence can't be
reordered with loads with stores after
the fence and acquire fence ensures that
loads before the fans can't be reordered
with closing stores after the fence and
a full fans ensures that loads and
stores before the fence can't be
reordered with loads and stores after so
now we'll look at some examples so in
this case we're constructing some data
we have an object X and we are assigning
its fields and then after that we'll do
a release fence and then we'll we'll
make that we'll make X globally visible
so make it access accessible to other
threads so the relief release fence
ensures that all the rights that we did
to X while we were assigning the fields
will complete before we do any other
right so this means that it won't be
possible to observe X in an incomplete
State we could do a similar thing with
set release so in the bottom right we'll
we'll assign the fields and then we'll
do a set release which makes that feel
globally visible so what's the benefit
of using fences so if we did a similar
example
this time we were sending up multiple
objects so x y&amp;amp;z so in this case we
would do a single release fence and then
make all those objects visible so it
lets you merge all your synchronization
points into one on the right you can see
that we're doing set release multiple
times which is just unnecessary so even
if you modify the example on the right
to use a single set release and then
plane rights after it would still be
more clear if you had used the fence at
least now you can see that it applies to
all the three objects that are being
made visible so another benefit is that
fences make it easier to optimize
specific cases in this scenario we know
that there are no loads before the fence
that can be reordered with a store after
so as previously stated a a release
fence basically makes sure that stores
and law stores and loads before the
fence can be ordered with stores after
in this case we know that there are no
loads before the fence so we can
actually downgrade our release fence to
a store store fence and the causality
constraint is still maintained in this
case there are some architectures that
have stores defense and release fence so
in those cases you would actually see
some improvements in the performance so
let's do a quick recap of for handle
operations so on the right are the VAR
handle operations that we've talked
about I'm only showing setters but
they're getters as well and on the left
are the equivalent unsafe ones so you'll
notice that and safe doesn't have
everything for example there is no of
course only for set will peak also with
the tommix and safe only supports int
long and reference but var handle for
handle support all the primitive types
and var handle has brought a support for
Atomics with memory ordering modes like
weak mode bitwise operations release
acquire and volatile also includes
things I get in exchange and it supports
store store and load load fences which
I'm safe doesn't so the point of showing
you the slide is to say that the var
handle API supports everything that you
may have been interested in unsafe and
more so it's a more complete feature set
so lastly we'll talk about performance
so it's good to have a nice API but for
it to be useful it has to perform well
so I'd open j9 our goal is to get var
handles to at least the level of unsafe
performance ensured what this means is
that the var handle calls need to be
optimized and compiled down to the
minimal instructions required for the
operation so on this front progress is
still ongoing we're still working on
this but I will show you a small example
of where we are currently so on the Left
we have a function called do batch it'll
take a long and then we'll iterate and
just set three of our handles and loop
in this case they are static for handle
so we don't pass in the instance and on
the right is the disassembly so I turned
off loop unrolling just to make it more
clear just make it more readable but we
can see that the three sets or optimized
and compiled down to three move
instructions which is what we ideally
you want in this case it's a very small
example but it just shows you what's
possible with all the other cases so
we'll look at what the compiler did so
Tyler was able to basically inline all
the calls all the way down you'll also
notice that we were using unsafe in this
case is that map's one-to-one with a VAR
handle call and unsafe is something that
Carly knows how to compile very well so
in this case we're able to get the Enlai
was able to inline everything down to
this to a single instruction
so to summarize concurrent programming
is challenging compiler optimizations
can change the order of the things that
you wrote hardware architectures may
also reorder operations so you need
tools that to help you define your
concurrency requirements what handles is
a great tool for doing this it comes
with a variety of access modes it comes
with more granular access modes comes
with more offenses and it's also a safe
tool you don't have to do any explicit
offsets you get bounced checks free and
you get performance that some hard with
unsafe so if you were someone that was
previously using and safe now you have a
supported way of doing the same thing
and you also have more options so here's
just some references there is a Doug
Lee's paper on this topic and there's
also a cool blog by pressing on lock
free programming that introduces you to
some of these concurrency topics so are
there any questions yeah
yeah I would I would think about what
platform you're on there are some
platforms using a weaker mode will not
help you so that's important to know as
well and also yeah it's a matter of like
how much time do you want to put it
versus how much performance gain will
you get from it is it worth it is it
worth the development time to do this
sort of thing so it's not really a
question I can answer but there are
trade-offs you have to make there are
some cases where you will get better
performance by using for handles and you
just have to determine whether it is it
worth it to go to put the time and
effort into it yeah
with volatile so all platforms will
support volatile there are some
platforms
I think arm is one of them that will
also support the weaker mode so they
will have you know they will support
store store fans and release acquired
fence so in those cases you will
actually you might see a difference by
you know optimizing for the correct type
of fence on the platforms that don't
support all the modes then the
implementation will just default to a
stronger mode so even if you use a
weaker fence the the generated code will
be the same and you don't really save
anything so I think arm is one that will
support more modes I think PowerPC as
well but this is something you'd have to
look at alright so if there are no
questions that'll be it then so thank
you for listening to our presentation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>