<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Embracing Database Diversity with Kafka and Debezium by Frank Lyaruu | Coder Coacher - Coaching Coders</title><meta content="Embracing Database Diversity with Kafka and Debezium by Frank Lyaruu - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Embracing Database Diversity with Kafka and Debezium by Frank Lyaruu</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QddFVJgdE1Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah let's go all right thanks for
coming thanks for getting out of bed
first a small story a long time ago I
was a child I got a pocketknife for my
birthday and I loved that pocket knife
for a while I played with it and I did
sketch stuff with it but at some point
there was a problem because my sister
was a little bit older than me she had a
slightly bigger pocketknife and it had
more gadgets and the problem was that
her pocket knife could have had a bottle
opener and mine didn't and that really
pissed me off
not that I really wanted to open wine
bottles but that was kind of beside the
point and that's somehow always what
happens with pocket knives it can do a
lot of things and you always want more
and slowly slowly they get bigger I
would have loved to cut this one didn't
get it it's like 500 euros but if you
look at it you don't really need to
think hard to see that it's it's really
bad at anything I mean anything it can
do it can do badly imagine turning a
screw with that whole block of metal in
your hand it's gonna be horrible or
chopping food with it and then trying to
clean it somehow it's it's terrible it's
kind of a symbolic piece of technology
that's really about the theoretical
ability to solve a problem and not so
much about actually solving a problem
right so every single part of that knife
has a as a counterpart that cost a few
cents and that's in every way better so
I'm not actually going to talk about
pocket knives I'm going to talk about us
because we tend to make a lot of pocket
knives things that can do a lot of
different things that maybe should not
be together and for us it it usually
starts
starts all right and then we have a good
fall so what we want to build but
features come in and slow and slowly we
have like more and more of a monster and
at some point it just gets a bit silly
so now if we talk about databases they
have a very interesting history because
storing data efficiently and reliably we
found out pretty early in our in our
history that it was hard and that you
don't want to do it every time you build
a new app that you want to rebuild the
database know so that those became big
off-the-shelf products huge flagship
products of big companies that were
extremely expensive and like Oracle db2
like those and they those databases
tried really hard to be the only
database you will ever need and because
because money so at some point they will
start to add features like Oracle at
some object store that was horrible or
some xml scheme storing and because they
just try to add more and more features
and it really doesn't make all that much
sense so at some point it was really
when the web scale started it was
obvious that one kind of database
couldn't do everything and that's when
the whole no sequel thing kind of
exploded and after that you got the huge
number of different different kinds of
databases but still I still think that
even with all the no sequel you can't do
everything with one single database so
you have you need to have more so today
I want to talk about embracing heaven
embracing the multiple databases so
don't stop trying to everything with one
database it's it's a habit we have from
the past I think
but we shouldn't a little bit about me
I'm frankly a who I'm CTO of a small
company in Amsterdam called excells
one of our biggest clients is sporting
sir provider for sports for soccer or
for volleyball all kind of things we
have been around for a long time it's
close to 50 now I think we manage all
the things sports associations and clubs
do we have a few million players we have
a few thousand clubs about 40,000
matches every week and we have very
spiky load but we can predict it very
well so that works out usually our
technology stack used to be an Oracle
database because that's what we had some
Java based application servers and the
diverse set of clients look something
like this
we have seen millions of those nothing
exciting there so in practice little is
a little bit more like a little bit
messier than this but something like
this so for the a few years back we got
a kind of a challenge because up until
then we always used to we used the clubs
would do pretty much everything so if
somebody changed address the club would
do that or they planned all the matches
or chip make all the changes and we want
to stop doing that we wanted to give
every player the opportunity to change
that so that means that we would
effectively go from 6,000 users to few
million users and that would that's a
that's that's not not something you
brush over easily
so we also knew that
all right still there I'm in my blind
panic yeah so we couldn't move away from
Oracle in the short-term and scaling
Oracle if it's technically possible or
not I don't know but we definitely knew
we're not going to be able to pay for it
so we had to do something right so this
was the plan we have our old stack which
is pretty much the same as it was we
have some magical synchronization thing
and here we have another stack based a
MongoDB we had good experience with that
and that can serve all the traffic so I
my initial thought was shouldn't be too
hard so we had a plan capture the data
from the Oracle database in real time
dump it into Kafka to be able to queue
all of it and the other side insert it
into MongoDB and then go on holiday or
something
so that's never the way things go but I
how familiar are you always tough guy
who knows Kafka that's good enough for
me
so Kafka it's it's a platform now it
used to be like a broker that
essentially you shove stuff in it and it
comes out in the other side cutting some
corners there yeah there are some other
parts that are common recently Connect
API that's a cent standard API to get
stuff into Kafka or out of Kafka and the
streams API that will allow you to do
stream processing on topics and I'll go
ahead back to those two in a minute well
this is a well we know that so change
data capture it's a it's a pretty old
field but it it is has been mark
to be bad until pretty recently and so
basically what this means is that you
you point to a database and you get all
the changes that are made in that datum
database in real time ideally you can
also get some historic few of the data
so you can get all the changes that have
happened until then or some compaction
of that that sounds like a pretty
straightforward thing to me to have it
isn't it isn't and there's a nice quote
of Martin Claremont and I really like
that because it says why can't I do my
c-45 elasticsearch straightforward so to
have useful even and I agree with him I
think databases are self-centered they
tried to do everything for you they
don't play nice with other databases
they don't really think about oh how do
I easily compose with other databases no
they just try to be one database and if
you need to do more you add the feature
request and maybe even give them some
money so but there is some some hope
there is a feature that basically
everybody the base has well most of them
and it looks like they all came up with
it independently somehow and that's the
right ahead log and what that means is
that if you make a change or an insert
into a database the first thing you do
is you put your log that request that
you want to make the change if when once
that has been saved to disk then you
actually use that log to change the data
in the database because if you have that
if something goes wrong after you log
that log you can still replay that log
later and fix the database if it crashes
somehow so it's a lot because the actual
changing data if you change data on a
disk and it
and the database crashed at some point
it might be corrupted so you need some
kind of log append-only log to recover
so Oracle said archive log MongoDB cuz
it will upload post cursor right
headlock and my sequel goes at the bin
log so another use of the right head log
is replication so with that right head
lock you can make a master master-slave
replication so the master takes all the
updates it writes into its log and it
distributes its log to to its to its
followers so they can build up the same
data set and if you query you can query
of all of those again there is some
eventual consistency there so it's not
completely trivial but this is a pretty
common set up as well so what we wanted
to do was follow that up log and put it
into Kafka pretty straightforward but
there is it's not it's not as easy as it
seems and I want to because we actually
like using a sort of internal system
that NASA never really has been exposed
as an API it's not like if you have a
sequel or some other query languages it
clearly clearly defined API no it's it's
a kind of internal thing like you have
to sort of dig dig down in the in the
documentation or maybe even the code to
figure out how it works I think a good
illustration of that is the
documentation of Postgres and it says
from 9.4 on they had something called
logical decoding and this this page is
placed what it is so that you have like
you can extract the persistent changes
in a coherent easy to understand format
which can be interpreted without the
detail
knowledge of the database and that says
also something about the situation
before this the way they put it here so
it was incoherent hard to understand
format that was impossible to understand
without internal knowledge of the
database and that was definitely true so
so kudos for Postgres to do that but the
other database have a similar problem so
now here comes the B team comes in who
has heard of the B team more than I
thought
it's a it's from Red Hat it's pretty
young it has been around for a few years
but now it starts to mature a little bit
it's basically it tries to standardize
that part of the of reading and write a
headlock of different database so they
are slowly adding more and more
databases there it's based on a research
project for bottled water that basically
they did that for both quests but they
try to expand it to as many database as
possible
add a support now post as my sequel
MongoDB or call is in progress so I
don't know if there will be more in the
future it's one of those products that
has a really dirty job I mean it's like
at first I didn't know this was a thing
that it was hard and once you have
something like this and it works you
forget about immediately because it
works and it doesn't have new I you just
pointed to a database into it Kafka
queue and it does its thing
so if we make this a bit more concrete
we have C code source database over
there it has it has a database some
application of does really matter and
there's a change for example some person
could be an update could could be an
insert doesn't really matter and you put
that into the Kafka queue and on the
other hand we have a MongoDB there
the bass that listens do the changes
change and insert Entomology me that is
pretty straightforward so it works but
there is a problem and I guess most of
you have figured that out because this
is not really diversity in itself
because we take a relational data model
and drop it into among in the document
database so it's not really relational
anymore because we're not in a
relational database you can join but
it's also not really a document because
well it's straight out of a relational
database so if you don't you're not able
to change the data model you are having
the end up with the worst of two worlds
right you you have a data model that
relies on joining and on that it's
relational and you have a database that
cannot do that so let's give an example
if you have like some basic model some
base data model you typically have
something like if you have different
kind of communications associate with
the person you have one table of that
and you have some person associated with
it and you join them over so over the
personal idea over there nothing
spectacular but in MongoDB you would
want to have something like this right
that if you play the person you get all
of those and not that you have to grab
them one by one from different corners
because we can join so we want to have a
also for performance to have as few
queries as possible so you have you want
optimize that document that you just get
one document that contains everything we
need for that we need stream processing
so stream processing is that we listen
to a stream in our case a Kafka topic
and we have
this case two streams of two different
change feeds from database tables and we
transform them so in this case we want
to do a join but a join on the stream is
not that straightforward if you think
about it because there might be millions
of people on one side coming in record
and the communications flowing from the
other side they will not be in the same
order right so if you show if a record
shows up with the communication of a
person you have never seen well that
person might show up later or never you
don't know so you have to store that for
that when that person shows up you can
join it and make the document into
MongoDB and that and that part that kind
of adds up
because what Kafka streams does is they
have it has an internal database called
rocks DB you can also plug in other ones
but Rob CB is very popular because it's
an in process database so it's actually
runs inside the stream the stream
processing and it's so it has really low
latency and that's important because
basically every time a record comes and
you need to query like is there
something that that visited in the past
that I need to join with so everything
needs to do several queries and this is
just one of them right this is basically
one place where you join in your
relational model is all that and and
that and that adds up because like in
our system we had like half a billion
rows of sequel data and that ended up
being half a terabyte of Kafka date and
not that bad but the rocks DB data just
to do the joints is nearly as big and
it's the only complete replica from
scratch takes many many hours so it is
not sure if you at all even though that
if we have the resulting
database not going to be isn't all that
big is not not terrible it's a lot
smaller than these because they're in a
very expensive format another thing to
take it to cuddle consideration when you
use stream processing with this is that
it's different than building queries or
something like if you if you are
building some piece of software that
queries a database you change your
software you try again is it what you
want no you change you try again for
stream processing it's not that
straightforward because there is so much
data that if you if you have a running
system I can change the system and
restarted and keep running but then the
whole there are some data that it
process with the old code and some
process with the new code and that's
that's a bit tricky because in some
cases you can get away with it but often
you kind of need to start from scratch
to be sure that that is all still makes
sense so then you need to what we call a
generation so you really run the whole
thing again only old data and you can do
in a parallel you the old one is still
running and happy but you can build a
new one
but it's expensive so for effective
development you need a test data a good
test dataset that's not too huge also a
lot of times we found contaminated data
and that was contaminated years ago and
it was not a problem until now for
example there was due to some small typo
or bug I don't know there was one soccer
match that had like 10,000 people
playing it it never happened but it was
in the database and now the whole
replication crashes because it tries to
make a document that's just too big and
there are a lot of those small things so
be ready for that because they might
have not have been from
the past but this is the kind of moment
when they emerge so from this part is
where I went to production last you it's
a it's behave pretty well it's still a
learning curve
Kafka streams was in a pretty rough
state when we started is so much better
now so in that space and they're in that
sense we were a little bit early but I'm
still I'm still stand by our decision
now I want to go into a little bit of
architecture I kind of tend to avoid
that a little bit but to go into the the
obvious microservice thing because you
got it there's a problem I always had
with microservices and that's like this
is kind of the idea Microsoft you have a
thing and it has a private store nobody
can know what it is you have some code
doesn't matter and you have an API and
this is great if you can get away with
this if you can if you can separate data
in different stores so that every every
my cells really has its own data and
what I have seen over and over again is
that conceptual different services do
need the same data only just not in the
exact same way but they do need it and
it's it's hard to to really store it
somewhere so if I put it in a bit of a
more concrete situation so I have some
some classic applications sequel
databases some code we have some
analytics tool to I don't know maybe
some dashboard or something and we have
a regular UI so conceptually it would
make sense to split those two in two
separate Microsoft so we have our
cursors and we have our analytic service
so that would look something like this
right so
we we now we are free to choose
analytics database that make sense and
we have the dashboard we have separate
services great and it's also I mean it
makes also sense because if you do
analytics it might be happy queries and
you don't really want to disturb the
sequel databases there but this part how
is this supposed to work I mean if it
depends of course on the exact use case
but generally if you have analytics it
really kind of wants all the data it
doesn't really want to store everything
or maybe just wants to do some
aggregation on it but it needs a lot of
it so it's hard to make an efficient API
that will not simply just copy the
entire database because that would
completely defeat the purpose here so
for that we have a thing called
offensive and micro services and that's
basically what we are building now
services they push offense onto a cue
and in this case they do that by simply
making changes in the database and let
change capture do that and if you are
interested in a certain piece of data
you can you can listen to that to
certain topics and you can you can
materialize your own database if you
want so usually has some kind of pops up
Kafka kind of thing so it looks like
this we have our application sequel
databases like where before you make the
changes and analytics just listen to the
changes and they populate their own
analytics database and that's a much
nicer way to to integrate also because
if you suffer some reason you mess up
your analytics database in if you use
things like Kafka you can ask after to
resend all the old data so you can
rewind basically and see all the old
stuff again so if you mess up your
database here you analytic database you
okay I'll throw the way I rebuild it and
the core service will never will never
even notice because you don't need to
ask it that service you can ask it in
the offenders so that makes it a lot
easier to develop and it makes a lot
safer to experiment event sourcing so if
if you if you are a very brave man or
woman you can take that system we built
the one before and you can actually and
there was not a good idea you can
actually kind of remove the source
database because if you capture all the
changes you can materialize any database
in one down the stream but the original
one isn't really necessary anymore
conceptually that does mean that your
real important source of truth is now
Kafka it's a big of a bit of a question
if you should want to do that but it is
an it is an interesting way of thinking
I think I personally have no no clue to
no plans to try it but it's a I think it
helps you it helps you thinking that if
you think purely in terms of changes and
not so much stateful data because you
can do a lot with that and even if you
leave that original database in place of
what I think if you have an old system
like pretty much everybody will do and
should do in my opinion it does open up
it gives you a lot of options so what
because now now we have this system we
can do fun things with it
because we have a stream of every
changes every change we can listen to it
and materialize other the database
without anybody else really noticing so
the risk is very low so I'll have give
some examples for things we can do for
example elasticsearch everybody knows
lastic service right yeah so it's it's
it's like a search in the indexer so if
you take a big data model with all kinds
tables and everything and you dump it
into elasticsearch you can offload a lot
of the search loads from your source
database because for example if you if I
have a sequel database and I want to
find somebody and I know a first name
and the city or something that's not
something that sequel database
particularly like that kind of queries
and in order to do all of those you need
to have a lot of indexes indexes slow
down your write throughput so if you can
make put the searching somewhere else it
can offload your database a lot and then
you basically put the searching in the
cheaper place because if your search is
behind a few seconds nobody will really
go you don't really need to bother your
transactional database with this also I
have seen that more than modern users
kind of expect Google anywhere so if you
have an application and you have some
search thing you just want to type
something in press enter and the thing
knows what it is what you mean and
something like sequel is just not really
suitable for that because it really
expects you to me know what you are
asking for exactly so another one neo4j
any graphics that's here there is
yeah so graph databases are really
interesting things it's I think if you
would use it as your central database in
the big application it will make you
pretty unhappy but the way of thinking
is really is really amazing and there
are some things that are completely
horrible in pretty much any other
database the query if you because it's a
graph database so if you visualize your
data as a graph and essentially the
difference between the graph database
and a relational database is that in the
relational database the relations
between tables in a graph database it's
between rows nodes so some things that
are really horribly horribly complicated
to do in sequel and pretty much any
other database are so so elegant in
neo4j and if you can pretty easily
materialize a graph database in some
corner of your application why wouldn't
you and its really simplifies some
problems and even yeah even just the
fact that you introduced a new way of
looking at your existing data and it's a
lot so um another example firebase real
time database who knows it more a
handful
so essentially firebase is like is by
Google it's a it's a it's like a big
JSON document in the sky you can yeah
it's hierarchical so you can change
certain nodes it and not only can change
your notes you can register for updates
on a certain node so there are some
really nice SDKs for web and for mobile
that you can say I want to
displayed the value of this little piece
of the JSON document on my web page or
on my mobile app and when it changes it
will handle all the updating so it
actually does it does a lot and it it
makes for example publishing things like
for us it's interesting for if if
there's some interested in scores or
something you want to put it on some
scoreboard or next to the field and you
just listen to the updates in firebase
and then we don't even get the traffic
anymore and if you would do it in a more
traditional way you would have to pull
it or something also we can kind of
loosen how we define what the database
is and also use this system to push data
to caches so we can because because we
know when something changes we can just
use that instead of de caches and and
beside that we can basically cache
forever
until we know there is a change and that
makes it very efficient also to flush
even further we can actually push data
to clients it's pretty much like the
real time database but sort of homemade
but it's so much more powerful way of
using data that you don't have to query
it all the time you just push the
changes so so my point isn't is somehow
that that databases are better as a team
used to be a time that it was completely
not feasible to have multiple databases
and that it was so much the overhead was
so much more than it would gain but I
think that times over I think it's a
it's easier now to use several databases
each exactly what it's good at instead
of one big one and try to make it do
everything that that's just getting
painful and there are so much
possibility we have now - yeah we have
so much more possibilities and also I
think it's it's it's very good for your
development because you can easily use
new database and and it really it helps
you thinking about data in a new way and
if you just keep plotting on on your old
database you kind of stuck so I want to
try a little bit of a demo it's probably
gonna fail so it's not very visual so
I'll start with explain what I'm doing
here I have balanced some containers on
top of my macbook so it's a Postgres
database we have zookeeper crafter needs
that we have Kafka we have Kafka connect
with the BCM that takes the changes from
Postgres and puts them into Kafka we
have crafted streams that will read the
topics and reformat them and we have a
MongoDB as well so we have a little bit
of a demo database nothing particularly
interesting here but the ones we'll be
looking for at least customers there and
and addresses
whichever one-to-many relation so this
is the data model in in in Postgres
and we want it to be something like this
in MongoDB so we can do it in a single
single document so let me try for a
moment
alright let's see this works I hope
everything's up the most important part
is that the IP address doesn't didn't
change since I set up and that's
something we'll find out I guess
so this is the data model I'll change
this piece of data from Alabama to Texas
or something and here we see we are
listening to the to the update queuing
Kafka so that the change was picked up
by the bees IAM and put into CAFTA i
know it's not like a visually dazzling
but it's if you fought enough for it
it's it's pretty pretty miraculous
another one so after that the the the
streams processing kicks in and that
listens to the topic and will join it
and make the join topic so I will try to
see if that still works I changed
something again
and there we are and now it's a bit odd
format but you see that you have the
primary message and you have a sub
message there that contains the address
with Iowa in it and so this you can
easily insert into MongoDB and then you
are pretty much in business for this
small case
yeah so that's based what happened
Postgres Buddhism basically pulls them
from Postgres puts them into Kafka takes
it through streams to join them and
Kafka connect also picks up the changes
from Kafka and insert them into MongoDB
this is like I think one of the worst
cliches in our business it's everywhere
it's right up there with all we don't
want to reinvent the wheel it's but I
think what's worse with this one is that
in my opinion it's used wrong it's used
usually when oh if you haven't some
shiny new hip toy you want to do
everything with it and that's of course
a bad idea but I think there is a deeper
point here that if you are holding a
hammer that hammer influences your
thinking the tool you're using
influences how you are thinking and that
makes it pretty scary because usually
you think that you are the one in charge
if you're holding a hammer but maybe
that's not completely the case and so if
all you have is sequel and I know a lot
of people like that they are complete
Wizards with sequel and they can make
anything work and that's true and that
will continue to be true but that's kind
of beside the point
we are smart people we can make anything
work doesn't make it a good idea so I I
would like to invite you to try to -
yeah so you will always think relational
so you always believe there's something
else so I would like to invite you to
self-examine a little bit how much our
tools are influencing our city our own
thinking because there might be a lot
more out there and I think today I have
shown some ways to with relatively low
impact because we can
ways to add to to expand our thinking
because as I said implementing this it's
relatively low impact because your whole
original stack is still there it doesn't
didn't change the only thing you
connected is the division tool to your
database and you catch the changes and
then you can start playing with it so I
would like to invite to be a bit
open-minded to to examine new things
because even if you don't really end up
using it it will make you a more
interesting person I think so with that
that's all I have questions right there
no no we don't now that that would be
really bad idea
I tried now for we are moving as well as
this we are moving from Oracle some
parts to
Postgres not all of it but some of it
and also at the place where we couldn't
get away with it we use triggers to
insert it straight into Kafka and bypass
the BCM but yeah Oracle is is pretty
sketchy at this point still but they're
working on it and I have no doubt that
in a few months or years it will yeah
you never know we've all been there it
will be very useful good point anything
else oh there
well I mean I'm her lawyer
so I I don't know what counts as
removing if I remove data from for my
database it will appropriates through
Kafka Kafka will compact the data and
essentially remove it from Kafka it
might take a long time before it
actually does that so I don't know if
that's good enough but yeah I do see
your point in the sense that the more
places you put your data
the bigger the issue is going to be and
yeah I don't really have an answer for
that I don't know anything else right
there
well our change load is not that
enormous it's it speaks at maybe five
six hundred changes per second so that
that's that's not so that wasn't really
all that excited exciting for us but we
have a lot of reads we have a lot of
reads so yeah so so we did not expect
any problem but even though I mean if we
do big things like you just change 1
million rows at the same time it handles
fine I mean it will lag behind a little
bit for a while and then it will catch
up so if you are not critical in the
sense that your data needs to be
up-to-date in one or two seconds then
you can basically do everything you want
right there yeah so so usually if you
mess up your your your read only
database then you can start that from
Kafka so so Kafka Oh has the relevant
data it does something called compaction
so it will if you update the same record
over and over again it might delete the
older ones because it's not critical for
rebuilding it but you basically say tell
it to reload Kafka and do over maybe you
need to do the offense processing
there's a stream person as well that
depends on your case what exactly went
wrong but if it's just your database you
just need to replay the the assemble
code assemble data and then you're in
business and that's among going to be
that's really quick anything else
then thank you and see you wrap</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>