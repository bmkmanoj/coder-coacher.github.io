<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks: Walkthrough by Katharine Beaumont | Coder Coacher - Coaching Coders</title><meta content="Neural Networks: Walkthrough by Katharine Beaumont - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Neural Networks: Walkthrough by Katharine Beaumont</b></h2><h5 class="post__date">2017-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8Sd6KOm9vcM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone so this isn't
intimidating at all so I'm sure we'll be
fine it'll be fine
and so we're looking at neural networks
and this is a very simple introduction
so if any of the math skits intimidating
just throw things at me it's not
supposed to and if you find that you're
not stimulated enough then I'm sorry
okay so I was staying with my friend
last night lives in Brussels and she
asked that question that everybody
dreads
she's a linguist she said you know she
didn't say what do you do she said what
are you speaking about tomorrow and I
thought oh crap how am I gonna explain
this there are these neurons and they
worked together and they stuff happens
you put in pictures of cats and a word
cat comes out something like that I
don't know so I thought I'm gonna have
to come up with some way to explain this
to people and then I'll understand it
myself better so your networks feel a
bit like magic
and hopefully at the end of this they
won't hopefully by the end of this
you'll think there's just really boring
function machines and then you'll have
got it okay so does anyone know I'm
gonna ask questions so just you've been
warned does anyone know what kind of
things we're trying to work out with
neural networks one of you must know
what kind of thing we're trying to work
out neural networks you smiled at me so
you must know here in the middle what
are we trying to work out
yeah I mean I have to start be there
because you're giving my talk it was
very it was very good answer it was one
of the things we're trying to work out
its classification so as you very
quickly said we get series of inputs
into the network and we're trying to
work out is this from Group one or group
two so that's one thing that we're
trying to work out you could feed in
pictures of you know like when you
you're saying I am NOT a robot and
you're doing a capture and you have to
click all of the pictures with cars in
and say that's the car that's car that's
the lamppost that's a lamppost so one
thing that neural networks do is try and
classify information clicker and and the
other thing is so classification I think
has been very well well very well
explained so one classic example is a
data set data set called the M NIST data
set which the data set of handwritten
digits and one it's it's a data set
that's used in a lot of neural network
competitions actually and it's trying to
with a new input workout which digit
someone who's written and we'll have a
look at that again later
and another thing we're trying to work
out is regression which is not as
friendly a word
I don't think and regression is where
you're trying to work out a continuous
output so it could be at the price
perhaps for example so again with this
this friend Beth if I showed her a
simple example of a neural network
working the first place that probably
goes the tents around playground I don't
work for Google but this is brilliance
but then the problem is was this was
learning rate
what's activation was regularization was
what's the problem type well we just did
that so you should know know and what's
the output what what is in Iran what is
the hidden layer and it goes on like
that so hopefully we'll answer these
we'll cover these in this talk
in terms of the structure because this
is a deep dive and it's a long session I
want to break it down a bit and tell you
what we're going to be looking at so in
the first part we're going to look at
neurons and there's going to be a
description of a biological neuron that
would offend a cognitive scientist so
I'm sorry in advance and then we're
going to look at artificial neurons and
we're going to go over some of the basic
types in the second part we're actually
going to look at learning so hi know
what Sloan and we're going to go over
the basics like cost functions
feed-forward gradient descent that kind
of thing in the final part then we'll
get on to neural network architectures
and a bit more and in between I thought
it might be nice if you had coffee
breaks so I'll try and time it like that
so I'll try and do 45 minutes
20 minute break 45 minutes we'll try and
make it easy on all of us okay so let's
get started with neurons
what is a neuron this is where we are oh
so I'm sorry I do have notes for this
part because I am NOT a neuro
neuroscientist so well one thing I can
tell you that that note says neurons are
the fundamental unit of the brain or
that's a theory they receive person and
process inputs from the world and they
send outputs and commands to muscles now
does anybody know how many neurons are
in the human brain five okay 300,000
let's go I know Jim knows the answer so
I'm just gonna ignore him it's not five
anyone go higher a billion we have a
billion any higher then I'm gonna lower
than trillions any higher than a billion
sixty brilliant bit higher oh well that
is a very common number that goes around
on the internet about the number of
neurons in the brain actually and
actually there well it's disputed as
everything is in science but
interestingly and slightly worrying if
you're male this doctor does pronounce
this wrong so I'm really sorry
susannah Hercule no who's L melted down
for adult male brains they were dead
first I should say and she melted them
into brain soup
I did actually google images of brain
soup and I stopped quite quickly and
then she counted a sample of them so
obviously you don't want to count up to
86 billion because you'd probably die
before you got there or you would I'm
not going to work that out but she
estimated that it was around 86 billion
and that's interesting in terms of the
internet number that goes around which
is 100 billion because 14 billion unit
14 billion neurons is actually the
number of neurons in a baboon brain so
that's quite a big discrepancy we're
missing a lot there so let's take a look
at this hmm
back to my notes so signal outputs from
one neuron can excite or inhibit other
neurons so if a threshold in the neuron
said that top left thing there with the
green circle in the middle if a certain
threshold is exceeded then it fires if
the threshold is exceeded it fires more
frequently so if it gets really excited
it lots and lots of fires and it's a
little bit excited it might not fire at
all might as fire in less frequently so
when it fires an electric pulse travels
down the axon and these axon terminals
and sorry just got bit distracted by the
noise there and new neurotransmitters so
they these neurotransmitters travel
across this synaptic cleft that space
there and they're going to bind into the
receptor molecules and then they go into
the next neuron and that gets excited or
inhibited so we have these signals going
from one neuron to the next and the more
excited and urine is the more often it
fires but the really interesting and
crucial thing about this and why I'm
talking about it is the connections
between the neurons are adaptive so
these synapse is
can send more chemicals and they can
receive more so they can change their
connections so we have these neurons
which are apparently the fundamental
units of the brain or are they
we'll get to that and they work together
in a modular brain so there are some
really interesting studies where people
who lose one sense like the sense of
sight can maybe when I say people I
think these studies are actually done on
mice
just to clarify it can relearn that
ability in other parts of the brain and
the really important thing to remember
is these adaptive connections so now I'm
going to look at Linda now because
you're from you're from around here I
think the correct term is quartets the
card game is that right okay so just
think of this as like that we're going
to collect these cards so got biological
neurons it's not that difficult to
understand if you take a really
simplistic view and they're the
fundamental units of the brain but are
they are they the fundamental units of
the brain it's always important in any
kind of scientific discourse to examine
where ideas came from and maybe look at
the other side so we're just going to
take a little look so these two men won
the Nobel Prize in 1906 I cannot
pronounce their surnames I'm really
sorry but I'm going to try and just if
you're familiar with the crêpes
notation just winced when I do it so
they hated each other they completely
hated each other so if I remembered it
correctly
Gogi or goggly or something
Golgi thank you thank you ecology
invented a staining technique using
silver nitrate to be able for the first
time to see neurons inside the brain and
Corral okay Amani Kyle I said that's my
very yorkshire vision I could go more
more yorkshire Romano and Kajal that
I'll try not to at home also looked at
the attorney and he proposed that the
smallest units look at was the
individual neuron and Gordie
thought otherwise but rahman iike his
view of the individual neuron went out
and he won't be good giving this talk at
devil it's because someone would be able
to correct me his view are now that the
correct units look at was individual
neuron so what happened since then so in
the meantime McCulloch and Pitt's um
wrote this paper a logical calculus that
is imminent in nervous activity so the
ideas that actually more correctly
attributed to McCulloch but they noticed
something really important about neurons
a really important prophecy which is
that they either fire or they don't
further on or they're off so one or zero
so binary so logics they made a link
between neurons and logic and then Frank
Rosenblatt did some early work on
perceptions which is an early learning
procedure using binary threshold neurons
which also con McCulloch and Pitt's
noorans perceptrons really excited
people for a while there's a lot of
research a lot of funding going around
into research and the kind of things
people could learn from perceptions but
we're not going to talk about
perceptrons because then unfortunately a
few years later
Minsky one of the heroes of AI and
puppet wrote this book called
perceptions that basically excuse my
language shat all over perceptions and
just said these are rubbish that the
here well not completely rubbish they
still use a lot but there are some
really serious limitations and so for
awhile things kind of dwindled and it
wasn't it quite so excited anymore and I
think if my history is right and things
like distributed AI took off instead of
neuro networks at that point in time so
then in 1986 a theory that had already
been kicking around and was praised by a
few different people the same time made
it into the journal I think it was the
journal Nature with Geoffrey Hinton Rome
AHA and Williams about backpropagation
nice really really important and we're
going to come and little talk about that
again we're gonna talk about a lot
actually so if they want and that was
really exciting because now back
propagation was an idea to efficiently
learn in networks so suddenly we have an
algorithm that speeds up learning and
with speeding up learning and then
greater computational capacity suddenly
we have an explosion of interest in
neural networks everyone's really
excited about neural networks but not
everybody agrees that this is how we
should be looking at things that we
should be modeling the brain that way
that I should be giving this talk
talking about artificial neural networks
and linking them to biological neurons
so but if it isn't an idea it has been
historically pervasive and it's now
deeply embedded in the way that we think
about neural networks so that was the
neuron doctrine the idea that the
fundamental unit is one neuron so I've
kind of explained why everyone is
excited about it but I wanted to talk
about the two different main branches of
research there's one really interesting
bunch of research it's this is Henry
Markram famous scientist University of
Lausanne working on the Blue Brain
Project and so that is using neural
networks artificial neural networks to
better understand the biological no
neural networks in the brain so that's
using neural networks to understand the
brain and then the other branch of
research is using neural networks to
understand better a style of parallel
computation and that's the one we're
looking at we're looking at the one is
divorced from biology and what we're
trying to do is we're trying to learn
we're trying to train neural networks to
learn things that we find so easy we
don't even think about them so computers
are really good at some things so first
person to answer this question wins t
shirt anyone know yeah it's a lot that's
the right answer and then can anyone and
there's no prizes for this I'm sorry but
can anyone shout out what kind of animal
this is yes I mean I expected more speed
and more volume from the rest of you but
may
you've never seen one before I don't
know so so that's what we're trying to
do we're trying to try to learn how we
learn but from now on just a change in
terminology I'm gonna limp I'll leave
biological neurons behind going to leave
them to the experts I'm going to stop
upsetting cognitive scientists I'm gonna
go on to computers which is what we all
know and love
so this is where we are we're now at the
artificial stage so linear neurons so
we're not really thinking about networks
yet we're still thinking about these
neurons but artificial neurons so we're
thinking about what's-his-name McCulloch
and Pitt's we're thinking about them
linking a neuron firing to a 1 or a 0 so
we're looking at some input coming in
and some output going out so here we are
his map this felt like a great idea at
the time and now I'm kind of irritated
that I've put this slide in everywhere
so sorry about that
ok so let's just look at a function for
your basic function so when you have
some input here so that in the top left
hand corner in case my handwriting's a
bit rubbish that is an X that is
representing some input that could be a
single value or it could be a vector
then we've got little W on an arrow and
that means we're going to times the X by
the W and then we've got another input B
and can anyone tell me what that looks
like that equation there so I think I
heard someone say line line yep is the
equation of a straight line yeah it's
when I ask questions I though it feels
like the answer should be more
complicated doesn't it but it's the
surface all it is yes so for example
let's just get rid of the W and the B
because they kind of I don't know it's a
Monday you're taking time off work and
one really wants to look at that so this
has put some twos in say it's just a
straight line so there we go
y-intercept is 2 if I had labeled the
axes you'd see that the gradient was
roughly two and that'd be there that's
just the the bias I'm going to call it
the bias is just the y-intercept I'm
just moving the line
so I'm just going to draw it in a
slightly different way
no with some circles same arrows and
there we go again now I've got two x's
two different inputs two different w's
two different weights two things two
times than by we're going to sum them
together in the output and we've got
some neurons so that's kind of what
we're trying to do with a linear neuron
we're trying to try to mimic a neuron by
putting in inputs so much and they're
coming from other neurons we're gonna do
something to you them but just to time
some of them together we're gonna open
them so maybe they're going to be more
inputs and I'm sorry now my jokes have
kind of run dry there's nothing funny
about this slide so we've got three
different inputs they're just going to
start I'm gonna write a Java conger
contract or a Java conference so we're
gonna index from zero
we're gonna go X zero times W 0 X 1
times w 1 X 2 times W 2
what are we gonna do in the middle no
prizes for guessing sorry we're gonna
add them together and we get our
straight line so that kind of question
mark there were adding them together if
you do read the any kind of
documentation do any courses that some
you'll see it written in a few different
ways you might see the B as a separate
thing like we had on the earlier slides
or you might just see C it slightly
differently but we're doing essentially
we're doing the same thing crucially
those w's represent the synapses the
adaptive they're adaptive connections so
what we really want to know is what
should their WS b which of the synapses
be what should the gradient of the line
be what should the bias be because what
we're really trying to do is trying to
map some inputs to an output linear
function so what can I do I'm going to
show you a little demo which is I'm
hopefully going to be ok right first of
all I know I know this is Python ok
it's just that I'm not going to you'd
only turn this on the code all you need
to know is that I'm just making a little
dataset we're in between 0 &amp;amp; 1 20
different points and I've got some
output here which is I'm just going to
add two times it by 3 and add some
random noise and then we're just going
to look at the graph okay so there's the
data and there's this line here so what
we really want to do is we want to get
that line to go through the data so if
anyone is feeling particularly kind of
generous to me could they guess what
they think the bias should be okay let's
try 2.6 and that's run again okay yeah
that that looks well it's good okay so
now let's try changing the gradient any
guesses I had a few different numbers
there I had two and five and three so
I'm going to a virgin three it's not it
doesn't average don't don't work it out
okay yeah that's good okay excellent
nice work nice work everyone so there we
go that's linear neuron that's my
husband George hi George right you
answer those questions excellent thank
you very much so linear neurons so we
saw them great so now it's if linear
neurons give a linear output didn't
particularly surprising sometimes called
the identity out activation function for
that reason so if anyone has their
laptops on them let's just quickly go to
tensorflow
and I didn't load that up because that
would have been really organized sorry
and let's just make that activation you
know they're linear so these data points
here this is a classification problem
we're trying to work out if a new input
if a new example that we haven't seen
before
should belong to the blue dots or the
orange dots so don't worry too much
though what's happening now
well we'll work through all of this
throughout this time but the thing I
just want to show you is that if you
make the activation unit linear that
we're not going to be able to separate
this data and this will run for a long
time so you're going to have to take my
word for it or open up your laptop's and
dry okay
I'd leave that running welcome back to
him that's what we want that's what I
want to get so that was a linear neuron
a little bit of maths I hope it was fine
and I'm saying that but most of you are
probably much better at maths than me so
don't feel too patronized the output
some of the inputs times the weights we
have a linear output awesome
so now binary threshold neurons touch
map McCulloch and Pitt's talked about
no one's been on/off firing or not
firing here is the map you know that
you're getting closer to coffee so it's
good and I want to go back to this
picture in your own again also I just
liked it I liked it as a picture and
here we have inputs going in we've got
our sum in the middle so what's missing
we want to activate it when that
activation of the neuron we want it to
fire want something tapping so ah here
we go some more maths so the binary
threshold neuron says okay here for the
sum of all the inputs times the weights
if it's greater than threshold I'm going
to fire one and if it's less than
threshold I'm gonna fire zero so we're
just fire it's just one zero it's one or
the other it's not that exciting so I
didn't do a demo on it sorry and but one
thing if you if you feel like it and if
you feel like doing is programming try
and solve for X or problems everyone
know what the xor xor is yeah so it's
fine and try and solve it with binary
threshold neurons and just as the
problem is without adding any extra
dimensions and I've kind of given the
answer there by writing why not haven't
I said it doesn't work
don't try it you're wasting time okay so
that was the binary threshold neuron ray
logistic neurons now this gets more
interesting so again we have our inputs
we want to do some kind of activation so
what we're going to do is going to apply
this function if you're taking notes
from this don't don't draw that graph
I'll show you a better picture of the
graph because it's it should look like
an S and it's not very good but I will
show you a better picture of that so
we're applying this this function to the
inputs and what that function basically
does and what the graph is going to tell
you is that it's squashing the out but
between 0 and 1 but the output can form
on this nonlinear Y nonlinear
non-straight Y so let's have a quick
look and again let's hope this all works
they're just going to show you a better
picture of the graph first because I'm
ashamed of my other graph so that red
line there that is the sigmoid function
if anyone is interested and if anyone's
kind of feeling bit under stimulated
right now the blue line is the
derivative and you can work out the
derivative by hand if you feel like so
let's just take a quick look at what the
sigmoid no one can do again
this is Python don't worry too much
about it I'm sorry this is a Java
conference and I'm doing this in Python
though we'll address this in a later
part to talk when inevitably there'll be
a question about programming languages
so I have some input data again this
time it's between minus 3 and 3 it's no
particular significance to that it's
just input data 100 data points I'm
going to try and map input to the output
using the sigmoid function ok so after
one iteration it's not that great what
I'm actually doing is running this
through a neural network after five
iterations still not great 50 situations
good okay it's learn to map the function
if it was a linear threshold yarn and
you'll have to take my sorry if it was a
linear neuron and you had to take my
word on this it would not be able to map
the inputs to that output the reason it
can do this is because of that s-shaped
curve so what we can do is we can change
different logistic neurons together to
get a more complicated output from a
functional point of view so a mapping
the function well there is even better
cool right
so that's realistic neuron real-valued
output between 0 &amp;amp; 1 we can map
nonlinear functions yes ok we're getting
closer to pictures of cats so go back to
tensorflow
let's see how that tensorflow thing is
doing actually let's just check on that
ok oh we're still going with the
Linnaean linear neuron so we'll just
kill that it's probably had enough now
but let's try with sigmoid
it's quite fun thing to watch if you're
bored okay great so what this picture
represents is we have the input features
and the input features are the
x-coordinates so up or down we have
these data points they lie in different
places in this space some of them belong
to the orange dataset some of them
belong to the blue dataset we've asked
the network to try and work out if a new
data point comes in where should it be
should it be in the blue range or the
orange range and we will talk more about
this okay
so what kind of problems yeah so no this
is question for you guys what kind of
problems can a logistic neuron solve
does anyone know I'm gonna start making
eye contact with people and picking on
them so does anyone feel like
volunteering as tribute yeah yeah thank
you maybe detecting smoke rates above a
city weather smoke smog sorry whether it
is higher or lower yet so we can work
out classification problems with a
logistic neurons which we couldn't
really do very well with linear neurons
because we're finding a linear function
so we've got a bit more versatility so
that was the magista clear on maths bit
more difficult especially if you look
into the actual function you try and
work out the derivative it's a bit more
difficult wait to find linear neurons
now these are really really popular at
the moon so all the cool kids using
which file in you yes so again we're
going to activate it activate our inputs
we're going to go to output the output
looks like this so what we're saying is
when you sum up all the inputs and the
output is going to be 0 if it inputs
less than 0 otherwise it's going to be
this linear output
and what can it do okay let's take a
quick look so here's what the function
looks like in a better version of the
graph because it was drawn by a computer
or not by me and that is the derivative
if anyone is interested derivatives will
become important later but you don't
actually need to remember them for the
purpose of this talk so now let's look
at a really simple network again let's
show you it and then I'll talk through
it I've got these blue dots are the
output so I've just made a function
anyone who reads the small print can see
that it's the sine function and ever
asked the neuro network to see if it can
work out that it is the sine function so
so I mean what I've done is I've fed in
some data and I've given it the answers
and the answers correspond to the sine
function and the network then has to
work that out for itself so the
rectified linear units are very fast and
very efficient you're working things out
there is another oh yes so I'll just do
my summary so before I move on so you
can go to tense flow again and try the
rectified linear unit is the activation
function and compare the speed of
sigmoid let's actually just do that now
because I think we we watch there's a
sigmoid function took shape we're
keeping everything else exactly the same
let's try it rectified linear neuron so
fast there's no no time even to relax
during that demo I just have to move on
to the next thing so okay we have the
rectified linear neurons slightly easier
with maths
I think tan H Thunder that's hyperbolic
tangent so that is the last one that
we're going to look up before your
coffee break again we have our inputs
and the output looks surprisingly like
the sigmoid function and that's because
it pretty much is it's just stretched
and let's take a look at what it can do
actually let's not let's just look at
the comparison between the two so right
blue line is the tan function tan H
sorry the hyperbolic tangent the red
line is the sigmoid function and they're
the derivatives look pretty similar as
well so it has a bit more versatility
than the sigmoid function and I'm not
going to go into that but if you're a
mathematician and you'll probably enjoy
knowing that it helps to Center the data
because it can fall between minus 1 and
1
rather than 0 and 1 okay so let's go
back to tensorflow let's take a look and
compare that to the sigmoid function so
I'm not going to do sigmoid again I
think we all remember that it was slow
here we go
very that's fast
sad sad sigmoid so it's redundant
sorry so there we go that's the
hyperbolic tangent neuron output between
minus 1 and 1 and that's that that's the
first section so we've looked at
biological neurons fundamental units of
the brain working together in a modular
brain we have looked at the neuron
doctrine the idea that to understand the
brain the smallest unit is the neuron we
have linear neurons are easy friends so
just really simple mapping identity
function without doing any kind of
activation we're just feeding it through
binary threshold neurons logistic neuron
is now that now we can map nonlinear
functions awesome we've got the
hyperbolic tangent neurons and we've got
the kind of the favorite though
everyone's favorite child rectified
linear neurons and that is I'll just
wait for you to take your photograph my
good friend at the front there okay so
we're not any close to working out
what's the picture I've kept this
picture of dog so I guess what we'll
have to do is probably have a cup of
coffee and come back and try and work
that out so the time is four minutes
past two if we come back for 25 plus two
I'll get started again thank you
you
just goes to show doesn't it you can
have all the degrees and you can just
not turn something on and not that I do
maybe I need a few more and then I'll
get it right so we talked about the
individual neurons and we looked at
tensorflow
and I'm very aware that a lot of the
things that on the tense flow that we
saw like the learning rate and the
output and the input data and the
features we haven't really covered those
yet but now we've got the basics and
we've got neurons we're going to I'm
going to move on to that so this is
learning in a simple network we're going
to start with some easy examples we're
in part two here we are we're going to
first of all look at a simple network
and this one has an input layer and
output layer and one hidden layer we're
gonna call it a hidden layer in the
middle so does anybody know how we would
count that how many layers it has
someone would shout out how many layers
it has one it feels like it should be
one it's actually two so by convention
we count this as two because we we don't
count the input layer but we do count
the output layer so the first one the
input layer is all of the data and now
this word features comes in does anyone
know what a feature is in this context
so if we if we have we're trying to work
out the price of a house what might a
feature be what might contribute to the
price of a house yeah I don't know how
you quantify that yeah yeah maybe you
could put in the coordinates of the
house I saw a hand raised
yeah the size yes the classic house
price if you do any kind of basic
introduction to machine learning you'll
always come across the house for example
guaranteed and if you don't have be very
surprised so an idea is I have the size
in square feet or square meters of a
house and I want to know the output I
want to know the price know what the
correlation is
and then again you might have an input
for location you might have an input for
whether or not there's a garage you
might have an input for again I don't
know how proximity to schools something
like that and each one of these would be
a feature if you were looking at a
picture of a cat what might an input
feature be it's not the answer isn't the
whole picture by the way okay I'll tell
ya well that would be something you're
trying to discover but the input would
actually be it's a bit of it's a mean
question sorry the input would be a
single pixel maybe and the
red-green-blue value of each pixel so we
need to find a way to put the data in
which is something I'll come to in part
three and we will find tails and the
hidden layer that there are a lot of if
you google why is the hidden layer
called the hidden layer there are a lot
of answers to this and some of the
answers are well because we you know we
have the training data all of the
different information about a house for
example and we have the output which is
the price and we don't tell the hidden
air watch dude and I'm sure I've given
that answer before but then I was
watching a talk with Geoffrey Hinton
he has been called The Godfather of deep
learning and he's one of the first
people to coin the term hidden layer and
his answer is well we thought it sounded
cool so we have a hidden there we have
our output and that might be a single
that might be the house price or it
might be a probability or it might be a
classification described by a
probability so when we move forward to
your network what we do is we feed in
for one example so for one house all the
days we have in the house we feed in the
data to the input layer and now I drew
w's earlier to signify the way it's here
I've just drawn this Greek letter theta
and you can just think of those as
double use it doesn't really matter what
symbol we use as long as we all
understand what it represents and you'll
see it written differently in different
places but the important thing is that
it is this
concept of a weight or if we're
remembering our biological neurons
conceptually it's the adapt it's
adapting it's something that signifies
an adaptation on the connection whether
we're going to make it bigger by having
a large value or smaller with a small
value where they're going to let more
neurotransmitters through or less so
that is our synapse so now what we're
going to do is we're going to times each
one of these input values by a weight
along the connections so we might that
x1 in the middle there is going to be
times by two different weights and it's
going to go in two different directions
two different hidden units in the hidden
layer we call those hidden units so
there are two hidden units in the hidden
layer and the one at the top is a bias
and that's just to remind you again when
he think back to the linear neuron we
had that y-intercept you'll come across
biases a lot in your networks and that's
just to remind you that we might draw it
without bias later but they're still
generally added in so now we go into the
hidden layer and those colors are to
symbolize that some kind of activation
is going to take place so it might be
our tongue H might be a sigmoid might be
the regular alized linear unit which is
difficult to say or it might be linear
might just be the identity function
might not be activating at all there we
go times everything by new serve weights
am I gonna get to our output and our
output is our prediction or you might
see it called the hypothesis and if we
were feeding through information about a
house price sorry features of a house
the output is what does the network
think the house should cost so that's
what we want to know we want to give it
data and we know the answer we know how
much the house does cost if we want to
know what the network thinks because we
want to train it to be able to get to
the right answer so that is feeling
forward through a network I'm just going
to go through that again
because it's a bit dry as a topic
so we're going to move forward we're
going to start with the input data we're
going to times it by some weights we're
going to input it into hidden units and
we're going to move to the output layer
to get our hypothesis which is what the
network is predicting when we looked at
the very first neuron example the linear
neuron and remembered the be feeding in
and the X feeding in with the W and did
anyone feel that that was simple yeah
we've got some nods at the front there
so that the concept is actually simple
so it's actually happening here along
the connections is actually quite simple
there's just a lot of it so it looks
like a mess looks like a mess because I
drew it by hand but it looks like a mess
because there are a lot of connections
so for each individual neuron what's
coming in isn't complicated
it's just that they all connect together
in a complicated way in deep learning
that is neural networks with many hidden
layers so here we just have one hidden
layer in deep learning you might have
hundreds and then it gets very complex
from the point of view of an individual
neuron it's still just as simple as our
first example maybe with an activation
function but suddenly have so many of
them that can be very off-putting so oh
how many layers is this Network curve I
hit small whisper three so I'm guessing
you're not to you confident but that's
the right answer so great now here's
another one and I've done this I mean
I've used a few different symbols you
might have noticed that I've switched
them quite a lot and that's we just keep
you awake and it's all said to be mean
and it's also to introduce you to the
idea that if you do we read while read
widely then you're noticed different
people using different notation this is
y hat it's got a little hat on the Y and
that again is just the hypothesis or the
outputs the predictions it's just
different ways to represent the same
thing just like you might use the theta
the circle with the line in the middle I
might use double use for the weights and
some horrible people use beta
for the weights I don't know why they do
that so maybe maybe in this network we
might use the sigmoid activation
function so what you'd be doing is you'd
have an input layer and here to simplify
it I've just on the input layer as one
circle I haven't bothered drawing
everything that it could represent and
each neuron might get the information in
activate it with a sigmoid function and
pass it along so you just get something
and pass it along and we could if
everyone was feeling like they wanted
some fun we could get some of you to
stand here and we could pass each other
things and maybe Jimmy could be output
and we could all feed forward to you so
we could have two people standing here
and I don't have anything to pass that's
where this doesn't work otherwise I
would have been mean and made he do it
so I just want to do a little example
again and IVs double use because I was
feeling like it when I do this
particular picture and just to be nice
so what's going to happen in this what's
going to happen for the input in this
neuron does anyone know what can I make
a guess it's one of those questions
where it's actually the answer is
surprisingly simple it's just the
question feels like it's going to be
difficult so the answer is that we're
just gonna add in oh no that's not the
right answer
ignore that slide let's look at this
instead just ignore that just burn it
from your memory okay
in this side what we're doing is that
top left neuron there I'm going to call
that a 1 I'm gonna index from 1 the next
one down is gonna be a two the next one
down is going to be a three next one's
gonna be a four
any guesses for the next one yeah a five
yeah I'll stop asking stupid questions
I'm sorry so so what we're going to do
to get the input the input to that
neuron is we're going to times the top
one by that weight W to one
it's cod - I've just caught it W - to
say these weights belong to that group
they're the second set
weights then we're gonna times the
second one down by the next weight the
third one down by the next weight so
this is just the first example wrote the
first linear neuron we're just feeding
stuff in and then we're going to
activate it with the sigmoid function
here so it eats know and that's really
what's happening it's just like I said
when you have hundreds of hidden units
and hidden layers it scales up and it
becomes very difficult to see what an
individual neuron is doing so I'm going
to show you an example
it's another Python example but the code
isn't important it could be in Java just
attend because we're a Java conference
that it's in Java and so again I've got
100 data points I've randomly
distributed them along a function so if
I've said I want 100 data points and I'm
going to put you through you I can't
remember by you sine or cosine here and
then just add a bit of noise just so
they don't fit perfectly on that line
does that is that making sense so far
excellent I'm aware that as soon as I
show graphs like this everyone kind of
goes oh god sorry let's keep going
so that out oh yeah there we go cosine
functions the output is a function of
added some random noise just to not make
it too easy and now because we've just
looked at that picture of the network
with the five hidden units in each
hidden layer I've actually just built a
little example so the network's got
these two hidden layers they've each got
five hidden units I'm going to use the
logistic activation function and we're
just going to see what happens with a
network and if you're feeling a little
bit lost right now what I want you to do
is to hold on to the idea that what
we're really doing is complex function
chaining we're just like you might get
the number three and times it by five
and pass it to the next function and
that might divide it by two and pass it
to the next function reading something
like that we're building it we're kind
of building maths and I will come to
caps later I promise
so
let's run this okay so after one
iteration and you may recognize this
example from the last section the
network hasn't done very well what's
happened is we've fed forward through
the network once we've randomly
initialized the weights so we've just
made them up
who said just be anything be something
different every time and we fed the data
through that network that we saw so went
from the input to the next hidden layer
to the next hidden layer to the output
and after one pass through the network
this is what it is predicted and then
I'm going to train the network and I'm
going to go onto how to train the
network so we'll come to that stage for
now it's a black box after five
iterations and see it's starting to find
its way starting to to learn and again I
will explain how that training works
next now after 50 iterations
it's tuned pretty well I think and after
a hundred they're great and if you can
see these numbers at the bottom that's
the loss and I will come to that but
what that loss means is what's the
difference between what the network has
predicted and what we know the right
answer is because we have the answers we
know what the answer should be we
plotted it on the graph and we fed the
input into the network and we said see
if you can work out what the answer
should be and the losses how wrong is
the network so it's getting less wrong
it's quite a negative way of looking at
things actually but there we go so that
was moving forward through network so
all we've really done is put in the
values to get the output so if we think
back to the linear neuron we've just put
in maybe the - 4 X - 4 the way - for the
bias and we've got 6 we've got our
answer but we've moved through lots of
layers to get there so now those cost
functions that I mentioned so
to discrepancy between the network
output and what the real answer is and
there are different ways to work out and
again when you kind of continue on your
neural network journey or if you're
already on it then you'll know or your
discover that there are different cost
functions depending on what we're trying
to work out and that comes down to the
output layer so in the example of house
prices and can anyone guess what kind of
number the output would be yeah will it
be the price that would be what we would
work out with the cost function no don't
worry it was it was one of these really
simple questions again so you feel like
the answer should be should be a lot
more complicated
the answer is I think you always forget
to do this for the microphone the answer
is what we'd get out of the net is we're
gonna get a number we're gonna get the
house price so we're feeding all of the
information what we want it to tell us
is the price even if it's wildly wrong
that's what we want we want a continuous
number from 0 to infinity and if we had
that kind of output then we'd have a
certain way of measuring the cost and
I'll just show you that now so for
people familiar with maths then you
might notice that this is the mean
squared error so it's the difference
it's like a sort of distance measure
between what the answer should be and
what the answer that the network has
given is I'm not going to go too much
into the maths but if you're interested
the symbol like that there is some so we
saw earlier it's adding everything up
and then we're taking the square
difference and we're getting an average
of that so we're saying if every
training example that I put into the
network what's the difference between
what the answer should be and what is
the answer and the answer that the
network gave me so what's the difference
if we were using a logistic function
that sigmoid function again then it
would be different
and you don't need to understand these
cost functions to use newer networks
with libraries that are available today
you don't need to understand this to use
the tensor flow playground but if you do
want to start building your own neural
networks then you do need to do some
investigation into different output
functions for the purpose of using
tensor flow and playing on the
playground just ignore this but this is
something that you will need to learn if
you want to start building your own
network so how are we going to actually
learn so one two back propagation so I
just want to start with an example each
of those blue lines represents a weight
as you can see we have many different
weights in that network I actually know
how many there there are but can anyone
tell me you could be you could say
anything yeah
many maybe yeah maybe let's say 35
anyone watching the YouTube video can
pause in the count and send me hate on
Twitter so each of those blue lines is a
weight each of those blue lines needs to
be trained because each of them is a
synapse in our brain and again here
using brain in the loosest possible term
so what we need to do to train the
network is to get the weights right
because we the weights are what is going
to the weights are these adaptive
connections so we need them to adapt
with the data coming in to get the
output
so for back propagation what we're
trying to do is we're trying to train
these weights so just going to look at
the output there the white hat the
prediction that the network's made for
if we change this weight here the second
one down in the first layer the effect
that that is going to have isn't just on
that one neuron
is also going to affect every neuron
that it then connects to in the next
layer and it's eventually going to
connect the output so every weight has
an impact on the cost of the network the
total error of the network and so what
there's an algorithm called gradient
descent and what we do with back
propagation is we start with the very
final layer of the network this is where
it does get very maths heavy and I
haven't included on these slides and we
work out at every layer how the weights
contribute the cost of the network
because what we want is for every single
weight we want to know how it affects
the output because we want to be able to
change every single weight and we want
to change it in such a way the total
cost of the network how wrong it is
decreases does that make sense so far
no one just took the head so I'm going
to take it as though you know this yes
so having moved forward through the
network and for every training example
worked out what the cost is and how
wrong it is now there every layer moving
backwards we need to work out how the
weights are affecting the cost and if
you want to understand how that works
when people are stealing maths for
machine learning and you need mass when
your networks not if you understand the
feeling or the intuition of how it works
but if you do want to work through this
you do need calculus and you do need
partial differentiation but if you
understand what I'm talking about and
you don't need to go and verify it with
pen and paper then be happy that
propagation is a notoriously difficult
thing to get your head around and if you
do and you English course on Coursera
the machine learning course not the deep
learning AI course he very kindly says
don't worry it took me a long time to
get back propagation it took me a long
time to get propagation every time I
feel like I have a grasp on it and
especially the mathematics
it slips away so it's one of those
things like you don't need to know every
library in Java as long as you know how
to find it
you don't know need to know how to work
out backpropagation but you need to
understand this concept and moving
forward through the network to get the
answers and get the cost and the moving
backwards through the network to work
out why the cost is wrong why how are
the weights contributing to this cost or
this error how can we change each of
those weights to reduce the cost because
what we really want is we want a really
small difference between what the
network says the answers should be and
the real answer in a perfect world that
would be zero but so I mentioned
gradient descent and we had this nice
picture here this bowl shape so imagine
one weight and the cost what we want to
do is we want to get to the bottom of
this bowl that's the cost of the C there
we want to get to a place where the cost
is smallest for that one particular
weight so that looks easy when we did
our example in Part one and we were
shouting out what the gradient could be
and what the bias could be we were
trying to get a point where the line fit
the data in the best possible way that
it could in that example it was convex
there's going to be a point where it is
minimal this is going to be a point
there's going to be a guess for the
gradient and the bias we had the lowest
possible cost function so in one
dimension that's easy just need find the
way to the bottom the ball when the
problem is with neural network some of
the complexities of which there are many
is well if you have a three-dimensional
landscape okay find that slightly more
difficult when you sketch the bottom the
right Valley so we need to walk across
this hilly landscape and we need to find
the perfect valley the lowest possible
Valley and that's what I want to go
that's the the minima that we're looking
for but the problem is in the example
with with the we've made up or someone
with a Quitely worked out the number 35
weights now we have 35 dimensions
that's nightmare so we need a way of
getting to the bottom of that
so backpropagation maths is very
difficult the concepts very difficult if
you feel a bit lost I'm really sorry it
wasn't my intention it's very difficult
subjects so but that's about as
difficult as it gets in this talk so
training before I get on to gradient
descent I just want to mention the
training I've mentioned these before I
just want to go over them again I talked
about randomly initializing the weights
the reason that we randomly initialize
the weights it's because we don't know
what they should be so if we set the
weights if we kind of predetermined what
they were we could introduce bias into
the network there no biases right we're
in this contest so we want to randomly
initialize the weights because we don't
want to accidentally send it in the
wrong direction in this hilly landscape
I want to give it a fighting chance so
per example we went forwards through the
network so imagine we've got 50 houses
we've got information for 50 houses for
each one house we want to feed in the
information we want to work out the
difference between what the answer
should be and what the network told us
and then we want to move backwards in
this back propagation step and then
we're going to run gradient descent
notice what I'm going to talk about next
so gradient descent right I'm going to
show you a little example is anyone here
familiar with linear regression oh cool
okay so this will make total sense to
you so here's a little JavaFX
application and we've got some data
we're going to pretend that it
represents the size in square feet of
houses in the price and we're going to
look at how changing a value changes the
cost so I've written that how theta one
changes the cost just think of that is
how the gradient changes the cost so
we've got this line that orange line
there and it doesn't perfectly fit the
data right so this red dot up here is
the cost for that value of the gradient
of the line so the gradient there is one
and the cost is pretty high say over
5,000 if we make the gradient smaller
yeah that's right sorry just had a
conceptual gap there if we make the
gradient smaller than the cost is going
up the cost is getting higher because
the line is getting further away from
the data so we're getting a really high
cost there now in the gradient descent
algorithm what it does is it looks at
how that weight that theater one there
is affecting the cost so we can see that
moving the gradient down is making the
cost higher so gradient descent looks at
the shape of that line there and it
looks at the direction that the
gradients in and it says okay if you're
getting higher then we need to move in
the other direction so the cost is
getting if we take a gradient descent
step now okay so that's something that's
coming later let me reset down right
back up here if we take gradient cent
step now we've got to a much lower point
because we've taught it change the
weight but change it in the direction
that you think will make it give a
better cost value so that's what
gradient descent is doing and the reason
it's called gradient descent is because
we're looking at the gradient in this
graph here that the graph of the cost
against the weight and we're saying if
you're getting higher if the cost is
increasing as the weight gets smaller
then you need to increase the size of
the weight you need to go the other way
and gradient descent where it's like
that any questions about gradient
descent because it's not the easiest
thing no okay cool let's leave that
right so I said here that we won
gradient descent I didn't say whether we
did it per example for the whole thing
and that's well it does anyone know
actually does anyone know when it should
be run or in the different time zone it
should be one does anyone know yeah do
you know would you do it it's the answer
was training training would you do it
for one example would you do it every
time you fed an example through the
network so one house today two from one
house would you run a step of gradient
descent yep exactly
so the answer is see your answers you
could do batching and there are
different ways to do it so online means
you run it per example so again remember
we're going to take one house the data
from one house we're going to run it
through our network maybe then get the
idea of how much it cost now we're going
to go back from the network and work out
how the weights affected that cost and
then we're going to adjust the ways no
we're going to take another example
we're going to go again and we're gonna
go back we're going to just the weights
then we're going to take another example
that can be slow and stochastic gradient
descent means pretty much run a massive
batch so imagine we've got a data set
we've got 100 examples of house prices
run all of the house prices through and
then adjust the ways mini-batch gradient
descent means maybe do ten at a time so
why do we have different ways of doing
this well one of the answers is if you
have a data set where you're trying to
work out for example is something a cat
or a dog and your data set has a nice
distribution so in the first half of the
data set you've got like 25 examples of
caps and 25 examples of dogs then he can
get a good result by running gradient
descent after that because you've got a
good representation of what you're
trying to work out but if the first half
has got 50 pictures of cats the second
half has few pictures of dogs it doesn't
make sense to batch it that way so it
depends on your data how you do it and
don't worry too much about that now if
if you're kind of really new to know a
network's just know that there are
different ways to do it
there are also alternatives to gradient
descent I'm not going to go into them
okay don't worry you don't have to do it
yourself there lots of libraries deep
planning for J is the Java machine
learning library sorry
neuro Network library which I recommend
so play with that you don't have to
write that propagation you don't have to
implement gradient descent it does all
of that for you you just need to learn
the library equally tensorflow if you're
feeling adventurous and you like Python
so that was gradient descent that was
quite difficult I'm sorry I lied earlier
that that probation would be the most
difficult thing
so we're reducing the weights in
proportion to the gradients right the
learning rate
so you notice maybe the the keen-eyed
and the awake of you that when I did
this the first time oh I would have got
something that was I made a mistake and
I said ignore that and I'll start again
let me show you that mistake again let's
just change this thing called the
learning rate into two let's change of
high let's make it three and that's
where we sorry the pointing on my screen
shows you nothing here we go
that's where we are right now that's our
theta value the weight there one point
eight nine let's take a gradient descent
step okay we're getting further and
further away this isn't how it's
supposed to be working okay why is that
anyone know does anyone know what the
learning rate represents yeah I'm sorry
someone someone beat to the step size
that's right so when we did this
gradient sent and we talked about moving
to a point where the weight is the
lowest there's some configuration called
the learning rate which is the step size
the step size is exactly as it sounds
it's how big a step to take at each
point so where am i sorry
right what if the learning weights too
high anyone tell me yeah the answer is
it might take the wrong decision sooner
and go the wrong way that's exactly
right
what if it's too small yeah it takes
forever exactly perfect
so here steps ice is too big we've got
this neuron trying to get to the bottom
of the valley but it's a giant so when
it steps across the valley it's going to
completely miss it it's gonna go to the
other side and that doesn't matter so
much in this picture but when we have
our three-dimensional landscape of hills
and valleys and you're a giant trying to
find the lowest point you're never going
to find it and equally if you're tiny
though you'd hit tiny steps soon take
forever okay so I showed you that so now
I'm going to show you an example again
of the learning rate and just to really
hammer that learning way in there and
back in the Python console apologies
people who hate Python I was like you
once okay I've got a really simple
Network here let me actually just show
you that again here's my really simple
Network this is much friendlier than all
of the other examples that we've looked
at we have one hidden unit in one hidden
layer we're just moving from the input
data to this hidden unit output the
white hat I've got 100 data points
between 0 and 5 and the output is a
function this is the example that we
looked at in the beginning so it should
be the example the answer should be
something like 3x plus 2 that's what we
want the network to work out that's what
want it to show us we're using the
linear function because it's a linear
problem so there's no problem no
limitations using that activation
function so after one iteration we don't
have a good example sorry that's not
after one iteration that's with a
learning rate that's too high
and so what's happened there is
just hasn't found where it should be and
that's where it should be there with
input data it's taken a really big step
and it's got all of the protections run
make the learning wait a bit lower it's
still run
that's better okay so we're getting
closer to a good learning rate there the
prediction is again in close to the
input data and again maybe that's a bit
too small it should have been run for
longer but the point is that the
learning rate is something that you have
to configure you can have like the
perfect Network you can have the perfect
network for your problem and if you have
the wrong learning rate then you're not
going to get anywhere and that is a
hyper parameter it's a hyper parameter
because it's something that you need to
configure in order to tune the network
let's go back and take a look at
tensorflow actually that looks cool
doesn't it we'll come to that it's my
playground so here we go that's the
learning rate so does anybody feel like
they understand the learning rate
everyone put your hands up come on yeah
thank you yeah okay there's some hands
down do you want me to start again no
okay good
sorry scale would say oh no so this is
this this is they're the people who
develop this at Google helping you out
by giving you some some starting points
I think it would have been quite cool of
them if they just made it a number input
box especially for beginners yeah
okay so that was the learning rate so we
have our Giants and our very small
people underfitting
does anybody know what underfitting is
no yes
yes so the answer there was you don't
have enough features for your
predictions so you don't build a model
that's complex enough for your problem
that's the right answer so imagine this
was meant to be a snail by the way
imagine we're trying to classify between
the pink dots and the gray dots and we
used the linear activation function well
that was stupid wasn't it so we don't
get anything so the network is too
simple so you could try more hidden
units
you could try more hidden layers and
I've been asked before and I'm sure
every everyone in my situation has been
asked this how do I choose how many
hidden layers and hidden units to use
and and that's one of I have the most
irritating answer for that which is well
it depends so all the activation
functions is suitable like I said we
can't map a complex in a function so
maybe we need to try something else
so that was under fitting predictions
don't fit the data well enough
overfitting so apart from anybody who
didn't previously know the answer to
this question can they tell me what
overfitting is yeah the answer is when
you use too much training data and the
answer becomes too specific it might not
be that you use too much training data
it might be that you maybe you use too
many features but your network is too
complicated it could be for a variety of
reasons
so overfitting you fit the data to well
there looks good
let's okay it doesn't really look like a
snail it's fine you don't need to tell
me whatever something we get some new
examples and now the network doesn't fit
that no it doesn't predict it so the
problem of overfitting is that the
network has learnt to fit the training
data really well it's like if you ignore
this picture just imagine it's a perfect
fit it's a perfect fit and now a new
data comes along it doesn't generalize
so you'd say it doesn't generalize well
to new examples
so one way to do that as a technique is
using not using all of your training
data when you train the network so we've
got a hundred bits in the hundred houses
we've got information from a hundred
houses filled our house price prediction
model we don't want to train the network
with all those hundred examples maybe we
want to train it with sixty examples and
then we want to use twenty examples to
do things like change alpha change the
learning rate make it higher and lower
and just test it out on those twenty
that we held back and see how that does
I want to change the number of hidden
units and hind' lairs and that would use
that on the validation set we'd use the
validation set to tune those different
but the network so the problem how many
hidden layers should I have how many
hidden units in each layer what should
my learning rate should be that's what
you use to work it out so it's a it's a
bit trial and error
really oh no it is Tanner and then the
final bit when you've chosen your hyper
parameters and you've got right I'm
going to use 0.1 is my learning rate I'm
going to use five hidden units in one
hidden layer then you test it on your
test set right at the end and that means
that when you train the network you're
not training it on everything and you're
kind of mimicking this idea of a new
example coming along
whether you are mimicking the idea of a
new example coming along and it not
knowing about it and saying okay so have
you found a general pattern or have you
just found a pattern for this data
because we want that general pattern
there are fewer the techniques there
again I won't go into here but if any
one is play with neural networks and
they came along to this talk to see if
they clone anything new and maybe you
haven't heard of bagging or early
stopping so take a look at those if
you're looking for ways to prevent
overfitting and I will reference a
course at the end of this talk that you
can take to learn more about that so
your networks too complicated maybe or
maybe use too many input features maybe
when we were trying to work out the
house price we fed in information about
the location and maybe that doesn't make
a difference to network maybe we fed
information about whether or not
the hose in the garden and that never
makes a difference to the price of a
house maybe we gave it too many features
so after overfitting when we fit the
data too well maybe we need to use
something called regularization this was
on the tensor flow playground there was
a drop down for regular regularization
and that is nearly coffee time by the
way so don't worry we need to shrink
down the features that we just don't
care about hey we need to MIT and go
away we don't care whether or not
there's a hose in the garden we fed the
data through the network anyway but we
need the network to ignore it so it's
kind of what regularization is doing
it's maybe the really important thing is
the size of the house and the number of
bedrooms that has the biggest effect
from the price but we've also fed in all
of these features that aren't relevant
so we need to make wait a min quieter
regularization does this this is a
slightest bit misleading because
actually does it by reducing some of the
weights not the input features but the
concept is similar so it's a wager in
training to reduce some of the weights
which has an effect of making these
features a bit quieter that we're not
interested in there's two different ways
you'll see on tanks flow l1 and l2 if
you're interested in what they are come
and talk to me in the break okay so that
is regularization so that's producing
overfitting by making some of those
weights quiet so what we done we've done
feed-forward we've moved forwards
through our network with move backwards
through our network because there's no
rest for the wicked and we've looked at
the weights and how they fit the error
so now we know how the weights if at the
error we can reduce them a bit or
increase them with gradient descent the
learning weight is going to determine
how fast we reach this point with
gradient descent underfitting is when
you haven't done well enough overfitting
is when you've done too well and
regularization is another one of those
things we'll see on the tentacle up
later and drop down a way to kind of
reduce overfitting so please go and get
some coffee have a break
walk around and we'll come back at
half-past if that's okay with everyone
thank you
okay so we had the the the fun section
which was part one with her smiling
neurons and we had the kind of like oh
no section last with the forward and
back propagation and now this is kind of
a bit more relaxing there's no maths I
promise I'm not going to try and explain
anything so it's a break for me as well
instead we're going to look at networks
so we had so one in your networks we had
the neurons we had them learning and now
we're gonna have them learning in a
network all of the examples we saw so
far you may have noticed a bit basic so
here we are just your map just in case
we're lost data right before we go
anywhere near a network we need to make
sure that the data is in a good place I
know this isn't a data mining talk this
isn't about cleaning data
this isn't I'm not going to explain
hootsburgh component analysis but I'm
going to say the word so I sound clever
but we are going to talk about getting
in the right shape because I get a lot
of questions I'm trying to do this with
newer networks what should I do the
first thing is we someone asked about
features how do you know that what the
right features are what's the data how
do you make your data numeric this is
like 50% of the difficulty of neural
networks once you've got beyond the
maths okay then it's 50% data and it's
50% those hyper parameters the learning
rate the regularization the number of
hidden units number of hidden layers
it's data data is the big thing so we
need to map the input to the output but
we need to make sure that the input is
in the right format first so first of
all has anyone heard of feature
selection yeah cool great we've got
three people maybe more so we need to
make sure that when we're picking the
house we're looking at a house price
problem and we've got our data about a
house we don't put information in about
whether or not this garden hose no one
cares no one cares unless you're like a
really really dedicated gardener who
just can't walk inside and fill up the
hose oh I don't know anything about
garden you can tell so we need to make
sure that we don't put in data that's
not relevant to the network this isn't
the same as regularization this is
before we even put anything in the net
so there are two techniques called and
there's feature selection using filters
and there's feature selection using
wrappers filters look at every feature
in isolation can anyone guess what why
that might be a problem if it's a
problem I've given you a clue but why do
you think it might be a problem anyone
yeah yes so the answer is maybe a
feature by itself isn't interesting but
when it's paired with another feature
then it becomes relevant so you might
have imagine in your house price example
you don't have the size and square feet
imagine you have the length of the house
and the width of a house by itself it
doesn't really tell you anything if you
know that your house is 10 meters long
but when you know that it's 10 meters
long and 10 meters wide then you have
some useful information that you can use
about the price of the house rappers are
interesting it's so know so we'll go
back to filters so filters look at
features in isolation and if anyone here
has done statistics and I'm going to
throw out the term information gain and
then maybe know anyone okay so what we
do is we look at the feature for example
and we say by yourself
how much do you tell me about the output
of this dataset and it's fast and that's
why it's good but it doesn't tell us
about the interplay of features which is
why it's bad our rap is you can guess
where this is going rappers do but
they're slow and rappers do because what
they do is they look at subsets of
features but they evaluate how effective
they are by running them through
whatever you're using to try and work
things out so it might be running them
through in your network and looking at
their accuracy it might be running them
through a classifier for example so I'm
just going to give you a little example
of how you can do that so there's a
program here and now I hope you're all
going to forgive me for using Python
earlier because Weka is written in Java
so I feel like I'm off the hook slightly
this there's this M I'm fairly sure it's
open source project called Weka you can
load in data so here what I've done is
I've gone
user file these are all Weka files and
they're a bit just think of it as like a
CSV it's not that loading our data set
we can take a look okay so this is wine
we're going to classify wine I'm sorry
it's not beer because that would be more
relevant to dev ox and we have all of
these features I don't know I can't even
pronounce most of them but you it's Hugh
the wine and it's probably a good get a
good guess that of all of those 13
features only a few of them will really
make an impact to the class of Hawaii so
if we go and we look at filter so I'm
using this information gain filter and
we run it through that was really fast
we can see that flavonoids is anyone
know what flavonoids are it's kind of
murmurings didn't catch anything so I'm
going to guess yes soon and she was
favorite sounds good and so and prolene
any chemists no well anyway it looks
like they make a difference to
classifying wine into different types if
we run it through the wife with a
wrapper then it takes a lot longer so I
actually started this running in the
break and it's still running but that
would give us a similar measure of how
good each feature is for what we're
trying to work out so thought is filters
and wrappers I shouldn't have told you
the answer I shut myself in the foot
there okay so um can anyone start out
why we'd want to standardize or and/or
normalize data yeah or it might be for
example if you if you have different
features and they all have really
different values so you might have your
size of square feet in meters squared
and then you might have the number of
bedrooms so then you've got this input
that might be in the hundreds and an
input that might be in the you know
below tens so you want everything to be
roughly the same shape because it really
helps the performance of your network
and this is the last point before I stop
talking about data so don't worry
dimensionality reduction so this is the
idea that
and you might have two features that
really highly correlated and so there's
a way to kind of combine them into one
feature so it's like they're timesing
the length of the house by the width of
the house and I apologize to anyone who
does understand principle component
analysis because you're probably going
that's not really what happens it's all
eigenvectors and magic but so let's take
a look at the iris dataset so anyone
know the iris dataset
apart from Jim Weaver I know you know it
I saw a few hands Jerry's dataset is a
classic data set if you want to start
doing some machine learning you will
start playing with these networks get
the iris dataset because people have
used it so much if you if you're trying
to work out how many hidden units you're
not sure what the answer should be
you're not sure if you're getting going
in the right direction it's used as a
toy example for so many machine learning
problems that you're guaranteed to learn
the best way to configure the network
and it's a good way to learn when
someone else has already given you the
answers I think and code doesn't really
work when it comes to exams but the rest
of the time it's great so we've got four
input features we've got the petal
lengths we've got the petal width we've
got the sepal length we've got the super
width don't ask me what c-plus
and we want to be able to work out
between three different types this
flower and so you can use something
called principal component analysis to
reduce the number of dimensions to a
smaller number and then it's faster to
run through our classifier sometimes
it's more accurate as anyone heard of
the curse of dimensionality so that's
the idea that when you're looking at a
house price for example and you have
your bedroom reach 2,000 square feet
whether or not there's a hose where
there's a school nearby where there's a
garage there's a certain point where
adding more features actually makes the
performance worse and that's why it's
important to do pre-processing like this
so we've got a little example in Python
again I have the iris dataset this is
what it looks like
and I can't illustrate to you what it
looks like in four dimensions so I've
done principal component analysis to
reduce it down to three just to show you
what it looks like so
they're the different classes they're
kind of separated in space not from this
angle but hopefully will earth you see
this is separation and that is principal
component and our principal component
analysis reduce the number of dimensions
down okay so it's anyone know who this
man is
shame on you shame on all of you
this is Marvin Minsky this is one of the
fathers of AI this is a wonderful man
and I'm going to show you another video
because I promise that this part the
talk would be nicer and everyone loves
videos it's like when you're in school
and your teacher decides to play like a
history drama instead of watching you so
we're going to watch Marvin Minsky talk
about some of the problems with
perceptions remember I told you about
perceptrons and his book in 1969 that
shot all over perceptrons well this is
interesting this is some of the
limitations of perceptions but this
sauce that this some of the limitations
of neural networks in general and it's
specifically stories getting your data
on or finding that you've actually
trained the network to learn the wrong
thing oh good sorry I'm going to start
again was to count the number of objects
in the picture that was because rather
obscure mathematical features of the
concept of counting reinforcements okay
so one story that he also tells for a
winch at play and all the videos is
about no no is that they trained on
tanks they trained them on American
tanks and Soviet tanks and they wanted a
network to know the difference between
American tanks and Soviet tanks useful
thing to know and ash they trained the
network and they fed in new examples
they realized that it was getting
everything wrong it doesn't really well
in training
overfit it and it wasn't it wasn't
correctly classifying them and somebody
realized that all of the pictures of
Soviet tanks were taken on a cloudy day
so what you think the network learn tell
the difference between a cloudy day and
a non cloudy day exactly so date is
important Chinese jr. is important
because the network is only as good as
you have taught it to be it's the funny
thing about learning with newer networks
is that you are learning as much as the
network is learning you're learning how
to tune it so architectures all of the
examples we saw were feed-forward neural
network and Marvin Minsky mentioned
leaving back feeling back into things
there are so many neuro network
architectures that I'm not going to go
into them I actually don't know all of
them so this is just a subset but
generally feed-forward are the ones that
we've seen moving everything moving and
fully connected layers who know all of
the neurons were connected to all of the
other neurons recurrent neural networks
are able to feed back on themselves I'll
talk about them but you may have heard
and you probably definitely will have
heard especially with image recognition
everyone talking about convolutional
neural networks so I just want to tell
you something about them so
convolutional neural networks are
specialized kind of feed-forward network
so you might input an image this is just
a tiny image 32 pixels by 30 pixels it's
got three different color values that's
our input and then it goes into a
convolutional layer and what this does
say you've got your image it's a cat
obviously we get something called a
filter and that moves across the image
like that and takes data from it there
we get another filter it moves across
the image and we filter after field you
have to filter after filter and what
they're doing is they're learning
different parts of the picture they're
learning little features so any have a
picture of a cat and I think we
mentioned earlier it has a tail cast
here most cats have tails so you can
think of it as learning these features
it's learning what makes the cut
whiskers make occur eyes make occur
it's occur famously there have been some
problems with Google's image recognition
not being able to distinguish between
certain types of dogs and mops so
convolutional neural networks have these
special filter filters and then they
might go through a normal layer like we
saw earlier with the rectified linear
units and then a pool layer which like a
way of shrinking all of the data it's a
bit like the same concept as those
principle component analysis and
outputting it in the final layer so
here's a little idea we're trying to
work out what makes a picture of a bird
a bird and the network has learned that
one feature is a beak and one feature is
the body and the wing and the other
feature is the eye we haven't told it
what those features are we just said it
in reviews features in a different sense
reviews features to mean the data that
we're feeding in and now we're using
features to mean what does the network
think is important about the data that
we fed in so all of when it sees all of
those components it knows that it's made
of bed imagine we have a jug with a
spout that looks suspiciously like a
beak so we have the spout the spout
matches up but it doesn't match the body
it doesn't match the eye so it's not
it's not a bird as you can probably see
and one thing if you do like me feel
like reading the 1986 paper by Hinton
Romo Hart and Williams find it's just
this wonderful way of describing what's
happening inside neural networks
internal representations the network is
learning about the data we're learning
how to chain the network and networks
learning the structure of the data some
really interesting neural networks that
aren't supervised everything we've
mentioned so far is supervised learning
and that's because we know what the
answer should be but giving the network
the data and we're saying try and get it
to this point please with unsupervised
learning we're saying here's some data
can you find some something meaningful
in it and there are some really
interesting neural networks if you look
up hopfield nets and Boltzmann machines
that take this idea
and they try and find structure in data
and the way they learn has been the
compared in psychology to dreaming they
did they take the data in and they try
and reach a point in the data reached a
good point and then they have to do some
unlearning and the theory is that when
you when you're walking around during
the day you're going bad chair Linda
that kind of thing and you go to sleep
you don't need to remember everything
that happened needs to a bit unlearning
that's just a side side note that sounds
interesting it may not make a lot of
sense so but these internal
representations are what in your
networks to do which is one of the
reasons I find them so fascinating so
recurrent neural networks nearly at the
end by the way so these deal with
sequential data or time sequence data so
as well as getting all of the
information in at one point at the
beginning you're also looking at what
you've had previously and you're feeding
it back in and looping around there are
some really example example for sorry
really interesting projects with the
Google magenta team rich Jim Weaver
pointed me in the direction of where
they're using recurrent neural networks
to compose music so using something
called reinforcement learning as well
which is subject for another talk they
teach a network to try and learn what
the best music and what the best notes
to play next and I don't have that but I
do want to show you before we leave
architecture behind the example of a
convolutional neural network so I'm very
touched about these layers this is a
project some Wonderstone
I'll make the link available later using
that data set I mentioned right the very
beginning if anyone remembers em lists
data sets all of those handwritten
digits and now I can draw a new digit
and it won't know this is new example
it's never been drawn before
philosophically and it's going to use
convolutional neural networks first of
all using the filters to work out these
features and then compressing it all
down to work out that I've drawn it too
so I really recommend playing with this
and
having a go because it's a really good
way to visualize what's happening inside
a network so alright let's go back to
tensorflow do we feel like we know
what's happening here no do you feel
like you have a better idea what's going
on
cool I'm taking the three yeses in the
nod it's unanimous consent that this
talk is a success so if you could all
just feed that feed that back please
awesome right now we're going to spend a
bit of time on there and beyond
first of all learning paths okay so I
actually started someone asked me
earlier and I was a developer who's very
very bored with my job and then I got a
job at Vox which was awesome Steffan
Jensen and Marc hazel and I was doing
some learning in my spare time and I was
using Coursera which is really cool
online learning you all heard of MOOCs
massive online something learning I'm
sure there's another oh there and
Stanford University published Angie ings
course general machine learning start
with that it does include maths
uses Fatah's instead of double use for
the weights if you then take the
Geoffrey Hinton course they'll find it
uses double use it's very confusing it's
really good it's really good way to get
started it's free as is the Geoffrey
Hinton course it's also a new series of
courses on Coursera called deep learning
AI
by undoing I should say I'm not paid so
any of this so do check those out you
can take them at your own pace and then
what I ended up doing next was lots of
wider reading and I've gone back to
university so that's the disclaimer
right resources I notice it someone
tweeted something in the break about a
YouTube channel which is really good so
I thought let's start our own hashtag
let's let's hijack one of DevOps is
hashtags so if you all have come across
resources in your own time that you find
useful please tweet them with this
hashtag I'll do the same after the
course and maybe we can take over the
DevOps well who knows right I'm gonna go
through a series of questions that I
I think need covering and then I'll open
up the floor first of all do I needs
maths so you might have your own
opinions about this now so you don't you
definitely don't need mouths to
understand what's happening in machine
learning but if you want to take one of
the courses and you want to start
looking at own algorithms and it is very
useful so don't get put off by the slide
by the way this is just if anyone does
want to go off and do some cramming like
I've done and get some textbooks here
are a few places to start so there's the
equation of a straight line it's a nice
one and quadratics because you might
have something called polynomial
regression where you're not fitting a
straight line for data you're at fitting
a curve differentiation that's if we
want to get close to gradient descent
partial differentiation I'm going to get
quieter and quieter and fade out at
least for all of the hate kind of
reaches peak levels so oh look there's
some more maths there you go
just the top right one just don't look
too close um it's really useful to
understand what's happening and I used
to say no no you don't need mastrantonio
Novak's but actually if you want to
start looking at things yourself you
want to start reading research papers
it's really good to start getting
familiar ization with us did I see a
thank you in the audience there you're a
mathematician all right I thought you
were grateful mathematician oh right
what programming knowledge okay so yeah
I use Python I'm sorry if apologize a
lot for that Java also deep learning 4j
there are lots of Java options you don't
need any programming language to do
universe there are so many resources
available but I do think that Python is
really good to quickly write things up
all of my examples that I showed you
took a very short amount of time you can
probably tell but something like when
actually comes down to performance in
some libraries deep waiting for J
library and the Python library that I
run some the examples I'll actually use
I think they you see underneath it all
so the performance is pretty much the
same you just have that overhead of the
language depending on the length of the
problem
he's got more overhead running Java
because you have to have loaded JVM it's
worth learning Python if you're going to
do any kind of scientific programming
there or MATLAB or something like that
okay watch the data look like so I think
we covered that and any more questions
on that awesome okay right this is my
favorite question I'm trying to solve a
timetabling problem where do I start
with no one knows I don't know I don't
know everything is so domain-specific if
you have a problem you're trying to sell
with no no it's you might not need
neuroleptics
could you something else I don't if you
want to start playing with no no it's
don't look for a problem and then look
for a new network look at example
datasets look at cavil calm look at
datasets already exists and start
playing with them and get a feel for
neural networks rather than think wow I
had this real read example maybe I can
solve it because you you need to load up
a lot more about your data before you
get to that stage and your own APIs when
do I use architecture X totally depends
on the problem you're trying to solve
again and the best thing to do is
probably to read through the Wikipedia
page about different architectures and
the person who gives the best answer
this question wins this t-shirt okay
miss the camera never and that's that's
my talk everyone so thank you very much
yeah he is the only one
they don't have any questions
okay so the question is how do you go
from numbers and images to the weights
and the neurons in publish no neuro
networks okay so how do you how do you
go from numbers to the fact that
something that's like so in the demo I
did with there with the handwritten
digits how do you go from the numbers to
looking like some things are - well um
so one answer is I'll point you to some
resources and the other answer is a
computer doesn't see you - and the way
that we see OH - and we're not training
the network to see you - in the way that
we see - so when you actually put an
image in the internet work you actually
just flatten the image out and you're
feeling it in this really long vector of
numbers of different numbers and the
computer isn't learning to see we
understand it is that it's learning to
see the letter - or the tail of a cat
but it's actually learning its own
patterns the mathematical patterns
behind that image of - so it's not
really learning going from the numbers
to learning in the way that we think it
would be rows the other answer to that
though if you look more into
convolutional neural networks
it's a convolutional neural networks a
base on I think it's the visual cortex
so any reaching back into my mind for a
cognitive science class I was in a few
weeks ago now so this might come out
really wrong but when you in your image
a field different points of light and
mapped to different parts of your brain
I'm not don't ask me any detailed
questions about that because I can't
tell you I haven't done the reading yeah
I'm behind but I'm so convolutional
neural networks work in a really similar
way but you but you wouldn't describe
but we would put the numbers together in
a mate
that shows us that they're in a certain
place whereas computer doesn't
understands the same structure okay how
do you go from the data in the hidden
layers to how you visualize it in the
demo and someone actually did that
someone actually made a really cool
YouTube video which I haven't asked
permission to show so I won't sure in
this talk maybe I'll show it when they
stop filming where somebody has taken
the the output of the unit it's not the
weights but the output the actual nodes
the units and they've assigned a color
value according to how big the numbers
and they've built this really cool
visualization of a whole network working
and the data moving through it using
these color values so it shows it firing
so maybe if they maybe maybe when we
finished I'll show the video
okay so I'll share it yes oh yeah
I want surgeon and I'll give you a
chance okay so the idea is that the
overlords will come first when we stop
feeling that way as pictures of cats and
other cute animals so the benefit of
everyone at the back and this gentleman
is saying that we're effectively
drugging your networks by feeding them
these images keeping them yeah I think
that's quite area you can't build a
flawless system when you're flawed
yourself can you so maybe if we all stop
on the cat subreddits we'll have more
time to devote to no networks
you know and so if you want to counter
that then all you can have a fight for
the t-shirt
any more questions should I move on to
the video okay
all right let's let's end the talk thank
you very much she turn myself off and I
want to be heard yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>