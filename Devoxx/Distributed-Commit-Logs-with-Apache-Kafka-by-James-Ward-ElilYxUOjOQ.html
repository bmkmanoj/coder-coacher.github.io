<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Commit Logs with Apache Kafka by James Ward | Coder Coacher - Coaching Coders</title><meta content="Distributed Commit Logs with Apache Kafka by James Ward - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Commit Logs with Apache Kafka by James Ward</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ElilYxUOjOQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right welcome everyone thanks for
coming I'm James Ward I'm a platform
evangelist at Salesforce and the session
is about distributed commit logs with
patchy kafka so let's dive in so I think
I might not be alone in that sometimes
when moving into this world of
distributed systems it can feel like I'm
this cat just pounding away on this
keyboard and really have no idea what
I'm doing I've been doing java
programming for a long time but
traditionally with just a database and a
web server you know pretty simple stuff
and what's happening now in our industry
is we now are having distributed systems
thrown at us from all sorts of different
angles like systems like spark are our
systems are really becoming totally
distributed we're moving from single
machines to clusters and so this has
been challenging for developers like me
to figure out how do I move into this
world of distributed programming with
all these new things to learn and all
these new challenges so for me one of
the things that made this easier with
kafka was the that Heroku now has a
managed Kafka service and that's when I
started to get into Kafka as when it
made it a little bit easier for me to
move into that distributed world because
I didn't have to think about managing a
whole cluster of kafka machines so so I
think that the there's a lot of talk
store at devoxx about cloud and I think
that it's it's really a great way for us
as developers to make it easier to make
that step in but this session is not
going to talk about cloud we're going to
talk about Kafka and and then talk about
how we can build these distributed event
streams on Kafka so let's dive in so
first let's start with a really bad
analogy
Iren allergies so does anyone here were
they building systems when raid 5 was
invented anyone few people ok so I was I
was a sysadmin back when raid 5 came
about and it was a game changer for how
we did data storage
because what it did was it allowed us to
now distribute our data across multiple
hard drives and have some redundancy and
performance while we did that so it's
pretty game-changing for for data
storage by taking the data splitting it
across disks and giving us redundancy
and performance at the same time so how
this relates to Kafka is I look at Kafka
as being like the raid 5 for event
streams what it is is it's this
breakthrough in how we do event
streaming technology that really kind of
take some of those ideas from raid 5 so
it's distributed out of the box and it's
redundant
so those are kind of the two key values
of Kafka that make it unique in this
event streaming world is that we get
some of these benefits the distributed
and redundancy without having to make
traditional sacrifices that we've had to
make when doing event streaming so so
that's that was what what initially
interested me and Kafka was having
something that was naturally distributed
and provided the redundancy so some of
the fundamentals of Kafka is one it's it
uses messaging system semantics so this
is really important I think a lot of us
are familiar with messaging systems
publish/subscribe systems whether from
JMS or other places and what Kafka did
that was pretty smart as they used those
same semantics around their event
streaming so it makes it a lot more
approachable so a lot of the the
terminology that we'll be using today is
shouldn't seem familiar to you if you've
if you've done any kind of messaging
system stuff before so another key part
of Kafka is that clustering is core so
with with kind of first-generation
events systems
I think the clustering was was usually
an afterthought and with Kafka
clustering was was really part of the
initial design and architecture for it
and in distributed systems that's what
we need we need that redundancy across
machines we need and we need the
performance of being able to to have
many machines working on things and then
the the last fundamental here that's
really important
is that Kafka gives you guarantees
around ordering and around durability
and so that's that's really important in
building these these modern systems so
let's talk about what what the use cases
for Kafka are so there's there's a
number of different things that we can
use it for I've what I've seen in
working with a lot of enterprises is
that Kafka is becoming really it's
moving into like wildfire moving into
basically every enterprise so most
enterprises out there are either already
using Kafka or are evaluating Kafka
right now because one there's so many
different things that it can be used for
but it also really solves a lot of the
needs that the enterprises have so let's
talk about what those are so one is
modern ETL and and change data capture
so how many people here have a system
that like takes CSV files and batch
imports them daily or something like
that batch imports
ok so quite a few of you yep so this is
very common in enterprise is is this ETL
process of that's that's pretty arcane
and Kafka is becoming a replacement for
those types of systems or at least
becoming a way to broker that ETL into a
more reliable fashion and in a way that
can be integrated with from a bunch of
different endpoints change data capture
is kind of a new newer idea and this is
where we want to capture all the changes
that are happening in a system and then
be able to take those changes and do
something with them in another system
and so Kafka is being used very heavily
for the for both of those sorts of
things the other one is data pipelines
or one of the other ones is data
pipelines so with data pipelines what
we're seeing is that we no longer have
these simple systems where I've got my
web server talking to a database we have
all sorts of different systems that all
need to share data and access the same
data and move that data through a
pipeline and so there there's all sorts
of great talks here about data pipelines
and so so one of the use cases of this
is I'm taking data it's coming into a
system but then I need to send some of
that data over or maybe all that data
over to a Cassandra system I need to
send some aggregates to a Postgres
database
need to send some other data to a no
sequel database I need to plug in spark
from for being able to do data analytics
I need to be able to do machine learning
on top of all this so now we're moving
into this world where we've got these
lambda architectures and we've got all
this data flowing through the system and
all these things being wired together
and Kafka is very heavily being used for
these data pipelines now as being the
the the system that can handle all the
different integrations and pipelines
across those so the last one probably
one of the most common use cases for
Kafka is for big data ingest so this is
providing an endpoint where your your
IOT devices your analytic data your
event data from your apps and from your
web sites can all be published to a
Kafka system and then anyone can then
subscribe to that and then hook it to a
data pipeline if they want to do that
but what essentially this is doing is
providing a really scalable buffer for
this large volume of data that can be
you know terabytes a day or whatever so
Kafka was created at LinkedIn as a way
to solve a number of these problems that
LinkedIn had they needed to be able to
have a redundant scalable system for
doing the event streaming that their
that their system was doing and what
they realized is that they couldn't do
it on the traditional architecture so
that's that's why they built Kafka okay
so that's our quick little introduction
to you to the why of Kafka but I want to
dive into some some actual details to do
about some of the the vernacular of
Kafka so the first piece of information
to know is records so Kafka uses records
and this could also be called a message
or an event so this is the the actual
container that's going to send the
information through the system so
records have a name a value in a
timestamp so those are the the core
components that exist on on every record
they're immutable so they can only be
written to you not not overwritten and
along with that they're append only so
when we're sending these records into
Kafka we're only appending to the system
there is no up
kind of process and then last thing
about records is that they're persisted
across the cluster and also persisted to
disk so so you're not gonna lose these
records there's different settings that
you can set on how long you actually
keep the messages for most people don't
keep messages forever in Kafka most most
of the time you'll have like a one-week
window or a month one month window for
durability so if you need forever
durability then you're gonna use one of
those data pipelines to take these
records and shove them into Cassandra or
some other data store but you most often
will not use Kafka for forever
persistence okay so this is also known
as a log so all of us I'm sure have
worked with logs before and you'll see a
lot of similarities between a record and
Kafka and a log so that's why you'll
hear Kafka being referred to as a
distributed commit log so the idea is
that we're committing so this is append
only immutable where we're committing
these log messages that have a name a
value and a timestamp into the Kafka
system and we're persisting them and of
course it's distributed so now here are
a very messaging system oriented
terminology here as producers and
consumers so if you've worked with
messaging systems definitely familiar
with this concept of producers and
consumers so in Kafka what we have is a
broker is a node in the cluster so so
that's just just another word for a node
in the cluster and a producer writes
messages to the cluster so they're
writing messages to to a broker and then
a consumer reads records from a broker
so this is actually important because
the consumer is not actually being
pushed messages so I was kind of
surprised you know I have been in the
the reactive world for a long time where
we want everything to be pushed based
but it turns out Kafka doesn't do push
really they the way that you do it is
your client will actually ask the a
broker do you have any mess
Jews for me or do you have any records
for me so it's actually reading records
from the broker instead of being pushed
those records so that's a bit of a
difference from from typical messaging
systems and we'll learn in a little bit
why we do that poling read instead of a
push so Kafka uses a leader follower
architecture for for cluster
distribution so what that means is that
there's going to be a elected leader for
a given part of Kafka and we'll talk
about those parts in a second and then
there will be followers that then are
getting the records that are being
written to the leader so leader follower
architecture okay so now it starts to
get a bit tricky because we've we've got
our records we've got our our producers
and consumers and that's pretty typical
with the messaging system but one of the
things that Kafka and we've got topics
so so also very standard messaging
system we've got a topic that we're
going to be able to send messages to a
topic and subscribe to a topic so that's
all very standard messaging system so
what is a topic though a topic is just
this logical name for one or more
partitions so now it gets a little bit
tricky because now we've got this new
element into this messaging system that
that is something that is much more
distributed system oriented then then
I've been used to and so with the idea
of a partition is that we're taking this
topic and we're dividing it into these
partitions and this is where it becomes
starts to look more like raid 5 is that
we've got these partitions each of those
partitions can be written to
concurrently that can exist on different
nodes and so so now we have the ability
to parallelize our message processing
our message posting and subscribing
across multiple nodes I'll talk more
about how that works in a minute so and
partitions are replicated so what
happens in the Kafka cluster is that the
partition will not exist on justice
machine usually it depends on your your
replication settings but usually a
partition will actually be copied across
multiple nodes and so if I'm connected
to a node and publishing messages to a
partition on a node and that node goes
down then a follower will have all the
data for that partition or multiple
followers will have all the data for
that partition and so then a new leader
will get elected and so that the Kafka
cluster will elect a new leader say okay
we're gonna now use this leader over
here and we're going to take this
follower and make it the leader and now
you can continue writing to that
partition on that system so Kafka is
managing all of this partitioning of
messages across the cluster and the
replication of those of those records
it's managing all of that for you so the
nice thing about Kafka is that it's
really hidden all this details of how
it's doing the whole distributed system
the leader the leader and follower stuff
how it's doing the election when a node
goes down all that stuff has been
abstracted from you and then you just
have a really simple interface for for
working with this okay so partitions are
replicated and then here's where we get
into one of the the values I talked
about earlier which is ordering so
oftentimes when we're dealing with with
event streams ordering is essential
because we really need to know for
message processing especially when you
get in to change data capture you really
need to know the order of the messages
so ordering is guaranteed for a
partition so across a topic there are
one or more partitions but ordering
isn't guaranteed across the whole topic
it's only guaranteed across each
partition within the topic so there's
going to be some trade-offs here so in
the very simple use case I have one
partition and that makes it really easy
now I can I can have publishers writing
messages that are ordered into that
partition and consumers consuming those
messages off of that that partition so
that's that's a really simple use case
but that has some obvious bottlenecks
so if I only have one partition then I'm
really only working with a single node
in the cluster so by hand that partition
will be replicated so we still have the
replication but to get the horizontal
scalability of Kafka I really need to
use more than one partition because
that's how I can now use multiple nodes
in the cluster for both publishing and
subscribing to the to the messages that
are going across that that partition or
across all the partitions so that the
partitions are what actually gives us
the horizontal scale across the Kafka
cluster ok so you've been seeing in
these images offsets are numbers in the
messages and those are what Kafka calls
offsets so every message has an offset
value and the way that those work are
that those are unique sequential IDs per
partition so the messages as they come
into a partition are given assigned the
next unique sequential ID so those are
only these the offsets are only unique
across the cluster or across the
partition so you can see in the image
there that as writes come in to
partition 0 they're being assigned new
unique IDs but those same unique ideas
would exist in other partitions the
ordering is only in a single partition
so the consumers are the ones that are
actually tracking their offset so
consumer when it's subscribed to
messages and receiving messages it knows
it's offset or can can ask for the
latest offset and it's the one that's
keeping track of where it is in the
stream processing so some of the
benefits of this are replay so I've I've
definitely built systems where I'm
reading through and at some point
something goes wrong in my data
processing and in a typical messaging
system I wouldn't be able to say like
like hang on I need to fix this
maybe my system that I'm writing to is
down or whatever
I'd have to start buffering or something
like that but in Kafka Kafka is already
doing the buffering for me and so what I
can do is I can say okay this is the
last offset that I know that I processed
so now when I recent the stream I can
say okay let's start back there instead
and so then I don't have to miss all
those messages or I don't have to worry
about how those messages are gonna be
buffered for me so they're just gonna be
buffered by by Kafka okay so that allows
us to do replay the other thing this
allows us to do with the offsets is have
consumers that are operating at
different speeds so in a data pipeline
use case my right to cassandra may be
really fast compared to my aggregate the
analysis that I'm doing with aggregates
and storing that data into Postgres so
because I might have consumers that have
very different very different
performance characteristics I need to be
able to support consumers that are at
totally different places in that stream
processing and so by using the offsets
now I can have different consumers that
are subscribed at different points and
reading at different speeds through the
messages and this is this is really
important for the data pipeline use case
of being able to have many different
consumers that are all doing their own
thing and doing them at their own speed
okay so those are our offsets so the
last bit of terminology I want to go
over with with Kafka is consumer groups
so consumer groups what they're doing is
they are creating a logical name for one
or more consumers so and these are
consumer nodes so if I have a consumer
group and it has two nodes with one as
c1 one is c2 I have to consumer nodes
but if we look up here and this would be
for a given topic a single topic if you
look up here at my Kafka cluster I have
two servers and then each of those
servers would have two partitions let's
say and now if I want to get all the
messages across all four of those
partitions but I only have two consumers
then what the consumer group is going to
do is assign two of the partitions to
each of those consumers I finally have
to but if I have another consumer group
and it has four nodes then I can get
each of those partitions on to each of
the nodes so so and then what what's
happening is that records are only being
delivered once across the consumer group
so when a message gets written to a
partition and into a topic that's only
going to be sent to one of the consumers
within the consumer group so it allows
me to not have to deal with with
multiple consumers both consuming most
multiple consumer nodes both consuming
the same message so it makes that nice
and easy so and then this also supports
load balancing so it uses load balancing
as a describe to to be able to manage
that load across different nodes okay so
now exciting part let's check out a demo
here so I'm going to go open up my
browser to localhost 9000 this is my
very ugly web app and you'll see that
it's connected so this is connected
through a WebSocket to a play server and
you'll see that the WebSocket is sending
out these messages so the message the
value on the left is that offset so this
is that sequential ID of each record and
then the method the value on the right
is just a random integer this is my very
boring simplest thing that could
possibly work application to show okay
I'm hooked up to Kafka I'm receiving
these these records that are being
written and we can see the offset and
the value so one of the things that I
built in here was to show the replay
ability is that I want to be able to hit
pause and so now I've actually
disconnected the WebSocket so my my
consumer is now disconnected from Kafka
and those messages are still being
written and I'll show you how those are
being written in a second the messages
are still being written to Kafka but now
my consumer is is disconnected so let's
now resume but before we do let's take a
look at that offset ID is three four
four zero and if I hit resume and we can
look back down here and see that it got
all the messages
that that had been buffered in the time
that I was not connected so that was the
ability to go back to an offset the
three four four zero offset and then say
give me all everything start from there
and give me all the messages so yeah and
I think you may have noticed that
there's a little what I did is I tracked
the last you'll see there's two of these
three four four zeros and what I did is
I'm tracking the last offset ID that I
read and that's where I'm resource
subscribing at so that's why we see that
in there twice and we'll see that in the
code in a minute obviously we could do
that differently if we want it okay so
that's that's my very very simple demo
let me just show you this is all running
locally in this case I've got my Kafka
server running locally I have my random
numbers worker process that's that's
sending messages to the Kafka cluster so
it's sending those random numbers every
half second to Kafka so and those are on
a random numbers topic you'll see
they're almost do that in a couple of
their places and then my play server is
has is running the WebSocket and serving
the web page and subscribing to Kafka so
that's my my producer and my consumer
out there okay so we'll take a look at
the code for for that in a minute we'll
just keep that running and go back here
okay so in order for to talk to Kafka
you need a Kafka client and there are
many Kafka clients out there so for
every system you could think of for the
most part someone is built a Kafka
client for it but the official one is
for the JVM it's a Java client so that's
great for us for most of us Java and
Scala and other JVM developers is that
now the the first-class citizen for
Kafka client is Java so that's that's
great so as I mentioned before it's
polling base so the client is the
consumer client is connecting and
pulling well we'll see that a little bit
more in a bit
okay so before we dive into the code I
have to give a little bit of an
introduction to akka streams anybody
here used akka streams okay so few
people are familiar with it so akka
streams is an akka which is an actor
system and akka based streaming DSL
essentially so what it's a
implementation of reactive stream so the
akka folks and a number of other people
got together and they defined a
specification for reactive streams and
akka streams is the actor based
implementation of reactive streams so
that's what I'm using in this this
application to wire everything together
and to give me a nice simple programming
model around Kafka so the akka streams
uses a source and sink paradigm so
source and sink are kind of synonymous
with publisher and subscriber so a lot
of the messaging system vernacular is
used in akka streams as well so akka
streams has a whole bunch of features
I'm not going to get into a lot of
details but one of the features they
tell a lot is back pressure so this is
the idea that if my consumer can't keep
up with my producer I can put back
pressure back on the producer and that
works for some use cases and and for
some use cases not but but definitely
could be useful in a Kafka system okay
so the reason one of the main reasons
why I'm using akka streams to wire this
stuff together is that there's this
really great Kafka adapter for for akka
streams so it's just reactive Kafka and
really makes interacting with khakha
pretty simple and does it all in a way
that works well with play for American
and the WebSocket stuff so we'll see
that in the code okay so let's dive into
the code and walk through how this is
all wired together so I'm going to start
with the random numbers app so this is
my application that's feeding data into
Kafka so let's just walk through the
code for this this is Scala code I've
got a little bit of setup here
but then I get this Kafka class so let's
let's go take a look at that Kafka class
so the Kafka class has a sink which is a
sink of Producer record so this producer
record is one that comes from the Kafka
Java client so I'm just there just
reusing that that same producer record
from there and then the string string is
saying that the key type is a string and
the value type is this string you could
make those whatever types you want my
Kafka class here also has a source and
so the source takes a topic and then
maybe an offset we'll look at that but
then you'll see here it uses a consumer
record which is again from the the Kafka
client Java client library and then
again the serialization for the type for
name and value okay so let's go take a
look down at the actual guts of this so
you'll see some SSL config stuff in here
I'm not connecting through SSL when I'm
running locally this just is used when
I'm running on Heroku because Heroku
uses SSL to connect to the Kafka cluster
but in this case don't actually even use
SSL because it's all local ok so then
let's take a look at a couple methods
down here so I have a producer settings
method which creates a producer settings
and then again I've primary ties these
with the name and value types for my
records and then I need to give Kafka a
deserialize err for a producer so this
is going to tell her sorry i serializer
so this is going to tell Kafka how to
serialize the records that I send it
both the name and the value and so that
it can obviously persist this okay so
then we create a producer settings and
I'm reading my Kafka config let's go
actually take a look at my Kafka config
just so you can see some of the config
parameters
there's lots of different tuning
parameters you can specify here but for
my producer I have a closed timeout and
then I'm configuring some information
for the Kafka client so what what I'm
how I'm gonna treat axe retries all
sorts of different different settings in
there okay so I take those settings I
give it my string serializer for both
the name and the value lies value
serializer and then I tell it the
bootstrap servers so Kafka uses
zookeeper as the way to manage the
information about the cluster and so
what I'm doing is I'm giving it these
zookeeper URLs and so that then it can
go ask the zookeeper tell me about all
the nodes that are in the Kafka cluster
and that's how it finds the leaders and
and then that information will change if
a node goes down or a new node comes up
but we need to give it some bootstrap
server so that it can discover the
initial state of the cluster okay so
that's my producer settings you'll see
consumer settings as well we'll look
more at that in a second and then I have
my sync and here's where I'm using that
reactive Kafka library pretty simple I
just say producer dot plain sync there
are some other different types of
producers that you can use and so this
depends on how you want to handle a
number of different factors around
producing to two partitions how you want
to deal with that but in this case I'm
just using a plain sync and giving it my
producer settings so that's my sync and
the sync is where I pour stuff into so
that's where I'm sending my messages to
so let's go look back at random numbers
and we'll see how we actually wire all
this up so I've got my Kafka here that's
some one we just looked at and then I'm
doing a source tick so in akka streams
always need a source and a sink so the
source is and sync it wired together and
so I need a source that's going to
produce something and so in this case
I'm doing a tick source and my tick
source is is just every so often doing a
tick in the system so this is my
starting how long ago and wait to start
and then this is how often I want to
tick so every 500 milliseconds I'm going
to do a tick and the type here of the
parameter that's generating the tick the
is unit so I'm not actually using
passing anything in but then I need to
take that tick that happens and creates
a unit every 500 essentially nothing
every 500 milliseconds and I need to
turn that into something so the way that
I'm doing is I'm doing a transform so
every time I get one of those tick
events in this source I'm gonna
transform that nothing into my random
number so this is a function that is now
taking that nothing that I generated and
and creating a random integer and turn
it to a string and so that's my tick
source so that's how I'm actually
generating those those random numbers
okay so I've got my tick source that's
great then what I need to do is I need
to do a transform on that random number
and turn it into a producer record so
the producer record is again from that
Kafka client API from from the Kafka
team and you'll see that it has the
parameters here again for the name and
value types and then we specify the
topic so that's the named topic that
we're going to be publishing these
messages to and then this second
parameter here is the value that we're
going to put into that record so you'll
see there's some other parameters here
on producer record so I'm using that
last one which is just taking a value so
I'm not even doing a key there's there's
definitely use cases where you want to
put a key into a record and there's
these cases where you don't need a key
so so in this case just taking that that
random number that was generated and
turned into a string and passing that
into the value but you'll see some of
the other options that we can specify on
this producer record are the partition
you could also specify the timestamp if
you want and then the key and value so
by not specifying a partition this is
actually going to be written to any
partition
and so the way that that works is
essentially load-balancing so I think
it's going to be scattering my my
records across to all the partitions
that I have in my system so how would
you decide to do your partitioning this
could be done in many different ways you
could do it based on each each web
server node that's receiving that's
receiving traffic receiving some events
being published to it from our T devices
or events maybe each of those gets its
own partition that may be the way that
you do your sharding across the
partition maybe you want to do it based
on what the data looks like so there's
all sorts of different ways that you
could decide how you want to do your
partitioning but in this case I'm just
letting it be the automatic load the
load balanced method where it's just
going to go across to all my partitions
ok so I've taken my ticks source I have
transformed those random integers into
producer records because that's what
Kafka needs and then I have now wired
together that tick source to my Kafka
sync so source is those producer records
the stream of producer records and the
sync is that Kafka sync and then I run
it and so that's what's actually every
500 milliseconds generating a random
number turning into a producer record
and then sending it to the Kafka cluster
in this case just the the one running on
my machine okay so that's that's our
producer obviously you could wire
together any type of producer that you
want and and send those messages in so
so just as an example here okay before I
move on is there any questions about the
consumer or the producer code here
yeah good ok materializer thank you so
they're in akka streams there is this
concept of a materializer
and the materializer has information
about the the akka cluster that it's
going to be using or akka the akka
actor system that it's going to be using
and so the materializer is what kind of
brings all this stuff together and gives
it a place to run
side of akka so this is typically passed
as an implicit parameter but I specified
it manually here but essentially that's
just telling akka streams how to
actually run this this flow inside of
akka or on toppika yeah good question
any other questions about the producer
okay okay so let's now move on to the
consumer side so this is a simple little
play application I have two HTTP
handlers one 4/1 for /ws so the slash
renders the web page the /ws is the
WebSocket so let's take a look at the
controller so here in the controller I
have my slash handler that is rendering
a web page and we'll take a look at that
web page in a second and then I have my
ws Handler and this one is is now
hooking up the WebSocket to akka streams
so let's first take a look at the
WebSocket one so Play Framework uses
akka streams for handling WebSockets so
because akka streams is all about event
publishing and subscribing to events
with a source and sync that works really
well with a WebSocket because a
WebSocket is a bi-directional event
stream right so so this is this is what
makes this this pretty easy to wire all
the stuff together is that plays
WebSocket uses akka streams so first
thing I do in this method is I check to
see was there an offset specified in the
request if there wasn't then I'm gonna
start at the latest if there was and I'm
gonna start at the specified offset so I
look to see was there an offset
specified then I go to my Kafka source
method and let's go take a look at that
one so down in Kafka source this is
where I'm setting up the the Kafka
source with with reactive Kafka so first
parameter here is a topic so the named
topic that I'm that I'm subscribing to
you and then maybe an offset
and then what I'm creating is a source
of consumer record string string so then
I need to create a subscription in the
subscription what it does is it bundles
together some information about what I'm
actually subscribing to so in the case
where I haven't specified an offset I'm
just gonna subscribe to the topic okay
so I'm just going to to say subscribe to
the topic and well we'll see up here
where we're actually using creating the
consumer settings is that I'm specifying
a group ID is just a random UUID so so
I'm not really using the consumer groups
functionality in this case so in that
case I'm subscribing to a topic and it's
given it the group ID through those
settings but if I specified an offset
then I'm going to get a subscription to
a assignment with offset which is using
a topic partition so I still give it my
topic but then I tell it what partition
that I want to subscribe to and I don't
think I mentioned this but partitions
are referred to in integers so this is
what I'm subscribing to you in this case
is the zero tot the zero partition and
so I'm not actually subscribing to all
the partitions in this case and the
reason for that is that I am subscribed
and I want to start at a specific offset
so this wouldn't really work if I had
more than one partition because I'd only
be in this case subscribing to a single
partition I'd also need to track what
partition I need I want to subscribe to
okay but I'm giving it the offset that I
want to to start with okay then I create
a consumer dot plain source and this is
from the reactive Kafka library we're
now I give it my settings and I give it
my subscriptions and now I have a
consumer that now I which is a source
here is the the name for that and so now
I have something that's going to give me
a way to read messages out of Kafka okay
so let's go look at how that gets used
here in the WebSocket so I'm I'm
subscribing to the random numbers topic
I'm
applying the maybe offset if it was
specified and then I'm doing a transform
here so every time we get a message on
that topic and Kafka that's delivered to
us I'm going to transform that consumer
record into a JSON object so this is how
I'm going to send the data out to my my
client in the browser so what I'm doing
is I'm taking the consumer record offset
and the consumer record value and I'm
just putting those into a JSON object
okay now what I need to do is construct
a flow so flow is that combination of a
source and a sink so you'll see that
what I'm doing is I'm actually ignoring
the sink
so because in this case my WebSocket is
only sending data out to the client it's
not receiving them for any information
so WebSocket is bi-directional in this
case I want to just ignore anything that
comes in from the clanks I'm not
actually using that part but what I want
to do is now wire together that web
socket with the kafka source so that
every time a message is received from
Kafka that will be then delivered down
through the WebSocket down to the client
okay so that's my WebSocket let's go
take a look at the web page here so now
some fun JavaScript so here in our app
let's start at the bottom we've got the
the very basic UI there we've got a way
to to show us our status and a button to
pause or resume and then our list of
messages so now in the JavaScript here's
what we're doing we are we have some
global variables here to track things
like the last offset so we get our URL
to our website that was just a page
parameter that was specified and then if
that last offset was not null then we're
going to send that as a request
parameter and that's where that may be
offset gets passed into that controller
function that we saw earlier okay then
we create a WebSocket now here's the
exciting part is that we have a
WebSocket on message function so what
we're going to do is
the data that we've received from the
WebSocket so every time we get a message
from the WebSocket we're gonna parse it
as JSON then we do a little bit of stuff
here to get the offset so we set the
last offset that's how we're keeping
track of where we were in that stream
and then what we're doing is we're
constructing a little Dom object this is
like no jQuery no web framework just raw
JavaScript in HTML but I'm constructing
a Dom element and depending it into the
Dom with the offset and the then the
message the record value okay so that's
why we see those and then I've got some
stuff here to update the status UI so on
open on closed right and then on startup
when the web page starts we set up a few
things connect to the WebSocket and then
have some handling on that button so
that's how that all works let's go check
it out again so here we've got still
receiving messages and when I hit that
pause button that disconnects the
WebSocket updates that that UI but the
messages are still being produced by
that random number producer and then
when I go back and resume now we've just
said Reese absque Ribe at the offset
that I last read so now remember that
this the way that Kafka works is that
there's these partitions and in this
case you may have noticed because all of
these IDs are unique I actually only
have one partition in this very simple
use case I have my one partition and I'm
writing messages to that one partition
and consuming messages off of that one
partition so that's not giving us any
horizontal scalability but but you can
definitely see how if we had if we told
Kafka when we create L Kafka when you
create a part when you create a topic
how many partitions you want on that
topic so that's the way that you set up
the number of partitions so now you can
see alright if I had created this topic
with more than one partition then we
could have our producer actually writing
to to the all of the partitions and then
we'd actually be seen multiple
duplicates
these IDs because each partition has its
own log essentially its own sequentially
ordered list of events okay so that is
our code demo is there any questions
about the producer side of this the
source from Kafka and the producer side
yeah good yeah so we we would have to if
we if we're using consumer groups then
what I've done is I've actually said
okay my consumer group is subscribed to
a topic and load-balanced
the messages across to all the
partitions in that topic
load balance across them for for my
consumer for this consumer right and so
in that that's what this one is
essentially doing is it's doing that
load balancing for the consumer group
for me but in this one in this case
where I've specified an offset is I need
to know the offset for a partition so
the offset is specific to a partition
and in this case I only have one
partition so I'm telling it subscribe to
the the partition zero right so if I had
multiple consumers that that needed to
know this
I'd have to actually program some logic
in here to tell them which partition or
they'd have to know which partition
they're subscribed to so an example of
this would be let's say every consumer
is going to get like load balance to a
specific partition right so I've got
WebSockets and they're all connected but
they're all subscribed to a specific
partition I'd have to know that
information
per client so I'd have to have the
consumer it would have to know which
partition it is actually working with so
there's those are the there's like the
high-level API of being a consumer and
using consumer group and just having the
delivery happen easily and not having to
think about partitions and then there's
a lower-level API where I actually want
to start using the partitions directly
so in this case it's not a good example
of how you would use the partitions in a
partition data set yeah yeah so yeah you
definitely would have to so let's say I
wanted to subscribe to all my partitions
then you're absolutely right I'd have to
keep track of my offsets for for each
partition yeah and so my client code
would actually be different because I'd
have to be tracking for this part too
I'd have to be sending out the partition
information yeah so definitely become
more more complicated in that case yeah
that's one of the things I actually
really like about the the Kafka reactive
Kafka library is that it gives me a
really easy way to get started and then
as I need more features as I need like
horizontal scaling I can start to dive
deeper down there's nice escape hatches
down into the the deeper levels of of
Kafka yeah okay other questions about
the producer sorry consumer that's a
consumer one okay and sorry effect if
you're raising your hand I can't see
everyone very well okay so that is all
the code it is all open source up on
github I have a blog post where I go
through the very simple version of this
so I created a different branch for for
the code that we just saw with the pause
and resume and so that code is on the
dev ox branch so the code that I just
showed you okay so we have about 10
minutes left for questions yeah go ahead
slow consumers yep
yep so you have to deal with buffering
when you're doing slow yep yeah yeah
exactly and that's one of the key uses
of Kafka's because it's doing the
durability of messages it's doing that
buffering for you and so you just need
to keep track well in the case of a slow
consumer that you may not have to think
about this at all you're when you read
when you subscribe it's going to take
you to the latest message if you have
the potential for that consumer to go
down and you need to be able to restart
that and do or do a replay then you have
to get a little bit deeper into the
stuff I was talking about with
partitions and keeping track of offsets
and those sorts of things but but
absolutely Kafka is gonna make that a
whole lot easier because Kafka is
essentially doing that buffering for you
yeah yeah so performance a Kafka that's
I didn't mention it but the performance
of Kafka is phenomenal they have a bunch
of stats on their website where they've
done all these performance tests like
it's it is crazy fast the the
characteristics of adding nodes doesn't
degrade the performance so it's check
them out on the Apache Kafka site but
really impressive performance I don't
know how how they do it but it's it's
essentially like Network speed is what
how fast they've gotten it yeah yeah
thanks yeah another question yeah so
Redis does have a messaging system I
think one of the main difference
differences between Kafka and Redis from
what I've seen is that Redis is is
usually used for for like forever
durable storage they don't have this
idea of okay we're just going to keep a
window of data whether it's a week or a
month or whatever so with with Redis
from from what I've used it for is is
mostly for like forever durable data not
in that
persistence and then the other thing
that is probably I haven't looked real
deep into Redis clustering but when I
did do Redis stuff it didn't seem like
clustering was was a first concern it
seemed like it was maybe more of an
afterthought added on later so I don't
know how they would deal with like
leader follower with partitioning some
of that kind of distributed stuff that
the Kafka is doing out of the box but
for simple stuff Redis message inna is
great it's it's super simple so yeah
good question and there questions yeah
good why are ya so why are the Kafka
clients polling so part of that I I
think is because when you when you do a
subscribe you're actually subscribing to
a specific offset right it's either like
the latest offset or a specific offset
and so from what I can tell they're
doing polling because this allows them
to say okay now give me all the messages
since this offset and then it asks again
like 50 milliseconds later give me all
the messages from this offset and then
again give me all the messages from this
offset and so as far as I can tell that
because they're doing this offset based
essentially query that it needs to be
polling I think the other aspect to that
is that it does allow the the guaranteed
delivery where I'm not going to I'm not
going to miss any messages because I'm
doing that offset based polling whereas
with push like messages it's it's easy
for messages to get lost in a push right
because then you have to have a way of
doing acts like okay when did you
receive the message and when you do push
so so with polling they don't have to do
the the acts and that in that way they
just say I've received these offsets now
let's let's get the messages from this
next offset that sort of thing yeah yeah
question
the Kafka client is doing the pulling
underneath the covers and then the akka
stream and reactive Kafka library are
providing like a nice API layer around
around that for me so they're kind of
transforming that the message pulling
into a source and a sink or in that case
into a source right so they're
transforming that all the polling
underneath things into a message source
that gets an event every time it has any
message yep yeah you can configure there
there is like like 300 different config
you can specify I didn't actually show
that I can show that real quick in the
application Kampf you can see in here my
points settings for my consumer so yep
absolutely all that stuff is
configurable along with all sorts of
other things yep yeah message ordering
across partitions
yeah so there isn't anything that I know
of that would help you help you to do it
do that I think you could potentially
use the time stamps so the time stamps
are a newer feature in Kafka 0.10 and so
before time stamps it was probably
really probably impossible to have
ordering across all the partitions in a
topic but with Kafka 0.10 they added the
time stamp into the record so every
record has a time stamp and so if your
time is is accurate right across your
cluster then you could create ordering
across all the partitions and in a topic
but that's the only way I can think of
to have some guarantees around ordering
across the partitions yeah good question
okay any other questions all right well
thank you so much for coming I hope you
learned something and now I'll be here
if you have any other questions so thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>