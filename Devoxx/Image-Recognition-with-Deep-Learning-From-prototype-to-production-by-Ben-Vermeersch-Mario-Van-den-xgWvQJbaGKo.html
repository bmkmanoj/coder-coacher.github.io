<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Image Recognition with Deep Learning From prototype to production by Ben Vermeersch &amp; Mario Van den | Coder Coacher - Coaching Coders</title><meta content="Image Recognition with Deep Learning From prototype to production by Ben Vermeersch &amp; Mario Van den - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Image Recognition with Deep Learning From prototype to production by Ben Vermeersch &amp; Mario Van den</b></h2><h5 class="post__date">2017-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xgWvQJbaGKo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good morning everyone thanks for
joining us for one of the last sessions
today on the books
thanks for being so numerous it's always
fun to see today me and my colleague
Mario who sits over there who will join
me in a minute we're going to talk a bit
about image recognition how you do that
we developed a product that you can use
to identify clothing may be solid we had
a booth on on the Fox this week on the
exhibition floor and we're going to tell
you a bit about how we developed the
deep Learning Network and how we
developed the interface to work with it
so my name is Ben Vermeer I work for
info farm part of the Explorer we
specialize in deep learning and big data
and AI stuff and EDA they specialize in
all things Web and Adobe but Mario will
probably talk about that a bit more so
over to the image recognition part how
do we do that how do we do you work with
image recognition with deep learning so
what have you built well the first
question was can we build something that
can identify clothing so based on the
images that our feed feed it into the
computer they wanted to see what's on
image is it a suit is it the t-shirt is
the blazer what color does it have just
have long sleeves shortly short sleeves
is it striped or not or plain color so
you wanted to be able to identify all of
that based on the imagery that we got in
but when we built that we notice that it
is also very suitable to build
recommender systems because when you can
identify images and see what's on there
you can use that information as well to
identify similar images and recommend
those images to users so that's what
we've built we've built a prototype
and in the end we've built something
that can be used in stores and is
actually used in stores right now with a
gesture interface that was developed by
EDA before we get into the specifics of
how we built it maybe it's a repetition
for some of you but I don't know who
attended all deep-learning talks this
week but a bit about deep learning deep
learning actually is a special kind of
machine learning we all know the
traditional machine and so what's the
big difference in traditional machine
learning say for example you want to see
if something is a car or another car in
traditional machine learning you have to
extract all the features that identify
something being a car by yourself so you
have to write some logic to see okay a
car it has two tires and it has some
windows and it has a drink and I don't
know what that what else but you have to
extract all those features from the
imagery yourself and then you can use
those feature and build a classifier
system saying this is a car or another
car or this is a car and this is a truck
the big difference with deep learning
systems is that deep learning systems do
the feature extraction for you you just
feel it with a bunch of examples in the
case of images you feed it with a bunch
of cars or trucks or cars or not course
and network itself it tries to extract
features from the imagery and in the end
classifies those features into something
being or not a car and we did the same
thing for clothing but you can do this
for yeah any type of object object of
course
so a neural network network how does
that work exactly
people always say and neural networks
they try to mimic the workings of the
human brain and you train the human
brain to recognize objects so when we
think of brains that still need to
develop we think of babies so meat
mid-step this is my son is about one
year old now and is my nearly my most
advanced neural network
that I've trained two days my wife
doesn't really like me calling him a
neural network but yeah what gives but
my son actually is a neural network in
itself so my son Lee really likes toys
and balls so what is really good at
right now is identifying something being
a ball or not so how does it start again
my son he sees a lot of ball seventeen
okay this is a ball this is a ball this
is a ball and then he tries a pattern
emerges from all those images of balls
and he sees okay a ball it is round so I
know at first thing is a ball and the
second object that's a ball as well
because it's also round and the third
object is a ball as well because that
ball is round but then my son he wants
to play with the ball and it tries to
pick up the last ball and he notices
okay this ball is kind of heavy and I
can't throw around with it and when I
kick it my feet hurt
so he gets a second example or something
not being about and if I say enough to
my son okay this last green thingy
that's not a ball but that's a
watermelon then my son will learn okay
large heavy green things that are round
on a balls they are watermelons and you
do the same thing for training neural
networks on a computer so you feed it
lots of images lots of examples and in
the end the system learns what it's fed
and tries to extract features from it
and it can detect what features belong
to which labels and we did the same
thing for clothing so we gave a lot of
images of clothing to our network and
said ok this is a t-shirt this is a
blazer this is a suit and in the end our
system was able to recognize clothing
and we used some special kinds of neural
network called a convolutional neural
network which actually takes small parts
of the image and tries to express
features from it so it starts by for
example using the left outermost pixels
four by four and then he scans the
entire image and it goes along with it
and it built a new image from those
small features so in the first step you
will probably detect edges and small
objects and and lines and when you go
over it and in the end you add a second
layer and a third layer and you do it
over and over again and then you get
lapels and buttons and really
identifying features you can do the same
thing for example with faces detecting
eyes and eyebrows and mouth and in the
end trying to detect what mood someone
is is in if he's happy or not
same same pattern emerges there so
that's a bit of how the network that we
used works in the background so how did
we train our metal model well by feeding
it lots and lots and lots of images we
actually used approximately 300,000
images of labeled data and saying again
this is this type of building it has
this kind of pattern it is this kind of
color and we used a stack of Gators for
the modeling with the Tiano back end and
sorry to say but that's all python-based
okay I like Java as well but for now
fighting is most suited to train and
work with neural networks you also have
something called deep learning 4j which
can actually use quieres build models as
well so we're looking into that but
there's still a performance hit so for
now we still stick with Python but I
think everyone will understand all the
code examples well we started just doing
CPU based training but we ended up
buying just one performant GPU card
because that goes a lot and a lot faster
so if I can give you a hint if again if
you get started with this invest in a
good GPU card to do your training
because it will speed up your
development stack cycle a hundredfold or
a thousand
so over to some coat how do you create a
model in in Python so the first thing
you do is just you create a new model so
that's the initialization of your model
there and then we say which shape we
will fit into the model so in this case
we had images of 128 by 128 pixels with
three layers a red green and blue layer
if you have only black and white you
only have one and then we add different
layers into the network so the more
layers we have the more features we can
distinguish of course this is this
doesn't necessarily scale or is useful
to scale into a bunch of layers I think
we ended up only using three or four
because every time you advance a layer
you use more and more of your image or
more and more of your features and yet
you end up with your all your image and
then it just starts to get confusing for
the network so we had a bunch of layers
so the convolution 2d layers that are
used in a convolutional neural network
and in the end we add a classifier layer
a logistic classifier that uses the
features that come from the other layers
to say out to classify this is this
object this is a suit this is a blazer
this is a t-shirt this is a dress and in
the end you just compile that model and
then you have an object a model object
that represents the hierarchy of your
model the layout of your model that you
can start training so how do we train
that model by feeding it lots and lots
of images and lots a lot of lots of lots
of labels but how do you do that in code
so we take the model that we previously
initialized and then we use the fit
generator method in this case why a
generator method we can also use the
regular imagery but using a generator we
can create more and more images based on
one on original images so what it does
is it shifts and it rotates and it adds
some distortion to the images in order
to get a better train model the
obviously the new images that you were
getting in aren't always that clean so
that's an easy way to add more and more
data so we feed our RGB data from our
image to the generator to generate more
and more images and the labels
associated with that image so for
example if I have an image of myself I
say okay this person has a blazer and
it's blue and it's plain for example and
then we also have some configuration and
in the end a callback method so what
that callback method does is save your
model to disk well in our case you can
do other things as well so this means
that in the end your train model is
persisted and it can be reused as well
you can load it in loaded from disk
transport it to other systems even put
it on mobile or something like that and
start working on it further you can even
use it as a layer in other models as
well so that comes in handy when
deploying to a server for example and
after doing this for thousands and
thousands and thousands of images and
after a couple of hours and a couple of
days of trial and error with fixing them
fixing the model you end up with a
trained model that you can use to
categorize images and categorizing
images is just plain and simple you open
up your image and you take your train
model say okay predict the glasses for
this new image and then the model says
okay this new image that I get in is an
image of a t-shirt and it's white and
it's plain or it's an image of a dress
and it's a long dress a blue dress and
it has a floral print for example so it
really yeah it looks quite simple of
course there goes a lot of tuning in
developing the right kind of network and
selecting the right kind of images but
I'll get to that later but just code
wise it's actually real simple in Cara's
also takes a lot of the conceptual
modeling out of your hands so in the end
it's it's not as hard as most people
think it's think it is before starting
with it so we can categorize image and
this is for example the output of one
image that we had so if you feed in this
image of this fine lady here and it's
entirely unreadable but it says okay
this is address that's the top one it is
black mainly and it has a floor Oakland
prints the cool thing here is that you
don't only have the first bed of your
model but also your second or third best
bed and optimizing for 100% accuracy on
the first bet is really really hard but
you should also look at accuracy in the
first three bits of your model because
it can also be used in products for
example if you want to use this to
automatically categorize imagery or for
you have a new catalogue in your store
that you want to put in your system
taking you can use the system and the
bulk of images will be right from the
first try with the things that you're
uncertain of you can feed to manual
operator who can check it and you can
also present him him or her with the
second and third options that your model
came up with so to make the selection
easier so now we've built a model and
that model can detect what's on an image
so the next thing we wanted to do is use
that model or the features of that image
to recommend similar clothing so how do
you do that how do you tell your model
what is similar and what is not one
thing you can do is train a model
separately for detecting similar items
so it will work kind of in the same way
feeding two images and saying okay these
images are similar and then the model
detects what is what are features of
similarity but what we did is kind of a
different approach we used a traditional
model of nearest neighbors to detect
similarity in images because the model
it comes up with a bunch of features
I think thousand-something features that
we have for every for every image and
those features that can be used in a
traditional way using traditional
machine learning recommender systems to
do recommendations and find similar
feature vectors so what we did is we
just got rid of the last step of the
model you saw previously in the slides
which was the classification step of our
model and if we get rid of that step you
just end up with a lot of features of
that model and we can use those around
features and put them in a feature
matrix and then whenever we get a new
image that we want to find similar
images for we just get the raw feature
vector of that image find the nearest
neighbor or the most similar feature
vector for that image in our catalog and
then we have a similar image based on
the catalog that we already trained so
how do you do that you take the
truncated model is the original model
that we had for the classification with
the last layer omitted
that's a truncated model that we created
and we use the same predicts method to
get the feature vectors now not the
labels but the feature vectors for the
image and then all of the speech
features they go to into a feature
matrix so for example you have a catalog
of thousand images you have a thousand
feature vectors vectors right in a
matrix and then we can use something
called the SCI file library in Python to
just build a tree and use that tree to
find the nearest neighbors so it's a
simple as squaring that tree with the
new image features for a new image that
we take into our systems and say okay I
want K nearest neighbors so for example
10 recommendations and you get 10 new
images in your system so to give you an
example the same lady here if we feed
that into our catalog we get these
recommendations if we get to the 8th
first one so that's why we see we all
get dresses they all have floral prints
or less insane color of course the lady
itself is also taking into account a bit
so the blonde hair counts as well but
yeah still a work in progress so what
have we learned by doing this first in
all use your GPU it makes your
development cycle a lot and a lot faster
you don't have to wait for three days
for your new model to train the last
percent or the marginal winds are really
hard as in every machine learning model
so you will first have some quick games
but then really specific cases can
become quite hard and a quite important
one is that the only thing your model
will know is what you what you'll learn
it what you train it for and you have to
be really careful not to introduce bias
into the model I'll give you an example
if and that's actually the case for
example for Blazers every blazer
in our training set is a blazer with a
close button which means if I take a
picture of myself with the close button
like this the model will detect okay
this is a blazer but if I take a picture
of myself with my blazer like this then
a model will think this is a cardigan
because the lapels in here are really
cardigan like and cardigans are all
opened up in our feature sets so if you
want to detect both open and close a
Blazers you will have to feed in imagery
of both opened and closed Blazers
because this is the most distinguishing
feature for a blazer the same goes for
suits for example the only thing to cut
the only way to categorize something as
a suit is when you wear a tie if you
don't wear a tie it's never a suit it's
because everyone wearing a suit in the
training set had a tie and the heart
that and that's a great distinction
between deep learning and a traditional
machine learning is that the deep
learning model it's choose the features
itself and you don't really know what
those features will be okay there are
methods to see what the features are you
can
the convolutional neural networks to
visualize them even but still the
training set is really important for the
outcome of your model and the last thing
he Nyx's in pandas it's a data frame
library in button they're really really
hard to work with so I still prefer Java
or Python even after this one so now we
have a fully functional recommender
model but of course you have their kind
of back-end guys so we didn't really
know how to build a nice interface for
this so we contacted our colleagues from
EDA and saying okay we have a very cool
model that we want to use in stores but
we don't yeah yeah you have to take
pictures so you have to take some
distance from the screen so we're
looking for a cool interface to work
with to put in stores and that's what it
has developed so I'll leave the word to
my colleague Mario to talk about the
interface that we built before this
model all right
hi everyone everyone in the room and
everyone watching us on YouTube so yeah
my name is Mario Van Daan and I'm a web
park attacked over at in a media foundry
we are everything web company and we
also do adobe experience manager but I
want to talk to you about how we came on
building the whole front end and pieces
of hardware that info firm used to build
a whole experience but I want to start
off with this guy if you don't know who
this is this is Tony Stark Tony Stark is
a billionaire philanthropist and he's
also iron man when I first saw the first
Iron Man movie I was fascinated by the
technology that he had now I know this
is not real
I know man cannot fly in suits like that
but he uses very advanced technologies
like hologram
so for example if you look at this he
uses his hands he uses gestures to
interact with content that is floating
in mid-air and I was actually wondering
how can I achieve the same experience
using hardware that is available for me
now I'm not a billionaire so I can buy
all this stuff so I wanted to find a way
that I can build this myself affordable
with the technology and the scripting
languages that I know so here you have
another example so he uses gestures to
rotate stuff and zoom in and zoom out so
I wanted to ask myself yeah how can I
for example represent a click if I see
things float in front of me how can I
select things how can I move things how
can i neva gait through content that is
actually not touchable so the question
was how can I achieve this and using
programming languages that I know and
what's the hardware available so I
tested a lot of sensors all of them from
webcams to version ones version 2 of
hardware devices motion liebe hardware
devices but one of them stood out above
all and that was the Microsoft Kinect
version 2 not version 1 and the
development kit that piece of hardware
did actually everything that I want it
now what did I want it measured up to
half to almost 5 meters so that's quite
amount of distance it can track up to
six bodies so if six people are in front
of the camera he can detect how there
are six people in front and that I can
interact with furthermore it can track
up to 25 joints and that was very
important and you'll see that in a later
in a later slide while it's very
important and you can see them on the
right from all the way to the head all
the way to the
every joint can be detected so that's
very important
it can also track the hand state more
and more on that later but also yeah I
want to know where my hands are in in a
special place in time also you have a
unique a unique skeleton ID which means
that every person that's being tracked
stays or gets the same ID every time so
we knows that you are person a and if
you move to the left or the right
you notice that you are still that
person ain't moving it goes up to 30
frames a second which is more than our
eyes can see
it can measure depth which is also very
important so I was already thinking that
I can if it can measure depth I can move
things in 3d space and it does face
tracking any motions now I haven't
tested this yet but in future project
this will come in handy
so basically when I started I was like
yeah I need this so I saw it I bought it
we currently have for at the office
that's how addicted at least I am to
this piece of hardware so the setup is
actually I'm very very basic so you have
the piece of hardware the Microsoft
Kinect we have a very high-end desktop
computer to hook it up to and then a
Samsung display that we can put in a
portrait mode so that we can actually
see our whole body so this is all very
basic and all very affordable so the
hardware yeah ok checked we bought it we
hooked it up it worked what was the next
step yeah how do I get the data from
that little box onto my to the desktop
and into my application so yeah how do I
get that data
the only programming languages that I
know is HTML CSS JavaScript
I don't know c-sharp I don't know dotnet
I don't know Java or Python and anything
it had to be JavaScript so fronting
twice this was a no-brainer for me I
used this day in day out so you used es6
with a wack backpack bundler a react
front end and a babble transpiler so we
had this off the shelf this was set up
in five minutes
and then I started thinking okay what do
I have to use as a back-end since it's
JavaScript I had to use nodejs
I know nodejs but then there still
wasn't a connection from my device to
the Microsoft Kinect so I had my fingers
crossed and I searched the web for a
node module or an NPM module that would
actually do this for me I didn't think
there was already someone in the past
that was crazy enough to actually do the
same thing but yeah thank God there was
so there are on the web to github repos
actually the second one is a fork from
the first one but we used the second one
because it uses a different edge in this
case edge dot J s now in short what's a
net it can just connect two nodes for
example you have no js' a net you have
two processes and it gaps the bridge
between the two of them see your nodejs
environment can call net functions and
vice-versa so in our case it had to run
a c-sharp commands from nodejs because
if you check out the repo you have
access to the code and it was in c-sharp
so all right fingers crossed it worked
and eventually it did work so I had my
hardware I had my setup I had a node
module that hopefully worked awesome so
I spend my days figuring it out saw the
first sample application and this is
actually the only pieces of code that I
had to create my hello world app so what
you see here if you are from
with web technology you import your
connect to you instantiate it
if the connect is found you see all the
lights go on you see all the output for
example the console open it says ok your
connect is connected and basically what
this piece of code does it just outputs
Jason Jason information to the console
and after five seconds the application
stops and if you get the console log
that lists all the bodies then you know
your connection works and now it's just
a matter of a way to find a way to get
that data into your application so if
you look at how this Jason looks like so
this is body index 0 up to 5 or 6 this
basically stretch the tracking ID it's
unique
as I said earlier and then the hand
State the hand State it can go up to 4
states so when it's closed Allah so not
tracked open and unknown if your hands
are for example out of the field of view
now the lasso tool that's something
different I didn't know what it was I
had to google it also but basically a
lasso tool is if you know we're all
familiar with computers of course if you
have the little index that goes like
this
basically the lasso tool is adding a
second finger now why is that basically
because the device measures depth one
finger isn't big enough to bounce the
race of light off so you have to use a
second finger to actually see the state
of the hand so that's basically it and
then you have all depth-x
colors the cameras orientation it can
detect if your hands like this like this
your head goes left to right stuff like
that so now this was just basically
finding the right coordinates to do the
math with so the hardware check getting
the data also worked now creating an
awesome app during a scientific
in Italy so while my girlfriend in the
next room was packing all of the bathing
suits and slippers I was packing my Mac
my my iPad my everything piece of
devices that I need and you know I was
up and running so basically what the
idea was I wanted to create an airplane
in the browser that I could control with
the Kinect so basically what this means
is if you stand in front of the device I
asked my little niece like how would you
portray flying an airplane an
immediately she goes like this you have
two wings okay so I was already thinking
I have these joints I can measure the
joints and how would you go up and she
bend their knees and the airplane went
up and then the the the next issue arose
yeah how do we make it go down we cannot
float yet into midair so that were
things that I need to tackle so the
steps I need to take our had to learn 3j
s 3j s is a 3d library it's very
powerful but I had to learn things like
geometry lighting how to load external
objects how do cameras move and interact
so I downloaded an e-book and started
reading the neck and the next thing is
how do I get the data from the Kinect
into the application because now it's
just still running on the back end on a
separate server and how do i met my body
that's actually in the physical world
how do i mak map these coordinates to
the coroner the coordinates in the
digital world these were all things that
I need to tackle to actually make the
plane go up and down left and right so
putting it all together
yeah I had to debug basically I had to
draw myself as a skeleton on a canvas
all right next I had to put a place an
airplane into 3d space I just
it's somewhere at an object of a little
airplane placed it in there and if it
showed up I was already happy and then I
had to make it fly those were actually
the three steps so this is what it
looked like so as you can see this is
actually skeleton data every red block
is actually my head my shoulders my
upper torso my waist my hands my my legs
my feet are not in there because they're
not important at this moment but
actually at the state also from the hand
can also be checked so Green was open
and blue was closed so basically this is
how this looked so this is me standing
on my Terrace on on our holiday home in
Italy surrounded by tens of others
people I don't know looking at me like
what the hell is he doing
my girlfriend sitting next to me reading
a comic book because she's addicted to
comic books and I was like it worked it
worked and she just lowered her comic
book going like what what are you doing
and then I explained everything to her
how this works in the end she thought it
was cool again so basically how was
dismayed actually if never did narrow it
down it's very simple so I had a local
host running on separate port that was
connected to the the device itself
giving me JSON data and every time a
body frame event occurred the device
just sped out events called body frames
everytime that event occurred I just
pass it along a socket and then I had
the front-end listen to that same event
on the same socket and then the data
would be passed sit it's very easy
actually so now I had a bridge between
my back end to my front end the data was
coming in so now it was just a matter of
of math and an JavaScript
so this is actually the piece of code
that gets the data in I clear my canvas
because the canvas gets removed and
filled in with all the red dots I loop
over all the bodies and if the body is
tracked at this point only iring was
crazy enough to stand and do things like
that
so there was only one body at this point
I created the little red blocks and then
I have a method update hand state that
would control if my hands were closed or
open and then the elbow state and the
elbow state was actually the most
important function because what I did
was if I stand like this
I had the coordinates of my elbow on the
ref on the left and on the right and
then I would measure the distance
between this and my center point like
this so if I went like this the plane
would go that way if not the other way
so that's basically the way that I
tackled this and this is actually how
that looked like so if my elbows are
practically on the same line we're
leveling the plane just goes straight if
my left elbow is higher then we go to
the left otherwise we go to the right
and we output it to the screen and if
the jet is loaded we just rotate his z
position and we're gone now we had
everything I tackled everything it was a
proof of concept and yeah it worked so
how could we translate everything that
we know into the clothing analyzer yeah
again what is a gesture how can we
navigate through the application how do
we define a swipe and how do we
translate all of this to code for
example a mouse click mouse click we can
interpret that in many different ways
for example do you go like this
do we use the whole lasso tool or do we
implement something that if you have a
long enough over an element that's
on-screen we would show a little
indicator and if the indicator is fully
filled then a click would occur or would
we go like this and if a fist if your
hand is open make a fist and open again
is that a click now we had so many
different opinions that basically the
only thing we could do is ask people to
come stand in front of the camera and
just start doing things intuitively or
naturally and basically the first result
was something that I called the Beyonce
every person and literally every person
that tested the application they went
like this really so they asked they
asked us how do we click and the element
like really like this just to tap but
then we explained yeah it's not working
like that so we implemented the whole
circle thing if you have a long enough
over an element so yeah the version 1 we
know how to code the gestures it works
so we measure the distance from one
point relative to another point but
there were very yeah there were a lot of
issues actually is the UI intuitive
enough so practically not the app
started lagging after a couple of
minutes and really really lagging at a
certain point it would ultimately freeze
and we had to reboot the whole
application and the third-party
libraries that we included couldn't be
wrapped into an electron app if you
don't know what an electron app is
basically it's a wrapper that enables
you to create web applications and build
them as a dmg or an XA or something
that has a really stand-alone
application and inside a note
environments gets gets pond and a
chromium environment for your front end
but yeah of course we built a version
too
what we did was we connected the connect
to a separate work a separate worker j/s
which was basically running on a
different thread it would still send the
body frame information we compressed the
data image because it was full HD it was
too big so we compressed it and we were
in control of the frames per minute
instead of actually running code every
color frame or body frame event so we
were in charge and we had to rebuild the
connect libraries that we used to a
specific electron version this was
actually the most time consuming and
thank God for Google Stack Overflow
eventually it worked we got it up and
running we made a standalone application
and it's still working today so this is
basically the setup of how the
application works we have the electron
wrapper in the node environments
services get spawned to upload photos to
s3 and the work js2 connect to the
Microsoft Connect and there is an IPC
main and an IPC renderer basically
that's the bridge between what the user
sees and what's running in in the
backend and you can send events over it
just to ask data and get data synchrony
or asynchronously and then the chromium
web browser is basically yet a chromium
web browser deaths just spawns up the
HTML running the react application and I
everything is up and running so these
are a few screenshots of early UI you
had to tell the the application that I'm
male or female once that was done you
get a little a little text that says
this is how the application works
so you can swipe so for example we
detect this hand position in time and
space relative to this point and if it
was past that during a second or two
seconds it would swipe left and this
would swipe right and then if if the
application took your picture it was
uploaded some magic thing happened in
the cloud and we get different versions
back in a JSON array and we just made it
look pretty on screen and then using
swipe gestures you can navigate to the
next and to the previous one but also we
had to know if the results were accurate
or not so the last screen was did the
results that you were given where they
actually were they accurate yes or no
and after 10 seconds the screen would
just disappear you jump back to the
start screen and the old flow just
starts over again so basically that's it
I want to thank my girlfriend for loving
coming books when we're on holiday so I
can do cool stuff like this the
colleagues over at EDA media foundry
especially Marco Lambrix who's actually
the brain behind all of this for coding
it figuring out how the swipe thing and
math Laura the winter and us tits on the
hustled who are our creative geniuses
and everyone would test at the
application and there's one more thing I
started the presentation or my part at
least with the whole Ironman thing he
has one other awesome piece of
technology and it's called Jarvis if you
don't know what Jarvis is I suggest you
google it we are working on it and maybe
this is something very interesting that
we can do next year this is mandatory
we're hiring so also Ben and I info farm
and Adhamiya foundry so this may be one
thing that I want you to see how does
big
um can you see this yes this is actually
a proof of concept that two students of
our did basically using the same
technology you can configure your
clothes for example a t-shirt a head
your trousers and your shoes just by
going like this I want to select the the
row and if you have selected a style it
gets mapped onto your body these are
these are all very low poly models but
if you have very detailed models
basically you can do just stand in front
of a digital mirror or something like
that or even in your living room and
just dry out clothes without even
leaving home so what they did is they
embedded functionality to control the
lighting the size stuff like that at
some point I even think there's
something like a dress in there or
something yeah all right so yeah so this
is Tess our male model so that at this
point you can already see that the
lagging is occurring so this is so as
you can see it's a little lagging at a
certain point in just it just died so
this is something that I worked on
during my free time a lot of a lot of
cool stuff is happening in the future
so all right thank you all for listening
and yeah enjoy the rest of your day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>