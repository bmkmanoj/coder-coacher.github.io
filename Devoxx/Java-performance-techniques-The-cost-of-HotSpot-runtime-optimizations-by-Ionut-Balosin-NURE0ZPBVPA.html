<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java performance techniques  The cost of HotSpot runtime optimizations by Ionut Balosin | Coder Coacher - Coaching Coders</title><meta content="Java performance techniques  The cost of HotSpot runtime optimizations by Ionut Balosin - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java performance techniques  The cost of HotSpot runtime optimizations by Ionut Balosin</b></h2><h5 class="post__date">2017-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NURE0ZPBVPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello everybody and welcome to job
performance optimizations talk today we
are going to discuss about some of the
optimizations that happens during the
runtime inside the hotspot in particular
but all the things that we discussing
here are also valid for the open JDK
because it's mainly the same source code
my name is jonas voloshina I'm currently
a Software Architect for luck soft
overall 10 plus years of software
development experience in particular
very keen on performance and tuning
topics plus software architecture I use
all the technical trainer inside the
training Department in NOC soft and as
well conference speaker from time to
time I write some articles on the
performance of the architecture our
agenda for today it's a bit contains a
lot of topics we'll start with
just-in-time compiler just in time of
briefing introduction about what means
the JIT and after that we'll go through
a few optimizations starting with
uncommon traps no sanity checks and
we'll see what's happened during the
runtime in regards to these
optimizations so how it behaves to
benefit of the best optimizations
watching for you to take away out of
this talk it's a better understanding of
all about what happens inside the the
hotpot virtual machine from the
just-in-time compiler standpoint as well
you will learn few just-in-time compiler
techniques how how it triggers those
optimizations and at the end we'll go
through some cases when just-in-time
compiler it's not able to compile
anymore the code so we will see some
corner cases which prevent the
just-in-time compiler to optimize our
code in a nutshell it will be useful for
you if you want to tune to squeeze the
ultimate performance out of your java
application few things about my
configuration because during this talk
we'll study a lot of we'll go through a
lot of tests so a few things about my
hardware configuration I was using i7
skylake processor
with 16 gig of ram from the software
perspective all of the tests were done
on Ubuntu but of course I used as well
her windows just for the sake of this
presentation the JDK was the last one
the last efficient 1/8 1.80 update one
to one and I was using JIT watch tool
and HP jmeter just to prepare the talk
ok so few things about just-in-time
compiler it's a very brief introduction
just to put a bit the context what we
have during the runtime it's it's mainly
the JVM with the execution engine the
execution engine made of the interpreter
in just-in-time compiler of course there
is a class loader get wish collector but
it's not our interest for today we were
mainly focus on the just-in-time
compiler for today just-in-time compiler
if you have if we have to zoom in a bit
it's made of the client a c1 compiler
which triggers only rock-solid of the
optimizations based on on the invocation
threshold which is around 1.5 K
iteration so it's around 1000 F 500
iterations the second one is the server
one which does better optimizations in
regards with c1 but it needs a higher
threshold so it's around 10k iterations
to trigger and to compile the code and
as well we have a tiered mode which is a
mixture between c1 plus c2 and it has a
lot of tears it starts in interpreter it
goes in C one without compile our
profiling with basic profiling with full
profiling and it reaches the second one
if we have to take a look over the speed
of course in the interpreter we have the
lowest performance speed out of the the
java code that we wrote initially
because many the bytecode is interpreted
the second level i would say is the c1
in the c1 we might have around five
times speed improve
month in comparing comparison with
interpreter and the best one is the situ
in in some cases it might reach like
hundred of times more than the
interpreter but of course it does a lot
of assumptions it as well do a lot of
the optimizations optimizations so it
needs more context by context I mean
more and more number of iterations all
of these being said just a bit of
context let's dig in into the
optimizations one by one to see what's
happening under the hood and what
just-in-time compiler does in order to
do that just as a remark during the next
optimizations we'll study will take a
look over the Java code and the assembly
code why because the assembly code it's
it's more interesting based on the
assembly code we will figure out what's
really happening under the hood and for
us to have a better understanding about
these optimizations we definitely need
to understand the the native code we
will skip for this talk the bytecode
okay the first optimization or technique
I would say it's more a technique it's
called uncommon trap
what's an uncommon trap it's mainly a
situation which is unlikely to happen
inside the during the runtime intra
inside the hotspot which means that
just-in-time compiler treated as that is
as a check if this check is happening it
bangs back into the interpreter that
this is a mainly an uncommon trap it's
mainly a situation unlikely to happen
but when it happens the just-in-time
compiler bails back into the interpreter
to understand it better let's see this
very trivial example what I did here I
created a loop inside that loop iterates
through the loop 20k times yes and
inside the loop I call a hot method all
the times with true the hot method it's
like in here it receives the boolean
and based on it if it is true it returns
one otherwise it returns true and
returns the counter why I chose here 20k
iterations because I wanted to be sure
it reaches the sea to the second level
and as well to benefit out of of the
best optimizations during the run time
and I took out this code and I launched
my JVM in here with print assembly print
assembly says for the JVM I want to
print my assembly during the runtime and
of course I want to print the assembly
in the intel syntax there is as well
AT&amp;amp;T syntax but for me I'm more use the
Intel it's mainly the same but is just a
matter of representation and I disabled
the tier compilation which means in our
context I was using only see - I don't
want to go through the c1 compiler I
want to reach directly the c2 I'm not
interesting in all the intermediate
optimizations only in the final ones and
of course the last one it's mine my
class and if we scroll and zoom it a bit
into the assembly code of course I
really simplified for this talk just to
be easier to be understandable what we
have here we have the hot method which
receives my boolean it's dasar DX it
compares the RDX with one if it is not
equal we see here it does a jump but if
it is equal it returns true which means
if my method receive one written returns
true but if it is not the boolean it if
is false it does a jump and this is the
jump to an uncommon trap
what's an uncommon trap and how it works
because I called my method during the
runtime only with true the hot method
was called only with true 420k times the
just-in-time compiler assumed that there
is an unlikely situation to be called
with false and just-in-time compiler
added an uncommon trap
and uncommon trap it's mainly a call to
this to this address when it happens
just in time compiler locks the method
Marquis marks us not being entrant and
switches back in the interpreter if so
based on this we can see that JIT
just-in-time compiler optimized at the
moment only for the true true branch and
for the fourth branch inside the code it
put this uncommon trap if we get back to
our initial snapshot of the code we see
that we have the hot method called only
we threw it gutka optimized it got
compiled and if we have to measure just
one iteration after it reached the c2 on
my laptop at least he took around 300
nanoseconds but the question is what if
I call my hot-metal it falls just
immediately and immediately after
calling the method wit falls just
immediately after and measuring the time
he left we see that one iteration took
like 100 times more than the previous
one and it leads me to the question why
why what happened under the hood inside
the JVM from the just-in-time compiler
perspective to have such big difference
between these two calls just to call him
in in a sequence to spot out what
happened we have to redo the test
relaunch the test mainly and this time I
was using print compilation print
compilation because I'm interesting in
all the things that happens under the
hood I want to see what was compiled
what was what was disabled for the
moment and if we check the output for
the hot method in here we see these two
lines but what it means because apart
the trivial methods we see a lot of
other information the first column
corresponds to a timestamp it's mainly
the
of milliseconds since the JVM has
started the second one it's mainly a
compilation ID for the just-in-time
compiler the third one that percentage
it stands for on stack replacement
what's on stack replacement for example
if you have a method and inside that
method you have a long-running loop that
long-running loop could trigger the
compilation for that matter this context
is the context of on stock replacement
what happens during the wrong time the
the the stack frame it's replaced by a
negative stack frame on the stack the
another thing is this around or around
four which is mainly the bytecode index
which triggered the on stack replacement
so it's corresponds to this percentage
and the number of bytes that got
compiled okay so getting back to to our
scenario after this explanation we can
see that when we trigger the method with
true it became compiled the hot method
initially became compiled and because
and as well in line and because we have
a long-running loop inside our main
method the main method itself was
compiled and this compilation was
triggered from the on stock replacement
perspective what happened afterwards
immediately when we call just once with
hot method or false we see here this
mark which is saying not made not
entrance which means all the assumptions
all the optimizations are made
previously when the method was called by
true are not valid anymore so mainly the
uncommon trap was hid and the method was
bailing back into the interpreter that's
why that's why there is that big
discrepancy like 100 times more when I
called once with false in regards with
the previous one if we have to
illustrate a brief from from the threads
perspective what's happening during the
runtime in
surely we had our hot method while
starting in interpreter after a certain
number of iterations the c2 compilation
level was reached the method was
compiled but when we called with false
parameter we hit the uncommon trap which
means at the wrong time we bailed back
into the interpreter that's why we had
did that we had that big discrepancy
between the time elapsed the question is
what happens for example if there are
other threads running the same version
of of my hot method the answer is that
if other threads run in the same time
the same version which was compiled and
if they don't reach the uncommon trip
they continue the execution but if I
have another thread like in here which
calls my hot method afterwards after one
of the threads reached the the common
thread
it starts from the interpreter from the
very beginning so this is the
explanation what's happening and during
the runtime now what what I did I
continued my experiment and I optimized
with true and false I also included hot
method or falls into a long-running loop
and I checked the compilation and in
here what we have it's the this the
first two lines are very similar to the
previous one which means we had the hot
method it was compiled we have here an
unsightly placement for the main methods
of the entire main method was compiled
and when we trigger the hot method just
once with false it became not entrance
but because we have this long-running
loop in here it became again optimized
and it was compiled and as well it
triggered again to the compilation for
the main method so as you can see during
the run time if you are using the c2
there are a lot of optimizations the
optimizations just-in-time compiler its
eager to
a lot of assumptions just to make the
better optimizations for our code and
going further if I have if we have to
take a look over the assembly if I have
to run with print assembly as we did
before we see here in this case we have
this C conditional a conditional move
not equal which means in this case after
we hit the uncommon trap the
just-in-time compiler falls back to the
traditional way of handling disk dis dis
checks so if the just-in-time compiler
has the context of being able to reach
both branches branches for true and good
branch for false it doesn't do any more
uncommon trap and it switches back to
the common way of handling this we using
many jump instructions that's about in
common traps some sort of summary for
you so can if a conditional statement
was following only by a single branch an
uncommon trap it's a good idea to
optimize this during the runtime as we
saw the thing is that once the uncommon
trap eats its heat the just-in-time
compiler have more context so in this
case it doesn't do any more uncommon
traps and just to summarize the uncommon
traps they they are a good solution to
optimize the code during the runtime but
since they are heat while they are well
while the credits hitting the uncommon
trap it's costly it's costly because it
has to do my zit has to bail back into
the interpreter and start the execution
in that interpreter
inside the inside the hot spot there are
a lot of uncommon traps situations and
for this talk I took out of them done
the null checks how hotspot handles the
null checks for this I took this code
again a hot method the same method and I
calling with an integer which is not
known and inside the hot method I say if
my if my integer it's not null
multiplying a returning result otherwise
return zero so I have this null check
what we have during the runtime during
the runtime again I started my program
when I went the JVM was triggered I said
print assembly because I want it to
check how it behaves from the native
code perspective and as we can see here
the assembly code is very very simple
it has mainly after I clean it a bit it
has this move and this EAX multiplied
this by topper ations a bit operation
sorry
which means mainly put the value from
the integer in EAX and does the
multiplication by 2 there is no null
check explicitly added in the in the
assembly code why because as we can see
here there is an implicit dispatched to
this to this address in case there is
something wrong what could be wrong it's
mainly a segmentation fault imagine the
the c or c++ what's happened when you
call something which is null you got a
segmentation fault it's implicitly
handled by by the hardware and in this
case once the segmentation fault it's in
here we have a null handler for our and
common trap and
again it bears back into the interpreter
and starts from from there so in this
case in this case of handling notes
inside the code there are no explicit
checks instead the just-in-time compiler
leverages on default oils signals
segmentation faults for example when a
segmentation fault
it's happening it is caught by by the OS
it is handled by the JVM and inside the
JVM it is for example there is a
decision if nullpointerexception it's
wrong for the programmer or
if it is not the case as we see here
because we have an explicit you know
check in our code it bears back to an
interpreter in starts the execution if
we have to optimize a bit more so we
call the method with null and after that
immediately 20k times with a value which
is thought null and we have to print as
well the assembly we see here the test
it's explicitly added what it is in here
mainly we test this RDX which is the
parameter our integer received if it is
not if it is now there is a jump in here
for our zero zero one label and in here
it sax or between the same register
which means return zero basically and we
have a jump to another label which is
the return of our method but if it is
not if it is not null it's the same code
as we did previously when the null was
implicitly dispatched to that address so
this is the way of of relying by default
on on signals inside the JVM
particularly for this case of
dispatching or of handling mainly Donal
checks inside the JVM but after the
moment that our code reach the the null
there is no any more uncommon trap as we
see here it's next
check just to sum up null checks relying
on signals are natively handled by
hardware and as well by OS and very well
handled by the JVM but in case they are
triggered of course it's costly it's
costly because you have to pay the cost
of of catching the signal the
segmentation fault signal handling it
bailing back into the interpreter and
continuing the execution and as a
conclusion
don't let now happening a lot in your
application because especially the first
time you'll have a lot of segmentation
for signals triggering under the hood
and after that after the segmentation
faults are triggered the codes
recompiled by this time with jump
instructions so are there in inside the
code there is those test conditions
which might be costly as well
another kind of optimization it's called
the virtualization what's the
virtualization it's mainly a technique
to turn polymorphic calls into direct
calls what it means for example on the
right side we have a parent class which
is called C mat which has a method
compute which has an which receives an
int and it is extended by l1 this l1 is
the first implementation it's mainly an
extension of this parent class and if we
have to test it I put this in a
long-running loop and inside the
long-running loop I call the hot method
in here and inside the hot method I say
math dot compute and what it does
internally method computed returns the D
I multiplied by a constant so very basic
implementation and afterwards I started
my program again we print assembly
because I was curious what's happening
one how just-in-time compiler handles
this these virtual calls because many of
you are call it's a polymorphic called
and if we zoom it inside the assembly we
see very simple things here it's mainly
the first line it's mainly a conversion
and on the second line it's mainly the
multiplication so it just received
something it does the the conversion to
double precision and afterwards it does
the multiplication and it puts the value
inside this register and it is returned
back afterwards as we can see there is
no check if my instance during the run
time is of type l1 so which leads me to
the conclusion that this case of
polymorphic calls if they have just one
single target which means the single
single target monomorphic calls they are
in line and the just-in-time compiler
doesn't add any uncommon trap for them
let's see if we go further what we have
in case we add the second implementation
and as well win we enrich our test by
this instance and this instance called
the same hot method in in the loop wrong
running loop if we if we check the
native code it's a bit more complex a
lot of complex I mean more instructions
added and in this case of two targets
two possible targets invocation
just-in-time compiler adds explicitly
the check what did what it has what it
does for example it checks if the
instances of type l1 which is in this
compare if it is of type out l1 it jumps
in here and it does the multiplication
after that if it is not the case it
compares with elk - if it is an instance
of l2 and if it is not the case it jumps
in here and it triggers the uncommon
trap so mainly if there is an instance
of lot
I'll go on neither L - I have to trigger
the uncommon trap but getting back if
the instances of type l1 it does as well
this multiplication so mainly in this
case of by more feet calls I'm to target
calls during the run time there was in
lining which was occurred so the
just-in-time compiler Didion lining and
as well added an uncommon trap for the
possible third or fourth extension of my
parent class let's see now let's let's
go further with this and let's see what
happens if we extend with the third
implementation the for the same base
class and as well we enrich our test and
we call the test or with hot method with
our elk tree and I in this case if we
bring the assembly it's very very simple
it's simple because it it contains just
a virtual call to the hot method which
leads me to the conclusion that the the
megamorph it calls megamorph it calls
are mainly the call starting with the
third implementation the just-in-time
compiler neither in line nor do any
uncommon trap it just does a visual call
which is even more costly than in case
of biomorphic calls and monomorphic
calls if we have to iterate a rate on it
monomorphic calls are in light without
any uncommon trap biomorphic calls are
as well in line as we saw but the reason
and common trap added in the code just
the just-in-time compiler adds a check
if if the instance is not of type r l1
neither L - I - Guardian common trap and
megamorph recalls are handled as virtual
calls the just-in-time compiler doesn't
do any optimization it just triggers the
individual calls in reality what's the
cost from for example from one
monomorphic to megamorph recall it's
around several nanoseconds i would say
during
my test I spot it out as being like
three five nanoseconds
so anyway it's it's very performant in
any case but you should you should have
this this idea in mind that once you
reach the third and the fourth and the
sixth implementation it becomes more
costly because it has to resolve the
virtual calls during the runtime or upon
each each method call the next one it's
called loop unrolling what's loop
unrolling it's mainly a technique to
reduce the number of of jump
instructions if we have long-running
loops for example what we have in here
we have a method and inside my method I
computed the sum of this array and at
the end I returned the sum of the array
if I have to compile and see the print
CD assembly as we see there are a lot of
additions a lot of additions inside this
loop is from here to here and the
counter it's the counter increases by
eight mainly which leads me to the
conclusion that the just-in-time
compiler did all loop unrolling and it
rolled the loop by eight iteration in
the same time this is a very powerful
technique because for example in case
you don't have data dependency between
all of these addition statements there
might be paralyzed and run in parallel
and another important thing is that as
you see in here it starts from one even
if my initial sample case at the java
code level i started from zero which is
another technique it's called loop
peeling so what mainly does the
just-in-time compiler over there during
the runtime it peels out of the loop
only the first iteration to do some
checks for example it could be useful as
well for memory alignment and starting
from starting putting aside the first
loop iteration it iterates and
paralyzed the number of additions and as
well in here at the bottom if there are
there is some remaining for example if
my array it's not multiple multiple of
eight it triggers here the the remaining
out of that list which and if I have to
put it in the cellar door code it looked
like this the loop peeling it's the
first iteration and from the counter of
one it does eight additions in parallel
and here at the bottom it triggers the
it handles the remaining of the the some
operations which leads me to the to the
question for you because we have now we
are aware we have in mind this sort of
loop unrolling technique would it be
useful for us to be a bit smarter in
advance for example having an array and
instead of iterating one by one to do
the sum to put like for for additions in
the same loop iteration what do you
think would be a good idea
mm-hmm okay any other guesses
it could do better
yeah so thanks let's see how how it does
what how the just-in-time compiler deals
with this some sort of eager
optimization in advance from from the
programmer perspective
I don't print again the assembly because
it's a bit more complicated
I try to translate it into pseudo code
just to be easier for for us to
understand and the assembly code looked
like this it did as well the the loop
peeling so it peels the the first
iteration which means that the forced
for additions so starting from from
fourth one it did the same so what it
did it mainly reloop what I did because
it thought it's not a very efficient way
of doing loop unrolling and it did loop
unrolling in a more efficient way that I
did in the beginning so which means
mainly it reload my loops and it
r-r-roll them again so at the end in
this we end up with the certain number
that the same number sorry off of
iterations but just to tell you that the
assembly code it's it's it's much more
complicated so we try a bit to to put
the just-in-time compiler into into
difficulties which leads me to to the
conclusion that it's not quite quite a
good idea to do this kind of
optimization in advance so let the
just-in-time compiler do them for you
because just-in-time compiler knows
better than us during the run time
especially for this case for the loop
unrolling loop unrolling as well just to
sum up it's it's it's a good technique
because it reduces the number of
branches and as well the number of jump
instructions and also opens the door for
instruction level parallelism which
might allow basic block level
vectorization optimist
and us we discussed avoid this manual
lupa rolling because let the
just-in-time compiler do them by by
itself because he knows better than us
what's what's income comparing in
compliance with the hardware platform to
your CPU to the number of of course to
the number of threads how to do it
better in a better way swing the
duplication it's the next optimization
that happens during the runtime which
many says that if I have the gear which
one enabled and starting with Java 1.8
Update 20 during the runtime during a GC
cycle the garbage collector searches for
Strings backed by the same charez and
how how it searches it does CH are
arrays the strings sorry should have the
FIR the same hashcode
and the same char array and if there are
some similarities if there are strings
that are backed all the redundant chair
arrays are removed in this case it it
optimizes the memory footprint during
the runtime this having having this sort
of explanation in mind I did some tests
on my machine and what I did in here I
created a very simple application I
created a lot of swings and ice at the I
set a threshold for my heap memory to
one gig and because I wanted to benefit
out of the the string the duplication
optimization technique I as well
explicitly set the G 1 G 1 is not
enabled by default in 1.8 and I did two
type of tests the first test was with
with string the duplication and the
second one was without string the
duplication as you can see here so what
we had from the stop or stop
world pauses in case of sting the
duplication the the red one we see a
bigger number so the number of stop the
world pauses took more stop doorpost
took more why because after a certain
number of GC cycles the g1 collector had
a bit of work to do in comparison with
the blue scenario when I dissipated the
sting the duplication so it had the
world to do off findings the strings and
removing the the redundant cherries from
the main memory from the heap mainly so
in this case of off context we say that
sting the duplication may add and not
may at it as definitely longer stop the
world pauses during the runtime but it
is also beneficial and in my case it was
beneficial because if we get back to
this bounded amount of memory in case we
used ring the duplication the memory was
freeze during it was reclaimed during
the runtime and I was able my
application ran for a certain number of
cycles more because I had more heap to
allocate more more strings in comparison
with the previous case when I've gotten
out of memory earlier why because in in
the heap memory I had a lot of redundant
generates so it's it's useful in in this
case and if I have to take a look as
well over the hidden memory usage I see
in here which without stirring the
duplication I see here the heap usage
after a GC cycle was bigger than the
heap in case of using single duplication
just to sum up single duplication
reduces mainly the memory fruit
footprint it's a good technique it's was
added in in the JVM just for this to
reduce the amount of hip and it's very
useful in case you deal with bounded
amount of
keep during the run time but it impacts
performance it impacts performance as we
saw the number the stop the world pauses
take take more of course because Ji Won
has additional work to do in regards
with reclaiming the chair erase bust
locking by locking it's eating it's in
there since 1.6 which says bias locking
it's a technique in case of an contented
locks for example I have a synchronized
method but during the run time I have
just one thread that calls it there is
an optimization which avoids the penalty
of the real synchronization this this
penalty of real synchronization relies
on OS mutexes conditional variable
signals under the hood so a lot of stuff
under the hood that it's triggered by
this synchronize synchronization unlocks
and the technically says don't go real
synchronization just biased the lock
towards the thread it owns it I did as
well some test to test to to check if
this biased locking scheme impacts
performance in my case what I had I
created like 50 threads inside each
thread I was I put a synchronized on
this shared instance and each thread
locking on the same share instance
shared instance incremented a counter I
started my trading here and I joined for
all of them and if I started my JVM with
bias locking startup delay 0 B why is
that because by default bias locking
scheme it's active after I think 4
seconds so it's not activated is not
enabled from the first very beginning so
I started my JVM with 0 because I want
from the first very beginning just for
the sake of my tests and I printed the
applications stop the world pauses
and as we can see the the GC output it's
a bit verbose it contains a lot of the
total time for each application threads
were stopped so a lot of them I did the
Sun and in in my case it's around one
millisecond
it's which means this millisecond was
mainly spending operations like revoke
bias and enable biased locking so we pay
a lot during the run time one
millisecond mainly mainly for this for
this bias locking mechanism I did the
same test again but this time I disable
biased locking I disable it from the
first very beginning and here the output
it's more clear without any stop the
word pause so or zero seconds
approximately so zero seconds which
leads me to the conclusion that bias
locking it's it's a good technique it
helps you a lot in in cases where you
don't have contention but if you have
contention you pay a lot the cost of the
stop duel pauses so that's why mainly
recommendation for you if you if you're
if in your application you have to deal
with a lot of threads there is
contention maybe maybe it would be
better to disable the biased locking
scheme because you have to to wait for
the stopped or pauses during the runtime
another technique is it's mainly deal
lining and today we are going to check
to study the concurrency implications in
regards with inlining what's the in
lining good for the first answer it will
be it saves us from the pop and push
overhead but it's just let's say I'll
say very naive explanation it's not like
that
in reality there are a lot of benefits
the first one is about the consistent
control flow of the code if just-in-time
compiler
seize that the code it's consistent the
flow of the code is consistent
it relies to move it around and deals
with optimizations it deals a lot with
optimizations across boundaries for
example caller and the callee if the
caller and the callee can be in line
together inside the big piece of of code
it does optimizations and as well if we
can inline something in some cases it
saves the number of memory loads and
write immediately we will see what means
this kind of technique to dig in inside
it first bit of context about the test
scenario again the same long run in loop
but this time inside my hot method
I called hate method twice and with
method twice so it's mainly a call to
reference a variable out of the memory
let's say and store it in in these
locals and at the end my method did some
multiplication and addition and returns
the result what we have in here because
between these calls on the height and on
the width methods we explicitly call in
between a method which can't be in line
anymore
what means can't be in line method that
just-in-time compiler cannot in lines so
the caller can you lied the collie body
in my case how it looks like it's it's a
bit it's a bit funny because I created
it in an explicit way to preserve to
prevent sorry the in line from the gist
perspective so it means for example it
calls M 2 &amp;amp; 3 &amp;amp; 4 up to an 11 which
identity turns 0 why I did that because
implicitly and the hotspot JVM has the
parameter which says max in line level
it's default 9
9 so after the ninth level of call that
just-in-time compiler is not able to
inline our code anymore having this this
said if we get back and we run our
example and check the assembly what's in
here we see the explicit call for the
height method to retrieve the value we
see this virtual call to the method
which can't be inlined and again the
same call to the to the height method
and if we continue zooming into the
assembly it repeats the same call to the
same method which can't be inlined
and twice two calls mainly the width
value twice twice with values out of the
main memory why is that putting it into
a bit of generic context if we have to
memory retrieval but in between we have
a method that just-in-time compiler
can't inline that method the
just-in-time compiler assumes that there
might be other threads that changed the
memory so it explicitly forces a new
reload from the main memory from this
height value that's why that's why we
have twice calls for the height and
twice call for the width if we repeat
the test and we increase the default max
inline level just to be able to inline
that method we see did a native native
code it's like in here it does only once
the retrieval from the height memory and
just once the retrieval from the width
and as well the memory the solely the
method can't be aligned was was
enlightened so this is a very very
interesting technique to keep in mind
because if we deal with methods that
can't be inlined
you prevent a bunch number of other
optimizations
so for you two to check inside the code
it's mainly based on on the size there
are methods that if they are bigger than
35 bytes they are not in line even if
they are called more than 250 times and
in case you have yeah so in case you
have frequent methods for example hot
methods those methods are not in line
anymore if they are bigger than 325
bytes so be careful with this because
you you prevent inlining so preventing
the enlightening you prevent any other
optimizations during runtime as well the
maxing line level that they call that
level that we discuss about it's 9 by
default and there is a maxi cursive in
line level for recursive calls which is
set to 1 by default how to check in your
code just a very straightforward
technique there is a jar called jar scan
so you can you can check out of the code
base on your side which methods are not
eligible to be inlined because because
their size it's greater than 325 bytes
so it's a very straightforward technique
that my my recommends you what to
refactor out of your code in order to
allow inlining to happen during the
runtime and the last part of the talk
it's in regards with methods that don't
cheat the first case of such method it's
a method which has a huge body by huge
body in this case it's a body that has
805 bytes so for this case of methods of
course the example it's a bit
exaggerated just to prove they are not
compiled and if we have to run this and
as well to print a compilation as we see
here there
is nothing which got compiled during the
runtime in regards to the hot method
only the main method if we have to check
inside the JIT watch tool we see written
in here no JIT compiled that's why why
is that because there is a flag inside
the machine which says don't compile
huge methods which is enabled implicitly
to true but what is a huge method huge
method there is as well task for the
open JDK a huge method it's mainly a
method that have has more than 8000
bytes so in this case they disabled
implicit with their compilation because
as it is written here they are not well
tested and probably they'll have more
other implication in regards with the
code cache to the flashing policy on the
code cache so if you have such huge
methods it's better maybe to to refactor
them because turning off don't compile
huge methods might cause any other
side-effects the next corner case of
method which are not compiled and more
our method with a lot of arguments in
here we have methods for example with
with 70 doubles very very big signature
and the return is very simple it returns
the sum of all in this case if we have
to compile with the just-in-time
compiler prints this kind of warnings
saying that unsupported incoming calling
sequence or unsupported calling sequence
if we search on the the web there is
also a task for the open JDK it's still
in open I think it's it's there since
the very early version that the sixth
one it's moved towards 10 so probably
there is the room
improvement or their side in regards to
these methods so all of these being said
I want in closing to end up with a code
from from Richard Feynman which he which
is a guy which really inspired me it's
very very smart it was mainly very smart
guy and he said that it doesn't make a
difference how beautiful your guess is
how smart you are who made that guess if
it disagrees with the experiment it's
wrong it's nothing that's why during
this talk I studied from the very high
level from the Java code till down to
that to the native code many to
understand what's happening under the
hood the hood and to prove the
optimizations otherwise if you if we
don't have a good experiment it's still
a guess and my advice for you it's
always to be curious to experience new
things only being curious only searching
under the hood you will understand the
things in a better way
thank you very much if you have
questions I think we are a bit behind
schedule
no questions okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>