<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bank to the Future: Bitcoin meets Hadoop by Daniel Cook | Coder Coacher - Coaching Coders</title><meta content="Bank to the Future: Bitcoin meets Hadoop by Daniel Cook - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bank to the Future: Bitcoin meets Hadoop by Daniel Cook</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fkiHhrWTNSA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so well done you made it to the end of
the day I'm the last thing standing
between you and a good drink so
hopefully if this goes nice and quickly
for us all so I'm Dan cook or Daniel
cook if I've been a naughty boy so I'm a
freelance big data architect or big data
technical architect I like specifying
that it's technical a lot words there in
reality I sort of shorten that to I'll
do anything for money
within reason but you know I I still
work across the stack during development
as well as the solution architecture as
well so I've got specialisms in Hadoop
and GIS so Jews facial information
systems so mapping in layman's terms
you wouldn't think there's much of an
overlap there but with the advent of
smartphones they're out there
everyone's got geolocation data we want
to analyze that quite often we're in
Hadoop clusters so I've built up Hadoop
as a service offering for the UK
hydrographic office a nice big
government department and the UK also
worked on cyber threat detection
platforms with NetFlow data looking for
things like port scanning in real time
processing about hundred fifty thousand
events a second in terms of mapping
which is completely unrelated to this
talk what does fro in any way because
they no one wants to catch up with me
later helped build the the indoor
equivalent Google Maps so for shopping
malls airports as well as tracking
people in those buildings you know
people want to know how long people take
it free security etc but where did this
talk all start
I just spoke about my sort of Hadoop
background and it all started here
really this is my friends I've got a lot
of attractive friends albeit male not so
many females but this is what my friends
get like when they talk about Bitcoin
they start off talking about technology
and then they end up saying it's gonna
solve
Altice all of our problems
redistribution of wealth etc etc and I
played the ignorant card for a while I
you know a live plain ignorant card
he'll never work
sounds like crap you know keep doing
that and eventually you've got to look
into these things I feel like I get a
lot of enjoyment about being ignorant
that at some point I have to challenge
what what I'm being told
even my friends that didn't have a
technical background was looking about
smart contracts and what they could mean
for their business so my friends kept
talking about listen it's all the time I
was sat in a car on the motorway I was a
children's birthday party so if anyone
hasn't got children and they want to
know what a children's birthday parties
like it's like going to a house party
but there's no alcohol there's normally
no music or if it is it's not sort of
dancing music you know I'd say there's
just no fun really for going back to the
tech I started reading about Bitcoin and
this is one of the first things that I
found the entire world effectively is
just rolling dice trying to get those
dice lower than a certain number and
what do I mean by that so this whole
concept in Bitcoin on the blockchain is
the transactions arrive you know someone
paying for their their cup of coffee or
whatever they're purchasing we group
these or miners group these transactions
together and then they hash them with a
bit of random data and the aim of the
game is to get a number lower than a
given number so the network sets a
number that you've got to go lower
bandwidth your hash and what you do is
you can start changing this random data
with your group of transactions trying
to get lower than that the more miners
that come in more people are throwing
those dice so it's shorter time to get
to the number because there's
probability saves as more people roll in
they're going to get there quicker so
this number gets lowered by the network
always trying to keep the block mining
within a 10-minute period
but crazy right just world rolling dice
that's that's our new innovation and I
dealt a bit deeper but what does it mean
by everyone rolling these dice and it
means this right I've got nothing
against coal
so I'm from England from Yorkshire in
England I grew up my great granddad was
a miner my granddad was a miner my dad
was a miner and I've got baby soft hands
I'm the first one in my family to not be
a miner and one of the reasons for that
is they stopped his mining in South
Yorkshire so yeah I'm still slightly
bitter about that watching communities
break up growing up but is this really
what we want for our future the amount
of energy it takes for people to do this
mining just hashing payloads it's
ridiculous amounts and the only places
where this is really viable the places
where energy is cheap so this is places
like Mongolia like China and what do
they burn in places like that it's coal
it's the thing it's the stuff that
pollutes the atmosphere and you know I
read a couple of months ago now it's a
few months ago now the the cost it takes
amount of energy to run a house for a
week to process a single Bitcoin
transaction that's someone paying for
their coffee you know a house has been
on all week using their tumble dryer
their lights except for heating is that
right but anyway enough conjecture
we need something practical to face face
to talk around and I need to talk a bit
about mechanics of the blockchain so
this is a very simplified view of a
transaction on the blockchain so
transactions have inputs transactions
have outputs inputs is your money coming
in I thought obviously going out and you
can have many inputs you can have many
outputs so you can group together money
that's its previously unspent
so I think the thing to mention in
Bitcoin on the blockchain is all the
monies within the system
at the very beginning there was some
money credited and then that was spent
somewhere it's been spent some lies
since every time I block his mind that's
a new way a new mechanism of money
getting in there's a there's an output a
special output that occurs when it
blocks mind credit to the minor with
more than money so this is how the
system builds up the money that's in
there
and you can only spend yeah the outputs
of a previous transaction as the inputs
but what does that that mean that means
that to figure out if you're actually
allowed to spend those outputs days and
spent transaction outputs you TXO
someone else must have not already spent
those or you must have not spent them
earlier on so what you have to do is you
have to start at the very beginning of
the blockchain it's this append-only
model and you have to scan it and say as
my output been spent here no as my
haven't been spent here no and keep
going no big oh that's nice big o n
operation and that's quite problematic
as the blockchain grows so it's about
150 gig now for all those transactions
that have been recorded for Bitcoin
since its inception and this 250 gig so
that the whole network is a bunch of
decentralized notes so it's people like
you and me spinning up a node in their
basement and to get that going you've
got to seed 150 gigs worth of data and
that takes a few days to do because
other people like like is that running
nodes in their basements and doing it
via their their normal internet
connectivity but that's not not quite
the key thing I think the key thing for
me that I spotted when I looked at this
which was this was a moment where I
thought I can build a talk around this
is the inputs and the outputs are
recorded there's one unit here which is
great there's no account with a balance
that gets debited or creditors the the
record of what monies where can be
determined by scanning the blockchain so
just looking at
and a little bit more detail so input we
reference a previous output of a
transaction that you TX so reference so
earlier on in the blockchain will say
when you use that output and then we
provide a locking unlocking script so
the whole thing in Bitcoin on the
blockchain is based around public
private keys
you know those normal SSH things so
quite often quite commonly you find that
someone credits some money to a public
key that's the one owns the private key
for if you can produce a signature with
the corresponding private key you can
unlock these funds you can then take
that money I specify an amount and
credit to someone else's public key ie a
lock in scripts but going back to that
point that the inputs the outputs it's
all recorded as one one unit so here's
the hypothesis to talk you don't need
transactions in banking so I've worked a
brief stint in the banking world in
investment banking world although doing
software so it's obviously not as
exciting as the trade life and I've got
a Hadoop background no SQL background
but I've always held relational
databases dear and messaging
technologies like a.m. QP and when I
initially saw banking it's like well
it's a clear use case for it so let's
say you want to move money between
accounts a message arise on like you say
activemq or keep it or a rabbit m key
you take it off there you debit one
account in your relational database and
then you go to another row and you
credit that row and then you commit the
transaction so both the rows though four
counts get rid and the DQ happens all
atomically and that's that's great so
you know that concepts of two-phase
commits but as soon as you say well
actually we don't need two accounts we
don't need to add eight two rows we can
just store this all as one thing things
things start to change
and then looking further into Bitcoin
and this is one of those I've gone from
putting random pictures up just to
random numbers now I don't know if
that's progress or again going backwards
this is one of the numbers you see when
you look at the scalability of Bitcoin
it's finally known as it gets quoted out
there this is pre segregated witness so
the numbers actually double this now and
this is the theoretical max number of
transactions a second it's it's pretty
small right but that could be said to be
unfair on Bitcoin you know it's quite a
even though it's got the publicity at
this moment in time it's still quite a
niche quite small group of users that
are actually really using out there you
know finding people that will actually
take payments in Bitcoin isn't that that
easy so there's never really been a need
probably to push this further than what
is there it's really a virtue of the
block size limit so the blocks and
there's a hard-coded value that plots
can be a particular size and it can I
believe it's one Meg obviously recording
and transaction inputs and outputs has
an average-size there's only so many
transactions you can fit into a block
and given the fact that we're always
trying to only mine blocks within a
10-minute window by adjusting the
difficulty you arrive at that number
given all those preconditions putting
this in perspective with something like
feyza feyza does about 2,000
transactions per second with a peak of
about 56,000 transactions per second so
you know it's can't scale anywhere near
what it needs to at this moment in time
to meet you know worldwide demand and
you could look disingenuously
and when people say that bitcoin the
blockchain is a store of value you could
say well actually what they're saying
there is it's going to be a niche thing
used for moving high-value items it's
going to be a bit of an asset that
people trade as a posts and then
used as a currency if you wanted to look
it out fairly so looking at what what
you could do and all of this is
predicated on let's say our internet
connection between machines is is
unlimited is infinite
you're always going to hit a hard limit
so at the moment in the Bitcoin world
we've got lots of people operating
bitcoin nodes so they could be used for
mining they're trying to actually mine
these blocks or they could be used for
supporting wallets maybe someones owned
wallets or other people's wallets and
they've got to hold this entire copy the
blockchain which as I said takes a few
days to seed the problem there is let's
even say you remove this whole whole
network bandwidth problem for my machine
in my basement and a lot of it actually
isn't the download speed it's the upload
speed most home internet providers it's
the upload that's the issue not only if
you've got to receive transactions
coming in there as they're happening to
be able to mine or support a wallet
capability you've also got to seed other
nodes on this decentralized network and
not just one potentially many of the
people to make this really work so let's
say we solved the network connection
problem we still got a problem with our
disks right we hit our next blocking
factor which is disks can only go
spinning disks hundred mega seconds so
even in with my hundred gig blockchain
let's say hundred gig for simplicity
it's still going to take 17 minutes to
seed a new node arriving on the network
Henry Meg 10 seconds a gig times by 6 60
X 17 minutes just about right never do
maths conference for that is gonna get
worse as more transactions go onto the
blockchain more people use this it's
gonna take three hours to see it a new
node if this goes to a terabyte which is
quite conceivable in the near future
and if it's widely uses it's going to be
terrific so let's say we've got this
decentralized network but a single node
isn't actually just a node it's a
cluster of machines and that cluster of
machines has something that looks like a
single disk but by leveraging a cluster
of machines you're actually gonna
leverage the power of all of those disks
so if you've got tendus in there you're
gonna get it down to just in the two
minutes to seed your hundred gig box
chain 17 minutes to do your terabyte and
not only that you can have this the
system where a new cluster in the
network comes along and other clusters
will see that first suit you know blocks
from those individual machines so but
one might be owned by one machine on an
existing cluster and it'll make a
network connection to one of the nodes
on the new cluster so you'll get all
those ten nodes of an existing cluster
opening network connections and sending
the blocks across so this is this is
potentially the Hadoop file system this
is how you get that unified disk and
what's also key in the Hadoop file
system is you've got this append-only
model you always write at the end but
what does that mean it means if you've
got spinning disks your platter is
always there and it's always able to
write it can really max out that hundred
mega second it hasn't got any random
accesses to worry about so that's how a
new node could come along or a new
cluster and receive the entire
blockchain quickly but that's the seed
use case what about the continual
arrival of transactions you know people
are paying for their coffee all of the
time so you probably want to use
something like Kafka so let's say we
want to go up to our 150,000 events a
second you know do better than them visa
which I know calf you can as I've done
in the past you want a clustered message
broker so I know right
fondness of AMQP I've used a lot of that
ActiveMQ but it's always been a real
pain for me whenever I've had a high
throughput problem I've always had to
partition manually create separate keys
and manage things like that and put them
on different machines
kaftan makes this all simple out of the
box so katka has this concept of a
distributed commit log commit log in the
fact it's append only it can max out
your spinning this is a message coming
through here and disputing the fact that
it partitions the topic so it takes a
chunk of the key or topic and it puts
each partition on a different machine so
when someone's writing on to that it
does a random allocation and writes
those messages to one node in the
cluster so your topic spans several
machines and you can get that throughput
you desire just like you can when you're
using HDFS as a file system and what we
probably want to do here is we want to
utilize a dispute kijima system like
Kafka Connect which hangs off the back
of these topics and they actually
aggregates these transactions together
into blocks we've probably been looking
for about 128 make 250 6 Meg block size
as opposed to warm make that we've got
currently in Bitcoin just to match the
size of blocks that that Hadoop stores
that sort of naturally white sits that
by default but it's all got a bit
theoretical right what I've just spoken
about and this is what we really care
about we really care about money and
therein if I've got my money to spend so
how do you know how much and spent
transaction app that you've got how much
money do you have in your account
append-only is great for right will be
able to scale this up hundreds of
thousands second millions per second way
way more if you throw the machines at it
it's not so great for the read what
happens when we get to this terabyte
hundred terabyte petabyte blockchain
it's gonna be really difficult to figure
out funds have already been spent
because we've got to scan it all the way
what we need is an index pool
the unspent transaction output so what
would it salt be dimension to do that
didn't have some that reduce so I had to
have to get it in there so on the top
right hand side is an even simpler view
of the transaction in the blockchain so
all we've got on this is the account ID
that we're using as input to the
transaction and then we've got the
account ID for the output and because
there's maybe more than one there's a
pipe separator in there to say input
transit account ID one is paying
accounts two and three if you work that
all the way through you see that
actually accounts three and six haven't
spent money that was created to them so
how do you do this in a spark job and in
that reduce job you start off with let's
say we've got these files on disks that
contain a format like that we split down
the middle let's just assume there's a
whitespace character in there we'll
split on that so instead of having a
stream of transaction rows we've now got
a stream that's tuples and on the left
hand side the tuple there's the inputs
on the right-hand side the tuple there's
the outputs and then we've got these two
flat map stages and all they're really
doing is saying look at the left-hand
side of the tuple and extract out of
that and create a new tuple that has the
account ID and the fact that that was
used as an input and on the right-hand
sides of the original tuple everything
there our output so take those account
IDs and produce new tuples that have the
account ID and the fact it was used as
an output so the new stream after this
is just a set well a list of tuples that
on the left-hand side as an account ID
on the right-hand side just as if it was
used as an input or output so that's on
map stage and then we'll do a group so
we're going to do a shuffle and then
reduce so at the end of this we'll know
or we'll have account IDs and after that
input and output or just input and once
you've thrown away everything
that's been used as an input we've just
got the outputs left but that'll work
great for our initial seed case so we
get all of the data of the blockchain we
can run a MapReduce job over it and we
can figure out what transaction here
what accounts have money available to be
spent where the youth EXO is that won't
work for the streaming use case the
continual use case as new transactions
arrive say we need a separate system for
that and separate systems having two
ways of doing the same thing and ever
great so maybe we need to look at this
in the context of can this do everything
can we do it differently
and achieve both for the seed case and
the continual case the streaming case so
all three of these are open source big
table implementations so BigTable paper
written by Google and implemented by
Google initially built for the nice
Google search to store the inverted
index so built for a great reason
although some of these technologies on
particularly HBase and accumulo on as in
widespread uses such a famous case as
the Google search bar to employ but
they're all petabyte scale random access
stores
that's what BigTable is to the left
we've got Cassandra so to the left is
less true to the original concepts of
BigTable so Cassandra tends to go down
and really appeal to those people that
have worked for relational databases
it's like the concept of tables and an
SQL base or styled query language
whereas the right hand side are a lot
burea
to what the BigTable paper specifies so
there's no SQL style query interface on
HBase or accumulo HBase is
I suppose the original open-source
implementation Akim though was developed
by the NSA and then given to the outside
world and as a result it's probably not
quite as popular because of its heritage
but for me accumulator has some great
features they've since been absorbed
into HBase so Aquino had this idea of
iterators and as you receive values in
the store you can apply processing at
the same time and HBase later absorbed
that as coprocessors Hakeem you know had
fine-grained security which in a
enterprise setting is often really
important and HBase picked that one up
as well and for me Hakeem lo has a
cleaner API it's really cell driven but
yeah I don't want to fall out with
anyone you can do what I'm about to
describe with HBase or Kimo you can I
often use them interchangeably so what
does BigTable look like I quite often
describe it as a key value store and
then people go down it's not a key value
store it's a colon family store and you
go well it's got a key in a value that's
in it so it's pretty much a key value
store the complication comes in the fact
that the key is actual can put actually
composed of several parts so all of
those things that are sitting under that
that key actually form part of it and it
looks quite confusing at first I used
HBase and Hakeem though several years
ago and I thought I understood this
model I first used it and the past
couple of years I realized I didn't
understand this model as well as I
thought I did and a key point is is that
everything that stored is completely
ordered on the key it's a sort sorted
list on the key which is you know across
a cluster which is pretty amazing across
several machines so just talking about
in relational speak euro ID you can
think of as a primary key almost writes
on the row ID or atomic which is good to
know so you can write several things
with the same row ID and it will all be
done in an atomic operation
and then we get into this Colin thing
the con qualifier really is your your
column in relational speak on a
relational table
but the fact is that a column qualifier
can actually be put in a column family
and this this is really drive the
storage things in a separate column
family in the accumulo world can be
stored in different files which is the
sort of columnar based storage in that
efficient reading which you often hear
spoken about I'll ignore the visibility
for now these stores are extremely fast
at writes and one of the reasons for
that is their append only again append
only and this is what the timestamp
helps you with so if you want to update
a record you just write a later version
of it a later time stamp and then when
you go and retrieve it it just gives you
back the most recent one and there's a
flexible schema here you can as your
systems run in it's not a predefined
schema you can shoehorn wherever you
want in as new columns new families on
the fly which is great so what does that
mean for our transactions how could we
represent those in the blockchain world
so our row ID for so we're looking at
here is a table showing just a single
transaction simple of idea of a single
transaction and we've given that a UITs
in the row ID so all the same you it
makes sense and then we've used separate
column families for the inputs and
outputs this means if we just want to
read the output we won't have to read
any of the input data even if we're
scanning across the entire store and
then what you quite often see in big
table stores is a concatenation of
column headers so because the
transaction can have several inputs
several outputs I've just simply
prefixed the column qualifier with the
index of each input and output so
there's one input in this case and two
outputs 0 and 1 and then the inputs are
just saying the transaction D the
previous transaction D where I'm going
to get this money from and then the
actual output
that money was was chronicity and then
in the outputs I did a similar thing I'm
saying please give this amount to this
public key you'll notice I've lost the
lock-in and the locking scripts for
simplicity so that's great it gives you
random access to the ledger to the
blockchain you can go looking for any
transaction and find out details of it
but it doesn't solve the unspent
transaction output problem so what we
can do here is we can write an
additional column family so for that
transaction I've just shown we can
actually store the fact we can add
another row in and so just looking at
the bottom part of the table at the
moment and the bottom row we had another
row in it says unspent transaction out
output index zero and a timestamp
basically saying this money is available
to spend and when someone comes along in
the future and claims that money what
they do is they write the row at the top
of the table and basically say well
here's the timestamp this is when I'm
taking it I've put delete marker in they
basically delete the thing and say I've
taken it no one else can have it so
someone else goes looking for that to
spend it it won't be there and what
we've done there is we've solved the the
ETX own problem well not quite it's now
a log n big o log N operation because
it's index we can go looking for the
previous transaction the money we're
spending and we'll find it quite quickly
but is is that the whole story so I
started out with this premise that you
don't need transactions in banking does
that still hold true I saw of waffled on
about the fact that it was so great that
the inputs and outputs were still
together as one single unit but that's
not true anymore what I've just done is
just broken that I've written across two
transactions so my new transaction in
the future that that was claimed in the
fence
if we just go back ii has a different no
it doesn't have different because i've
not got it on there it has a different
row ID so new transaction i'm right in
all my details that this new
transactions taken these inputs and
spending these outputs and then i'm
going back to a previous transaction and
saying I took your output so I'm writing
across row IDs and I said earlier to
write into row IDs is a atomic right but
writing across row IDs right into two
isn't atomic so how how do we fix this I
think first we just got us a set a
precondition that we're going to reserve
the money we're going to spend first
we're going to go and find all the
previous transactions for money we want
to spend and take that money and then
we're going to write all the details of
our new transaction and we've got two
coping mechanism mechanisms here to
solve our problem the first one isn't
really related to the DHT I I've just
spoken about but it's a necessity
we need conditional mutation this is
something that accumulate provides we
need a atomic check-in set if we're
going to check money's available to
spend and then write a new record to
delete it and say we've actually claimed
it we need to do that in an atomic way
well why someone else might come in and
take it so that's one of the things
that's gonna solve our problem I said
the ordering was important and one of
the reasons is is what if my transaction
has several inputs and let's say it's
got three inputs I go and successfully
claim two of those inputs and then I
can't take the third one because someone
else has already spent that money it's
not available to me I'm actually just
deleted and spent transaction output the
first two things are cleaned and made it
not available for anyone else so I've
got to release those funds and I'll show
you the rows for that in a moment this
is all really about do you want to stay
in the confines of a relational database
tried interested with known
implementations that work transactions
that by doing so
you potentially limits your cell to the
scalability of the single machine or do
you want to unlock and scale across a
cluster of machines and really go at a
high throughput but at the expense of
doing that you you have to compromise
and these are some of the trade-offs you
have to make you have to implement these
things yourself so right infants release
infants should I say is quite simple
just go back and know someone made it
available I took it and then I I need to
go back and actually make it available
again just right the row back in and say
this is now available for us to spend
because I don't need it but does that
work in the case of failure what if I've
taken too am I out of my three inputs
and then I fall over my machine dies
I've claimed them but there's there's no
record that I I was the one that took
them before I fell over you know they're
just not there for dispense so you don't
really want to do deletes what you want
to do is actually record the fact you
want to write a new row not tombstone
row just an update to the ET XO column
of the transaction you clone the money
from and basically in the value
basically put your transaction ID in
there and say I took the money it was me
and what this means is if you fall over
mint mid transaction in your machine
starts back up so you go back to your
Kafka key and you go on the dart process
in my transaction again from the
beginning because it's always nice to
start again from the beginning
your checks are slightly different than
you go it's it is it there to spend no
ones reserved it okay I can take it if
it's not there to spend was it need that
reserved it is the value column does
that have my transaction ID in it if it
does great just carry on and go after
all the other inputs it I require
so it's been a long day and I've just
delved into the world of BigTable HDFS
kafka spark I thrown it all in there if
you don't have a headache I'm sure what
I'm about to do will probably tip you
over the edge distributed systems are
hard so what I've done is I spoken about
the blockchain and Bitcoin the Bitcoin
network which is a decentralized system
or distribution and then what I propose
is we take each node and we make that a
distributed system as well so it's a bit
like Inception we've gone distributed
system distributed system we know we're
second level of the dream or nightmare
so trying to tie it all back together
how do we do the seed case we probably
throw away what I spoke about with HDFS
one thing I didn't say is hakeem you
know and HBase actually do their storage
in HDFS so we're doing it anyway but
what we probably want to do is if a new
node in the cluster comes along it's
going to come along at a particular time
stamp what we want to do is we'll clone
an existing accumulo cluster in the
top-left will say send me all your data
up to this timestamp to find all the
rows in a kilo that have been written up
to this timestamp and put them in our
new cluster but while this is happening
we want to backup all the transactions
that are streaming through as people are
paying for their coffee on our cash
cookie and as soon as the seed finishes
we switch the tap and we take everything
from Kafka politics in a presentation so
we spoken about the fact that we're
going to spin up a new accumulate
cluster it also makes sense that we want
to spin up alongside that a new katka
cluster you have two of them and it
makes sense from a perspective that
let's say we've got our your
in cluster Barney a cluster as as I call
it and everyone in your office did in
their transactions and writing them on
today European topic if transactions are
occurring in the UK got this was this
was a mistake wasn't it
if transactions are occurring in the UK
we don't want to necessarily go after
the European cluster because of the
latency issue so we want to write them
to our own cluster to our own hosted in
London UK topic but then we're gonna get
this segregation that doesn't quite work
european topic is going to have
transactions on there to be processed
and the UK one is going to have ones on
there to be processed we know we want
the anti node to function as one you
know in May and it has tea for the
mining of this to work without the split
occurring so European cluster has tea or
the UK cluster pulls the nigel farage
cluster pulls messages transactions from
the European topic onto there which it
then can send to the human local astir
for processing and similarly it goes the
away free movement fantastic a stumble
through there but going going back to
this beautiful stuff this black gold I
don't know why people are collecting
bitcoins when they could just collect
coal look at it what about mining so
I've sort of along the way forgotten all
about mining this whole thing that we
have to group transactions together and
hash them the way that could work in in
this new view of the world would be
again we collect them together
potentially with kapha connect write
them into a kilo
once we've got enough to form a block we
then have to do this hashing process but
as soon as successful hash occurs we
have to actually write that back onto
the the Kafka topic and say I mined this
block this block ID and here all the
transactions that go in it anyone else
is trying to mine a block for that say
mighty as soon as they receive our
messages and they can validate that the
hash is indeed correct with a little bit
of random data they've got to abandon
and build in the block with that ID they
were trying to do and start again so you
know it's just going to go through the
Kafka topic twice to achieve this but
this is the real world right so you
could easily pick holes in what I just
presents it hopefully it's given a
little bit of a flavor of Bitcoin the
people that know Hadoop or vice versa
or a little bit of both or you just want
a drink I think that the key message for
me is hopefully this has shown that you
can build a banking system without
transactions I suppose so you know
people already prove it the Bitcoin
blockchain is out there now right but
hopefully this has made it more concrete
about how you could to do those debits
and credits and manage what's been spent
and not being spent you could easily do
this with real bank accounts
I say easily the core of it could be
done with real bank accounts obviously
when you come into transfers between
institutions etc it's going to get more
complex and some of the mechanics have
become really really complicated so I
went for a really inspirational quote
and I found wonder spoke about Bitcoin
be in the future of the monetary system
and how you know it's going to change
everything and I thought all this this
quote is fantastic but I got the
attribution wrong I just realized how
embarrassing that could be I four years
from the director of the Federal Reserve
of the US Federal Reserve but it turned
out I just read the order wrong and it
was actually just some crazy blockchain
fruit loop that was just quoting it so I
compromised and went with the simpler
goal for nerds after all that's why it
is questions
it's got to be one kripp saying create a
question okay fantastic
everyone everyone obviously wants a
drink thank you very much the other time
and I hope you enjoyed the evening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>