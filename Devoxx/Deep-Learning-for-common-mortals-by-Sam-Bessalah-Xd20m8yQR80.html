<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning for common mortals. by Sam Bessalah | Coder Coacher - Coaching Coders</title><meta content="Deep Learning for common mortals. by Sam Bessalah - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning for common mortals. by Sam Bessalah</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Xd20m8yQR80" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone Wow nice to see all these
people have that's fun so before we get
started so my name is Sam I'm software
engineer i'm not i'm not a deed
knurling-grip' expert so take everything
that I'm gonna say here with a grain of
salt don't believe anything I say just
gonna talk about what deep learning is
how it's working if you scared about the
match don't be there won't be that much
behind so we're gonna try to see in
reality what deep learning actually is
just before we get started I want to
know who does Big Data he I mean with
place we've had oops park all these kind
of crazy things okay not many people who
calls himself a data scientist I love
this club no dad I hate that word don't
tell them yeah ah who here does machine
learning for phone at home by night or
just maybe some somewhere okay okay good
not you dollied always a handle me you
dig data right okay ah so we're gonna
stay a little bit above all the double
of stuffy from a shiver with deep
learning so brace with me so but the
talk was called deep learning from your
motor but I changed the name to call it
a shallow dive into deep learning
shallow you contrast to deep run who
knows what shallow dive is or gonna stay
on top of the surface so let's get going
just for your own good this third is
birth converse word compliant it has a
lot of buzz words many of them might
make you sound pedantic at dinner of
your friends so if you want to use them
later be careful so deep learning if you
look on Google you see on google trends
from 2005 two year 2014 I mean almost
nobody was looking at it nobody cared
about it what did ronnie was and yet
suddenly we've seen this growth of
searches so that's the group of interest
between deep learning
and it's prompted many people to
actually wonder why does the reason
actually does this much interest into
deeper knowledge in the same wave we've
seen scientific articles I mean those
are scientifically cell research or
actually publish a lot of papers not
just on machine learning that
specifically on deep neural networks and
this has skyrocketed in the past 23
years I mean it's almost as if the other
part of machine learning doesn't exist
anymore most people actually publishing
things on deep learning so might ask
yourself why well in the same ventu in
the news you see things like this
YouTube uses deep learning to pick video
thumbnails okay so every time we go to
youtube so instead of giving us roar
recommendation the choose part of the
video the capturing image the show it so
we can have an idea of what's in the
video and they use deep running for that
too good Apple by secretive deep
learning start because Apple wasn't
secretive enough so yeah we're like okay
deep learning is everywhere now
Microsoft is lodging is launching an IP
an API to track emotions pitch
recognition deep learning again good and
you have these crazy things deep dream a
I code who knows what did brim is good
some people are crazy enough here good
so yeah we see a little bit what the
dream looks like so basically like okay
since everybody's talking about
deploying well we'd better take a back
elbow look seriously but what this thing
is but before we see what deep learning
is that's exactly a real application I
mean you have mini application that we
don't see but let's have some fun just
for the second vid I don't work for
Microsoft's I'm not a free affiliated
with them as anything that this thing
called Skype translator what it does
actually is that you have to who knows
with skype prosper Tories okay well
Skype translator is actually you having
a talk with someone you're speaking a
different language
you have the machine actually the doing
with translation so I speak in English
for my part and you speaking in Dutch
from your parts but you're not hearing
me speak English but actually you're
hearing someone translating in life on
life translating what I'm saying in
Dutch and with minimum actually latency
and that's nice so let's what the video
does speak your language the guest on
Ke$ha problem but I wanted to talk to
you about the email that I sent you
yesterday golf the rocket organ right
here in da mornin do you need any
changes there was one thing though could
you change the green to a lighter shade
can koozie does going to shine inhaler
and half tone n done yeah the current
does blue and feel hello MA yes we can
make the green much brighter extended
the order out on annoy editor I'm
sending you a new file tonight that
sounds great I look forward to it not
clean time with a Skype translator it's
a full human to human okay thanks Peter
that's nice but of course skype being
skype it has to work first so that's
another story but how come on I'm just
throwing some jobs but it's that's
really cool for me I am I have many
people who speak don't speak English or
French that well so sometimes they can
speak in thing and we have the Babel
Tower absolutely for real good bye hey
that's really cool and that's something
that really excites me and let's go back
here would so skype that's actually a
huge problem dissolving because what's
happening behind is actually have to
think speech recognition so what I'm
saying and machine translation and
speech recognition is a separate field
when it comes to machine learning
machine translation is another one so
they have to do the blue between those
two and that's where deep learning
actually is important because speed
persisted for ignition we have a state
of art that's now being vision to
machine learning as well machine
translation have a set of that exists
already but
making them communicate this way life
that's really break for another thing is
google deepmind dick man will start up
actually that Google but sometimes ago
and they do some cool things who knows
Atari everybody knows I'm sure yes who
plays a Terry still today shame on you
okay so you have they use this thing
called q-learning q-learning is a an
algorithm that's used in something
called reinforcement learning when we
talk about machine learning usually
people say you have two classes of
machine learning algorithms so you have
supervised learning and unsupervised
learning I'm sure at least one all of
you I've heard about this but there's
another part that's usually forgotten
that's where the people actually dream
of robots actually guiding us next to
moral actually try to work it's called
reinforcements line meaning that you're
learning step by step of what you're
doing that's another part of machine
learning and it's something that's not
really it's bit complicated and it's
used in things like when you you keep
looking at the website when you have to
do a be testing you use things like
bandit algorithms so in the meantime you
can say okay I'm gonna show this I will
show that you take that decision in real
time those kind of algorithms are coming
or something called reinforcements
running they use the community armed
bandit but the Cure running is something
different so what Google did actually
with Atari the guys from deepmind
actually did from Atari is like they
throw in something they want the throw
in a program that was actually going to
learn to play Atari it has no idea or
prior no data but actually learning from
experience until it's actually going to
be better way better than a human being
so let's look it's a one-minute video so
let's look at this okay it's self
explanatory
there's no domain knowledge from the
very beginning so a group doesn't know
any concept of bar of all the place so
10 minutes of training it fails
miserably let's look back long egg okay
has no idea what it's doing so yes let's
move around let's try to do okay that's
what it's doing just like a human being
act like a child oh the boys coming can
I catch them oh so I can do that fine
two hours later it plays like an expert
because it's all so I can keep this I
can keep this part out do 22 22 22 22
good but it's in failing right some bars
like it's not cashing in stock catching
everything dr. Turk Turk gegege okay
fast fast fast fast fast four hours
later now yeah what's he gonna do it's
always very good yes now it's kicking
kicking kicking ass at this game so ya
bum bum bum bum well now it plays better
than a human being doesn't matter what
expert you right he does the magic on
top so yeah yes this is deep learning
behind so that's reality that's a paper
that's been published not so long ago
and they have this record is available
of course I mean you can play with this
kind of thing this is real deep learning
this is machine learning for real so now
that we've had some fun let's go back to
shady things deep learning so now we've
seen all the cool stuff so what is it
exactly deploring I mean peep one has
its own definition of what deep learning
actually is I mean when you ask someone
you say yes it's like playing with non
convex transformations yes I mean
bringing something that also we can
define the plot names actually it's not
one algorithm but a rather than is set
of multiple algorithms what they do is
that they take high-level abstractions
so for example
speaking what is gonna do is that from
representation of data that from this my
speech what the computers gonna learn is
like the actual numbers it going to
learn some representations and actually
learn again by adding multiple your key
of learning we'll see how it goes so
they usually built using neural networks
we'll see what neural networks moon I'm
sure everybody who knows what a neural
network is good we know is what a
perceptron is okay good who knows what a
recurrent neural network it is a curious
guy could and the same thing happen and
deep learning is actually about learning
representations we see how it relates to
what we do in deploying in common
machine learning task level so when we
do machine learning in reality you don't
just take your data flow in the
algorithms and wait for wizard that's
only works on cattle but so if you don't
go beyond catgirl that's nice doesn't
work but the reality is that you have a
lot of data that data has to be fed into
the algorithms but you don't fit that
data as it is it's not raw data actually
pulling to the algorithms you have to
learn features features that's where the
domain the domain the domain knowledge
comes in so those features you create
vectors over there you feed them into
your machine learning out good
algorithms that's called the training
part of the algorithms from there you
can add labels labels actually take
telling Vegard where that okay these
data represents this is actually your
building vectors with this kind of stuff
and then once you have trained your
Algol algorithms what you're going to do
is that from a new data coming in it's
going to do some transformation based on
what it has learned from previous data
that's what a normal machine learning
algorithms but the real job actually
happens here because for that you have
libraries when did you imagine running
well you see few the Python called use
scikit-learn call difficult I don't know
predict give me logistic regression
you're going to give something you have
algorithm for this for that well
libraries do that too but here that's
what the magic happens that's where the
real complication actually is because
you can't automate all you can always
automate this kind of knowing from your
data if you reading texts you're not
going to interpret texts the same way
you going to interpret images it's not
the same thing so you have to have in a
base knowledge you have to you need to
have a business knowledge or a domain
knowledge to actually to actually create
feature feature vectors so you're doing
feature engineering you trying out some
complex or combination what kind of
feature is going to be okay which one is
going to work so let's take an example
for example face detection face face
recognition what you're going to do is
that you have a frame of picture you
want to detect the face you're going to
find where the face really really is
you're going to extract the features now
it's an image what you're going to use
our pixels those pics are going to build
some vectors and those vectors are going
to be computed to do something else and
you're going to train here down down
below you have a database of data where
you train all your features from
multiple images you create multiple
vectors from your learning and you do
your algorithms you're going to have to
have to create a classification engine
so for every new image you can compute
the same thing it's the same thing as
here so the real problem as i said is
here so what deep learning want to do by
doing what i call learning
representations is from every kind of
data automates the learning of this
representation and by doing multiple
yerkes knowing exactly what my def
amines what this representation and
actually to me till the output that's
the magic and that's really complicated
to do I mean that really completely was
really complicated to do until recently
so to do that the you deployed and use
of course neural networks so what whore
neural networks just a little bit this
is a brand new run
we should we say that yes a neural
network is like a human brain neuron yes
but not quite it's an imitation of this
kind of thing that's not really it's not
really that but yes what you have you
have done right synapses and you want to
plug these these signals coming in and
you have a function that you can I go
what we map this way of thinking into
what we're going to do this is called
the perceptron so you have multiple
signals coming in you have an activation
phone we have an activation function and
the activation function is going to give
you an output by that's computed by
function so the first version of neural
networks come from the 50s so it was
called a perceptron this is what it
looks like so a person has multiple
inputs of data come again you have a
transfer function that is such the
signature I stayed a little bit light on
the mat and you have a function that
going to activate this neuron and from
that function is going to have an input
and that input is usually completed
computed computed using a function
that's known from many people that's
called a sigmoid this is the function
down here this is usually how a
perception works and the output error is
computing by using this formula and the
goal of every machine learning algorithm
is to minimize your cost function so to
do that what they use is stochastic
gradient descent who knows what an STD
is okay some people not gonna dive into
the real formula but basically a
stochastic gradient descent is a way to
actually minimize the global error of
your function because these are all
approximations so you want to have this
thing to fit together so you're going to
minimize that error so you use things
that comes from linear programming like
the stochastic gradient descent and
since that error is go is going to
propagate from multiple level you want
to do you want that error to be cut
somewhere by using something called back
propagation so when you want don't want
to do the computation from the first
lever here but you actually want to
error
to be managed from here and then you
throw it back behind that's what's
called back propagation and back pocket
propagation is actually a way to
minimize error by changing the weight
between those new neurons so to do that
we have here is an example of what you
have with a regression I think we all
know what regression it is well on on
the image over there on the Left what
you have is a typical machine learning
regression so you see that that you have
a huge error because even if we it's
hard to generalize actually you're out
good for this because you have all the
red points Andrew the greenpoint are
actually scatter around so to find a
proper function to do the prediction of
this is really complicated in a neural
network since it's using nonlinear
functions so you can see all the things
dancing around it's not convex so you
going all the way around but you have
things that are separated correctly so
that's one of the advantage but this in
machine learning causes some problems we
see why they're on so you have one lick
one neuron it for that's the perception
but what you want actually there on is
actually from one layer 41 only one
layer perceptron you want to have to
learn multiple things so you're going to
add / layers so this is a multi-layer
perceptron it's also known as a
feed-forward neural network you hiding
layers you adding layers so you can
learn representation step by step so you
have data coming in from an image you
want to learn representation from those
image okay is that is that a head or I
don't know a bag well you have one way
to learn that back you add another layer
that's going to learn and you adding
layer layer those layers I wanna that's
what's starting to look like deep
learning you're going deep into you
layers but that's costly that's a
problem that's actually been for a long
time the real problem so how do you call
what you call deep usually you say you
deep once you have something close to
freely once you go beyond three layers
you have over algorithms called like som
support vector machines they're not deep
they behave a little bit like this but
they're not deep that's when you
start to go beyond three layers that's
you starting to go to some to encounter
the problems that has been plaguing the
norva neural network community for
decades now and why because your
graduates is the function that keep
going and it's you have this prom that
you stochastic gradient descent is
disappearing it's used to call the
vanishing dragon we're not going to dive
into those kind of thing but if you want
to see here within the neural network
you see that you have all these things
that actually just take the red red dot
so it's doing some circles around the
data this is overfitting this is
something that's a problem in machine
learning because you cannot generalize
this kind of this kind of problem if you
take your training data images from on
top of which you have trained your
neural network well when when you're
going to go and when you train DVD to
you from which you've trained your
neural network if you want to go to have
prediction from new data it's not gonna
work because it's only knows the data as
it is it cannot generalize that's the
real problem with machine learn and
especially with neural networks so it's
hard when you are starting to add big
layers well that start to become issues
and you can see here when you have one
neuron and different different layer you
seems to be that it's over fitting it's
closing it's chasing actually all the
data where it is which is good in normal
in a normal setting but when you eat
hard to generalize when you data that's
things that actually let what you used
to call a way I winter of neural network
so it's been sometimes what people were
like neural network dead not working
because you can do much with it that was
in 90s and some over years so after 8i
winter the first one was in the 70s bank
like toradol record properly but in the
90s you had things called convolution
network and then died again at zero in
two thousand in 2010 so this is a little
time line from the perceptron how we
have the neural networks evolution of
community has evolved new things but
what's actually striking is that
you had the same thing in 1999 not by
young looking at the french guy who
leads the Facebook AI research
laboratory but I yes a research lab in
New York it's but he's the one who
actually managed to use things called
convolutional network for email for
handwriting and image recognition we see
what convolution networks huh but that's
in the 90s and somehow people forgot
about them you had SVM logistic
regression those were the new things in
machine learning and suddenly in 2006
you have this thing called imagenet
that's a competition well imaginary hat
Geoffrey Geoffrey Hinton that's the guy
who's been working frantically on a
neural network since the 70s will
actually one done and they won the
competition by using a new by adding new
techniques to actually compute grudge
and designed to critical to do this kind
of Ruto to have machine LAN neural
networks actually work that was in 2006
and in 2012 Jeff Dean that's dear God at
Google he actually manages to bring
neural network into it to actually train
them in a distributed fashion that's
hard to train this neural network in a
distributed fashion and that's what
actually you've seen all these
frameworks all these elements coming out
of now a Google of neural networks
because Google actually went in did and
the publisher lot of papers other people
followed suit Microsoft Facebook
Microsoft I've been working on it for a
while now but they've been actually they
put a lot of deep learning knowledge
into Android speech recognition when use
google now you actually using a lot of
deep learning system without training
behind when you're talking to when
Google now is actually telling you in
the morning okay you so late that you
have to take your bus at this place
before you'd walk before and US unless
you don't unless you're gonna miss your
plane so it actually guides you there's
a reason for that it's working on so
much data so much inputs that they have
system that are trends so well that you
come out that you came up with actually
accurate results and they keep
publishing a lot of things and one funny
things that came out of google and you
here if you a few weeks back is actually
autoreply finger
that's the using NLP so from your email
from your previous email with your gmail
is capable actually replying at your
place so you say okay I don't want to
talk this before you actually the thing
this is a sure I've way better call you
back in 10 years so come back again so
that and I've been dreaming or having
the same thing for slack because slack
is good but still yeah that's a
digression so let's go let's go back to
the neural network so why suddenly in
2006 it started working well first of
all because you had some clever new
ideas I want into developing these
algorithms and one thing that change is
that well it's google it's all these
people we had a hot load of data you
have a lot lot lot of data mean we spend
so much time on the internet now I mean
I'm not gonna give you the big data
gospel here but we all flow data or all
the places so there's all the places you
can pick up data so that's what make a
difference and the other thing is that
you had more compute power power but
then when you had one computer was yes
grades you have to want to train a deep
neural network well by an entire
building now you don't have to do that
but yes cpu are great but not that great
so most of the bottom of a deep learning
work tend to work on GPUs there's a
reason for that because computing
gradient designs is a highly iterative
thing you can paralyze that you have a
lot of way to do that but great and
distance usually work only on GPUs work
very well on GPU so that's why you see a
lot of work that really goes into GPU so
convolution networks let's go back to
this thing which is something like to
example of this on down we'll chatter of
it convolution networks what are
convolution network what basically what
you have are images when you have images
here let's say Albert this is albert
einstein you have images you have pixels
from these pixels you have weights so
you have inputs you learn from these
inputs and from those inputs have this
big surge have vectors on these vectors
you want to pull them together so we
have what's called share weights
sometimes you take you take
you call them taking pools of your data
so we want to capture the context not
just this vector alone but what's around
him and try to learn what you have so
this what's called convolution network
so in order to make it work you have
things like feature maps so you map all
your elements you you separates all your
images actually at exploding all the
features that you have so you can learn
them in the very very different cutting
oil in the very very high context and
then you apply a different set of
compare of transformation so you're
adding over learning layers let's see
how it works here that's what those were
convolution network in the 90s so as we
can see result they could recognize and
working quite easily that's what that
was okay so that's why p banks actually
we're working to recognize yes I'm
sending this check so these were the
kind of things that you already have in
it's not perfect but yes it was already
getting in that's good but in the 90s
what's came up is actually a way to
learn multiple layers that like we
talked so we take this picture here is
actually the same that showing up here
this is actually what the match looks
like so you have these different sides
where you're going to learn you have
different functions you do that turns
out I'm not going to dive into this
match but just to give an idea I have
pictures that are coming in you convolve
them you actually exploding them for
different feature maps and then from
this feature you have different learning
layers and you have transformation some
of them are called max pooling and that
these are formulas and you pulling all
those those learning leaders together to
have to have actually proper resort so
not going to dive into the thing that
just given idea this is how it works and
in 2012 references 2506 imagenet use
this Fink convolutional network trained
very well to actually win the
competition so what they did is actually
not just taking images but actually from
an image in curing what the image is
actually describing so you can see that
I have a containership by looking by
learning from
a bank of you a pile above closed with
more than a million images learn from
them you could they could actually
managed to infer that in that image it's
a container ship and that was in 2006
and the what it did is actually that's
rooted it with something using just
convolution network with some tricks to
make it work using things like
restricted Boltzmann machine and a lot
of other things actually those were CNN
so that was really nice that's what that
actually started with deep learning deep
learning wave thrilled with the tree
still living now all the types of neural
networks are recurrent neural networks
well recurrent neural networks are
between testing because what they do is
that they have an internal state on top
of from which they can learn sequential
kind of data and that's interesting for
example are we work we live in a world
we have disconnected devices I would see
data so it I did it actually you have
time series data coming in especially
with ins and you have speech recognition
as well that can be model as time series
data so using this recruit neural
network they work very well but the only
problem is that if you remember the
diagram of actually our neural networks
you can propagate only going forward I
mean you can go back but you can go back
so far because it's losing its state so
that's a bit of a drawback and another
thing that's actually trying to solve
this problem you think is called long
short term memory no network is it
called lstm this is a good thing this is
things that are coming over for example
for sentiment analysis that you're using
this kind of thing and I there's a lot
of work going in and this is really good
for NLP and if you have a lot of paper
now a lot of people's I have did it that
have been published for this and to
illustrate how it works a little bit is
that let's see this as being layer so
you have one to one layer as you see
they're not really going back it's hard
to propagate back it's only going
forward they say these are kind of
recurrent
the neural network is are these are
different set of architectures and some
wizards like we saw before with imagenet
so we have this you could infer what the
images will go to image below what's
image we had inside but combining
convolutional neural network with
recurrent run all right neural networks
well end up with things like this you
can describe proper nudges with a single
word but actually inferring what's
really happening and this is not just
saying okay there's a guide that's a
container ship but you see exactly what
the guy is doing that's multimodal and
this was a break food was not so long
ago so they mixing actually visual and
semantic actually alignment to actually
give a description to add to a picture
you're not just auto-tagging a picture
but actually giving a full description
to your picture so here for example you
have the baseball player is flowing a
ball in a game well that's automated and
the same thing goes over there for the
boy is doing a backflip on the wakeboard
how couldn't have imagined that the
computer will do something like that I
mean okay that's good some humans can
even recognize this so it's nice to see
and you have a stroke of a lot of over
deep neural deep neural network are good
you have Auto encoders they actually do
compression of your data so you can
produce different new encoders our BM
did believe that's what you really have
at Google now that's powers most of the
on twit things these are different a lot
of oil over a lot of over deep neural
network are boots out there and I don't
know if it's clear this gives an idea of
what kind of algorithm you can use for
deep for neural net for differ for deep
learning each deep learning algorithm of
Rebecca and Terry dealing with text for
example sentiment analysis well you can
use the belief network or recurrent
neural network train over there with
moving windows so that's the kind of a
gram so this gives you an idea what kind
of other if you can use for images for
example well did belief network that's
what you have on google photos on
google+ this is the kind of thing that
you Sita to recognize object that's the
same thing and for voice record
well you use recurrent nights same thing
for 10 series and for predictive
analytics for example we've or something
like an ad anomaly detection recurrent
neural networks really work well for
this kind of thing so this is a nice
thing just to a reminder to find out
what you're going to do and there are a
lot of actually libraries out there that
you can use to work with neural networks
so let's look at how you can use neural
network in your thing I'm not gonna give
the code or something that there are a
lot of frameworks or libraries for
neural networks most of them actually
come usually came from Ike from academia
so you had this tiny thing over there
but hopefully you have a huge open
source community but not much on the JVM
and we're at the work so this is a movie
most egm focused conference on a finger
crowd here is too but most of them
actually come from C++ from Titan why
because it's way easier for them to work
with GPUs and most of our other neural
network algorithms actually work where
we've CP with GPU like I said so a lot
of work has been done that came out
actually from in the Python community
because they have a dual of machine
learning in other communities as well
one of them is the piano too ya know
that's a Python library that's really
used it's open sources of course the
other one is torch this came out from
facebook and the torch guy and with Jana
guy hid themselves quite deeply I still
don't know why but you have religions
and technology right so torch is built
with lua so if you into Lua and it's
really fast it's really really really
nice you have a lot of library a lot of
weight a lot of things to learn to learn
you have cafe that's come that's from
Berkeley that's something you can try to
see if you into C++ and Python that's
nice and on the JVM at last you have
something like h2o h2o is imagine
learning library for come on machine
learning library and have some algorithm
for deep learning and the good thing
with that if you doing spark or Hadoop
is that you can use them on spot as well
and distribute distribute your
computation using diplo using h2o h2o
that's that's nice you have a lot of
things you can try that but it's really
neat and the other one is deep learning
for Jay that's they called it's done by
a company called sky minds so the
columns this them serve the enterprise
deep learning so you can use that we
spark to it has some banned amines
keralites returning java oh of course
hence before Jay over there it's a nice
trick and the picture had the matrix
therefore all the algorithms came out
from this to the website so it's
something very nice to look at so yeah
you can try that and there's the new one
that came out earlier this week that's
Google tensorflow and this I think it's
really neat because it changes a lot of
thing it's in Titan fortunately if you
do in Java but it's really nice because
you have all you need to build an
application and its really well thought
API is clean now it's something if you
really want to try different deeply deep
neural network try terms of Rho just to
have an idea or read a few things but
transfer flow is actually nice so it's
nice will it really nice to see how the
over have it other libraries will
compare and try to evolve from this so
if you want to get started with in
machine learning I advise you to go on
Coursera there's a great cruise that's
been there for a while corner or network
it's B it's been it's given by geoff
hinton Geoffrey Hinton that's actually
the guy we've been walking on neural
network for a long time and he's the guy
who won the 2006 you mad imaginate
actually competition and the course is
really good start to learn a lot of
things not just on the basics of neural
networks but also on deep learning
because the guys on expert so it's nice
go on with deep longing for the sites
you have a lot of resources there to get
started and look at the planning that
Nets it's a bit you have a lot of match
at deep learning that net because you
try to give you all the the foundation
actually to come up with machine
learning and deployment but it's really
nice to see and that's what i had for me
thank you
there's a lot of time of questions for
questions so if you have some faith
which you should questions yeah yeah I
think there's a mic over here see if you
can come I can free with people like
okay that's fine okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>