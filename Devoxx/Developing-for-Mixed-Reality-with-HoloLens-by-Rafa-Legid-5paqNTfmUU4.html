<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Developing for Mixed Reality with HoloLens by Rafał Legiędż | Coder Coacher - Coaching Coders</title><meta content="Developing for Mixed Reality with HoloLens by Rafał Legiędż - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Developing for Mixed Reality with HoloLens by Rafał Legiędż</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5paqNTfmUU4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to I'm going to tell you
about developing course how to start
development for mixed reality and what
is mixed reality and how to start
developing for mixed reality with hollow
lenses and what this device is about but
let me first introduce myself a bit I'm
Rahul again I'm from Poland we can find
me on Twitter at traffic I'm a software
engineer at the company called solid
brain software house in Krakow Poland
and I'm for last like one year I'm
mainly focused on Collinses and
augmented reality I speak with different
businesses I create different proof of
concepts for this technology and try to
come and trying to convince some
businesses that this technology is
something we they would like to invest
in that this is the future and I'm
actually trying to show that it the
technology I mean augmented reality and
augmented reality on goggles can
actually improve your performance can
actually any money etc so that's my main
focus and that's something I'm focused
on within the company I work for also I
am a conference organiser I organize a
conference called Def Con it's in Krakov
we're doing it since 2011 so it's eight
years now it's in September so if you
would like to visit Krakow that's a good
opportunity to visit the conference as
well and also I try to be a conference
speaker so whenever I think I have
something interesting to say I applied
to conferences and hands I'm here for
example and I talked a lot about
homelessness during last year and
Microsoft decided to award me with MVP
award so I guess that's that's cool
thing to do ok anyway around hall lenses
and in general VR and they are pretty
common thing there are a lot of
discussions around it there are a lot of
articles on the internet that VR is the
future AR is the future or whatever so
what I would like to do in this slide is
to differentiate to explain what is the
actual difference between those two
technologies or those those experiences
because
still a lot of confusion out there so VR
is stands for virtual reality which is
basically the experience where the user
puts the headset headset on and it's
it's it it occludes the user's vision
and the user is fully immersed into
digital environment so everything the
user sees is fully synthetic and those
devices those experiences are designed
in a way so that the users mind is being
tricked into thinking that the real
world basically doesn't exist right how
many of you have tried before like
oculus rift of HTC vive
so we already probably know how
immersive how convincing the experience
is for those who hasn't tried it yet I
strongly recommend trying you probably
you can can do it in plenty of places
right now and if not you can also see on
the videos and YouTube
people people trying VR and how they
freaked out when something external from
the external world touching them or
something like that it means this
experience is very convincing and use
the real things that the real world
doesn't exist on the other hand we have
AR which stands for augmented reality as
the name says we are at melting
augmenting the world here so usually
those experiences are on mobile devices
that's the most common way they're
delivered right now on our mobile phones
or tablets so those experiences usually
use the camera lenses of our phones and
whatever the users see on the screen I
mean whatever the lens lenses see it's
being displayed on the screen and then
some digital content is being overlaid
on that on that screen sometimes AR
devices come in the form of goggles or
glasses like whole Lance's so there are
more fancy than I mean the experience is
more immersive so
we're where our wholeness are the VR or
air or air but because this is the
question I usually get from people so if
we would if you would think about our
world like like there is a physical
world or and digital digital food
digital world where the physical world
is the one we interact with on a regular
daily basis and digital world is like
everything when everything is rendered
we have the whole spectrum of
experiences that we can get between
those two I mean spanning those two
extremes and those experiences depends
on the kind of device that we are using
on kake'land kind of software that we
are using on those devices so most of
our devices they'll a very linear this
physical reality that's because most of
them as I already mentioned runs on our
on our mobile phones or tablets so the
user isn't fully immersed into this
experience the user is still fully aware
that the physical environment is there
is around him or her and the digital
augmentation only occurs on that on that
screen so it's not very convincing
some of them devices are a little bit
more sophisticated so for example some
kind of glasses like Google glasses or
any other or some phones with with some
more some better sensors for example and
on the other hand on the digital reality
end of the spectrum we have all the VR
experiences and devices that we have on
the on the market that means that
whenever we are using VR we are
completely not aware of physical reality
so digital reality is the center of the
of the experience so Halle lenses are
somewhere here this is basically the
device that is an augmented reality
device but the these are in the hardware
that is inside the the thing that it is
a head-mounted display as well it all it
all
all combined gives a better and more
convincing experience to the user
everything that the user see well that
depends on the on the application
actually but usually all the user C can
seamlessly blend with their environment
can interact with our physical things as
well so that this this immersion effect
in in Holland sees is way better than on
any other AR platform out there on the
market so that's why it's fast a little
bit more of this of this spectrum
because this digital content that is
delivered on this device is more
convincing to the user and that it
actually is appears in that physical
world on the other hand it's worth to
mention that with Microsoft did recently
which is mid-october last year they
released a bunch of mixed reality
headsets those are regular VR headsets
like oculus rift or HTC vive but the
difference with the other ones is that
they employ a little bit of technology
from from ho lenses which means inside
out tracking that means that those new
VR headsets they do not require external
external sensors for movement for
tracking their head and controllers
movement so they are easier to setup and
they're easier to use than than oculus
or vive for example and also they work
on the same platform as a whole lens
which means Windows 10 and that would
that's what is what mixed reality and
that's why Microsoft nowadays that's why
they used Windows mixed related term a
lot because they're trying to create one
one big common platform for our VR and
AR experiences which which they named
mixed reality and what Microsoft is
trying to do I think what what they
trying to do here is that they trying to
fill the whole spectrum of the devices
or one ultimate device that will be able
to deliver all kind of experiences to
the user
so holus what's what's that that does
this device how many of you have already
seen this the device not many of you how
so for most of you it's it's very new
thing but it's well it's been announced
I think two or three years ago I think
it was 2015 when they were announced so
it's quite a while
it's a device made by Microsoft and it
was released in mid-2016 I think the
first wave of devices was shipped to
developers in June 2016 so it's been a
while since they're on the market and
actually there are a lot of companies
and lot of developers that are
leveraging this technology and they're
delivering real apps for that and they
are solving some business cases or just
improving performance at some companies
with that and what I can say it's an
it's a independent independent device so
it's untethered you don't need any cable
or any external computer to run it or to
power it in any way and so it's it's a
PC desktop in there right self-contained
so this is a big load because that's the
only thing you have to take with you it
runs Windows 10 which might be good or
not depends if you like the system but
what it means is that it's not any
closed system or anything exotic it's
just Windows 10 and we program
applications there as we would be
programming for Windows 10 so basically
we could use Universal Windows platform
or just unity for anything
3d ok so whenever the user puts the
headset on it's not a clue that it might
seem as it is it's loaded but we can
actually see through through it so we
see our physical world through it and we
are firing we fire up the applications
any application and then all the sudden
we have some digital content rendered
around us which we can interact with we
can place whenever in our environment
and it
works like this so we have to have this
device on to see the Holograms that's
that's that's how we named the 3d
objects that are being displayed in our
application we named them Holograms in
this case so there are lots of scenarios
which I'm going to tell later but this
looks basically like this one of the
coolest feature features of Hollis's is
that as you can see here there are two
people wearing ha lenses is that we can
have an application that application
that can be fired on on to wholeness or
more and they can communicate with each
other like any multiplayer game I would
say because the mechanism under the code
is the same and those users can see the
very same holographic word around them
so they can easily collaborate on some
content and they can work on a connect
on a piece of 3d thing being rendered in
front of them so this is pretty cool and
this is what most of the companies are
asking for nowadays when it comes to
Collins's collaboration but let's talk a
little bit more about hardware so to
make all this magic happens we need a
lot of sensors of course so right in
front of in in front of it on our
forehead we have a bunch of bunch of
oops sorry we have a bunch of sensors to
make long story short we can say that
this is an enhanced connect device being
attached here to the glasses but it's a
little bit more than that we have for
environmental understanding camera as
those cameras are responsible for
scanning users environments so they
gather information about where are the
surfaces around the user I mean where is
the floor where is where our walls where
is the ceiling where are the chairs etc
so they scan the environment all the
time we have LGB camera we have mixed
mixed capture mixed reality capture
sensor which allows us for streaming
whatever the user see to the 2 to 12x
computer or make or record videos or
making screenshot also we have four four
microphones which are there for voice
commands and there are four of them
because the device has to make sure that
the voice command comes from the user
itself not not the person standing next
to it and there are some other sensors
like depth sensor or ambient light
sensor the the difference here is when
it comes to connect because that's the
most common question as well and comes
to sensors lots of developers ask if we
can get a rod stream of data from a
depth sensors depth sensors which was
possible with Kinect and with in this
case Microsoft didn't they didn't get us
access to that so we can't use raw deaf
depth sensor data the other thing is to
display those convincing Holograms we
need a set of really cool this place in
front of our eyes and to be honest I I
haven't I haven't delved into this
particular technology how are how are
they done and this is not important in
what what what's important here is that
they can render 3d objects in a way that
you can feel if there are near you or
further from you and the the resolution
resolution is pretty ok as well because
there are two HD engines one per each
eye which means it's it's 72 people I
the other thing is the brain which is
hole graphic processing unit this is a
core processor specially designed for
halal answers and this is a the
processor that processes all the sensory
data that that comes from all the
sensors so all the environmental
understanding data all the gesture
recognition or voice recognition all
comes for this and then is being
transferred to regular processors
processor when it comes to the rest of
the hardware there is two gigs of ram 60
gigs of SSD storage there's a lot of
there's Wi-Fi there are of course
batteries that
for two or three out two to three hours
of active use and the regular processor
which is 32-bit which is which might be
surprising because most of the most of
devices we have mobile devices we have
right now are reserved are 64 bits but
in this case is 32 bits so how do we use
how do we interact with our applications
I mean there is no mouse there is no
keyboard so we have to have a way to
interact with whatever we see there is
something which is called GG V input
paradigm which means gaze gesture and
voice those are mines main ways to
interact and issue commands to whatever
we see so gaze is this is very similar
to how it's done in VR so basically we
have a cursor in front of our eyes and
this cursor follow follow our gaze so it
doesn't follow our pupils it follows our
gaze so we need to actually move our
head to move the cursor so that's that
that's our cursor so we can imagine this
as a laser being attached to our
forehead so whenever whenever there is
whenever we're looking at something in
with a head movement the cursor moves
moves with our head the other thing is
our gestures we point some things with
the cursor with our head movement and
then if something is focused based on
where our cursor is where our gaze is we
can issue comments and one one of the
way to issue comment is by gestures so
the the sensors that are being here
tracks our hands they can they knows
when they know when when the hand is in
the area where the justice can be
detected and then they can detect
gestures so for developers there's
basically one gesture to handle which is
this this means air time this is like
catching a fly or ion pinching and this
is the own justice that we can handle in
our application there there are no
custom gestures we can't program
anything
custom which is a good thing in my
opinion because just imagine like every
application you would run would have to
you have to have a tutorial about what
what just are available in the
application and first you would have to
learn all of them and then you would be
able to use the applications so that
makes sense in some case I'm in most of
the cases of course so based on this
gesture
we have several gestures derived I would
say the one is hold gesture which is
like a half-hour tap and the other one
is manipulation or navigation gesture
which is also like hold gesture and then
we can move our hand and then we can
just track the hand movement and that
allows us for different interactions
like for example rotations or resizing
the other way to issue commands aware
application is by voice so basically we
can program voice commands the only
language of the able here and supported
is English but it's it makes things a
little bit easier when we can just look
at something and just say something and
something will happen okay so based on
that as I mentioned before there are
lots of companies that already are
leveraging this technology lots of
things are happening on the market and
just to name few few examples and I mean
there are way way more than that you can
google it you can google whole lens
manufacturing holdings medical apps
hallways whatever and you could you can
probably find lots of example examples
in and every industry so this is a city
bank they might trading application for
the traders so basically they can
visualize trading in for in real time
and then they can manipulate those data
interact with them in that holographic
word and they can share actually
whatever they see they can share to
other user with a tablet or computer
Ford recently announced that they're
there
employing Hollens to their design
processes so they can talk about new
designs of their cars or design them
with with this device and this is where
the collaboration thing is happening as
well Schneider Electric they are doing
application for service workers
so whenever the service worker
approaches some some piece of machinery
that the he or she is going to do some
maintenance maintenance work on there
can be like instructions being displayed
in front of the eyes and literally they
can show steps to perform on the machine
to complete the task and for all of
those who can actually find the YouTube
videos and see how those applications
work so this this is I think one of the
most common scenarios maintenance
manufacturing where apps for service
workers or field workers of deliver it
for Hall answers so that they can
perform better and those tasks Tyson
group the company that specializes in
creating elevators they also have an
application for service workers so
whenever the worker needs to do any
maintenance work on an elevator he or
she can get lots of contextual
information in front just in front of
the eyes like manuals video tutorials or
video manuals or they can perform a
Skype call or just see in large 3d model
of an object that he or she is going to
perform some maintenance tasks on any
other I mean there are other experiences
also those experiences they don't show
one of the coolest feature of whole
answers which is spatial mapping this is
where this is what happens when we put
the data gathered by a environmental
understanding cameras into yours I know
this is this doesn't say much but if you
blue your blue your vision for awhile
you will see it this is a model of the
room
this is a mesh of the scanned scanned
location so as I mentioned before
hallways and environmental understanding
cameras they scan user environment they
gathered that data about all the
surfaces that are around the user I mean
not all usually those surfaces that are
three meters I mean up to three meters
in front of the user and so to scan the
whole room we have to walk around a lot
but from the moment the whole answers
are turned on they start scanning the
environment and we can't stop it well we
can't disable Hollis's we can turn and
turn them off but whenever they turned
on they scan so it doesn't matter if
there is any application running on them
or not we do not program it it's there
it's on an operational system level and
based on the location holon says they
detect where they are based on some
location features and then then they
update the model underlying model of
this location all the time so for
example if I would just turn them on and
even if they would be just laying like
that they would gather whatever they see
in front of them
and this is cool we as the developers we
can make use of this data we can program
against this specific API to get to this
data and create models and that's one of
the the this model this is the very
simple application when we're basically
visualized
whatever whatever is being gathered by
the by the device so whenever there was
surface
I just rendered the white grid and
that's it right so for example for this
room it would be basically the model of
this room but whenever we have a surface
it would be visualized as this mesh this
is not very exciting this is only for
visualization and that the the room is
is scanned but the more more exciting
thing is that with all those meshes with
all the them with the model of the
location we also get colliders and
colliders is colliders are
are are objects that are invisible and
what they provide they provide all the
physics behind it to our to our
applications so we can imagine that
whenever we have a surface like a real
surface in our physical world we have a
nicely aligned Collider within our
application so whenever we have some
freedom object in our application we
would drop it on the floor it would hit
the collider that is being aligned with
the floor so the effect would be like we
have a virtual ball that bounces of the
physical physical floor and that's where
this merit motion is elevated to a level
that is very convincing so those are
examples of free games are labeled for
for whole versus I mean that those are
not all games that are available but
free most advanced ones that they show
how to leverage the device to to full
extent and we can see in those games I
mean I really recommend playing them
it's again Concord fragments and rubber
right so in one of it we can have like
virtual characters that that basically
they run on our physical for jump on our
bad on our table they fight each other
they bounces on the floor or walls
fragments is the game where your room
becomes crime scene so we have to find
clues which are hidden actually hidden
in your room you have holographic
characters coming into your room and
they can actually sit on your physical
sofa this really convincing stuff and
Robert is about an alien invasion
you have aliens that are driven through
your walls and attack you and those
damages to real wars are pretty
convincing as well as well so if you'd
like to try one of those just just find
me after the lecture I have a device
with myself we can try it okay I have to
speed up because I have some code to
solve as well so to develop things to
this device we usually use unity and
visual studio and that's well that's
because it's Microsoft things so we have
to have Visual Studio here apparently in
c-sharp under the hood but yeah but
unity is the most common one it
don't have to use unit you can have you
can use C++ if you like and directly
direct X you can use Unreal Engine as
well but the vast majority of community
gathered around this around this
technology and Microsoft itself they
support unity and that's the that's
that's basically technology that you
would like to use because I mean another
way you would be you probably buy your
own basically on the internet using
anything else so we use unity for
setting up our scene to program to put
together 3d objects we use Visual Studio
for writing scripts and building and
deploying the application there is one
problem with unity there are a lot of
versions they are releasing new versions
like crazy recently and apparently
Universal Windows platform is not their
main focus so from version to version
they they break stuff so if you would
like to start developing for hololens
just don't go with the newest version of
unity you have to go with one like ten
versions ago because nowadays whenever a
unity releases new new new update they
usually break something that used to
work and introduce some fix to to
something that used to not works and
vice-versa
and that's pretty insane insane nowadays
and there are lots of patch releases as
well and there is a batteries as well so
just just don't go with the newest one
because you would have like lots of
problems lots of problems seriously
there are on a whole whole lot available
slack there are lots of discussions
around that and actually we are
wondering if unity even care about that
but well that's that's some issue that
probably going is going to be solved
anyway and to speed up our development
we have two open source libraries
released by Microsoft one is mixed
rarity took it just to be called hollow
toolkit and this is just a bunch of a
bunch of scripts and
ready objects or assets for unity that
wrap some unity stuff and deliver them
to us in a more digestive way I would
say I mean all the developers that
started developing hello hololens
applications at the beginning they of
course they noticed some patterns in
what they were doing so they just
wrapped it up and released as a as there
is the open source library and it's
still well maintained and it's it's
being updated all the time so it's
pretty cool and the other one mixed
reality design labs it's also open
source library with well they're
focusing on components and there are two
applications there as well like like
games which is open source and you can
just download them and see how things
should be done you can treat them as as
a reference thing to do your stuff and
there are some components really
components there as well like buttons
for for AR or bounding boxes etc all of
them are available at geek on github so
you can find and download them from
there
ok demo time I guess are there any
questions to whatever I've said so far
if not I would just fire up unity and
we'll show you how to create how to
create simple application oops I have
two can you switch to mirror
okay so it looks like this have you ever
seen this ID okay so it's it's probably
something new for you how to zoom it
okay so this is unity one of the newest
ones and that's why I have lots of
problems with that as well
but it looks well it looks very similar
to other ideas for like 3d design or
where we create 3d scenes or animations
but if you're not familiar with that I'm
going to just quickly go for the for the
food through the how ID looks like and
what's where so here the main thing is
the scene view that's where we put up
our scene that's where we put the
objects that's where we construct the
whole experience this is the car
hierarchy view that's basically all the
objects that are in our scene there
they're here aligned with some some kind
of hierarchy here's project view that's
where we have all our assets that we are
using in our solution that's here and
here is an inspector pane on the on the
right that's a pain for whenever
whenever we have some object in our
hierarchy or project pane selected
selected we have properties here being
displayed okay so that's that was long
story short what I wanted to show you is
how to literally how to create like a
hello world application which is using
mixed reality toolkit so that you will
see that to deliver really simple really
simple application like rendering some
kind of an object in front of the user
eyes and placing it on the floor
detecting the floor and placing in the
floor is really really easy and what's
more it's usually what what businesses
usually ask for at the very beginning so
whenever I'm
I'm talking with any business and new
business any manager at some companies
about this technology how we can how we
can start working with and and start
solving some companies issue a problems
with that we usually start with a simple
proof of concept I usually get some kind
of a CAD project of some of some piece
of machinery from them and I have to
just display it or or place it on the
floor or whatever or just resize it or
just make it make it rotate or whatever
like that just to give them
understanding how the model would look
like on those glasses and then we
iterate over this kind of simple
application so to start things as I said
we're going to use mixed rarity toolkit
I already imported it it's under the
name whole toolkit here it's you just
download the package from github and
just import it in unity like like this
import package custom package and that's
it I'm not going to show you how how to
do it right now because that takes a
time and we don't want to take a waste
it right now but it's it's imported and
and right now I have all the components
that are in the packages available to my
application I have I have them organized
in folders which are named after the
purpose of those components right so if
I would like to if I would like to
program gestures or gays or curser I
just go to an input folder and just grab
some ready-made objects for for those
interactions but first I have to
configure I have to tell unity that I'm
going to create homeless application
here so that unity knows how to build
the application and that it has to
target Universal Windows platform
so under mixed rarity many I have I have
configure apply mixed rarity project
settings so what I have to say is that
I'm targeting Universal Windows platform
I'm enabling mixed reality I'm
direct direct VD and I have dotnet
scripting back-end I'm just applying it
and it should work okay that's all in my
build settings whoops I have to add my
scene so whatever I'm designing right
now I'm adding to the build and also as
we are going to use as we're going to
use our special special that data
gathered by Hollis's I need to ask for a
permission it's like asking for like
Internet connectivity permission on
mobile phones or anything like that I
need to ask for permission give me
access to special pre-sales perception
data that's basic basic configuration
now I can just put input start putting
things together to to achieve the effect
so camera camera is a component that
that gives us that shows us I mean
that's the target where all the
rendering will happen so whatever the
camera sees will be the end result that
the user will see and we will have
rendered the default camera is not
appropriate for hallways so I just did
it and there is a ready-made component
in holo toolkit which is which is called
Holland's camera which is well-suited
for Hall answers I'm just dropping it
here I'm gonna use gestures because we
what we're gonna do in this application
is just we're gonna load some external
model and we'll just write a script for
like placing it on the floor and
rotating it as simple as that so we're
gonna use gestures so there's input
manager for gestures I'm gonna place it
here we would like to have a cursor
which which are done here cursor is for
visualizing gate so whenever we will be
looking at something we would like to
have a little dot or a ring around it at
the very end and I simplest that without
writing a single line of code I have I
have my simple application which I think
can run that's it
you see we have we have a car sir this
cursor is a blurred blurred dot which
means we're not hitting any any object
because we don't have any object here
but we're visualizing our gaze here and
with the whole 2d component I have a I
have a support for a keyboard within
within unity so I can use WASD keys to
navigate and to move within that
application and mouse for looking and
and creating and issuing gestures so the
whole book is really handy thing so that
we do not have to leave you editor for
debugging okay now we would like to make
use of our spatial data gathered by the
by the hallways so I need a component
for for spatial mapping which is called
spatial mapping and well my computer it
doesn't have sensors it doesn't know
what room we are in so we have to
provide some kind of a model of the room
for for our debugging purposes so I have
a model of the room because what you can
do if all this as you can scan a room
you can get the whole glasses with you
right we've I P address of the whole
lenses I mean through that browser and
basically you can download the model of
the room that you're in I'm downloading
the model and then you can import it in
unity and it would serve as a as a model
for the for the application but that's
only for debugging purposes so I have
one of those already done and I can
import it
it's under default room and strategy and
I'm just dropping it here
and bam I have my room here okay see
that's that's that's a simulation of the
room that's basically a 3d object that's
all that's nothing special that's that's
only a bunch of triangles that looks
like a model of the room so right now we
can see we can see our cursor which is
here it's now a rink not not another
blurred dot that means we're hitting
some surface we're hitting some
colliders so that the user experience is
is better here so that the user knows
where exactly we are looking at so now
we would like to look at the floor or
whatever I'd create an I mean issue an
air tap gesture and then we'd like to
render some things so first I'm going to
input some object I have created that
forklift object for this demo so I'm
just I'm just going to import it okay so
let's create that script to have scripts
running within our unity application we
have to have some object to attach the
script and onto it so this script for
like placing placing a thing in our
environment isn't bound to anything
particular any physical object I mean
any virtual object in our application so
I'm just I'm just going to create empty
object empty object here and I'm just
going to name it system and having this
object selected I can add component to
this object and I will just name this
component placement and that's going
going to be script having this done I
can edit the script by double clicking
it and it should open
Visual Studio that's right
bigger okay you can see it it's good or
bigger like that okay so we have to add
the possibility to to handle the gesture
so having mixed rarity toolkit in place
we can just program against some
interfaces instead of going through
unity API and subscribing to some some
events etc mixed rarity took it nicely
wrap this this for us and to handle alt
up gesture we simply have to implement
in handler and this interface has only
one method to implement which is on
input click and this is going to be
invoked whenever the device detects that
we were doing this right as we do not
have anything on our scene and we do not
point at any particular object when we
are performing this gesture we need to
say to the application that this gesture
is kind of a global gesture so that it's
it's not based on some focused object to
do that we just need to tell input
manager that this object this object
that we are programming the empty game
object with this script which is game
object it's going to be a global
listener the funny thing is that lots of
things things in unique and mixed
reality cook it are singleton so that's
if you're an enterprise developer that
might be something unusual here and
we're trying as enterprise developers
we're usually trying to avoid this but
in unity we have lots of lots of single
Atos so that's something you have to get
used to
whenever programming for unity okay so
whenever we're actually issuing an
airdrop gesture we would like to detect
if we're looking at the floor and
if look we're looking at the floor then
render something so first we need to get
information about where actually we were
looking so there's this one there is one
one handy component for that which is
called focus manager of course it's a
it's a singleton but with this focus
manager that's a component from majority
toolkit get focus details we can get
focus details with where off of the
users gaze so based on case whatever
case manager is giving us so whatever
the user is looking at just give us this
point whenever we are crossing some some
virtual object in our case that's going
to be the special mesh right so we can
then if we're having if we do have the
details about this point where our gaze
crosses the special match then we can
get all the coordinates and normal
vectors from that point so now if we're
looking on the floor or anything else
any other mesh we have to detect if it's
a floor so if it's a if it's a flat
thing so we can get we can check if the
normal vector of this of whenever we are
looking at is how much it deviates from
the up Vector abnormal vector and based
on that angle we can save something in
the floor or not so what I'm going to
quickly do here is I'm going to
calculate that angle I go from normal
which means give me the focus details
normal which is normal in a point where
we're looking at and vector free app so
normal vector pointing up so if this if
this angle is like less than for example
15 degrees let's say that's only for
demo purposes then we can say okay it's
a floor okay so if it's a floor we can
instantiate our our forklift model that
that happens with instant method from
unity and that's going to be forklift
and that's where I will show you how
dependency injection works in unity it's
with public fields and I will show you
in a second okay so we're instantiating
forklift and then we're changing its
position to whenever we are looking so
whenever we are hitting that that that
floor does does the point where we would
like to render this this hologram and I
think with that I hope it will work it
should it should appear on the floor
where we were looking at we would like
to also disable this special mesh so
that we don't have those triangles
around us special
draw this one measures false and also
after the event is handled we can we
have to tell our even pipeline that we
used this event so that it's not bubbled
feather so as simple as that we also
have to define this forklift here and
given that it should update here pretty
soon okay so I updated my script and and
filled in in properties pane I can see a
forklift think here and that's the every
public field on our script will be
displayed here and that's our dependency
injection mechanism so I can literally I
can drag I can drag my prefab my
forklift prefab from here to here and
that's where the magic happened with
dependency injection so now whenever I
would look in the floor oops
BAM I clicked on the floor and now I
have my forklift on the floor right
whatever is black here will be
transpired on ha lenses so I don't know
how much time left I have probably
probably it's is it done already
okay so can I just show you one more
script it's a it's a lunch like five
minutes okay okay so I would like to add
one more script for like rotating the
the left the forklift so I would like to
I would have to add the script to this
this object that I'm using which is
forklift model here so I'm just
selecting this this model here and I'm
adding a component which is which is a
script as well and I'm just going to
name it forklift behavior
and we're doing it
okay I have no script this is attached
it's being attached to my forklift model
so I would like to handle manipulation
gestures so I would like to be able to
click on it and just move my hand like
on x-axis and then make it rotate on the
y-axis so it rotates like that so for
that I need to implement an eye
manipulation manipulation Handler gives
us ability to handle and handle our hand
movement we have a bunch of mad methods
to implement here like manipulation
cancel which we do not bother with right
now manipulation completed we do not
bother with that right now as well
started also with that we have
manipulation update that's where we will
be getting the Delta and a value of our
hand movement and that's that's what
what what it was interested for us so
here we're getting Delta of our hand
movement which is in even data
cumulative data and based on that Delta
would like to rotate our our model our
model is something that this group is
attached to so under our transform field
we have transform of our model so we can
rotate this transform only particular
axis we would like to rotate it in
rotated on an y axis so x axis will be 0
now we would like we have to give some
value for y Y rotation which will be
Delta X so our X of our hand hand
movement move would would be translated
into Y rotation for that for the object
I have to place it - here because of the
values that are being given to rotated
in a correct way and that's pretty much
it
let's see now
okay I'm missing one thing my cursor
disappears when I'm looking on the
object because the object doesn't have a
collider so what I need to do is to find
my I need to find my object and I need
to add Collider to it mesh Collider and
that's that's it
okay now the the the model has a
Collider in it so my gaze collides with
the object and I can see my cursor have
been sticking to them with the object
and now I can just grab it and I can
rotate it as simple as that
so as you can see with a very little
amount of code like two lines here and
several lines here we can create a very
simple demo but when you would create
something like that and put it on your
bosses or the clients head and show that
this is something real and you can
interact with this stuff
and this stuff actually is in front of
you that's really convincing and that's
that's where a lot of interesting
discussion and can start I would deploy
it to a whole ants but I actually have
it deployed so if you'd like to see it
you can approach me and I can show you
this application on the whole laces and
I can show you any other so now it's
time for questions I guess if there is
any if not just just approach me after
this session I can show you some more
within the device okay I guess
everyone's waiting for lunch see you
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>