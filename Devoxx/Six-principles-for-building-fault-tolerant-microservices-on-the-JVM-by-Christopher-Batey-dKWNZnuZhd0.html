<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Six principles for building fault tolerant microservices on the JVM by Christopher Batey | Coder Coacher - Coaching Coders</title><meta content="Six principles for building fault tolerant microservices on the JVM by Christopher Batey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Six principles for building fault tolerant microservices on the JVM by Christopher Batey</b></h2><h5 class="post__date">2015-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dKWNZnuZhd0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone well thank you very much
for choosing this is your a final talk
of the day I wonder how many people are
here just to make sure they get a seat
for the movie because you could just
stay here huh maybe I would have
probably done that so this talk is going
to be from practical experience I had
building a large system over the course
of 18 months to two years by the top
widened by the title of the talk you can
probably realize that we did go down a
micro service route this was a few years
ago so we weren't really calling it that
we were just calling it multiple she
medium ish services that interact to
each other over a network
I normally title this slightly more
negatively though the dangers of micro
services so don't worry there's no micro
service gonna hurt you or anything but
they could keep you up at night because
probably the most apt title I think I
can come up with is this one because
this is pretty much what happened to us
lots of different things happen when you
decide to spread your logic over many
many servers I spent a long time working
on services spread across many servers
as well as working with distributed
databases so yeah stuff does happen a
little bit about me I'm a freelance
developer slash consultant the system
I'm talking about was built at a
large-scale internet television provider
over in the UK I then spent time working
for the company behind Apache Cassandra
data stacks and now I'm back freelancing
mainly at the same company I was at
before I do lots of work with building
our platform for them with docker
kubernetes and large amounts of
Cassandra and large amounts of JVM
services running inside docker
containers I spent a lot of time working
with Cassandra also used lots of other
databases I also worked for a little
company called the last pickle these are
the best Cassandra consultants in the
world so if you actually want some help
for this stuff then then go and speak to
them if you're with your happen to be
from the UK on London and you want a job
then come and work at my main client
right now because they are hiring
heavily if this stuff interests you I'm
working on a massive new Greenfield
project and we need another 20 30 people
so do come and talk to me afterward
anyway on to the real stuff so we'll
define what we mean by a micro sir
I've got a very good definition for that
that will trump all other definitions I
will talk about what type of faults
we're going to deal with and then you're
not going to get six principles I'm I
apologize the title of the talk has lied
to you we're only going to get five but
we're gonna get a live demo of one
because the last time I did this talk
someone came to me after and said they
didn't believe me
they said that can't happen No so we're
gonna do a live demo which could go
terribly which I guess will be
entertaining for you and or embarrassing
or it could be a good demo we'll see one
of the emphases of this talk there will
be anything that can happen in
production we can reproduce locally in
our say continuous integration
environment environment I don't believe
that you can classify something as a
network glitch and then say or only
happens in production I'm going to talk
about ways in which we can build
automated tests to recreate things like
saturated and slow networks so a bit of
background about the SIS system that
were built it started off with a cut
like most great stories Tom cut in this
instance and it was some time ago now
when internet television was not popular
we still had to turn up at 9:00 p.m. to
watch our favorite program yet the
company I was working for at the time
they thought maybe people will want to
watch Game of Thrones on an iPad on a
toilet
even though iPads didn't exist probably
and neither did Game of Thrones and it
did it got popular so by the time I got
there it looked more like this it was
many many Tom cuts for the nice
expensive load balancer in front
however this application had not been
built with horizontal scalability in
mind so it is using lots of our friends
called session state right so soon as
someone was logged in and watching a
program then they had to be pinned to
that note it had sticky sessions and
what generally happened is either as one
of two things the first thing is the
database would fall over it was a
database produced by our favorite
company here Oracle and sometimes it
wouldn't keep up with the fact because
we weren't necessarily scaling the
database it may have been running on a
largest server but we weren't
horizontally scaling it but we were
running more and more instances of our
application
if that didn't happen then we'd normally
need to take down a couple of cats now
these could fall over they could just
run out of memory they could have
something to do with perm Jen who knows
and what would generally happen then is
all of the clients that were logged onto
these servers would come out they'd be
logged off which in generally means you
end up with unhappy clients but more
importantly they come back to the load
balancer it can't be a sticky session
now because that server doesn't exist
anymore and then this happens it takes
down everything because we have all of
those users coming back and hitting all
of the rest of them so what are we going
to do about that or what do you think
the main thing that caused this to
happen though two big reasons and we'll
forget about technology for a moment and
we'll talk about the true problems to
see whether we could solve those if we
could just get rid of Game of Thrones
because that every time there's a new
episode of Game of Thrones this would
happen because it's ridiculously popular
and a silly football team called
Manchester United because everyone in
the UK wants to watch them and actually
one of the biggest events in terms of
concurrent users on the system was when
Manchester United used to get to the
knockout stages of the Champions League
and they happened to play Real Madrid
hey what we gonna do about it so we
decided to work out which parts of that
application were actually causing it to
fall over there is a particular area
which hit the database extremely hard it
was keeping track of every single event
that happened whether someone logged on
logged off watch Batman turned off after
Real Madrid scored who knows and and
that they all need to be persistent and
that would also be used for billing so
that needed to be properly persisted and
it was putting too much load on the
database the other was a service which
was what we needed to do for every
device that was watching television it
would need to keep a session open with
it and it needs a heartbeat we stopped
people from watching you know using
their account on multiple services and
you need to keep this going so we wanted
to separate these into small components
maybe micro services and have them
running separately and scale them
separately we didn't do this right away
so we did the classic pattern of pulling
small sections out the bits are actually
causing it to fall over to buy us time
to eventually you know replace the
entire system and one of the other
architecture
principles we followed was that each
application then got its own database so
never did two services share a database
we happen to use Apache Cassandra for
everything but they were separate
clusters they were separate database
instances and then eventually what does
it actually look like now in production
it's lots of services all with their own
database cluster and everyone lived
happily ever after
so we're going to go through all of the
five principles now you don't get the
sixth that essentially had to change my
mindset to still be successful building
software this way and I pulled out a
little example which of course is on
github like the rest of the world and
we're them I'm going to do some demos
we're gonna have a look at some code etc
the technology involved we're going to
use drop wizard so no spring no je sorry
we're gonna inject faults into our
system using a really cool thing called
wire mock if you haven't heard of wire
mock you will leave this wanting to use
why mock lots because I love it we won't
talk much about history but I'll just
tell you it's there and we're going to
use a really cool tool called saboteur
not saboteur is as a wrapper around a
few linux commands which enable us to
say slow down packets in a network etc
which means that we can actually create
deterministic test of things which
previously we just say it's a network
glitch anyone ever had that no no it was
GC followed by a network glitch that's
the best excuse so what do we mean by
microservice well here's mine you don't
need it the fancy injection framework
this is opinionated don't worry about
this but yeah we mainly use drop wizard
just by itself if we're going to have a
small java service I can put things into
a constructor myself I don't worry too
much about that the other part for the
thing is we're going to be creating
tests so the organizations I've worked
out recently have been really quite good
at having automated accept acceptance
tests for each piece of functionality
that we deliver right if you don't have
an automated acceptance test I don't
think it's a feature it's more just a
rumor might be there today might be gone
tomorrow who knows but what I really
want to encourage is for all of the
different faults that can happen in our
system we also have a test that runs and
we want it to be a quick test I don't
want to wait until I get to my say
single
component NFT which does a soak test
over a sailor days and weeks I want to
find 99.99% of the problems early on and
I want as soon as I check in my Cove for
some automated tests to build and to
find these faults and if one if we miss
one which we obviously do when we
actually come to reproduce it the first
thing I want to do is create an
automated test for it before I fix it to
actually prove that we fixed it so yeah
you don't know things a fault-tolerant
if you don't test fault we're going to
use lots of test doubles for this and I
don't mean something like in process so
you know maketo etc go ahead do that
that's fine but the type of test we're
going to talk about here are going to
use something I call a protocol level
testable so this means that when our
service is interacting with it let's say
we build a war file and deploy it to Tom
cut or we build an executable jar with
an embedded jetty what that when that
when we're testing that in isolation
then it's going to interact with test
doubles and it's not going to realize
that they're not the real thing so it's
going to be real TCP for HTTP of course
it's going to be HTTP we're going to use
why mock I spent my evenings and
weekends building the equivalent for
Cassandra so you can pretend that you've
got a 30 node cluster and pretend half
of them are down and see if your
application actually handles it and I
started playing around with one for
Kafka and this is a lot of the concepts
for this was stolen from from release it
which is a fantastic book that predates
this whole micro service micro-service
thing and it's actually talking about
you know problems in connection pools
between two very large monolithic
applications so these are what the tests
are gonna look like we're going to have
a protocol level test of all pretending
to be our dependencies and I'm typically
going to run that on a virtual machine
for reasons that we'll see we'll see you
soon
we're going to be deploying our service
exactly like it is in production if it's
in a docker container that's what it
will be if it's an executable jar that's
what it'll be and it's all going to be
real protocol here as far as I'm
concerned if if you bring a jar into
your application that you say is yours
and you test and you say it's ready for
production the jars even the
dependencies even if they're from a
different company or they're from the
internet because everything is
trustworthy on the Internet
then that's part then if your process
that you're supporting and I test those
as much as I would test my own business
logic I do in a different way you know
it's not typically unit testing of logic
it's basically putting it under adverse
failure scenarios which we'll do today
so let's get on to the principles as
you'll notice we've only got five but
we're going to demo the first one we're
gonna show you that if you're dependency
trickles network traffic back to you you
can block forever regardless of what
network level time aren't you sir so
what why should we why should we time
out quickly well let's say we've got
multiple instances of our application we
could be an individual instance which is
misbehaving etc we want our clients to
be reroute it to another one in some of
my favorite technologies like Cassandra
having a slow node in a distributed
system is vastly worse than having a
dead node you know people wait for it
and then you everyone blocks and it cuts
Cades all the way up your dependency
hierarchy so even if your business do
not give us l8 define them yourselves
too technical isolate you know and
define them say the average and then a
percentile level and you want your
services you want test to fail if for
any reason you can go above this so
which timeouts are we gonna set so we're
going to concentrate on when were
calling out over the network so HTTP
clients database clients queuing clients
all of these things that reach out over
the network though some of this is
applicable regardless of what the codes
doing I'm the first thing you generally
look at and hopefully you have these
settings on your HTTP clients etc are of
course your socket connection timeout
the amount of time it takes to establish
a TCP connection on the default for this
on most systems is something around four
minutes and you probably don't want your
SLA to be four minutes so we want to set
something there once we've actually
established the TCP connection then we
want to timeout waiting for data to come
back and the default here for most
systems is unlimited so if you have a
nice firewall that sits between you and
your dependency and decides to drop your
connection without setting a reset
either way then things can hang forever
this happened to me in one company where
everybody interacted via Web CRM
the firewalls had knocked down all of
the connections and the every single
service was up and happy everything was
hunky-dory however the whole system from
the customer's point of view was frozen
so we had like health checks and status
checks going to each individual
application but absolutely nothing
worked and if we set these two timeouts
on all of our database drivers and HTTP
clients and we stopped using the ones
that don't let you set them then
everything's gonna go really well but
not for long
all right if we only rely on Network
level timeouts some bad things will
eventually happen they'll definitely be
a sub Thunder involved and you'll
eventually get shouted up because I work
as a freelancer I might not get paid
which is a really bad thing so what do
we do next so we've set these two and
we're good to go so what else can take
time apart from garbage collection we
won't talk about garbage collection
today well the next is resource
acquisition so most of these HTTP
clients and database drivers that we use
they I hope they don't do not make a
separate connection and amuse a separate
request every time you ask them to
otherwise you'll do some kind of
distribute denial of service on your
dependencies if you get a flood of
traffic so normally have a very small
pool of connections maybe five-ten mix
cetera or if you're using a technology
like Cassandra where you can multiplex
over the same connection it might be -
might be very small number two or three
so imagine you fix this you realize that
you can set this and when you think
about it it's it becomes intuitive
because if you go into your piece of
code which is say a database driver and
before they even send your request
before we get anywhere near the network
it's waiting it's in a queue it's
waiting for what a connection to become
available you haven't gone there the
network so obviously the other timers
haven't started yet which is why it can
take much longer but if you find this
you'll become the hero you'll set this
and your customers will be very happy
until they're not and eventually it will
hang again and someone will blame it on
the network they'll say it's a glitch
and they're not sure what to do and this
is exactly what someone told me they
didn't believe so this is what we're
going to start to do a demo if we
quickly trying to not analyze what's
happening though before we demo it you
know I like to draw diagrams when I
don't understand how
something's working and I didn't when
this first happened so we've set these
three timeouts we've set a resource
timeout we've set a connection timeout
we've set the recruiting of the socket
read timeout surely this can't take more
than 700 milliseconds plus or minus some
maybe not so we're gonna we're gonna
create an automated test for it I will
be the test executor rather than
something like you know cucumber or some
framework but anything I do here is very
easily automatable we're going to use
vagrant to do it and this is so we can
spin up a VM to run our dependencies in
now most self-respecting Ops people are
not going to let you to run things like
iptables and TC etc on all of your
continuous integration machines so
that's why we're running it inside of a
VM it's also very useful that if in a
repository and it's a repository for an
application which has many dependencies
that if you include you know a vagrant
VM a vagrant with it then you can
actually spin up and it can be
self-contained etc and someone doesn't
need to get dependency x y&amp;amp;z to get it
running one of the things we're gonna
run inside that VM is why mock that's
gonna pretend to be an external HTTP
service so it's gonna look a bit like
this just like the picture at the start
except we're going to sneak Salvatore in
the front which is then going to play
around with you know IP tables and TC
and it's going to slow down Network so
during the demo to keep your focus these
are the things you need to remember if
we're hitting this IP address so I'm
hitting an IP address it's one 92168 2.2
I'm inside the virtual machine it's
where my dependencies are running if I'm
hitting localhost that's going to be our
application we're just going to be
running that inside IntelliJ and we're
gonna be interacting with it from curl
why a mock allows you to prime it you
can do things like when someone calls
you on slash name return Chris you know
200 or you can say 500 or you can say
delay the response for a while and see
what happens so we're gonna we're going
to put and you can prime it over HTTP
interface so when I'm doing priming it's
going to be over this thing here this is
the Chrome advanced rest client so this
is going to be priming wire mock when we
actually interact with our service when
we're pretending to be our test we're
going to be doing it with
curl on the command line such as the
thing to keep on keeping your mind if
I'm inside Chrome advanced restclient
we're doing a prime from the command
line then we're going to be hitting our
service and seeing what happens so let's
see what happens so I got a couple this
looks big enough that's the advantage of
being in a cinema so on this side on the
Left we've got localhost this is where
we're going to be interacting with our
service on here I've got a VM so this is
this is Linux because we're going to be
using some commands which don't exist on
Mac OS X and then this is localhost
this is the service we're going to be
playing with it's a really important one
it's called enterprise hello world and
what it's going to do is it's not going
to say hello to the world it's going to
be more specific than that it's going to
say hello to a particular individual
this is drop wizard and it's Jersey so
all it's going to do is make a call out
this is the Apache HTTP fluent API it's
going to execute a request it's going to
return the content as a string and then
it's going to return the response hello
followed by whatever came back from the
external service so this is micro
services 101 right we just spread it
into two separate services and this is
going to be running on say hello we can
start this up or maybe I've already
started it we'll start it up I'm
actually wearing a zero turnaround
t-shirt and restarting my java
application I'm certain irony there I'll
tell me off so let's see here the other
thing I'm going to do is time everything
so I've got time curl which is just a
command line you know thing for hitting
HTTP and we're gonna go to localhost
8080 say hello what did we get back
we've got back a 200 ok so everything's
good and it said hello Chris why a mock
is default Prime is to respond with
Chris when you call it on slash name and
this took our whopping 400 milliseconds
and that's just because it's the first
time I hit this service if I hit it
again we're down to 19 milliseconds can
everyone see that I sure I there we go
19 19 million
what we can also do is hit our
dependency directly so here I'm actually
going to hit one 92168 2.2 so this is
not going through our application under
test this is going directly to our
dependency and that just responds with
Chris so the first thing we'll show you
will show you how you can use this for
happy path testing so with why mock what
you can do is you can go to its admin
interface which is running on this URL
don't worry about the syntax of why
mocks there are many different certain
many different pieces of tech which can
do this but it's it's pretty intuitive
you say when someone calls you with get
on slash name return at 200 but this
time we're gonna change our name to
Brady so our micro service that we're
calling out to is going to change its
behavior this is a prove you that -
dynamic call out so once we've done that
Prime I can call it again say hello and
hopefully it should say hello Brady
instead of hello Chris which it does
fantastic that is not very exciting
though let's start doing something more
exciting one of the coolest features of
why Mach is to be able to delay
responses but have them still be
successful so here we're going to be a
200 it's still gonna be Brady so we
haven't moved back to Chris but we're
actually gonna delay the response by
20,000 milliseconds let's make that
let's make that ten thousand
milliseconds so here we've got the fixed
delay milliseconds so I'm gonna send
that in and then we'll hit our service
again and we're hanging so now I need to
entertain you for 10 seconds while this
finishes which is not a good thing if we
were monitoring these separate services
we would show the name service as being
slow and we'd maybe call the developers
up and say hey don't your service is
slow but our service as well that's also
being slow you know we haven't narrowed
down the problem we've got a cascading
failure because service X has been slow
service Y which calls out to it is also
been slow so this isn't very good and we
shouldn't do this imagine this if you've
got 50 micro services and you've got one
calling another you know you'll never
finish a response so let's change the
code of it so this API is pretty
intuitive we can do socket timeout and
it's in milliseconds so maybe we can set
it to 500
so we're starting along the line of
having the technical requirement that we
want to either respond within 500
milliseconds or fail so again I'm gonna
have to restart because I didn't get J
Roble but fortunately this is not a
large server this is a little embedded
jetty so it's going to take about three
seconds so that's restarted so if I do
it again now bear in mind that why mark
is persistent it still is going to be
delaying responses but our server should
know time out quite quickly it's the
first time I've hit it since I've
restarted the JVM so it still took a
while but here we've flipped to a 500
but we've received we've responded and
under a second hit that again should be
around you know just over 500
milliseconds so even though our
dependencies been slow it's still taking
10 seconds we can set it we can actually
respond quickly if we looked at our
response times in our say graphite
metrics view we'd see us is responding
with 500 and in 500 milliseconds we'd
see the dependency has been slow and we
should probably wake those people have
not my team which is good so how we done
shoot a stop no so the next thing we're
going to show is why you definitely need
to set a connection time up so in why
not you can do this thing called reset
which basically puts it back to the
original state so you can if you're
running this as an automated test
harness you can reset it between them so
I'm going to reset which should mean
that my service should become fast again
and it has so it's done it in 35
milliseconds followed by 18 milliseconds
which is great this time we're going to
be in the scenario where our dependency
is fine we're fine but there's an issue
in between us so we're going to use a
command called TC which is a linux
traffic shaping command I've got myself
a little script here which is going to
run this funky little thing and
essentially it takes one parameter and
it's going to delay the outbound traffic
by that many milliseconds so I can do a
delay of hmm 400 milliseconds and then I
can hit our service again so do we
expect this to succeed who thinks yes
it's good right cuz it's under 500
milliseconds I hope this succeeds it's
meant to succeed shouldn't tell you that
should I know I did it was 200 but it
was
19 milliseconds so we're slow we haven't
hit the 500 milliseconds yet so we're
doing pretty well you know we're on the
edge let's make things worse right so
let's reset things and let's let's say
that the network is having a really bad
day
and we're gonna delay it to 2,000
milliseconds who thinks this is gonna
work maybe who knows what worked
done it no because we already had a
connection established right the HTTP
client we're using is fine but what
happens if it then doesn't it doesn't it
doesn't have a connection established
then we hang we wait and we're down to
two and a half seconds which is not good
we're not we're not doing our job as
meeting our SLA so we can fix this one
quite straightforwardly by sending a
connection timeout restart our
application and the great thing I
actually like about this as well is
we're seeing the exceptions which are
HTTP clients except for throw which is
really nice because we have Java it's
statically typed language and then we
throw runtime exceptions everywhere so
we just completely circumvent the
compiler and thus when we go up to say a
one point release on all of our
dependencies they can completely change
their behavior but if we have tests like
these we'll know they've changed their
behavior so what we can actually do
because this just says this is quite
small it says darn blast I have no idea
what to do but in reality we do know
what to do now we can catch the
exception which that library throws
anyway let's restart this and see if
that's fixed our problem so just to
recap service is fine we're just
delaying each packet now by 2,000
milliseconds or two seconds first ones a
bit slow as we expect but then yet we're
failing 500 and then just over 500
milliseconds it's pretty nice so how we
done no so we're gonna reset it again
and then we're going to set the delay
back to 400 milliseconds
however a penalty is going to do
something which we didn't quite expect
it's going to return us someone with a
rather large name
I'd say who his name is his name is
actually a snippet from Wikipedia
describing TCP could you never should
trust you
so I'll just put that priming then we're
gonna send it back not that one oh this
isn't good it was 200 yet it took two
seconds what is going on here our one
point six seconds
now we're imagine this multiplied down
your dependency hierarchy of deciding to
spread your service into you know 20
different processors running on
different boxes with good old cloud
networks in between that's what we did
anyway let's go back to the presentation
I'm not going to show you how to fix
that we-well don't worry so that demo
worked so I'm coming so that went pretty
well but what I really wanted to do is
just encourage you to realize that we
cannot rely on network level timeouts to
implement things like SLA s and if
you're making one external call to one
database or say to one HTTP dependency
there might happen every so often but
when we turn one of our customer
requests into 50 separate calls over the
network this happens you know pretty
much every request and we have a look at
a little bit more detail about what
could be happening here so we've got the
bit that we expected we already had this
we take time to get the resource perhaps
from our HTTP client we have time to
establish a connection maybe that
connection has been dropped and we had a
meeting then we wait you know for a
response to come back on that socket but
it has no idea what that message is
going to be right all it is saying is
I'm gonna wait so long to read data from
a socket
it doesn't need to actually be all of
your data right so if your dependency is
slow or the network slow and things are
trickling back and we actually receive a
small amount of that data you know every
every 400 every 300 milliseconds we
never actually reach the socket read
timeout which is why we shouldn't
implement SLA s with it another
important note is if you're using
frameworks like spring boot and drop
visit etc they give you some really cool
metrics out-of-the-box they allow you to
record the service time of all of your
applications you know as soon as the
request comes in say into your URL what
it then does is it starts a time where
it comes all the way around and then it
stops the time and then maybe you can
push that metric out to something like
graphite
log it to a file and you know pick it up
with a Splunk or something if we did
that for all of our applications so
we're going to record it this doesn't
work we're going to record it from the
edge of the app and the test and we're
gonna record it from our dependencies
but what we really want is this which is
the response time from the dependency
and the delay here with that TC command
was coming out of the network on the
dependencies box so if we were recording
I say the service time of our dependency
in the service time of our application
the application that we highlighted to
being slow would be our application
right because it would distillate it
would slow down our response time to our
client and we could be making 20
different calls out that rhetoric would
not actually be recording you know which
wouldn't be narrowing down exactly where
the slowness is so we need to make sure
you record a response time and there are
actually quite a few instrumented say
HTTP clients and database drivers etc
which will do this for us so we record
the service time of every application
but we also record separately the
response time when we make calls out of
course the service time of our
dependency is just this and as far as
he's concerned he is completely fine
right but what we really want to do is
narrow down this even if we're making
multiple calls out so how do we get
around it well I'm assuming most people
here and this project was primarily
built on technologies like Jersey and
drop Wizard there was some spring in
there etc but they were all synchronous
i/o right you get a they're all based on
servlets so thread comes in it calls do
get but if you're using something like
spring on top or Jersey etc it calls
your nice controller with your nice
annotations but your responsibility as a
developer there is to do some cool stuff
which hopefully means that your company
earned some money and then respond on
the same thread that's your job what you
can't do is give your thread through an
external dependency because they can
keep it for as long as they like we've
got some examples of when this happens
when it's not the network so we can't
let that request thread be hijacked a
lot of the new services on a platform
which I'm building at the moment
with a technology called rat pack which
is a different game where we're going to
be using all lots of things like
non-blocking an asynchronous I'm just
showing you a couple of examples of that
too so how do we do it well assuming
that we're still in this servlet based
programming model we have a thread we
have to respond on the same thread then
it's our job to protect it we're not
going to give it to anyone we're not
gonna we're not gonna let anyone take
control of that thread and
simplistically you could just put new
thread run throughout your code not
start oops don't do don't run all right
that's not a good idea you're going to
be creating threads it takes quite some
time etc so you'll probably use a thread
pool etc the abstraction for that in
Java of course having a queue and a
thread pool as an executor or we could
use something like history alright
hysterics basically does this for you it
puts lead calls out to dependencies on
separate threads or it uses a semaphore
to limit the number of concurrent
requests you have out so you don't use
up all of your container threads if
you're in a synchronous programming
model you know calling out to a failed
dependency and it does more than that
gives you a facility to have a fallback
so rather than just failing because your
dependency is slow you can actually just
make up an answer I'm or show an example
of that - I actually this is a
disclaimer I'm going to show you an
example with spring cloud which i think
is actually pretty cool i disclaimer
here's I don't use this in production
where's everything else I do and rather
than having to code this up yourself
spring can do it for you with a magic
annotation all right so imagine we have
this code here just bring rest
controller we have a mapping called
slash Gary now if we assume when we call
into any of our dependencies the code
really looks like this we will build
vastly more fault-tolerant services so
what air or all of our dependencies
really do is check if the time is even
if it's even good if it's not even
they're gonna sleep and take that thread
from you for five seconds maybe 10
seconds etc so let's just assume
everyone does this I put a couple of log
statements in just to see what thread
were actually running on I'm not
surprisingly spring boot uses imbedded
tongue cut by default we're using this
HTTP 8080 exact one for both here both
in our controller and when we call into
our very scary dependency so what we've
done there is potentially every
the requests we're going to sleep for
five seconds and respond slowly we're
not going to meet our ass late this is
pretty cool so with some class pass
magic class puff magic and a bit of
configuration you can start to just
annotate things and guess what happens
so this is basically telling spring to
tell hysterics that this potentially is
an external call or something scary the
only change here as we've annotated the
method and with that very small change
we can see that when we log out we're
actually on a different thread and you
can configure hystrix do timeout
separately but if you are dealing with
an asynchronous system which offers you
an api which perhaps returns a future
you don't necessarily need to do this
right so if you look at the cassandra
client for instance it isn't in sent
essentially an asynchronous system it
does not use a thread per request out to
Cassandra
neither does Cassandra internally so
everything you do with Cassandra is
asynchronous there is an execute method
for like executing queries etc but all
it does is call the async one and then
called get and block so really
everything is asynchronous and of course
once you've got a future
you can timeout independently of the
network request going out to your HTTP
dependency or your database so sometimes
when I'm dealing with these systems I
wouldn't bother wrapping it in a system
like history and I generally drive this
vehicle aemon's so there's a mixture of
services which use both so the first
thing I say is I want to write a test to
make sure I reliably timeout all right
if I'm dealing with a synchronous
dependency say relational database or
something based on a JDBC I'm probably
just going to wrap it in history right
away if I building with an async with
async system then I'll probably just use
you know timing out on the future
independently maybe then you want to add
some throttling you know that's an extra
feature at this point I might consider
to start using a framework which is
going to offer me this but as soon as I
start wanting to monitor all of the
failures on all of the successes and
perhaps measuring the response time say
over the last ten seconds at that point
some clever guys at Netflix have done a
lot of effort for Yui you can just use
history so that was number one we've got
four more to go but don't worry they are
shorter all right and the takeaway from
this is essentially do not use Network
level timeouts wrestle a
and if someone says you can't test them
just hit them with a stick because you
can't determinist we test all of this
stuff and once you've done it for one
service one interaction with one service
out to another it is Soak that one did
take about a week but then it takes such
a little time to then just have it your
default process I'm integrating with
something over a network I have this
barrage of tests for it and then then
you're good to go and this doesn't
always happen when there's actually a
problem with the network if your
dependency is overloaded it's actually
going to start dropping new incoming
connections it could be garbage
collecting it could be doing anything
right we don't really know whether a
dependency is slow or down etc so the
next thing I noticed that we do a lot or
I certainly did a lot was trying to do
things even though there was no hope in
me succeeding and it reminded me of this
this is a UK post office UK post offices
if you ever visit the United Kingdom
they are the slowest queues in the world
I'm not sure what's behind this glass
window here but they ain't doing much
alright and if I will have five minutes
left in my lunch break I would not wait
in this queue yet for many years I built
software that did exactly the same thing
right
I would queue up and I'd sit in the
queue even though there's no chance I'd
any chance of getting to the front
within my time up and this is this is
definitely true I'm not sure where this
quote comes from but it's definitely not
original I put it in quotes but it's not
right we've got a modern version of this
for the JVM and for Java applications we
don't generally send emails or if we did
we would definitely use a Q and a thread
pool to do it you might not find I'll
see they keep the thread pools and
queues right away but as soon as you
actually looked inside any frameworks
you're using any HTTP clients any
drivers there are gonna be hundreds of
them we love them and we have a really
cool facility for doing that inside the
JVM right we have many many different
implementers of X you to execute your
services X scheduled executor services
and they're an abstraction essentially
around a thread pool and then a worker
queue we even have a really convenient
factory for creating lots of them it's
called executors so you generally don't
even look at the constructors and the
configuration parameters
how many of these do you think from that
executor Factory executors factory have
a bounded queue any of these your
favorite every single one I've looked
into in every single one that's been in
any code I've ever written has an
unbounded queue and if that queue is in
front of a call out to a dependency that
is being slowed it gets bigger and
bigger and bigger
until eventually that are your JVM even
though there's nothing wrong with you if
your external dependency will run out of
memory this is not good because we had
the ability to service requests just not
at the rate we were receiving them so
we're a good service in this respect
we'd start rejecting it we'd get to
capacity then we reject future work we
shouldn't just be optimistic and start
queuing at the post office until we
actually end up back at work ok we need
to start rejecting work most of the time
though it's actually a mission of going
into your dependencies and finding all
of the places these exist and if they
don't if they're not exposed say the
configuration options for changing these
queue sizes etc then let's hope that the
drive you are using etc is open source
and you can raise a pull request and
again this type of thing becomes quite
easy to produce a test for I once went
to the company where they wouldn't test
this in functional tests in acceptance
tests they said we'll catch that in load
testing etc which is just is often near
many days later by the time you run a
week soak test but what you can actually
do let's say we're testing an HTTP
clients behavior when all of the
connections are in use right we can we
can set the time out to be very high
because in this respect we don't want to
timeout for this test we can then use
wire mocks to add a very large delay so
the requests that are going out are
going to hang for a long time
hopefully these libraries are going to
offer you the configuration to say how
many concurrent connections and how many
say the size of say there are internal
worker queue then we can send in two
requests one will be called out using
the thread say and one could be inside
the queue what happens when we send the
third request right so this is an
acceptance test that can run in a couple
of milliseconds right we send the
request we send another request then we
actually test the behavior on the third
what it is is up to you you can say that
requests should fail instantly or if
you're testing in a more integrated
environment it should be routed via the
load balancer to another instance etc
but we should be writing automated tests
for this so number three is failed
gracefully
who here develops a HTTP service or
interacts with a HTTP service that
produces Jason anyone most people XML
plaintext no right out of the people who
are either integrating with the system
that is producing you Jason or XML how
many people have seen a stack trace
inside HTML most it's good right we
don't do this because we're good
developers however the services that
we're interacting with will always do
this to you so the first thing you do is
try and parse it with say Jackson then
bad things are happened you'll have a
nice runtime exception and things will
cascade and why mock again is a really
cool tool for seeing what your HTTP
client does when your dependency is a
little bit naughty so you can write
tests for invalid HTTP which sounds
ludicrous until you have a faulty saying
network switch or something and it
literally does send you some invalid
HTTP malformed response bodies this can
be as simple as expecting it not to be
the actual format that the content
content type says this happens
reasonably often and as soon until
you've actually closed and read the
entire thing off the wire expecting
runtime exceptions to be thrown ideally
have a look at the code on the covers
and the most important one which we've
seen with mr. mr. TCP speckies name is
expects a different size response to
what you're up to what you're expecting
and if we look at some of the
functionality of why mock we can do some
really funky stuff here's one so it has
a Java API as well if you don't want to
send in Jason to prime it so you can say
when we call the dependency path
actually send a malformed response trunk
or we can say when Zaman call slash
fault send them an empty response body
even though the content size is rather
large my favorite there is this one if
you want to understand what the runtime
behavior on the failures of your
favorite Haiti
decline is prime this random data then
closed and this sounds ludicrous but the
chances are your HTTP client is going to
throw some type of runtime exception
that you should cut you should deal with
and then you should log something useful
say this dependency has sent me some
junk and then failed quickly to your
client as soon as you bump up a version
let's say you test this manually they
could change that entire behavior they
could be using a different runtime
exception they could change the
returning null you just don't know
because the compiler isn't going to help
you we actually changed every single
HTTP client in the system we were using
a HTTP client I forget the name which
had a connection full bug so we switched
over to a platter we had the tests which
were using real HTTP of a real TCP which
meant we could instantly rerun those
tests after we committed the change and
we knew exactly of our behavior changed
we're using completely different HTTP
client for that integration but all of
our fault tolerant behavior should be
the same otherwise our automated test
would tell us that it wasn't I spent my
time building the equivalent for
Cassandra so you're integrating with a
distributed database what happens when
half of the distributed database is
turned off because a hurricane hit New
York then you could set up a multi
datacenter spanning across two clusters
or you can use a protocol levels test
double in which you can just prime it to
make your application think that this
has happened and it does that
essentially by implementing the same
Network protocol that the database drive
that the database itself does so as far
as the client is concerned it's the real
database so last one controversially
named is don't hurt horses or don't hit
things which are down for me this is
quite a popular one no you know it's
going to come but people normally call
it circuit braking but it's a big big
culture shift I find this is not a
technical problem this is convincing
your business that you should plan for
your very fault-tolerant system that
they've just paid you a vast quantity of
money to build to go down and everyone's
a bit like what why have we hired you so
imagine we were creating the movie
player and just play a movie were
calling out to a user service to
validate a user and then maybe we're
doing some kind of device check to make
sure you weren't on fifty devices and
given your password to all of your
and then the last check we needed to do
was to say call app in service because
maybe when you purchase a new movie they
want to check maybe your age you have to
type in a pin or maybe just to make sure
that like the bill payer can you know
set a pin so not you know anyone who
comes into the house can't buy movies
what do we do when that's down well it's
we're gonna fail quickly we've already
established that but are we gonna are we
gonna fail instantly are we going to
keep hitting it if the poor pin service
team are frantically trying to you know
put their service back up and deploy new
versions and the first thing you do is
flood them with hundreds and thousands
of requests and knock it down again is
that a good idea maybe not so this will
happen however much you put effort into
making your services fault tolerant
you'll have a low balance a problem
you'll have a bad release something is
going to go wrong right so yeah even
though we're putting all of the effort
into failing quickly etc and dealing
with our dependencies been down at some
point one of them will go down and we'll
need to deal with it and ideally we'll
stop hitting them one of the most
interesting things to do here is to
apply for backs so we as a developer
could just decide well I would rather
watch Batman and not enter my pin when
the pin service is down and you could
make that decision yourself but you
probably shouldn't this is a business
decision that uniess need to give to
them and say hey you know we can either
of you downtime so a lack of
availability that will obviously annoy
customers or we might let their children
buy 50 movies when they thought they're
out for a date that's a business
decision they can make that decision but
in general I find it very hard to
convince companies to actually spend
development effort because it's a
reasonably complex problem I've stolen a
picture of the circuit breaker from the
aqueducts I think it's one of the best
ones but as essentially is around all of
your dependencies you're going to keep
track of the success rate over a very
short period say the last 10 seconds you
don't care if it failed three minutes
ago
if too many fail in a short period of
time then you cut it off and then you
can either go to an extremely fail quick
so this is even better than timing
outright we're not even gonna make the
external call um or you can go to a
fallback
so we can just basically hard-code it to
say true or false yep in checks fine and
this is again is up to your immaterial
business but if your business don't
allow you to do this the foot the
easiest one I think to do and this
introduced this perhaps without
permission is turning off broken stuff
also called the kill switch so we don't
need a fancy framework like hysterics to
implement a kill switch
we need an if statement we can do that
possibly without permission then what we
need is maybe some type of dynamic
configuration to switch it or a rolling
restart mechanism without downtime so
you can actually set the configuration
flag so this scenario I think generally
happens is when something bad happens at
a weekend some of the developers get
called let's say the pin service is down
and no one can buy movies and the
company's losing lots of money what you
can then turn around and say is would
you like us to just turn off pin
checking for a short amount of time and
we can start getting some some money
again and cost your manager says yes
because I'm a freelancer my weekend rate
is very expensive so yes do it quickly
and then leave and then you know you're
the hero it happens again the following
weekend our knee rings you up instantly
and says please press the magic button
right he's very happy about that when it
happens the third week though at that
point your manager is gonna say come on
you're a software engineer don't you
automate stuff can you please just make
sure this happens when they go down and
I generally find once you've had some
reasonable outages which is a terrible
thing to say
the business will have buy-in for
circuit breakers and on the system I was
discussing at the start every single
call out to every single database and
every single HTTP service is has a
circuit breaker wrapped around either
implemented with code or with hysterics
so the kill switch can be a really easy
one to to sneak into an organization to
get them thinking thinking that way so
just to finish up by far the most
important one biggest takeaway just
remember that you're putting it an added
complexity if you're going to go down
this micro service bandwagon right we're
going to be calling out over there
Network a lot so we're going to have to
deal with lots of lots of slow requests
and just think about 99% our what does
that actually mean it means say my
response time of 200 milliseconds are
the 99 percentile
only one in a hundred customers gets it
but if you make a hundred calls right a
near 99th percentile suddenly becomes a
lot more significant let's just forget
that we do not want to go to if we go to
the UK do not want to send any mail at a
post office this is extremely important
and of course we're going to expect
everyone else to fail badly but when
they do we're going to stop hitting them
right we're going to do certain
techniques techniques like circuit
breakers and we could sneak those in
with things like kill switches if you
want to take a look at the examples I'll
play around with the demo that is the
vagrant wire mock saboteur the priming I
did inside the Google advance rest
clients you can actually export those as
projects you know so you can have all of
the primes that's also checked in the
project so you can also do the priming
etc I definitely suggest people have a
read up on history even if you don't use
it read the documentation and it will
make you think of all of the things
which a very large organization has
found have gone wrong and have automated
ways to deal with so it's quite
educational and if you want to actually
test have automated tests for these
network faults etc definitely have a
look at saboteur because what it is is
essentially a nice little rest interface
around the commands we were running we
were running on the command line so on
that note I will say thank you for
listening and hopefully one enjoys the
movie tonight and thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>