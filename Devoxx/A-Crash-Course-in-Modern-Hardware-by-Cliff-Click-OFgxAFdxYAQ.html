<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Crash Course in Modern Hardware by Cliff Click | Coder Coacher - Coaching Coders</title><meta content="A Crash Course in Modern Hardware by Cliff Click - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Devoxx/">Devoxx</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>A Crash Course in Modern Hardware by Cliff Click</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OFgxAFdxYAQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yup come on now hi welcome to devoxx
2016 I guess I've never been to Antwerp
before so I'm getting a good dose of
non-california weather had as much cold
and drizzle as I'm prepared to have here
so not your father's one Norman machine
right crash course in modern hardware I
gave this talk like six years ago and
then I went back through the slides
recently to do it for DevOps here and
the answer is not much actually has
changed in the six years in terms of the
hardware a lot has changed under the
hood but it's mostly and not where you
were hoping we're not where we used to
get it so you know the the classic von
Neumann architecture is there's you know
program and stored memory memory shared
between programs and data sequential
execution model one instruction and the
next and the next in the next to get
your problem done great model for
thinking about algorithms um it's not
actually how computers work today it did
used to be that way so it's a nice
abstraction um it's it's a good enough
abstraction that that's what we always
typically think about until you start
getting into like real multi-core
multi-threaded kind of situations or
you're trying to get performance out and
you don't understand why your code is
slow and so I'm going to dive under the
hood and look at what goes on in modern
hardware and then map it back to what it
means to your programming model so the
chart on the left is one I did six years
ago the chart on the white is one I did
last week and and the scale the what the
metric I've used is a little bit
different but it's a log scale so it's
all sort of doesn't matter and what I
can see here is that since you know
2004-2005 throughput per core has gone
up about ten percent a year so in the
last eight years we've got roughly a
doubling in performance on a single
thread now we've got a lot more threads
we've got a lot more cores but any
individual core is only a little
faster in the last decade and that
brings up two different model
performance you can't write sequential
code on a bigger and bigger and bigger
problem and have it solve it just by
waiting to the next faster machine you
actually after write parallel code you
have to think about it that way and if
you have performance problems we're
trying to diagnose what's going on with
them um what has happened is that that
what what is important to performance
has changed a lot in the last twenty
years and it's not necessarily what
people think it is so let's dive in a
little bit and in roll back the clock
nineteen eighty eight people are using
Sisk is a why because they were meant to
be written by humans people were writing
their programs in assembly style and
then I had compilers yes and use a lot
of compilers but used a lot assembly and
the the Sisk approach was you know gated
from like ten twenty years earlier when
the expectation was you would write it
in assembly and compilers for this
newfangled thing that you might
experiment with as opposed the other way
around
so you had these very exotic Hardware
primitives that we would now only ever
put in a library things like string
pattern matching or polynomial
evaluation were actual machine hardware
instructions on our backs back so that
are a multiple level of memory and
directions per instruction it made it
very easy program mentally very hard to
program in hardware to make the hardware
go faster
I mean that led to a real problem with
performance as you went forward but at
the time the the cycles per instruction
the speed was basically dominated by
page faults right so you knew how many
clock cycles were on an instruction you
could just look at it and you could add
them all up and you've got a number but
if you missed a page fault you had a
swap to disk really slow locality was
important because you had to fit
everything in a small amount of RAM and
every time you touch disk you you sucked
so you tried hard to get your code to
co-locate and the linker mattered and
overlay linkers and crap like that but
performance was basically you know get
rid of page faults and then count
instructions then we have this you know
grand flip to RISC processors for
exactly the 50% a year I'm mentioning
there for about 15 years
processors got fifty percent faster per
year and that was simply because we went
to simpler processors and
people could begin to think about how to
make the hardware go faster a lot of it
was done with better pipelining but it
was a lot of transistor shrinkage
turning into higher clock rate as well
the is A's became more difficult to code
by hand people still would write
assembly code in small amounts now
though for all kind of rappers or
special instructions video codecs audio
codecs things like that but you weren't
thinking that you would ever write your
entire app and simply of course not you
wrote your app and see or later Java but
but assembly was reserved for pulling
out funny special custom instructions
and rarely for a hot code like the
codecs memory got a lot cheaper you got
a lot more of it you page fault too
quick really mattering you could run a
machine out of memory and page fault
itself to death but you had to try now
the default answer was you didn't suffer
enough page faults that you actually
cared so you you kept getting faster and
faster until we hit all of us at the
same time a bunch of different speed
walls power wall power goes up is a cube
of frequency so if you raise your
frequency a little bit your power is up
by a lot well if you get more power into
the chip you got to get out of the chip
or eventually the chip just melts right
just gets hotter and hotter and hotter
so you know people had these grand
theories that you know next year Intel's
10 gigahertz processor would have the
chip hit the you know temperatures equal
the surface of the Sun and you just do
the cube law and say oh my god it's
gonna be you know gonna blaze like a
like a thermal storm or something and
then you hit this ILP wall instructions
you know overlap parallel instruction
execution law wall which was which was
basically you know Intel was a king of
brainiacs there's the speed demon versus
Brainiac Wars I don't even remember
those um Intel was the was the champion
of the Brainiac approach and they hit a
wall that they couldn't get more
performance out by being smarter with
how they executed instructions although
they tried very very hard and then you
know memory got no faster um it kept
getting bigger kept getting more
bandwidth fatter pipes but the pipes
weren't quicker and that was some of it
was just simply speed of light their
distance between the memory chip and the
processor is you know this whatever
right okay well you know Cray came along
and said well you know nanosecond is a
foot so if you're this
- a no just for speed of light but you
still actually had to you know raise
voltages and drop voltages in a way that
you could count and you couldn't
actually hit speed of light
so whatever fraction you got that
distance mattered and it wasn't getting
any shorter and it was laws of physics
you just could make that better so
memory got no faster and so now you
couldn't you just had to you know if he
needed something from memory and it
wasn't available on the processor it was
over there you had to go get it and then
it that was just going to take the
length of time and it did so clock rates
are flat for the last 15 years or so and
you can get slightly faster by paying a
lot of money and putting giant coolers
on it but that's sort of a limit instead
we started getting more cores lots more
cars so so not faster and then the
question comes you know what is the
performance model now when I go get my
next grand x86 how do I think about
making my my program run faster and you
have to think about ok I need to do a
parallel execution and then you know one
of these I'll go through today here is
what that means in terms of coding style
and then there's the other major
performance model we have to take away
is that you're gonna be cache miss
dominated and you have to count cache
misses you know count page faults you
don't count instructions count cache
misses and that means you count levels
of indirection
well we'll cover that here and more so
let's look through what ILP man this is
the Brainiac approach to speed and where
it went and and then I'll give you an
example of where it died where it
stopped getting better right so a bunch
of techniques were used to get
instructions to execute faster most of
which were actually done to not raise
the clock rate but instead we're done to
allow parallel execution so instruction
level parallelism each instruction that
you run on a piece of Mini modern
hardware actually takes more than one
clock cycle people think I got a three
gigahertz clock I get one instruction
per clock no instead instead the
instructions all take some variable
count but four to ten something like
that and then you pipeline and
pipelining means you lay one on top of
another on top of another as fast as you
can and that works as long as the
instructions are all independent from
each other so here's a little tiny
instruction set
instructions you'll see me do a lot of
x86 assembly here so just I'm just sort
of faking it but this works on every
processor including arms and powers and
and all kinds of embedded systems that
are inside your your phone will have you
know a low-power Intel low-power arm
plus several DSP chips doing all kinds
of fun stuff they all look roughly the
same here so on an early machine this
would be for clocks I mean I worked on a
z80 chip and that was was for clocks
pipelining makes them appear like
they're one clock because you start to
add on one clock cycle my bottom picture
here one clock later the ad has done
instruction fetch you begin the
instruction fetch for the compare while
you decode the ad then you grab the
register and you take the 16 you run
through the adder unit and you get a
right back to you know RB acts of plus
16 and the last clock meanwhile the the
comp is one instruction one clock cycle
behind all the way down the pipe so
pipeline is very common that's been done
for a very long time all CPUs these days
do it so it improves throughput but not
latency which means it works great until
you have instructions that depend on
each other so the deeper the pipeline
the theoretically the higher the effect
of CPI you can go because you can get
more and more and more instructions
running in parallel the the major gain
from RISC CPUs over the older sis ones
was exactly this the simpler pipelining
the simpler instructions allowed better
pipelining deeper pipelining and and
simpler pipelining where you could tell
what you had to dependency or not with
much fewer gates and instruction
hardware and that let you run the clock
rate up as well along the way you lost
the multiple memory ops per instruction
you only do one memory cycle on any
given instruction it's either load or
store but it's not some random blend of
both or several loads or whatever and
then some ops dim board never got
pipelined or didn't get pipelined until
very recently or - not such a great
depth in your division and many
floating-point opted to get pipelined
and that meant that they had greater
Layton sees there so the the the major
conflict is that you have
sort of CPU resource you have a conflict
between and it causes a pipeline stall
or hazard the obvious one being where
you have to fetch a value from memory
before you can use it to do something
useful in the old days that would cause
a pipeline stall and maybe even a
complete pipeline flush and I'll talk
about that in a minute but a pipeline
flush on the modern x86 is like a 30
clock cycle delay and if you're counting
clock cycles that adds up pretty fast so
when does that happen it happens when
you miss predict when you miss predict
when you're walking through for instance
a large complicated data structure we
have to make a decision at every point
such as any sort of you know b-tree
style lookup you know if you're
predictable at the first few layers
you're almost always not predictable
further down in and then what looks like
this should be really fast it's just
load compare branch load compare branch
it's 30 clocks a pop and suddenly it
takes you know hundreds of nanos you get
into micros to do a lookup and if you're
trying to do anything fast that's going
to add up really quick so the the the
major thing that happened though was
cache misses so a cache hitting load so
here's a load instruction I'm gonna go
fetch from some address and RBX and put
the result into our ax so if I hit in my
cache usually I'm talking about l1 hit
it's a couple clock cycles 2 or 3 but if
I miss and I have to go all out to main
memory it's possibly 200 to 300 clock
cycles so you see a hundred X variation
in the execution time of that load
instruction so it matters a whole lot to
know whether or not this is going to hit
reliably or not and any sort of sort of
hot performance loop but one of the key
themes here is that I may not have my
value in our ax available to me for
hundreds of clock cycles that the the
the hazard cost for this or the hazard
the way you handle it on simple CPUs you
might stall until it shows up this would
be true for instance for GPU but many
programs many CPUs will run on until the
values needed so commonly you can do
execution while our ax is unknown of the
instructions in the middle and when it
comes to a point where you have to
use it then you stall and that lets you
get a chance to do some work in the mist
shadow in a cache miss shadow it does
tie up the load store unit which is
pending holding on to the memory bus
waiting for things to come back but it's
a really common speed up it lets you
work over a cache miss most modern CPUs
of any you know level of complexity will
have this including most your DSPs will
will have that much brains in them the
next thing people discovered was branch
prediction and so branch prediction is
one where you don't know yet how which
way the branch will go so you're gonna
make a prediction here I don't know what
the result of the compare on racks is
because the value of sine come back from
memory so I'm gonna make a guess it
turns out that for many branch
instructions I can guess highly reliably
95% of the time easily right and if I'm
right I can carry on and start issuing
the next instructions of course I'm
wrong I got a giant mess to clean up and
this gets into the Miss predict penalty
for instance I have a store instruction
coming up should I execute the store
prospectively what if I do do I write
memory and then I have to clean it up
with a branch misprediction I do that
because some other and core CPU might
see that right which not supposed to
happen so I put the store off into a
store buffer and I pinned it and I make
more and more things sort of pending
behind the branch misses and maybe I
won't have several branch predictions in
a row and if any of them fail I have to
unwind but since I'm 95 percent right
maybe I get them all right and then then
I get to go faster so it's a it's a
fantastic way to speed things up
it would look at that Mouse wandering
around here it was been done for a long
time in a lot of course and then we get
into the wide issue or multi issue and
here you might get multiple instructions
done on a clock and your cycles per
instruction would keep dropping as you
can execute a whole lot of things in one
clock cycle and again you have all the
same constraints of data contention or
miss prediction or whatever and an even
more expected of executions required but
it actually still commonly applies so
here I have this set of instructions
that are unrelated so I could why do
shoo them both same clock cycle for both
ops when they start they're going to
read and write unrelated
have to tech check that and that's a
quadratic operation as I add more
instructions to the issue but dual issue
is common on almost all CPUs except on
the most simplest in bed ones or like
the GPUs might not do dual issue then
you get more complicated stuff so
register renaming the branch prediction
speculation out of order execution
they're all synergistic they allow you
to continue forward progress in the in
the face of various kinds of hardware
stalls and cache misses and miss
predicts the the register renaming lets
you store all your output results all
these instructions are sort of speculate
I'm hoping this is what value when a
store into our ax but I don't know yet
and if I'm right I will then take this
extra register and call it Rx and say
hey that's the actual machine register
state and if I'm wrong I'll just throw
that thing away and and do something
else for re X so this was a great way to
let you execute further and further and
further it sort of into the unknown into
I don't know what the value is gonna
come back from memory I don't know how
this branch is going to predict but if
I'm right then I have this already all
those executions ready I will just take
it and go on to the next step so really
the goal here has shifted from just run
more instructions to get to run until I
can start the next cache miss because
the cache misses are totally dominating
so let's go through this here's here's
an example a little bit bigger I'm going
to load an instruction let's pretend
this is the one that misses from memory
it's 300 cycles to get it back so I want
to do something for that length of time
so I'm going to do another instruction
and I'm going to wide issue so I'm still
in clock zero and I'm gonna it's an
unrelated register read read and write
and I'm gonna you know continue going
forward now I'm using register renaming
because the first instruction is reading
rvx
and the second one is writing it that's
reading it and writing it the output
can't collide with what the load store
units using on input but I got register
renaming going on so it's all good they
don't want to do this one and I'm gonna
attempt white issue again because I'm
Neck City six and I can do quad issue I
don't have my flags so I have to wait
this instruction until the flags come
back from re ox
so I'm gonna I'm gonna you know
speculatively know that the flags are
well I'm going to know the flags are
unknown so when I get to this and I'm
gonna have a branch predict but my
branch prediction is pretty good so I'm
gonna pretend I get it right and I'm
limiting my forward execution I want to
start another clock astrology cycle so
now I'm speculative so I don't know what
that branch is gonna do but on the next
clock so I do a store into a store
buffer and why do I his store buffer and
set up in my cache or to my memory
because it might not actually be there
so the store buffer is a holding spot
for an unknown store instruction or
structuring that might not execute and I
can throw it away and it doesn't update
my cache if I throw it away
it could be also that RBX here turns
into know after that ad and I should
throw a TLB fault too but I'm gonna
pretend that it doesn't I'm gonna write
the address and the value in my store
buffer and carry on now that was a load
store unit and that's pretend I can only
do one of those a clock cycle so I
wanted to another clock cycle now I have
another cache mess that I'm gonna start
execution on and now I have two cache
misses running the memory also at 300
clocks and the load store units busy
again and so I come down here the Flast
instruction which needs our IX to even
start the load for the next one needs
the prior load get the next one so I'm
pointer chasing one pointer gets to get
from memory it's missing I need that
first pointer for I can even begin
getting the second pointer so what
happens and four clock cycles
I started seven ops got two cache misses
running they're gonna come back in
cycles 300 and 300 two so I had seven
ops in 300 two clock cycles the Miss is
totally dominating and this is the
normal case now you're on a modern
processor its cache miss 2 cache miss
that is your performance count your
cache misses don't count your
instructions so now you have to know
what pieces of memory you're touching to
know how fast your code goes every time
you add that new capital I integer you
pick up a level of indirection every
time you added a hash table wrapped
around a you know XML we wrapped around
a JSON wrapped around whatever every
level of indirection is what kills you
okay so looking back at how this all
worked out
Intel decided that it was having trouble
with the Brainiac so they tried a
billion dollars on titanium to mine out
static ILP which looked great in theory
assuming he had infinitely wide machines
with infinite number of registers and
cache misses possible and infant amounts
of speculation and the answer came back
is that actually didn't work the limits
of compiler knowledge prevented you from
doing all kinds of optimizations that
were necessary to get the actual count
of parallel infinite cache misses
running in parallel so it worked well on
some narrow set of domains and just did
not work out well on anything outside of
that in particular things that look like
Java or JavaScript too badly whereas the
x86 successfully mined ILP for about 15
years just an incremental addition of
perform attacks ever deeper pipelines
ever wider issue more parallel dispatch
giant reorder buffers lots and lots of
functional units
you know 120 instructions in flight I
think these days they'll support 10
parallel cache misses in flight but
they're hitting the limits limited by
cache miss rate and branch mispredict
both miss rates are really low but a
miss will cost you you know 300 clocks I
know for white issue miss Costas you
1200 instructions issued so a 5% miss
rate completely dominates so basically
ILP got mined out people kept throwing
more and more transistors at it and the
extra transistors were not helping
enough and Intel's actually backed off
from that a little bit and they don't do
quite so aggressively now instead
they're they're headed for the lots more
cores we kept getting more and more
transistors you know whatever Moore's
law means that it still means you get
more transistors so what you do with
them well if you try to get a little
more wider issue a little bigger caches
a little better branch prediction you
don't get that much performance because
those are basically mined out so they
went to the the multi-core solution
I may look a little bit at memory
because that is the dominant cost
I didn't update this chart but it hasn't
changed very much either so basically
there's an exponentially widening gap in
performance and it's slowed down and the
year since but the gap between CPU
performance and memory remains at this
constant very large amount memory since
then has gotten wider more bandwidth but
the latency has not improved
substantially it's still hundreds of
squawk cycles to get from cache miss
into your CPU core to do something so
cache misses still completely dominate
I'm going to touch this really quickly I
hope everyone knows there is an SRAM a
dram in the world SRAM is the fast but
expensive version this is what all your
caches look like on the low-low caches
in your x86 DRAM has a lot more a lot
fewer transistors and and because it has
fewer transistors is a lot denser you
get a lot more cache out of it and main
memories also done this way but the lack
of transistors means that they bleed a
voltage out over time and you have to
refresh it so it has more complications
to using it in the hardware and some
more variation performance and a lot
more density and a lot more densities
what people were looking at but it
doesn't get any faster
you can't Jam power into DRAM to have it
come out faster because it doesn't have
enough transistors for that you have to
very carefully read this one transistors
a little storage of power and then put
it back when you're done
whereas an SRAM you can keep smashing
power in until you melt the chip and get
things go faster yeah people use both
and that's because the closer you are to
the CPU the faster you go and that's the
speed of light but the the the you need
faster to keep up with the CPU so you
use the SRAM close but it's not very
dense you can't have much of it close or
you run out of chip space so you end up
with a blend of them both and it's all
down to like speed of light and and wire
delay I talked about before you know
here's there's a nanosecond a speed of
light well in copper on you know your
PCB board substrate you know it's
nanoseconds smaller and smaller and
suddenly you're in multi nanos to go get
it
so from there let's look a little bit
about what what happens here with caches
and why they matter the way they do so
the is people got faster and faster CPUs
they kept adding cache layers where the
the registers will see registers on the
CPU core the orange support there would
basically be one clock but white issue
so a quarter clock if you like you know
less than a clock cycle to get at a
register an l1 cache hit we're only
about three clock cycles and latency
modèle to is 10 to 15 it kind of varies
but that's the wife order magnitude
where main memories maybe 300 clock
cycles away and you end up sharing
caches so typically there is a lowest
level shared cache which these days is
in l3 although it used to be in l2 or
even in l1 and then below that is memory
and the elephant cache is guarding that
the cores from memory if they're able to
share amongst themselves and then the l1
and l2 are typically private procore but
with the high memory latency you know
the ILP is not actually helping all that
much because you're just stuck behind
the cache misses and and you know you
wanted ways to think about it is that
because of the giant gap between the CPU
and the memory speed not only the cache
misses dominate it's essentially
memories like you know the old disk
right if you page faulted that was your
dominant cost 20 years ago
these days if you cache miss that's your
dominant cost what can you Jam in your
own cache that's where your performance
goes soon as you're in l2 it's so still
ok when you get past l3 your performance
is going to fall off by 10x so in an
effort to make things go faster with
memory people started relaxing coherency
constraints and x86 went back and forth
on this a little bit and then they
stabilized out at the very a very
conservative model and various hardware
vendors have tried more aggressive
models to get more performance out and
it does work up to a point and I'll step
people through it but what I'm going to
show you today will actually also happen
on an x86 and just the exact same way
and it's a standard problem you have
that everyone is like double check
blocking
kinds of things and the lack of a
volatile keyword so let's look at what
really happens here because with the
these you know complex or programming
models where each core is doing its own
thing and has its own view of the world
the order of execution is relative to
the observing CPU you know shades of
Einstein here your view is its relative
different CPUs will see the world in a
different way and that's because they're
physically separate devices so here's my
little fake multi-core it's too simple
by an order of magnitude maybe two I'm
showing one level of cache with just two
cache lines but reality of course has
many cache line layers each one tends to
be about 10x bigger and 10x flower then
the cache level before it the main issue
here is that data will be replicated
there's no single home location for the
data I think oh I have this flag value
I've got it in memory somewhere no flag
value exists in memory yes but it exists
in your caches as well and maybe in your
registers at some point right and
because it exists in multiple places
there's a time and space for to come out
of sync with each other and we'll get
into that in a second the caches have to
talk to each other through some protocol
here I'm using a very simple one but
actual protocols are a lot more
complicated
this one just modified exclusive shared
invalid or I have an I on the right of
the lines and of all dashes it's invalid
this cache line holds no data s for
shared meaning the flag of zero is
shared it's value is known to be correct
in both memory and that cache and maybe
in other caches as well but not in this
case same for data on the right and then
last things know that memory controller
in memory it's not a first-in first-out
simple I just asked for a value and get
it asked for value and get it no it
supports lots of parallel cache misses
and the time to return a value varies a
lot according to what the hell the
memory controller is doing at the moment
so it could be that some things come
back fast some things go back slow they
get reordered inside the memory
controller all the time the memory
control is doing a best-effort
throughput it's not doing a FIFO simple
sort of thing
there's my little program classic double
check read kind of problem I have some
sort of initializer I run once which
sets some value to data set
a true saying data's available and then
a bunch of threads want to read it and
they test the flag and it's not set they
do something probably initialize it but
if it said hey they can go grab the data
and they need is immutable at this point
and they're all happy they get their
values this is like a good fast way to
have multiple threads where you
initialize the value wants said I forgot
the volatile keyword so where's the
button well how does this bug work so
the initial values here my caches flags
are all zero but you notice it's
replicated in a few places I'm gonna run
the Java code you know it's not flag and
the hardware instruction for it is
loaded into rax from the address a flag
so my CPU number one on the right the
green guy says I won't know what the
value rax is I don't know so I'll flag
it as in flight I'm trying to get rax I
go to load store you didn't say give me
the contents of flying so that hits down
to the cache layer in a clock cycle says
go to the cache give me the contents of
like so that cache knows that he wants
the value of flag he doesn't have it but
he's gonna get a copy he using a copy
from memory because that's where the the
golden copy is when he gets a copy if
somebody else has written a flag they
need to flush that value out so he sends
out an invalid to the other caches if
you're holding on to a copy of flag you
should write your copy to memory because
I want to have a copy I don't have the
latest copy so he sends out it
invalidate which comes back in a minute
with everyone saying we don't have a
special copy of flag we have the shared
one that's all fine meanwhile CPU number
one is back around to hey let's go
branch on the contents of rax is it
equal to zero or not well I don't know
it's not back from memory yet so I'm
gonna predict and they all in under my
speculation I'm gonna go expect little
execution I'm gonna go load the value of
data and RBX okay that means I have to
go to the memory controller and ask for
data so I send all question off to
memory controller please mr. memory mr.
l1 cache tell me the value of data
meanwhile down at the memory controller
there's a request for the value of flag
finally hit there and that will go on
for a long time so I show it in the
memory controller but it's probably
physically in the wires because it takes
several clock cycles to move just into
the wires before it's on to the chip and
the memory controller and you can begin
to figure out where the hell the value
for flag' is meanwhile my other core
begins to do something it writes a value
in iraq
and then stores it the store is going
from the CPU to the l1 cache the same
time the other CPU is asking for the
value of data here's my true data race
to physically different devices they're
separated on the die by you know
millimeters not a huge distance but they
are physically separate so these two
things aren't aware of each other one is
writing it one is asking for it so the
guy that's writing it says hey you
should invalidate your copy if you have
one because I'm about to write the
golden value but it's too late the first
CPU CPU number one has already grabbed
the value for data that he had in his
cache so then the acknowledge comes back
yeah I flushed it and the cache on the
right lost his value data he's gonna
fetch it again some future value from
the golden value in memory but the value
he had was valid at the time he had it
and he handed it off to RB X sorry X has
aspect of value of 0 and the invalidate
comes back and the right happens into my
you know l1 cache and the state has
changed to modified because I have the
golden value of it in my cache there is
a state of it in memory stell and that's
because these Isis are physically
separate they're both talking about the
same thing the value of data one is in
my cache one is in memory on a different
chip on the same motherboard they have
different values and that's normal this
is the this is you know it's all
relative kind of thing pins I'm who's
who's asking who's looking yeah so it's
it's it's it's in two places and the
value is a relative to the observer
should L want a CPU one immediately look
at memory he can't it's not fast enough
but if he could he could find the 0
there but he can't I mean while CPU 0 if
he looks he'll find it in his own one
cache and see 1 2 3 so the two different
CPUs are gonna disagree in what it looks
like and then finally CPU number zero
writes out the new value for flag'
saying hey initialization is done so
mister cpu 1 and validate your copy of
flag if you have one you don't that's
all fine so I write my 1 I'm modified in
CPU zero I get the knowledge back from C
you ones cash I don't have a copy of
flag then the right hits the memory
controller for data one two three and
I'm still trying to get that flag value
out of memory controller by the way I
haven't got it yet and that's because
the memory controllers doing what the
hell it can but it's behind it's got 20
other you know pending cache misses it's
trying to resolve at the same time and
the flag things just in the best effort
bin along comes to write from CPU more
zero of the flag value well hits the
memory controller at that time the
American troller can say wait wait I
don't want to go out to my dear AM it's
really slow
I just got written on these bus lines
hey here's a new value for flag' so
there you go answer flag so I'm going to
send it back so the flag value goes up
keep you in ruins cache it goes up into
CPU number ones registers okay I'm at my
speculum execution flight was in fact
one so the suspect of execution must
been okay so I'm going to carry on I'm
going to take this value I got an RB X
for data and use it well the answer is
it's not mad dress it's no it's stay all
whatever was the wrong thing crashing
burn right so real chips really reorder
stuff and the answer is you have to
think about this stuff now because we
have to do multi-core to get performance
out so we hit this limit of diminishing
returns for having speculative out of
order or cache coherency protocols
everything you know branch prediction
all that stuff we went to risk and and
started out simple and now it got very
complicated as well and it is the same
issue we're not able to get more
performance out of a single core anymore
so we still have the transistors but
burning them on an ever bigger cache or
a never smarter branch predictor isn't
actually effective anymore so we're
still you know we're into the land of
chip multi-core and it's just that
concurrency is a hard problem there's
been a chip multi-threading thing that's
actually very common but it has the same
issue
this is like very old gone Niagara
Niagara right this is actually true on
all x86 is with hyper-threading it's the
same idea and that's happening same
piece of hardware act like it's got
multiple cores on the one CPU and
they're going to switch which core
they're pertaining to run which logical
core on the same physical hardware if
you have a stall it's just an efficiency
hack but it has every bit the same
problem as things we've done here before
and you get to have more more thread of
execution that are unrelated and so they
can get more cache misses that are
unrelated they're not dependent on each
other going in parallel and that's the
reason why you might want 10 and
parallel instead of 2 but it the same
limits are hit and that's because any
one core can't get more than 2 or 3
cache misses running in parallel before
it needs a value of a cache miss to go
on to the next step so what's this mean
you know the dominant operation has
changed over the years used to page
faults then it was just count of
instructions multiplies were expensive
and loads were cheap I don't even
remembers that day now now multiplies
our cheap and loads are expensive and
then cache misses and now the cache
misses didn't go away it's the same
problem but you get a lot of cores and
you have to use them to get your
performance to go up to the next level
and that means you have to think about
how to handle how to write code in a
multi-core era I think I've said this
enough times already
let me let me pound this one though it's
the data in directions that kill you
this is where your performance goes and
so the way to think about this is to
look at your data structures and how
you're laying them out and look for
where places where you've added rappers
on rappers and rappers that you can get
rid of you remember back in the day the
buffer copy was bad and then you know
80s OS is people went to zero copy zero
copy network stacks is a pain in the
butt to get it right but it was that
copy that hurt you
and now it's whatever layers you're
using between each of your major data
forms every time you run through a
conversion you end up burning through
all your data and it doesn't fit in your
cache so you you have to reload it all
from memory again and again again and
that's where the cost goes
don't touch it the data if you can't
help it don't have in directions if you
can help it so one of the ways you can
get away from having indirection is to
look at data that's immutable and and
then sharing and then flattening those
data structures out so shared data is
easy to run fast because it runs in your
caches and it's actually ok to mutate
data on a single core because it's going
to run fast in your l1 caches and as
soon as you're both chaired and mutable
you have a really hard concurrency
problem to work out and you have a
really hard performance problem to work
out as well and that turns into all
kinds of issues with requiring
synchronization which can be both very
slow and inexpensive as well so if you
can figure out how to write your data
where it's immutable when you share it
you're your life is much better because
everyone will have it in their caches
the shared copies and you'll get the
cache miss the cache hits and run out of
cache instead of run out of memory as
soon as you start mutating it you have a
horrible problem all right if you wrap
this up here yeah tip multi-threading it
looked like a great idea this is an old
slide I should throw it out now it does
its uses it has its uses but it's not
really any more effective than just
having another core you still have to
program through the core and it still
gets no more throughput as a regular car
so one of the things that came out of
this was that CPU utilization tools
often have they're often misleading so
if you look at your per far and it shows
your pegged your CPU is busy what is it
doing well you don't know and what you
don't know is where the performance is
going so what's missing these days is
cache utilization tools bandwidth tools
if you're running out of cash you're
working set doesn't fit in your cache it
looks like your code is just all slow
but you don't know what faster slow is
so it's all that speed that it is but it
could be that you're working sets just
too big and there's a smaller working
set usually by getting rid of layers of
indirection and layers of wrappers
that lets your data get denser and fit
in your cache then everything goes
faster it's it's and there aren't any
good tools to tell you that you're out
of bandwidth for instance that you're
really stuck behind unable to fetch
stuff from memory fast enough so the
summer here is that the damn things are
really moving really complicated beasts
with lots of moving parts under the hood
there's lots of internal parallelism to
make things go faster the performance
model has slipped around until now it's
basically cache misses and unless you
profile fairly deeply fairly
intelligently you just don't know where
the you know the the low-level
performance goes and then this premature
opposition is a root of much evil it
really is it's this is a game where to
understand where the performance goes it
really helps to understand your code
very clearly and that in turn means
actually having some simpler code and
then backing up for a second and saying
what are we doing why are we touching
this data in the first place can we not
touch it and that will lead you down the
path of okay what is actually we're
trying to accomplish here and this is
how we can think about performance and
then once you can get to you know the
clarity there you can simplify your data
structures simplify the code goes with
it and you'll get your performance out
and that's it
so I got 10 minutes or so for questions
yeah so right wouldn't the easy solution
be to increase the cache size so the
answer to that one is it's been tried
and tried and tried and people have
increased the cache sizes sort of
perpetually every year and then they hit
this point of diminishing returns so
exactly the issue is that the cache hit
rate kept climbing climbing climbing and
then it hit like 95% so you double it
and and and you don't go to 100% you go
to 95.5% and then you look at you know
what you got out of the doubling was
this tiny amount of extra performance so
you could throw an entire extra quorum
so you look at a modern x86 it's like 90
95 % cache it's already almost entirely
cache you cannot double the cache size
anymore there's no physical die space
for it you're already busy the cache you
can get and there's no more performance
we had by getting your cache bigger
yeah there any tools in Java that does
do this um I don't know if they need
that do a good job so Kirk frege's he
has these play as an answer here yep you
have a name for the tooling you vtune
yeah I knew vtune we do but it's like a
pain in the butt to use ah okay so
Solaris Sun Studio apparently has a
profile we'll do this as well and then
works on Linux also okay that sounds a
good answer I know that I spent a lot of
work out as all doing performance
monitoring and got reasonably close to
be figuring out where about cache misses
were going and coming from but I had to
use the hardware performance counters
but I was mapping them back to the to
the Java level in a reasonable way and
vtune doesn't do that very well but
that's the answers you have to really
get down to the hardware level
performance counters to know that that's
what's hitting you so often see case you
can look though if you just do a sort of
a york hit style profiler look at what
the hot code is and look at what it's
touching and then ask yourself you know
here's some loop I'm spending a lot of
time in what's involved in the loop and
you'll find some hash table lookups and
you'll find some in directions of going
on and if that's where all the time goes
you know sort through that piece and
simplify what has to be touch to get
that job done
usually by flattening out some of the
the layers of indirection and life will
go faster of branch mispredict
oh
backwards branches in the bottom of the
loops yeah okay I'll talk about it and
the answer is not much so so branch
predictors are they've done a good job
on on on a lot of the styles of branches
and the backwards branches obama loop is
one that's sort of very easy predict so
the the there's it that one doesn't
typically cost you like a typical loop
will have a mod
trip count of 10 maybe and that's fine
the prediction will do the right thing
for that kind of game and if you have a
very short trip count loop and you visit
it inside another loop pretty quickly in
x86 will figure out that this branch
goes you know back back forward back
back forward back back forward he's
pretty complicated and what he'll
predict and does a good job there I
think backwards branches for loops are
not actually an issue for prediction
because ok it's a hotspot while loop
unrolling cuz you get a you get another
set of effects that'll open rolling what
loop and rolling lets you do typically
you loop or you okay here we're doing
loop and rolling it's you're having some
walk through a large array and the walk
through large array is reading and
writing a couple different arrays when
you unroll the loop the loads and stores
for those reads and writes also get
unrolled and exposed to compile our main
get rescheduled then you can get
multiple cache misses for the the array
to get started in parallel so I'll do
load load load load load for you know
all the way down the the set of arrays
you're reading and then store store
store store store at the bottom all
those load load load load loads trigger
off all your cash missing loads all at
once and they overlap so unrolling can
often get you 2 X 4 X 10 X speed ups
huge speed ups that's why you do it but
it's it's to do is to do cache miss
starting so hotspot jet does that and I
have manually unrolled only when I doing
GPU style deep learning kernels like
really tight dense numerical kernels
where I had to do the anti-aliasing
myself and that I knew the jet wasn't
going to anti-alias and then I did the
loop unrolling so I could manually
shuffle the loads around myself against
what the you know the jet thought was
legal in order to get the cache misses
to start up in parallel so that was me
doing a essentially a video codec
internal loop it was a deep learning
inner loop but the same same kind of you
know tight dense small code I did some
horrible evil hack at the Java layer to
to make it go fast
so your estimate hyper-threading that's
the chip multi-threading I was talking
about there yeah and the answer there is
sort of there's a limited number of of
issue cycles available to those chip
multi-threading and it's actually a
single physical core that's doing the
issuing and the the goal is to run two
different unrelated threads of execution
so you can start the cache misses from
the next to the next but if you line
them all up and have them all go what
really happens is the one guy cycles
between them all if they have no stall
conditions it didn't help it's only
useful if he bounces between the the
logical cores and this one stalled so he
let it go and start that memory and then
he bounced between these three till this
one stalled means balancing these two
kind of thing right it didn't get any
faster if they were all running without
any stalls it didn't get any faster than
having the one guy right then that's the
limit so yes you typically have to and
he bounces between them if you have no
memory stalls it was no faster than
having one it's the count of
hyper-threaded course to actual course
if you have no stalls which you can
arrange for various kinds of dense
codecs and alike also single linear scan
striding memory accesses are well
predicted by the x86 load store units
and they will prefetch them very
aggressively so if you're just walking
down a big array you can you can more or
less not have stalls for memory until
you run out of memory bandwidth so you
can also then run out of memory
bandwidth and then you're stuck because
you're trying to inhale too big of an
array into your cores and that's all the
limit it's all fashions going to go yeah
right okay so so I gave some some hints
that like think about your mindset your
program your mindset differently and
then where does the sort of the building
blocks so I don't have an answer there
and I don't think anyone does a bunch of
different people are trying different
this is this is all about what is it
what is a good concurrent programming
style right this is where do I find my
parallelism what's the right way to do
it so I know that some people have
promoted strongly the notion of shared
immutable data so closure for instance
is all immutable data structures and you
do you know Delta encoding it's all done
under the hood for you to t'me do and
apparently do it right you're actually
doing a Delta change and the old stuffs
left immutable and therefore other cores
can use it without having to do cache
cache to cache invalidate games it's all
just reads if you will that's sort of
good up to a point another coding style
that I've been playing around with is
implicit parallelism where I have a
large data parallel arrays and the
arrays might be actually very very large
much larger than memory and much larger
will fit on one machine and and the you
you write to it as if it were individual
array operations as a building block and
then the array operation internally runs
parallel and that works for a different
kind of coding style right this is for I
have embarrassingly parallel data but
fairly complicated operations on it but
I still have a lot of data that can run
in parallel and so the coding style
looks easy but it only happened you only
works on a certain domain so I don't
know of any really awesomely good coding
style for parallel execution
I know parallel coding I know that what
Java hands out is sort of like pair
assembly building blocks it's you know
locks and blocks synchronized wait
notify new thread and it's not the level
you want to be thinking at that you know
my parallel ray hack lets me think about
whole Ray's as operations that I'm doing
math on them one after another and
there's a huge amount of parallelism
under the hood and if your domain looks
like that it works great and if your
domain doesn't it didn't help you
anything all right and so you know the
closure coding style says because it's
all immutable I can have multiple
threads and then when you want to have
threads communicate through memory you
have to very explicitly talk about what
is shared memory what is not that gets
into there refs you know what is a
shared variable and that's you know
useful again but you had to change your
programming model a bunch and you know
the other thing is that that shared
immutability required a delta data
structure encoding which is a layer of
indirection and so closure code is by
design much slower than the equivalent
java code because of that I say much so
so 2x and you can tweak it up or down
Scala has much the same issue there's a
way to do it make Scala try really
hardly down to the primitives without
indirection layers but I've tried
actually very hard on any modest size
program it's not reliable that I get
down to the fast version of Scala
although I tried actually quite hard for
a while my plain old Java code was
always 2 X faster than my Scala code
despite my best efforts to do all things
you could do in Scala to make it go fast
and that was because of layers of
indirection that just came out of the
language by design there is a memory
pressure well ok so so memory memory
that the layers of indirection bite you
in two ways one is that the cache miss
requires you to load load load and if
you're missing you'd have to wait till
the next one shows up you can't overlap
those loads the other one is that the
layer of indirection typically burns up
space in your caches and because it's
burning up space in your caches your
caches are half their size effectively
because they got half there's volume
taken up by indirection wrapper layers
that are not necessarily to holding the
the meat of the data right and that's
another that's another place where the
2x slow down
from no no that was the original intent
so question was like this clearing final
having have any meaning here and the
answer is final variables are not final
from a hardware point of view right that
they exist they can be read before
they're initialized so they have to read
a zero so the the new operator in fact
does store a zero down so that another
thread when you reading it will find the
zero if it happens to read it before the
final field gets set there are a few
cases where you can sidestep that and
you know the thing doesn't escape but if
the if during the construction of the
object you escape it to global memory
before you hit the final field then you
must write the zero and then you write
the final value and from the hardware's
point of view it just changes it went
from zero to non zero and so there's no
nothing special happened there
instead what final can do is let you
know that past the constructor I'll get
to use a second pass the constructor it
is not changing so as a cost model in
your head you can think this thing
should sit in cash and be cacheable okay
that's the first step the next thing you
do is you get rid of the getters and
setters say way everyone does getters
and steps no so there's supposed to
inline away and they mostly do and the
answer of course is they mostly do and
when they don't you don't know that and
you silently fail to inline a key you
know key function call and loop and then
the you know the whole loop unrolling
optimizations that but less
optimizations don't trigger if the loop
has unknown function calls in them or is
beyond a certain size every extra
function call and a hot loop runs the
risk of falling over the threshold of
saying it's too big and I want in line
now so mostly getters and setters in
line do the right thing but not always
so if you're actually worried about
performance get rid of get rid of the
getter and setter in your hot work and
then as soon as you get rid of the
getter in southern hot low there's no
point having any where I see he's like
you shake head no no yes I have done
this multiple times I will happily I'll
happily show you code where you're just
wrong here Kirk
so okay so so his argument is one of
good coding style which I'm not gonna
argue against instead I'm gonna argue
that if I need performance
I'll gank it I'll yank it because it has
bitten me on multiple occasions and I
know why it bites me I understand the
rules involved there I also know that if
I need to find out where a field gets
set my ID will just tell me and I don't
need to get her a setter to find the
places where get set the ID just knows
including my Emacs IDE let IntelliJ and
everyone if they all do it right it's
not a mission yeah I have a particular
coding style I've used for four years
it's highly performant and let me build
all kinds of system level stuff the
world uses all over and all kinds of
domains such as hotspot and in in
getters and setters are never part of it
and I know exactly why I do it the way I
do and I'm happy to argue people and I
know a lot of people don't believe it
that way and if you want to we can go
toe to toe on at any time so some people
there had a question
yeah okay so this is Erlang which has
this notion of the data being immutable
and you know a multi threading model
that's different it's one of the more
interesting multi-threading miles is the
actor model and you're gonna tectors
talk to each other with like messages
passing kind of things or the message
passion you can actually be fairly cheap
I'd say the erlangs been around a long
time and for whatever reason it hasn't
stormed the world as the right way to
write multiple threading code so I don't
know that it's the answer that you know
history tells us that it's not the best
way to go but there's definitely a
subset of the program community that
heads down that style whether it's
directly Erlang or there's a Java
version of relying or if it's you know
the actor style coding there is a there
is something there but it's not storm
the world as the right way to write
parallel code and I think it's because
you have to change your head around a
different coding style and maybe you
need some other performance hacks I know
that the the Erlang code that I've seen
looking at the performance of it it's
not necessarily as performant as you
might think
and there are some levels of there's ma
positions that are missing that would
need to happen at the very low level to
make it that code actually go faster
because the message passing style has a
cost to it that's I have to write a
bunch of things out and then I have to
hand them off and that it's a
pass-through memory to do the handoff
and I can get pretty cheap but it's
never free and you know if I'm hoping
that my allocation was into cash only
and I filled it in cash only and then I
handed it cash to cash to the other guy
who then read it out of cash but it
actually hits main memory a bunch no
matter what you do I can step you
through why and that and so the right
answer now is that you have to have an
operation that understands how the actor
model works in order to inline the
actors into each other under the hood
and maybe there's a performance to be
had there but it's not currently there
and I don't know if that's the reason
why Erlang has the Erlang style hasn't
taken off but that is certainly an issue
ok I'm basically out of time so I need
to free up the room and go find lunch
but I'll be around here for a few
minutes that people and ask me questions
I talked about getters and setters all
right fine</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>