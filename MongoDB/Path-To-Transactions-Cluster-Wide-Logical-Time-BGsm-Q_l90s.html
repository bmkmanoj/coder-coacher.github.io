<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Path To Transactions - Cluster Wide Logical Time | Coder Coacher - Coaching Coders</title><meta content="Path To Transactions - Cluster Wide Logical Time - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MongoDB/">MongoDB</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Path To Transactions - Cluster Wide Logical Time</b></h2><h5 class="post__date">2018-03-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BGsm-Q_l90s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the path to transactions
today we're going to talk about the
global logical clock as you guys know
MongoDB is coming out with multi
document asset transactions in forw
and this is one of the exciting projects
that helped us get there today I have
Misha with me an engineer on the
sharding team and we're gonna walk
through this project so I'm gonna pass
it over to you and Misha hey there
so I mean Misha I worked on this project
with the rest of the team for about a
year and we have built two things one is
a global logical clock which runs in
MongoDB start in version 3 6 and then on
top of it we have built causal
consistency support and global logical
clock in general allows to establish
global snapshot which is very necessary
and helpful for building up the
transactions which is going to be our
next station right because you need a
global snapshot in order to have
snapshot isolation across the shorter
transaction yeah great we need to have a
consistent view of all data across all
sorts right so let's start with just a
very brief overview of what MongoDB is
in case you don't if you want to refresh
its distributed Charlotte replicated
database and internally supports no
sequel so it consists of multiple nodes
and they have different roles their
storage nodes which can group together
into replicas sets to provide
reliability of the data so it's kind of
fault tolerance if one of the nodes goes
down the other one will will basically
picking up the role of the master node
which allows us to do writes and the
horizontal data could be partitioned
using Chardon which means that shard 1
and char 2 will have different ranges of
keys
so therefore when query comes it will be
routed to the appropriate
Netta chardon it allows to distribute
the load across multiple logical unions
so looking at this diagram the blue
boxes represent replicas
where they contain the same set of data
and the orange boxes represent represent
charts which contain a distinct
partition of data so they don't share
the same pieces of data yes exactly and
the green boxes are routers which do not
hold any data and that ask is to know
where to send the queries which coming
from clients which are here marked as an
observer it's on the top so because this
is simply says if event a causes event B
then this order will be noticed on all
nodes in this order
that's about it so there is a lot of
research which happens about that and it
goes back into 1978 where Leslie Lamport
introduced the first way to provide
causal consistency essentially ordering
event in the distributed system and so
basically what he brought it and this is
very very influential article in
distributed systems actually but
basically he introduced a Lambert clock
in Lambert clock is just a counter it's
a scalar value which assigned to each
event and the event is are the
processing data on the node or sending
data to another node so it's exchange of
processing of the data and essentially
it's a way of numbering those events the
way that if event a causes event B then
the number on the event a will be less
than number on the vampy the converse is
not true if number of an event a is less
than number of n it doesn't really mean
a cause as beautiful in anything right
but this was enough to start ordering
events on the system and when we were
working on that global logical clock we
basically were looking in different
variants of lamp or clocks and so
obviously since 1970 there are a lot of
research in this area there's different
variants of that effector clocks for
example there ways to implement clock
synchronization if there is certain
hardware available using atomic clocks
and there's some quite recent article
which describes the hybrid logical clock
which
very similar to the Lampert clock but
they provide ability to bond to physical
time and this implementation also happen
to be very close to our way we interact
with the time on the replica sets and no
up lock so it was very easy for us
technically to implement that and
convert the existing model from just
incrementing what we call up lock up
time which is time in the up lock to
convert it to the logical time which
unlike up time which is local to the
replicas that will be global to the
sharted question but of course to make
it global for the Charlotte cluster we
need to build some form of exchange of
this data and now this is what now the
diagram which I'm going to show there we
go
so in general cluster time this is like
our term internally for this global
logical clock has just two rules and
those two rules are quite different from
I think at least the article over
academic research and rule number one is
that we separate events from sending and
receiving messages
so we consider events only what changes
durable state of the system meaning its
any right into the up lock so this is
the event in MongoDB cluster time
meaning and therefore only those events
can tick meaning increment the logical
clock this is rule number one right and
the only node or process in MongoDB
responsible for interpreting those
changes is the mangodi raised to the
Mongo D is the one who would increments
time one go to primary MongoDB program
yeah MongoDB primary yes so because
Monga is replicated said it has one
primary which always take writes and
secondaries which replicate those checks
only primary has the ability to tick the
clock right everybody else here follow
rule number two which is keep track of
the greatest scene logical time and
always distributed so
that's about it so in every request that
I send from any other process I just
send the latest time I'm aware of yeah
and then what happens is the node that
receives that jumps to that time yeah
exactly so and this is pretty much where
research stops yeah but would production
system and as I said I mean not so many
systems actually even were looking into
causal consistency implementation which
yeah makes sense for end-users and once
we started working on this we realized
that there is one scenario which was I
mean at least I haven't seen it being
covered which is very dangerous for our
database essentially like assuming the
client is not a good client but some
malicious attacker competitor whatever
who just wants to break the database and
we've like naive implementation which I
was just talking about the clients they
send some value and then up log puts
this value into the database it's very
easy to break it because it's it's a
number yep so it's value cap right so
socially it's like 64 bits yeah
and it's possible to just send the
maximum value minus one so it will be
able to increment that meaning tick this
right and after this that would be
basically end of the game because
there's no way to increment and that
means because if this has been majority
committed to the database the only way
to fix it is actually to unload the data
clean it manual reload so it's major
major outage so all of those are
requests from the normal clients would
fail in this case right because it's
saying I've already incremented to the
end of time I can't increment any more
so I just not just cause we do have some
time internal rights as well so
basically the base will be like really
broken me in a big way so it's
absolutely not go so we started thinking
of how would we fix it how would we make
sure that while we are like open source
database we assume that anybody can
operate it to assume that a user should
notice the clients are not always fully
trusted like basically meaning that
they're inside some authentic a
pyramid her and stuff like this it'd be
anybody like using the driver who sends
the request and we want to make sure
those out working fine with causal
consistency and doesn't break the
database and so our solution was just
use the signature so we generate the
signature for cluster time which is
generated by Mongo geez and therefore
Klein just cannot change it because in
order to change it they need to have an
access to the private keys which are
securely generated and kept inside at
the monkey like either primary if it's a
replica said or fixed over but there's
the inside the perimeter so we can trust
that users cannot get them easily and
that's basically how we implemented that
and we of course worried a little bit
about the performance overhead because
it basically means that every message
has to be validated and every message
has to be signed but we came with some
optimizations which which mitigate the
processing overhead and still keep the
time safe yeah
and I think there would be I mean some
materials which will go into this
details of you interested but the bottom
line is that it's not taking too much
resources to provide this level of
security right so that was basically how
we build the logical time and what like
the major challenge and what does a
major outstanding feature we have like
comparing to academic research or other
implementations so one of the questions
that I have before we end this video is
around the common question that we get
why did we not choose the vector clock
that's one of the popular ones right now
in the industry so well vector clock is
basically the difference between vector
clock and lampert clock is that vector
is a vector Lamport is a scalar meaning
that vector essentially passing the
clock which ticks on each Charlaine it
shard allowed to have the different
value and therefore
when we need to synchronize at the time
in one char sometimes it has to generate
the no operational regice am value has
to be put into the applaud in order to
generate the value of the logical clock
which will make this short consistent
for the rest so this is a major I would
say before major implementation detail
which concerns our users sometimes
so vector clock do not have this problem
because they do know they could detect
the consistency we are doing this so
pride but at the exchange at the expense
of putting the in our case is just one
number and it will be a number equal to
number of shards and since we are should
be able to scale to like hundreds of
chars at least just not going to work it
will be like a huge overhead and
communication it will introduce major
performance heat because you'd have to
have a value per shirt or this is a
global value a singular global value on
the clock wouldn't scale to like a
large-scale sharded deployment yes so in
this case is will the signature it's
like approximately I will say 50 wise
and over there it will be like 5 K or
something like that and just not gonna
work yeah yeah it's not feasible to send
with every request in every operation
which is how this gossiping works yeah I
mean we usually have some workloads
where network is getting saturated if
users like really driving the loads of
like looks like small inserts for
example some updates and if we just make
messages 10 times bigger it will not
help them that sounds good thank you me
shucks so much for your time on this
walking us through well you're welcome</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>