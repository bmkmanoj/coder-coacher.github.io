<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NYC MongoDB Meet Up - Julio Viera, Pixable | Coder Coacher - Coaching Coders</title><meta content="NYC MongoDB Meet Up - Julio Viera, Pixable - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MongoDB/">MongoDB</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NYC MongoDB Meet Up - Julio Viera, Pixable</b></h2><h5 class="post__date">2012-02-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uAGTRk_zQAE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'm not only talking about
moving stuff from my sequel to Mongo we
currently store 5 million photos in all
that ice sorry I didn't reduce myself
from Julio never get pixel and the vp of
engineering I've been in pixel for
around more one than one year and we
started everything my sequel and right
now we're going to move a lot of tough
Mongo mostly everything we have
everything in amazon so one of you ask
you want to see more detail on Amazon
then we can meet later or you can talk
to our system architect which knows a
lot of details on that ok so much big
civil so right now there is a big
problem about fragmentation about photos
so you have facebook flickr instagram
twitter and you have a lot of places
where you can post photos if a user
needs to consume those photos they have
to go to these services one by one there
is no a single place where everything is
centralized so pixel is trying to
centralize all that information and
organize it in a way that is easy for
the users to consume so for example you
can have photos in top of the day that
belong to multiple services and not
simply going you'll have to go to
facebook to the news feed or going to
most popular on Instagram you have
everything in one place recently we have
any featured which highly the capacity
to add hashtags to photos so it doesn't
marry the photo is taken in New York
City from Instagram or Facebook or
whatever but it matters is the footage
from New York City so you have different
photos taking on different services but
they kind of mean the same thing so
that's what we're trying to do like puta
put order on all this mess of photos are
there so it sounds easy now well it
isn't a
the first thing that we have to store 5
million photos and it's 32 delights of
information even though that we have
like a compressed storage and everything
is still three terabytes so it's a lot
of information that we have the store
right now we have to deal with 20 new
million photos today that come from
different services so it's a big number
we have to maintain like 50 million
category for our users I category means
that for example for you you have top of
the day or you tap top months like we
have meant to have to maintain around 50
million of them each one with
approximately 100 for but their users
that can have 600 photos in them our
data is servers among all of them
received 16 million rights and our
rights like it that those could be
updates or inserts and we could only
have a master master my sequel set up
with each one of them has like three
slaves and then we have a Charlotte
environment to store the five billion
photos their story in a different Mexico
server and the main storage when I say 3
terabyte is only metadata it's not the
binary it's only URLs and captions and
stuff like that so it's a lot of data we
could only look like 30 million reason I
were more than less depending on the
shards I do a little more because they
have more data
and we develop over the time some tools
for logging and profiling one of the
tools does have 50k inserts per second
and all of the data is index all that i
read log entry that we do have indexes
so we can query them later and we do we
have more than two in this system we
have tables that have more than two
billion rows so this object of logging a
profiling it's like a separate
completely separate storing and we're
not going to be color here we can talk
about that later we could do an order
meet later but I'm going to focus right
now on the top part which is the photo
storage or a challenge that we have we
have to crawl every night oh sorry 24
hours photo from facebook twitter and
other services i put favor and twitter
separately because crawling through
facebook is they have to simply have too
much photos it's amazing the kind of the
number of photos i have so crawling from
then is a pretty big deal and we have to
evaluate custom crawling changing
especially for handling facebook and
learning twitter is another subject and
twitter you have photos well you have
photos now but when you started if you
didn't have photos you have tweets and
the twigs have URLs and they point to
different services so you have to
actually parse it to its get the Eurotas
that means that our photos and impart
the photos in the services and get them
back so it's like an order it's a big
deal also then you have public api for
instagram flickr telegraph or it
relatively easy to integrate with the
system then we have to store all this
metadata in our servers okay and it's a
pretty big deal because I as I say with
we have like 20 million photos new every
day another problem is that we have to
off link role because we need to have
our data up-to-date in the database
there's no point of having two months
old contains data we have to update
every night and send notifications to
use it hey you have new new content in
pixels so they come back and recently we
are the hashtag functionality and I put
this number fifteen hundred hashtag per
second because that's a capacity we can
achieve with the New York detector and
everything that I'm going to say here
was a foundation on building this
and I mean it really works well and it
really were fast we can actually predict
how is it going to scale because we did
everything the right way from the
beginning so I want to start a little
talking about the entity relationship
model so the entity relationship model
is rate I mean it's great to
understanding how to link the
requirement for a client or from a
particular project to the data is that I
particularly use it to understand the
project in a technical way because
people start talking like less resistant
to that list this and you need to
translate that into technical language
and I think this is a very good approach
to do it is one hundred percent based on
normalization so you have to think your
data in a normalized way the problem is
that the entity relationship model does
not into account any database software
characteristic and I'm not saying it
should but it doesn't so you as an
engineer you have to take that into
account it also doesn't take into
account the number of row so it doesn't
know if you're going to have billion
rows table it doesn't take into account
shouting and traffic picks I'm not
saying it should but the thing is it
doesn't so you need to make sure that
you understand that so what happens when
you reach hundreds of millions of
throwing my sequel you have large joints
that become painful and I when I say
painful is 300 milliseconds but they
used to take at the beginning 10
milliseconds now they take 300 and
that's going to be getting worse every
day your slave starts to lag and they
lag at particular times in the day or
all day long i can remember our slaves
lagging for three hours and there was
nothing that we could do all right but
wait you know once i stirred is this
once this starts happening then there's
no way back the leading from tables is
really really expensive to the point
that we once we stop deleting from
tables and running all those elite at
night because it was really expensive
and before my sequel 55 he was even
worse
then you have this crong that was
running at 1am and it was taking 10
minutes and then 10 months 10 months
later is taking now six hours and is
bringing the system down for hours at
night now that's happened when you reach
millions of rows then you have a lot of
secondary indexes in your table and they
work really well until you need to
charter partition your data and then
starting a new slave of my signal from
scratch is one hundred percent manual
operation I'm sure that most of you know
what I'm talking about you have to copy
data you have to remember the coordinate
of the replication and and you have to
do everything from scratch then while
shouting my sequel there is no built-in
solution for sharing unless you're using
my super cluster which is totally
different product or you're using
third-party tools like the Twitter one
and there are other ones that they work
really well but they're not built into
my sequel and then the question how do
you alter tables when you have 40 giggle
I table 100 gigabyte tables how do you
do to alter the tables so basically
we're assigned you know we stopped
operating tables and we decided to leave
the legacy columns there and live with
them and then do new tables that
complement whatever it was missing on
the other ones because even though that
we were hard master master set up
switching from a master to an order is a
risky operation it's one hundred percent
manual so it's not built in and I
shouldn't I mean when you do try to do
it and you cannot do you think that
something is growing it has to be an
easier way so some good practice for
data storage and how do we use them in
pixel
so the basic normal I schema from pixel
was something like this we have a user
stable and then we have photo table and
then relationship in the middle this is
kima work really well for a couple of
months like four months until we start
reaching the 500,000 users so will
happen when I realize we store around
1000 photos per user if you do the math
it says that the user for a table we
have around 1 billion photos when you
reach 1 million users in that at that
moment the table is going to be super
inefficient the problem is that is
tricky because if you go to my sequel
and you do that worry about yourself you
say I'll discredit to 100 milliseconds
so why are my slaves two hours behind
and the problem is that that query that
took 100 milliseconds at the beginning
was taking 5 milliseconds when when the
slaves were not lagging so you start
facing the scalability issues and even
though you feel the system fast and you
can do queries on there fast in reality
they're not another problem with this is
quitting a single user photo for example
if I query the users one photo I have to
do one query in the user photo table and
then go to the other one and do the
query in the other one and I'm going to
be hitting random places in the other
table which probably my sequel doesn't
have them in memory so you will have to
start loading from disk a lot of places
in the table so the first step we took
to work on this problem was merging the
table so we got we got rid of a photo
table and we put it inside a user folder
there for duplicating our data because
now we were studying urls from facebook
inside the relationship table that
effectively is copy its duplicating the
data all over the table why do we do
this because now reading all users
photos can now be done in 140 so you
throw one party to the table and you get
all the photos for user ID well this
Boris like two or three months to
implement the final solution because the
user photo we still be a number of user
per 1,000 gross table although this
really this helped a lot on a
replication lag and stuff he didn't
one hundred percent so glass that's why
we implemented the second solution which
is the one that weird currently using
that's this one instead of storing
multiple rows 1 / photo we're not out
starting one row per user and we're
compressing all the data into one Jason
and discipline it so when I say that we
have three terabytes of data we have it
in GC strings so remaining how much it
will be viewed on compressed data so it
will be crazy so right now if when I get
all the photo for user ID number one is
only one query in one table in one row
so it's really really fast and by
primary key you can ensure this way that
a user photo will always be equal or
less in quantity or froze and the user
photo than the users table sorry but
then you lose the ability to query by
URL or 40 ad because you're compressing
all the data into a single column so you
lose the ability so you have to make
sure that if you do this you can't lose
it there are ways to overcome this
so another tip that I can give is
partitioned order data my sequel have a
great great functionality I don't know
if you are all familiar with that is
called partition tails and I think we
wouldn't be here without that feature
the dell principal about partitioning
tables is you have a key which you use
for keep for partitioning and every time
you do an insert in this case is a photo
column mexicali will hash the value and
we design a partition for it so in this
case we're a partition in this table we
have three partitions and we partition
by the photo internally my sequel
handles every partition as a separate
table so if you have a 30 million rows
table with three partitions internally
my sequel is held in three different
tables with 10 million rows each so in
this particular table we have two
indexes one of them is a photo and when
another one is attack so in this case
the photo number the photo a has tagged
new year CD with three count with the
country and then it also has the tag
party with a count egg so if I do a
query that says give me the give me
everything we're 40 plus a my sequel is
going to get the value from the front
where it's going to hash it and it's
going to say okay I'm gonna I'm gonna
query only partition number one so
you're going to effectively recording
one-third of your data which helps a lot
the downside of this is that it doesn't
look really well but if you query right
the second column by tag my simple
doesn't know which partition you will
have to query so you will put all the
partitions which is not good so one way
to overcome this problem is with an
inverted index on the left side we have
the same tilde which I show you before
and the right side we have the inverted
index so you do the inverted index and
your partition by the primary game and
then do you store in the other column
which photos belongs to the tax so for
the tech near city you can store the
photo a and B then you can go to this
table and query them or you can simply
restore them the
located indoor table so it's not really
really quick query on other things that
we did so we put that ruling pixel about
a couple of months ago and it's
optimized reading the most you can so if
you have three different solutions for
the problem you're going to pick the one
that optimize the reading the most
because when i use her clicks on a
category he wants to see the photos
instantly he doesn't have 2001 to wait
so we always decide the solution that
optimizes reading the most we can we
always read the primary key so it
doesn't matter what I will reading we
have to rely primary key if we have to
undertale for it we do it but our rule
is we have to read my primary key we
avoid all end-to-end relationship on
huge tables we still have a couple of
them but there's really Drella tively
small tables like 100 rows or something
so it don't make any problem and these
two are really really important the
first one is move the data in lazy mode
and do one change at a time so we have
this in the past we have a table that
was storing which categories every user
had so for example for me it had Julio
had the category ID number 1 2 3 4 5 and
then format for example they we have the
category ID 10 11 12 and 13 and there
was around two out incremental ID the
problem that was a dull was the primary
key and the user ID was a secondary key
so we couldn't partition or data by any
by any of those because it's your
partition by stream ID then you cannot
query all the the stream for a
particular user so we have to change a
lot of stuff in the code we have to
bring the user ID from all the calls to
the query itself and we have a bunch of
a synchronous process so suppose a day
we have api call and data API call
trigger a job and that job was get god
by one of the job queue worker
processors and the job was effectively
executed on our server so we have to put
the user ID from the a play called put
it in the Europe Q and not all the way
through because we didn't have the user
ID all over the place so that change was
not
big architectural chain but it was it
touch like fifty percent of the files in
the system so we did that change and
then we said okay what are we going to
do now are we going to trigger out turn
on this chain for every user we're going
to destroy the system now so what we did
was migrated data in lazy mode so the
code wake support the boat architecture
the old one and the new one and every
time an user logs into a system we
decided if we went to migrating or not
so that way we could say let's migrate
five percent of the users only so every
time a user comes in we decide with my
writing and if the check does yes then
we migrate the user and the user
starving breathing and read from the New
York ejector this way we were able we're
able to attack a lot of problem in the
early stages and not when all the users
were using the application delicate
texture sorry and then the second rule
is make the code so you can use the new
and all architectures at the same time
right now in production we have a code
that supports my sequel and Mongo at the
same time we can store the user photo in
my secret or Mongo just by flipping 9
and in sugar on the database in fact all
of the pixel employees are using manga
summer abusers and we can switch them
whenever we want so we're running in
both places but we're reading and only
one of them and since we're running in
all in both places for all the users we
can also actually benchmark a lot of the
manga stuff we can say we can measure
how many insert we can do we can make
sure the locks times our group I are
higher so you cannot query than the data
and we're trying to do everything will
change now in this way put it in both
places and then choose whatever you want
to use
so you might be saying well this have to
do with manga this guy has been talking
here for 10 minutes and nothing has to
do with Mongo know so it turns out that
after we did all the change to store the
images it was really compatible with the
manga format for all of you that have
these Mongo Mongo doesn't have tables or
columns in the document based storage so
whatever you store in there can have any
structure ok so moving from this
architecture to the Mongo one was really
see was changing only the places that we
would write right into the database and
instead of writing a g-sib string we
were reading a manga beast on object
also it turns out that partitioning my
sequel is exactly the same principle as
charging in manga so all the work that
we did in my sequel to bring the user ID
to the queries and to reorganize all the
tables and duplicated just to make sure
they fit the partitioning schema it
turns out that it was exactly the same
work that we had to do with Mongo and we
already did it so it was really easy
when we have the chance to move to Mongo
it was really easy to move everything
from my signal tomorrow
so why moving tomorrow so why they made
the decision one of the key officials
what hot is that Mongo has built-in
sharding I know there are many solutions
in my simple for sharing but the thing
is with Mongo is built int and it's
really easy to start up it's real easy
to set up and start using it we tried my
secret closer as I said but the problem
with my single cluster is that it's a
very robust solution but configuring and
starting up for the first time is it's
really really hard the second thing is
that replica set features automatic data
kelowna synchronization so if you need
an ensleg you just put in literally in
the replica set and forget about it you
will do everything by itself it will
contact an order stereo from the same
replica said copy data and start
abrogating it's like that oh and also
features primary failover so if you have
three servers that are replicas all of
them are replicas and one of them goes
down and it was a primary or master
Mongo we'll figure it out what do i do
have to do anything on application and
to fix the problem
so our data also fits perfectly in the
MongoDB probably so most of our data can
be translated ready for my sequel to
Mongo we have any problem in fact some
of our structures are very sweet from
longer than my signal Mon voice is Kim a
list so forget about doing the altar
tables that take hours to complete or
switching from master so you can
manually do the altar here and then
switch back you can forget out all that
then among go since its supports
replication I'm Charlene built-in you
can have many small machines to the job
that one superstar Barry Mexico does and
I'm going to talk about this later
also mono features background in this
creation so if in my secret if you went
to a new index in one table for X reason
you have to log the table and you have
to wait until my sequel finishes the
creation among what you just create an
English and you say background through
and that's it it will create the inyx
and once it's ready manga will start
using it for query but you won't lock
the system so some numbers about
comparing my sequel among gwa so
building backups this is one key feature
that manga provides in my sequel there
is no really app built in way to do
backups efficiently you can do my sequel
done but I mean when you have large data
set is not a good solution so building
back up for us not including the three
Tellarites photo storage with my secret
is 45 minutes and we have to bring one
of the slaves down and do everything
manually we're not using extra backup in
picture I haven't use I use it in the
past but it's a great solution for the
online backups in Mexico in MongoDB
there's a built-in function to do
backups and it takes around 30 minutes
to the full non-blocking back up and
it's built in so used to a type of
comment and it will connect to the
sharding and you will start dumping the
launching a new slave in my sequel is at
least three hours and is everything is
manual there is no single process that
is automatic and then once you finish
setting up the slave and copying
everything over and everything is like
two hours behind so you have to wait
under the sinks so you start at least
three hours among go you just put the
slave in the replica set and it will do
everything we have done several tests in
like in full load environments and it
takes like 20 to 25 minutes too late to
25 minutes but the thing is that you
have almost non modeling the interim
version so it's so easy that we have
another one out to made a script so we
can launch and terminate slaves whenever
we want creating a new shirt and balance
balance the data in my sequel we
currently have no way to do it we're not
using any framework for sharding we're
using discharging ourselves but if we
launched three new shirts today then we
there's no way we can balance the data
we have measures to say the system only
putting users here but we cannot balance
in MongoDB we started with two shots and
then we had a new one just to test the
charting balancing and it took 20
minutes on the full load similar
circumstances
so our their small machines in mongrel
so my fickle supports role they were
looking that means you can have a table
with a bunch of rows and there could be
a lot of processes accessing the same
table in different places on different
roles my signal that will take care of
it and it will lock only the rows are
being updated this means that a single
machine can handle a lot a lot of
transactions per second by the other
honey manga manga blocks this there's a
global block so if someone is grand into
a collection no one else can be reading
from that collection on any other one I
think they were working on the on the
back / collection login oh ok ok so
that's why we at the beginning we
started using big machines for Mongo but
then we were facing this problem the
mono queries will were getting really
slow and the lock tanks were super high
and the CPU was almost a zero and the
problem was that one core doing one
query can be looking the entire eyes so
we decided to have very small machine
for Mongo and spread the load across
them all of them and that turns out to
be the solution because we have three
charged right now with three machines is
short and and we can I mean it those
those three shots and getting almost the
same traffic than my simple machine I
would say the same and we are not seen
any performance products on that and
it's much cheaper here no I mean I am
what I was saying like suppose one of
this machine goes down amazon for some
reason the volume was lost or they
terminated for some reason the
performance of your application will not
be decreasing because the load is spread
across multiple machines with my sequel
since we have this big server and we
have slaves and stuff but supposin for
some reason we lost two of the slate
then the performance of application will
be will be really really bad in that
case
so I would way this is my personal
recommendation for starting with mom we
didn't do it this way but i think is the
best way to start so since the lock-in
prowling in monkeys is something that
you should have to take into account you
have to start with a replica set a
replica that means one primary or master
and one of multiple slaves because
you're going to be you're going to be
great into the master and you don't want
to read from the master because he's
going to be locked so you have to start
with one replica set if you start with
the replica said and then you have to
move to a shouting environment there is
a way to do it which was the way that we
did but my personal suggestion is that
you if you're in the cloud start with a
shorter environment from the beginning
and do one shot only the extra cost of
this is going to be the three service on
top which holds the Charlotte
information but those could be micro
servers which are really really cheap so
I think if you're in the cloud there's
no point of not starting like this then
in every observer your web servers or
job service or whatever do installed
among us which is a router it knows
exactly which data belongs to which
chart and you connect locally to them so
your engine eggs or PHP or whatever is
connecting locally to a Mongol process
running on the same machine and that is
going to route you to the Charlotte you
need to connect if you start like this
and then suddenly you need to grow is
literally just a matter of adding a new
char MI and Mungo is going to start
balancing the data and you have to touch
the application everything is going to
be handled automatically remove
some other considerations so in my
sequel you can store 1 2 3 as a number
and then query when two 3s a string for
my figure those bodies are the same it
will do the casting automatically among
go that's not the same so you have to
make sure that if you're starting
integrals among god you have to make
sure you're starting interiors and not
put quotes around it because Mongo will
store them as the strings and we had
that problem at the beginning also when
you query them if you query if the late
Isis towards a string and you query as
an integral then the quad is going to
tell you that it doesn't exist so this
is a big you have to be sure that you're
starting to drag data a secondary
indexes have the same effect on Mexico
partitions that in manga shot that's
when when you Mongo shattering you have
to select a key for partitioning so if
you do a query in Mongo and enjoyed
using an index that it's not the partner
sharding index you're going to be
hitting all the shots it doesn't it is
not a big deal if those queries are for
administration purpose but if you're
doing for the application then it's
going to be it's going to be a problem
you shouldn't be eating more than one or
two chars the Mongol also supports
transaction auto Missa tee in our
particular case we don't care about it
we never used transactions in my sequel
anyway so but if your application needs
to have transactions and I begin in a
community or roll back then you have to
implement those manually and then for a
conclusion there is no there's no table
that says if you use these if you're
going to do this use Mongo if you're
going to do this use my sequel there is
a I mean my secret is a great product is
much more mature than the moment has a
lot of features and built in stuff and
one way is much more simpler but it
makes your life easier to start up so I
think to the side you have to analyze
your case your particular case with a
lot of detail and make a decision based
on that
then thank you very much if you have any
question
I have polymer my team here so you can
do questions for them also yes all the
way with your storing data this is the
number image grow the data is on top yes
what is 16 max per document so one
document is one for us one document is I
use ready and the category and then all
the the photos itself and we have a
limit on the photo so when everything is
going to grow no no we unleash 16 max
per document yet yet well yeah I think
they may have what I think is you have
to do motive or you have your hand you
can do them how do you call the file
system to store big stuff yeah Chris
purposefully five-year strings are
bigger than 16 language
yeah a lot of identifiers
yeah huge amount of data what were you
saying about charting and how you can
set up like engine X or some sort of
like web or load balancer okay this set
up yeah okay so in this set up do you
have three servers here that whole
discharging information those are called
mono conflicts then you have your
replica said where is the data is
actually in the replica sets so in the
first char you have a one portion of the
data and it's all replicated and then
you have all the Sharks then on each
application server for example in your
web servers to install a very
lightweight manga process that is called
manga ways that process is going to
query the config service and it's going
to know which chunk of data are in which
chart so when you connect to the Mongol
you don't connect to these servers or to
these ones you connect to local host and
that's going to be the manga process
running in your machine and and that
process knows where the data is so if
you were to give me the data for a user
ID three that process knows that user ID
dressing sharp number two and it's going
to connect to shell number two and get
it for you so you want to forward the
query that you do to them to the
particular shop I'm gonna have more
questions about this I'm sorry ok yeah
right
dc2 so how many servers do you have
right now
okay for mice mice ql we have a master
master configuration why must I mean two
masters each one of them has I think two
or three slaves and there are really big
servers are m22 x-large they have like
three gigs of ram and then on the other
hand we have six standalone mexico
servers for storing it to the three
terabytes of data and then we have the
mono configuration which is a three
shards of three servers each or each one
of them is has seven gigs of ram and
then we have the limo config does it for
the database perspective then we have
around 10 or 20 web servers depending on
the load on the time of the day and then
we have 50 I think 50 up servers they
are querying photos all over the place
24-7 and they're hitting the data is
constantly with that so the little
answer the question ok so in your
process of migration you describe the
most difficult problem you guys have and
how are you solving is that unity for
much more furniture it's bigger ok how
would you go about big papa was a big
people ok the biggest problem was our
code we watch which was not very Molina
at the moment and apart from that we did
a lot of testing with Mongo and we spent
month until we found this this thing we
were trying to use Mongo in a big
machine because we thought that more RAM
you bought was better and we were
reaching high lock times and the queries
were really slow so once we start
splitting the load across multiple
machines you start working really well
and I think that was one of the most
difficult stuff because
yeah I mean we were we were experienced
high-low lock times and they told us
that they were using mutex or something
when I was giving some problem for Mongo
200 yeah and then after we change it we
applied to mongo202 we were in sprint
the high lock tanks there was a lot of
using this really really low but using
the standard library mutants who some
virtualization parents so we switch the
way for doing that we still have a
single global walk over using and
customization it didn't consider see
yeah exactly but then even after we are
even after we open anything we were
still having a lot of times so basically
was trial and error like thinking and
thinking and thinking why is happening
why is happening Momo cannot be this
like this I mean until we understood
that we have like low CPU low memory
usage and high lock time and we just
decided to try it with smaller machinist
and it works well Mongoose come is not
sequel is completely different so before
we change all the coal we have like
sequel all over the place now we're
trying to centralize yes that's a
problem with our coal was a prominent
manga we were this kind of making my
sequel like you know she of all the
steps of shower and then with one
document that was a difficult part of
months right but then after in done that
my head tomorrow was great yeah it's
almost there
we're not tired of that conversion
outside he'll try having multiple
charges hardware big hard work no we
strive to a multi my changes because
since maybe we had physical machines
that approach will be great but since in
a mess and you can do smaller machines
and the price is basically the ram you
use them no it's not wait we didn't try
it yeah which is raising pet dragon's
PHP and the standard Mongol drivers and
we had a problem with that like if you
have 32-bit machine and 64-bit machines
which we have you have to make sure you
use the integra classes from Mungo
because you're going to have problems
then when you store some integrals that
I've greater than 32 with precision this
guy can yes how big is it all fit in
memory listen if not do you have trouble
okay em since we haven't migrated all of
the data to Mongo we all of the data now
fits in memory the thing is that we know
that eventually we're not going to be
able to feed all them the light on
memory so we're going to fit the indexes
only so we want to fight to get the in
the system memory i'm part of the data I
think that's okay know that you can
confirm that hey yes anchor it on a
Google peripheral i also say that micro
instances are not necessarily suitable
it can take service we say you are using
my currencies yes we we read a paper
that there is I think it's among a
website willing can confirm jesse was
among the website now yeah so we're
instances and all the two bids are out
of the scene you must use
for me except for one place always good
believe so I was skeptical about that I
was a little scared when he told me but
like micro but yeah but they work out
really good the moma configure my own
fixer my crew and the arbiters also are
my crew yes did you in your pretty
sharpish you have a she please yes so we
currently have catching on my signal in
some memcache layer and it's manual
implement is no it's nothing straight
into my signal you know that there's
some plugins from Mexico but we don't
use them so we catch everything manually
since we're still using my symbol for
some stuff we will catch him on wall so
we we haven't done any tests if we need
to catch it or not
those are from my sequel statistics yeah
so men cash is getting some of the load
and then okay yeah my single version
that you're ready you say the more using
extra back we wanted
no we're running my cibil 55 55 points
yeah high five basically we migrated
from 51 like six no sir but we know you
think we're gonna wheel yes if there is
some particular point A or point today
that were the critical decision to
switch away from my sequel I generating
mental advantage you guys said some of
this either but yes okay basically one
of the downsides of this set up on my
sequel is that since this data is jiseop
and it's not string you cannot update
stuff atomically inside the photos so
one of the requirements we have the
hashtags is that in the future we need
to be able to do it because it's really
expensive to be loading that in memory
doing on the process and starting it all
again so one of the key decisions
although we're not using it yet I think
we're going to start be using that very
soon is that that doesn't was a personal
what do you see what was the thing I was
going on within your attention from the
driving a decision
were there anything else not networking
balance yeah I think the key decision
was the AC of growing up so we want to
be ready to grow up very quickly and we
don't think that it was that you're
fighting and fighting and fighting to
scale scaling should be something that
is you get tools that make you easier to
scale and we think that with Mongo we're
if we if we set up a crown to scale is
going to be really easy to grow up or
that yeah it's not slavery talk about
lazy vibration of super silver change
yep just overall how was your experience
with that I mean if you want to well if
I means harnesses was like kind of a
nightmare now yeah the thing is that you
have to support both version on the code
so the code gets a little nasty and then
but I mean if you do it correctly you
can spot problems at the beginning how
much do you under my great oh yes so we
with within one month progressively and
then I then we did a batch process that
Maghreb my breath no I think we until we
might need everything we have like 60 or
something when we let the the Charlotte
all tables oh you're giving that lazy
ass users long ago how much of it easily
and how much of it did you again i would
say at least sixty percent but I have
the number one very conservative about
how did I mean obviously we were scared
to death of the time with either the
beginning was like difficult that once
you have them hope I'm seeing that
everything was working fine you know
what he gets the clinic we should forget
about it was like oh we're running both
assumptions out whether or not is up we
get a lot of people worry about that
exactly the Federation of everything
else yeah one of our big bottlenecks was
our code it was spread out all over and
so doing doing this lady migration
really forced us to centralize all of
the access points and it really helped a
lot actually like it forced us to clean
up the code yeah so right now we are
writing in my secret among go a lot of
information and we reading for some
users in my Steven and reinforcing long
ago and as he say like the beginning was
a nightmare but right now it paid off
because we can actually switch whenever
we want yes wait for some time with
close to doing actual beliefs and give
me my signal a Mongo either yeah so
right now I am remember I told that we
went sequel 551 we have to defer to the
leading for later way we were going
right now we're not doing it with
deleting on the fly and we haven't had
any problem with that I mean the problem
was for my sequel 51 which has a really
overhead when you delete but since we
migrated to 55 then we didn't have any
more delete problem so we're deleting
all the time and like all photos and
stuff like that and doesn't make any
problem considering a similar pressure
with your house key well yeah yeah it's
working right now like if you want to
pick some like Nigel your data with me
on my sequin among go and everything
that happens on my sequel will be will
happen among oh yeah we did along with
yeah yes sir yes not everything is
normal especially a one-page latest
mm-hmm in your system is serving 23
itttt practices between
for example let's say comments before
turning right and the comment can be
later so you have another parent okay
yes for the photo data is all duplicated
then we have the hashtag in problems
because we have duplicated photos but
the hashtags are common for the photo so
we have a separate storage that relates
a particular photo and says how many
hashtag of what time does he have and
that's globally yeah if and separate
collection that holds the reference
would ID and if you wanna do the query
you have to quit get further photo from
the user and then you have to query that
table and get the hashtag yes so what
percent of your date is set aside
business right now the Mongol for
grading is one hundred percent yeah so
everything that is getting green into my
sequel he's getting written to mongo
except some tables that doesn't have any
major impact on the system and for
reading a one hundred percent of the
user are reading some fields from Mongo
because that was another thing that with
it instead of creating new columns on my
sequel everything that is we have to
create new on one table we're doing it
only on manga so mogul compliments
whatever we have on my sequel so first
one hundred percent of a user's we're
using some tables in manga like that the
only data that is for that table is
there we have a holla back over Mexico
yes the index is filter I mean I I'm not
taking into account the three taillights
of photos this is only users and
category style stuff and up some part of
the photos where we left are moving the
forest very soon number of sharks well I
afraid that do and you have seen
installation with 100 shots huh so I'm
trusting on these guys if anything
happens get blame these guys yeah one
more question ok ok a hit right Cheryl I
our CTO try to try couch TV and hit Rio
Darwin's I personally tried a amazon
simple DB but it had some limitations
and the amount of drawers that you can
put into a table and then after we
decided to go to Mongo amazon releases
the i forgot the name you know
dynamically but we didn't try it because
we already were in the middle of the
Mongol immigration also we tried my
sequel cluster which is an sequel
solution but at the end you have to use
the ha no sequel and we have the
experience that we told you and we went
looking for too much no simple solutions
because we have like two or three
advisors we're using Mongo in their
companies in a big pretty big
installations and daily they were for us
on this ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>