<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Center Awareness - Scott Hernandez - 10gen - MongoSF 2012 | Coder Coacher - Coaching Coders</title><meta content="Data Center Awareness - Scott Hernandez - 10gen - MongoSF 2012 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MongoDB/">MongoDB</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Center Awareness - Scott Hernandez - 10gen - MongoSF 2012</b></h2><h5 class="post__date">2012-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NYXGELbcgpE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today we're talking about data center
awareness specifically going into sort
of what is data center awareness how
does it lets you have an agenda actually
fly with the infrastructure behind data
center awareness so if you need to send
our weirdness the odds are you need some
kind of infrastructure in front of it
we'll talk about what that is you have a
bunch of players and moving pieces in
different data centers will talk about
how they interact together we'll get
into what the goals and challenges
making this interaction seamless and or
work at all are and we'll talk about how
long ago DD has solutions for you now
and in the future the futures is closed
and we'll talk about how close that is
at the very end and feel free to ask any
questions as I go it's a small group
feel free to come forward or stay where
you are raise your hands or shout out
it's the end of a long day for all of us
so I'm here for you all right let's talk
about a really simple scenario we've got
three data centers European Asian data
center in USA and in Europe clearly I've
created two nodes here one for the
application one for the data this is
very general not so much Mongo specific
so I'm going to start out with a very
general sort of data center awareness
and then we'll get the details of how it
gets implemented with Mongo and could be
implemented with Mongo so the first
thing you might think is look at Asia it
has no data where is it going to get
data when it needs to read probably not
from the US maybe not the best option
maybe you probably want to see this read
not go to the US but to go to Europe
which is arguably closer so this sort of
highlight some of the issues that
multiple data centers that are live
might deal with you can start to think
okay well if we're talking about these
kind of scenarios what kind of
infrastructure exists in front of this
or in use for it so we're talking about
geographically disparate clients clients
all around the world somehow we're going
to have to get those clients and their
requests to the closest resource to them
in order to service them more quickly
and localize the information and
availability hopefully and how failover
all these great things to intermix
but first and foremost I think a lot of
people are think about data center
awareness in the context of how do I get
regional data centers how do I keep data
there and keep users in the region they
come from at least that's what I think
about so the other thing you might have
in your infrastructure is accelerators
reverse proxies in those regions and
those data centers in order to speed up
or reduce the latency for users
interacting with your systems right
you're probably going to have some kind
of server application running in those
regional data centers or in those
different areas so sort of the players
we know the players their users they're
our favorite they're everywhere somehow
we're going to get them into a data
center probably through some kind of
load balance geo DNS system once they
get into the data center we've got an
application server for them that's going
to do some work on their behalf it's
going to access a database or many
databases possibly in the same data
center possibly across boundaries the
application awareness part comes from
some bit of configuration and there's
some reason for that configuration and
the reason for that configuration is to
try to keep things as local as close as
possible this sound familiar to everyone
is this kind of what you're expecting
for data center awareness am I on target
for everyone two thumbs up with one
great all right so our goals for this
whole project for rolling out multi data
centers across the world are reducing
network traffic right hopefully first
and foremost we want to reduce latency
reduce inner DC or data center
communication we want to essentially
create these little little enclosed
worlds all around the world which can
hopefully serve as much of the data and
answer as many questions and provide the
users that are close to them with as
much of the service that you offer as
possible so we're going to localize
those resource uses we're going to
reduce failure cases because we're going
to make those little tiny worlds those
data centers as self-sufficient as
possible and that of course increases
availability because theoretically if
you're creating these little
world's they're probably going to work
pretty much independently of the other
little worlds you have now there of
course going to be connections and we'll
talk about how MongoDB deals with these
entered DC connections and how to deal
with for example replication and
sharding and of course hopefully in this
little picture that we're drawing
because we duplicate all these services
in these different data centers around
the world will be able to have better
dependency isolation so it'll be easier
to debug there'll be better ways of
figuring out where things go wrong right
one of the biggest problems with
distributed systems which I'm basically
drawing a big picture of here is there
really complicated and the more we can
do to simplify the small little pieces
that interact with each other the better
off we're going to be in deploying
things and managing the maintaining
things and providing good user feedback
and interaction so one of the other
goals and it's kind of a secondary goal
i'm not going to go into it too much
detail is partitioning the data based on
geographic boundaries you may have
different requirements that for example
data for different users exist in the
region that user lives so there may be
legal reasons that the data for some EU
members can't leave the EU or even some
set of subset of that data we're not
going to get too much into that here but
it is a requirement for some people all
right so let's talk about the challenges
for building a system like this in
general so the biggest challenge is
going to be in some ways data
consistency and what is data consistency
mean well it usually defines the user
experience so the user experience is
paramount one might say I would hope and
what we want to do is give the user a
consistent experience one which doesn't
confuse them one which doesn't surprise
them one which seems good but the
interesting thing about user experience
is it's usually relative to the user so
we're going to talk about some tricks
and some tools that we have and that are
generally useful for making the user
experience consistent to a user but not
necessarily consistent across all users
so then the reason for having consistent
data is course doing back up and
operational operations but so one of the
other challenges of building out multi
data centers is at the issue of scaling
so we'll talk about how to do scaling in
terms of rights we'll talk about how to
do scaly in terms of reeds and how to
keep availability good while doing those
two things so in addition to scaling out
the data reads and writes we have to
talk about partitioning shardene of that
data because we're talking about
multiple data centers and theoretically
there has to be some authoritative copy
of the data if there isn't an
authoritative copy the data there are
all kinds of bad things that can happen
with user experience at consistency so
we'll go back to my picture again this
time we're going to talk about a little
differently before I said hey we
probably don't want to send that read
halfway across the world but for
consistency reasons the authority of
that data may in fact live in the u.s.
so even though it probably isn't the
best option if you need to have a right
go to the authoritative database the
authoritative databases in the US then
you need to send it across the world but
that doesn't mean that you have to read
that data from the same place it's
possible that if your application
supports that you can then read the data
or other data from a more local source
all right so now let's get a little bit
into the default behavior in MongoDB and
how this sort of applies to this data
center picture that I've drawn up on the
board a little bit
so the default for mongodb is there's a
primary it takes all the reads and all
the rights we have a single primary
system where there's one authoritative
copy of the data and all the replicas in
replica sets or master slave make a copy
of it and that is some time delayed it
could be time 0 basically down to the
millisecond or it could be minutes or
hours so theoretically if you want
consistent data you want to read and
write from the primary the primary only
this means that you're never going to
get stale reads so this is the default
behavior to someone familiar with what a
stale read is or the concept mostly
shaking okay so the general idea is you
want the authoritative copy if it's not
the authoritative copy it could be stale
most applications don't do well we'll
still data however in multi DC scenarios
you generally want reads locally done
which means you probably have to opt
into some stale reads right this seems
like hopefully an understandable
trade-off it's one that you guarantee in
terms of network latency and throughput
such that you can get better isolation
better experience but it does mean that
your application has people to handle
these kind of constraints or loosen its
constraints in terms of how close to
current the data is
alright so currently we have replication
so MongoDB replication works very
similarly to which way i just described
we have primary nodes and secondary
nodes or an authoritative copy which is
one node and the rest of replicas which
are non authoritative however you can
use them for reads if you opt into it
they hold a complete copy of the data
there's no sharding or partitioning
replica sets are individual nodes that
have the same data across the board
there's something which is called a
right concern so generally when you
write to the primary you'd like to send
some indication of what expectation you
have for that right by default rights in
MongoDB are as we say fire-and-forget
meaning that we send the right off it
goes to the primary hopefully and it
stays it's there now there are different
levels of the concern that you might
send off you might say I actually care
for an acknowledgement I want to know
that right made it to the primary server
you even go farther and say I want to
know that right made to the primary
server and it made it to a replica and
we'll talk about some of the future
stuff here one of the things with right
concerns we have now and mongodb is I
want to know that right made it to the
primary and it got replicated based on
some tagging of the replicas so one
thing you could do today is tag each of
your replicas with information metadata
essentially says this one's in this rack
this one's in this data center in Europe
this one is really important this one
slow this one's fast whatever tag you
want to put on there you can describe
your right including those tags so you
can say i want to write to the primary i
want to write in addition to the primary
to the disaster recovery node that I
have somewhere off in the corner I want
to make sure that every right that I
send is acknowledged by not just one
server but two servers or three servers
including this set of servers so you
have verifiable rights verifiable not
just based on a single primary but
verifiable in terms of a subset of the
possible nodes in your replica set
so this provides a few features replica
sets in general do it provides isolation
because it's possible to read from a
secondary or to a replica that isn't the
primary so these can be geographically
dispersed you can have them all across
the place and the only thing that ties
them together is replication but each on
its own is actually a full copy of data
which means you can read from it but you
can't write to it it also gives you
increased availability as well as as I
mentioned being able to possibly do
still reads with a combination of the
right concern that I mentioned and the
ability to read locally it means you can
put these two things together and say
things like I want to make sure that you
know my Asia node has to copy this data
so when they do reads it's always up to
date and so every time you write
something important you can guarantee
the right concern for that gets to those
nodes that kind of make sense yeah so
the cost of that the question is what's
the cost of that the cost of that is
waiting until that happens when you
write yeah
so the question is what happens when
you're right to the master dies it
depends how it dies but if you don't get
back verifiable right then something
happened what happened is you don't know
it could be anywhere between success and
failure but it's not success
yeah it keeps the powers off
so this doesn't have a quorum system so
replica sets have a well it depends let
me let me ask you a question what do you
mean by quorum so for in yeah so for a
replica set to have a primary there must
be a majority of nodes up that are
configured which is kind of like a
quorum but usually when people talk
about quorums the topic quorum reads or
form rights meaning that there's a
certain number of nodes that you read
from in order to know if you have a
current read that the most latest read
ah ok so I think your question is how do
you guarantee that when the primary
fails if you accessible right that it's
available after that failure and the
answer for that is you must as you said
guarantee that you have a majority of
your members that have that right yeah
so there is an option to have majority
it doesn't specify which members right
currently which ones are tagged to make
up that majority it just requires that a
majority of them have the right
currently you cannot tag and have a
majority but that is possible in the
future just not right now and the future
I'm talking about is hopefully the next
release which will be the end of the
summer yes right tagging is available in
version 2 point 0
so when I said hopefully the next
release I meant it is something that's
been discussed as something that will be
in the next release but not that is rid
code written for it yet so the next
release is 2.2 currently we're on the
latest stables 204 205 explain how you
look at it uh and then currently 2.1
this is going to get old fast cool
alright so I'll just do this every
second so the the next version is going
to be to point to the next stable
version and in between the even-numbered
releases we have odd numbers which are
development releases so currently you
can get actually none of the stuff I
talk about the future is available in
code yet but it's being worked on some
more than others I said 2.2 is hopefully
by the end of summer yes I there's no
hard date it's mostly defined by
features and of course as you know you
can have one or two things features are
time so we don't have a hard date
however it's looking reasonably good
that it'll be out I'd I'm not the
authority on this but I'm I feel
reasonably good that July is a good time
to think that it'll be out which is
still summer I think yeah
so the question is how can you guarantee
there's only one primary master at a
time and the short answer much like we
went to earlier as you must have a
majority of the members elekton the
primary our master so if you have a
split in which we've got two over here
and three over here well three out of
five is a majority if this is asia or
europe then the masher goes there
basically it's a little more complicated
than that but that's how it works so the
soon as there's an election process and
as soon as that happens no one's primary
anymore basically so he steps down he
becomes read-only secondary and this
could happen on purpose or not on
purpose there's there's many
configurations like you'd imagine if you
had four nodes and they had a network
split two and two you're going to have
no one being master yeah so the question
is assuming you have a field right how
do you know what to do next do you do
retry it or do you not so currently when
you have a failed right with tagging or
some other restriction on it which
guarantee which can't be met basically
you don't have a lot of information to
work from you don't know first if the
well okay you know some things so if you
get back a response you know the primary
was up and something happened so one of
the things you can set when you talk
about these concerns is a timeout so you
say hey go try to do this and if you
don't do it within X number of seconds
or milliseconds come back and tell me
now assuming you get a response you know
for sure that it's been written on the
primary and it just hasn't been
replicated yet to the other node so you
can ask again hey is this priority met
yet or is this restriction met and you
can wait until it times that again if
you want you can even call back and say
okay I changed my mind I don't care
about a majority I just care about to
maybe not what you really wanted but you
may have reasons in your application to
accept lesser guarantees yep
so the question is what do we suggest in
order to make replication happen across
the networks and what are the security
ramifications around that is that good
yeah so the short answer is if you can
make them talk to each other then it's
good to go how you do that is really up
to you if you want to run your own VPN
if you want to run an SSH tunnel if you
want to open up the Internet those are
all acceptable answers some with varying
degrees of security we don't have a hard
steady like there's there's nothing I
could say this is the way you must do it
because any way that works is arguably a
good way so there's degrees of good
though so opening them to the internet
without doing any kind of security from
the transport level probably not the
best way we have I know a lot of people
who run things like Open VPN and SH
tunnels between nodes the only thing
that matters is that every node be able
to resolve every there node and have a
way of communicating with it however you
can guarantee that requirement you're
good to go all right so that's the basic
for replication one of the things I
didn't mention in here is the most
drivers and I'll mention this in the
future for charted situation most
drivers will read from the local replica
set member if you allow non primary
reads so it'll basically use some kind
of ping to find whoever's closest and
read from there that's if you don't give
it any other requirements like well let
me go into that now okay so let's take a
few scenarios for rights because it's
kind of complicated so your client
doesn't write New York data centers the
primary replication kicks in it isn't
quite this way but this is a good
diagram and it gets replicated to Europe
and California right so there's a simple
right no work no requirements no
restrictions it just was a right got
sent out replication happened
asynchronously pretty close to the time
of the right but let's say you wanted to
do a right and guaranteed its replica
safe replica safe meaning just one other
member in the data center well in that
case
case you're going to send the you're
going to send the replica or the
replication is going to happen and
you're going to wait for an
acknowledgement that it met that
criteria that it went to two nodes this
actually this picture is not quite right
because theoretically well actually in
practice it only requires one of these
two lines to exist before it responds
but I'm not so good with the animations
as we'll see here hopefully not yeah
okay ignore that replica safe thing okay
so let's assume that we now introduce a
new client Europe and it wants to do a
query well the u.s. node and New York is
still the primary so if it opts to do
non-primary reads it can do it from the
closest data center which is going to be
in Europe so this is an example of the
rights been sent it's been acknowledged
in one of the other data centers maybe
you're up if we tagged it we could have
guaranteed that acknowledgment was from
Europe and then when the client goes to
read it they're going to get the most
up-to-date data that make sense okay so
good uses for tagged writes multiple
data centers racks for availability and
reliance reliability for our backups so
back obsessed recovery kind of go hand
to hand and so much is that you probably
want to guarantee that wherever you're
doing backups and wherever you're doing
disaster recovery always have most
up-to-date rights if you care about
keeping them in the future
you
alright so let's complicate this a
little bit with sharding and replication
the odds are pretty good if you're going
to do shard your data meaning take your
full data set and cut up into pieces
you're probably going to keep replicas
or copies of each of those pieces so you
have high availability and reliability
to data so we do shard based sorry range
based sharding which is to say we break
up the shard key which when I can get
into much detail but you define a shard
key it's a key space let's say from
negative infinity to infinity you break
it up into chunks each one of those
chunks represents a group of documents
those documents are not tagged where
they get balanced and distributed
relatively randomly every chunk stays
together but let's say for example we
had a shard key which was a through z
and we had a through you know em and
then em through k and so on and so forth
each of those ranges could be different
sizes based on the number of documents
within that range and they don't have to
be contiguous between them each one is
contiguous within its range but between
them it isn't so you might have some
eggs and some z's on one shard you want
I have some bees and some Z's on another
shard it really depends on where the
boundaries are drawn so that's all good
it works fine now but some would argue
it's not very data center where because
you might want to set preference based
on your shard key where that data should
live so we'll get to that in a minute
right now not really possible as I
mentioned we get all the advantages of
replica sets which shardene rights go to
the primary replica set member for each
chunk for each shard key value reads by
default go to the primary and only the
primary but in this multi data center
world will probably want to send reads
two secondaries so we want those reads
to have some kind of read preference so
now we move into the future new features
we have something called read
preferences so before we had a feature
called slave okay it was not named very
well in my opinion I think read
preferences makes a lot more sense
because it gives you an indication of
you want out of the read not the actual
bit that we set on the query so by
default the read preference is primary
only currently we have a read preference
of secondary which is preferred meaning
that if you can contact a secondary you
should and you should use that read
which gives you the basically
consequence that you could have stale
reads we're also going to be adding
these three other options which are
primary first yeah secondary only
meaning don't ever use the primary for
reads and any meaning closest all right
so currently this read preference stuff
is not implemented everywhere some of
the languages implement their drivers
this way so when you could directly
connect to a replica set which is just
which is spread out in multiple data
centers whatever wherever the client is
they will basically go to the closest
primary or close to secondary in order
to do their read this does not currently
work with shardene which sharding it
basically chooses a random secondary to
do reads if you are going with read
preference secondary but in the next
version it will follow the same behavior
that replica sets are that drivers do
for replica sets and will read from the
nearest one of these categories make
sense to point to yes actually I think I
think that code will be in 212 if you're
tracking in the development branches but
it's not in 211 which was just released
two days ago whatever it was yeah
yes so the question is since stale data
can be such an issue for some people
what about doing reads from the closest
secondary and the problem with that is
it's it's a good idea it's a little more
complicated and and it requires some
kind of balance of prioritization
between closed and up-to-date and
there's nothing inherently wrong with it
just that it's complicated it's not
currently on the schedule but it's it's
definitely something people want so
things people want generally get done
file a ticket yes please everyone vote
for his ticket okay so let's move on to
the new new stuff so in the sharding
world we're going to be introducing
something called tagged balancing we
actually don't I don't know what it's
gonna be called yet but it's
conceptually that the idea is that
you'll be able to annotate your ranges
of the chunks of the shard key with some
kind of affinity to shards or two
replicas set members so that that data
moves to those replicas set members or
that replica set shard so that you can
do things like say hey I'm going to
build a replica set I'm going to make
sure the primaries in Europe and I want
to associate all European data by some
range to stay on that shard so we
balance data between shards we split it
up and move it around we're going to try
to follow those affinities based on
tagging I can't give you too much detail
about this because once again this is
not done but it will it is scheduled for
the next release will also be
implementing the read preferences stuff
I talked about which allows you to do
basically closest reads which will be
very helpful so the new behavior will be
basically local when non-primary tagged
which gives the ability to custom
tagging much as you can now with writing
but not with reading yet and you will
basically make choices about what region
data center rack you want to do those
reads from so you can now pair up you're
right tagging concerns with your read
tagging concern
which hopefully will give you a lot more
power to control where data is both
located and read from alright so back to
this whole balancing distribution thing
so we will to tag chunks with some kind
of affinity key or some relationship so
that we can associate them with shards
and those shards could move on their own
or sorry the shard primary could move
because theoretically the replica sets
so we want that data to stay there but
it could have replica sets can have
something called a priority which gives
them a ire chance of being on a certain
node so you could set priority for
replica set in your charge cluster to
Europe for example or USA and then you
could associate those chunks of
documents to those individual shards so
they stay within the region for example
like you might for example a shard on
region and user ID so you can break up
those different user records within each
region but still tag the region so that
they all stay in the region they're
actually hosted in so you might say like
you know USA or europe or asia colon
user ID and create a rule basically a
regular expression for the tagging part
of it that says anything that starts
with USA put it on the shard which lives
in the USA and another thing which i
don't know if we're going to do or not
is a the ability to have super chunks
which are groupings of chunks which stay
together but aren't necessarily
dependent on any particular shard so the
idea is you might say some of this data
you know you'll be using at the same
time and same queries and you want to
keep it together right it's going to be
broken up in chunks because that's how
we do how we break things down but we'll
try to keep them together in like the
same data center or even on the same
shard that's a little less defined but
it's the direction we're going in that
make sense and I think I'm pretty much
out of time and at the end of my slide
which I for
have to put a question and thank you
slide so thank you and anyone have any
questions yes
you
yes so the thing that finds the right
shard is called Mongo s it's a proxy or
router and they're essentially can be
run on each client and it knows where
the chunks are and where that affinity
is for their it knows where they are so
you'd have multiple those in each region
in fact you can kind of think of it as
an extension of the client for the most
part so the client whatever client you
have will have that information either
directly or by proxy through this
process yep so the question is in the
future is there an idea of having more
than three config servers and the answer
is yes in the future when that future
may come I couldn't tell you currently
there there's a lot of there's a lot of
discussion around it there's two or
three issues in JIRA for different ideas
about what that could be some people
want them to act more like replica sets
and I think that's kind of true they
want them to be able to you want to be
able to bring up a new config server and
not have to do anything to and it
magically gets all the data and works
but it's not in the next release yep
so the question is we spike and make
this shorter assuming that i'm using geo
DNS and trying to help redistribute load
both of my application processing and in
my data how do i move that data in the
background to stay with the distribution
of hardware resources that I'm
reallocating or adding on and the short
answer is data migration like that is
complicated and we don't I don't have an
answer we don't we don't have a way of
what one answer might be that you have
to change your tagging to make it more
granular but since we haven't done this
yet I can't give you a definitive answer
off the top my head that would be my
first guess is that you have to write
more complicated rules for the tagging
and essentially what happens is the
tagging gets applied during distribution
so as long as there's an imbalance of
nodes then it will cause the tags to be
evaluated which will cause things to
move and migrate so it'll happen in the
background on the non-automated side
there's always the option of moving
stuff manually not a great solution but
it exists today you can turn off the
auto balancer and you can move stuff
around distribute any way you like
yeah so the question is what happens to
users who move from Europe to the USA
and you want to move their data with
them there's nothing built in here to
date to deal with that most likely we do
is read the data rewrite the data with a
new key which indicates their new
location it's a manual migration process
for the most part it's it's a very
similar answer unfortunately to what
happens when I want to change the
meaning of my shard key it's not an easy
thing to do okay i think i'm supposed to
finish up here so any more questions no
oh 10 minutes no I think I'm overtime ah
I thought it was for 45 in that case I
really finished early so yes more
questions sorry what's that yeah Robert
ah yes so I mentioned that the user
experience is important per user but not
necessarily in aggregate of our cross
users so one of the things you can do
for this which would really help is this
tag reading idea that you read from the
same or consistent set of data so you
won't be bouncing around different
versions of copies of the data basically
so from that user perspective they're
always getting one view because that's
one server or you know one node
essentially and so that gives the user
the experience is consistent right they
may get newer data but they're never
going to get newer data and then old
data in the newer data and then old data
which is what really throws people off
all right well I guess I'll take more
questions over here otherwise people
seem to filling in for the summary and
closing thing thank you very much for
coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>