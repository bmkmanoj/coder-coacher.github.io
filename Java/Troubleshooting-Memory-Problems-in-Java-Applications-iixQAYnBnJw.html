<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Troubleshooting Memory Problems in Java Applications | Coder Coacher - Coaching Coders</title><meta content="Troubleshooting Memory Problems in Java Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Troubleshooting Memory Problems in Java Applications</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iixQAYnBnJw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hello and welcome I'm Pooh
number half I'm working in JVM
sustaining engineering team at Oracle a
gentleman here asked you know what does
JVM sustaining engineer it's an
interesting title
interesting title so JVM sustaining
engineering as a group in Oracle where
we work really hard on resolving
customer escalated problems against
hotspot JVM so here's a little bit about
me I'm a co-author of the book Java
performance companion I have been
speaking at Java one for a couple of
years and a Java one rockstar I've been
speaking at other Java conferences as
well I maintain a blog here at this link
and I published a number of articles on
how to troubleshoot problems in hotspot
JVM so all right how many of you have
seen this out of memory error exception
everybody ok so so you are in the right
room so whenever we see this exception
you know we are kind of scared oh this
means that there is some sort of memory
related issue with my application and
I'll have to spend days and nights
figuring out what's wrong with my
application you know why this exception
is occurring so this is kind of a
symptom that there is something related
to memory which is you know which is
which is not right with the application
so not just out of memory error
exception there are other symptoms such
as unexpected memory growth in our
application or poor application
performance which kind of indicate that
our application is struggling with
memory issues and we should be
investigating that and oftentimes these
symptoms are connected to GC activities
in Java applications from
and those you know can be conformed by
looking at GC logs or monitoring our
applications and conforming that there
are are there long GC pauses are their
excessive GCS and my application which
could be caused due to miss
configuration of memory pools excessive
use of finalized errs explicit GC
invocations or even a memory leak in the
application so in this session our goal
is to understand how we can go about
from symptoms or behavior those specific
behaviors and our application to the
bottom box what is causing those
symptoms in our application this is what
the agenda looks like we'll be talking
about memory issues related to Java he
permanent generation meta space code
cache native memory and what you know
what is causing those issues and how do
we troubleshoot them we'll also take a
look at out of memory related issues due
to finalization so Java heap memory
issues so when we when we look at those
symptoms you know just rather rather
than just jumping at the conclusion that
all my application is having a memory
leak because I see those symptoms there
is unexpected memory growth there is
poor poor performance of the application
or my application is failing with out of
memory error before doing that before
jumping at the conclusion that there is
a memory leak we first should confirm
whether there is a memory leak we should
monitor our Java heap usage over time if
there are full G C's occurring during
the runtime of our application
are those fuji c's able to claim reclaim
space in the old generation of the Java
heap if they are not then probably it's
just a configuration issue our Java heap
size may not be appropriately configured
it might be sized too small so the first
step is increase the heap size test the
application again and if there is still
can take continuous memory growth in the
application in our processes then
probably you know there is a likelihood
that there is a memory leak in the
application so we can use various tools
to monitor heap usage of our application
for example in this case this is J
console connected to a live application
and monitoring the heap usage in this
case it is clear that permanent
generation occupancy is growing over
time if it is not possible to do life
monitoring of the application we can
collect GC logs and we can analyze those
those GC logs to see what is the heap
usage of our application you know is it
is it sized appropriately is it good
enough is it big enough to hold the live
data set of our application for example
in this case from the GC log you can see
that even after full GCS the heap usage
is continuously growing so the option we
should be using to appropriately sized
our Java heap is - X MX which sets the
maximum heap size for our Java heap and
we recommend setting this value equal to
the value of minus X ms which sets the
initial value of Java heap so so there
are three steps of troubleshooting any
problem you know understand that what
the problem is for example in this case
we are looking at out of memory or
memory related issues to Java heap
second step is to get insights into the
problem to collect diagnostic data from
the system and the third step is to
analyze that data to understand why the
problem is occurring and find the
solution so let's go ahead with the step
of collecting diagnostic data what
diagnostic data we should be collecting
to to troubleshoot Java heap related
memory problem
and so far for for Java heap related
memory issues we should be collecting GC
logs heap dump heap histograms so let's
take a look at you know in detail one by
one how we which tools we can use to
collect this data and how this each each
each diagnostic data helps us in
troubleshooting GC locks they are very
helpful in determining the heap
requirements they tell us what is the
you know the appropriate size of Java
heap
they tell us if there are excessive GCS
or if there are long GC pauses in
application execution these are the GC
logging options that can be used to
collect GC logs for Java 9 we can we use
my new JVM option minus X log GC to
collect GC logs and you know these these
in the first two lines tell us the GC
log options to be used with G 1 and non
G 1 collectors and for prior Java
versions we can use print GC details
print GC time stamps date stamps and
minus X log GC these options are not
available in Jellico 9 they have been
you know you would see a warning that
these are deprecated but internally they
are mapped to using minus X log option
so here in this example we can see that
from GC logs we can see heap usage in
this example there is a full GC and it
is not able to collect any spare any any
you know it is not able to free up any
space in all generation which means that
probably old generation is not big
enough to hold data or hold the live
data set of our application GC logs also
can show us if there are excessive G C's
in this case there are back-to-back
frequent full GC s which you know impact
the application performance very badly
GC rocks can show us if there are long
GC pauses
this case there's a full GC which is
taking 50 seconds of time and that's
pretty long so we should investigate and
find out why these long cc's are
occurring and how we can avoid full G
season our application so the next
diagnostic data which is the most
important diagnostic data to
troubleshoot memory related issues is
heap dumps it can be collected using J
command J map J console Java mission
control and also there is an option he
pump on out of memory error which is you
know pretty handy because you have this
in place when you launch your java
applications and if in case your
application fails with out of memory
error you would have heap dump available
with you to to to troubleshoot later on
you know why your application failed
with out of memory error so in this
snapshot I hope you can see clearly at
the back so this is J console showing
that there are n beans in this case hot
spot diagnostic M bean and it has an
operation dump heap using that operation
we can collect heap dumps from live
running applications so this screenshot
shows Java Mission Control through Java
Mission Control as well it is possible
to collect heap dump again there are two
am beans a hot spot diagnostic and
diagnostic command using their
operations in this case dump heap
operation of hot hot spot diagnostic and
bean we can collect heap dumps so as I
mentioned there is a JVM option heap
dump on out of memory error which can be
used to collect heap dumps in case there
is a JVM fails with out of memory error
so here we can see that you know on the
on the left hand side from the GC logs
we can see that the application is
failing with GC overhead
limit exceeded out of memory error and
after that there it is it is creating
the heap dump so there are scenarios
where you know especially parallel
collector it can continuously put in its
efforts to collect you know or reclaim
some space and the in in Java heap even
though the returns of that are very
minimal so in such cases you know we
have to instruct the garbage collector
that don't you know don't put in so many
so much effort when the gains are are
not significant you should exit come out
of it and you know let the application
stop or let the if there is a if there
is a you know application server hosted
application let it restart on its own so
in such scenarios it can actually delay
the exit or restart of the application
and it delays the creation of heap dump
as well so so this delay can actually
have performance penalties for our
application so first for such scenarios
we can use two JVM options GC time limit
and GC heap free limit so GC time limit
sets an upper limit on the amount of
time that GCS can spend in percent of
total time and GC heap free limit Setser
lower limit on the amount of space that
should be free after garbage collections
so the default value of GC time limit is
98 percent on the default value of GC
heap free limit is two percent which
means if the garbage collections are
able to meet these two conditions it can
you know the garbage collections are
allowed to take up to 98 percent of the
time and if there is at least two
percent of the heap space free after
collections the garbage collections will
continue they they won't exit you know
there will be back-to-back frequent full
GCS so we can tune these two options we
can try to in
Cree's GC time limit and we can try to
increase GC heap a free limit so that we
don't have this this issue of
back-to-back frequent full G C's in
cases where the applique expect the
application to to exit rather than
lingering around there so an out of
memory error is thrown after a full GC
if the previous five consecutive GCS are
not able to meet the the value set by
these two JVM options so the next
diagnostic data is heap histogram he
pistol Graham's give quick glimpse of
objects in Java heap so you don't have
you don't have to follow two steps you
know collect heap dumps and then analyze
them using some tool so these are
textual you know output where you can
quickly see which objects are which kind
of objects are growing in heap heap now
this heap histograms can be collected
using the print class histogram J
command J map and there is a new tool J
HSD be added in jdk 9 which can be used
to collect heap dumps from code files or
nonresponsive processes as well and we
can use diagnostic commands in Java
Mission Control to collect heap
histograms let's see some examples here
we can see Java Mission Control in that
we can execute GC dot class histogram
operation
you know diagnostic command on the live
process to collect heap histogram and it
shows the heap histogram at the bottom
pane of Java Mission Control so once we
have collected data we have GC logs we
have heap terms we have histograms which
tools should be used to analyze that
data to see you know what what kind of
memory problem I'm encountering GC log
and GC logs analysis so in the GC logs
what do we want to look for we want to
see if there are too many Fuji sees if
are those G sees
having long pauses are those eces
occurring too frequently we can do
manual inspection of the GC logs are
there are automatic analysis tools
available which we can use to you know
understand what what's what's happening
what's wrong with my memory
configuration are why these GCS are
occurring too frequently or why they are
taking too much time so here this is a
GC viewer tool in this week if we look
closely there are GC pause you know to
space exhausted events there are two
collections which are full GCS and they
took around four seconds each you know
not each they took in total they took
around four seconds which is you know a
long pause for for response and
intensive intensive applications so in
such cases you know this is a this is a
GC log of G one collector and two space
exhausted GC me DC's mean that there is
not enough room available in the old
regions to promote objects from the
young regions so inside it's in such
scenarios we have to make sure you know
our old region's old generation and
young generation are sized appropriately
there is enough room available in the
old generation and in g1 there are mixed
collections
you know the collections in which some
of the old regions are collected along
with young regions so we have to make
sure those mixed GCS are running
effectively
if we look closely there is an option
you know there is at the bottom we can
see that initiating occupancy fraction
which is a value that indicates at which
fraction the concurrent marking cycle of
g1 collector should start which prepares
the old generation regions
so that they can be included in mixed
regions
you know if the mixed collections are
not not having old generation regions as
part of them the old data doesn't get
collected the only objects that have
been renewed that have been moved to all
generation don't get collected so here
we can see that the this occupancy
fraction is set at as 87 percent if we
reduce this percentage we would you know
start the the concurrent marking cycle
of g1 collector earlier which would
which would make sure that the old
regions are ready for mixed collections
and would leave enough room in the older
generation for promotion of promotions
of objects from young regions so next
thing to look for is do we have explicit
Fuji sees because explicit Fuji sees can
cause long pauses in our application and
they are easy to spot in the GC logs if
you look for a system strength in the GC
log entries that means that there are
explicit full GC is being invoked these
explicit full GC s can be invoked using
system dot GC call if you are using RM I
and you know the RM I applications at
certain intervals which is specified
with these properties invoke explicit
full GCS and if we need to we need to
make sure that the diagnostic tools we
are using to collect the diagnostic data
they are not you know unknowingly they
are not invoking full GCS in our
application for example if we have
option print class histogram in our
application enabled and if we send sick
break or secret signal to the
application it would invoke a full GC
before collecting the class histogram or
the object histogram so in such
scenarios we have to be aware
that we our application doesn't have
explicit full GCS getting invoked there
is an option disable explicit GC which
can you know completely disable the the
explicit GC invocation for our
applications so once we have heap dumps
collected how do we analyze heap dumps
there are you know plenty of tools
available for heap dump analysis until
JDK 8 we had J ha Java visual VM which
have been removed from JDK 9 but you
know the open source project of Java
visual VM is still available if you if
you wish to run your JDK 9 applications
you know if you wish to analyze heap
dumps from your JDK 9 applications you
can you could still use Java visual vm
and there is an excellent tool eclipse
mat which is a community developed
open-source tool and it has you know
amazing features which lets you explore
dig deep into your heap dumps and see
what might be consuming memory in your
heaps so here I have a heap dump open an
eclipse mat in this case we can see that
the instances of byte array are taking
up to four hundred and six megabytes and
we can in Eclipse matte we can go and
look at the instance instances of this
byte array and here we can see a pattern
that you know byte array of length one
zero two four are repeating there there
are too many objects of one zero two
four length byte array in eclipse mat we
can see the reference chains of these
objects to their GC roots which means
the roots in the JVM which are holding
on to these objects and which are not
letting them be collected by the garbage
collector so in this case we can see
that there is a vector long-lived object
in memory Li class which is actually
responsible for holding on to this these
byte array instances
so permanent generation metaphase memory
issues so in this case as well like we
did in Java heap with Java heap memory
related issues in this case also we need
to first confirm if there is a memory
leak monitored your permanent generation
or meta space usage over time and if
full G C's are not able to claim any
space from meta space or form John that
could mean that your permanent
generation or meta space is not sized
appropriately increase the size test
your application again and if it stills
you know after the application reaches a
steady state if it still keeps growing
in size its its occupancy still keeps
growing that would indicate that
probably there is a memory leak to
configure it is native but it is
collected by the garbage collector so
configure to configure our permanent
generation we can use two options
permanent generation
palm size and Max palm size to set the
initial and the maximum permanent
generation size a point to note here is
that permanent generation has been
removed in JDK 8 so we don't have how
many generations JDK 8 onwards there is
meta space where we store class metadata
information so to configure meta space
meta space size and max meta space size
JVM options if max meta space size is
not used then the JVM is you know free
to use all of the native memory
available to it because the meta space
unlike permanent generation is allocated
from native memory alright so we talked
about out of memory error or memory leak
situation with meta space and what what
is this
you know another space compressed class
space so compressed class space is
actually part of meta space which is a
you know logical region
it's not physically part of meta space
it's a logical it's a region which is
logically included into the meta space
and we have meta space you know meta
space having compressed class space if
the option use compressed class pointers
is enabled which is enabled if use
compressed oops is enable and which is
true for 64-bit platforms so by default
you would have used compressed class
pointers on on your 64-bit JVM and with
this option on this the 64-bit class
pointers are represented by 32-bit
offset so there is a you know separate
space a separate memory pool and the
class metadata in that memory pool can
be referenced by 32-bit offsets instead
of 64-bit pointers by default 1gb of
address space is reserved for the
compressed class space when you know the
JVM initializes and but this can be
configured using option compressed class
space size and an important point here
is that the the the option max meta
space size sets an upper limit on on the
committed sizes of both these spaces
meta space and compressed class space so
to understand this better if we don't
have compressed class pointers enable in
the Java heap we have you know objects
in objects we have these class pointers
so these class pointers on 64-bit
platforms are 64-bit 64-bit platforms
our 64-bit pointers and these 64-bit
pointers are then used to reference to
class metadata in meta space but when we
have used compressed class pointers
enabled these underscore class pointers
in Java objects in the Java heap are
actually 32 bit offset so it says you
know space in our Java heaps if we have
32-bit pointers instead of 64-bit so
these 32-bit offsets can be used to
reference to this class metadata in
pressed glass space and when we have
used compressed glass pointers on and if
we look at the GC log entry we can see
that there are you know two lines one
for marrow space and the another one is
for compressed class space and note here
that you know the committed size of meta
space is 4 8 6 4 K which includes the
committed size of compressed class space
so 512 K is not you know a separate
space it is included in 4 8 6 4 k for
meta space diagnostic data collection
and analysis for permanent generation
and meta space so we should be
collecting GC logs and those can be
collected with verbose class or trace
class loading trace class unloading
options we can collect it we should be
collecting data with J map - pom stat
for JDK is up to JDK 7 and J map - CL
start for JDK 8 onwards and he prompts
help in diagnosing or investigating
permanent generation on meta space
related issues as well and JDK 8 onwards
class statistics information can be
collected with J command GC dot class
underscore stats command so you know
while investigating the classes or class
loader related memory problems we should
we need to make sure that the classes
are getting unloaded make sure we you
know there the option - X no class GC is
not in use if it is there your classes
will not get unloaded
you know the JVM would keep loading
classes and none of the classes get
unloaded you will bloat the the meta
space or permanent generation usage and
ensure that CMS class unloading enabled
option is on when using CMS on Java 6 or
Java 7 -
sure that classes are getting unloaded
during concurrent marking cycles
otherwise they get unloaded only at full
GCS so here this is the logs collected
with - for both class and these logs are
important to tell us
you know if the classes are getting
loaded from correct package or jar files
and if they are getting unloaded when we
expect them to so in this specific GC
logs entry we can see that there is a
full GC and it is happening because meta
space has reached is high-water mark and
here we can see that you know there is
enough room available in meta space meta
space capacities at 6 GB but its usage
is only at 4 GB and it is still you know
throwing out of memory error if you look
closely the out of memory error being
thrown is for compressed class space not
for meta space so that that you know
separate region we talked about in
previous slides the compressed class
space which is part of meta space
doesn't have enough room to load more
classes so we should be using you know
compressed class space size option to
configure the size for compressed class
space we can see class loader statistics
collected with the palm stat the JDK is
you know up to JDK 7 which have a
permanent generation and JDK 8 onwards
we can use J map there are CL stats
option to collect class loader stressed
statistics so class loader statistics is
important because it gives us
information about the class loader
instances available and the classes
loaded by them whether these classes
class loaders are alive or dead in
memory or you know the the number of
classes loaded by them or the bytes
occupied by those classes in Metis P
permanent generation so heap dumps help
in this case as well we should be
looking for classes that should have
been unloaded and eclipse matt has a
very nice feature called duplicate
classes which can show us the duplicate
classes which have been loaded you know
by multiple instances of class loaders
here in this case you know this example
shows duplicate classes and here we can
see that the jacks be generated classes
there are multiple instances of those
classes which are loaded by different
class loader instances and in this case
the issue was you know we had the the
customer who reported this issue they
had different version of Jack's B
libraries on their class path and which
was causing class linking errors and
that that led to loading of multiple
classes by different class loaders so
you know duplicate classes can give us
clues where we should be looking for why
these classes are getting you know
loaded multiple number of times so out
of memory error due to finalization so
as we all know finalization is a feature
in Java where your class can have a
finalized method and instruct JVM that
you know don't collect my objects until
my finalized method has been executed so
even if you're even if the objects of
offers in a specific class which has a
finalized method are eligible for
collection they won't be collected until
finalizer thread has the has the chance
to to execute finalized method on those
instances so we have the single only
thread finalizer thread which you know
has the responsibility to to execute
finalized method on objects which are
pending finalization and if this single
thread is not able to keep up with the
width
Lord that has been added to its queue
you know if it lags behind then we will
have you know a list of objects which
are otherwise garbage and if if they
were collected we would have room
available in Java heap or you know
corresponding room in native memory but
they are still there because their
finalize method needs to be executed so
this this could waste a lot of memory
and good news is that finalization has
been deprecated in Java 9 and this is
the CR ID for that change which you know
with which it has been deprecated in
Java 9 and this bug report actually
talks about alternative solutions which
should be used rather than using
finalization so which diagnostic data
and tools can help us we can see objects
pending finalization using j console
using j map - finalizar info and we can
see you know information about finalized
the objects pending finalization using
heap dumps as well so here in J console
we can see that you know pending
finalization objects the count is 0 here
but if this count is high this means
that the objects which are pending
finalization are wasting our Java heap
memory and if those objects are holding
something in native memory if they have
allocated buffers in native memory then
those resources are also being held on
because of the objects pending
finalization we can see information
about finalized errs in eclipse matte as
well
by analyzing heap dumps so code cache is
full and compiler has been disabled so
there is a separate memory pool in the
JVM which which stores the compiled code
generated by JIT compilers and this
memory pool you know has a defined size
specified size and that it can also get
full and when
this memory pool becomes full you would
see this message that the code cache is
full and compiler has been disabled
though you won't see it's just a warning
your application won't exit you you
won't see any out of memory error but
there are disadvantages you know there
are performance penalties when when you
or when you see this message in your
application run so when when code cache
becomes full and emergency code cache
cleanup is invoked which actually
discards almost half of the compiled
code and if that compile code is
required soon after you know soon it is
discarded then the compiler has to work
again to compile those methods so you
would have performance degradation
because your your compile code is gone
though that compiled those methods will
run an interpreted mode now and the
compiler will take resources to
recompile those methods so the we should
make efforts to avoid this situation and
we should ensure that the code cache
size is sufficient enough to hold our
compiled code and there is an option
reserved code cache size using which we
can configure our code cache size so out
of memory error for native memory so
these two examples show that JVM is
running out of native memory so when we
when we see this error message this
means that JVM is not able to allocate
from native memory which is not managed
by the JVM not directly managed by the
JVM so this process our current process
or other processes on the system might
be consuming native memory and we can
make more room for you know native
memory by reducing the Java heap size
are reducing the permanent generation or
meta space size reducing the number of
active threads are there stack sizes are
reducing by terminating some of the
processes on our system which we don't
need at that moment and if the above
doesn't help we might be facing a native
memory leak for example you know j'ni
code might be j'ni or JVM ti coal might
be allocating native buffers and might
not be D allocating them as necessary
so which diagnostic data can help us to
troubleshoot native memory issues native
memory leaks you know two kinds of
native memory leaks it could be in the
JVM or it could be outside JVM you know
some code outside JVM might be might be
leaking native memory so for tracking
native memory leaks in the JVM we have a
tool called native memory tracker we
should use this tool to to understand
the usage of native memory by by the JVM
or we could use you know for for
tracking native memory leaks outside JVM
we need to you know rely on platform
native OS level tools for example pea
map to look at the process map to
understand where you know which parts of
the memory is consumed by you know which
allocations and results from native
memory leaks tools leak tools such as
Libya mam valgrind etc and of course
inspecting different segments of the
core file can also reveal information
about where what kind of native memory
allocations are causing Nitin and the
the observed native out of memory error
we can use a native memory - tracker
tool as I said to track native memory
allocations which are done by the JVM
and it cannot track memory allocated
outside the JVM or by native libraries
so to use nmt we can use option native
memory tracking and we need to you know
launch our applications with this option
on and
this option can be used at two levels
somebody and detail depending upon the
verbosity level of information we would
like to see and then we can use J
command to connect to the process and
extract native memory in usage
information using VM dot native
underscore memory command this is how
the output from VM dot native underscore
memory looks like this shows you know
the memory used internally by the JVM by
different components of the JVM by you
know Java heap threads compiler GC so
this gives us a clue you know which
which part of the JVM is consuming more
memory or is you know growing in usage
of memory and we can collect a baseline
output using nmt and that baseline can
be compared against you know outputs
collected at different stages of
application run and see which parts are
growing in memory usage so for native
memory leaks outside JVM we need to use
a native platform native memory leak
detection tools for example we can use
dbx laboum m valgrind purify and so on
so to summarize causes of memory related
problems could be you know miss
configuration of memory pools excessive
use of finalized errs like explicit GC
invocations or memory leaks and we the
first step to understand or to
troubleshoot memory related problems is
to size our memory pools appropriately
and the tools that we can use to
troubleshoot memory problems are heap
dump on out of memory error and print
class histogram options J console J
command J map GC logs Eclipse mat and MT
are native memory detection tools
so these are some references you can you
know look at refer to troubleshooting
guides here I provided links for JDK 9
and JDK 8 troubleshooting guides I have
a blog where i share my troubleshooting
experiences you know quite often and
there is a free ongoing online course
for troubleshooting memory issues in
java applications the enrollments are
still open until october 11 so you could
enroll into that course and you know
learn more about how to troubleshoot
memory issues and there's an article on
troubleshooting Java memory issues on
info cube the link is given here so
that's all I had to share thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>