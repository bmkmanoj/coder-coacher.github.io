<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Future of JCache | Coder Coacher - Coaching Coders</title><meta content="The Future of JCache - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Future of JCache</b></h2><h5 class="post__date">2016-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yLA6ks_in3s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">one two three starts art welcome to Java
one welcome to java 1 i'm here at the
night hacking booth at the Java hop at
the Java one 2016 in San Francisco and
that my name is Victor gamma i work with
hazel cast and with me today Greg lock
his co-leader of je cache standard and
the creator of each cash maybe you heard
about this libraries and using stuff and
today we're going to talk about the
future of je cache so je cache was
released in 2013 right right first
version finally and right now we have a
huge adoption of je cache like how many
implementation of je cache we have a
net10 implementation to check cash all
right so we have a 10 implementation je
cache and today we're going to talk
about the future so what we're going to
expect from the je cache 1.1 or probably
2.0 mm-hmm okay so can you um can you
talk about how you know you become the
je cache choline and you know how it
started for you like a JK so before we
talking about the future we're going to
talk about fast a little bit like I say
say say years ago I used to work at
thoughtworks and we got sent out for six
weeks short gig at what if calm and they
had a very successful business model but
the website was down about two or three
hours a day so that get 10 or 15,000
concurrent users would come in and then
they would search for inventory and the
what-if model which is still unique no
one has actually done this throughout
all of trouble is have a single web page
result yet comes back so you search for
Sydney you get 300 hotels come back with
all the inventory all the information on
one page it's about three megabytes of
data imagine that and Sydney was the
most requested page for example you get
5,000 of those per second that it
connected to sequel server enterprise
and a traditional model with stored
procedures and so on and didn't work
didn't scale and this was you know early
on when that standard enterprise
architectures that they didn't scale was
actually not well understood by anybody
so so we went in thinking oh yeah we'll
know
of them smart guys will figure something
out and we realized we then realized wow
this is actually a serious problem so
our starting point was to was to try and
simplify use of the database and we went
with hibernate and kind of hibernate
really is about speed of development not
the only speed of execution use you
thought it was asking for trouble it was
also very very early days in our RM yeah
in fact we started with an internal
thought works library that was just
being handed around yeah and then I
chose hibernate and we got it working
and then we went into production and in
10 seconds it failed we had to roll back
fail fast this is like a very good
concept so I actually my partner was out
of Murdock who's the CTO of Gradle just
over there so he actually gone on
vacation for two weeks he left on Friday
the goal of was on Monday so it was all
up to me I didn't really understand
concurrency very well at the time so we
so i spent about a week scratching my
head and finally figured out that there
was a bug in apache JCS the casing uh oh
gee so i created patch for it we patched
our version we got into production and
you know we ended up being it for
thought works we're at water for about
five or six years ended up exiting as a
billion dollar company so and the thing
that's spun out of that was I started
with one patch to JCS and then I told
the hibernate mulling list about it and
like yeah you should patch your JCS
before you go into production and then
Gavin King heard about it was all upset
because this was his baby he was trying
to make it successful and he said I the
guy from JCS it was summer in the US and
the guy was on vacation or not available
not responsive yeah so um he's like why
don't you fork JCS and I'm like that's a
big step and you know in the end he
convinced me and I said I cold off I
 JCS will you include that as one of
the cases in the hibernate distribution
he said yes they end up forking it at a
Murdock actually came up with the name
of eh cash and we worked on it together
for a while then I kept maintaining it
and so so this went along for a number
of years eh cash became successful as
you
I yeah what if became successful yeah
eventually like concurrency how do we
the Holy a learning concurrence in hard
way yeah and and so then i was i got
paid i got paid by a large Silicon
Valley company i think it's NDA so I
still won't say who it was but they pay
for some consulting around the edge cash
and one of the things they paid for what
they wanted was they wanted me to
implement je cache so Jay cash had been
created back in what are we 2016 so it
must have been about 2003 a guy from
Oracle proposed the casing spec mm-hmm
and I think what they had more in mind
was something closer to what a khmer is
like with edge side caching yep but done
sort of like an inner in a job a
friendly way anyway it kind of
languished for a while it got partly
done I tried implementing it which was a
disaster because yes it was incomplete
couldn't really implement anything but I
ever mattered what I could and then as
part of that I joined the expert group
and then then I was kind of like you
know somebody should really finish this
and so I met Cameron Purdy from Oracle
and he's like yeah well I want to finish
it I've just been too busy I'm like well
you know let's try so I became co
speckly it what year it was 2007 yeah so
I just want to like emphasize that it's
like Jay cash like just I want and
selling this one of the longest-running
the standards it's actually the longest
running it's the longest running jsr
that ever successfully completed this is
good point it was finally successfully
completed out I've actually changed the
rules and you get a if you go what's the
what's the word like vacant it's like
you buy everything slides down there's
no progress being made then I think it's
once every two years under the current
process you actually there's a vote
basically say you know you go to this
one year this year's hey Heather one
years every one year if you go if you go
if you become inactive
then the the JCP will proactively
contact the spec leads and then usually
what they say is I yeah yeah we're still
we're still going still going but then
then it comes to a vote and then we have
a vote now I ironically is because i'm
on the JCP executive committee
ironically i'm actually a real hard-ass
and other people that have gone inactive
so i always vote I always vote to kill
the jsr oh wow yeah cuz that caused the
end of the day like if everybody's just
nice everybody just says okay yeah vote
vote vote like nobody gets a wake-up
call yeah if myself and maybe a couple
of other people vote no and always put
my reasons then it's like a wake-up call
and it makes the spec Lee to either get
going I really actually think hard about
it yeah so somebody should have done
that somebody should have done that to
107 like many years before so so you
know so basically I'd partly implemented
it and because I'd been paid to do this
because like nobody if you weren't being
paid or will he get deleted two to Rea
the standards ladies are sinners so I
hear I kind of knew like some
fundamental problems with it but
actually doing the spec ended up taking
about two years of about fifty percent
of my time and there was also a
counterpart over at Oracle does guy
called unesco's monopolist and then
after him brian oliver and it was
enormously time consuming so as you said
we got it out in 2013 yep at this point
of all the in-memory data grids and
distributed case is out there that are
that are well-known everybody's
implemented it except for IBM extreme
scale and pivotal gemfire now the IBM
guys did actually say that they have a
plan to implement it later this year so
that'll just leave the pivotal guys who
still despite constant requests from
their customers yeah how are still
saying that they're not going to
implement so the thing is we're from
what I like about the Jacobs pack itself
it's actually doesn't force you to you
know to implement it for like in process
cash or distributed cache so it's
actually like very you know good
possibility for moving different parts
of the application
like four from in-process cash into the
distributed cache for example just
switching the jars and because of the
tck and all this implementation need to
pass the tck and everything that
including like the consistency logic
that explains the standard needs to be
complied with and this is kind of very
nice thing so yesterday we had the ball
forge a cash-in in Joey and we have the
the people from the payara which is a
commercial implementation of glassfish
told me tribe from a commercial
implementation of the tomcat base Joey
server and the multiple things that
people already doing this is you know
implementing the extension for CDI for
example so they can intercept different
calls and inject the cached values for
certain places which is like very nice
way how they can like a cheap speed up
the application performance like very
cheap just you know cash everything so
and yeah this is one direction like that
actually doesn't doesn't directly depend
on what's happening with Jay cash right
now so this standard is there by a bunch
of implementation so the Java you guys
needs to form the Alliance to bring this
yeah yeah yeah I mean like when we did
that the spec was a little bit awkward
because because like you said pooja
annotations per joe annotations are
actually really really useful yeah now
that that requires a dependency
injection container which is above and
beyond the JDK yeah so so the the j cash
spec is a little bit unusual in that it
only depends on the jdk however if you
want to use if you want to use
annotations then you need you need you
need an annotations processing engine
which you get from CDI or you get from
spring or from juice or something yeah
so what we did in the spec was in the
reference implementation so another
reference implementation the tck tck we
actually created test integrations with
CDI with juice and also with spring to
make sure that it would all work and so
it really shouldn't be that difficult to
take that code and actually
use that with an existing e container
the spring guys already already added
the JK annotations back in spring for
dot one so I don't know that this is
this is what from our point of view
we're done yeah go and we've defined the
annotations the annotations had a lot of
thought put into them the the guys from
Spring and Eric Dale quest to as the guy
originally created the spring ehk
annotations library all those Rick
Hightower all these guys were heavily
involved and borrow a couple of bugs
it's done and the spring guys have
adopted does yes so it really just needs
somebody on the eee side to to go ahead
and and use those annotations and then
glue it into glue it into JK yeah so
they problem with with GE later said its
many vendors already adopted this the
problem is that there is no like a
standard things like the new standard
like jsr and tck that can be integrated
with other component as pyara has pie
are already added the young okay well
thing is you know what i think is the
best way to go everybody should just add
the add the annotations in proprietary
way along so the annotations are
standard JK shiz standard the binding
why don't they just do that in a
proprietary way for each observer and
then that then once everybody's done it
that then creates extra pressure on the
e community to actually standardize it
yeah yeah so this is exactly what
happens because the payara has it Tommy
tribe has it did I really yes that they
using the Apache what the library said
look Jessie s yeah jesse s this is still
still using this still around yeah yeah
so so we'll see how how would it go show
the and this in this like yesterday Bob
they try to like throw some ideas there
is a there is a adopt GSR project that
has j cash for Joey sub-project and a
bunch of a bunch of ideas or like a
ticket open there like how to integrate
je cache with GTA like so the
transaction can be propagated through
the caching effect i well in terms of in
terms of je cache with JT I that was
originally part of the skype for one
point I mm-hm if anybody's interested in
that we actually have an entire chapter
of
specification for how that should work
and also the there's not much in the API
it's just a couple of methods but we
actually did all that so we can actually
I can just get it back and the email to
somebody yeah if you let me know who
that is yeah so well that will the
permission doesn't need to be reinvented
we did and once again around
transactions I work with my colleague at
the time from terra cotta ludovic or van
who who created the patron extrends
action manager and we also work with
brian oliver so we actually did a lot of
work on it and we're really happy with
it and and then we just the we were
going to make it part of the spec as an
optional part but the JCP said that the
reference implementation needed to
provide of transactions implementation
yeah and will like our man nobody wants
in another three years what another
three years work I'm kidding well you
know anyway nobody needs to reinvent
that wheel because yes did it yeah so
okay so now 2016 we have a 10
implementations both in process caching
and distributed caching solutions so now
what is current state you know in terms
of standard so I know like you're
working right now to you know to
releasing the 1.1 so and what's going to
be in the 1.1 and what's going to be
next like after what are you going to do
after 1.1 so so we did dual you know we
did a lot of work on one point oh and
there's actually a very very small bug
count mm-hmm and there's something you
know in terms of terms of where the bugs
are there's just you know some things
like a catch and a grammar error in the
spec or you know or you know the spec
will say something wrong different to
the Javadoc there's there was actually
there was actually a concurrency issue
in the completion listener which brian
oliver right by the way I had I had
Peter Peter venture was involved in the
fix for that but but by and large the
reason like this tend upon 1010
implementations some of those
implementations of raising
issues right and then other people other
people like other people have been kind
of very you know very picky but I don't
know this so one dot one the bugs came
in pretty slowly a lot of a lot of the
so-called bugs were people that really
just had questions about why was it done
that way so so the reason it's taken
kind of a period of time to put it out
is because there was no urgency to
really to really do it so it was finally
like well written at the end yeah it
took time if what's all written when it
was one 11.0 no yeah yeah so so so 1.1
the way it is right now it's kind of
pretty much ready to be released so
they're still I would like to get it out
in the next month or two like I kind of
would have liked to have gotten out by
Java one but i just said there's no
there's no super pressing issues yeah
one of the problems that you have is
when I so I'm actually working on this
myself and then when I go and do some
work on it that the problem we've always
had with the spec is that it seems like
a simple thing but there's so many
different points of view so you're going
work on something and then people wake
up from their slumber haven't heard them
in six months I see that some work got
done and then they're like oh no I want
it like this and and there's actually
some issues when I when I release one
not one that wheat some issues that are
open because I close them and the the
reporter just reopens you have this open
closed crap going on so I just put put a
flag on it same clothes right little
color code same clothes yeah but still
like an open discussion meet me but
there's no there's no like real bug all
right so yeah so one dot one wonder one
should should come out soon as I said
well the so we'll we'll put a
maintenance request through one of the
things that held it up was last year I
wanted to add asynchronous operations
and I wanted to find those and then you
know had a couple of month argument with
everybody saying oh no you can't add
things to you can't add you can't add
one interface yeah technically you can
if you look at the definition of binary
compatibility but it does seem to be
against
the spirit of the spirit of things so so
in the end I back down in terms of but
some some idea so you know one dot ones
should come out in the next few months
and then I expect that every that
everybody that's implemented 10 will
fairly quickly over the neck you know in
their next release cycle so maybe three
or six months they'll move to one dot
one from the point of view of end users
basically no different because it's
really just an errata release yeah and
then so yeah what about like next next
thing so you talk about like a sink
methods and because of right now like
the the concept of asking programming or
reactive programming you know becoming
like more and more just discussed topic
and the people talking about the things
and from perspective of the support of
the library javed provides a complete
able future that allows or enables the
kind of reactive style of programming so
what do you think about this end do you
think it is makes sense to have the
support on the yeah yeah I really I
can't really felt I really felt like we
kind of missed it not adding I sink in
because because most much distribute
implementations including hazel cast and
an terracotta actually have have I have
a sink methods you know because
depending on the nature of the data if
the data is if it's okay to lose the
data like it's true cache data yeah then
I think it's basically okay yeah you
want to just go as fast as you can yeah
but in this case you need to sort of
refine the consistency guarantees and
standards so right now he has a happens
before type of semantics so if one
operation executed so the neck
separation will see the result of
previous operation stuff like that so in
this in this I think you need to refine
the Houthis visibility and all this like
happens before going to happen so and it
might be different from perspective of
in process caches and distributed cache
so so the couple things yet we did on we
did actually complete the before I
abandon it so it's actually there in
github we did complete the eye
p I changes including the including the
contracts on consistency can't remember
exactly what they were right now but I'm
still in DC key somewhere because the DC
Kate sort of like a checks the
consistency of ever method that's in
that at the moment with synchronous
methods yes of a synchronous or
asynchronous methods the consistency is
very very clear it's strong consistency
yeah now the spec says it's strong
consistency it also says the
implementations may provide further
consistency models but we're obviously
with async you can't get strong
consistency yes and also if you've got
in process it might be possible but and
distribute implementation you also can't
provide ordering guarantees yes so but
but still I think I think because you
know the the flu p I think asynchronous
operations always include batching
there's actually the batching that gives
you the performance improvement because
you know you're just doing one side like
a back pressure jungle thing right when
you do it on the client or in on the
server you know before you send him back
yep there's an interesting algorithm
with back pressure by the way so what we
discovered at hazel cast and brian
oliver said that they were using the
same number and coherence it was every i
think was every tenth tenth 100th
operation you you actually do a flush
you actually flush the q it's more like
a heuristic or like to heuristic yeah so
when we at hazel cast when we added the
async methods we were actually tested we
actually tested ten a hundred a thousand
ten thousand i think i think it was a
hundred that we settled on and so does
so does coherence so that so that gives
you most of the network savings while at
the same time giving you sort of that
there's basically very very little it's
like an asymptotic function yes there's
almost no performance improvement on
actually going either this like over
that batch number exactly that actually
the nice thing about that number that
heuristic is it actually keeps the keeps
the buffers actually very small so it
doesn't create any memory issues or
anything right and so once you get to
100 then that
and then you then it causes a flush
which then naturally applies back
pressure yeah yeah so now so the
giggling yeah if I see I think would be
really nice that you had it and then
that would work well with reactive and
all the other stuff going on yeah the
other thing on the flip side the flip
side would be nice to probably do
transactions I think there's enough
appetite out there to do transactions
pretty much all of the distributed I
mdgs have got transactions said it'd be
nice to standardize that area what about
again we know the caching it's it's not
data store so it's caching it's a
temporal data but many people were
asking about how about like a getting
data not only by key but also by certain
properties that you store inside the
cash that sort of like a query type of
language is it like do you think is it
makes sense to have it in the in the
cache or the cash should be you know
solely temporal key value once again I
mean the the approach that we've taken
was to try and create a useful standards
around what already existed yeah so you
can take an empirical approach and just
say okay well if everybody's implemented
something and it's a well-used feature
why don't we standardize it no now query
does pass that test query or predicate
however the where the problem comes in
is what predicate language to use
because there's not a pre-existing jsr
that we can just import and so we're
going to use this one or maybe something
like g PQ el from from JP a that defines
like the object query type of language
that which is not like the sequel or not
direct translation too sick or it was
designed to be a direct translation from
the object model into the sequel model
so potentially j PQ el this is kind of
thing that maybe make sense so you have
a sort of GP q el statement where you
can query q way to store I don't know
this is kind of some of the ideas that I
heard from the from the way we don't
want to do is we don't want to define a
caching predicate like right yet exactly
because
it's just that's just ridiculous because
you're not making the world better
you're making the world worse you know
yeah yeah so it's it's still like there
are some some questions open and I guess
this is like right to we are or the Java
one and we talking to the developers
we're talking to the people who like
vendors and this way like it's it's good
it's good time to you know to have a
discontent with noticing some nice
things that have happened you know
spring adopted it recently hibernate was
at 5.2 yeah 5.2 actually added JK Shin
as a provider spring guys like spring
boot have got JK she's a provider so so
over time so we've got the
implementations and then what's really
really nice is that the whole point was
so that framework says other frameworks
could actually use yes and of course the
other the other benefit is friend using
companies so they don't have to have a
dependency on a specific implementation
and like you said you can very very
easily switch from an in-process to a
distributed yeah without a lot of rework
yeah one thing that one thing that we've
learnt out of all this is that in in
your true in-memory data grid use cases
caching itself is is only a minority of
what people do with in-memory degree yes
so an Andrew processor is another go we
added it more like a we added anything
processor which which was like an odd in
that direction yeah because that's the
most widely used in memory data grid
feature but um I would say that that in
terms of adoption in industry like
financial services which is the heaviest
user of in-memory data grid are also
probably the least adopters of Jay cash
yeah and it's because they their usage
is more complex exactly like in hazel
cast the the cash API is maybe five or
ten percent of the whole API and and the
thing is it's kind of hardly worth
messing around with with Jay cash if
you've already got a proprietary
dependency so so but I don't think I
don't I can't really see anybody
kind of going out I mean we could take
JK Shin extend it to more imdg but
there's you get a lot more variability
on approach once you move outside of
caching yes you just another think what
you just said about the financial
companies in another thing that just
struck to my mind that the microservices
world and I know you're gonna have you
going to join the panel and the night
hacking about microservices so just to
quickly like we're also like running out
of time already which is like which I
don't claim to be an expert on by the
way yeah but still do you think the J
cash make sense in things like like
micro profile which the defenders of
enterprise application servers proposing
right now this like new the micro
profile for like building and deploying
microservices or web application using
language services do you think like a.j
cash has some potential in this area
like the for example like help to
integrate different parts of the system
or you know the cash storage can be used
as a backbone for for referring to
changing the data between them well you
know as I said another marker service
expert but my perspective I mean what
was interesting actually at what if calm
we started with the monolithic
application and we realized that that
was a big part of the problem so yeah we
ended up how much time we've got left I
were already over I can wrap it up but
um we we broke a monolith into about
fifteen services yes microservices so we
had one what gift was this is like long
time ago two thousand ish this was a
thousand and we started the process of
breaking it apart and like 2006 or
something yeah thrilled about 2009 and
so the idea worked very very well but
the infrastructure was much more complex
yeah but anyway each microservices each
microservice is a basically a full stack
application so the thing that you have
to ask yourself is okay do I need
caching and generally write unique ashi
in the monolith then once you decompose
micro services do you some of those
maybe not all of them but some of them
will need caching and what so so hazel
cast is very popular for for micro
services and you know why not use J case
there but the key thing is isolation so
the key thing to remember is that you
want a separate cluster / microservice
right so you've got true true isolation
but yeah I think the micro profile guys
look is going to be too slow like but
you know what are they what are they at
j case there's a lot of demand for it
exactly yeah so this is why this way
also interesting in a dingy cash so
we're almost like will ready to our time
so thanks for your time for talking
about the j cash thank you and stay on
the nine hacking channel it's going to
be any other very awesome discussions
and going to be like it tomorrow panel
about the micro services this afternoon
oh this afternoon so they stay tuned if
you want to learn about like
microservices in Joey world thanks again
Greg luck from from hazel cast and
Victor gamma from hazel cast and also
we've got Cole lead of Jacob's back
thank you very much guys and stay tuned
with the new new stuff thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>