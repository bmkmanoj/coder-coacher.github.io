<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java at Speed: Getting the Most Out of Modern Hardware | Coder Coacher - Coaching Coders</title><meta content="Java at Speed: Getting the Most Out of Modern Hardware - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java at Speed: Getting the Most Out of Modern Hardware</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PIC3naLrdys" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">just before we start as a background
thing that way I won't burn some of the
tight 45 minutes we have for lots and
lots of slides anybody recognize this
picture
okay so for those of you and know that
what that is cool
anybody that hasn't seen it before when
I guess what that is this is this is a
documentation of the ultimate in duct
tape engineering this is the Apollo 13
co2 scrubber or you literally have a
square peg that needed to fit on top of
a round hole with duct tape and
engineers got to do that from hundreds
of thousands of miles away to save
people's lives which is amazing it's
like a truly heroic act and fact that
they got to get this stuff working
together and figured it out
with the little things that that on a
spacecraft was great but I like to use
it as an example of both a truly heroic
act the kind of engineering we would all
be proud to do and if you get to do
something that cool once in a lifetime
you it's amazing but it's also something
to learn from anytime you see one of
these things the main thing you should
be thinking is my job is to make it so
that's never needed duct tape
Engineering is really cool it's a great
skill if you can use it to save the day
but it's mostly something that should be
avoided and whenever you look at the
daily things that you do that amount to
putting duct tape on things to make the
problem go away just keep that in mind
it's a it's a useful thing to try to
avoid okay I'm gonna start with the
actual site that we're going to talk
about java at speed focusing on modern
hardware what modern hardware seems to
be and and how we get the most out of it
this will by definition be an outdated
talk in a few years because modern
hardware will not be modern back then
but I will try and focus on some of the
cutting-edge hardware you could get your
hands on today as a high-level agenda
we're going to do some intro motivation
talk about hardware transit features
talk about some compiler stuff because
to get actual usefulness compilers have
to do their job and we'll start with
basics and build up from that we'll take
a little detour into a micro benchmark
example and hopefully well have time for
that then some more compiler stuff a
little more cool compiler stuff the kind
of things just get to just get to do
that it's really cool and then talk a
little bit about other problems on
modern hardware and modern compilers
like warm up and then putting it all
together and maybe a little bit of
bragging towards then I'll try and keep
the bragging down until then but at the
end I will break I'm Gill tena and the
CTO versus Oh systems we make JVMs we
think that the best JVM s-- on earth
I've been working on things like garbage
collection for about fourteen years
thirteen years
this is documentation of me actually
doing that that that was a trash
compactor in my kitchen it was broken
trash compactors are supposed to do
minor garbage collections during a week
so the full GC only happens once a week
that's what we do with Java all the time
anyway here it was broken the
defragmentation function wasn't working
right you can see the fragments falling
out the back I had to fix it and this
was taken in 2004 was just thirteen
years ago so I really need new pictures
I've built a lot of stuff which means
I've made a lot of mistakes and I
learned from some of them and I try to
kind of you know tell other people about
the mistakes maybe they'll learn about
them too and as all we make JVMs that
focus on speed focus on latency focusing
focus on consistency so we we know about
it some about it and and I also you know
as a side subject like to talk about
things like how people measure things
and response time and latency so you can
find talks of mine probably online about
rents on that subject which this is not
this is about speed it's a speed what is
it good for absolutely nothing
say it again
but but what is it good for when we say
speak what do we mean so you know how we
fast are we slow that's a valid question
but it has to have some context so for
example when are we fast are we fast at
night when nobody's using this or when
it's important are we fast for example
when we're rolling out new code you know
20 times a day are we still fast while
that's happening or maybe we slow down
and we do go dead dead slow for 15
minutes then everybody's happy later
what about peak times we fast when it
really matters when when when it's Black
Friday or when a market opens or you
know when things really matter when slow
and fast are important not just on
average and you know when you actually
do something like trading for example or
or or buying something online
are you fast at that point in time the
other question is are you reliably fast
so fast is really good but sometimes you
can be fast but very unreliable and the
results right now that's kind of an
important thing to keep in mind really
really fast could be dangerous
really really fast and then completely
stalled for a minute is usually not a
really good thing so we'd like speed to
look like this this is how we think of
speed when we say how fast are you we'd
like to have an answer for that question
but that's not the right question
because no reality looks like that in
Java the reality tends to look like this
speed is all over the place you start
slow you go fast then you go slow then
you go fast and go slowly and fast and
understanding what this fast slow
transitions are and what they made up of
is one of the things we'll try and talk
about today I'll talk about the two or
three main things that affect it and
then we'll drive into cool things about
speed
so speed in the Java world looks like
that because for example the code we
actually execute goes through an
evolution we start off with slow
interpreted code were literally
interpreting wine byte code
from the classes and loading them and
it's really slow then once that's warm
enough the JVM decides oh maybe I should
do something about it it compiles it but
not yet fully optimized it compiles it
with profiling code so you could learn
what's going on that's this green zone
it is much faster than interpreted but
not nearly as fast as you could go and
once you figure out what the code
actually does then you throw an actual
optimizing JIT compiler at it and make
it go fast and that's that blue code
where you hope most of your stuff is
actually running and over time you'll
stabilize where most of your stuff or
hot stuff will actually be running good
blue code but you don't start there so
what does that mean when you look at the
behavior speed over time response time
over time response time over time looks
like this because we're running this
really slow code when we start we're
really slow then we start replacing that
code with faster code but not still
still not really really fast just better
than interpret it so green stuff and
then eventually that blue stuff will
dominate and will go fast those red
spikes in the middle are other things
that make us slow like pausing for
garbage collection pausing for the
optimization pausing for whatever does
happen all the time as well if we look
at this as speed rather than response
time
it's basically an inverse and it looks
like that you're really slow when you
start that is how slow you are we're
talking about a 30 X ratio not about a
you know it's nice effects kind of stuff
then you get faster but really not
really that fast and then you get to go
really really fast eventually but you
have these drops to an absolute zero
speed all the time because you pause
that's the behavior over time for speed
in Java applications almost invariably
so that's just something to keep in mind
we'll get back to the picture but you
know when we look at the parts and what
we can affect which part of speed we can
affect the different technologies and
different capabilities for each of them
so let's look at what modern servers are
what is it that we're running on so
machines were running on
this is a mostly intel servers and
talking about the server space and these
are the last end generations of intel
server chips we're right now right
around here skylake SP is out Brad Wells
Brownlee has been around for a year and
a half if you're running on a modern
machine that was recently bought or
you're running on the cloud on EWS or
Google or Azure or one of the others
you're probably running on one of those
two and if you're not you really should
move to one of those two why why are you
paying for the old hardware especially
in the cloud in what you have there on
the right is some interesting stuff
beyond just more cores and more cache
and that kind of stuff that you can
expect there are new features new CPUs
have had new features come into them
over the deck of the last several
generations and those features can be
used for speed for extreme speed if
they're used right if the mission if the
code running on them actually
understands those features there and can
be used and I'll give you some examples
of those when we look at features
vectorization AVX avx2
and avx-512 are really cool
vectorization capabilities that evolved
over the generations recently BMI and
BMI two are Bittman emulation
instructions including things like
scattering gathering bits that are kind
of cool and hle in TSX our Hardware
transactional memory features that have
come and now are commodity every new CPU
has them on a server using those
features can give a speed beyond the
features themselves the CPUs are just
getting better they were already really
really good and they just keep getting
and better and better you could see some
trends here how many instructions out of
order instructions the CPU can keep
juggling at the same time without them
yet completing is growing it's also it's
already been very very impressive but
we're keep going up and up and a CPU
these days it's not this simple serial
thing that's doing an instruction at a
time or four of those or eight of those
interleaved it's literally throwing up
200 instructions in air and catching
them and then when they're done and and
it's keeping the Chow going out
not going it's got what do we have there
like 96 or 72 different loads
outstanding in 56 different stores
outstanding none completed out of order
going to the memory just to keep the
pipes full and what that means is the
mental model of what's going on in the
CPU is hard to to project because you
may think that this happens before that
but the CPU is gonna do anything it can
to pick up work and do it out of order
just to get the work done and that's
where it's so big is a window that human
capacity to order well to it is almost
non-existent even compilers have a hard
time doing it well and you need to think
in terms of fundamentals of operations
rather than how to micro optimize to CPU
here are some nice pictures and how this
stuff breaks down from a Architecture
point of view and some of the evolutions
of them so for example this is sort of
the front end of the data stuff we've
got things coming in we've got
schedulers going into the different
operations and in a cache and the rest
going on but we can see in evolution for
example from a semi bridge which is a
few generations back to Haswell where we
just have more bandwidth the width of
the buses are bigger the units the
number of operations to the cache grows
so we get more bandwidth in there if you
look at the actual execution units so
then the helm is the oldest on the top
on the bottom Sandy Bridge to has well
we get more execution units that could
do things at the same time they're wider
they could do wider operations CPUs do
get better over time and the caches
themselves have looked like this for
quite a while where each core had about
32k of exactly 30 TK of data and
instruction caches 256 K of l2 and a
shared cache across all the cores
together in a socket so the more cores
you have the more cash you had below
that skylake SP which is the latest
generation you can get that right now
you can run on Google Cloud engine and
run them
has changed that model somewhat the
shifts of caches have gone around
another huge improvement that has
happened fairly recently with the
Haswell generation is an increase in the
TLB cache the TLB cache is a CPU cache
that keeps mappings around and up to
Haswell the number of non-small page
maps were very small in the low tens but
with as well and broad well and skylake
we now have thousands of entries to
cache two megabyte page mappings which
means that the concurrently cached
mappings for virtual memory as have gone
up by multiple orders of magnitude so
finally we can actually access a large
heap and data and not thrash the TLB and
misson in all time that's a huge step
between Ivy Bridge and Haswell and it
hasn't been talked about a lot but we
can see it in results especially on
large datasets
looking at the apology just how do these
come together most of the servers you
will end up running on look like this
two sockets with multiple cores on each
and memory controllers on each
interconnected between the two sockets
with some high-speed qpi or other kind
of interconnect each of the sockets has
cores and cash on them if we look at an
example of a specific one you look at
the cores they're sitting in there and
the socket on this high-speed ring the
caches are localized to each coin in a
distributed common cache around the ring
that's a nice default you to think of if
you think of a socket has a bunch of
course shared key on the ring well
that's no longer a valid model because
that used to be good after certain
number of course but we just can you
know fit that many cores on a ring
anymore so now we're looking like this
they're multiple rings on a single
socket to get from one socket to the l3
cache on the same socket you might be
crossing a harp and doing all kinds of
other thing in fact you can configure
this today with configurations that say
treat this is one symmetric flat socket
or was something called cluster and I
that says this is like two separate
sockets a new mono Darren in window
they're half the cache each but we don't
cross this interconnect between them
because it's higher latency you can
configure that at the bias level should
you probably not if you really
understand if you don't really
understand it well but four people are
fine-tuning latency performance want to
get every every ounce of performance out
of it these are things to keep in mind
now the other thing to keep in mind is
this is a full socket it's got all this
all the cores but you have options you
can buy eight eight cores 12 cores 14 16
18 24 sometimes you buy this you buy one
and a half rings like it's not actually
that all the cars are built equal the
ones that are built equal the one are
certain sizes the ones in the middle
often have that kind of happen too now
with skylake this picture with the ring
also changed dramatically you can see
the previous architectures on the Left
skylake shifted to a mesh architecture
on the chip it's sort of a 2d matrix
where rather than a ring you kind of go
down a matrix of interconnects that look
like that and in and that's kind of an
interesting level and you can sort of
see what what a small tent core thing
looks like in a larger 18 core thing
looks like down there these are all
stuff you can find from you know all
kinds of articles about micro
architecture for skylake and Broadwell
and things online so that's a little
context and the stuff we're running on
now let's go a little lower and if you
like machine code you're gonna really
enjoy this if you don't like looking at
machine code take a deep breath this
won't take very long
okay promise won't hurt I like to look
at the actual instructions we asked the
CPUs to do and play with them and see
what's going on because we have all
these different CPUs the really cool
thing about a JIT is it can choose to
optimize for the CPU it's actually
running on there wasn't binary code that
was shipped beforehand so dedicated JVM
it can run on older hardware newer newer
newer hardware and it could generate
different instructions for each so let's
see what actually goes on when we do
that so when i zoom into machine code I
use a feature of zing that's that I
think is really cool it's kind of built
in
profiler you can basically if you put
your code in hot loops like here I've
got several benchmarks and loops I can
pop up a web-based screen and see how
much time I'm spending in each I can
click on one of those and see the actual
machine instructions look like that
that's very readable right
I could go figure out what each
instruction does and how that maps to my
code it's kind of hard to do
here's code this is Java code this is an
actual loop and a benchmark that you
know we're summing an array just a
simple sum everything in an array and
here's the code that that generates now
this in itself is not very readable I
don't try to read that a really cool
reason that I use a profiler to read
code is that the profile also tells me
where I'm spending time and you see
those percents on the side that's the
code I care about
it's the code that actually takes time
since the the little percents on the
site tell me I'm executing that code now
I know that's the actual loop everything
else is crud I don't really need to read
it it's entering the loop exiting the
loop doing with exceptional conditions
that's not important that's the loop and
what we can see here this is on a
relatively old Westmere processor with
128-bit vectorized SOC and what we can
see here is that that loop doesn't
translate to let's do one integer at a
time but we have these sorry we have
these vectorize instructions that
basically say read 128 bits that's 4
integers into one register 4 integers
into another register that's eight
different integers into registers then
do vector ads of those and loop around
eventually they'll add the vector adds
together so it's a nice vectorization
optimization by the way most je viens
will do this hotspot will do this rjv
and will do this this is less where it's
been around for a long time right simple
that's exactly what vectors are good for
loops on arrays and things like that
right but if we look at exactly the same
thing than your hardware here's a broad
wall mesh
it's got vector instructions that are
twice as wide and here you know our jet
compiler for example will recognize that
this is capable of doing that and
generate instructions that are both
twice as wide but even more importantly
there's instructions that load and add
in one instruction rather than to
separate load and add instructions so
we're giving a lot more integers per
iteration of the loop
AVX two instructions on broad will also
get to execute two of these per cycle
which is more than before so we get much
faster so those are nice demonstrations
are just simple vectors and what we
could do with them but simple vectors
are a little boring because yeah how men
do you just sum this stuff in an array
what if you're searching for something
and what if you only want to add certain
numbers together what if you stuck in if
in your loop so this is a good example
where I'm running on the array but I'm
only gonna add the even numbers I test
the number if it's even then I'll add it
otherwise skip it now this presents a
problem for vectors because we don't
want to add everything we only want to
add stuff selectively and only if it
matches if we look at what a traditional
JIT would do with that we get this code
you can actually see it kind of unrolls
the code the logic twice but it's
literally take the integer test it skip
it if it's not even do the add and if
there's two of those per loop but
there's no vectorization here at all why
because there's an F right we can't just
blindly go do this well a V x2 in x86
added a really cool capability to
vectors it's a masking capability and
here's what we get with our latest JIT
the Falcon JIT that we have in Xing this
is the same code but it Maps and
recognizes it's running on avx2 capable
hardware and what it actually does is
this this code has four interleaved
striped sets of operations but there are
no branches in here it loads the data
and then tests all the data in a vector
and then uses the result of that test
mask loads to mask ads and to mask
stores so only the cells that actually
had a yes in them are actually applied
in the vector because the instructions
cannot do that and because the
instructions can now do that we get to
run about six times faster in the loop
and it just kind of impressive now this
is avx2 you know that was latest
greatest until a few months ago the
latest greatest now is skylake which
goes to avx-512 it's twice again it's
512 bits as a vector that's a lot of
hints right so it's 64 bytes it's 16
integers in a single vector instruction
this is what the same code looks like on
the latest Scylla care speeds and the
Google engine I did that this morning
this loop performs 64 elements per
iteration of the loop the processor is
super scalar and will execute two of
those instructions per cycle and the
impressive thing about this is I clocked
how fast this goes this specific loop
runs at about 16 or 17 billion
operations per second on a CPU that's
running at 2.3 gigahertz
when I say billion operations I mean
billion ads per second and if you think
about what's involved in a single ad you
have to go around the loop that's an
operation just you know
well count don't run the loop test load
the integer test the integer decide
whether or not to do something about the
integer I'm reading the code there right
you know and then add that integer to a
sum and and store it into the other
array you're looking at a total of
something like 8 or 9 separate logical
operations per iteration and we're
running on a CPU that's doing 2.3
gigahertz but it's adding about 17
billion of these per second it's all
because vectors exists and it's all
because the instructions in there and
were able to match the code to the CPU
and say oh that and you sleepy you could
do even better
so this is giving a feel for what modern
hardware can do at the end this comes
down to one really simple thing take
this thing and make it faster
right so faster code means lift that
right side of the thing right go faster
that's most of what better hardware
better optimization all those will give
us so because that's usually the things
most people are interested and I'm gonna
get into some specific compiler
optimizations and we're done with the
machine code if you're leaving because
of the machine code you can come back
I'm gonna start with very simple
compiler optimization sort of the ABCs
and build up from there you don't need
any background for this release so let's
start with simple compiler tricks the
compiler is allowed to take your code
and morph it to something that is
logically equivalent that is
semantically doing the same thing and as
long as the same thing it's allowed to
execute it that way so let's take stupid
code here stupid code what can the
compiler do to this for example it could
change the order of operations it
doesn't have to do them in the order you
wrote them as long as you can't perceive
the difference I can move that and that
that line around you I can also take a
code that has no effect on my result in
killing that's what called what's called
dead code so for example that line
there's nothing whether I run it or not
does not affect the result the compiler
figures this out and doesn't do it you
can't measure whether I did it or not so
I don't do it it's fast really really
fast do not do it I can also take values
and propagate them across operations
rather than do them the way you wrote
them so for example I can take that and
translate it to this code I don't
actually have to create temporary
variables put things in them put them
back at the end you don't need them
they're gonna get thrown away I could
just do the math and within that math I
could also simplify the math see the
plus y minus y so I can just say that's
X plus X it's a very simple people
optimization of that code it's not very
smart code compiler will do that
not this and virtually all compilers are
gonna do that for you any compiler that
you told to try to optimize so it's it's
not rare to see this kind of stuff now
let's look at a few more tricks
propagation of information of data of
computations can actually affect flow so
take this example here's a piece of code
we have an if if this value is big u we
do something if it's small we do
something else but we know that the
value is 5 the compiler is allowed to
take that code and simplify it to this
because it's 5 the first part of the if
won't happen the second part of that if
means bias is 1 we return bias well that
means we return 1 we don't actually have
to execute any of that stuff that's
propagation of a value that affects
whether or not I even do flow and I kill
part of the code because of it in
propagating optimizations we can also
cache reads if I read something from a
field from an array I don't have to read
it every time you wrote to read it I
could read it once and use it many times
for example if I have this code that
says read a XY z-- I'm allowed to
effectively read a dot X once into a
temporary and use it multiple times now
this seems like a simple ok I'll get a
little bit win for this but let's take
this to them to a higher level what if
we have this loop and you know that that
read can be cached you know I'm running
a loop as long as the flag is not set
but I read the flag once and I don't
read it again so what's gonna happen
even if you set the flag that's an
infinite loop this is what the word
volatile is for well that means do not
cache this read I really mean that you
need to read it every time around the
loop don't do that right but without it
you'll get a really fast infinite loop
there's a lot of things per second but
you never get out rights can be
eliminated so for example you take these
operations and they write into a dot X 3
times well I just have to do the last
write I don't actually
have to do all of them cuz maybe I could
do them so fast you can't see the
difference here's really fast you can't
see the difference you can't claim
that's wrong cuz you're nuts I could be
so fast you would never see between
those instructions so if extras I never
did them so that's a valid optimization
and you could do this to a much larger
power take this operation that says
let's do a million writes think it throw
away the loop it's not just three
operations I can get rid of I could take
a loop away if the next write I could
take the entire loop away and the writes
in the loop away if after the loop I
write on to it again I could run that
loop really really fast see how fast I
run it zero time so you're not observing
the intermediate values okay those are
nice simple things now let's look at
inlining inlining is a very powerful
optimization in fact inlining is
probably the most powerful thing
optimizers do not because of the
inlining itself not the removal of the
overhead of calling something it's
because when you're in line code you get
to apply all these other cool techniques
on a wider scope let's take an example
simple example of inlining here's a
final method that I now won't change its
route it returns X and Here I am calling
something to get X are simple getter
compiler can simply say I don't need to
call a method for this all just taxes X
X's are fast we don't actually make a
calls with them but let's look at how
this plays with other things here's a
similar method to what I had before but
this time I don't know what value is
it's a parameter so I can't optimize
this method and throw away half of it or
propagate anything in it because I don't
know what's coming in
however the caller is passing five if I
inline that method at the call site I
can optimize it to this if I don't
inline it I actually have to run all
that code in lining is what makes it
possible to optimize away most of the
code in a called function and this goes
very deep you'll see JIT compilers
inlining dying nine deep to get this
kind of up to
across a very wide scope okay so there's
always simple steps you know nice simple
pretty much all compilers will do these
nothing special two Jet's actually I'm
going to take a little sidetrack here
into micro benchmarking just to ruin
your day if you think you can measure
this stuff easily let's look at some
simple loops and measure how fast they
are here's a really simple loop you know
that's my benchmark is run this thing in
all time how long it takes it adds it
does it adds basically there's a plus
plus in time so right however many count
times I tell to do so turns out that
this code is really really really fast I
mean if I measure how fast this goes it
goes that fast that is impossibly fast
well something is weird here when you
get numbers there this I faster by three
orders of advice six order of magnitude
than the frequency of the CPU that
problem means it isn't do any of this
code it's just figured out you don't
need to run it so why is it so fast
well not quite sure obviously an
optimization happened we don't want to
have happen if we want to count how fast
we loop right oh so nobody uses the sum
right okay so let's fix that actually
let's let's do two tries to fix this is
let's make it more complicated in the
loop I'll add I rather than add one
maybe that'll help turns out that's
still impossible fast right and that's
because this is provably dead code to
your point nobody uses the sum this is a
void net function it has no side effects
I've said the function I can immediately
return I don't need to run this loop
that's why it's so fast so how do we fix
it we return sum right so let's do that
now we have public long this and now
we're returning some propagating the
thing so we have to do the math right so
is that better well it's still
impossibly fast and and why is that
it turns out the compiler is smart
enough to know that if you do that loop
sum will be equal to count it doesn't
have to run a loop to figure this out
right great
so let's complicate it and make it
harder let's make it do some actual math
rather than just add one as many times
as count well add I and all that so is
that better well it turns out that it is
better because then now you're gonna
time a loop on previous JIT compilers
unfortunately JIT compilers are getting
better for example our latest falcon
looks at that code and says that's an
arithmetic series I know how to do that
math and it figures out that it's count
times count minus one over two and
doesn't do the loop and then you can
also say let's make it more complicated
in some other way and you know you go
run this thing times I instead of plus I
and turns out that you can propagate the
zero and show that the loop doesn't
happen there's a lot of things that'll
kill your loops I'm just trying to show
you here and right now right now the
code I writes to make loops happen looks
like this and it's important to point
that it's right now because somebody's
gonna read this slide make an optimize
let's say I know what he's doing
there's only seven or eight possible
options for how this goes we're gonna
write something that figures out what
you know what the modulus counties and
what the answer is right so we can
always have compilers defeat the code
later they get smarter in fact this
thing about the arithmetic series we
didn't write that we use LVM and some
smart guy decided to optimizing
arithmetic series add addition is
interesting I don't know why I mean it
mean it the only use of that is to kill
benchmarks right but you know academics
do a lot of cool things we get to
inherit them right okay so that was our
detour into micro benchmarking and what
are the takeaways well Mike we're in
checking is hard your write code you try
to measure how fast something is you put
a loop around it maybe you're not doing
too something and and you may not be
measuring what you think you're doing
you may be measuring some other side
effect like entering the function and
executing it once or something
the trickiness changes over time what
works today may not work tomorrow I just
showed you that right and you need to
basically sanity check and check
everything everything suspect everything
don't believe the numbers check to see
if they make sense very them if you do
twice the work that you take twice as
long right if I double the count and I
get the same perfect at the same time
probably something wrong and if you
really want to do this well then use
tools that do it well gmh is a really
cool tool for doing micro benchmarks so
ugh ugh ugh that's your takeaway but
even if you use J mates you still need
to suspect everything why all the
benchmarks are just showed you will run
with jmh J which doesn't help you not
have these mistakes it little helps you
not have other mistakes but everything I
just showed you was measured with J
image it's just optimizer optimized
stuff away ok let's get back to some
compiler tricks how are we doing on time
I've got a few minutes good speculative
things JIT compilers in run times get to
do things you can't do
statically the main thing is you can
speculate you can actually decide to say
I want to believe something is true and
I'll optimize the code assuming it's
true but I can't prove it's true if it
stops being true I'll throw the code
away but as long as it remains true I
get to run fast code that is incorrect
under certain conditions but since I'm
checking the conditions I can run it you
cannot do that in a static optimization
so let's take examples of that simple
untaken path right so I've got this
method I get a value coming in I don't
know what the value is what can I do
about that well I can profile this and
saying you know I've been running this a
million times and I've never seen a
value greater than 10 I want to believe
that it'll never be greater than 10
let's do this code now I have to
validate my assumption in the code in
this case right but I basically say if
it's greater than thin just don't run
this uncommon trap go run the interpret
instead but if it's not then we have an
optimization for you there's a valid
question I could have done an if there
and put other optimized code that went
without a way but once you do this
combinatoric Li the code exponent gets
slow uncommon traps and ways of not even
thinking about the code not generating
and killing it assuming it will never
happen and then dealing with really slow
ways if it does happen another cool
example is implicit mal checks in Java
every access to every field in every
array it requires the JVM to go do that
when you say get that field from exit
actually means check that foo is not
null and if it's not null then go do
this every single place you do that and
if we actually did that every time that
would be slow so the JVM wants to
believe you don't give it an ounce to
follow once to believe that can't prove
it sometimes you give us nails but what
it does is it generates the code is if
it's not gonna be now what could happen
well can give us a null and we're gonna
take a seg vien crash well the JVM
intercepts this ugly says where did it
happen it happened there I should have
had an if there let's like act like
you're had an if there that works great
it's really really powerful as an
optimization but when you do have an all
it's like a hundred thousand times
slower than doing that if so if you run
into that code and see a lot of times
where that one throws a
nullpointerexception
you basically change the code back to
this through the old code away and put
this back it's an example of a specular
adaptive optimization class arcane
analysis is a really strong optimization
and it basically takes this approach
right now in the world I can see classes
have this relationship I can't prove
that'll be true in the future but unless
that changes I can optimize certain
things here's a concrete example of that
remember I showed you the in lining and
a final method that's ugly to have to
put fine on methods just to make them go
fast and you don't have to do that
here's that same method without final on
it this is an animal this is how you get
a color from an animal there are dogs
and cats and birds but none of them have
overridden this mess
in the current universe so if I have
that caller calling this and I can prove
that there's only one implementer of
animal get color I can inline it no
conditions no checks no nothing when
does this stop being true when you load
a class that breaks my assumptions so
we'll check every time we load a class
whether the assumptions are still true I
don't have to slow the code down if I
load a chameleon and the chameleon
computes the color based on the branch
it's sitting on now I have to
implementers and I have to throw away
this optimization and run it with the
actual virtual potentially so this by
the way is why getters and setters are
fast in Java it's why you can write
clean object-oriented code without this
optimization you'd have ugly code or
you'd be exposing your fields or your do
final getters without overrides and he
can't write chameleons this lets you
write clean object-oriented code that is
really fast very powerful now what if I
had a chameleon so I got this chameleon
and now I've got a virtual method I
can't do that thing but you know what
yeah there's a chameleon in there dogs
and cats but the code I'm running in is
a dog kennel and for some reason people
have never put a chameleon in the dog
kennel so this loop that I'm running has
never seen anything other than a dog we
profiled that we figure it out and we
generate this code if it's not a dog I
don't know what to do here just go
around the interpreter but if it's a dog
boom optimize the heart of it no calls
it's not as fast as I don't even have to
check but it's really really fast
compared to a virtual call and I get to
in line and optimize everything in the
method which I can't do if it's a
virtual call okay so all these cool
things were there cool things
speculative things there's a whole list
of additional speculations but
speculation depends on something very
important that the JVM has to be able to
do it's the optimization we're
optimizing based on assumptions that are
not provable they're observed we're
hoping that they're true
we really hope they're true but often
they won't be so we have to be able to
take the code we generated throw it away
go back to this other code slow code and
go again that's what the optimization is
without the optimizations you do not get
to do speculations like this so first
you need to be able to be optimized and
remember this picture the reason this is
not a smooth yellow to green to blue
thing and we have spikes going in there
is because sometimes our optimizations
are wrong
sometimes we speculate we find out we're
wrong we have to throw away the code go
back to the yellow or green and go again
that's where you have these spikes where
it was blue but that went back to green
or yellow and back again eventually
it'll stabilize it'll learn it'll
speculate that was wrong it'll stabilize
and only have the code that survives the
speculations for a long time so the
optimization is another gnarly thing if
you think Michael bench ranking hard
dealing with warm-up and the
optimization is much much harder to
figure out especially if you run really
really short benchmarks or really small
data sets so the idea is that you know
often people look at this and say okay
that the optimization make me go really
slow how do I warm things up so I won't
have the slow code let's exercise it
give it some real stuff and then run the
real code the real note I want and the
problem with that is you often think
you're warm
but then the behavior of the world
changed just the number changed if
something happened in a packet somewhere
our message and suddenly things are not
what you thought it right they don't
have to happen in the first few minutes
they can happen at 3:00 p.m. so many
many potential causes for the
optimization um the reason warm-up
doesn't cut it is easy to demonstrate
with the concrete real-world motivation
imagine you're writing a trading system
and it's in Java because you can write
fast and go to market fast but when you
trade the market opens and everybody
rushes in in the volatility is high and
that's where most of the cool stuff is
happening and you just started your JVM
and you running into
credit code and it's still profiling so
you want to warm it up you want to
before you start this you're gonna
you're gonna run stuff we'll say okay
you know what I don't want to wait for
10,000 operations to happen before I get
off to mice code let's run prime it with
you know 20,000 traffic things fake
traffic that maybe I recorded yesterday
exercise the codes so it's nice and
ready for the first transaction at
market open and it's already warm or
teach it it and it does that right so
you warm it up to just go a while they
speculate they figure out the wrong they
do whatever it is they settle but what
actually happens here what you're doing
is running fake traffic through you
don't actually mean to do 20,000 trades
right with somebody else's money so
usually the fake logic has something in
it that says go to the other but but
it's fake so don't really send it what
is the JIT compiler gonna optimize for
it's gonna optimize for not trading
because it's run twenty thousand things
that it's never traded maybe this never
happens and what happens in the first
straight we do optimized go back to the
beginning and start collecting stuff
from scratch right it looks like this
you warmed it up you know really really
fast the boom market opens you go slow
again so in reality this warm-up in
learning an aggressive optimization
happens here the optimization happens
here then you re optimized with the real
knowledge that's kind of the evolution
of things okay that's warmup that's the
optimization what can we do about this
well one capability we actually built
into a JVM is there build is you log in
replay optimizations across runs record
the inputs to optimizations from
yesterday start today with what we
learned yesterday including what not to
do so think of it simply as you tell a
JVM to record why and how it compiled
things and tomorrow you tell the gym to
start with that recording it Prime's up
it starts everything JIT compiler
everything you're ready for the first
transaction to go fast rather than 10
thousand people or purchases or trades
going slow before you go fast and you
can actually build a workflow that will
prove to you that nothing that you
applied yesterday has not yet been
applied the impact of that is to take
that the optimization and flatten it out
you know make it happy but also to take
that warmup time that actually requires
you to train things and bring that down
to practically nothing because you don't
have to run traffic through it you just
need to initialize things so you get
this nice flat operation what does this
mean to this picture so I already showed
you that better ginning raises the bar
this brings it over to the left and
flattens it right you take away all that
warm-up curve so that's better
now what's left here what do we have
left to do what's missing this thing
still drops to zero speed every once in
a while and that's that last component
what if we could make it not drop to
zero speed every once in a while and
then we get this nice flat speed close
to what we actually want to see that's
where c4 garbage collector comes and
this is the really proud brow you know
just beating thing that I get to say c4
in zing JVM you can run any of you have
applications on basically eliminates GC
as a problem it takes things that look
like the thing on the left that glitch
all the time and they can look like the
thing on the right they only glitch some
of the time those glitches are not
because of the JVM there are many other
reasons you clutch we take away the JVM
glitch reasons we also take away all the
reason to tune GC so you might have
recognized and some of these flags you
might be doing these sometimes you want
to run fast you want to and without
pauses you want to delay the pauses so
you tune things and you need to tune
things right right this application in
that application diametrically opposed
values for flag X which means you've
tried a lot of things and you figured
out how to tune this for your current
workload if you think those are a lot of
flags here are a few more flags you can
choose how to tune any identities the
modern way to tune things with us is
this
you give it a big heat and it works now
that's a waste
so you shrink the heap and shrink that
he'd been in shape and you figure out
how small how small you can go before it
breaks you triple the heap and you go
home that was GC tuning
today right now in practice I'm thinking
not trade like this is Cassandra running
on a normal Broadwell machine and that's
the one milli second passe level running
under full stress of a benchmark so if
you care about 400 micro second glitches
yeah we do glitch but Linux Fletch is
bigger than that Cassandra delicious
bigger your i/o glitch is bigger than
that this is the noise the rest of the
signal so you know in practice we get to
do things like this Cassandra an EWS
under an SLA simple question is I need
to meet that SLA with that many nodes
how much traffic can they run through it
you run it with hotspot with G one with
two months of consulting of tuning and
you get the speed this is an actual real
case from an actual customer you run it
with zing with no tuning and you get
that those are real numbers for
Cassandra on AWS today on the latest
greatest instances now this is not
because we run the code five times
faster we do run the code about 11%
faster and we start up quicker but it
doesn't affect this this is because you
can press this thing much harder before
the glitches break down something this
is about removing glitches and what that
translates into in reality is things
like this if you actually monitor load
and timeouts or errors or field success
rates which is just what this orange
line is every time it dips that's a
failure 100% is a thunk 98% at the
bottom you get up to 2% failure in this
case you turn it on to zing and it runs
smooth you get speed like you want you
get this kind of speed so that one's me
bragging at the end I apologize for that
hopefully the beginning of it was more
educational if you really want to know
more about it you can come to our booth
to come talk to me but at this point I
think we're done on time
I'll be happy to take some Q&amp;amp;A as I wrap
up and clear the room for the next
speaker and I'll hang around here as
long as you guys want so feel free to
ask questions while I start wrapping up
this stuff thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>