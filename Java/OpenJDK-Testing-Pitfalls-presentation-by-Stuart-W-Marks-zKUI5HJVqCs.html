<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>OpenJDK Testing Pitfalls presentation by Stuart W. Marks | Coder Coacher - Coaching Coders</title><meta content="OpenJDK Testing Pitfalls presentation by Stuart W. Marks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>OpenJDK Testing Pitfalls presentation by Stuart W. Marks</b></h2><h5 class="post__date">2013-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zKUI5HJVqCs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello my name is Stuart marks I'm an
engineer in the core libraries group
working on JDK and this talk is about
openjdk testing and in particular the
pitfalls will open JDK testing so I'm
going to assume that you have some
background in what the tests look like
in OpenJDK and some basics already I
hope you've seen some of the
presentations my colleagues have given
that are an introduction to testing and
OpenJDK so this presentation is about
pitfalls that can arise in testing and
it's really going to be example driven
so there are going to be a lot of
example tests &amp;amp; test code in this
presentation and so with that
introduction I will dive right in so
here we have an example test it really
doesn't do anything interesting but the
point of this is to look at the test
tags at the top so in the comment at the
top of the source code there is ask test
which marks this file as as a test and
then there's a summary line that that
describes it there's nothing interesting
going on here we've added a third tag
suppose we've modified the tests to may
have a dependency on some class named
another class and so we need to add a
tag at build another class that tells
the JT reg test framework to build that
other class all right well that seems
pretty simple we've added an appt build
tag what could go wrong with this
there's some description here of why
that previous test introduced a bug
there's a rule in JT reg the tags that
appear at the top of the tests are kind
of a little programming language and
there's a rule that says if there is no
run action than an at run main test
class or whatever the name of the class
is at run main is assumed if there is no
run action in those tags so if you were
to look back at the first example there
was no run action and so it assumed that
at run main was there in the second
example
there was an at build tag it turns out
that at build is actually shorthand for
at run build putting an appt build tag
into the test actually does have an at
run action so that means that this
implicit behavior of having an at run
main tag is no longer invoked so the app
build counts as the run action and so
that default behavior does not occur so
what happens what JT reg does is it
finds this test it says at build another
class and so it goes and builds it and
since there are no more actions in the
tag language it simply does nothing so
really the problem with the test code it
does some compilation but it does not
actually run the test and so that's a
very surprising outcome for having added
an appt build tag what is necessary here
is to have at run tag that tells JT reg
that yes you really do want to run the
test what I've adopted for the test that
I've written a rule that is every test
must have an appt run tag even if
strictly speaking it's not necessary so
now in recent versions of JT reg this
change was put into the open source
probably i think in april of 2013
there's a new rule that says that the
last run action must not be a build
action in fact if the last action turns
out to be a build action than JT reg
will now consider that to be an error so
the example on the previous slide where
it would actually not run the test that
would actually be considered an error by
JT reg now there may be builds of JT reg
floating around that still have the old
behavior so that's something to look out
for it if you've grabbed a build of JT
reg still might have this behavior and
you might inadvertently write a test
that doesn't actually run because of the
way you've set up the tags so my
recommendation there is to get the
latest JT reg sources and build them and
use that version of JT reg so that you
can make sure that you don't fall into
pitfall so what I have here is an
example of a shell script test this is
an actual shell script from the JDK
SourceTree for brevity I've taken out
the copyrights but this is copyrighted
there's an apt test tag and that bug tag
that identifies which bug this is a test
for and then there's some shell script
syntax here which gets the operating
system name and puts it into the
variable OS and then we have a shell
case statement what this does is it says
for certain operating systems it assigns
the PS and FS variables so PS is path
separator and FS is file separator so
you can see for the unix-like systems we
use colon and / for that cygwin is a
special case where we use semicolon on
Windows we have PS and fs be set to the
characters that are appropriate for
Windows and finally we have a default
case that says if it's not any of the
Linux or UNIX systems or cygwin or
Windows then you know we don't know how
to deal with it so we error out with an
unrecognized system we have the logic of
the test it's using the fs and PS
variables in the appropriate places when
it needs to construct path names and
actual paths like the classpath if you
read through this very carefully you can
see it's compiling some java source
files and adding some directories to the
classpath and then it's removing some
temporary files that might have been
left over from the previous test run and
then it's running the java vm on the
test file that was just compiled and
it's passing it the argument word first
and then it invokes java again the
second time on the same test file and
passes it the argument which is the word
second well that's pretty messy so let's
discuss what's going on here the first
two slides was really assigning
variables based on what operating system
so that the path variables and path
separator characters could could be used
properly depending on what operating
system was in use actually it's
clear whether this is still necessary it
might have been true on old versions of
cygwin or old versions of other support
systems that gave unix-like behavior on
window systems that were very sensitive
to having backslashes versus slashes in
the pathnames but in fact this is no
longer necessary if you took all that
stuff out and just used / and colon then
the test would probably continue to work
nonetheless you will find this sort of
the case statement over the operating
system and the assignment of path
separator and file separator variables
you will find that at the top of mini
mini shell scripts using those variables
in the pathnames makes the script very
very hard to read instead of colon or
slash separating pathname elements then
you have this variable interpolation in
there it just makes makes it very
difficult to see what's going on in the
actual logic part of the script but more
importantly if you set aside the the
syntactic noise introduced by those
variables there are some more serious
problems with the test specifically the
test first compiles a Java class using
the Java C compiler the problem is if
there's a syntax error in the file I
mean that shouldn't occur but it's
possible if somebody's editing the file
they might introduce a syntax error or
the classpath might be wrong and it
might be missing a class or one of the
variables is wrong or something there
are many reasons that a compilation can
fail well the problem is that it doesn't
actually handle errors at the
compilation step now java sea will
certainly emit some error messages and
returned and exit status that is not
zero but the shell script does not do
anything with that and so the shell
script will just charge ahead it might
or might not fail when it tries to
execute that class now if there's an old
version of the class that happens to be
left over from the from a previous test
run it might go ahead and execute that
instead so you might actually have a
compilation error and execute an old
class file and the test will pass even
though there was a compilation error so
that's a pretty serious problem there's
another problem with shell script test
which is that the way JT reg gets the
pass or fail information
from a shell script test is it looks at
the script exit status it's possible in
writing shell script to set the exit
status explicitly by using the exit
command but what this shell script does
is it relies on a subtle feature of
shell script which is that the exit
status of the script is the exit status
of the last command in the script what
this test does is it actually invokes
the java test class twice first with an
argument of the word first and the
second time with the argument of the
word second now suppose there was a
problem with the first invocation of the
test so that might emit some error
messages and it might return an exit
status of nonzero but then the shell
script would just go ahead and execute
the second test pass and if that
succeeded then the entire test would be
considered to have passed and so what
that did was that covered up the failing
exit status from the first part of the
test run and so that's another serious
problem this kind of shell script
technique is actually very fragile
suppose I were editing this and I wanted
to add some debugging output or I just
wanted to add a message like an echo
command and I happen to put that at the
bottom of the script well because of
this shell script behavior of the exit
status of the script being the exit
status of the last command if the last
command is echo well echo actually
always returns success I've never seen
the echo command return failing exit
status what that means is that JT reg
will get an exit status of success based
on the echo command which is the last
command in the script and regardless of
what happened when script executed the
Java commands if those passed or failed
that would be ignored by the shell
script and the shell scripts active
status would always be success because
that was the last command in the script
relying on the shell script behavior of
the last commands exit status
determining the exits
matis of the shell script is very very
fragile so my recommendation there is to
make sure that the exit status of the
shell script is always set using an
explicit exit command
this is a shell script called test
serialization mismatch this is a draft
of the test based in January 2013 this
test has since been rewritten in Java
after looking through the shell script
test you'll be able to see why there are
many many problems with it but first
let's just look at the source code of
the shell script we have an invocation
of Java Sea and it compiles some Java
source files and these are for the
client part of the test and then the
next line is another invocation of Java
Sea which compiles the server part of
the test just below that now we have a
fairly complicated command that invokes
the Java Virtual Machine and it adds the
server classes to the classpath and then
invokes the server class so here what
we're doing is we are starting up the
server side part of the test now it puts
this command in parenthesis and
redirects the output to a file called
URL path the variable URL path is
assigned earlier in the script it's not
actually shown here but it's point it's
pointing to something valid so we
redirect the output to the URL path file
that includes both the standard output
and standard error from this invocation
put that into the background and then
take the process ID of the server
process and put that into a variable
called server pit the next thing we do
is we go into a while loop where we wait
for the URL path file to become present
and that's what the minus f test in
brackets does when that file is present
we break out of the loop otherwise we
sleep for two seconds and try again then
once the file is actually present we
read a line of text from the URL path
file and put that into the variable
called jmx URL here what we do is we run
the Java VM on the client side of the
test and we wrap this in parenthesis
redirect the output and pipe all of that
into grep for a particular error message
which is severe failed fetch
notification and so
so we grep for that error message and
then if you look very carefully this is
all inside of the shell back quote
syntax so it takes the output of that
Java pipe to grep and puts that into the
has errors shell variable once we've
done that we kill off the server process
and then what the test does is it tests
the has errors variable and if it is
empty that is the minus Z the minus Z
test in the if statement has errors is
empty then we consider the test to have
passed and if it's not empty then we
consider the test who has failed we echo
the output of has errors and we exit
exit status one following my previous
recommendation this shell script
provides an explicit exit status in all
pass through the shell script instead of
relying on the exit status of the last
command in the script but there are a
whole bunch of problems in this test and
so let's go over them one by one so what
this test is trying to do is it needs to
compile two separate incompatible
versions of what is essentially the same
class file that's really what the test
is trying to test it wants to put these
different versions on one on the client
side and one on the server side it's
difficult to do this with JT reg compile
or build tags because those basically
really want to compile something only
once it's very difficult to have two
separate versions so instead the author
of this test decided okay well since
since we can't use JT reg build tags to
do that than what we should do is invoke
the compiler explicitly using a shell
script and have the compiler run Java
Sea and compile those classes for me so
similar to the previous example there's
a problem which is what if the
compilations fail that's not tested for
at all in the shell script subsequent
parts of the script when they go to
invoke the Java VM they might fail at
that point but they might not if they
happen to find some old version the
class lying around there the compilation
steps if they fail do not cause the
entire test to fail so that's one
problem there's another problem which is
kind of a small thing but they're a
bunch of variables put into the
environment by JT reg in particular test
Java and compile java test Java is the
points to the JDK under test whereas
compile Java is the JDK that is actually
being used to run JT reg itself and so
really if you're compiling things you
want to use compile Java because that is
a known good JDK as opposed to the JDK
that is being tested which is in test
Java the shell script is actually
compiling the classes with the wrong
version of Java it's unlikely to cause a
problem unless there has a bug and the
compiler but still it's using the wrong
JDK which could possibly result in some
strange errors if if you're not careful
or if there's something strange in the
environment first thing that the shell
script did was to start up the server
and then there was a while loop where
the client had to wait for the server to
start so that's why we have sleep inside
of a loop here but there's actually a
race condition here so the URL path file
is not created by the server it's
actually created by the shell that's
running the shell script and then the
JVM running the server code is
redirected into this pre-existing file
so what happens is this while loop is
actually always going to terminate
immediately because the URL path file
will have been created immediately by
the shell even before the server has
started running so the problem is
there's a race condition where when the
client wants to read the URL that was
supposedly written by the server it
might encounter an empty file so there's
a race between the client and server
where the server is going to write the
URL into the file and the client is
trying to read it so the client might
have tried to read it before the server
actually wrote it but there's another
problem which is that the way the shell
script is written if the server for
instance has exited with an error or if
there's something else going on the
server where the first line of text in
the file is not actually the URL well
the read command from the shell is only
going to read the first line and what it
reads might not actually be a URL so the
client might try to connect to something
that it thinks is URL but is in fact
garbage
that's another problem the way the
client is run suppose the client throw
some unexpected exception well the
clients output is piped through graph
the graph pattern is for a specific
error message and if that specific error
message is not present in the clients
output the output will be empty and the
test will be considered to pass even
though the client might have thrown some
entirely unrelated exit so this is a
extremely serious problem if the client
fails then it might emit some error
message that will be thrown away by the
grep command and then the test will test
the output find it empty and consider
the test to have passed the test reports
pass even though the client actually
failed and worse we've destroyed
information about why the client failed
because that's thrown away by grep this
kind of test it's possible for there to
be failures or errors in this kind of
test you might have automated test
cycles that run this test frequently or
even continuously and it might be
failing and you'll never know even if
somebody goes to look at the test log
because the test framework is always
getting an indication of pass from this
shell script another issue is that the
exit status of the client is not checked
since it's in a shell pipeline it's
actually kind of hard to get the exit
status of something that is earlier in a
shell pipeline so that's more
information that's thrown away it's very
fragile because the grep pattern is a
very very specific error message and the
client might fail because of some other
kind of error message or actually the
system might change or the system might
be localized so that the proper error is
coming out but in fact the error message
might be different and so that would
cause the test to misbehave as well
you can see that there are a lot of
problems that arise when writing shell
scripts and so much so that my
recommendation is simply to avoid
writing shell scripts it seems like it
ought to be easier to manage
subprocesses using a shell script than
in Java but that's not really true we
had some logic in the previous shell
script that forked one sub process and
then wanted to wait until particular
event occurred before forking the second
one well that's actually pretty hard in
the shell script and you can see that
there was some logic that attempted to
do that in the example but it had some
problems with it worse suppose we wanted
to wait and retry periodically every two
seconds we wanted to avoid waiting for
say more than 30 seconds so we'd have to
add a bunch more logic in that while
loop to make sure that we didn't wait
too long if a command inside that while
loop were to hang we might never break
out of that while loop there are
actually quite a number of subtle
behaviors and difficulties in trying to
do things like timeouts using shell
scripts in Java if you were to try to
manage sub processes there is a set of
process libraries in the JDK already
java dot lang process and process
builder and those are pretty low-level
it's actually pretty difficult to manage
sub processes using those so what's
necessary is some kind of higher-level
library that deals with certain certain
issues of managing sub processes from
Java more effectively and so what's
happening is that certain areas of the
test suite are developing test libraries
that are shared across a bunch of
different tests that encapsulate common
functionality for example in the rmi
tests there is something called the rmi
test library that has a bunch of utility
code that manages sub processes now
these are not very general in that they
are managing rmi specific sub processes
but the techniques in use here could
eventually be generalized to manage sub
processes and must replace the sub
process management that one would like
to do from shell tests especially in
tests that want to do client-server
communication there are a lot of shell
scripts
wrinkled around the test suite that used
a fixed ports for communicating between
a client and the server and this is a
problem if somebody else happens to be
using that port then the test will fail
unexpectedly and also intermittently now
this test in particular happens to
choose a random port that's actually in
the server code but what it requires is
that the information about which port is
being used be passed from the server to
the client the way that this shell
scripts did that was by trying to
communicate between the server and the
client through the file system so the
server has to tell client where to
connect to it and the only way to do
that in the shell script is kind of
through the file system so that's why
there's this very clumsy redirect the
output to a file test of the file is
there read a line from it and so forth
if you are writing this test in Java
then you can manage the server and the
client using the java programming
language and then if they were inside
the same jvm then you would just pass
things like the unique port number
directly to the client if the client
we're using a subprocess then the JVM
could simply pass the port that the
server was using through a command-line
argument or through a pipe or something
like that it turns out that using Java
there are many more effective ways to
pass information between a client and a
server like a unique port number instead
of using this very clumsy mechanism that
shell scripts use
so one of the things that we do need to
do in the jdk test suite is to develop
more and better test idioms and test
library support in particular one of the
things we do need to do some work on is
to call the compiler from Java programs
so we've seen a couple shell script
examples where the shell script itself
invokes the Java C compiler turns out
that it is possible for Java programs to
invoke the compiler for instance the jsr
199 is a compiler API where you can get
very fine grain access to the code of
the compiler itself running in this JVM
it's also possible to simply invoke the
compilers main method and pass it an
array of strings just as if those
strings had been passed to java sea in
the command line I believe it actually
runs java sea in that case in the same
jvm so we've avoided some fork and exec
overhead but we get the additional
advantage of running the boiler in this
JVM so that we can have complete control
over what it's doing and catch
exceptions and compare its exit status
and so forth there are large areas of
the JDK that have specialized needs from
testing framework in our environment we
developed that specialized code to do
that kind of within a testing library
that is specific to that subsystem
examples for instance our jmx or rmi
they have their own test library that
supports a specialized needs and this
technique needs to be pursued and
evolved further I'd also recommend that
such frameworks try to figure out how to
use kind of a white box style testing
instead of black box testing of testing
the output so if you look at a shell
script it can't actually go inside the
JVM so the only thing that it can do in
this case is inspect the output in this
case it is trying to find a particular
error message for a variety of reasons
that's quite fragile if this test were
running inside the same jvm as the
client then what it could do is you
could use some internal interfaces into
the class library to examine the state
of what's going on and get a much more
precise indication of whether the test
passed or failed so it is possible to
rewrite shell scripts in Java and they
become much more robust and reliable
this shell script test was actually
rewritten in Java and this was checked
into openjdk in May of 2013 and the Java
code that reimplemented shell script is
somewhat verbose but it is much much
more robust and it's actually fairly
easy to read the shell script logic for
capturing output and pipe and grep and
capturing the process IDs and killing
things and so forth killing the server
sub process is actually fairly subtle
and it's easy to get wrong and in fact
as we've seen this shell script does get
it wrong quite a bit the rewritten
version in Java should be much more
robust it is somewhat more verbose but
actually it's it's quite easy to
understand should be quite reliable I'd
like to conclude with some issues for
discussion the test suite in the JDK is
a work in progress and there are a lot
of variations in behaviors across the
different tests different people have
different opinions about what tests
ought to do a lot of this needs to be
discussed and some decisions need to be
arrived at over the history of the JDK
different engineers have written tests
in different styles and they behave
inconsistently these are some things
that need to be cleaned up there's an
open question of whether tests should
clean up after themselves we saw in a
couple examples here tests that created
temporary files some people like the
idea of program creates a temporary file
then that program should be responsible
for cleaning it up and that makes a
certain amount of sense from a
cleanliness perspective on the other
hand the JD reg test framework will
automatically clean up any files that
the test happens to leave in the scratch
directory so since JT reg does that
clean up then maybe we don't need to
have that logic inside of every test
that creates a temporary file an
additional point about relying on GT reg
to clean up temporary files is that if
the tests were to clean up temporary
files for itself if the test encountered
a problem
then there might be some interesting
information in those temporary files
that might be useful for debugging that
problem but if the tests were to clean
up those temporary files itself it might
destroy that diagnostic information and
so GT reg can be instructed not to clean
up temporary files somebody developing
those tests can go and look at those
temporary files left around after a test
run those might be helpful in diagnosing
problems somewhat related to the idea of
cleaning up temporary files is whether
tests necessarily need runnable in a
standalone fashion or whether they can
rely on having been run by JT reg so
historically all of the tests start by
invoking the main method of a class
which is very easy to invoke from the
command line there are certain things
like the behavior of JT reg and build
tags and run tags and so forth that
really start to make the tests rely on
JT reg potentially this notion of JT reg
cleaning up temporary files that's an
additional reliance on JT read what's
clear that is necessary is we need to
develop test libraries for certain sub
systems such as jmx and RM I and so
those tests libraries have a lot of
logic that is specific to test in those
areas but if you were to try to invoke a
standalone test you have to do a lot of
typing to put the librarian of the
classpath and so forth and it just makes
it much more cumbersome to run tests in
a standalone fashion if you have test
libraries in the picture and so JT reg
does this for you that's another point
slightly in favor of just relying on JT
reg instead of holding on to this notion
of running tests in a standalone fashion
another issue that needs to be discussed
is however both should tests be there
certain test frameworks such as J unit
which are which are pretty quiet if
tests are successful and that's nice and
there's something to be said for that in
the JT reg tests tend to have a style of
being more verbose in particular
emitting lots of messages even if they
and I think the reason there is that
it's a little too easy for a test under
JT reg to miss a case or to accidentally
report success when it really failed as
we've seen in some examples here JT reg
only really looks at the exit status of
tests for instance if you have some
subtle logic in a test if you might miss
a case if your tests main method happens
to exit normally well JT reg considers
that to be success and so even if
something bad happened in the test and
you it'd emitted error messages and so
forth JT reg might still consider that
to be success the style that I've
pursued and several of my colleagues
have also pursued is to be fairly
verbose even if the test is successful
that way somebody can look at the test
log and say oh yes executed the test it
tested a particular case and it did so
successfully if that's not present in
the log then you kind of have to go read
the source code for the test to figure
out whether it actually was testing
anything properly or whether there might
be in logic error in the test where it
it might have skipped a test
occasionally somebody will make a
suggestion that a test have something
other than a boolean output pass or fail
in particular test might look around and
see oh well there's something wrong in
the environment like there's not enough
memory or file system space on the test
and somebody will suggest oh maybe the
test should issue a warning that there's
a problem with this system or something
like that well the problem is that JT
reg really only collects a boolean from
the test pass or fail it does collect a
bunch of error output but if you were to
issue a warning from a test but still
have that tests indicate passing then
that warning might end up in a log file
somewhere but probably no one will ever
see it because no one ever looks and at
a test log file if the test said that it
passed warnings from a test that do not
cause failure are really not very useful
at all if there's something wrong my
recommendation is that the tests
indicate failure even if it's something
that you might not consider to be an
ordinary test fail because issuing
warnings and having the
Pass is just not going to be useful in
an ordinary test run there might be
thousands of tests thousands of test log
files and warnings buried in there are
never going to be seen by anybody
so that concludes my presentation there
are lots of interesting issues raised
here some things that could be discussed
and further discussions that would be
had in the future and so if you're
interested in discussing any of these
issues please tune in to the quality
discuss mailing list at OpenJDK java.net
thanks very much for listening
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>