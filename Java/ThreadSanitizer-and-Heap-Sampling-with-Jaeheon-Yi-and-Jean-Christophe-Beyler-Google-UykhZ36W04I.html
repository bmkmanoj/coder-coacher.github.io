<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ThreadSanitizer and Heap Sampling with Jaeheon Yi and Jean Christophe Beyler - Google | Coder Coacher - Coaching Coders</title><meta content="ThreadSanitizer and Heap Sampling with Jaeheon Yi and Jean Christophe Beyler - Google - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ThreadSanitizer and Heap Sampling with Jaeheon Yi and Jean Christophe Beyler - Google</b></h2><h5 class="post__date">2017-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UykhZ36W04I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Jhon and this is my colleague
TC we're from Google and today we're
going to talk about thread sanitizer and
some sampling click try again so basic
outline is intro thread sanitizer keep
sampling will conclude it's very simple
so as program fit more complex there's a
lot of questions people ask about what's
going on in their program especially
when things go wrong so we're going to
talk about two tools today one is thread
sanitizer which finds data races for you
and one is keep sampling which tells you
what is being allocated when you run
your program in production okay so first
thread sanitizer so all of you in this
audience know very well what a data race
is you've found them you've caught them
you fix them but to illustrate what the
tool does let me just walk you through a
very very basic database so here's a
Java class T has a data int field called
X they were setter and a getter there is
complete absence of synchronization so
when you compile and run this program
you may see a trace that looks like this
there's two threads key 0 T 1 T 0 enters
the set method rights X exits T 1 enters
baguette method B duck exits very simple
stuff and what the tool does is tell you
that ah you have a race on ax and that's
very binary it's not really interesting
so it actually gives you a lot more this
is a sort of back-trace type the race
report that you will see and it's a lot
of text but it's worth going over a
little bit the content so at the very
top you can see that ha ha it's the
thread sanitizer tool that's tell you
there's a data race in this program
seven six one six there's four bytes
being read out of some machine address
by thread 26 and the the place it was
read what he get on line seven which was
called by a docket on on line six of a
Java a little below you see a see
function that was calling this Java
function and below that you see a J&amp;amp;I
stub so you can see that thread has been
wandering
through various languages and it's the
same thing for the right so the rest of
the talk is to kind of explain what goes
on between here and how we get to here
so to explain what's going on I do have
to talk a little bit about half as
before we already saw what happens for
is yesterday with John roses talk but um
let me just set the stage a little bit
here so let's say we have this trace
just like we saw before and and the
question that we really want to answer
is does the right of X by t0 happen
before the read of X by T 1 so this is
the question we're going to keep coming
back to as we progress through the
slides so here is a very condensed
simplified definition what happened to 4
first we take what I might call the
thread event order there intuitive so in
T 0 enter upset happens before the right
of X and right of X happens before the
exit of set very simple in T 1 the same
thing enter again happens before we'd of
X happens before exit of guard the
second part of the definition has to do
with synchronization so let's pretend
that the set and the get method where it
was properly synchronized with say a
class lock so the trace might look
something like this between around every
access there is an apply release there
so this thing is properly synchronized
and now I can bring up the release
acquire order where the release
operation of a particular law happens
before a subsequent acquire of that lock
and finally to bring it all together you
bring in transitivity and and now we can
talk about the famous cones of causality
where all the events that are known to t
0 up until that release happen before
all the events that follow in T 1 that
happen that occur after that applier so
that's all good now since the right of X
happens for the release happens before
the fire happens for the read
sensitivity tells us that yes right of X
happens before the read of X and so we
may legitimately conclude
that there is no data raise contacts so
going back to our original trace the
RACI trace we can examine the happens of
relation and we say yes there is tons of
thread in bed bordering but there's no
release acquire order and no Matt no
matter how much you transitivity you add
you're not going to get from right of X
to read of X so there is no happens
before relation and so we may conclude
that there is actually a raise on X the
thing to know here though is that half
as before is a mathematical relation if
you if you fire gdb and examine your
program you're not going to find that
happens the correlation in your program
it's in our heads so so to to actually
do this analysis what you actually need
to do is track it in some instant
fashion and and the way we do it is with
something called vector clocks and in
some ways a vector clock is kind of
self-explanatory it's an array of
logical clocks it's an actual data
structure but the way we use it is is a
particular and and we have to interpret
it correctly so instead of going through
the algorithm as per definition I'll
just walk you through an example so
let's say by way of example p0 happens
to have a vector clock 4 comma 0 at the
time after the acquire of that lock so
the way to interpret this is the indices
in that vector clock all are for each
thread in the system so if there's a
hundred threads this vector clock would
be a hundred long it's 40 zero so if you
index in a zero you see the number four
and that is the current time for thread
T zero all the other indices all the
other values in the vector clock
indicate the last time at which another
thread has synchronized with this thread
so next thing to do is we do a write and
when we do a write we simply remember
what the timestamp was so we just record
it off to the side we advance we
actually do the right and now we come to
release and when we do release we do the
same thing we kind of remember aha the
the vector clock when we did a revenues
for a seed class was 4 comma 0
the interesting thing here is according
to the algorithm we will increment the
current clock from four to five and then
life goes on for this thread meanwhile
41 by way of example let's say that the
current vector clock 41 was 0 comma 8 so
we did 331 and you when you index into 1
the current time is 8 and 0 on the other
clock means we haven't synchronized with
T 0 when we do an acquire this is the
way we do the acquire update we we take
the current vector clock we take the
last vector clock scene for that
particular lock and we do a pairwise
update throughout the vector so for the
first entry 0 &amp;amp; 4 we take the maximum 4
for the second one it's 8 and 0 so we
take the maximum so the vector clock
gets update to 4 comma 841 we do a read
and just like before we remember what
the vector clock was what we did that
feed and life goes on for this side but
now we're ready to do a particular check
we say okay the question of does the
right of X happen before the read of X
gets translated to this equivalent
problem of is the right of X vector
clock pairwise less than or equal to the
read of X vector clock so we take the
two vector clocks that we've seen before
and we do a pairwise comparison and we
say is 4 less equal to 4 yes 0 less
equal to 8 yes the answer is yes and the
this pair of accesses to not race so now
it's very simple to go to the first
example the racing example we start with
the same vector clock 4 comma 0 we
remember it we continue and thread T 1
we have the same vector clog 0 comma 8
we do a read we remember it and we're
ready to do that check so the check is
is the write a vector clock pairwise
less equal to the read vector clock and
here the answer is no because 4 is not
less equal to 0 so because of that we we
can we can say that there is
zone X so this is e happens before
approach to data rate detection and it's
the the primary benefit is that there
are no false positives and what I mean
by that is when this tool generates or
issue the race report for you you can be
assured that it is a real data race so
developers like this sort of property
may be the catch is that there is some
work that we have to do that is we have
to instrument every synchronization
operation that goes on in the system if
we happen to not see the acquire release
pair then we will false report arrays so
that's that's something to to keep in
mind and in all this is not new this has
been ongoing research for the past
thirty years
and a lot of work has been done to do
performance optimizations for the vector
clock algorithm to minimize the amount
of like updates and Burgess and whatnot
you have to do and also there's a lot of
work that went on to have a rigorous
mathematical proof that whatever vector
clock algorithm you invent that it
actually corresponds to the happen sort
relationship in a in a tight way all
right here's references standard
references you can go go read if you
want to learn more so we didn't go out
and reinvent to re-implement vector
clocks instead we took vector clocks
that are already implemented by thread
sanitizer and this is a fee plus plus
beta rays detector from the llen project
it's fast
it's optimized it's maintained and in
its typical usage what you do is you
take your native code and you compile it
with PSN mode turned on and it will
embed a hooks into your object code what
and then when you link your native code
into a final executable the TCN runtime
engine comes along with it when you run
the program in the background there is
record clock being generated and version
updated and checked and on a bad check
that's when the tool will issue a data
race report so that's how we use it
for our purposes we need to ensure that
the third sanitizer engine and the JDK
play well together
and this has not been an easy process
because both are complex runtimes that
each try to assume ownership of the
process resources which I won't go into
here but you can ask evader the basic
though is the that TS and provides c
hooks that are conceptually very simple
and what we do is we embed those C hooks
into various places in the interpreter
in digit so some may ask why we haven't
pursued by code rewriting and basically
the answer is native code so at Google
every Java program pretty much standard
comes with tons and tons of native code
and both the bytecode and the native
code the Java code in the native code
keep evolving very quickly so there's
never like a stable point where it's
like Ruby safe so it's important for
developers to be able to catch data
races in both the Java side and the
native side because of the no false
positive guarantee that we want to
provide we do have to see all the
synchronization and that means not only
Java blocks but also the native locks
and also the jdk locks in etc and it is
the case that in some programs Java data
is protected by native locks and vice
versa sometimes Java data is protected
by native on the idea native lot name
data is protected by Java locks and Java
a Java data is portrayed by data blocks
it's not common but it does happen and
if you have a bytecode rewriting
approach you're not going to see that
all and and because it is the reality
that a lot of Java threads will wander
through Java and J&amp;amp;I invade it and all
that we do want to see the full stack
trace for usability purposes and we
believe that having an embedded approach
gives us more performance Headroom to
work with later when we do want to like
actually tune this tool for performance
reasons okay so kind of the basic
approaches you want to tell piece down
everything and let's start with how TCM
manages metadata like the vector clocks
and whatnot
the basic approach $0.40 internally is
when you allocate a c object or C++
object you have the pointer and that
pointer is literally the key at which if
you can access its metadata
for efficiency now since we have
internal access to the representation of
a Java object the Boop is a natural fit
to give TC and attract so life seems
good but then objects move around and
and this is problematic because TC
typically doesn't really deal with C++
objects that kind of assembly move
around so what do you do we tell piece
on about the object lifecycle so on a
new we tell all here here's it using an
object when it gets collected we say
okay don't worry about this object
anymore
in particular when when GC happens we
tell TCM that hey this object move from
place a to place B it's an
implementation note that we don't
instrument the GC itself for performance
reasons we don't want to touch the hot
piles in the GC but we can pretty much
accomplish this in a post GC phase
synchronization so we take these two a
callbacks called peace Angela plier and
peace and Java release and you just
pepper them and throughout the
interpreter energy appropriate places
and it happens that there's a lot of
subsystems to go cover so there is the
optic synchronized area there's a
volatile there is a placement
interpreter in JIT there is a job detail
concurrent or saving Thierry locks it
goes on and on so there's you want to
have a good coverage of all of its
Equalization and finally the reason
we're doing all this is because we want
to track fields so T's envied and he
said right
aproximately are the are the right
callback to use here and most you'll get
instrumented except we want to cut out
instrumentation when we can so volatile
for example are our have synchronization
semantics so they will never race you
know in the technical sense so so we
don't bother tracking the weather
volatiles race or not we do keep track
of their synchronization
similarly final fields never race so so
we just skip those and for special lazy
initialization idioms used the the tool
can't really understand it that well so
instead we go annotate those in the
source code and those annotations are
tracked and and we make sure to not
track certain fields
so I said let's see I'll keep down
everything but you don't want to tell
about like everything everything we
won't tell it like almost everything and
the things we don't want to tell ki Sam
are things like what's going on in the
VM itself
so in particular we don't take lip jvm
data so and and compile it with what
piece and instrumentation and we what we
don't do is we don't make the save
points or think the internal VM you text
is visible to teach them either and the
reason is if we did that it would expose
a lot of internal synchronization that
it's not visible or usable to user code
and what happens is there is a lot of
parasitic synchronization that gets
tracked by it happens before or they're
tracked by the tool and suddenly you're
not going to see a lot of voices
finalizes tend to be annoying and we
decide to ignore those for now and the
precise behavior of final field
references turn out to be quite tricky
to track properly so we cheat a little
and and tailpiece end up those are
volatile and it's actually too strong it
introduces too many happens before edges
but it's good enough for now so that all
gives gets us to the place of okay here
is a data race and and now we can clan
talk about ok how do we get this sack
report in particular how do we get the
top to stack reports for example the top
two frames so the basic approach is what
keith on does is for native code it's
going to take program counters as 8 byte
tokens and maintain what they call a
shadow stack so upon function call and
entry or entry and exit the two
callbacks are tea set entry and exit and
you feed it the program counter of where
you came from
for memory access you just remember
where which particular program counter
that you're doing that read or write but
Java doesn't really have a program
counter per se so for Java what we've
done is take the j method ID and the by
coding dex and smash them into an 8 byte
token and give that to TC on to use and
that turns out to be 4
good compromise so this is what T sin
does most of the time it's just
recording a recording recording it's
just remembering a lot of little program
counter tokens to to remember where it
was now when it comes time to generate a
report there's a real data race then
what it does is extract the shadow stack
and start symbolizing each of program
countering and in each frame when it
sees a Java program counter we it goes
ask the JVM to please turn this into a C
string and because races can happen at
arbitrary times we have to be able to
regurgitate or symbolize these tokens on
demand so we can't wait till like the
the VM is at a nice place or anything so
to put this all together again we have
this receipt race if we kind of look at
the hooks that are in play here we see
you may see something like keys and
enter and exit the right I was
instrumented with a TC and write
function and the the curly braces
indicate a J method ID and if I could
index pair so so so this is something
like that what might see and the
operation T sin does to to get you that
stack trace will look something like
this where when we do the read and it
say okay there's a race on X it first
queries the shadow stack and says okay
the top of the stack is M comma 5 and
the next thing is K comma 4 then it says
okay please symbolize M comma 5 and and
the function goes and queries internally
the JVM what with it what the nice C
string is and it'll say sub Lexi iff 1 7
and K dot 4 will do the same thing will
go at the JVM for 4 again for non Java
program counters you simply it has its
own internal routine to symbolize so the
impact of this tool across school has
been that it's in use by various teams
across Google the typical way one uses
it is you take a particular VM flag and
you stick it in your automated test
and I kept on continuous build and test
it'll it'll exercise this this
configuration it is the case that the
native only mode for thread sanitizer is
still the most widely used one that is
we don't turn on filled Java filled
tracking as the default and that is due
to 10-15 years of job development
without a tool to catch these races so
there is a it's pending at eco cleanup
so that we can just turn this on by
default when you use this tool
it forces cleanup of dodgy code as one
might expect but this has turned out to
be a slow process even for our own team
where we do understand these things very
deeply on the other hand it does has a
force a lot of fixes in third-party code
so that's a side benefit and the other
interesting phenomena is that running
this tool we've we've seen a steady
stream of races databases exposed that
are internal to the JDK and typically
though when we go at the experts they
say yeah that's benign don't worry about
it
so so typically what you do what we do
is we suppress them because the reason
we're doing the tool is because users
want to figure out what so what the
databases are in their in their own code
not necessarily what all the races are
in the jdk itself so ongoing in future
work we have the interpret all working
see one is in progress and typically our
our concerns are is it correct and is it
usable and then and then we'll chase
performance as I had alluded to final
fields and finalize errs aren't really
going to be we haven't quite dealt with
those yet so that's future work JDK 9 is
coming and so there's a Ford port in my
teacher and we'd like to perhaps think
about up streaming this one day so that
it's a big patch though so we don't
really want to have to carry forever
okay so that was my talk and I when I
hand it over to Jacey
- so I get trust next so once you've
actually played with Fred sanitizer and
you made your code totally Fred safe and
with no other problems
a common question becomes what is
actually being allocated by the heap and
often I find that when you try to
understand heap in general you'll start
having questions of quantitive how much
heap am I using at some point you're
going to go well this is either okay or
not and when it's not okay you're
tensing okay wait where is what is
allocating what kind of objects how many
etc and the reason we started this was
basically we want to understand memory
usage using sampling to have a very low
overhead and this whole my part of the
talk is kind of like a mix between going
down memory lane of 10 years at Google
with an implementation that's a decade
old and the work of actually up
streaming this with a JEP and open with
a serviceability dev email Fred that's
pretty long by now so I'll go through
that and so basically the main features
of what we want is a very little
overhead 1% or 2% the reason we want
that is that we like having this on by
default all the time and so people can
actually go and figure out straight away
what is being allocated what is what has
been garbage collected types of objects
at runtime with no cost and we want to
be able to pull that directly and the
information that we provide to the users
is accurate accurate stack traces so of
the objects their sizes how many of
those samples of those objects that we
see which ones are live which ones are
garbage and I'll go through that as I
talk so I always do a couple of
preambles of presentations so that
people know what to expect I don't have
any benchmark numbers yet of this
internally we know it's one percent two
percent I'm going to be working in the
near future of getting numbers on the
service fitting dev threads so that
people can see what to expect we don't
expect anything else than 1% 2% that's
what we see internally but still if you
do want more numbers if you want know it
now just contact me I'm currently on
paternity leave officially but send me
an e-mail I'll handle it
back second one that I do and I
hesitated putting this one but I thought
it wasn't could be an elephant in the
room what about a JFR event there are
JFR events that handle like t lab full
events that would tell you this is the
object I was allocated the T lab was
full the thing is that we like having
the means that the user can say I want
to know every megabyte in average and
that average is the important concept
because we don't want it to be an exact
megabyte because if you have a recurring
pattern of allocations you'd always hit
the same object in that pattern we want
it to be moving around that megabyte
area so that you can actually really
sample correctly the different objects
and we don't want to be defined by
whatever T lab is doing for its sizing
the we want to preserve the stack traces
for garbage-collected objects we want
you to be able to say hey what is what
has been garbage collected what are the
objects that are being churned by my
system and so and we wanted to have an
adult way of being able to get those
sample objects so and the third one is
like I said there's a jet for this and
so you can look at the jet there's an
ongoing email thread which is becoming
long since it started the story with the
internal implementation and now has
morph to where we're going towards
getting it in and I'm aiming for JDK 10
and working with Oracle to get into JDK
10 and we'll see how that goes so
information these slides is either
details of what we have internally or
what we're currently suggesting as
footage app and web revs and please stop
me if you have questions at any time
doing this presentation so hopefully
this is not too small but I want to get
all the API up in one go it's handled by
JVM Ti
it basically has a way that you can
programmatically say start my heap
sampler what is the monitoring rate so
do you want every megabyte you want
every 512 K do you want more or less
whatever what's the max storage I'll
come to that in a minute but basically
it's the fact of how many garbage
collected objects do you really want to
keep as the live objects you basically
are adding a tag and little of
information to each live object so
you're not going to be adding too much
memory in heap but if you started trying
to remember
garbage-collected object from the start
of your program well if you're turning a
lot of objects that could become quite
big and so we have a max storage dare
you ever stop which is just a stop the
stop actually for the little details
we'll remember what was allocated and
what died so you can actually pull it
afterwards and then you've got your
three getters get your life objects that
have sampled get the garbage ones that
are currently recently garbage get the
frequent ones and so that uses a tiny
little algorithm to remember object
allocations in a way that it will
actually what were the frequent ones and
then you've got release to be able to
release whatever was allocated by to get
like at garbage get frequent and the
last one that I've added is a statistics
of how often do we sample what are the
object sizes that we sampled things like
that which allow you to double-check
what is going on and allows me to easily
debug what's going on it Google
basically we have 512 K as a default for
the monitoring rate and our GC objects
is set to 200 so we remembered a 200
plus 2 garbage collected objects and
that generally has been enough to figure
out what's going on from a data
structure point of view we created in
for this web Rev and new JVM stack
traces and then basically it has a stack
trace array which becomes a JVM TI stack
trace which we create also and this one
uses JVM TI frame info which already
existed right and that basic system is
the method ID and the location in the
method and that's all you need
afterwards now what does this really
mean for the user that uses this
currently is that basically you have the
basic information but as a user of this
you still have to decode this because
you only have the method ID in your
location there is actually an
implementation GT regs
the testing system I put that will
decode this and make it into a nice text
format I'm not I currently don't know if
this will remain in GT rec only will we
make an open source slide we don't use
this I have to start talking and
figuring out what would be better
so either sampling overview really what
happens when you do heap sampling right
basically what you've got is you've got
a nice dollar job which creates a nice
little object
and at some point and I say at some
point because I have two implementations
I have the Google one that we have and
the one that we're working on and Jeff
and I'll show the differences you're
going to hit a fresh hold and you're
going to say well I want to sample now I
actually want to remember this and so
you're collector stacktrace you'll get
the object information about it and at
some point what you're going to do is
you're going to want to store it
somewhere and the storage you're going
to give an API like I just showed you
and that API will be available for
debugging profiling monitoring right
logical things and those items to want
the way we do with JVM TI is we provide
it to your user and Java job directly
and the user Java job can use that to
actually retrieve and request that data
right so then your Java job can
automatically provide this to as a
server for example to whatever users
that want to see it so as an example of
really what you've got it looks like all
the other stack traces that I've seen
today you basically if you want to look
at live objects with stack traces you'd
have a number that tells you what is the
accumulated size of those objects that
I've seen right so you have an idea of
how important this stack trace really is
it also gives you the number of samples
that have been seen right which gives
you also the frequency that you've seen
it and then it gives you evidently two
frames the method line and the number in
the file right the only difference that
we have here compared to what we have
internally is internally we do what Jhon
was talking about is we provide native
frames also and here currently in my
version it's in the upstream web Rev we
only have the Java frames and the
differences are because we don't use a
single trace we use the V frame Walker
right so what do you have for garbage
like I said garbage provides you
information about object turning it'll
provide you what is the frequent garbage
collection that has been allocated
recent really is the last two hundred
for example if I have 200 as my number
and frequent has a way of filling up
that buffer and then less and less
updating it so that really what happens
is you need a lot of objects to be
sampled before you get a new entry in it
which will have an effect of that area
is going to remain the most more
identical throughout time and you need a
lot of objects to update it so you
really will get at the end what was the
most frequent and so once we have those
I gave you a text format but internally
what we've got is we provide a protobufs
and then we provide this other tools
afterwards which is easy to pass around
or we have that text format that is
really easy for debugging and that's
basically my view of this for the JVM TI
api was to keep it bare bone and then
just let the user build up whatever they
want on the other side so like I've been
saying we've got two inversions and I'm
going to have the next couple of slides
just show you the differences between
the initial version used compiler
changes and so there was modifications
to the interpreter C 1 and C 2 to handle
the sampling and the second version
comes from a conversation of two years
ago between Jerry medicine from Google
and Tony Benn and Nancy Benson thank you
from Twitter see I I looked at the name
this morning but anyway and he was
advocating for piggybacking on T lab
system and I'll explain how it works to
piggyback on the T lab system to make
this work and so that is the current one
that I'm testing in the web Rev upstream
and so basically oversimplifying at a
JVM language summit right so don't kill
me
you've got Java bytecode got interpreter
UFC 1 C 2 and the internal version
basically changed idle oba code to your
interpreter to your C 1 to your C 2 and
basically all this code would always say
the same thing which is how many bytes
do I have less still next sample and
they would all use the same friend local
amount to be able to say okay please
sample this and that code would just
jump in to collect the data which we
common code for between 3 and at that
point you have the internal storage
which handles what we showed in a couple
fights ago so the disadvantages because
I like sparring with disadvantages code
generation now becomes complex you have
to change interpreter c 1 c 2 you're
adding code to code generation which
means you might be changing the way
optimizations are handling it
line and you have to handle each
compiler tier there are advantages
though to this which are you can easily
turn off one tier if you want you don't
have to have it on for all and the code
is directly handling the byte let's a
bytes left subtraction and I'll show why
in t lab it's a little bit more tricky
alright so those are the advantages to
this however one good advantage with a T
lab is that basically they all share
relatively a T lab version there's fast
T lab refill which is a side-story um
which I'll keep aside ask me questions
if you want and they all follow that
down into at some point a common code
that says well as T lab fool yes and if
the tip is full please get me a new T
lab and at that moment we can add our
own code in a single spot in hotspot and
say okay please double check actually is
this a real T lab end or am i wanting
the sample and how you do that
internally is a T lab has a start point
and an end point if the end point is two
megabytes down the road but I just want
to see what the sample would be in a
megabyte I cheat the system and tell it
that they'll end it actually a megabyte
away and when I come back to that point
into here I say oh wait actually I
tricked you actually let me put back the
T lab end to where it wants to be now I
know I want to sample this and so I'm
using the exact same code generation
system I've just cheated pointers and
bump pointers around and so the
implementation is quite simple and again
you can see it it's in a web Grove
available to all so then what happens is
you fall back into collect data and you
fall back into internal storage and that
code is exactly the same regardless of
where you came from the disadvantages of
this is that well teal has to be on
otherwise you don't get it and you do
add complexity of implementation because
now so historically they're even space
pointers is the t11 plantation is
piggybacking on the Eden storage
pointers and now we're piggybacking on
the T lab pointers to say oh wait we
want a sample so we got like a bunch of
things here that are happening at the
same time that could go wrong so it's a
little bit hairy advantages though very
little common code has changed its
currently at least architectural
independent there's no changes to the
architecture code and it can be turned
off and on the flip of a switch and you
don't have to recompile things in the
compiler world if you have got a c2
compiled code that is using the sampler
and you no longer want to have the
sample you have to recompile it here you
don't so that's the advantages internal
implementation details just because all
sample life objects are actually in the
list what they do is the list gets
trapped reversed at GC events to keep
the pointers up-to-date that objects get
sent like I said to a round-robin arrow
and then the user can define that size
of the array the next sample is not
always executed kilobyte like I said if
you want to be able to hit meaningful
data you don't want to hit every exact
512k for example you want to hit around
it so that if you have a pacman location
you hit all those elements and so that's
why you do that whereas the web drove up
to a lot of code is done initially I had
dumped a simplified version what we had
internally to Google I had kit put out a
couple things and then I started adding
to it and then we moved to the T lab
implementation and so now we can compare
both if we wanted to there's a lot of
testing going on I've added a lot of JT
reg tests so you can actually see how it
would be used and a bunch of variations
of what you would want so you can
actually play with it and you see
directly examples so I don't show a demo
here but you have it directly in the
j-team it folders and we had and we have
internally a version that handles
multi-threading because evidently if you
have multiple objects start hitting your
common storage that sounds like a bad
idea if you're trying to do it without
any protections then free T fret
sanitizer would yell at you and so but
for simplicity right now that's out of
the reve reve I'm going to be added as a
future item as we move forward at
handling native frames is something that
we like to have
so future potential items and I put
future potential because I don't want to
force myself in a corner and I don't
want to force people to have to accept
things in the corner whatever
so the potential is there to keep me
safe its handling more stacks as such as
the natives and make sure that we
actually get those because they're
important keeping track of additional
information so we have you have the fact
that this object was is alive but you
don't know how long ago it was and there
has been requests at least internally of
like figuring out how many GC cycles
it's actually survived and since we go
through that list anyway it's kind of
easy to bump an integer up every time
and then add user context information
it's it's a feature we have which is to
allow you to say well I'm at this point
at that point I would love to know
sample objects of this part of the code
so could you just filter out what I
don't care about right now and so that's
something that's practical and not too
difficult to add in there's also a
callback sampling system so if you
actually want to get a callback at that
moment of the sampler to do something
it's something that's tricky to do
because at that callback you're not
allowed to do a bunch of things um
Jeremy knows way more than I do on this
one but I know that I think can all have
to do Jane I calls and things like that
into those callbacks so there are things
that would you allow that to happen or
not and let the user take that risk or
not as a conversation to be had and then
like I said providing use the library to
generalize it's a deciphering because as
it is if your user wants to use this JVM
Ti as is you basically get an area of
integers and Long's and that's not
really practical so you probably want to
straight away say well if you want to do
this JVM TI API here is actually a nice
friendly user API on top and so that's
something we have to work on so I don't
know how fast I went I apologize I had
not prepared a dry run on this
low-overhead solution to understand
memory allocations is really what this
was about
we really want to provide a meaningful
way of knowing what has been allocated
how much it's been allocated and be able
to provide that as a user when you want
to try to reduce your heap usage
knowing the object calcium the stacks
have been really the most important
parts of this and knowing exactly like
an accurate stack where to find
information like I said you have a
general presentation of it in a JEP here
and then there's an ongoing email thread
on the serviceability dev depths of
disability mailing list that you can
find it's a long conversation now but
jump in so as a real general conclusion
I suppose what we try to show jehannum
myself here is that we work on tools
such as fret sanitizer that help you
understand where to our databases
databases are often really complicated
bugs and to get where they are it's
difficult with a nice stack trace it
makes it much easier and then heap
sampler is a better way of understanding
memory allocations and generally having
it on all the time has also helped a lot
often these kind of tools are run it but
then your performance can be terrible
it's not going to be exactly what's
happening knowing my job is acting
bizarrely let me quick look at what is
happening on the heap sampling and I
don't have to rerun it or risk trying
every one and not seeing this effect has
been something that has helped quite a
bit
and both tools have helped internally
really help us understand Java programs
and I believe I get to my slide
questions so I don't know how fast it
was I can drink water but but more
seriously are there any questions so so
the way it works but it's way ignoring
so T lab where basically the idea is all
of the interpreter c1 and c2 currently
have code that say I want to allocate
and when they allocate they say please T
lab give me space and that that moment
says well T lab there's no more spacing
the way they notice that is at the start
point of an end pointer any compares
them it says well end is going to be
further often my start plus my size you
know let me fill it up and what we do
internally is we change that end so we
want a for example sample at i/o know a
thousand bytes after the start in T lab
because that's where we're up to in our
like calculation we'll just change the
end to a thousand
then we put it back yeah that's correct
but when you sample do you sample just
the last object but in fact going to
know we sample the last object that
created that like mismatch okay yeah
sorry I apologize
like I said yeah you know what Jeremy
okay thank you
have a loaded question actually uh but
our folks are still one so she left
comes and go right so they are not
necessarily set a location failure or
something
for example collapse can park because
just because the thurid are doing
synchronization so uh and also some
other objects if it's bigger enough so
that you cannot allocate into T that yes
then that would be good to share heap
yes have you done anything particular
for this kind of situation just make
sure that you cover everything so the
first case I don't know that I have
handled it so probably answer's no
because I'm not aware so I'll want to
talk to you to figure out what I'm
missing the second one yes because the
second one the code pretty much says is
it small enough to go into a key lab
allocation yes then I hit it and no I
still hit it by saying well how much
would you want to allocate is it less
than next time I want to sample or is it
more and at that point we just reset it
so we handle the outside of T lab could
go to pay execute you know yeah and also
for the T lab I think the hope for is T
lab is to make it make it a pump bogging
punker quick faster yeah say allocation
is faster and without any race I don't
know what's your details of the
implementation so sort of hope yeah I
only hope that you are not going to
display that progress by adding other so
so the way it works right is you change
the end pointer and with that you don't
do it in generated code so basically the
whole generated code is exactly thing
it's
doesn't we don't touch it so you have
the same kind of let me go get the
starter mighty lab let me will my top on
mighty lab where I'm up to let me get
the end let me do top plus size compared
to the end the only difference of what's
happening here is if you turn all start
keep sampling then what will happen is
that your ends going to be changed it
won't be the automatic end of the T lab
it's going to be a different one
so you're generated code is the same run
this fast however because you're
sampling you're going to get an overhead
of that sampling and so the smaller your
sample rate makes the bigger your
overhead but the larger the less because
you would never really hit it and so I
guess what I'm trying to get to is
internally we set it to 512 K though
it's a compiler version of it not the T
lab version and the compiler version of
it has a 1 to 2% overhead right so the
theory that we've had and I haven't had
time to do benchmarking since eggs left
opportunities leaves was that T lab
version would have less than that
because we're not even touching the
compiled code anymore right so we'd be
in the 1/2 percent or even less but I
haven't any proof right now to like tell
you oh yeah look at these slides right
so that after that oh yeah please
correct me if I'm wrong so like you mix
the chilli curry fail at that time then
you do the same thing exactly said
that's a we're accepting yeah okay great
so as long as you don't sample for as
long is not sampling it doesn't change
your performance it's as soon as you
start sampling that you will spend time
sampling now you go back yeah oh yeah
that's all right that's a little okay
yes thank you Dania okay let's do one
last question
so I expected and I think you will
mention this that as you as people use
the thread sanitizer there's lots of
bugs that are present and there multiple
instant instances of that the same bug
that'll appear in any particular
execution do you remember one instance
and not flag all of them or how do you
reduce the amount of data that you're
actually writing out in terms of the
races that exist
so for example in a dynamic execution
system which is I think what you're what
you're looking at there's one instance
of a race and then there's another
subsequent instance of the same race
oh yes yes I believe what Keith and does
is remember that such a race has
occurred and and remembers to not okay
bother trying to report that again and
and the other question was when
your application code is using dicen and
approach you're generating these these
reports do you
I would imagine takes a bit of time
before someone fixes an existing race
and how do you deal with flagging new
races versus I mean what do you have
anything built behind it to prevent
program well so the thing is that the
signal is where you discover there's a
new race and the noise is where you
already know for race which you're
rediscovering again and the user of the
tool doesn't want to be reminded about
races that he's so it may be he's filed
a bug in a universe where there is a
known race and we don't want to see it
right now yes right that's that's the
issue there's a an extensive suppression
mechanism that you can you could in at
entry to that we'll just fourth key
Santa to not report stuff and we try not
to add to that but sometimes you have to
the name of community okay thank you
okay thank you thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>