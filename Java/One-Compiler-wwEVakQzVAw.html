<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>One Compiler | Coder Coacher - Coaching Coders</title><meta content="One Compiler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>One Compiler</b></h2><h5 class="post__date">2016-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wwEVakQzVAw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone I am Christian
Wimmer from Oracle labs which is the
research organization of electron today
I want to talk about one compiler and
you will see in a few slides what I mean
about that first of course the usual
disclaimer slide I hope you like what I
talked about but it's not a product yet
which means don't buy oracle stock just
because of this talk I want to start out
with looking at our all beloved Java
hotspot VM and well let's have a little
bit of a tech snapshot when you run the
NASA on JavaScript engine and see what
what do we have on our stack and I
actually split it in two parts so you
see the stack he has a left and a right
half the left one is more or less what I
see is the physical stack layout and the
right half is what I see see is the
logical stack layout so we actually the
suus of the code is coming from and for
this talk I want to focus more on the
physical side just to say that up front
what do we have on the on the stack we
have stack frames in lots of different
formats because we have of course in
read native code which is C++ code
coming from the Java hotspot VM which is
C code coming from the Java native
interface we have Java code executed on
the bytecode interpreter so we have a
stack layout for bytecode interpreters
we have just-in-time compiled a code
which and a hotspot VM you have to just
in time compilers client compiler and so
our compiler fortunately at least the
stack frame layout for these two
compilers is the same so we have a
compiled code on the stack and then when
we look at what we are actually
executing native code is either things
as I said from the Java hotspot VM or
from Chennai code and a bytecode set of
executing in the interpreter at the
client compiler is either white coats
coming from the top Java files
well it's dynamically generated by toad
coming from JavaScript do you see that's
a very colorful picture and if you look
at what the VM has to do it's pretty
difficult so if you want to run a
garbage collection then the VM has to
find out all the route pointers that are
on this stack so we have to traverse our
stack and we have to do everything three
times so we have to look at the native
code we have to look at bytecode
interpreter frames we have to look at a
compiled frames and it's not only for
this GC root pointer stuff I would say
most things if not everything in the
Java hotspot VM exists three times so
you have stack frame layout to the set
byte for them to operate the frames
close to the JVM link language
specification with oh you have local
reroute you have expressions take slots
compile code by the client and so a
compiler you have spill slots that the
compiler emits and in the native code
you have whatever the C compiler is
doing you have no clue it's text frame
size for byte code frames is variable
for compiled frames it's fixed and
native code again we have no clue if we
want to do exception handling in Java
and the bytecode interpreter first about
route pointers in the bytecode
interpreter we have to do some lightness
analysis during garbage collection to
find out which of our local rebels are
currently objects which are non objects
so we have to do some expensive
computation during garbage collection
the compiled code the compiler gives us
pointer Maps and in native code we have
explicit handles and everyone who is
locked in the hotspot VM knows it's
pretty difficult to really capture every
Java object in a handle at the correct
places and if you make a mistake then
you will get crashes at random places
after garbage collection because you
haven't updated a root pointer a same
for exception handling you have three
different implementations if you have
one to put the hotspot we aim to new and
your architecture you have to put the
bytecode interpreter right assembly code
you have to write to treat compiler
back-end
and you have to write the GCC back-end -
for the C code debugging again you have
Java debugging for the travel parts you
have native debugging for the C++ parts
so you have a lot of complexity and the
real question for us is can be
simplified and the way we want to
simplify that is by saying well why do
we need these three different execution
environments so why do we need
everything three time why not just have
one compiler and unify all of that and
get a much simpler system but those of
you who have followed our Oracle labs
research we have this project called
growl and prefer an Oracle labs and I
have to give you a few slides to set the
context yes it's also our usual
advertising slide so you can see where
that's the the commercial break here but
you also have to understand the basics
so that you can follow the end mode I'm
talking about later on
so what traveler gives you is it's a
language implementation framework where
you just write an ace team to a beretta
for every language you want to execute
so we have an implementation for
JavaScript for our for Ruby for LLVM bit
code so you ran all of that managed on
the Java or on this truffle layer
written in Java and integral RAL is our
trust in time compiler it's
automatically of optimizes and compiles
all these languages and on the other
side we also have tools so you have we
have debugging for all of these
languages and the main takeaway point
from this slide is that tools and
just-in-time compilation are not at all
related to the language implementation
you see there's no line between growl
and the languages which means when you
implement a new language you have to
absolutely nothing to do with
compilation you get that for free you
have to do very little for tool support
and you get automatically for example a
debugger or for free the whole stack is
written in Java so you can run it on the
Java hotspot vm for java 8 you have to
download our peer
which exposes the just-in-time compiler
using the JVM CI API
starting with Java 9 you can take a
stock hotspot vm which will be a big
simplification because you all the steep
hotspot changes are already India but
what I want to talk about in this talk
actually is a substrate vm which is our
reduced environment to run all these
truffle languages on a much simpler
stack and that's where we see our stick
will look really better if you're more
interested in growling truffle is
actually a workshop later today via we
can answer our questions but if you want
to dive deeper we have again total about
six hours of tutorial videos on YouTube
that you can watch on all sorts of
garage and truffle topics the way
truffle optimizes it you write a nice
team to a beretta but you write it in a
self specializing way so you provide a
feedback and in the interpreter in your
three notes and say well I have an
addition that I specialized to borrow
that's an integer addition and that's
what you do is a language implement them
but then the compilation which uses
partial evaluation is completely for
free because that's done transparently
by crowd and also going back from the
ast into a bread from the compiled code
to the east into a Beretta in case your
speculation that's not mirrored or not
the complete truth then that's also done
by the infrastructure and two important
things on this slide whether they are
relevant for this talk is that we have
this compilation using partial
evaluation I'm going to talk a little
bit about that but I'm also I'm going to
especially talk about this transfer back
to the east into a beretta or two called
the optimization and how we implement
that on on subscription
but you nearly at the end of the
commercial break just what do you get if
you run all this infrastructure just on
the hotspot VM and that's actually
saying you can go to our page on the
Oracle technical network you can
download the whole thing you can run all
that yourself so you can reproduce all
the numbers at home if you want for Java
code we drive gives you the same
performance more or less as the Java
hotspots you have a compiler for what we
call more modern forms of travel byte
codes like emitted by the by Scala growl
already beats this we have a compiler by
about 20% for languages like Ruby and R
that don't have a fully optimizing
infrastructure in the basic language or
for Ruby where you have a JRuby where
you have this layered approach on the
hot spot VM our truffle approach really
gives you big speed ups in the order of
4 to 5 X interesting native if you run a
C code of watering code on top of the
truffle infrastructure on the Java
hotspot VM you get about 85% of the
performance already that you get when
you have highly optimized static
binaries coming from from GCC or LLVM
and for JavaScript of course the
comparison is with v8 this depay is much
higher but we reach also more less
already the peak performance of highly
optimized our JavaScript runtimes but
unfortunately just running the whole
stack on the Java hotspot VM doesn't
really solve our big mess on the stick
the right side the logical source of
where the this was code that we execute
is coming from is a bit simpler because
at least with troffer what you have is
everything that you execute and the
order byte codes are actually coming
really from Java code so you never have
automatically generate a byte codes that
are generated at runtime like nasan is
doing but the left hand side our
physical stack layout is still exactly
the same as before so there's no
improvement on that side because you
still
have your native code you still have
your bytecode interpreter frames you
still have the compiled frames and well
you have a gripe compiled frames now but
have the same stick a frame layout as
the client and this way of a compiler
and hit before so running on top of the
hotspot VM is lift referral is not not
really simplifying our our life so what
we did with on the substrate vm project
is we want to run and this our our
truffle and garage tech on a much
reduced system that we can really embed
into other products and the embedding I
mean not real small embedded systems but
I mean large to other applications
written and C C++ and obviously Oracle
has a large legacy code bases like the
database maybe want to integrate in but
we want to embed our stuff that's
written in Java it's actually written in
a subset of Java because we simplify our
life by saying well we don't need things
like the enemy class loading or
reflection so we really focus on stuff
that trapper needs ahead of time compile
or this Java code using rar and then we
get a static executable and we can debug
a profile that with all the usual
low-level tools like GDP or retune or
all your viewer native code profiles and
now suddenly we get a very simple
picture when we look at our stick
because we only have code on the stack
that's coming from one compiler and in
our case that's the crowd compiler
because our whole VM is written in Java
and äôt compiled all of the VM code like
even the garbage collector is a cup code
compiled DeGraw R and we know exactly
what's the frame layout it has startup
Java code that's a ot compiled Java code
things that were used before a native
code of Chennai that's a motive right in
low-level dialect of Java I will talk
about that a little later on but it's
also still Java code our JavaScript code
first runs in the es team to operator
and the easte interpreter is written in
Java so again Java code is running and
what Java code and hot JavaScript code
of course that's then just-in-time
compiled and also grad is the
just-in-time compiler so we really only
have stack frames coming from one
compiler and one layout and also the
logical source of the stack frames is
the same because everything is actually
coming really from byte codes that are
somewhere lying around as a Java file on
your system so there's nothing
automatically generated there's no
automatically generated bytecode you can
really always go back to oh I have this
machine instruction and using all the
people at the optoma or the meter data
that we have for debugging you can
always pin down every machine
instruction to really a line of travel
code which is really good with for for
debugging and trying to find out what's
going on and if you want to optimize
languages like JavaScript what you
really need is specialization and that's
what a truffle is giving you but
specialization always requires the
optimization this is this transfer back
to the est interpreter and since we want
to run everything in this one compiler
on one stack frame layout setting we
obviously cannot do P mais a like
hotspot is doing we cannot be optimized
back to an to a bytecode interpreter we
have to be optimized back to aut
compiled a code so we have to be
optimized to code that's also generated
by the grog compiler and I will talk
about that in more detail in a second
but a few more words on how substrate VM
is working so what we are doing is we
take all the Java byte codes that we
have which is the JDK which is our
troffer language
imitation which is the substrate VM
runtime code we do a static and point to
analysis to find out what is reachable
and then we use prior to ahead of time
compile that and you get a single
executable on Linux you get the elf
binary and that's then your JavaScript
binary the building blocks our runtime
system all written in Java
sounds more like research than it
actually is because garbage collectors
have been written in Java for decades
now so IBM studied that with the trikes
IBM San lips and then Oracle Labs had
mixing VM which is the garbage collector
written in Java so there's nothing
really exciting about writing the whole
runtime system in Java so I'm not going
to talk much that I'm only going to talk
about the optimization system points to
analysis I have a few slides on that on
the end but probably I won't have time
to really go into that because I want to
spend a little bit more time than
actually on the system Java part which
is our integration with C code so that
you get some some details how we do that
why does this one compiler approach work
with
draal is the one compiler it's because
growl is really designed for flexibility
and the flexibility I mean it's really
aggressively optimizing but you can
configure everything it has this modular
architecture where you can add faces
remove faces very easily but also we are
the compiler and PBM is strictly
separated so you can take the compiler
so you can take 95% of the whole project
which is a VM independent and just write
a little bit of glue code to the VM and
then you can run growl in a completely
different set up in a completely
different VM so you can run the same
compiler on the hotspot VM you can ring
on on the substrate VM you can use the
same compiler to do a head
time compilation and dynamic
just-in-time compilation and that all
works nicely and it also gives you
really good peak performance because
it's designed from the ground up for
these speculative optimizations that you
need for languages like JavaScript and
all the other dynamic languages like
Ruby and R let's talk about the
optimization a bit to get into some
technical details here the optimization
it has many names sometimes it's called
an uncommon trip some people call it on
stack replacement it's a term that I
don't really like because hotspot uses
that term for when it switches from the
interpreter to the compiler but in some
literature also it's called on stack
replacement we call it I think the
optimization pretty consistently it's
when you transfer from optimized machine
code back to some sort of an optimized
code and what it gives you is you can
have much more aggressively optimized a
code with and not sure the optimized
code because you don't have to deal with
all the corner cases so the hotspot has
been doing that since the very beginning
for things like well you can
speculatively optimize virtual mess of
course and if you later on load a new
class that invalidates your inlining
decision for a virtual method call so
the calls actually can no longer be
brutalized then you just be optimized
and throw away the machine code and for
JavaScript that's used much more it's
doing the interpreter you profile
well this local variable is always of
type int so I aggressively compile
things as if it really is an int but
it's JavaScript so at runtime it can at
anytime and be anything else it can be a
string it can be an object and in that
case you just roll away their code and
you optimize reoptimize again so the
optimization is really the key part to
enable speculative optimizations and you
cannot get good performance for a
dynamic programming languages
about it how is it done on a java
hotspot VM you have an optimized method
on the stack here in green on the left
hand side and you have usually you have
inline methods you say well I have a
root method f1 that I compiled an inline
to other madness of f2 and f3 and when
you do Mize you actually on the physical
stack you reconstruct three are in
bytecode interpreter stack frames and
the layout of these invite code
interpreter stack frames is really given
by the Java by the travel language
specification because the interpreter is
more less following this model very cool
so it is really that on the bytecode
interpreter frame you have a area for
for all the local rivers you for all
expressions text slots and the compiler
emits metadata to map from the optimized
code back to this Java specification of
local rivers so the compiler has
metadata saying if at this point I have
to be optimized then local rebel zero
has to be filled with this text lot and
local River one is dead and local River
2 has to be filled from this optimized
stack slot and then the optimization
takes that and builds and interpret the
frame and to make that work actually
without an interpreter we have to switch
a change our right hand side here
because we don't want to be optimized to
interpret the frame we want to be
optimized to a compiled ring but it
turns out that is actually not too
difficult because we can just a compile
in these frames this the optimization
entry points with garage r2 as the
compiler and we compile it in a special
mode where we you could say disabled a
lot of optimizations that one a our
biggest optimization that with this
April is is method inlining because
obviously when you really want to
reconstruct individual frames it would
be far too complicated to match up a
mess up with an arbitrary number of
inline frames with some other
arbitrary number of inline frames to
release a value have for every message
you want to reconstruct on the frame you
have a ahead of time compiled a trial
method for that and when we do this a
special compilation for what is the opt
entry points we generate the same
metadata that we also generate a for for
the T optimization to us so what we now
have is instead of matching compiled
frames using this meta data directly to
interpret the frames they actually have
a two-way matching because both the
source and the target are now have have
a matching based on the Java language
specification and then you can do this
mention in say well if I want to
reconstruct my method f1 on on the
target stack then I know where local
rebel zero is actually should be in this
text slot so how do I fill this next
slot I say well I fill it with local
variable zero which in my original
metadata comes from this tech slot so
instead of a one-way matching you have a
two-way matching but it's not not much
different
that's all what I just said let's look
at a little bit of an example for that I
have a simpler test case a synthetic
example where we say well we have a java
method it's named in proxy needed for
reasons you will see immediately and we
assume we want to actually
deoptimization
why do we want to be optimized after the
fields do it the way crud works is that
every instruction that actually changes
some memory state might be at the
optimization entry point because the the
the rationale is that as long as you're
executing code that has no side effect
on the memory you actually you don't
really need to be optimized exactly at
at the place in there you can savor
you can always be optimized tuna
immediately after the last state
changing instruction and then just me
execute everything that you did in the
optimized code so you can reduce the
number of the optimization entry points
that you have by saying I don't have to
do my's exactly to the point after I did
an addition or after I did a
multiplication because you can easily
redo that after the optimization so you
can always the optimize back to a point
immediately after the last state change
because you cannot undo memories tools
you cannot undo arbitrary method calls
without without transactional memory
that would be too complicated but you
can easily undo things that only affect
a local rebels that's why when we
actually do a really normal graph
compilation of of this method what we
get is that's a little snapshot on of
bralla are here on the bottom left side
you get control flow nodes where they
have a mess of start node then you do to
a field and then you to a return because
these are the instructions that you
really have to execute in control flow
order and you have things like the
multiplication these are instructions
that you can freely reorder so these are
floating instructions that are not in a
control flow and draft as the obvious
thing you have the same multiplication
twice so you have to permit the X you
multiply by two you multiply by two
again so you get a global value
numbering and in the compiled code you
only get one multiplication and the
output of the multiplication is both
written to the field and return what you
have in green in this and cria graph
these are the frame States for the
optimization you say if you want to add
e optimized to the beginning of the
method then you need to be optimized to
the method proxy needed at spite could
index zero which is the beginning of the
method and when you optimize to this
point then you have to fill the first
low career row actually with this
parameter because
that's the method parameter is always
it's also there the first local rebel
according to the to the Java
specification and if you after this
field store then you cannot be optimized
to the beginning of the method again you
really have to be optimized to the point
after the fields door which is then by
to the index six but still the first
look of the thing the stage you have to
reconstruct is you have to reconstruct
the first local rebel has to be filled
with the parameter so you have two
different the optimization points where
you can be optimized so we have to
support these two Optima entry points in
our the opt entry code and we do that
really explicitly in graph by saying we
insert fixed control flow note saying
well we have a do entry point here and
we have another table entry point here
and they both have their own frame state
again saying if you'd be up to here then
yes you have again by could index 0 and
if you optimize to this frame state then
you'll be optimized to by to the index 6
so then let's assume we do opt to this
bytecode index 6 at this point what
happens is to look up this d of entry
point the steps are you have optimized
measuring code with a program counter
you look up the meter data to find that
this frame state these frame states as
well it's this method with bike route
index 6 you look up the deopt entry
point for this method which will be this
graph you look in this graph for the
metric frame state of a deopt entry with
bytecode index 6 which would be this
point and then you know where a distance
that this is the point you have to be
optimized to there's one little thing
that we still have to account for us we
have to make sure we don't optimize
these DOP entry points too much because
now
execution really continues in the middle
of a method so when we when we start
executing at this point actually the
part previously in the message here are
not run again because well we are coming
actually from this point so we have to
make sure that here we don't do any too
aggressive optimizations because we now
have to really run a dis multiplication
here again because our Java level frame
state only says that we have the
parameter X available so there's no way
that during the optimization we can
actually capture and store intermediate
results like X times two that have not
been stored in a local rep so we have to
make sure that we really reacts acute
that after the optimization and they may
be adrift that is that after the day of
entry we just insert little proxy graph
nodes into the grey graph which make
sure that the value numbering which
growl normally does automatically does
not happen so now grab can just normally
compile this method but it knows that it
can never remove the second
multiplication because actually the
input argument is different and just by
wrapping the input argument into a proxy
placeholder node we make sure that a
growl does not do optimizations that we
don't care about so we don't have to
reconfigure browser to compile aut NSD
of the entry points we just omit a
little a few extra nodes and then
everything falls in place automatically
second thing I want to talk a little bit
about is our low level system Chava pads
so when we write everything in Java when
you write the garbage collector in Java
you also have to make sure that you can
really do raw memory exists because
we're if you write the garbage collector
you you cannot do that just by following
a Java travel rules and we use the same
infrastructure to integrate with C code
and then we have pre-existing Java code
like the JDK that that violates you
could see our constraints because it JDK
uses reflection and dynamic class
loading then we just fetch this Java
code and say well we just substitute a
fewer JDK missiles what is the system
Java versus Chennai what's the
difference the difference is that with
Jane I you write a custom z code so when
you interface from Java to see you have
to write C code wrappers so the C code
knows about your Java types and Java
objects are also passed to this secret
and Java codes are passed to the secret
which means you have to wrap them in
handles and that makes things
complicated because now the garbage
collector has to correctly traverse
these handles in contrast if system Java
you write all this custom a glue code in
Java and the Java code now knows about
the C types so there is no need ever to
pass Java objects actually to C code
which means you don't have to deal with
all the complicated handle stuff because
if you really have to call the code and
at some point you have if you want to do
a print line then at some point you have
to call into the OS kernel now to really
do on the output but that's a code when
you already give out only native memory
and so the garbage collector does not
need to deal with these frames the only
requirement for that is a low-level a
low-level raw pointer type that are
actually a machine word type currently
we fake that using
an interface but I'm really looking
forward to value types in Java because
then we could just say well we have a
value type for an unboxing word size and
things will look much nicer but until we
have that we actually import and fake
you could say a see information using a
Java interfaces so for example if you
say well you have you want to import the
the time spec C structure which is used
by buy time functions so you have this C
code for the structure you have to
structure fields for seconds and
nanoseconds and you import these
structures in these fields to the Java
side using an interface that the word
type interface and it has to exist
semesters so it looks really nice on
Java code because now you can write a
low-level Java code like we have here in
the bottom left corner that actually
calls out or to see code but you can
still argue it it looks like like Java
code and you get some sort of type
safety you get now because at least you
have to do explicit cast between
structure types if you need to but you
don't get any memory safety because you
can still of course access null pointers
raw memory as as you want but if this
little snippet we say well we have the
time spec structure imported to Java we
want to get the size of it we want to
allocate memory on our stack frame so
that we can pass the pointer out to AC
function the C function returns and then
we access the the elements of the C
structure and sum them up correctly and
return them so let's coat that with G
and I you would usually write a in C and
we write it in Java
a few words on points to analysis if
you're interested then talk to me or
look at the slides later we also use
graph as a static analysis framework
that's the quick summary and because
growl and the hosting hotspot we already
know how to pass byte codes how to load
classes so we don't want to implement
all of that so we use growl also as the
basis for first static analysis and I
don't go into these details a few
results actually I have a little
JavaScript benchmark that we can talk
about but I think I want to show you
instead one we start really life and
that's what you really get is improved
startup performance so when you want to
start up normally a JavaScript VM retn
on top of the hotspot VM you have to
start up hotspot which takes half a
second and you have lots of memory that
is allocated instead what what we are
doing the substrate VM is we are
building a native executable and I built
a special executable that has a flag
enabled to to sum up or memory
allocation so that we see at the end
well how much memory do we be the
executor to be really allocate and I
start up my JavaScript engine and you
see the prompt really comes immediately
so it takes only milliseconds to startup
and it really is JavaScript so you can
do something if I exit my JavaScript um
prints out what you've allocated but you
see well we executed JavaScript code we
started up a full Java Script VM
everything and we totally located 90
kilobytes of of memory so that's really
the same level that you would get if you
start up v8 natively what might be I'm
already less than what you get with with
v8 because we can do a lot of actually a
lot more authoring when we build the
executable you see where are we running
JavaScript and I was only running Java
code here so everything that in this
JavaScript engine
whole runtime system is written in Java
but we still gets the the same
performance the same memory behavior as
we get with with native code and if this
little bit a bigger benchmark we just
sum up numbers and I have a couple of
slides here you see well if we run only
very few iterations then we we do a
stopped-up benchmark there we see a
substrate vm in this line no it's
actually starting up in more or less no
time so you get the same startup
behavior as sv8 at some point and of
course we start compiling so on the long
run if you run really many iterations
you get the same peak performance that
you get also when you run a truffle on
the hotspot vm and so this benchmark
actually the graph compiler is doing a
much better job than then all the other
javascript engines because you see here
I have actually object allocation and
object access in my loop and growl has a
very aggressive escape analysis so growl
can really reduce this loop into just
summing up numbers versus all the other
JavaScript engines have to do a memory
accesses in a relocate duration it's why
the peak performance is so much better
okay summary substrate game uses one
compiler approach which really
simplifies the design because we have
one code to walk the stack and it's
really simple because we have only one
kind of stack frame we have one kind of
exception handling we have a long
compiler that we need to put two new
architectures we have one we can look at
our whole stack with one a debugging
tool like gdb and one profiling tool
like medium we use Braille in many
different configurations for all levels
so we compile Java code we compile our
low level system Java code we compile
JavaScript code we compile Ruby code we
compile our code all with the same
compiler we compile the optimization
entry points with GWAR and we use growl
for aesthetic points to analysis and a
growl really is flexible enough to
support all of these use cases because
of the clear compiler VM separation that
you can just plug in new vm backends and
because of the configuration of faces
all of that is of course not done by me
alone so we have a pretty big team at
oracle work working on all levels of the
growl and trapper stack and we have lots
of university collaborations a small
army of master and PhD students helping
us out too if that that's all our hats
so thanks for your attention and I'm
happy to take questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>