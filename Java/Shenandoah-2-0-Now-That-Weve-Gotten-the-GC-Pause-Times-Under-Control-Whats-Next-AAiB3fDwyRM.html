<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Shenandoah 2 0: Now That We’ve Gotten the GC Pause Times Under Control, What’s Next? | Coder Coacher - Coaching Coders</title><meta content="Shenandoah 2 0: Now That We’ve Gotten the GC Pause Times Under Control, What’s Next? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Shenandoah 2 0: Now That We’ve Gotten the GC Pause Times Under Control, What’s Next?</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AAiB3fDwyRM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right it's 5:30 so I'll get started
I'm Christine flood I work for Red Hat
we've developed a low pause garbage
collector and this is a little bit about
garbage collection a lot about
Shenandoah and then a little bit about
what we're doing in Shenandoah 2.0
Shenandoah 1.0 is out there for
everybody that uses rail or Fedora or
any Linux distribution built off of
those but it's not up an open JDK yet so
I have these little star slides if you
like get lost and wait for the next star
you can get back on track and it's also
a queue to me to breathe because
otherwise I talk way too fast so if you
were here for the last talk we're gonna
talk a little bit about traditional semi
space collectors and then we're gonna
talk about region based collectors like
Jiwon and Shenandoah we're gonna talk
about the Shenandoah however of them
we're gonna give you some results I'm
gonna give a brief demo of a visualizer
that lets you watch your keep while
you're allocating and garbage collecting
and it can give you a clue as to how to
better optimize your garbage collection
algorithm for your application and then
I'm going to talk about what's next what
are we working on now that we've got
this working so traditional semi space
garbage collection algorithms if you
were here for Gil's talks you saw a lot
of this I'll give a little bit more
detail I think so they break the heap up
into young and old generation and they
use a card table to keep track of the
pointers from the old to the new so here
you can see that we've created an object
foo in the old generation and it points
to bar as soon as we write that pointer
to bar in we have to make a mark in the
card table and that's how we keep track
of all them pointers from old to young
and this is for serial parallel part of
new CMS they all keep track of a card
table this way to keep track of the
pointers so they can collect just the
young generation in isolation so if your
objects are dying young this works great
because you're collecting all of the all
of the dead stuff out of the end
generation without ever having to look
at this big old generation
however we've moved on to region based
collectors like Jiwon and rather than
having a dedicated old generation and a
young generation they divide the heap up
into regions and we allocate in regions
sometimes we have objects that are too
big to fit in the region and those are
humongous and they're dealt with
specially but mostly we allocate objects
and regions and then when it comes time
to garbage collect we can copy the live
objects to a new region so here we've
got two regions full of objects some of
which that are dead we copy the live
objects over and then we can have those
two objects those two regions free to
continue allocating in and all of our
objects have been compacted so we don't
necessarily have to have the 50%
overhead that a 70 space collector would
have so g1 it's the best of my knowledge
the last time I looked at it which I
feel right I should probably should have
given a caveat awhile ago I worked on g1
when it was an original just an idea and
I haven't really looked at it lately but
back then we had a card table to keep
track of old to young references and we
had remembered sets for keeping track of
old to old references so if you have a
pointer from food a bar in the old
generation you have to do the mark in
the card table like I said and this says
that we can collect all the young
regions by just looking at the young
regions or root set and the card table
but if we make them our region pointer
from an old region to an old region we
have to store that location in a
remembrance set and that's how we can
collect just the old region that
collects foo without looking at the
entire Hiba so this is sort of giving
you a background so when I talk to you
about how we're doing this in China Noah
it'll make sense another way to look at
this is this is this is sort of
demonstrating what the garbage
collectors are doing so we have these
card tables and these remembrance sets
and that allows us to do things like we
can have a young generation pause and
another young generation pause then we
can do the
concurrent mark and figure out all the
stuff that's live that's not in young
generation regions and then we can do a
mixed collection in g1 so Shenandoah is
different we still do the concurrent
marking that they do but we copy the
objects of all the Java threads are
running so here you can say we still
have stopped the world pauses but
they're much much shorter so concurrent
marking is hard and I think if you were
here for Gil's talk he described it as
pointers behind you I'm gonna describe
it a little kind of the same way so if
you have a and a points to be you mark a
and you mark B if after you've marked a
that pointer moves down to point to D
and the point of the pointed to D points
to B we now have this object D over here
that's live and is never going to get
marked because the marking the the front
line of the marking has moved past it
and so what snapshot at the beginning
does is it says that anything that was
live at the beginning is still alive
so we know that D is alive sorry and we
know that D is alive so we add D to a Q
and we know that we can come back
afterwards so whenever you overwrite a
pointer you stash the old value on a q
and that way you can make sure that
you've marked the entire heap anything
that's newly allocated is considered
live as well so here we have the stuff
that's unique to Shenandoah which is
concurring evacuation so you have a
problem and the problem with concurrent
evacuation is that you're competing
between the garbage collector that wants
to copy objects and the Java threads
that want to write two objects so here
you can see that the GC thread wants to
copy a with its field of Fred and the
javathread wants to write to the object
and it wants to update the thread to
point to Bob so if you're not careful
and you don't do things carefully you
can end up with a copy of your object
that doesn't include the latest writes
and the way we solve that in Shenandoah
is that we copy the objects first and
then we perform the right
so here the javathread has to copy a and
then perform the right so the other
problem with concurrent evacuation is
keeping track of which copy is the real
copy of a right because I now have two
copies of a in my Heat so I have an
indirection pointer this is probably the
most controversial part of shenandoah
well maybe they're not being
generational exactly but by adding an
extra pointer to an object it allows us
to move the objects while the Java
threads are running just by updating the
indirection pointer so I could have 17
things pointing to a and once I cast
that pointer to point to the new a prime
everything else will read through the
interaction pointer and get the new copy
all at once and the way that happens is
with a read barrier so in the regular
compiler you will read the location of
foo so you know that foo lives in a
certain address and say ok I'm going to
read that address plus whatever the
field offset is which shenandoah what
happens is you read the field before the
pointer to foo and that tells you where
foo really lives so most of the time it
just points down to the object below it
and warms up your cache for you but
sometimes it means that you have to go
somewhere else to find foo and then you
have to go to offset from that somewhere
else so results this is the
elasticsearch benchmark that I wrote
it's not out there for everybody to try
so you can be a little skeptical but I
to be honest I indexed a big data from
common crawl which is the source of big
data I looked at all the Wikipedia
queries and I index them using elastic
search and you can see that here
shenandoah versus g1 I don't want to
throw anything at you one I was on the
original g1 paper g1 is a great
collector for what it does but in terms
of Enda and run time it beats
and it will always beat us because we're
going through that interaction pointer
except on very specialized workloads but
in terms of total pause times were
significantly shorter we have paused the
world for all of five hundred twenty
milliseconds whereas they pause the work
well for four seconds our average pause
times are in order of magnitude better
our max past times you can see here 679
milliseconds versus 30 milliseconds
we've we've succeeded in solving the
positon problem we do have more pauses
so right now the time to save point that
Gail was talking about is a problem for
us we have found in some cases that the
time to save point is longer than the
time where we're actually doing GC work
and we have some ideas for how to
possibly fix that but I think that when
you get to that point you know that
you've succeeded here's my favorite
benchmark and it's Gil says he's never
seen something like this in production
and probably not but I have talked to
people who have this problem right if
they have a they had at least recently
used web cache so they would they would
have a URL that pointed to like a
gigabyte of data and they would fill up
a table with URLs and then when they had
to add one when there was no more space
they got rid of the oldest one and in
that case generational garbage
collection losses it's one of those
cases where you're so used to this tool
working for you and then you come into a
situation where the weak generational
hypothesis does not hold and then you're
like what happened oh my gosh so for
this case shenandoah can really kick
butt because we don't assume anything
about the lifetimes of our objects so
here our runtime is significantly faster
our pause times are significantly faster
it's a total GC pause times 17 seconds
versus 210 milliseconds
our max GC pause of 235 milliseconds
versus 12 milliseconds and we even have
fewer pauses because
our puzzles are more productive right we
have all the stuff that's all dead in
the Orion regions that we can get rid of
very quickly we don't have to do as many
of them so just to be fair I wrote a
benchmark that did generational trees
where you allocate one megabyte trees
and discard them this is sort of perfect
for the generational hypothesis because
these things are dying in the young
generation and never getting promoted
we're not that much worse
we're a couple of seconds worse our GC
pauses are still better because we're
doing in the evacuation concurrently but
this is sort of a worst case for us and
you can see that our max pause time is
only a little bit less than theirs and
we do more of them so our average is
lower that's how come we end up with
less total but basically this is the
worst case for us so status we are
shipping if you have rel 74 or better or
Fedora 24 or better all you have to do
is add dash X X colon + u shenandoah GC
is this being recorded okay you're mine
um and you can go into our wiki page to
get more information about it about how
to play with it and there's a lot of
information that alexey shavelev has put
up on how to to tune it if you want to
tune it but it runs pretty well right
out of the box with just that argument
okay the visualizer let's see if I can
do this without losing everything
okay ah that's not getting the sweater
there
okay it's very pretty on my screen but
that doesn't help you all right there is
a visualizers out there that you can go
if you go to our website and look at it
you can download it and you can watch
what's going on
so what's next this is the alleged topic
of my talk and it's probably gonna be a
little bit short feel free to ask any
questions you still won't have yes I
have been could you come up and help me
figure out why I lost my display and I'm
gonna say this um I got I got taken out
to the woodshed I originally stated it
was for 16 megabyte heaps and more but
my current language is the same
predictable pause times from two
gigabytes to 200 gigabytes so it the
what what when you look at the pause
times rate it's a second per gigabyte
perhaps that's just a rule of thumb that
was in the previous talk but if you can
sustain a pause time for the size of
your heap right if you have a 250
megabyte heat you can do a full GC on
that heap in milliseconds so you don't
really want to pay the overhead of
shenandoah so it all depends on where
your pain point is at what point are the
pause times too long for you to tolerate
and that's when you want to look at
something like Shenandoah
CMS is not a compacting collector we are
we are we're compacting things
concurrently so we do not have that
issue I cannot say that we never hit a
full GC but we okay so this was another
argument that I had this one I won we
have the ability to do a full GC if you
need it if everything is is dead but if
you are in a process of concurrent
marking and you're almost done
we'll finish concurrent marking start a
concurrent evacuation and then let you
guys go so stopping the world is only a
worst-case scenario or if you call
system that GC then we will give you the
ability to do a stop the world and have
a completely clean heap before you want
to start a benchmark or something but
full G C's should be they could have
been we could have left them out we
could have left them out and throttled
the allocator and kept going but we I
personally think that you rather have a
full GC than the throttled allocator but
if you'd rather have a throttled
allocator speak up and let me know
in
okay we have a flag that lets you do
that I don't remember it off the top of
my head but it there is a flag that says
that fools you see is not a fold you see
it's a concurrent chasing and it will
throttle you mutator because your beauty
toes are stuck they can't allocate but I
would argue that if your mutators can't
allocate you know they might as well be
stopped right if they're stopped at an
allocation point they're stopped
regardless
if our concurrent mark is in progress so
we've gotten some data already
we will not throw that away we will
finish our concurrent mark and keep
going it's only the case that if for
some reason now I'm talking out of my
eye
we don't need to do full GCS but they're
there and you can disable them if you
want to I guess as the is the bottom
line
that's true but so you say you don't got
you guys don't do the fool she sees ever
and I'm wondering does that harm your
benchmarking numbers because you never
have a fully compacted heap okay
all right um where was I
you get to the right slide
so and I lost it again I'm sorry I can't
we can't stay on the right slide and I
have 11 minutes so being don't fix this
we'll talk to you a little bit about
where other than having I'm gonna show a
picture of this in a minute but rather
than having a card table
I want this slide to be the one that we
see okay this is and when you brought it
up before it came to the inside and so I
went back to here to find where I was
going and then I lost it if you want
this slide to be there yes
what we have is sort of a very very
coarse one one item entry that we keep
to keep track of pointers between
regions and we call it a connection
matrix so we don't have designated old
regions and young regions what we do
have here we go so here when I talked
about g1 I talked about the remembered
sets in the card table and how you have
different things for keeping track of
what points to what we have one thing we
call a connection matrix and all that
keeps track of is pointers from region
to region so we still have a rate
barrier at the rate barrier rates a
single entry in the connection matrix to
tell you these regions are connected so
here you can see that region 3 points to
region 1 and I've just written a pointer
into region 5 and that just means that
we have to write a bit in our connection
matrix to say that they're connected and
this gives us the capability to choose
regions that we want to collect and
collect them independently so if we want
to collect region 3 we have to go
through our route set and we have to
scan region 5 as a route set as part of
the route set to collect region 3 so we
have new terminology that we're
introducing I can still remember back in
1998 when Steve Heller said this is
parallel and this is concurrent even
though Hans Bowman said it the other way
around years ago but this is what we
want to call we want to call our
region's to collect our collection set
regions and we want to tell our regions
to scan our roots our regions and so
we're just we're just
treating our regions as another part of
the route set so that we can do more
collections so here what we do is we
assign an allocation epoch to each of
our regions that says when the last
allocation happened in that region and
that gives us the ability to do pretty
much any kind of collection and strategy
that we want if we want to do a young
collection here you can see region if we
want to put Region seven which is the
latest allocated region in our
collection set we can do it and we have
one pointer that points to it from
region six so all we have to do is scan
region 6 and then we can collect region
7 now this is more scanning work than if
it was just a card table right a region
is bigger than a card but it's a lot
more flexible because it allows us to
treat old and young regions exactly the
same and it allows us to define
strategies that work for a young
generation for an application that needs
a young generation or we could also do
it for an old generation we could
collect the oldest generation by simply
scanning the one region the points to it
and so or we could go with the regions
with the least number of references in
the least place places we need to update
so this is giving us the flexibility to
explore new cheesy policy and heuristics
this isn't you don't get this with well
in Fodor yet this is still in
experimental phases but we do have a
stop the world partial collection
working and we are very close to having
incremental partial collection working
sorry
concurrent partial collection working
the same forwarding pointer that we use
for the big collection we can use for a
concurrent partial collection so we
won't be stopping to do young-jun
collections will just be stopping the
world choosing our collection set and
starting up again um another thing
missus I added this at the last minute
because I was afraid I wasn't gonna have
enough time but it looks like I'm
running out of time those rates five
fifty and six o'clock so we're
to skip this part so this is basically a
look at what Shenandoah 2.0 is going to
look like because we are doing these
partial collections we can perform a
collection without concurrent marking
information so we don't have to wait for
the full concurrent mark and that's as I
said like when you ran out of space and
you couldn't do it you couldn't do a
full collection because you never cook
are marking data we can do these then
and so we can do an it partial and we
can do a concurrent partial and we can
also do a concurrent mark and then a
concurrent evacuation which picks all of
the productive regions to collect if you
want more information again we have our
open JDK web page which has a wiki Java
GC at Red Hat feel free to send
questions there and we'll get back to
you that goes to our whole team or you
can send directly to me CHF a redhead
comm I am looking for I'm having a hard
time um I'm in a containerized world and
everybody wants to run in 200 megabytes
these days so if you have an application
that runs in gigabytes and you have a GC
problem please get in contact with us we
want to work for you we want to be able
to show you that we can do that without
stopping for more than a handful of
milliseconds any questions yes
we are shipping as a tech preview in 74
we have had no problems so we should be
we are supported right
we are not behind an experimental
options flag so Red Hat will support
Shenandoah and rel and in fedora and
basically if you have a Linux
distribution that takes the RPMs from
them will still support that the
Shenandoah team still wants to hear from
you we want to we want to have some
real-life user stories any other
questions yes
okay we haven't measured the connection
matrix at we're still right now we hit
we have ways we can make that smaller
but in terms of the shenandoah 1.0 our
only overhead is a word project so we're
absolutely honest if you say - xmx port
gig you get four gig for your object
space there's no there's no off to the
side data structures except for the
bitmap but but there's there's no card
table there's no remember it said
there's none of that I can't say that
it's more compact we actually do use
more space write it but I can't tell you
how much more space unless I know your
object size distribution right if you
have three very large objects and you
know our space that we take up as
minimal if all of your objects are you
know have one value in them then then
our overhead is large yes
if your we choose a collection set
before we enter into an evacuation phase
and if you write to an object that's in
one of these regions that has chosen to
be evacuated the garbage collector will
first evacuate the object and then write
to it it no no it's it's not the la most
lightweight rate barrier in the world
but we have a very quick check of a
thread local variable that says are we
in an evacuation phase and if you're not
then it doesn't do it so it's only most
of the time it's extremely quick and
it's only when you're doing evacuation
that you have to copy the object yes
it is out of my control we are slated
for 10:00 but given what's going on with
OpenJDK now and you know there's there
I don't remember what's public and
what's not but we are slated for 10:00
I don't know when ten is going to be so
I can't say when we're going to be put
back upstream we are working in a 10:00
work space we have it out there we're
hopeful anything else so yeah I told you
I'd be done early thank you very much
for your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>