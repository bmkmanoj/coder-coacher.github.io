<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Twitter’s Quest for a Wholly Graal Runtime | Coder Coacher - Coaching Coders</title><meta content="Twitter’s Quest for a Wholly Graal Runtime - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Twitter’s Quest for a Wholly Graal Runtime</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G-vlQaPMAxg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey I'm Chris I work at Twitter yeah in
the VM team so Twitter has its own VM
team and if you tweet about this talk it
would be nice if you had that hashtag to
give a little off to our team and so the
talk what's the goal of this talk I'll
basically show you what we did at
Twitter with growl and and what the
outcome was and and the goal for us was
always to save money right because
Twitter is a big service lots of machine
service and if you can shave off a bunch
of CPU time you will save a lot of money
right so I guess everyone knows what
Twitter is it's just you know to recap
it's a huge distributed system we have
many many services I don't even know how
many but there are some main services
like the tweet service right that reads
and writes tweets and and they use a
service that shows you data off the
profiles and stuff like that and the
time line service which you know
provides you the time line on your
client app social graph you know all
that stuff these are important services
and and we run many instances of them so
we have many TV aims for service as I
just said especially the main ones
thousands of instances of one service
and we have thousands of machines
running thousands of gvm so we you know
all of every time a little bit of saving
you can have multiplies thousand fold
right so and then on top of it we have
multiple data centers who you know it's
a big system right we also do open
source we use a lot of open source and
there it there is this this link we have
lots of open source projects and github
so when we thought about trying Raleigh
was a good fit because it's an open
source project you know you go and get
help you download it you try it out you
see how it works so that's a good fit
for us and it's also easy for us to
contribute back which we actually do so
we have our own JDK built which is based
on open JDK 8 you basically what we did
is so there is 243 that's the JVM CI
which we back ported from 9 to 8 into
our JDK and then we have Grall in there
and then we have something called
contrail which is a chi of our
replacement type of thing and and we
have a bunch of CMS improvements so this
is the JDK we're using and this this JDK
runs all of our services so why growling
general is C - I didn't know how many
people in this room know C - and not a
lot of you know the source code of C -
but it's really old and very complex and
getting ramped up on C - takes years
right so it's it's very hard to hire
people very hard to train them and
sometimes they don't even stay long
enough to get to a point where they're
actually really useful and do the stuff
so that's where that's why in situ there
were no major optimizations in the last
couple of years right there were there
were small improvements especially
intrinsic improvements until did a lot
of work there but there was no major oh
we redid the whole inlining or we
reimplemented it's capable nothing like
that happened right so and in my own
opinion it reached its end of life oh
alright a couple of years ago so and
again probably in my opinion I think
that broad is much easier to understand
it has a modular design you have
dependencies between the modules and the
build system takes care that you're not
breaking these dependencies and you're
not introducing circular dependencies
things
that their modules that are
platform-independent
they can't depend on platform they
depend on module stuff like that so it
it has a really nice design and very
important this that the inlining is much
better and be escaped analysis - that's
where we mostly which you will see later
where that's where we gain so when we
when I joined Twitter little more than a
year ago the first thing was okay let's
bring this over to our Twitter jdk and
see if it works right you know start up
a service and see if it works there were
a few bugs I'll highlight a few of them
there would there weren't you know it
this is basically almost the whole list
- there was not a hundred bugs and I'm
showing you that the three most
important ones this is more or less what
we found so it was this reoccurring
thing that on stack replacement
compilations didn't work because crawl
didn't support or Czar's with locks with
synchronizations
so and we've up the bug for it the
exception some things looked something
like this right and then we discussed
this a little bit I discussed it with
Tom Rodriguez and we decided at that
time that was almost a year ago we just
say it's not really a big issue because
basically grogg will fail a bunch of
times and then it will say okay I can
compile this one and see one will take
care of it
and and there were not you know
important methods for us so we didn't
care too much so but at some point
someone actually fixed it down there and
since then it works so basically 128 is
close but not really because when I
tried to verify that it's actually
working there were a bunch of other ones
popping up like this
no on secretase Monteux generated and
then there's this other one without or
is our entry loop I think that means I
can't
so tom was asking for a bunch of graphs
and then for the first two ones I think
yeah he he found out that there's a tiny
little piece of code in situ that
basically makes sure if you're accessing
a static final field in a constructor
that you can actually do it because
otherwise if you can't you would compile
code that would be optimized as soon as
you executed it so there wouldn't be a
point there would not be a point of
doing it so that was just a timed up
that's the c2 equivalent method that
does that right and so better to check
now then did the optimized as soon as we
executed you know of course so that one
was very easy to fix right it was only
like one file changed at basically a
chava version of that method and do that
and everything was good and we had this
bug that I only saw it with long-running
processes and I was super annoying
because I thought oh my god if that's
you know one of these compiler bugs that
you that you'll find after days and you
don't even know where it's coming from
so I would I dug around a little bit and
then I found out that we're using it an
agent called hipster and what it does it
it does some heat profiling and that
means it has to instrument the bytecode
and that changes some byte code of
especially allocation sites Graz written
in Java so there's a kind of this you
know meta-circular bootstrapping problem
so it turned out that it was it was an
issue with something called byte code
snippets it's something very special in
ground it's not a regular Java code type
of thing that you compile down with Java
C and then you just run it it's treated
specially by graph it's something like
an intrinsically put it this way so and
that was an issue because suddenly the
byte code was changed for that snip
and that broke it so this one was closed
with a rod a big change right but it
works now so that was good and we had
this one the service started up fine and
suddenly the log would grow and grow and
I thought what's going on right and it
was throwing exceptions like crazy all
the time and I didn't know where it was
coming from right and and this was going
on for at least a couple of weeks and I
didn't know really where to start to
look where it's failing I was digging
around in core library code and seeing
where the exception was coming from and
so on and then at one point you know
sometimes to help you have to put things
on the side and do something else and so
that's what I did and then I was I was
running nettie tests at the test suite
and I found out that the buffer one
failed and the reason for that is there
is like an optimization in most
compilers that when you do a reverse buy
could that it just you know swapped it
around with one machine instruction that
one was broken right it didn't swap the
bytes which is surprising because it
should fail all the time right but it
did not so we fixed that that was really
easy five change files so that was
alright and that was really it these
were all the bugs we found and since
then we basically run and growl in all
of our services at least the ones we run
we don't run all of them on grout but
the ones we do run fine
we started contributing back a little
bit this is still work-in-progress
so one thing I started doing is because
you know when I did the slides 9 wasn't
out yet but now it is so it would be
good if we had that 9 has something
called compact compact strings and the
implementation is a lot of intrinsic
code unfortunately because C 2
especially and also C 1 and to some
degree couldn't handle on safe
implementations and in a good way so he
couldn't if you would implement the
the methods with unsafe you could not
get the same performance if you were
right an intrinsic for it that was
unfortunate but that's the way it is so
they're a bunch of methods in in situ
that are hand optimized SSE AVX avx-512
so we need to port this over to grab to
get the same performance so that I
started doing that I started with the
first one so far it's the which one is
it
oh yeah the string compare to method and
you can see the that was yeah with still
the case because I haven't pushed it yet
so if you download ah you get like these
scores and then at the bottom you see
especially you know the third line with
the biggest size you see it's much
faster and that's exactly what do you
get with C do so I just showed this and
tell you C 2 is the same performance so
we need to part all these as I said I'm
still working on this one I need to add
a test case of course and we might want
to make it work with JDK two so we'll
see and then they're like three four ish
more of them but they're up there
smaller well we'll get there so numbers
I think that's the interesting part I
chose the treat service which is one of
our bigger ones not the biggest service
but it's it's like the first one I've
tried so I was just using that one it
doesn't really matter it's it's a
finagle thrift service so finagle is an
open source project that you can
download and check it out
I couldn't tell you exactly what it is
because I don't really know but it's
some HTTP you know save me a thrift
request and I'll give you something back
type of thing so this one runs 100% on
Gras in production like all the
instances we have run on grog today
including a bunch of other services not
limited to DS but including also they
use the servers into social craft
service or pretty much everything you do
on the web or on your
Twitter client runs through brawl
generated code today and I think it
works fine right does it so it it's like
testing it is a little tricky especially
in the data center you know first you
have different types of machines
different hardware then you have
obviously not just one process running
on the machine so you have very noisy
neighbors and things like that so the
test setup I have is is one dedicated
machine for one instance exact same
hardware I run with JVM CI syrup on 30
and Gras VM 0.22 which is slightly old
now I think we are it's your 30 years
something like this I can't remember but
it's still about the same performance we
we do it you know pretty standard tiered
setup so we we compile with c1 and then
we compile with with growl as as the
top-level compiler talk to your compiler
and to it's it's pretty much like what
do you have today when you download
Oracle or OpenJDK we just replace we
swap out CT with well that's all there
is something I'm not going into details
here but since Grodd is written in Java
you when you start compiling your
application and you start compiling your
first hot methods with the top tier
withdrawal itself crawl will execute as
Java code and then it will compile
itself which is you know that can't take
up some time of your startup you can do
something called Agora bootstrap with a
command-line option that usually take
depending on your machine and how many
threads you're using but that takes
between you know let's say fifteen or
thirty seconds something like this right
so if you just let it go you know let it
run it kind of eats twenty seconds into
your application startup you just read
out across your first minute or
something
but for us it really doesn't matter
because we start up the services they
warm up themselves anyway they have to
talk to a bunch of neighbors to set up
all the you know routes and connections
and whatnot so it was never it no it's
not true unlike it was once an issue
because there is some kind of a watchdog
timeout that makes sure that the service
actually came up and and they had a tiny
little bit of margin then we just pushed
it over the edge but we increased it by
a few seconds and everything was cool so
if that's an issue for you for us it
isn't at this point but in JDK 9 you can
you can äôt compile ghrelin and all
this goes away but since we're running
on 8 we can't do that so this is this is
one day of requests per second so this
I'm just showing this slide so that you
can trust me that all of them get the
exact same requests that's very
important because if a tweet you know
forget it three requests and and the
tweet has one character or 140
characters - 80 to 80 characters that's
a big difference in how much memory you
have to allocate right so all of them
get the exact same requests for the same
tweet IDs and so on so dedicated
machines save requests I had some 24
hours oh yeah and I can't you will
notice there is no y-axis because I
can't tell you the exact numbers
obviously but we'll get to some relative
numbers later and then so this
particular service runs parallel GC and
this is c2 at the bottom you see the
colors and the compilers right this is
c2 that's how many scavenge cycles we do
and it's it's a moving average of 16
minutes because otherwise it's neither
jumping up and down but it gives you a
good idea right
this is grown so we're already saving
here quite a bit
I throw in JDK 9 as well just to show
you that it's roughly the same
I have to I didn't put that in the
slides but what do you get when you
download open JDK 9 with Oracle JDK 9 is
not the corral version that I'm using
here basically I'm using because when
JDK 9 was prepared you know there's a
there's a long time ramp down phases and
so on and so on
I think the pro version that it actually
in JDK 9 is from December last year so
and then there were a bunch of
improvements in the first couple months
of this year so when I've been using is
better than the one you would get when
you download it today but you can go to
github go to Groth download that built
the module add a bunch of funky
command-line arguments and you get the
same thing this is really just to show
that there's no difference between eight
and nine and growl as you can see it's
the same so so we already saving there
so if we don't have to this this means
two things right first of all we are
saving the garbage collection time but
the the more important thing here is
that since we're actually not producing
as much garbage we are producing much
better code and escape analysis works
better this is what we really want right
so we get like when when the loads high
we get 2.7 percent less scavenge cycles
and 2.5 at the bottom let's say 2.5 old
chin is something that this question I
get them a lot on I think Jim it just
does that and it also does which I've we
don't have really good metrics to show
that but I'm just telling you and you
have to believe me but it's it's this
it's the same for metadata right you
load a bunch of classes in it has a
bunch of methods you have to allocate
some metadata but it's not people freak
out right they say wait next slide it's
roughly 40 40 megabytes but people
freaked out because they think all this
will grow right no that's it right it's
40 it will never change if the old chin
is 200 Meg's of will be 40 of the old
chin is 10 gigs it will be 40 right
there's no difference
and and growl doesn't suddenly grow
inside so anything it just has its its
state and that's it so this is the
important one right this is what the
whole talks about CP q10 let's see to
this crawl there's a situ taken now
which is surprising you know what can
happen is that that especially when you
run teared that in laning decisions can
be different from run to run so I think
that's that I don't think that c2 is
actually better in GT k9 I think it just
made slightly smarter decisions on that
instance - in line then on the one
before and that's Carl cheetah k9 same
and now I can show you why X is right -
ratio so this is it and we're saving 11%
of CPU this is huge I mean I don't know
how many compiler engineers are in this
room but if you ever worked on a
compiler and try to make something
better and faster you you know you you
you crawl around down there and the one
percent range and you're it's super
happy if it's - right 11 is ridiculous
so I I never expected that right when I
when I joined Twitter I pitched this
idea that oh look there's a yes compiler
it's an Oracle labs research project no
one has ever used it in production
before but I think it would help us in
America let's try it as a one
super-stoked there worked out otherwise
that would be don't have a job right now
so how much
I can't tell you obviously but we can't
you know just make up some random
numbers and get at least get a feel for
it right so let's assume you have a
company and and you're hosting your
stuff in the cloud right so I was I was
typing in cloud hosting service prices
probably into Google and a bunch of them
popped up they all have different prices
and different ways of to calculate them
and show it to you so basically I did
the calculation for you it's like from
$170 per year per pair one CPU core - no
wait the other way around 72 to 210 so
choose your cloud provider vice Lee it's
makes a big difference
so it's you know let's say the average
is 100 $27 per CPU per year so then of
obviously how many quarts do you have as
I said you know earlier about Twitter we
have lots you you might not but in fact
it doesn't really matter because oh wait
no I didn't want to go there because the
size of the company also has to do with
your revenue in a way right so and if
you have two servers and you have to
spin okay - is my finale enough but say
you have 10 and you have to spend $2,000
a year for them if you could save 10%
$200 you would be happy right if it's in
the millions yeah it's a lot of money
for us but for the company it's still
only 10 percent right so it doesn't
really matter but you know just just to
get a feel for how big things can be and
to be honest I didn't even know how big
twitter is but so there was an article
this was one I found right you can
google it maybe there's even an article
about Twitter how many servers we have I
don't know
that's from 2014 so it's already three
years old they're talking about two
million servers right they say something
from data centers fifty to eighty
thousand servers and
I don't know 24 cores so a lot right
your company's definitely smaller than
Google I would assume so I'm gonna pick
a random though of 10,000 servers a I
think that's reasonable
each is usually 24 cores or you multiply
that out three million dollars you have
to pay just to run your whatever you
have right you can save to 11 percent of
that
that's more than a salary here in the
Bay Area so you know you can hire one
more guy so can we save even more
potentially I was just you know I I
spent one afternoon really toying around
Groth has a bunch of in landing
parameters as c2 does so I was touring
around and again my best side the
request and I did an experiment which
you know it's just another instance of
my of my dedicated cluster and so this
is what we've seen before and this is my
experiment so we get less scavenge
cycles again and about 1.5 percent less
which is quite a bit and CPU time it
amounts to this a little bit and on the
ratio we're not a 2 percent and that
that was only like four hours of really
just randomly picking in lining numbers
right so I think that there is a lot
more you can get you might not be able
to get it with at least that's what
that's my current thinking I wanted to
mention here but I think I forgot I just
mentioned now
people on my team on the Twitter VN team
they're actually working on if get the
gift gave presentations and I wanted to
link it about something called auto-tune
that does you can tell the machine
learning thing to you know try different
cbn parameters and tell you which one's
the best so that that's what we want to
try you know tell it Oh
try these in learning parents from A to
B and you know play around with the
machine learning a little bit and then
giving me back what's the best that's
what we want to do but at this point I
have a feeling that the inlining policy
that's the default in Gras is not really
the right one for it I think we have to
possibly write our own to get to better
numbers so that's probably important to
you it's it's to a degree important for
us too but at this point you know we are
happy right we have a compiler that
works we have our own VM team we have a
bunch of people who can work on problems
if they come up I think all of you have
your own VM team so you might be
interested in official support for that
so yeah we'll see you know JD canine has
Gras because the ante solution in JDK 9
actually uses Gras so and what's not
announced by Oracle is that you can
actually use that version of growl
that's in the Cherokee you can actually
use it as a chicken platter it's a it's
an experimental option that's why they
didn't announce it or or not even that
so hey you can't use it but at this
point it's not supported
but so there was this email on the
mailing list completely innocent looking
right
it just says okay let's build cross we
crawled regardless of if you build it he
or not but there's one important
sentence in here let me zoom in on that
one that they would like to do growth as
an experiment chicken pate the next
release which at the time when this was
sent Oracle didn't announced the new
release cadence so this sentence would
mean Cheney content and I exactly sure
what that means today if that means 18.3
or not we'll see the the only thing I
know is that the JDK 9 version that I
would see the chorale version in JDK 9
that I was using was basically back
ported all the cheetah quitan change
sets from upstream I didn't I didn't
want to put it myself I just took the
change to the Europe like them they
apply cleanly and and that was it so in
the mainline of open JDK right now is
the growl version that I was using and
that it might be 18.3 or 18.9 so you
might have to wait a year but what I'm
trying to say is I think we'll get
official support pretty soon so if you
want to toy around with that do it today
we run it in production we don't have an
issue there might be you know smaller
bugs that you run into but there's a
community for it you file a bug and
github and will be fixed right so do you
want to save some money you can do it
that way I'm not saying everyone will
get 10% right I'm not saying that but
there's there's the possibility that you
can and I think that wasn't my last
slide actually so
give it a try really this this is this
isn't my my message I'm going to you
know do a road show with that talk I
want people to know that it actually
works um I don't know
as Lee as far as I know we are the only
ones who actually use that in production
there might be companies out there
somewhere that I don't know about right
smaller companies but I think at a
bigger scale we are the only ones who
are doing this and it works great for us
and we are we want to move all of our
services to grow I'm not exactly sure
when that will happen but you know there
will be the odd one out that won't work
or will run slower or who knows but but
we want to get there because so far I
mean as I said earlier we have thousands
of servers I don't know how many so far
we've converted maybe not even 20 right
we started with the big ones because
there you get the most savings but still
the smaller ones add up so the amount of
dollars we're saving today per year will
be double or triple once we're done
that's the whole thing if you tweet
about it let my team know thank you
question
if inlining depends on the cpu no no it
really doesn't
I know we talked before to talk a little
bit about it so I think I know what
you're aiming at so growl has support
for some vector instruction sets it's
not how should I say this well one thing
that's missing in growl at least in the
open-source version is that it's not
doing vectorization at all which c2 has
for us it it didn't matter so far there
might be a service that I don't know of
yet that will need it and then we have
to do something about it but today it
does not do vectorization but that
doesn't mean that it's not using vector
instructions for for other types of
things it's just not vectorizing Loops
Oh
well yeah it can in a way it usually
doesn't but you know in lining it's it's
just it's this walk on the edge between
pushing it over the edge of the method
becoming too big right because that your
instruction cache is basically
influencing that so but I didn't really
follow how the instruction caches are
today but they were all up about the
same size right on me
right
I did not the reason why I did not is
because it you know I basically the last
year
besides you know preparing that JDK and
I'm putting the stuff over and including
it in a chicken makes you know test it
to make sure that it works fine I spend
most of my time hand-holding service
owners right it's like you go to a
server zona and he's responsible for the
thing to be up and you will get into
trouble when Twitter's down but and then
you go to them and say hey look I have
to remember that no one ever used it
before you want to try it yeah so I
spend most of my time social engineering
than actually engineering so no I did
not have time for that but it should be
done yet
right I I also don't fully know but I'm
pretty sure that most of the savings
we're seeing is is because we're just
allocating less memory and add that also
means you get tighter code right you
don't have to you know call out to to
the runtime allocate the code and so on
and so on so you can your code gets a
little tighter one thing that I was
thinking about I should I shouldn't run
this experiment really turn off escape
analysis for both competitors and see if
that changes anything there is I can I'm
not sure if there's another talk about
Enterprise crawl at JavaOne this year
but it's basically Oracle labs as its
it's a pure add-on to the open source
version and that one has as far as I
know vectorization support and it has
better inlining as far as I know we've
tried it I did not present the numbers
in this talk but I've done it before so
I have previous talks somewhere that
actually shown am was within the price
crawl as well and and they're impressive
so the I just tried it last week
actually again because I wanted to see
where it stands today but I think it was
like the 11% you saw Enterprise crawl I
think was 22 and that also means that I
know that we still can get more
so that $300,000 number you saw earlier
is 600 now or can be right yeah it's
it's not open source it's it's a
commercial product you have to pay for
it but yeah well I'm it depends on the
license you are getting from Oracle um
your negotiation skills are more
important to you then I guess but yeah
if the money you were paying is less
than the money you're saving yeah go for
it
against the workloads oh yes yeah that's
true
that's a very good question and I can't
really answer that because Twitter is
like 95% Scala and I don't know of any
service that's actually written 100% in
Java because most of our service is run
on finagle it's just the basic framework
that's written in Scala so you know I'm
not sure it it might not be as much so
I'm really not a scholar expert but from
the rumors are here is that it allocates
more temporary objects then Scala does
other than Java does so that would
explain why we are seeing you know less
scavenge cycles we allocate less memory
because because all these temporary ones
day with better inlining that could just
scale replaced so I'm not sure it can
probably not tan
but if you get 5 I mean it's still
amazing
yeah I was bringing that up a bunch of
times it's like all the other languages
we use they're just not big enough we
use other languages for different things
but it's it's nothing's as big as
everything that runs on top of the of
the JVM like Scala a little bit of Java
so I I don't think so I was pitching the
idea of you know we have sometimes we
have these odd languages that run just
this one service we probably acquired at
some point in time and it's this
outdated
who knows what Python version or whatnot
it would be good if if we could you know
my great dad
to the JVM and then we could leverage
something like truffle yeah
yeah
that's true so I left that out purposely
because that's that's another Holt are
talking about this so there there are
two main issues with having a compiler
written in Java and one is the
bootstrapping issue with which I briefly
touched and the other one is the
allocation on the Java heap problem the
main issue with that one is when you are
later in your run and your application
is already fully up and using all of
your heap because everyone's tightly
tuning the heaps of their application
just fits into the heap right that's
what everyone does if if you then
suddenly would get I don't know if
everyone knows what a deoptimization is
but if you suddenly have to kick out one
of your compiled methods and recompile
it it could kick you over over the
boundary and give you an out of memory
and the way it is today this out of
memory era could end up anywhere he
could up if you're lucky in the compiler
but it usually won't you want to end up
in the application and the application
goes down so that's an issue the reason
why you don't see anything here is
because all of this compiling usually
happens in the first couple minutes but
we run our services for at least a day
or a couple days right later in the game
you really have compilations you do
sometimes but for us it's not a big
issue and we I mean it like Twitter is
the perfect you know breeding ground to
try this compiler because pretty much
everything that Twitter does except the
database in the background is stateless
right you gotta treat requests you load
it up and return it there's nothing just
safe there some state so everything
happens in the young gen and so whatever
if you if you allocate a little bit and
then you have to do a collection yeah
well do it so for us it was never an
issue but yeah it's something to think
about yeah
any more questions
all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>