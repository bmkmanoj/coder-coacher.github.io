<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Make Your CPU Cores Sweat with Parallel Streams | Coder Coacher - Coaching Coders</title><meta content="Make Your CPU Cores Sweat with Parallel Streams - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Make Your CPU Cores Sweat with Parallel Streams</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PdEViU_S0ME" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right good morning I think everyone
can hear me I can hear myself quite well
i'm lucas butter and in the next 45
minutes i would like to show you what
happens under a hood when you evil
invoke a parallel stream so the idea is
to show you what happens under the hood
and basically based on this to draw some
draw some conclusions when can you make
your applications run faster when not
I'm a software engineer at CERN for the
last nine years and okay so CERN what is
it it's a in a nutshell it's a word
largest Physics Laboratory so basically
we study the like the building blocks of
nature and we'll try to answer like
serious questions like what when was the
university that what are the origins of
it and so on and in the physics it's
kind of funny because for the smaller
you look with the bigger you need so how
do we do it
well by building and maintaining
operating huge particle accelerators and
maybe one of those you know is the Large
Hadron Collider that biggest machine
ever built by a man it's like 17 miles
right 17 miles long
300 meters below the ground and yeah so
it's really placed between French and
Swiss border a bit further in the
background you will find a beautiful
beautiful Alps in the best skiing
resorts in Europe so the key elements in
the map and of course by doing this kind
of things by pushing the frontiers of of
knowledge at some times there are very
nice spin-offs created so maybe well
CERN is also the place where the web was
born you probably heard about it it was
invented by Tim berners-lee in 89 if I
if I'm not mistaken so this is the like
the work first web server ever
and obviously the gentleman wasn't very
fond of switching it off because well by
switching it off you would switch the
whole web of so on not the not the best
idea but okay as much as I would like to
talk more about that soon we can have a
chat afterwards
let's not let's not keep our CPUs little
so let's go let's go straight to the
business you don't really you don't need
to follow what the code does it's just
something to start with so I think that
every one of you would agree that
collections are one of the most heaviest
api's in Java but actually despite being
necessary for almost every job
application manipulating them it's
always kind of painful so whenever you
have to do something you always get like
a fine point that man why I'm actually
struggling again with this so you know
you know yourself best how many times
did you find yourself free implementing
over and over the same database like
operations using all these control
blocks like all the slopes ifs and so on
and this is exactly what stream API is
trying to address so basically here on
the slide you have the exactly the same
functionality and it's nice it really
reads like a problem statement statement
and this is actually exactly what
streams are about so they are an update
to java api that simply let you
manipulate collections well not only
collections are actually like a sequence
set of values in a declarative way so
you say you say what you want instead of
actually how really to do it if you if
you look for a metaphor it's like when
you when you watch a movie over the
internet it's like a stream so that the
frames the elements of the stream keep
flowing if you have like a movie on on a
DVD it'll be more like a collection if
you if you interval Asif II a stream
would be like a set of values spread
over time and whereas a collection would
be like maybe set a value spread over
space
well computer space in our case one more
example with the with the stream well
what we do here we have a bunch of bunch
of numbers we map them which will filter
only those that are primes then we check
we convert them to the binary string
then we check whether it's a palindrome
so whether if you read it from left to
the right or from right to the left it
will be the same and we count our
how many we have of those and actually
although this probably this example
doesn't make sense at all
still you need like you need like five
seconds to figure it out what it exactly
does and this is exactly what the what
the streams are about but what about
what about its performance so like is
prime calculating prime numbers it's
kind of a difficult task well it has to
be difficult because otherwise all the
symmetric cryptography would break so
it's kind of CPOE first e and if you
think about the progress in the in the
chip making so when you look at when you
look back so how did the characteristics
of the main sleep chips changed over the
years so for like I don't know two three
decades we had a very nice times but
basically everywhere every year we had
this exponential row growth and in the
clock speed so basically all you had to
do to make this application faster is to
go to the is to go to the store buy a
new PC every year or every 18 months and
voila it was twice as fast and well the
life was that was that good
till like 2000 something when they were
when the Moore's Law on it could give
you actually were not really faster
course but more cores so in order
actually to speed up to speed up the
application really had to leverage the
multi-core architectures and I think in
Java is particularly nice to see how
they have the hardware trends were
really shaping that the software trend
so if you think about when you go again
a bit back in time sorta so from the
very early days you had the frets API it
was well what what it does it basically
Forks off from the main execution line
and you can simply execute some stuff in
the background
it wasn't about multi-core at this time
well beginning and end of 90s it was
more more of to make your UI not
responsive and so on and I think that's
also the message in the presentation is
that the good ol Fred it's still good
for a certain tasks if you have
something very very little you would
like to do in the background well go you
see it can go for a fret because if you
apply the parallel streams everywhere
well you were very simply a very quickly
make a conclusion that Java 8 completely
sucks and it definitely doesn't you it
just that the parallel streams they have
this discrete responsibility to handle
CPU hungry tasks if we go if we move a
bit forward in time while JDK was JDK 5
was quite generous we went because when
the multi-core CPUs were becoming more
popular so you had you had concurrent
collections you had the semaphores you
had the blocking queues you also had an
executor service and the fretful thread
pools I just listed it here because we
know we're gonna reference and back to
it so the idea of the executor service
well is very simple you have just like a
container a bunch of threads it's just
the guys that are waiting for your work
so you just submit the work there and it
will be done for you so it's really it's
gonna take care of of this creation of
the creation management and execution of
the threat and you can you can have
different variations of the of the
executor service and the fretful so for
example you can have that with a fixed
number of threads you can have the
scheduled one that basically consents
something and say hey execute this thing
in half an hour or execute it
periodically every five minutes so it
was very nice addition all that to the
api okay if we move forward in the again
forward in time we land with the will
will land with the with the parallel
streams so which is on the previous
slide this nice and sweet declarative
syntax to process your your data your
collections it's just one of the goodies
that the streams are about so what they
what they also let you to do so on top
of that you can execute those those
streams almost for free on your on your
core CPUs so basically all you have to
do is to turn your swim into a parallel
stream so as you can see on this light
on that well is the code visible in the
in the back I'm not sure what what what
I do if it wasn't but well probably it's
polite to ask as well so basically on
the on the on the left side we have
again our nonsensical prime number
palindromic example if it's if the
streams rice runs sequentially it takes
like a bit less than nine seconds to
execute if I just make it if I just
indicate that this should be executed in
parallel it's gonna take well it's it's
up by the factor five so how is how is
this possible what is what is what is
the trick well the main trick is that
with Java streams API it employs the
internal iteration again if we think
about the metaphor when my fiancee tells
me to was to wash the dishes she can she
can do it she can apply the external
iteration so she can tell me was the
fork was the knife was the plate or she
can really if she has that trust in me
she can say just wash the dishes and
it's gonna be who who is going to pick
up the plates the knives and maybe I can
introduce some optimizations so this is
like the main basically the main trick
it does and interval and in the
background it employs this fork/join
framework and the splitter writer and
this is exactly what we gonna see in in
detail but before that I told you about
turning a stream into a parallel one so
what does it what does it really mean
how do you do it so we basically have
two options the first is if the stream
source we say is a collection basically
all you have to do is to just evoke the
parallel swim method on your source if
it's like a generator function or
something else but it still can be
actually a collection you can always you
have this in your stream pipeline you
have this parallel
method and it doesn't really matter
doesn't it doesn't really matter where
this method is actually how
we're in your pipeline it is placed so
whether you place it like Indy in the
end or you place it in the middle it
does not really matter why because when
you think about the anatomy of the of
the of the stream all those operation
except the last one are those
intermediate operations so is it it's
very much like the Builder pattern so
you just keep saying stuff that what you
want to do but nothing is gonna really
happen till the terminal operation it's
gonna be invoked so really the terminal
operation is the boss so it's really the
thing is not gonna do anything until we
invoke the terminal operation okay so I
said before that we have this focus on
framework which is the heart of the
parallel swimming this is what is being
done this is what is being employed in
the background and let's have a look at
it it was when I did a little time
travel before I was a bit I skipped it
over deliberately so this was added in a
JDK five and yeah but it's maybe it
wasn't that famous bit is it because
it's a kind of low-level and I think
it's a bit of overstatement to call it a
framework but yeah it wasn't really me
doing it all right so what is a
fork/join framework it's also
implementation of the executor service
so it's really nothing more than the
again then the thread pool with a bunch
of threads that you just sent your you
just send that it's that can you that
that lets you execute our code in in
parallel so the major difference however
comparing to the executor service we saw
a few slides a few slides ago he saw is
that it was designed to for a CPU hungry
tasks so it was really designed to to
recursively split well fork process and
finally join and the results so
if we think about if you think about an
example how does it how does it work so
imagine this can be an array of number
and all we want to do we would like to
add those numbers and as we have a lot
of the of those numbers we would like to
do it in parallel so basically what
you're doing it's just the famous divide
and conquer you just skip you just
partition the array till it will be
small enough that you will conclude okay
actually it's not worth any more to
executed to paralyze it further further
I do it sequentially and then this is
where the steps jumps in and then of
course you did you did the divided step
now we need to conquer it you simply
join those tasks to carry on with the
with this example it's I think it's not
even crucial really Toto follow all the
code it's more again about the idea how
it how it works
so as I said we have a Fortran framework
which is a key implementation a graph of
the executor service so we must send it
we must send it a task please please do
it and of course so first we must
implement the task so basically your you
always should you can you must implement
it must be there the incident ask must
be the instance of the recursive task or
every recursive action the difference is
if it recursive task it returns a result
if it's not a recursive if it's a
recursive action it doesn't return the
result so it's like a consumer it just
does something but it doesn't do
anything I think on the Oracle website
when you want to have the article about
parallel streams they have a very nice
example that they basically they have a
array of some numbers of like a picture
and they blur the picture and this would
be like a recursive action because you
just we don't return anything you just
process and they array okay but in our
case with the with the numbers we would
like to sum them up so there is only one
method that we need to implement which
is which is implement overwrite that we
need to overwrite is the compute method
and it's a kind of a simple approach so
first we see what is the
of our task and then we have this well
the choice to make if it's small enough
okay I don't bother dividing it further
however if it's still big big enough so
it's worth - Voltorb to paralyze it I
just do it so basically we just created
the same two tasks we passed the same
numbers
however the left and the left tasks will
have well the left half of the array the
right task the right half of the array
and we simply execute it we forked so on
our left task which we call the formal
method meaning call this call this a
synchronously and on our right task we
can either call the fourth method again
but actually as we are already in the
India fret we know we do not need to do
any more for the time being with it we
can also compute it synchronously and
then we then we wait when two of them
are we done and combined the result
again it's not so crucial really to
understand understand it it's just just
the idea so we have our we have our
tasks so right now we need to submit it
somewhere and this is their witnesses
and this is the fork/join and this is
the fork/join pool and okay so what do
we do here so we instantiate our our
some method we create the instance of
the fork/join pool and this is kind of
already crucial for the parallel streams
so you have two options so either you
create a forging pull on your on your
own your Stan she ate it with a
constructor you supply how many how many
frets how many those guys that will be
doing that job for you should be should
be there or you can use the common full
instance it's like very much like a like
a like a single tone so and what is
characteristic for the parallel streams
is that they always use the same common
pole instance so they really they always
are all the time doesn't matter how many
parallel streams you have running how
many of them
running in the background they always
use the same thread pool you might you
might actually ask yourself okay like
what the heck would default join
framework because basically we did all
the job on our own yes it is a kind of
low a low level thing however it has few
characteristics that are a crucial
crucial also for the parallel streams so
first as you saw before we have this
fork and join a method that we can
nicely well fork off and and join it and
it all supplies this and this actually
like the brill the main characteristics
and the fork join pool applies their
work still in calgary so basically in
your fork giant pool you have like a
bunch of frets how many we will see in
in a moment
and those guys well they eat each each
guy each fred has a queue of tasks it
has to execute and if if a certain Fred
run run runs out of the tasks well it is
greedy so it's simply looking for other
in other threads excuse if there is
something to do and this kind of if you
were wondering like in the previous
example what happened if my left or
right task would be finished before day
before the other well it's not a big
deal because if it will be finished way
before and will be bored well it's not
going to be it's gonna still the tasks
for from other Fred okay again all the
parallel streams they usually a one and
the same common pull instance you might
wonder okay how many frets we we have
there the number of it the the number
it's a number of your CPUs minus one
okay why actually -1 why what what
happened to the other two the last two
the last call on the last CA oh and the
thing is that this is the how the
library is designed that also the the
fret that invokes that invokes the
submits the task to the for example it's
also becoming a worker so that the end
it's this minus one plus one
so technically effectively you have the
number your common pool is gonna work
with as many threads as you have CPUs in
your new PC if you would like change
those well you can use this property to
to set it but again if you think about
it that the parallel streams they should
be executed well they should really take
care of the CPU hungry tasks I'm not
really sure if it makes make sense
because if you if you create more than
you have CPUs well it just would be like
a certain cost dozen overhead also as it
as the common pool it's a kind of a
single ton of course when you do it you
must always do it before you actually
invoke your first parallel stream so you
cannot find granulated okay let's
execute this parallel stream with two
cores change this parameter to eight
let's execute it with eight cores and so
on no it doesn't work this way
once you invoke your very first parallel
stream this this won't be considered any
more so probably you should pass it like
on the on the comment line okay so I've
been I kept repeating for like five
hundred times that parallel streams they
always use the same instance of the of
the fork/join pool the question is can
you use your can you use your own so can
you say okay this parallel stream should
include this should be executed really
with this one and only for example the
answer is yes however I I just show it
for the completeness sake because it's
something I wouldn't I wouldn't really
do so how do you do it well if you look
at the fork method what it exactly does
this guy is a kind of context-sensitive
so that if it's running if it's already
running in a certain pool it just gonna
is just gonna stick to there so
basically the trick is to to create your
fork joint pool with as many threads as
you want and then simply submit submit
your parallel stream execution and there
and again Shawn for completeness sake
it's not something about I would what I
would really do you're probably can have
well if you look for a reasons to do it
well I'm against it but I'm trying to be
as objective as possible like a kind of
a lawyer or something so if you think
why should you do it
well first you don't have well of course
you have a you have a separate pool
you're not gonna in your common pool
it's not going to be affected by it
maybe if you had like a case is that
because of this threat sharing you know
that the threat that calls it it's also
involved to you who you would have like
a threats of a different life cycles I
could potentially imagine some problems
with that with the transactions and so
on the second reason is that if you
invoke the parallel stream and one of
the threads get gets stuck what it gets
taxed it gets it gets stuck forever so
this is something watch what you can
influence in this case because well you
can really define a time out a parallel
stream please do it for me but actually
wait only max ten seconds so again this
is like the reasons that could be a
reason actually to to go for it okay if
we try to draw some conclusions from
from the fork fork/join framework again
nothing more than a just like a
recursive divide and conquer algorithm
so as we have one short pull with not so
many threads or basically as many
threads as you have a CPU scores of the
virtual course is avoid IO because of
course every every time you wait for
something bit like a disk or network
resource you're gonna burn all these CPU
cycles again used for intensive CPU CPU
intensive tasks so when you're when you
stick to it then all the common pool all
the defaults they were really perfectly
perfectly and make sense mind if you
have like parallel streams because well
if you have like a parallel stream
having maybe like a for each and in the
for each you have another power per
other stream because then you can look
and then you can might have some
anomalies we can clearly have a chat
about it later I did some tests I there
is no not really enough time to show it
but yeah you can you can you can
actually find out yourself that I think
will will be pretty slow and because of
the worker pull so they were the key why
the for join framework actually exists
always prefer to have more more tasks
and then necessary because if you think
if you have 128 course but you have like
mega 80 of mega tasks well you can only
use like I don't know your CPU for 60 70
percent I mean the power you you have
and again rather stick to the common
apple because it just it just makes
sense okay so this is the fork/join
framework and right now in the example
when we were adding the numbers you saw
that we kind nice and evenly splitted
our array of numbers in the left and and
the right part
but as you know stream can have many
many sources actually you can also
create your own streams so some
sometimes we need a way to to influence
how this thing is how this thing is
actual annoying and get split so when we
are in this fork part well we need
somehow instructed or it somehow must be
smart enough to instruct itself how this
thing should be splitted and this is
something what was introduced in a new
interface that was introduced in Java 8
which is a splitter writer which is
split table iterator it even stands here
ok
and it's basically designed designed to
go through your elements in parallel so
yeah to kind of apply the iterator
in whatever number of course you have so
how how does it work
it has well it has actually a bunch of
methods but those are the main free ones
so as you know the classic iterator it
has this has past next and next so
simply if do I have something do I still
have something and if yes please give it
to me
the try advanced it's kind of
incorporation of those two methods so it
returns a boolean telling you whether it
there is still something in the in the
in your splitter writer but it also
takes a consumer that simply does
something with your with your element
that it was currently iterating through
and it when you think about it it's
really nicely pep makes sense how does
how everything connects together because
when you work with your streams you
nicely past you can use the lambdas to
pass to pass the behavior and again if
you have a consumers and so on derive
originally passed there another method
which is like the heart of the split
aerator is the Tri split so it does two
things
it returns a new splitter writer and
it's changing the coverage of the
current one we also have the
characteristics when you speed it just
like some hints to environment to speed
up to introduce some optimistic
optimizations and so on so let's see
that try split in action little because
again it has some consequences how you
can what should you think about when you
think about the performance of your
parallel stream so we have our splitter
writer again beat beat again a huge
array of numbers so when we invoke the
Tri split on this guy it does two things
so first it's returning a new splitter
writer which would be like okay I don't
know if the numbers are from one to
hundred it will terminate from 1 to 50
but at the same time it's changing the
coverage of the splitter writer so this
would be from 1 to 50 this would be from
51 to hundred so this is exactly
actually the the whole trick it does
you do it again and it does exactly the
same thing so on our on our splitter
rater one it returns a new splitter
writer would be from 1 to 25 and it's
changing the coverage of of these guys
what would be from 26 to 50 so this is
the whole and the whole trick how
actually the parallel stream gets forked
and we do this thing over and over till
that till the Tri speed return channel
and then we know okay we're done with
the forking step now we can another
parallel stream can process this in
sequentially and later join it so when
you look at the splitter rater one it's
really this guy it's all the time the
same object it just with the it got to
times
tri-split invoked on it so it's coverage
was shrinking the question is can you
use your own splitter rater for a stream
you can so you have the stream support
class it's like a low-level class that
you can construct as streams and yeah so
basically you have this stream method
that you pass your your splitter writer
I'm not sure if you ever really going
you need to need to do it again it's
good to understand how this thing is
working under the hood maybe if you have
like a enumeration I'm not sure if you
can create an enumeration out if you can
create a stream out of the enumeration
what we might be like one of the cases
if you were writing your new collection
and you're not very fond of the default
implementation of the Tri speed would be
one also like the case or simply if the
elements in your streams they have
certain semantics that the default
default splitting process wouldn't make
sense for you then you can influence the
process but again probably you're not
really going to need it
okay so when we again stick to the to
the to display splitter writer so
basically imagine we have a we have a
linked list
we call a we call a splitter rater on it
well it returns a linked list splitter
writer of course of a type what are my
elements in the list and right now what
we do first on our splitter writer the
many ones are really the one on on top
it has 10 K 10 K elements if we try
split again it does it does two things
is it it returns a new one so this is
the one is being returned so it's size
is like 1k so you can see it on the
picture and it's changing the coverage
of its of of its of itself so basically
this is what hey what happened and if we
do the exactly the same thing for a for
a for an arraylist well we have a
realistic splitter writer where we see
what is his size we call it
tri-split and then we see what is the
size of the of the new one and all the
wonder that coverage coverage got
changed and if you look at the array
list it's nicely and evenly divided
however if you look at the if you look
at the linked list well it's like this
kind of mini mini list that was created
and why is it like this because well
whenever we start whenever we start a
problem that we want execute in parallel
we always start at one core I mean the
problem it's never multicolor from the
from the from the from the start so one
actually we call this tri-split
and you know in order to split a linked
list you really have to have to go
through the elements so when we actually
doing it even if we have 500 course it's
like 499 course that will be slipping
because it's only one core that will
it's invoking that right split so the
guy saw the guys at at Oracle they just
provided some intelligent
implementations depending on the of the
data structure is like okay if I have a
linked list please I want to actually
fork as soon as
for cough as soon as possible so this is
actually the reason why this is not so
well not evenly divided and this of
course means that for a strategy for a
parallel stream certain sources are more
friendlier than the other so whenever
you have this when you create when you
create create the tree when you go from
the root to the Leafs it kind of matters
what what's your what you're doing so if
we have a linked list
you already saw it again it's a knot you
need to walk through the guy iteratively
so it can be expensive when you use the
generator function well it's a iterative
generator it has the same
characteristics to to generate the
elements as a as a linked list I've set
a net reset but something like in the in
the middle is probably it won't be a oh
oh and it would be like something all
logarithms and and of course I released
all the stages generators they are fine
so again like one of the lessons
whenever you have a whenever you create
a stream parallel stream think about the
stream stream source few a few examples
so basically what we what we have here
we have like 1 billion elements ok let's
be let's be bold why not all we would
like to do is to compute their computer
their sum so first with the we're not
going parallel it's a nice sequential
like the execution we have like one
second if we just all we do here if we
just try to parallel in parallel right
this guy with the default with the
default heap size when you start a JVM
it explodes it actually explodes when
you when you try to when you try to
invoke the try speed method on the on
the on the supreme source or even more
generally speaking if you create a
linked list with 1 billion elements we
invoke a tries please try split-- on it
it explodes so yeah because
again when you think about the overhead
it's quite heavy if we go for a
stateless generator function which is
friendly and you do it in sequentially
it's taking like 600 milliseconds and
you might actually wonder okay like what
the heck those two guys they should have
a kind of similar performance because
they are off it's a sequential run so as
I as I do it sequentially I mean the
range shouldn't give me any benefit well
it it does because the iterate method it
returns boxed objects so you have like 1
billion objects that you need to box and
hunt box
so yeah this can this really basically
improves the preview improves the
performance by twice if you don't if you
don't do it so even though it might be
like a bit confusing ok I have I have a
long streams I have the relief
specialized stream this is what I read
on the Stack Overflow to do but still
the iterator function or regenerate box
objects and the the the some expects
unboxed objects and you have like boxing
and unboxing a very unnecessarily so
this is exactly what the wrench in this
case has changed and right now if we
call if we call the parallel method on
it still we have like different thing
execute five times faster ok all right
so find any and unordered so well maybe
the example probably explains its best
so imagine we have this kind of very
simplistic stream of six elements and
all we do here we just pre we just print
the elements of it nothing more . and of
course as as we executing in parallel
that is as it does the as it does the
tree whenever you execute it it's
producing something else obviously if
you if if we remove the power parallel
out of here it would be nice like 1 2 3
4 5 6 however if it's as it's a parallel
stream well you cannot really predict
the
order and if you if you want like to
find a evoke defined first a method on
this dream well it will the API it's
smart it will always if you if we invoke
a find first it will always return one
so it would really obey to what you want
to what you want but this is an overhead
because when you think about it okay the
kind of natural order of this parallel
stream it's random it is this so if I
tell this guy give me the first one he
must he must do he must do some effort
okay what was actually on your first
element so just prefer just preferred to
use if you not really worried if you'd
not really don't care which one should
it be we turn find any because find
first would be like the one always one
but if you invoke a fine tiny little
piddly AK four six two if you don't
really care about it go for it it will
be faster at the same about a limit
method so if you invoke the limit free
on this stream it will return like one
two three or so it won't be one two it
would be one two three as a set not
really exactly in this in this order but
it will be always 1 2 3 2 3 1 3 2 1 and
so on however again this is an overhead
because the natural order of the stream
the first three elements like four six
one four four six four five and so on so
whenever you do the limit and you have a
parallel stream like well think about it
that maybe I don't really need the first
and element so just give it give it a
hint that this is actually unordered I
don't care about the order so just give
me just give me any element all right
yeah as a down-to-earth proof so we have
we have a bunch of numbers and we just
filter those that are bigger than the
500 okay and if I invoke the find first
well the guy is smart it obeys it
returns exactly the first the very first
number that
that is matching this condition however
if I don't really care about about the
order if I'm interested in any this is
what I'm doing here it's for for us four
times faster but of course it returns
the element that basically every time
you invoke it it would be a different
one but depending how long you keep
invoking so again the owner unordered
and find any can be also your friend
okay yeah avoiding short state so also
to think you think about an example so
imagine we have like a accumulator class
well all it does it's it's its
accumulator so basically it has this
some method that you that you'll keep
that you keep invoking and it's simply
increasing the it's simply increasing
the result so right now if you do this
if you do this kind of thing that you
instantiate your accumulator and you try
to invoke invoke this in parallel so
basically some of the numbers from 1 to
100 what you're gonna have different
numbers all the time because you're
going to have a race conditions all over
the place so again with parallel streams
it's a kind of also try simply to
unlearn certain certain patterns because
in this case when we have a function
that maintains a state it doesn't go so
much so well along with the streams
which of course what you can do is to
make it is to make this method
synchronized but actually well then all
day all day all the effort to have
parallel stream won't be water because
they know the secret synchronize it's
very expensive when you have multi
course that they communicate with each
other saying ok do this no don't do this
because I'm doing something this is very
expensive so all your benefits of a
parallel stream will be will be lost
so basically lose like a reduction
function and again if you have parallel
streams stick to the stick to the pure
functions I'm sure there are plenty of
lectures here about functional
programming so whatever you learn those
there just apply it to parallel streams
okay almost at the end is the sequential
sequential and parallel so as explained
we have this we have this well as stream
is basically behaves like a builder
pattern so it does really nothing till
the very end when you involve when
you're in when you call the boss the
terminal operation please just execute
me and you might be tempted by having
those two methods that one is giving a
hint okay when you arrive to the
terminal operation though it's
sequential you the parallel one do it do
it do it in parallel so you might be
like tempted okay maybe I could
something could do something like this I
have a bunch of I have another range of
numbers in if I have a is if I'm
computing primes it's expensive
it costs time let's do it let's do it in
let's do it in in parallel however when
I when I have my accumulator so you saw
one slide before accumulators are evil
you cannot really do what you shouldn't
do it let's be smart and do it
sequentially no it won't really work
well it will work however the whole
stream will be invoked in a sequential
way so you will lose the benefits of
parallelism okay sure
shall I go back
it's it's it's it's a very good question
well this is just a matter of fact it's
it is like this but they are they are
however libraries that you can kind of
overcome it that you can that you can
achieve a fine graced find a grained
control what is it parallel what is what
is not we can speak gladly about it and
actually I couldn't know you simply well
in yeah yeah so you really well if you
stick to the pure JDK 8 so not some
other library libraries on top of it I
would probably split it or just get rid
of the accumulator to have like a
reduction or something but yeah it's a
it's something also I'm personally
missing so we can all right so almost at
the end
sorry checking the time all the time in
our coming from Switzerland a lot the
length of watches so well you have to
keep checking it alright so I think that
the main conclusion is because making a
parallel stream is so easy
well it's so easy to turn your parallel
streams you stream to parallel stream it
doesn't mean you should do it on the
contrary it's always just do it wisely
and well if you have like a application
that has like I don't know if it's big a
big chunk of it is a parallel stream
does does something apply always those
three steps which is measure measure and
measure to find out whether it really
behaves as it should because you you
might you might find that okay
underperforming or it completely does
something wrong so use it for CPU
intensive tasks choose data structure
that are easy to decompose for example
link is linked list is not choose data
structures that are also easy to compose
I mean link list it's easy to compose
but if you think about the heart set if
you have like a collector scooping by
they are not so trivial
to compose again so think about it and
rather prefer it for a big set of data
so you have like I don't know ten
elements in your stream don't bother
well thank you very much for being an
awesome audience and have a blast in
direct well first</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>