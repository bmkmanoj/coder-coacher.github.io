<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JVMLS 2015 - VM Design Choices | Coder Coacher - Coaching Coders</title><meta content="JVMLS 2015 - VM Design Choices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JVMLS 2015 - VM Design Choices</b></h2><h5 class="post__date">2015-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BdjcPgIUbnA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is a talk I put together a
decade ago and it turns out that I
didn't have the right audience at that
time to to do do it justice so I'm
really happy to give it now and this is
exactly the right crowd for this stuff
although now some of the slides are
dated so we'll just have to work through
what's dated here so you know some
history here hotspot started out a long
time ago as the self vm poured in Java
and it had a you know serial
single-threaded software old GC a simple
stage zero JIT maybe if you want to call
it single threaded multi-threading was a
much later addition it was x86 only the
SPARC hardware team paid money for a
port to spark was the second port if you
could call the x86 only thing at Port
I've been working on hot spot for about
20 years the last three years I've taken
off and been doing some unrelated stuff
um but I have a long history with it
I watched it become Trevor bust I
watched a ton of port so I participated
in most of those ports I worked on c2
obviously by the tons of compiler
infrastructure and shared infrastructure
all throughout the G the VM I watched a
bunch of new GC show up including people
having different requirements for out of
the one of the diddid code as it relates
to the GC and then a bunch of other
stuff your reflection and thin locks and
Java memory model and J&amp;amp;I code and on
and on line okay so the nature of this
talk so this talk is about looking at
the kinds of decisions that you end up
making when you build a VM and I know a
lot of these are our hard-won experience
for which this is exactly the right
audience to understand what those
choices are and why and so the the the
things that we're going to point out
here is that many of these topics I'm
going to cover here you optimize for you
work very hard an engineer hard to make
something go either ultimately fast or
low-power or small footprint or whatever
it's going to be and these choices
you're making are gonna interlock badly
in funny non-obvious ways and suddenly
you get in down this rat hole where
you're stuck and you have to redo some
critical piece of infrastructure in
order to make some you know next gain in
performance or power whatever it's going
to be so I have lots and lots of things
I can talk about I'm happy to stop for
questions so you know if someone says if
your can
you someone else's to stop and ask me
otherwise I'm going to talk at this kind
of a speed all the way through Dulli
okay some choices to make
so vm's are our big complicated beasties
or maybe not maybe they're really really
small and tiny things and it kind of
pins on your feature set and meaning the
features that go into jaebeum's
interact in bad ways that bring up the
complexity and the interactions are
often not obvious coming in I just said
that I want the big desktop server out
but turns out to be very different
choices for my say cell phone guys this
is the day to topic it's something small
form-factor embedded if you know what
your target audience is if you know your
target problem is if you have complete
control the whole world scenario with
your native code then many the decisions
that are hard can be solved completely
easily completely casually if you--if
you want to handle like all possible
versions of native code all possible
scenarios of threading and scale and
size suddenly have a whole ton of hard
problems that make the VM complicated
that wouldn't be there if you were
living in a smaller world so some
obvious choices you know portable or not
does that mean you're generating machine
code or you're not or you call I'm using
your building with C C++ hotspots all
C++
what are your calling conventions on x86
there's a dozen you have to pick one if
your inner opt was native code many the
calling conventions date from like C the
early C years and they are not
necessarily the most optimal versions
especially for strongly typed language
like Java and so maybe you don't want to
use any of the native calling
conventions and this happened a lot
you have thread so the OS threads are
the green threads are ume POSIX
compliant or not which with your Stax
grow turned out to be really pain the
but to flip the Stax around between the
x86 and sparks they grew different
directions that was woven all throughout
the VM was very difficult to get rid of
and I've got a pretty bad feedback if
somebody can tweak my audio
I don't know how to do that okay thank
you
so other games like footprint you have
64-bit pointers the 32-bit pointers
right are you talking about x86 all
worlds on x86 unless maybe it's an arm
or a GPU or DSP chip are you
interpreting only are you getting only
multi-threaded you know if you have one
thread locking is a non-issue if you
have two threads on one core locking but
that all locking locking is kind of an
issue you have to think about it if you
have cooperative preemption then you
always you know cooperate to not flip a
thread in the middle of holding a lock
as soon as you have preemption well then
you might end up flipping a thread while
you're in the middle of acquiring a lock
and then you have to have some sort of
locking idiom that works and as soon as
you have multiple CPUs you have to have
an atomic operator you cannot is not
sufficient to do some sort of load right
store thing you must actually use a kaz
instruction or you will be wrong okay
better Oh much better
okay interpreter choices you know simple
or complicated you write when in pure C
it's very easy to get started it's easy
to understand how a piercing interpreter
works it has a certain speed to it all
tied up with a dispatch you go to GCC
label VARs is about twice as fast this
good c interpreter pure assembler are we
a hotspots had one for a long time about
twice as fast again we looked at
hardware support at various times varies
companies have I don't think it's
actually necessary you you want to get
instead obviously but there's lots of
fun ways to inline and schedule your
dispatch logic seen a couple different
ways to slice and dice this having solid
good performance out of the interpreter
is actually fairly important because
it's your startup time and it's your
fallback position when your JIT fails
right you don't want to like cheat too
badly here however as soon as you have
an interpreter you
typically well for job at least she have
these um stack oriented byte codes the
the stack layout is sort of really bad
for didn't code in general so the
interpreter what it wants and how it
looks at the stack and what the Jets
went when they look at the stack are
very different and interfacing those two
becomes a giant engineering mess so if
you can get away with no interpreter
your life's easier but there are some
real costs to no interpreter at all so
people talk about hey I'm gonna give up
the interpret I'm gonna have stage 0
jets a template style just slap out
machine instructions as fast as I can
and then you know there's a lightweight
optimization I'll call this you know see
one linear scan allocator some classic
the obvious high payoff constant folding
CSC kind of things in lining obviously
or you know c2 which is every possible
bell and whistle you can imagine graphic
calling allocator no one says it no one
talks about very much but right after in
lining graphically register allocator is
the number one way to get performance
out of crap code you get miles and miles
and miles of crappy code that's the next
secret in ok you're going to inter call
with natives how expensive is that turns
out that's all about calling conventions
including some fairly subtle issues with
what you do is objects and horrible
register layout games mixed with the
interpreter that's another horrible
calling convention game this is a this
is a reference to the optimization class
loading resume lining on finals
most other jaebeum's have an issue where
when they inline it on final they have a
line in the sand which they cannot
schedule instructions above in case you
overload the non final and you have to
slap an instruction down on that line
and sansei thou shalt not proceed past
here because duty in lining this code
that follows his crap hots positive
something even more aggressive we can
take the in lining completely disappears
of non finals in case scheduled
completely throughout the code as an
expense we have a very clever the
optimization path that's sort of not as
far as I know it's not done by anybody
else I'm happy to talk to people about
it later it does have interesting
performance impacts there so stage zero
jets means you can skip the interpreter
this is the template generation codes
very
quality bit very fast generated
typically you can generate it in your l1
cache faster than read from memory so
it's actually faster to generate it
fresh and run it than it is to like miss
from memory to get it no funny stack
like that it's layouts that's great
that means easy calling conventions it's
less engineering method ah still slower
than interpreting one runs code and
there's a lot of run once code at
startup time so there is a cost to doing
it this way you get a lot of bulky code
which matters if you care about
footprint but it's so cheap to make you
just throw it away constantly - so you
make us some small code cache size you
generate the crap code in there as fast
you can you run it and then you throw
out pretty fast and some hot stuff
sticks around after all you got to get
out of this mode or you're not gonna be
running all that fast but it's a great
way to get started if you don't wanna
deal with the complications of
intermixing and interpreter and get it
code some GC choices simple stop the
world's hotspot started fast what does
fast mean does it mean high throughput
does it mean low latency you've got a
moving collector or are you gonna have a
conservative collector moving collectors
let you do compact the heap
defragmentation you can do bump pointer
allocation it has some real interesting
performance gains conservative means you
don't have to track all your objects and
this is a ton of engineering work that
had to go in the early hotspot jets that
were you know the C C++ compiler guys
are all saying you know wasn't worth it
trying true you know you can make these
things in criminally overtime fancy a
new algorithm parallel that's hard
concurrent really really hard both
parallel concurrent like insanely hard
many many PhDs come out of that I've
participated in a couple such GC
algorithms in the you know design
construction of them it can be made to
work obviously ask people with the bug
tale on CMS and g1 is all I will throw
the Azul push out here that the read
barrier changing the algorithm worked
out a lot better and a lot easier more
GC choices here safe points versus stop
anywhere so the issue of safe points is
simply find all the pointers when you
stop into a GC cycle
so you're on some running throat over
here some guy over here runs the heap
out me a GC cycle you're doing some
unrelated thing has nothing to do with
GC you don't care but you got to get
stopped and have your stack scrawled
okay you get stopped where you're
holding pointers maybe they're gonna
move right so find them so you have a
map at every program counter that's too
bulky you go to interpret instructions
so you roll forward to the next map
maybe have fixed map layouts like all
the even number stack slots have oops
and the odd number ones never have oops
and the even registers have oops and the
odds do not and then you just know oops
or not talk to Phillip is low he
suggested this one you're conservative
on the stack and your exact in the heap
sorry with the idea being in the land of
64-bit pointers false collisions between
a pointer and an int or kind of rare and
so you can get away with being
conservative at least some of the time
and see just be conservative on the
stack and you get some crap extra
dangling pointers that you can't move
but then be exact in the heap whereas if
you go to a safe point then when you
stop however that stopping happens you
stop at a point where you have an OOP
map and you have a mapping for what is a
pointer and furthermore you can change
those pointers by updating the hardware
registers and then reloading and going
again right and then how do you stop
turned out that Azul tried a couple of
ways but polling in software is fine
you just have some cheap way to poll but
an x86 will do a you know a load from
cache and predicted branch with you know
essentially no cost it's just like free
preemption can preempt you in the wrong
place and so it's talking about segments
with threads meaning you're not at a
safe point and that means if you have a
thousand runnable x' on your 32 core
machine and you stop into a GC cycle and
nine hundred of the runner balls are not
at safe points you have to roll them all
forward to save points where you can do
something with them different people
have tried different things wide end up
doing it as well it worked out really
well we simply have a call back from the
OS saying you're about to run your time
splice I'll find a safe point and then
when you did you told us I'm out and if
you didn't you took too long then he
pulled you off hard way
and if a GC cycle came along then we
took the latency to roll that guy
forward but most threats most the time
would always be stopped at safe points
even if there were run of holes and that
meant when it was time to do a GC cycle
you didn't have to roll anybody forward
you you had all their stacks ready to go
multi-threading costs all your
operations are no longer atomic you can
preempted inconveniently in the middle
of say a walking idiom where you've done
some sort of load compare and and not
yet the calves or I don't know what you
know I have to have locking didn't eat
it before so simple VMs can skip locking
in lots of places or complicated ones
cannot threads obvious you can block for
io GC requires you'd have a stack
pointer in a program counter so
obviously you should be able to go ask a
thread for his stack pointer and program
counter except that the most OS is well
every OS have tried on the planet this
has failed with low frequency and giving
me rarely a crap stack pointer for which
the stack crawl is now wrong and your
gosh invert a lot of what I did it Azul
was headed for cooperative
multi-threading but it was always a
preemptive backup plan so the performant
low latency so performant not throughput
but performance for latency was all done
with cooperation
so that's what I mentioned before we had
the OS just issue a call back to a
process saying yeah and then he still
had to be pre-emptive in case you were
wrong the case you didn't honor the the
call back in case you weren't going to
be cooperative right so some guys in a
badly written mem copy loop and he's
just going to run this gigabyte through
it's gonna take him a while right and
they always said I wouldn't do a GC and
he says oh sure in a couple seconds
right in everyone else is not blocked so
you have to be pre-emptive right it it
but but you can get away with it mostly
mostly good it's in Red Hat the main
distro it's like okay as soon as you
have multiple CPUs as opposed to
multiple threads you have to have atomic
operations not just locks so you have to
have calves instructions have to have
coherency issues have memory fences you
have to honor the Java memory model if
no the hell they metal is and how it
maps on your hardware unit with low
frequency data erased bugs with a long
bug tale to go hunt down and track right
so yes expect much harder debugging now
you have to have scalable locks energy
VM holding all the Java Virtual Machine
internal resources have to be in some
sort of scaleable locks because a lot of
threads might pile in on a class loader
trying to load new classes or to confirm
the classes loaded while somebody else
is trying to inject or whatever how it's
going to be right so not just for
correctness but they have to be scalable
for performance you have to get those
same locks into the running code where
the thousand runnable czar actually
trying to compete for some stupid-ass
lock they asked me spinning and retries
asked me fair locks one or super high
contention or else you get thread
starvation issues like that so these
make the locks much more complicated
than what would happen if you're just
like hey I'm gonna do the obvious thing
here you know you have to work hard to
make these work well right GC convenien
progress look obviously when I throw it
away confirm a lock or native code or
whatever and therefore the threads now
you take some
form of a GC lock in order to be able to
run as they come out of the native code
the kind of locking idiom maybe they're
stacks in the middle of being rewritten
by GC they have to have a GC lock before
they're allowed to run so the some
atomic operation that's competing with
the GC it has to happen before they can
go yes because the OS does not get fair
locks yeah yeah yeah yeah yeah yes
anyone who runs a big busy web server
and gets thread starvation is like a
lost customer the guys waiting and
waiting for amazon.com to say we love
you we sold you this thing and they get
tired they like bailout good eBay so
parallax had to be there and it didn't
have be perfect it just had to be
you can't starve some poor slob and the
OS says I got a thousand runner Bowls in
32 cores
I'll put eight on each run queue and now
I've got I had 100 and threads runnable
and you know 800 or not the 800 or not
never run and and then they start out
right people time out transactions die
things fail out left and right it just
sucks no you had to do something smart
there
yes so we played games there too okay
here's a stupid hack 64-bit math the
major uses of longs turns out to be the
big integer package doing crypto by Web
Services everywhere like who'd a thunk
well big integer uses as always really
as a pair of in store they carry between
them where you mask off one half you
mask off the other half you load this
half you shift it right you shift it
left the dance and dance deficits works
out really well as a pair of intz turns
out that a pair of innis with a carry
and c2 still has support for this in it
works out as well as really straight up
good 64-bit code because of the number
of uses where you actually really only
one one of those 32 bits are one of the
other you don't want to load 64 than
mask or load 64 and shift you actually
want to load 4 on the other side and
then be done who'd a thunk right ok so
now I'm gonna go take a deep dive in
native calls looks like a day of doing
native calls and and I'm gonna pick on
spark so it's beat on spark day so this
is old spark chips and I'm picking on
spark only because it's exemplary for
being bad but the same issues arise on
various calling divisions on this 86 and
on armed and on titanium in different
ways but it's the same general problem
so ideally you'd like to bleed I'm gonna
make a native call I can just actually
use it at the hardware's call
instruction show you really fast really
simple and for small embedded systems
well I know that my target code is
cooperating with me he's gonna be polite
about GCS may be applied about safe
points he's gonna follow the rules um
that holds true but as soon as I say I'm
gonna call some random piece of code but
I have no idea where it came from I have
to be defensive soon as I have to go to
the defensive place things get
complicated fast so standard sort of
calling convention on a spark register
window push same on exit e6 you're gonna
push the BP red shirt to make a stack
frame you're gonna do an argument
shuffle one of the registers in the
wrong place fine you make your call
you're going to shuffle the return
result back and unwind the stack however
that's done returning yeah that looks
easy
so obviously the registers are wrong
because I'm passing a double and on see
the native code because they do printf
they like to have their floats and their
doubles and their install run together
in the same register set in the same
order so varargs works and that means
that your doubles get passed in the in
tread stirs whereas on java with its
strong typing you know that doubles are
could be kept in double registers and
doubles can call registry or routines
with doubles and the double registers
and that's in fact what happens when I
have control on both sides of things in
the in hotspot but in the case of spark
here I had to shuffle and there wasn't a
reg reg move between the float and the
int registers so it had to pass it to
the stack so I had to have a hold on
stack for it and he had a store down
loaded up and that this pointer is no
zero so the double comes up in
misaligned
oh 102 so you can't even do a single
load but this is you know I'm that's
just me picking on spark now okay so the
next thing is that I can't trust the
native code to be nice with my oops so I
have to hand him a pointer to an hoop I
handle instead of the hoop itself so I'm
going to handle eyes though oops it's
part of an argument Scheffel and that
means I'm going to take you know I'm
gonna do the null check because I know a
handleable no but no and then I'm gonna
store the damn thing down into some
place on stack and put the address and
that's the handle and hand that to the
native code and of course I have to
reverse out in the way back if he hands
me handle back I have to say is it a no
oh not okay then load it and go again so
this is me being defensive against the
native code hiding oops from the GC when
he blocks for IO for the next decade
maybe I need to lock him I'll cut short
the eye chart but it's basically too
obvious grab some bits out of the header
word scribble with them store some bits
somewhere else
Duke has instruction and then unwind
that with the Kaz again on the tail end
when you come out I have to do stack
crawls for GCE but GC needs a stack
pointer and a PC which the portable s's
won't give us at least not reliably so
I'm going to throw it down before I call
the native code so that is you can't
read it now because the the charts off
the bottom but basically says build a
constant which is a return program
counter it's a unique token specifying
the call location it's not a
cool trick it tells me exactly where
this call site is that should return PC
on x86 I get it a different way but
there's some way to get that damn return
PC right it's a constant and I store it
into someplace and thread-local storage
and as ole vertical storage for any
given thread was found by masking off
the low order bits because all our
stacks were too mega lined so there's
some cheap way to get thread-local
storage and you store this thing down
you start on your stack pointer that
store the stack pointer is the atomic op
that enables the stack crawl and as soon
as that goes down the very next clock
cycle the GC can take the stack lock by
casting on the stack pointer value and
then owning this thread stack scribble
with it to his heart's content and if
the guy comes out of the call to the
native code he has to take the lock from
GC again at the bottom can't see but the
eye chart is basically cast my stack
pointer back down hoping to put an O
over the stack pointer if I win I own my
stack I can go and if I lose somebody
else owns my stack and I have to go
go to sleep somewhere and wait for GC to
be done usually active what happens here
is you you run off and immediately try
to do the work of crawling your stock
with the GC so you can get moving again
there's some you know lock free idiom or
wait for you to them you want to do here
but you have to do a caz op with the GC
odds and ends for the day night calling
convention need the Janna in the
environment which is just some place in
thread-local storage use a service for a
native code you have a temporary handle
set that you let them have a stack of
handles and then you push and pop it so
you store the address of it in beginning
and you reset at the end and the native
code can make handles really quickly and
cheaply and then you can throw them away
fast so the little extra work there
profiling tags different times I've done
plus plus on stat counter so how many
times a native call gets made or set a
tag down so I'm in it and then if you do
a late into a stack crawl across all
your threads you can say what native
code a thread is in or not and then you
can do good profiling heuristics after
that so then a native call looks
something like this make a stack frame
Jane im's handle eyes some argument
shuffle the rest arguments around
because the floats
doubles in the wrong place put your PC
and stack pointer down so you get a
stack crawl make your native call grab
the GC lock again
unhand eliezer results clean your stock
up return it's like 4050 instructions
and all that complexities there's
because you're being defensive against
what the hell in ativ coach didn't do
you have no idea okay
so dead silence more than you would
think would be necessary by a fair
amount yeah yeah I didn't go to the
cases were like oh you had to pass 20 or
30 arguments for some of these you know
swing calls and then you have to like
make a huge stack slot and you have to
put handles in the stack slots not just
handles and registers and it just goes
on and on on there Mike okay I don't
have to we would you you do you do if
you fail the GC lock but not if you
don't okay fine okay so now we talk
about things that worked well things
that I did that I would do again so I
love the notion of safe points they seem
to work out really well in practice easy
for the server compiler to track and
optimize and actually this was key it's
not just for GC also this is your D op
point this is your unwind from a broken
optimization point and so you have to
have a complete state map for how to go
from you know the generated machine code
to some architectural e define Java
point Java bytecode point right okay so
this this mapping is complicated in
full-fledged and fairly tricky and
keeping it right means you're caring a
lot of state around the states expensive
to carry around so it hurts performance
so if you never Diop you can throw away
all this dead crap but if you're going
to do you have to like rebuild the
interpreter state even if the JIT can
prove that this variable is dead the
interpreter is going to use it and do
something with it right so you have to
carry the value along for the
interpreter
I think I can break and look in this
debugger and get the variables out right
even though they're dead by execution so
you don't want so many of them defining
them as a safe point instead of
everywhere means in between safe points
you can optimize the hell out of things
and so the the C to aggressively removes
them stretches them out makes them as
far apart as you can get as long as
every code path has one somewhere so you
can take a GC usually there are thousand
instructions apart or further bonamana
rec sneaked 6000 ops is you know a
nanosecond silent with house it's pretty
quick so safe points come often enough
but when they're stretched out like a
thousand apart the amount of extra state
you're carrying along from from one
point to the next can be reduced by a
lot the next thing you do is when you
stop a thread that you can tell him to
do many self service tasks in particular
his own cash is hot in his own his own
stack is hot in his own cash so if you
want to do a GC crawl on a running
thread don't have a GC thread call that
thread because everyone's caches are all
wrong till that thread crawl you're
damned stack and then come up with a
list of oops and put them down in your
own memory space somewhere and later in
the background the GC to ID can come by
and scrape them and say thank you mister
for your stack words and carry on right
turns out polling was pretty cheap on
next city six does I mentioned before
grabbing a bit from thread local memory
was you know masked the stack pointer
compared against a branch it's perfectly
predicted and in away you go it was just
dirty and then the cooperative
preemption we talked about already
heavyweight JIT compiler another thing
that I think worked out really well in
practice this is what gave Java the
reputation that it was as fast to see it
was this heavyweight compiler for peak
performance and when we first started
down this path we didn't know how
heavyweight we could be and still be ok
so the early versions of C 2 were in
fact really fast as fast as C 1 is now
and over time I could get more peak
performance by slowing the jet down and
everyone would say no no it's too slows
too slow but we want a better spec score
so better specs score it was every time
and then you got more and more
organizations and it turned out things
like the loop optimizations which
very heavyweight loop unrolling loop
peeling invariant code motion range tech
elimination by doing loop cloning the
start and in loops and stuff like that
actually fairly cheap to do and they
paid off massively you got C or Fortran
speeds out of the inner loops you had a
straightforward striding stencil style
calculation inner loop same code as a C
compiler same performance as the C
compiler cool stuff and we can't because
Java arrays are ragged so there is a
well understood hack for doing it but no
one's paid the engineering costs to go
there I could tell you how to implement
that but it's you know a couple man
years to go burn but then you then you
could you could get that you could do
loop tiling and stuff like that in Java
it's possible graph I are so very
non-traditional but it turns out to be
very fast and light once you understand
how it works
it's like graph rewriting rules it's
very easy to extend throw in a new node
in the graph IR and add optimizations
that do it it's all people optimizations
99% of C to ops people optimizations now
finally the last piece total resistance
the graph coloring allocator it is
robust in the face of over inlining and
that is crucial because it means I can I
don't have to tune in lining knob so
tightly so the common bad pattern for
register allocators and I've written at
least eight in my life of which at least
three have been in common widespread
production use is that once you get too
much register pressure and begin to
spill one good spill deserves another so
there's a real threshold and passed it
your performance tanks and you're all
tongue full spill code so I worked hard
diligently to make that allocator be
robust in the face of over inlining
where if you began to spill you just
spilled as if you would be doing call or
call a save and you had an in mind you
know the goal of in line you just let
you have more scope and do more
organizations around the in line stuff
if you fail that in line in with call or
call e save register shuffle games right
at the call boundaries and then you're
running the hardware you know card was
running instructions in the middle
anyhow if you over in line if you in
line you get rid of the caller color say
boundaries and maybe there's some code
across the diffuse boundaries that's
great if you over in line you begin
spilling if the spill costs are no more
then call or call or save costs then
they didn't cost you anything to in line
right you didn't lose for in lining so
once we got that fixed we could crank up
the in lining knob a lot and
occasionally would over in line and
occasionally it was useless but it
didn't matter it was just as good as not
in lining but even the other way and you
failed in line life can frequently suck
as attested by everybody who's
complaining about method handle
performance you know once that call
chain hits the nine a number I wrote 20
years ago I did pick nine sorry yeah now
you know you a fails to in line and then
you know performance just tanks so in
line is the crucial optimization but
getting the allocator to not spill when
you've over in line is the thing that
makes over in line impossible it's
actually very much a region-based
allocator under the hood if you go stare
at what it does it breaks up things by
frequency groups and with disjoint
sections between them and tries very
hard to spill around regions of similar
high frequency there's high pressure
notion and a high frequency notion I can
I well you can get better and worse
here's the trick you have to get on
average better allocations but when I
whenever I went in the hot spot and went
to get for more performance out the
number one place I could get it was this
guy because it's the handling the miles
and miles of crap code well there's the
key to getting performance out is you
can't do it there's no magic bullet
optimization that's what I'm saying
there's none of these like loop
invariant code motion of those things
weren't applying and all the crap code
it's the register allocator okay other
things that worked out well
portable stack crawling code I needed a
notion of a stack pointer in a PC II
need a notion of go me the next stack
frame the next the next basically a
frame iterator for hardware stack
turns out this works for a wide range of
CPUs and OSS including with you know
Itanium register windows spark roger
williams register windows and azul
register windows which are all different
forms of register windows and people who
didn't have register windows people had
multiple different kinds of stacks frame
adapters as opposed to adapter frames if
you've been in the guts of hospital
while you've probably seen adapter
frames so a frame adapter is one where i
am adjusting the call layout from one
kind of call to another i'm coming from
an interpreted call to a tinted call
from a duty call to a native call or
back and i want to shuffle the registers
the exact shuffle depends on the
signatures involved but nothing else so
it's a little snippet of code that says
shuffle retro A to B C to D it's a
parallel read Greg copy operation and
then jump on to some other place and all
you're doing is to shuffle it's the same
with what method handles basically do
it's an argument shuffle and go once you
get this thing figured out it's really
cheap and it lets you jump between
different layouts for different
generator code generators really quickly
the notion of a code cache one of the
key things that didn't seem like key but
it does now is that all your code lives
in the same 4 gig space even on a 64-bit
VM so that you can use a 32-bit program
counter everywhere you use the cheap
version of a local call instruction
instead of a 64-bit call which typically
has a lot more overhead and cost today
so big savings on both x86 and SPARC and
most risks I looked at versus some sort
of far call so the code is contained
within a four gig region didn't matter
where blah blah blah a lot debugging
flags I think somebody else mentioned
this here hotspot has a lot of them and
what they're really doing is testing all
the rare one-off cases so you know you
have some semantics to match mostly the
good thing happens you allocate it's a
bump point and you're done and every now
and then the bad thing happens the the
heap ran out and you had to do a GC
cycle the class got loaded in the code
you generated it's crap the the lock you
were trying to acquire was in fact
contended and now you have to go to
sleep no less so
all those rare one-off cases you make
them common by doing these uh a lot
flags say the cool optimization I did to
make bumper allocation work failed call
him the VM as if a GC cycle has happened
and the mealy VM does like oh you
shaft need a safe point let me go verify
that you're in fact R and then I'm
tracking all the loops things like that
so you get all kinds of great of sorts
and stuff a lot of bugs pop out very
easily here you can stress test all
these cases very easily you can hand it
to the QA department say add this flag
and run a long time because things
obviously get slow because you're
cutting out the fast path but you get a
lot of bugs caught it really quickly
thin locks
bacon bits hot spots in locks whatever
seen a dozen different names for it
basically it's caz on an object word on
the lock calves on the unlock hardly
matters because it's all cash hot now if
you successfully got the lock if you
rent a long time then the Cashman's
didn't matter but if you ran a short
time you unlock what's in your cash so
it doesn't matter here so it's just caz
on the lock that counts this is old now
I don't know if this is a bias locking
on Azul turn on bias locking years ago
but son was slow but I thought it was
getting there it's off it is on ok fine
so this is just saying it was
interesting too many Java locks are
never contended you want a thinner lock
which is you know I own it but I don't
and then eventually if somebody actually
wants that you have to go find out
what's really going on and Java memory
model I was certainly there before the
memory model came out I did the first
reference implementation of memory model
work obviously in practice that has
worked out really well because we all do
Java util concurrent wallet wah forever
in a day ok hard things but worth doing
so things that took a lot of more
engineering work than I thought they
would being portable being portable was
a giant pain in the buck but but what it
did was it made the system it separated
out implementation from architecture in
a great way so the ideas the
architecture was an
got split you got much better code
discipline and suddenly things got a lot
easy to understand what was intended
there was an implementation of oh here's
how you call a stack and then there was
a garbage collector wants to crawl the
stack and find all the oops
and the implementation of crawl the
stock is not anything the GC guys really
care about they just want a collection
of oops that a root set and go right so
there's a lot of places where this
turned out to be really well how you do
an inline cache how you do any of the
code cache games at all how you intercal
between the interpreter that you're
trying to write in some portable way
versus a digit code separating these
things out was forced by the port and
was useful ultimately it was a big
engineering job when it first happened
and then adding a middle tier as I
mentioned I've done c1 c2 I know people
have tried a lighter-weight c2 where's
the heavy c2 I don't know what's the
current default and hotspot is it c1
plus c2 or is it a it is okay yeah for a
server yeah okay deopt I mentioned this
before hotspot does this no cost at all
in line on vitals including no wine in
the sand if you do override the code
ultimately you have to recompile but
otherwise it's full you know game on you
have to flip your compiled frame into an
interpreted frame and it's it's not
rocket science but it is nitpicky so
just be prepared to understand the bits
and bytes of stack layouts and machine
instructions but once you get it right
it's right self-modifying code there's a
lot of it so you do a lot of code
patching for inline caches every here
nobody inline caches so who here fail
does not know when inline caches that's
good that way around there's a handful
of people
okay so near line cache is the is the
thing that makes java calls like cheap
enough to bother otherwise you never the
language never got anywhere it is by C++
things are non virtual by default you
have say virtual and then you know
you're getting an expensive call site
where somebody does a load load jump
register and job it says the way around
by default everything is virtual so the
obvious implementation is
Oh jump register and actually in hotspot
it's three loads they're all data
dependent and then a jump register which
is going to you know cost you 30 clock
cycles or something horrible whereas a
regular calls like one clock cycle so
it's you know it's a huge difference so
in in like cash says gosh the call that
I took here last time around is to the
same method as the call I'm taking this
time around can't take a there's a
one entry cash in your code the cash is
the key to the cash is your class and
the target of the cash is the target
method as encoded as a machine
instruction directly in the call
instruction so you load the class of the
object you've got you compared against
the cash that is the class that is the
cash here if you get a hit you take a
call instruction go into your target and
you can order the calls on the branches
in different ways and bounced about the
same thing but it's a load compare
branch inland caches in practice they
either fail out right away like your
mega morphic that calls that goes multi
targets or they never fail for the life
of the JVM so if they're never failing
it's a load compare branch call where
the load compare branch is entirely
predicted so the x86 just those at this
pipeline it takes a clock cycle to go
through of course if it fails you have
to patch the code and say well that
didn't work so I need to do something
where I go jump from a call to like a
load load load call and have to have all
the right set a load soda in there so
you have to patch the code around and
then you have to patch it in the face of
racing Java threads who are busy running
this code hot in their caches they've
been running it a long time it's in
their eye cache it's in all the layers
that are D caches and you're patching it
and what did they do well they saw some
partial piece of the the patch they saw
all the before the Hat after the in
between if you ever cross a cache line
boundary you can't be atomic about it
there's a bunch of issue so patching is
a pain in the butt but it can be done
and you have to make sure that and you
can't like as a mission you can't do it
actually can't do an atomic update here
um you can it helps but you can't across
cache line boundaries to some parts of
instructions are not allowed to cross a
cache line boundary so that you can
patch them therefore sometimes you put
an OPS in in order to push the thing to
not miss a line on a cache line
to do this hot spot uses a high level
language assembler so you're writing
assembly code by hand except you're
actually running C code which looks like
assembly code but when it runs it just
emits that assembly code into a buffer
it's very easy right oh but you get all
kinds of cool invariants provided by
this high-level assembler he will check
that you didn't break any of the rules
he'll check that you're doing the right
register said he'll check that you
you're not missing to a safe point I'm
making a blocking call by accident
he'll check that you don't have the yeah
insert the right knobs for these inline
cache patching things there's other
places where you have instructions that
you know have to be patched and so they
all soft be aligned you get all kinds of
cool support out of this and then when
you're done he like marked all the
places where you had inline caches
because they have to go in the hoop maps
and the D op maps and such so you can
find them patch them later if you have a
GC point he marked that if you have
other kinds of calls and stuff there's a
bunch of things he does so this notion
worked out really well you're writing
assembly code by hand because
instruction by instruction you have to
know what dies but then there's a lot of
times that you want some help and you
can get automated help out of this thing
Azula went to a single word header on a
64-bit VM turns out that's actually
pretty substantial savings in memory
which turns into speed in your caches
because your objects all shrink by one
object word and therefore you got that
much more in your first level table old
D cash right so to make that work
instead of using a 64 bit class pointer
a header we have a class ID so we came
up with a notion of however many bits
you want
16's to few but 19 or something does it
also in there we had a thread IDE which
I mentioned before we took threads by
chopping off the low order bits to get
the thread local storage well the same
trick was just shift it right and that's
your thread ID and then we had more bits
left over once we did this compression
that we get at hash code take all 32
bits because turns out when you have a
really big VM with you know billions and
billions of objects 32-bit hash code
he's getting a little scare so you're
guaranteed to get lots of hits lots of
collisions
okay here I just said this align your
stacks on a boundary and then shove this
thing right or left to get the stack ID
out use a TLB page protection to catch
your under flows and overflows at the
edges we had various cool asserts where
we would protect the whole stack and say
we think everyone's at a safe point now
and we're gonna crawl the stacks but
we're gonna throw on we're gonna you
know TLB read protect or repeal the
write protect the stack and then start
doing our DC cycles and occasionally
you'd catch some thread that ran on past
the safe point because he skipped it
somehow and started scribbling on the
stack and you catch these bugs that way
thread pointer is very common in core VM
code turns out getting your thread
pointer is very common in hot versions
of all kind of Java code where people
say thread current to get threaded cool
storage the Java layer you get it in
like one clock cycle here by just taking
your stack pointer and shoving it over
with that notion that we can get at
thread-local storage quick we came the
notions we could stop to individual
threads with just the software pulling
where they each thread asks his own
thread local storage should I stop now
and do something and if he does he
discovers he has some self service tasks
to go do common cases be like a call
yourself stack for GC roots or maybe
just flip all the bits on the pointers
maybe you got told you took an exception
some other thread through a thread that
death at you well you're only gonna
install that a safe point but now you
have to begin throwing an exception
where you had no idea you would ever
throw it before or you get a stack
overflow or an out of memory things like
that you get these horrible exceptions
revoke a bios lock you hold that into a
lock but you didn't really you're
weren't paying attention anymore you're
trying to be fast about it somebody else
says I want that lock and you have it do
you actually have it or you just like
lying about it then yes to call his own
stack and discovers you locked or not so
it's a self-service block text same for
debugger hooks conditional breakpoints
inspection stack cleaning in like caches
certainly inline caches I was just
talking about there's a bunch of phase
transitions you can do and then provably
there's a final phase transition which
you can never do unless everyone's at a
safe point and that is you can't clean
the inline cache and reset it and I can
talk to people about why
is what it is but it can never be undone
unless everyone is guaranteed not in the
middle of an inline cache idiom and you
do that by stopping people this way so
and then the cooperative preemption was
the other obvious case that we went to
so we had a lot of fun use cases for
safe pointing single threads so know
what we have now does okay so as all
certain head Hardware transactional
memory we play with it for a lot of
choices actually for the inline caches
saying the the transition from dirty to
clean is one that only needs to be taken
very rarely so it's not performance
critical the other transitions we
figured out how to do them with single
word atomic patches a series of single
word atomic patches and that was cheap
enough but it was definitely it's hard
to engineer it right and after like four
times around I know how to do it I have
a very nice little state machine I draw
and I list all the transitions carefully
and the listening instructions the
pieces are going to change the data data
and then and then the code for it is
very simple it's really you know it's a
couple couple of cows instructions it's
like dirt cheap right so you're not
worried about performance changing the
state on it you're mostly worried that
the guys screaming through it at full
speed get the right answer okay things
that I did that I won't do again yeah
write a VM and C+ flies
Oh Java is fast now and in particular
mixing oops in a non GC language is a
total pain because in the hotspot VM the
this pointer is frequently a nuke and if
you span a call it takes a block for any
reason or allocates for any reason
though this pointer as an OOP might get
moved up from under you now the C++
compiler has no clue that that happened
so he might have the old version and he
might have the new version he might you
know mix the two you might be stomp on
the old one whatever fine oh my god so
you had to wrap every this point of
reference accidentally that might happen
across a handle and you had to have some
sort of support to track them or else
she failed to wrap them and it was just
endless grief well I just said Java
plenty fast now but there are other
languages floating around there as well
but I would write one in a language that
knows about the oops directly so that
the hard now heart piece we're now
beginning the garbage collector to be
written in the language that is spinner
you know in its own self language and
that one I've seen people attempt and I
don't know how well that's actually
would work out in practice but like the
rest of the VM he was class loading and
getting code you know all the runtime
support for managing the jaded code it's
all easily done in Java in fact it is
all we put all the the perm gen went
into the main there were no perms and
goes in the main heap the jaded code
goes into the main heap as well so all
the lifetime management of the
code is done by the garbage collector
there's a ton of that stuff but we just
like yeah why do we did hell we managing
it for so there's a lot of roll-your-own
Malick's
you lookin hot spot right now and count
the number of times people have a better
way to manage memory that's not GC but
it's it's stacks it's regions or
whatever c2 is got plenty of them um
they're like ten at one point I counted
a ton of them
c2 does burrs bottom-up pattern matching
kind of useful if you're writing Kovacs
never needed on a risk chip never needed
on next 86 at least not for a long time
now x86 architecture is actually fairly
regular the actual encoding the
instructions this kind of bizarre but
the use of one register for whatever
location that's all it's all the same
and furthermore dropping or adding the
instruction here in there because you
didn't get it perfectly correct is like
not interesting for performance but it
adds this giant level of indirection and
the JIT engineering to get the you know
the generated code out from the guy
who's doing the generating they have a
you know it's like playing the piano
with boxing gloves on the layering
directions just in your way I have
claimed no I claimed it is not easy to
maintain it is not you could do
something with just c++ overloads for
code emission that would be one hell lot
easier oh no I wouldn't use yak either
no I would never do any of these tool no
I would never use a tool in between the
cogent and the compiler like this again
I have done this repeatedly in all
compilers since like the 70s we're
always using some flavor of burs and yak
and Lex whatever hell is a tool between
the optimization in and the cogeneration
end and that thing was always like in
the way but not so badly in sort of a
classic C compiler mode but in hotspot
where I had to have fine control over
what was an open what wasn't who was
being tracked and where you can align
these structions are not crap like that
it was always just in my face I'm
serious we can go down that line if you
want but I'd done this for a long time
and I swore at this thing in the sleigh
never ever do it again
okay hotspot used to have patch and roll
forward safe points I don't know if they
still have them it made it very very
heavyweight to safe point a single
thread at Azul the cost a safe point of
thread well as you said a bit and you
waited and you waited usually less than
a thousand clock cycles and then he was
safe pointed and so that was just so
much faster than what hotspot has been
doing which was stopped the thread ask
for his PC past the code he's in full of
breakpoints and then restart him
and then some other thread you didn't
want to stop had to have the non past
version you had to have two versions of
code patched and not patched
occasionally the OS told you the wrong
thread and you passed wrong thread you
waited awhile he didn't actually stop
because he didn't patch him there he was
in other the code use an and it was just
like a giant mess just pulling it
totally did in fact I'm sure I fought
for it you know 20 years ago and now I'm
going to fight against a generic collie
save registers it made it a real mess to
crawl stacks
x86 never really had enough registers to
need them register windows work fine are
almost unrelated but but there's only a
few inch tart wearers that have many
common registers and no register windows
for which you might contemplate Kali C
registers being useful but they're not
worth the engineering costs to maintain
a stack crawl on them as people know
what I'm talking about here and the
khalaby's the issue is somebody called
you and you promised them that you're
gonna save register RDX so whatever your
colleagues say register is so you save
RDX and you restore it when you're done
you block for GC some in the middle it's
already X containing OOP or not well you
don't know you just got told to save
this register well different call paths
will have it as an uber nots you have to
go up to call or and ask him hey did you
pass already X what was it was a new per
not well he said I copied it from our CX
and got it from somewhere else and yeah
again again and actually it can be
exponential this is not even a linear
chain so it's just but last year in hell
ok adapter frames versus frame adapters
this is a thing that I put in and I
totally regretted it and this was a way
to do a shuffle between registers as I
mentioned before except the adapter
frame left a frame on the stack as
opposed to frame adapter which just did
the shuffle and is a tail call out of
there didn't leave a frame on the stack
when you leave a frame on the stack the
extra frame screws up all kinds of
things having you do a stack crawls
because it has no semantic meaning to
anyone else it's just there as a
leftover from from reorganizing the the
arguments it does let you reorganize the
return address and if you don't have one
then it means the return register has to
be the same for interpreter and gated
code which is not typically an issue
you're just gonna pick a register you
can return a value in and everyone
agrees that that's the register green
turn in don't do it don't don't put any
extra frames on the stack one frame per
architected language frame or fewer if
you've got inlining constant oops encode
on an x86 that looked x86 that looked
really good as a 32 media instruction
except of course it would be misaligned
across a cache line so you had to patch
the damn thing
but on SPARC and PowerPC and other risk
tips you couldn't build a 32 bit
immediate so you had to have multi
instruction patches and so because you
couldn't get those atomically you had to
only patch them with all thread stop
which meant you had to stop the roll
pause and the GC was required to patch
the OOP and there generated running code
and then had to flush all your eye
caches okay fine instead just pay to
load the die from the table put in the
code an offset into a table so there's
some table base plus offset that's not
ever changing it's some constant thing
fine and then you do a load get your OOP
out of memory well you can schedule
around some known latency load it's
entirely friendly to all GC algorithms
because I just have a big old table of
loops so they can patch whenever they
want do what the hell they're gonna do
and turned out to be on the on the big
VMs we had 64-bit pointers as much for
your instructions too it was actually
faster locked object header and stack I
think hotspot might still be doing that
which meant you didn't have to count a
recursion count because you had
instances of a lock header and the stack
as your account and it was unary
counting it was a total pain when you
want to put a hash code in or inflate
the lock due to contention or you wanted
to move the thing during GCE
don't do it just just update the header
with unlocked or not and some thread ID
notion saying which thread owns that
lock and then from there if you actually
are worried about that lock you can go
to that thread and find where he took a
contended lock and track the who from
there I retract the locking issues from
there okay so I mean look at the time I
talk fast it's been an hour my voice is
getting raw I have another set of slides
equally as bad is what you just saw
for codon loading and why you know life
sucks for people who had to unload code
especially if they want have high
performance and unload code I know that
no one's actually running it at any
given clock cycle but I think I'm out of
time so maybe a QA a little bit or
something here okay if people want them
I'll go in it doesn't make sense as a
lightning talk so a QA and then for like
one or two and then be done all right
everyone's too scared I'm gonna bail huh
yeah
that was the that was some the JIT
decided that he could take due to
whatever random spill decisions this
register or that one would end up in the
same collie save over some call site
going forward so when you worked it
backwards you didn't know which code
path he'd come through so like in Tri
finally there's these replicated blocks
of code for the finally that you've
smooshed together in the optimizer and
so you have these horrible code paths
from all over that are folding down to
the finally clause which then has a call
to like build a stack trace or other
stupid crazy crap and so he took call
Issei registers wherever they happened
to be and jammed them into whatever
other thing was convenient because the
register allocator doesn't have any clue
about these things he's just doing live
range folding left and right right
so you would get bizarre back live
traces no webs in the in the library
that were bad yeah oh yeah sure so no
you're off by orders and orders of
magnitude so so in a typical large type
of program say half of all call sites
are statically determined with a jet and
and he does where he does with a static
call psyche in line so he makes a static
call or whatever the hell the other half
go dynamic the ones that go dynamic
ninety-five percent live as an inline
cast their entire time to take a one
time cold cash go from empty two sets to
the inline cash and then ever change
again for the entire lifetime those
instructions will get executed millions
to billions of times perhaps billions
per second
so a what not I mean is a a key
performance optimization the five
percent that fail out go mega morphic
really fast so like a two-way cash you
know to doesn't ever actually pay off if
you went more than one usually one
twenty or thirty and those guys just end
up doing a load load load jump register
and then just pay the price and go but
for the inline caches they really pay
off well it's a key performance hack
well you know every time that your
performance sucks and you go look and
you see it didn't like do it then then
the answer is you know you lost the
inline cash game yeah yeah um I didn't
talk about how many times you went
through an optimized call where the JIT
decided he knew what it was I was
talking about number of times you
actually went through and I actually
know the percentages because the inline
ones like really in line they just pure
way so actually counting them is pretty
expensive and it screws with you but you
can do this games it's interesting that
they were out there when Java was first
like really exciting people doing all
this work on it those kind of numbers
came around this what the relative
ratios were but it's for inline caches
you know it's it's hundreds of thousands
to millions of inline cache instances
each of which are running millions to
billions of times and without if I
should bail unless something else go
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>