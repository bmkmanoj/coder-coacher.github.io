<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JVMLS 2015 - Compilers are Databases | Coder Coacher - Coaching Coders</title><meta content="JVMLS 2015 - Compilers are Databases - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JVMLS 2015 - Compilers are Databases</b></h2><h5 class="post__date">2015-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WxyyJyB_Ssc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">but first thanks for having me here give
the keynote I'm very much honored by by
the privileged and it's great to be
finally here the jvm language is summit
after for so many years I just couldn't
make it because it was always during
vacation season in Europe now we decided
we do vacation in California so that's
what we did and I can come to the summit
great so you might say well that's
really a shameless thing for Oracle to
say well over since Oracle the database
company now this guy's claiming the
compilers are databases just to suck up
to Oracle so now it's a it's not that
in fact compilers I've come to realize
are very much like databases and I hope
that in my talk I will explain a little
bit why that's a valid mode of thinking
so if you look at compilers then a
compiler is essentially that sustain
that picture if you look on Google Image
Search that's the first thing that pops
up probably so compile is a source
program it translates into an executable
program and that thing then can be run
in a database database we all know it's
this big round thing right there so
that's the thing so so this thing looks
pretty square and this thing looks
pretty round so aren't we putting a
square peg into a round hole or vice
versa well we'll see so in this talk
what I'm going to do is I kind of report
on a new compiler architecture for the
dotty compiler dotty Scala compiler that
I've been working on over the last two
years a little bit more than two years
now and that compiler has a mostly
functional architecture which is a new
thing typically with compilers you say
well a compiler is not allowed to use
any of the goodies that the language
provides it typically is written in a
very low-level language like C or it's
written in Java but typically you don't
make use of any of the high-level things
because compilers have to be fast and so
essentially you you do all this for
everybody else but you can't partake
of it so that we have changed here so
now essentially the compiler tries to
use all of the functional goodies that
we have in the language and it turns out
that some of the concepts in the
compiler are very much inspired by
databases and in this case I mean
functional databases so I've been
essentially compiler guy for all my life
I started in the ideas I wrote a Pascal
compiler that got bootstrapped on an
Osborn 152 kilobytes of memory first
version was in assembly code because the
only freely available language at the
time was basic and that wasn't unusable
so we wrote it in assembly and then we
bootstrapped Pascal mini Pascal to micro
pass micro Pascal mini tasks are for
Pascal and that became modular too and
those compilers were all essentially
very fast single pass compilers pretty
much actually like the go compiler and I
think it's no coincidence that actually
one of my colleagues in the group of
McLeod's work oh but grace emma is now
one of the core members of the go team
so there you see sort of the genes
spreading around so then in the 90s I
wrote a Java compiler called espresso
and that compiler actually became the
basis of the language which is a very
interesting language in its own right
that essentially is vaguely Java based
and it also became the the compiler of
Borland's jbuilder so that used
essentially a very object-oriented
approach where at the time object
orientation was all new in Chinese so
everybody had to do this so the
conventional wisdom was well you should
have an AST and it has like 40 years
classes for the different kinds of notes
and essentially every node is a class
and we put all the methods in the class
because that was the object-oriented
dogma so essentially you would have 40
methods to type-check 40 methods to do
flow analysis 40 methods to do pretty
printing 40 methods to have cogeneration
and of course it was all a huge mess
because logic was spread out in each
case over 40 classes so that we learned
to do better than in the next compiler
architecture that started with the pizza
language in the 90s that became GJ and
that became Java C from essentially one
point for one point for now one point
three on and that became then in the end
Scala see the first colors he used a
very similar he textured so first gonna
see was actually written in a dialect of
Java which is was sort of like pizza and
somebody else wrote it so it wasn't
exactly pizza but it was Java plus
pattern matching because para matching
is just too good not to have when you
write compilers and but otherwise it was
basically Java and we wrote Scala see in
that Java so then from 2004 on we had
the second cause Scala compiler which is
still the workhorse that everybody is
using for SCADA that's called NSC before
knew Scala compiler the time it was new
and that essentially was written from
bits in Scala from the ground up they
made use of some of the functional
capabilities of of Scala not that much I
have to say because speed is important
and we didn't really trust the
functional abstractions to be fast
enough at the time so over the time this
thing grew a lot so the first thing we
added was a wrapper than we added
essentially the presentation compiler
interface for IDE so the Eclipse IDE and
I think also the NetBeans IDE and the
enzyme plugin for it's actually editors
all uses essentially the standard Scala
compiler to do essentially anything
helps and then in 210 that came the
macros and where the toolboxes is
actually right where you could interact
with the compiler at runtime and that's
the code base for the official scalar C
compiler for the current version 211 and
also the next version 212 so what I have
been working on for the last two and a
half years was now essentially a new
compiler base where where the project
name is called dot e and the idea there
was we wanted to rethink
compiler architecture from the ground up
and also introduced some language
changes with the aim of better
regularity
so we're currently close to bootstrap
but the whole thing is still rough rough
around the edges so it's not really
production quality yet so what I'm gonna
talk now is essentially what came out of
the thinking of the architecture of this
compiler so a traditional view of a
compiler is it's essentially a pipe it's
a transformer that takes source programs
to output programs classical function
you could say it takes it takes an input
it produces an output and well of course
typically you have several sources and
if you add separate compilation then the
picture becomes a little bit more
complicated so the compiler doesn't just
produce the output the byte codes or the
the assembly code but also essentially
some simple table information that can
then get fed into as an input when you
do separate compilation so essentially
for every one of these simple table
things essentially gives you the API of
a separately compiled file and these get
set into the compiler but basically it's
still a transformer it takes it takes
inputs and produces outputs so the
challenges in this view is that a
compiler for language like Scala is
actually rather hard to write so there
are quite a few challenges the most
important are these things get complex
the other challenge is we want the
compiler to have reasonably high speed
and that's very hard to achieve we want
it to be fast in the latency that means
for a rapper or for an presentation
compiler it's important that you get the
back that you get the output in a in a
split second and we also need to use the
same compiler code base in a number of
different systems so we want to make
this have this code base be reusable for
many different applications so
complexity well the complexity you could
say well isn't that is that inherent or
did you just maneuver yourself into into
essentially a a hole here well I think
it's to a large degree inherent
that a compiler for a complex input
language like Scala is also a complex
thing so the input language Scala is
complicated but when I say that I would
say well essentially it's not
fundamentally different than let's say
Java 8 to Java 8 is by now also fairly
complicated input language and I guess
comp Kotlin would be a fairly
complicated input languages these things
are not like simple academic languages
which you can describe on a page they
have a lot of real world concerns the
next problem is that the output language
the JVM is also complicated so you have
essentially a complicated input language
a complicated output language and for
Java the saving grace is the two are not
that different Java and the JVM is
actually developed in in in lockstep and
so essentially a lot of the things that
the language needs the JVM will provide
whereas for Scala that's not true
there's a huge semantic gap between
words what Scala is and what the JVM is
we can bridge it but we require a lot of
transformations so it means that we have
a essentially a complicated system that
requires a lot of different
transformations to map into another
complicated system if you compare to
let's say simple low-level languages
like system f4 SSA they have it much
much easier so for instance the Haskell
language is also a complicated language
the type checker is quite quite subtle
and complicated but once they're done
they're map into essentially system F
plus constraints and that's a very
simple intermediate language which is
much more malleable than what we have to
do so we can't map into that of course
because then we cannot recover the
high-level information when we want to
map into the JVM and be interoperable
with Java that's why we have to stay
essentially on the complex high level
all the time so the next reason why it's
complicated is that there's a very very
deep transformation pipeline so I don't
know for Java C right now how many faces
would there be between between the type
checker at the backend I don't know
Cizek so that's that's a subset of the
current system so I think we have like
40 and they bunched into in
macro faces or their fuse typically but
conceptually there are 40 different
things that have to have to happen here
so that's of course much much harder to
to achieve good reliability so to get
good reliability here you need excellent
modularity and also it should be clear
now that you need to minimize side
effects so the the nightmare bug is that
a face somewhere here
introduces something fishy and things
propagate and then somewhere here things
blow up and and you say well what what
what's the reason for that so how can we
trace this back to where it was
introduced so you really need a lot of
discipline to avoid that and you need to
essentially minimize the side-effects
and be able to test a lot of your post
conditions so for all that reason it
really would be very attractive to have
a functional style because functional
code is known to be essentially much
safer in that way you can control your
effects much better if you write things
in a you know predominantly functional
stuff so the next challenge was speed so
the current scalar see a compiler
achieves about 500 to 700 lines per
second in idiomatic Scala code if you
write Scala like you write Java you can
get faster than that but very few people
do if you write Scala like let's say
shapeless or if you write scholar like
Haskell plus plus type level Haskell
then this discount can drop dramatically
it can go down to in one in one case
maybe I I saw a single line query that
took six minutes to compile why well it
was it was a shapeless thing so
essentially they have a very elaborate
recursive implicit search it's basically
you write in Prolog write a single
Prolog query can easily take six minutes
it can take six days to run write and
implicit is basically gives you the same
functionality as Prolog and well people
use that and so that was in particular
essentially I
a way to write records in a statically
typed safe way
sort of like database records and all
these things work fine if the record is
maybe five columns and ten columns four
people said no no we want to use it for
records with lots of columns hundreds of
columns because that's what the real
databases give us and then of course
well that's I this was some I don't know
high polynomial or exponential factor
and it just blew up so it just tells you
well that's sort of the EB the worst
case normal calipers calico doesn't have
that you get maybe 500 to 700 lines but
again Java C is much much faster than
that so we would rather want to be able
to be let's say a factor of five faster
than the 500 700 lines so everybody
would like it to be faster and it's very
hard to achieve so one of the over the
years one of them graphs that I was most
I watched a lot and it was very
frustrating was the build times so this
kind of C build times well so that's
essentially the time that it takes color
C to compile itself and the graph looked
pretty much like this so it was a an
inclined plane and there were lines
there are some outliers when we did
something stupid so something that
introduced much slower compile time and
then we corrected it and went back but
there was this background increase which
essentially happened because people
introduced more and more of the let's
say high level nice features so as
people manage the code they said this is
a while loop this is a nasty let's let's
let's use a higher order function
instead and things like that and all
these things have sort of a microscopic
cost so every way and everything
essentially adds up and that led to
these things the other problem is that
optimizations are singularity singularly
ineffective for compilers if you've ever
looked at speck marks let's say Java C
and you see Java C is one of the spec
marks spec marked benchmarks then you
see essentially all the optimizations
that do something sometimes something
dramatic for all the inputs most of the
inputs except Java C Java C never moves
no optimization is effective
for Java C and I think that's not a
thing for Java CH that's not a property
of Java C alone that's a property
basically of all compilers they don't
really have hotspots it's really all
smeared out where the complexity goes
they're memory bound it's not even clear
whether the memory bounded or CPU bound
so that's why essentially most of the
optimizations or all the optimizations
all the simple ones I've seen don't work
for compilers okay so the other
challenge we facing is latency so some
applications require not so much
essentially baseline throughput
performance but very far very very fast
turnaround so for instance a ripple you
type a program line the thing but what
happens is after you type you to think
it's compiled the compiler spits out a
class file representing the lines in the
classifier gets executed and all this
should of course happen in a fraction of
a second worksheet is basically like a
wrapper just sort of a more
two-dimensional layout and then there's
the presentation compiler for IDE S
which essentially has to do a lot of
type checking for things like hyper
linking command completion and so on so
that means that all these things require
to keep things loaded and not to have
the compiler start from scratch and the
final challenge was reusability so
compiler has nowadays many clients it's
no longer just a batch command so you
can use it from the command line but
it's also used from build tools and
build tools sometimes want to be smart
about incremental compilation it's used
in IDE s it's used in the repple it's
used in meta programming so that means
that essentially the abstractions that
the compiler provides they must not lead
leak you you want to have essentially
and a good API with which these
essential environments can interact with
your compiler okay so that's sort of the
the challenges that we want to face now
and come and come up with essentially
some of the questions to ask for an
architecture and some of the answers we
had on these so let's start with the
first question
every compiler has to answer
question or questions like that say I
have a class C parameterize class and we
have a math method in the class called F
and at some point we changed the class
to this one here so now the method has
two parameter lists and the question I
ask is what's the signature of seed are
f okay the question is ill-formed
because the question should be well
what's the signature at zero that's at a
given point in time because I've just
changed it so and that's not it's not
just changes that the programmer does
it's also changes that the compiler
deaths or transformations that the
compiler this so initially this method F
had this signature X t2 t now there's a
phase in the compiler called arisia that
exists also in java c that essentially
maps down to the essentially JVM model
with essentially unreal fight types so
all the types type variables get erased
and you get any to any in Java you would
get object to object after the Edit you
have this curried function with two
parameter lists and there's a phase
before a ratio called Ann Curry that
actually merges these parameter lists so
now you get one parameter list and after
eurasia
you're back to this one here so that the
same name F in class C has many many
different types and some of them
essentially changed while the compiler
is running and the change from here to
here was prompted by an edit of the of
the user so the naive functional
approach would be to say well let's pick
essentially all the functions in my
world all the classes and just compile
them with all these intermediate
representations of which there can be
many and we get the output and then we
if you have a delta in the world so we
have an edit and we have a new world and
we do the same thing again the problem
with that is that that world is
essentially all these functions that you
can get at the world can be very large
the world can be as large as the
internet is if you have transitive
dependencies to jars that also can
change and
from different projects and things like
that so the world can be humongous and
that's why this model is essentially not
feasible you can't you can't you can't
use that as a basis of a compiler
architecture so a most practical
strategy takes inspiration from
functional databases in that case and
also functional reactive programming the
two are actually quite quite similar and
that idea is to say well we treat every
value like the signature of F as a time
varying function we just model it
directly to say well they if we want to
take the signature of FC dot F then we
have to ask well at a given point in
time what is the signature of CDRs and
remember we want to be functional of
course if you were imperative then well
any time you ask you might get a
different value so that's that's not
that that's not an issue but we want to
be we want to model this thing
functionally so what we then need to do
is we need to index every piece of
information with the time where it holds
so that's essentially our information
space that you say signature of C dot s
at a given period so what is time so
time for us is essentially a pair of a
run ID and a phase ID so the run idea is
essentially every time the compiler is
run it gets incremented so you get two
you get one one identity one number for
every every one counter for the every
time the compiler is run and the second
part of the time is the face ID that we
say we have all these transformation
phases and the types can potentially
fail a change after each phase so that's
essentially the minor part of our period
so the phase ID when runs from country
about maybe 1 to 50 or something like
that that's the faces we have and to run
ID is incremented on each compare
compiler run so that means that if we
look at the signature of seed or F now
then we have actually a well-defined
function we just essentially give it the
run interface and that would give you
the signal just that you've seen before
so I'd run one
and face Parsa you would have that at
face erasure you would have that at one
- you would have that and so on so the
task of the compiler n is to compute all
values needed for analysis in
cogeneration over all periods where
they're ever relevant that's simple
enough but again of course the problem
is I've just reformulated the problem
statement the patife graph of this
function is again humongous so it's just
like essentially taking the world and
recombining it every time I just did
have since you have sliced and diced
things slightly differently so the graph
is very big so we need more work to make
that efficiently explorable but for a
start it looks like the right model so
it looks like a high-level model it
really models what goes on here so what
I'm gonna do now is I'm look at some of
the core data types of a compiler and
how they relate to this model so the
types I'm looking at will be abstract
syntax trees types and something called
references something called annotations
and something called symbols and in
traditional compilers I think you
recognize abstract syntax trees they all
have that basically you recognize types
of course they all have that and you
recognize symbols but these things they
probably require some more explanation
so let's look at abstract syntax trees
forms first so here we have a simple
abstract syntax tree for an expression
like x times 2 so that would be an apply
node function application well in in
Scala I should say that's X dot star is
a method called name and 2 is the
argument
so it's an apply note the left hand the
function of the apply node is a
selection of X on the identity of sorry
star on the identifier X and the
argument of the apply node is a list
that consists of the single argument
which is a literal and the number 2 of
the literal of the number 2 ok so that's
the simple abstract syntax tree and I
know that of course various ways to
write that but they're all essentially
equivalent to something like that but
these are
sort of the things that we get from the
parcel once the compiler gets going more
things have to be added so we have to
put typically attributes in this tree so
what attributes do these tree carry and
in DSC we've simplified that as much as
we could and we just left with two
attributes in the tree so every tree
node has exactly two attributes one is
its position so it's essentially a start
position and position that you need for
error reporting and the IDE work and
that's intrinsic that so it means the
parser generates that already for us and
the other is the type and the type needs
to be computed by the type data and
that's it so that no other attributes
are no symbols or anything else but the
job of the type checker then is to
transform untyped trees into type trees
so type tree would like look something
like that so we would say for each node
we century ad the type which is done
here and right so let's say the
identifier X is determined to be of type
int then the select node would be of
type B to star int to int the literal is
of type constant type 2 which is known
to be a subtype of in turn then the
application is again of type end so in
the current scholar see we didn't
distinguish untyped from type trees so a
tree was essentially just had a variable
which was this type attribute and that
got set by the type checker and in
retrospect I think that was a big
mistake in particular for riu for
reusability because the distinction
whether the tree holds a type or doesn't
is actually very quite fundamental so
it's very important to know this tree is
typed or its untyped so in do you see we
made an effort to actually distinguish
type from an type trees in the type of
the tree itself so what we did here is
we said well let's give a parameter a
type parameter to the tree type and the
type parameter tells you what kind of
attribute it holds so our type tree
holds a type so it's a tree of type an
untyped reeve holds nothing so it's a
tree of nothing so we have this very
cute
nothing type which is the bottom type
which says well there's not
there if you try to get anything you get
an exception or non termination or
whatever so that leads to the following
class we have essentially based the base
type of all nodes in the ASD is of this
type tree of T where we can get the type
out and it will give us a T either type
or nothing and then there's a method
with type which is essentially called by
the type checker and the type checker
essentially it gives it a type and then
we get out a tree of type and there's
actually a clever copy-on-write strategy
that we say well if you start with a
none type type and we on type tree and
we haven't given it a type yet then the
first time we give it a type essentially
we don't need to create a new node we
can just you reuse the old node but then
if somebody gives it a new type because
the context it's used in a different
context say then you will copy the tree
at that point ok so now we actually have
an interesting question of variance so
Scala has declaration site variance for
Java see the problem would be slightly
different it would be essentially where
you whether you put the extension to
super so essentially how should we use
these trees so a question is what what's
the right variance of this type
parameter T has be cared to be non
variant but maybe that's not the best
choice well if we look at usages usages
what which of the two would be more
useful a type tree is also an untyped
ring or an untyped tree is a type tree
but what do you think
well I would claim it's this one here so
it's going from type to untyped just
means forget about the types just use
the basic structure re-type check it
again so I can always forget types but
going from an untyped ring just assuming
it has a types just doesn't seem to be
sound right so it should be the first on
the other hand what relationships do the
variants you'll hear imply well the
variants will say well it appears
covalently right so that's that's that's
a result so that must be a plus here
covariant but covariant would mean that
because untyped is nothing it would mean
that so a tree of nothing is a subtype
of a tree of type because nothing is a
subtype of time ah interesting
so it seems our variance checking has
has has hit a problem here so let's try
to resolve that so I first resolve it
just with a big hammer I put unchecked
variance here and say okay I'm done I
say well it's kind its contravariant
because that's the most more useful
relationship and I put uncheck variance
here if I put uncheck variance here of
course it means I'm unsound right so I
just say I'm smarter than the type
checker is so how am i smart what could
go wrong if I write this so what can go
wrong here is that given an untyped tree
I take its type and I think I will get
our nothing but in fact it gives me a
type that's that's the only thing that
can go wrong but if I think I can get a
nothing what happens there is no value
of type nothing that means I'm calling
for an exception to be caught and well
it doesn't give me the exception gives
me actually something nice which is a
type so that seems to be a quite benign
unsoundness in that case so I think we
can live with that and that's actually
interesting because it shows that having
an escape hatch for essentially any sort
of hard typing rule is actually very
important because they are surprising
situations like this where you say well
no type type system wouldn't have
thought of that and it's actually pretty
good to be I gives the compiler enough
to give the compiler writer enough
wiggle room to convince the the language
that the rules are still okay here okay
so let's go let's go on so types
carrying most of the essential
information of trees and symbols and
there are two kinds of types that the
value types that are the types of
expressions like and or into end or the
pair of boolean strings and then there
are types of definitions like that's a
signature of a method that's the low and
upper bound of an abstract type or these
information of a class and their
representatives the subtypes of the same
type type for just for convenience so
then the next thing is references
so let's here we have two abstract
syntax tree classes they're both extend
tree one is called select and the other
is called ident and they do essentially
the usual thing and identifier or a
selection so what should their types be
normally in Java C and the current
scalar c and basically all compilers I
know of these things would have a simple
attribute I would say well I point to a
given symbol and I pull the type out of
the symbol but we don't have a simple
attribute for good reasons so what
should its type be well the traditional
scheme in in a compiler is to say well
we have this transformation and then
there's this symbol table which is since
you stores the value of all symbols and
then we essentially put stuff in there
and pull things out but it's not very
functional and we've already seen the
thing is actually quite complex because
the types of these symbols they also
change all the time so the question then
is if we have let's say a selection like
object dot function what what's its
meaning the type will be part of the
meaning but what's its meaning and the
answer of course it well it depends on
the period it depends on when you ask
that question that's what we started
with so does that mean that object dot
from the tree object dot fun has
different types depending on the period
no we want to be functional trees should
be immutable so once we have a type the
type shouldn't change so what can
do them so what we do is we introduce
references which are essentially a
subclass of type and they're two kinds
of references two terms and two types
and they both share a common superclass
called name type and that type has as
its only attributes a prefix potentially
which can be a type or no prefix and the
name so a reference is just the name
basically and the thing on which you
select that's all there is and then here
you have these subclasses for term
recent I press okay and rest furthermore
references I immutable they exist
forever so I can say well the thing
called F in Class C and that's a valid
reference for all periods of time I
could have different meanings but the
reference is the same now you if you if
your compiler writer you will
immediately come up with this question
what about overloads so if you reference
a term like a method then it can be
shared by several overloaded the name
can be shared by several overloaded
method members of the class so how do we
determine which member is meant if you
have a symbol that's easy each of these
members will have a different symbol we
just point to the symbol but I just said
we don't want to have symbols we have
these references only in a nutshell that
question is why overloading is so
universally hated by compiler writers
that's really what makes things messy
the trick we use here is that we
essentially allows signature the
signature of things as part of the term
name essentially we encode the erase
type sort of sort of what the JVM does
is essentially with as a term name we
add the signature and that will
disambiguate overloaded methods have
overloaded names okay so then we say
okay we have these immutable references
what do they reference well you could
say well surely now it's a symbol right
a reference gives me a symbol I'll
actually know it's still wrong because
references actually capture more than a
symbol so the meaningful for reference
is more than a single symbol and
sometimes they don't refer to a single
symbol at all so it's neither subset nor
a superset
it's still disjoint from cymbals so
let's look at these things
references capture more than a symbol so
here we have a case of a Class C again
the method F and we have a prefix and we
say well create a new value of C event
so now we are siient now if the
reference prefix Doris but it will
resolve to this method F but we also
need to know that the type that we
resolved to is here a method into int
not T to t TP t to t wouldn't even be
legal because T is local to the class C
so we can't have a reference resolved to
that type so with the reference we
actually need to carry the particular
type of the reference which is not the
same as a type of the method that's like
Scala C again didn't do that this
modeling with references and it's the
most common rookie mistake of compiler
writers in Scala C to just say well we
have a reference to a symbol we just
take out pick the type of the symbol
this is not wrong it's not the the bad
thing is it works in simple cases as
soon as things get parameterised and
generic and complex it falls apart so
very easy mistake to make
so the right modeling then is to the
both of these pieces of information are
part of the meaning of prefixed RS the
other side of the coin is sometimes
reference is point to no symbol at all
here's an example using Union types
which I'm newly supported by the
compiler and we'll be in a future
version of SCADA so we would have two
classes and they both have this method F
and then we have a prefix and it's now a
or B we don't know which it is it's one
or the other and the two classes they
don't have a common superclass except
object and we return either one or the
other then the question is prefixed or F
what definition does it refer to and the
answer is we don't know it could be
either one so again the reference does
not is not tied to a symbol okay so what
does it mean them so well we just say
well we introduced a new thing so a
reference denotes something so the thing
it's
in notes is obviously at denotation so
that's how we named it
so a denotation then is either a multi
denotation it means it's overloaded we
don't know yet which ones or it's a
single D notation and then it might have
a symbol and it always has a type and
then there's a subclass of single D
notation called simply notation which
says well in some cases actually I do
have a single symbol and that's what the
symbol is so what then is a symbol so a
symbol represents a declaration in some
source file right so it's what I write
in some source file what the compiler
extracts from that and the symbol lives
as long as the source file is unchanged
so symbols actually have lifespans the
other things haven't so the other things
are immutable but a symbol has a
lifespan and simples have denotations
again and that depend on the period so
in particular the compiler faces ok so
then the question is well if we have
these day notations and the compiler
then goes forward through different
phases and edits how do we compute the
new day notations from the old ones so
if you have a reference pre door F and
we can just recompute the member at the
new phase but for a symbol how do we do
that so it simple has a meaning and then
we transform it and then it will have a
different meaning if all my symbols were
parts of trees it would be simple I
would just transform the tree and I
would get essentially the new tree and
that would give me the new did the new
information but that's not the case the
symbol can come from any jar and these
jars have a high level information and
then that information gets changed so
again I don't want to essentially do
that for the whole world
so what I need to do instead is
essentially when I use a symbol I want
to be lazy and say when I use a symbol I
want to be able to compute its meaning
at a given period from what it had when
it was loaded so what I need in each
phase then is essentially two methods
one is the tree transformer transforms
one tree to another its energy gives me
the transformed
and the other is the denotation
transformer which says I give it a
symbol and it says its type was that
before your face
what is its type afterwards so for
instance in the Ankara phase the trans
denote transformer would say well if a
symbol has this type then afterwards you
will have this type okay so now we're
getting to some bigger images so the
symbol here then what a simple is is it
has a bunch of D notations which depend
on the phase and these two notations are
computed from essentially that's the
first one on phase 1 2 3 with these
trans denote things and they're stored
so that if ever I come back to a
previous phase or I call want to know
the meaning several times they're cached
so they're cashed in essentially a ring
here and symbol essentially just has a
pointer to what its last denotation was
and that tells me essentially where we
were and then if we want to go to a D
notation in a new phase then I will
essentially apply these transformers as
needed or if it's already in the ring I
will just put it out pull it out here so
symbols are essentially functions from
period to D notation and here
essentially since I saw that they're
memorized functions I said there's an
efficient caching strategy to memorize
them okay so putting it all together
what we've seen since I argued that
compilers a databases I need to give
give you an entity relationship diagram
of course because that's what people
with databases do that actually it's
very instructive for the compiler as
well to look at that so we've seen the
trees there over here and I have the
single attribute called type position we
ignore that's that's trivial so a type
then the Verity of types but the
important one is there these name types
which are the references and the name
types
they now have a D notation but that the
notation is time varying so a name type
can point to different denotations
according to the phase of the compiler
and according to the edits of the user
so both of these things can affect that
thing so that's essentially the
denotation is indexed by the p where p
use this period so that's important that
this is time varying whereas all the
other things are constant they're
immutable so adi notation then has
always an info which is again a type and
it can be a single D notation or a multi
denotation if it's a multi denotation
and essentially each denotation gives me
a part of the one of the symbols in the
overloaded family of symbols if it's a
single day notation then it has
potentially a symbol might not have but
some of them do and assemble them points
again to ad notation which is the
current denotation of the symbol and
again that thing is time varying so that
thing has to be efficiently recomputed
so now you have it so what what I've
done here is essentially to say well
traditionally compilers are these big
machines that lots of moving parts
things change all the time and if you
say well no let's not do that let's
freeze let's be completely functional
let's have something like like
essentially an immutable data base time
varying data base what are the pieces
that really have to be indexed by time
and that's that one and that one all the
rest we can keep constant so lessons
learned we're not done yet you're still
learning I think databases for modeling
it's a good idea a lot of things we
learned for that database community FP
for transformations is also a good idea
and then the question is how to get the
efficiency there on the one hand you pay
a price that's functional on the other
hand there's lots of opportunities
because once you are purely functional
you have much more once you are
functional you have much more
opportunities for caching because you
know that essentially once you've
computed their value the next time you
compute the value it's the same time
it's the same value whereas otherwise if
you have lots of imperative moving parts
essentially caching is unfeasible
because you never know whether the next
time you ask that question it might
depend on something that has changed in
the meantime so that's why essentially
functional gives you a lot of
opportunities for getting better
efficiency through mostly caching so the
idea is think
for transformations use caching to get
the efficiency but take care not to
compromise the high-level semantics so
to find out more so there's a project
called dotty can go to a page and see
what what's going on currently yeah I
don't know how much time I have I've got
plenty of time okay I can go go to what
some of the supplement slides how to
make it fast so first I should say we
don't really have definitely speed
numbers yet so I don't know how fast it
is it's hopefully faster than the
current scholar C compiler I don't know
right now by how much because we
essentially we haven't taken out the the
the training wheels so there's a lot of
instrumentation that slows it down and
things like that so we'd have to take
that out and then measure but we haven't
done that yet so to make it fast
for symbols before it is seen that they
catch the last e notation so essentially
book if to say well what does this
symbol mean you just essentially go
through the cache and say well what is
it still valid then take it for name
types it's the same that cache the last
in the patience cache these caches they
stamped essentially with the validity
interval a validity interval is
essentially two IDs well it's a it's
always a single run ID and then
essentially two faces it's valid from
this face to that face and if it's if
it's outside the validity period then
you need to update but not otherwise the
other thing we do is that if you want to
find it to get the member so something
like this so we say ok prefix or s
that's of course a very common operation
that you say well take the member of a
thing of a certain because certain
prefix what's its type and then
essentially you have to always come back
with two pieces of information namely
essentially the symbol if it has one and
the type so the nice thing would be you
create a new object every time you ask a
member or a shin and that can be very
costly because member operations are
done all the time internally by the
compiler so what we actually do is we
have at least recently used cache of
these things
we say well be given a prefix we store
the last eight members or so that were
asked for and we essentially give you
back the type immediately so some of the
same techniques which probably jits
would also do and it also turns out here
I'm saying that because the I want to
impress on you let me just put put this
on on electricity
otherwise we'll be short okay so I'm
saying that to just impress on you that
caching is not something that can be
done completely automatically like
function memorization in functional
languages or just lazy values or things
like that no it's it's pretty much an
art to pick the right cache for the
right problem sometimes you have to do
things like least least recently used
caches which definitely you have to do
manually no language will give you that
automatically okay
so caches then our lazy Wells
memorization Zell ru caches and they
rely on essentially the purely
functional semantics and access to but
in a sense you can have this only if
your language or if your architecture is
predominantly purely functional but you
still need access to the low-level
imperative implementations to implement
your caches and that's sort of the nice
thing what I what I like most is sort of
to be in between those two levels so
also to have to do really very
sophisticated imperative code to get a
very clean high-level functional
semantics over there and I very much
value a language that lets lets me do
both of these things it's of course
important to keep the levels of
abstractions apart so you need to once
you do that you need to be very sure
that you implement the consistent high
level model because if it leaks out then
essentially all cows will ensue you will
have essentially something that is
unpredictable at any level there's a lot
of magic and there's a lot of hidden
things so leaky abstractions would be
terrible in this thing and of course it
would be nice to have better language
support to keep those abstractions apart
scholarly
scholars doesn't have them
so in Scala essentially we rely on
discipline programming discipline
conventions to keep them apart certain
maturity of the programmers in a sense
would be nice to do it to have something
more but as long as we are not very sure
what we want to add we we won't add
anything in that department another idea
that helps a lot is I've seen as shown
in these faces like 50 faces so it
actually turns out that a lot of the
performance problems of a compiler stem
from memory locality so if you have a
large program then your trees won't fit
in any cache so they will definitely be
in main memory and if your
transformation pattern is essentially
full tree traversal generate new tree
essentially you have cache misses all
along so that's that's very bad so one
way you can remove that or lower that
cost is by having larger faces faces do
more things but that means you give up
modularity so these faces now
essentially I grab backup features and
it's very hard than to argue what they
do so what we did is essentially
resolved that architectural e so we have
many faces but we essentially have an
automatic fusion face so essentially a
phase registered on what notes they they
they do any transformations and then you
have a reflection based fusion framework
that where you can put many faces into a
group and all these faces will then be
applied together in a single tree
traversal but you can essentially simply
move faces from one group to the next
with the same single edit from one on a
single list and it will just refuse
automatically so that was a big win in
performance because we have essentially
over all we have I think five or five
groups which brings us back to you since
your Java sees things five or six groups
something like that but we have many
many more faces in those groups okay
that's it that's all I had so if you
have more time then of course happy to
answer questions yeah
so I want to see this taken further
maybe you've already taken so one of the
reason is while you put data in a
database without me having to ask you as
the data priest to go do a query
operation for me and so in this model
you have all your transactions against
the database are being generated by the
compiler but then you have clients like
the IDE that are doing read-only queries
against the database and so we're we're
I hope you're going with this is that by
actually presenting a functional
database view of your program that
captures the whole history of the
program edits then the IDE can just
operate on that I don't know so for me
the sort of the the source of the data
so that the raw data is just essentially
the the diversions of the source code so
think get some somewhere to a version
control system that's that's where it
starts from though so essentially all
the compiler does is vo management so
you say well the truth is always in the
source but I want to get this entry a
different view on this thing like what
are the types which require complex
calculations and that's the job of the
compiler to essentially do view
management on a database which is
essentially a version control system so
I'm not sure I think it's probably very
similar to what you what you want it or
is it do you think the differences
database to the internals of the
compiler or is this taking it further
where there actually is a database
database in quotes that you know the
compiler is putting transactions right
so essentially the question is do we
persist those views in the database so
right now no so we compute them on the
fly we could I guess but I haven't
thought about that yet yes people do
execution that's sort of part of the
whole full process you're compiling
executing so probably would it make
sense for that execution to be a page
that will depend on your inputs whether
your inputs are also essentially
persistent and then you I guess you
could take it further yeah so right so
this is not yet so this needs to be
augmented to be a full build system
where essentially you need to keep track
of the references so it's not so what's
typically done is in Scala see is that
the SBT or not nowadays reaction I've
actually generalized that to other build
tools as well there it's a compiler
phase that extracts the references that
so to say well what while we compile
this module that those are the things we
touch so if any of these things changes
then you have to recompile but that's
not done that's not so what what this
thing gives you is to say well once you
track the references you can actually
keep essentially the whole compiler
memory and you can just recompile you
don't need to essentially break these
things from this Kenny more things like
that because we can sort of seamlessly
move things over to new runs and and
essentially keep a consistent ref
because all the references are
sister my reference just says well it's
this name in this prefix whatever it's
it's denotation is if it's imitation a
changes because somebody edited it or
maybe the transitive recompiles have
changed the the VD notation then we will
survive that so so the compiler
essentially has that in its basic
architecture to say well if it has a
nudey notation now there's nothing to
invalidate III will just take that
everything is just cached no the run
idea is essentially on an ID on every
keystroke I run the compiler again that
gives me a new run ID so every time I
run the compiler again on whatever small
subset of files I get a new run ID even
if the sauce hasn't changed yeah yeah
yeah you could say it's a transaction ID
yeah so we would need to actually
persist only essentially the public API
which is done the typical simple
persisting what we do actually now is
much more we persist the whole tree so
we persist the whole the whole AST
because we want to do essentially
aggressive in lining and fusion and
things like that we haven't we haven't
gone very far yet on this one so I think
some of the backend phases can be run in
parallel if you want to the tricky bit
of paralyzing is essentially similar to
exploring in a chess program because
essentially you have a lot of lazy
computations and you either have to
synchronize them to say well somebody's
already looking at that you shouldn't
recompute this
this again exactly it's it's all the
same the same thing the other the other
observation is these things are really
extremely memory bound so that means
parallelization doesn't necessarily buy
you a lot so it was one one thing I
looked a a while ago we looked at scalar
C performance with Pepperdine who's a
performance specialist and said oh wow
this is cool you're using all four cores
it paralyzes very nicely had to save it
well actually that's that's the GC and
the JIT yes each face can essentially
have preconditions that it has to run
after another face or after group of
faces so that they are you can't like
they're not arbitrarily we order but you
have a lot of freedom yes so there's
actually there's a scheme where you say
all these transformations are done
bottom-up so essentially for every
subtree you do you apply all the faces
in the group and then you go to the to
the parent node and you apply all the
faces again
and that gives you a certain restriction
on essentially where the information can
flow so if you need like context
dependent information from earlier
phases and you need to run in in a
different crew
yes yeah
so a type is what the compiler knows
about about an entity such as a tree so
a denotation is but a type the
difference is really in terms of life
spans a type is is immutable so it lives
forever so you say that tree which is
immutable has this type and that that
answer must be immutable so valid
forever
whereas the denotation is something that
can change according to the current
state of your database according to the
context so if we look at we just don't
go at this one again if you look at the
where do we have to
oops Wow okay so now we getting there so
so that's a type so the type just says
well if I have a selection let's say
then I have a prefix which is a type and
I have a name that's all I have so the
type tells me that and that's the only
thing that's immutable that's the only
thing that will would be valid for all
periods where then if I asked well what
does it mean what does this reference
mean then that then depends on Senshi on
the current state of your database so
that's time varying and that's the
difference so what I what the compiler
knows is where the source files have
changed or not and it will recompute
only if the source file has so
essentially it knows that by because the
interface of the compiler is well we
compile the source file so it doesn't
actually look at the hash of the source
file if somebody tells the compiler
recompile it they will assume that the
salsa has changed because otherwise
there would be no good reason for it so
if the source file wasn't touched then
the compiler will not recompute the type
of the symbols I says well nobody told
me to recompute that file so the file
must be the same so the denotation of
the symbol is still the same so that's
how that works
so for each denotation essentially also
for nested symbols you essentially you
crawl up the owner china until you hit
the top-level symbol in thesis well is
that still the same one as it was before
if yes then I can just roll all the
annotations forward
took a new computation run if the
compiler is is essentially the only
thing that generates its these views we
don't persist them if it is sufficiently
fast compiler now I mean you keep these
things in memory and when you turn off
the plug then you will compute them
again I'm not sure whether we should I
think that's a separate that's an
orthogonal question to it Andre you had
a question no well if it's if it's a
type that if it's the type int int then
yes it will essentially the denotation
will be the class int if it's a value of
like two then the denotation well no if
it's if it's an we have a name here a
let's say n of type int then the
denotation will be if it's a single
symbol i will say well that's the symbol
and its type is int will be the symbol
and the type so so here it it's here we
get both the symbol because there is a
single one but we also get a type namely
it's of type int to int which is not
apparent from the symbol and the symbol
says it's type t to t so so we get the
symbol and the type at the current
corresponding to the current prefix
not as a separate object but of course
you have you have that type in aeterna
tation and you can follow the symbol and
you see you will see that type here so
you have both types but we don't keep
the substitution as a separate object
yeah oh yeah sometimes we need to it's
not super common but for instance the
the back end has to compute Java see
signatures so essentially the java c
generic information and that's only the
the Java back-end needs to do that the
JavaScript back-end doesn't care about
these things so what we do then is where
it fades back end and the signatures get
essentially they get lost during sides
airasia so we actually go back and to
say well before Eurasia started what was
its type and compute the signature for
that type of course we could say I mean
you can always refactor things and say
well just keep these signatures as
essentially compute them before Eurasia
and then sort of string them along but
that would essentially give us more
heterogeneous data types to maintain so
we don't do that so we essentially go
back and and look at it at an earlier
phase to compute them well that's so
some of the caches are indexed by
essentially validity period and then if
you're outside that period you
invalidate others are like the caches
that determine whether the the symbol is
still valid essentially you have to go
back and says well has anybody asked ask
you to recompile the source file or
things like that but of course cache
invalidation is one of the hardest
problem in this whole thing yeah yeah so
that there you have to be very careful
yeah we have lots of switches because we
to say turn the caching off and if we
suspect the bug we turn all the caches
off but actually lightly we have we
didn't
find that bugs were resolved that ways
of the caches it wasn't the caches fault
that the things bad things happen
so it's either inherently bounded like
if you take the denotations of a symbol
during all the phases well you can't
have more than I don't know how many
phases change the type maybe ten so you
can't have more than more than ten
versions and then we don't maintain it
or when we say well for instance here
what's the denotation of prefix dot F we
actually have a bounded size LRU of
theis eyesight so to say well we get for
a given class prefix we get essentially
eight-member denotations the 8/8 last
times we asked a member on that class we
get we get that that thing so there it's
inherently bounded because otherwise it
would call very large any other
questions is not mutable time varying
that intonation value is also immutable
that a type can have many denote X so
the link form a type two the notation is
mutable in a sense of course did the so
so so the type of I just says it's the I
in this class it's a member I of this
class and then you say well what's it
well what's the denotation then you say
well here's a symbol and here's the
symbols type or seen from where but what
my prefixes so there are two different
types as a type which
says it's I and there's the other type
which says well actually the eye isn't
and that second link can change over
time exactly so it's you think a type is
just what you write it's nothing there's
no hidden meaning behind it yeah okay
cool
good thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>