<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JVMLS 2015 - State of java.lang.invoke | Coder Coacher - Coaching Coders</title><meta content="JVMLS 2015 - State of java.lang.invoke - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JVMLS 2015 - State of java.lang.invoke</b></h2><h5 class="post__date">2015-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lhGPndh8R3g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Vladimir Ivanov i am i'm a
software engineer working at oracle on
GV m compilers sent for the past two
years I have been contributing to
travelling invoke implementation in
OpenJDK so I would like to share with
you what happened during that period and
what are the plans for the future what
problems are still there and where do I
think we think we should move the
implementation so I'll start with a
brief introduction how does it look like
how did it look like before how does it
look like right now describe what what
have been done during 8th and the 8
Update releases for the past year and a
half and then talk about the future
directions so you should know this slide
so let's move on so G Saito 92 which
introduced method handles and Java long
invoke was integrated into Java 7 but it
started back in 2005 from design
perspective there were a couple of major
overhauls and the outcome was that in
Java 7 we got support for dynamic
languages so the six years let us to get
right the design but there are some weak
points in implementation so the design
allowed the two heavily optimized method
handles but it didn't work that well in
all the cases so there was problems
there were problems with inlining
generic invoke was slow Charlie
complained a lot about no class they
found and so on
and the implementation design wasn't
that satisfying so it turned out that
the problem is the wrong I are for the
vm so vm optimized method handles looked
at method handle chains but after some
thought it turned out that it's it's a
wrong guy are wrong abstraction so in 7
update 40 in 2012 john kreese and
michelle they came up with in you
implementation java.lang invoke
implementation based on so called lambda
forms so they introduced a clean
separation between GBM and java jdk code
so it healed many problems which were in
original implementation so no more no
class they found exceptions and it plays
well with the GBM machinery then the
forms were translated into byte code and
the GBM knows how to work with bytecodes
it work with it from the very beginning
so what are lambda phones so how many
people use method handles great so you
know that so message channels does it
typed directly executable reference to
either behavior or field method and
lambda foam is a symbolic representation
of the semantic invocation semantics of
a method so that's the separation from
the implementation perspective lambda
phone based implementation allowed to
clearly separate different levels so if
you are user you work with method
handles method types you link in the
invoke dynamic instructions with call
sides and then is that there is a whole
new world of lambda foam based stuff and
it interacts
with the GBM but through a very limited
set of vm primitives there are linkers
like method handle linked to interface
in linked to visual link to static
they're a bunch of up calls and down
calls into the vm but the the surface is
very limited and from the vm it looks
very clean comparing to the previous
implementation so what what's what is
lambda form how many people saw lambda
forms before Wow as many as used method
he knows that's surprising so let's look
at the example so here is a lambda form
for a method handle which does a field
update so it's a four statements so like
feel the compute field of sad check
check cast a base check caster our value
so a 0 is a method handle a one is a
receiver the object where the field will
be stored and the a2 is a value which
should be stored so in order to do it in
a type safe manner because we here we
you see unsafe put object we we know
what we are doing so if we want to use a
just filled upset with unsafe we need to
ensure the the value compliance with the
field value so we do it shaky chikis and
before we do an actual right so it was a
textual representation from the
structural perfect structural
perspective it's it's a exploded
representation so you can iterate over
it and the pick and poke different
information from this and notable thing
is that all the types I raised two basic
types so you don't see here
java.lang.string javelin whatever
you see object long int that's it and
all so it's basic type in a GVM
specification sense so all small
primitives like bytes bullen's shorts
chairs I raised two integers so it ends
up with five primitives for primitives
and end the reference type plus void to
represent return type so if in in order
to inspect the lambda form there is a
fine grained representation for that
information and here I presented a
textual representation so all these
functions are available as names so both
values and parameters are referenced
through a names array so here at seven
in total seven values so as we saw
lambda form is non-executive all
representation or for the invocation
semantics but it's not that helpful if
we can execute it so how do we get the
actual behavior so trivial interpreting
the structure we can iterate over the
values and computing invoking dead
functions and getting the result done on
the actual implementation there are two
are ways to execute a lambda phone so
there is a special field lambda phone
which points to a member name which
points to some method in the program
it's either a entry point to lambda from
interpreter which generically implement
that were iteration over the structure
of lambda foam or for efficiency
purposes a lambda form is compiled to
bytecode translate
to buy it with and then gvm directly
execute it so lambda form interpreter
isn't supposed to be fast it's really
implemented in just a page of code it
looks clean so it's not as fast as it
should be but it looks nice I would say
and the bytecode translation is trivial
we can iterate over the structure and
translate Avery statement into a bunch
of byte codes so it's a trivial trivial
teamplay translation we see a function
we generate bytecode to invoke it an
interesting thing is that that in order
to get so here we have an unsafe I think
in this audience are more people used
unsafe than method handles so you know
that all the methods are instance
methods so you have to do to get that
you should cover instance unsafe
instance in order to invoke anything so
and here we have a static structure so
we've injected unsafe instance so how do
we get that on byte code level lambda
forms are compiled into bytecode and
load it as VM anonymous classes its vm
anonymous classes not java language
anonymous classes it's a specific vm
instance so it allows to instrument
instrumental constant pool and inject
actual objects there so when we load a
class we put unsafe instance there and
we can use it from the bytecode so we
generate a fake placeholder string load
constant
but after the patching it holds the
unsafe instance so we can safely cast it
to some miss can save and then use it
since the as you see from the name is vm
anonymous classes this classes doesn't
don't have names so you can't look up
this class you can only a pass the
reference to the class so the the
consequence is that you can't reference
it reference it's that class even from
the bytecode of that class so this is
it's impossible to access field of the
object of that class so we don't have
any other choice because i was asked
some time ago come on can we just add
static fields to that class and use them
why do we need to inject anything into
bytecode it's looks hockey so the
problem is that since there is no name
name in vm manner not in language it
limited the machinery we can use but it
looks really powerful because we don't
need to care about a constant pool
resolution because we have already pre
resolved stuff at our hands and we can
just put it to resolve it during loading
it's a powerful machinery so let's move
on so let's look at method handle and
lambda forms again and since method
handles are reference lambda forms there
are two cases we can have shared lambda
forms shared by means of method handles
share the same lambda foam or lambda
forms can be customized to some
particular method handle so let's look
at some examples so for example again
field setter so
there are two calls here we're
constructing a two different field
setters so in first case it's in
completely unrelated classes fields are
different filter type is few types are
different but we construct as we can
reuse the same lambda form to actually
implement the behavior of that field
because what matters here is that we use
proper field for proper class classes
for for type checks and we use proper
offsets otherwise lambda form can be
used and this information resides in
method handle which which is passed as a
fair as a zero the first argument and
the vm just extract the lambda form
justic stress in the all necessary
information from the method handle so we
can reuse the lambda form and so does
sharing lambda forms among numerous
method handles on the other hand it's
limited since we specialize four basic
types so here if we have int and long
field we lambda from structure differs
so we use poutine and put long so we
can't share the the lambda phone so the
be hey actual behavior is a similar so
since Jason risa method candles were
introduced in 7g lambda forms going to
integrate it into seven-year 40 and in
eight we got a number of internal
customers let's call it that way so
project lambda used start to use invoke
dynamic and a GD k got in you javascript
engine called NASCAR Marcus an idea XD
the great work
moving that forward so we get a lot of
we got a lot of feedback on quality of
implementation and the number of
complaints so the most notable complain
was the footprint lambda phones were
heavy and we generated lots of them
start up warm up from peak performance
perspective it was good so we reach the
pretty well level of performance but
from the start up and warm up it was too
heavy weight and the dynamic footprint
wasn't great as well so for a 240 we
started an effort called the lambda foam
sharing we the goal was to considerably
reduce memory footprint while preserving
the peak performance at the current
level so what did we oh god so on
average I used octane benchmarks to
gather performance statistics so on
average shown relatively large benchmark
like box2d there I'll around the
millions of method handles and before a
2-14 pre a 240 GDK there are millions of
lambda forms constructed they aren't
translated into byte codes right away so
there are there were about tens of
thousands of vm anonymous classes loaded
for that after lambda from cashing
effort or we got thousands three order
less lambda forms and the classes of the
same level like every cash the lambda
foam most probably will be compiled
because they are heavily shared so let's
see what does does it look like to share
a lambda phone so consider God with
tests so it's a even else button method
handle
so you pass three method handles and you
get a new color and you combine them I
get a new method handle which
effectively implement if the nails
branch on a land reform level it looked
in a 243 a 240 binaries as follows so
all these three methods handles were
injected into the structure it means you
can share God with test it's it's
customized for that particular method
handle and it did all these method
handles are represented as byte code as
well they injected into the bytecode so
the bike it isn't shareable as well how
can we change that why don't remove all
these three methods handles into a
corresponding method handle for grad
with tests and just load them from there
and use so this lambda form shape can be
shared problem solved right so what did
we got with lambda from sharing
considerably smaller dynamic footprint
both in number of loaded classes and the
meta space size faster startup warm up
because as you can see from the lambda
form structure it's not the cheap to
construct them the color quite complex
entities so then if you use it long
enough you get it compile to bytecode
you'll owe the class and that's it then
that class will be some at some point
compiled into native code so reducing
the lamb of lambda forms in
significantly improved startup and the
warm-up and moreover it allows us to
further optimize enable other
optimizations like lambda phone pre
compilation so let's look at some
numbers so here are dynamic footprint so
the e Greek axis is
ha x so for example on box2d the meta
space consumption was 4x smaller than
before so it's a 240 before lambda form
changes with Grant in and they're almost
the same on sorry for the color it's the
gray one doesn't look good here so end
on the class as well it's for X less
classes loaded for box2d so significant
reduction in dynamic footprint I'll talk
about that later wait wait 45 minutes
please I'll I have a dedicated section
for that so from startup and warm up
perspective it looks good as well most
of the benchmarks benefit for diamond
reg ex / 3x3 in dahab x faster when it's
a duration of the first iteration of
octane sub benchmark sprays place
suffers quite significantly but that's
mostly because the new bytecode shapes
are heavier there are more operations
performed and it doesn't get much from
lambda form sharing not many lambda
forms are shared in that particular
benchmarks so there are some regressions
but otherwise but most of the benchmarks
improve a lot so regarding your
optimization around the phone brick
compilation so before lambda from
cashing lambda form started in the form
execution started as interpretation in
the lambda form interpretive interpreter
and after 13 vacations they are compiled
into bytecode so it allowed the two
considerably reduce the number of Lodis
classes because as you saw we we had
millions of lambda forms but we had the
only tens of thousands of compiled
classes of loaded classes for
forms after we expect our to
aggressively share lambda forms we don't
need to bother about that anymore we had
we have thousands of lambda forms so we
can simply compile them right away and
the completely bypassed lambda form
interpreter so what did it solve Charlie
do you know does it look familiar to you
Charlie complained a lot about steak
consumption and here is a these tech
depths for a trivial call just adjust
the chain of two depths of two it's only
two materials in the chain but for every
method handle in that chain we got seven
stack frames when it's interpreted wow
it the four yeah for for long enough for
deep enough metal handle chains it's
usually blows up stack pretty quickly
with their pre-compiled when the lambda
form is compiled it got just a single
stack frame for every method handle so
we get down to four stack frames
significant reduction in the in the
state consumption and here is a big
performance so so some benchmarks
improve surprisingly but it's not due to
lambda formal caching but but but due to
related work on GBM I had to do to
change compilation policy to play better
with lambda forms so it allowed to
improve some of the benchmarks but there
were some regressions so the goal of the
jab in a 240 did wasn't completely mad
we regressed on some of the benchmarks
so what were the problems profile
pollution and the non constant method
handle calls
they work the slower than it it they
were before so let's focus on dead
topics so let's look at customized
version and God with tests so we had all
three message handles injected and on
bytecode level we got the following okay
we have always we have guard with test
with the unbalanced if then else branch
it always the condition is always true
so when we look at profiling data GBM
sees that okay I never execute the false
branch great I can completely eliminate
that branch from in the generated code
Wow let's do it if it's always false we
clearly see that in the branch
frequencies 100% the false branch was
there was taken but in a shared version
if the same lambda foam and hence byte
code is shared between unbalanced branch
unbalanced guard with tests Combinator's
GBM doesn't know anything about the
bride shriek depending on the actual
execution it can arrive from 0 to 100
whatever so that's the profile pollution
problem sharing lambda from sharing and
hence byte code sharing calls it a less
accurate profiling information gathered
so I came up with an idea how to fig
that why don't we simply custom I do a
customized profiling on a pair method
handle base right why do the question is
how to do that we want to collect that
information only during warm-up but
completely bypass it during the
optimized where when we
warmed up and the we execute optimized
code we wanted to work in the same way
as it works for GBM you're on a GBM
level so some details how I did that so
I instrumented to lambda forms and
introduced a compiler a profiling logic
there so we can simply gather
information of what values the deep
condition the test returns and collect
that information in array of counters
how many times we invoke at a true or
false branch so in order to get a zero
overhead an optimized version I needed
some help from c2 so during profiling a
during warmup it's either we executed in
interpreter there is a special logic
which does the collection but for
example in THC tab c2 can reduce its
okay its not need not necessarily
cheering but for example when we warm up
a new method scandal it's not compiled
yet and we have a dedicated native
method / method handle / / lambda form
so in that case we need to do profiling
as well and only get rid of it when we
in line the whole message handle chain
so when it's warming up it does
profiling but when she two kicks in and
compiles the whole matter handle chain
it's optimized into almost nope because
the counts of a collected counts they
injected into ir of the compiled method
that's its no overhead on profiling
after that point I achieved that by
introducing an intrinsic for that
profile boolean thing
a branch profiling is just one case of
profiling there are other profiling
going on in GBM so what about thai
profiling my performance experiments
turned out that type profiling isn't a
problem after Roland did a type
speculation in situ and it completely at
least at all type profiling doesn't
affect the performance peak performance
in any way so I got the same numbers or
even better numbers with lambda from
sharing and there is another case the
optimization counts GBM collects
information of how many jobs happened at
some particular BCI's and sub I see
places in byte code and it caused some
problems like the following bug when
adapting jobs in God with test for
unbalanced branches so when we prune a
branch and we at some point kick got to
eat we need to do and vm records its
information and at some point there is a
logic when we do that too frequently too
many times we just bail out and don't
compile that method anymore so
performance be performing just dropped
in the floor so it's like very slow
became very slow yeah we generally know
what we're doing yeah yeah yeah yeah
yeah yeah but that's different layer bug
it's I it was fixed but there is still
no generic solution for the optimization
profile pollution it's still possible so
if you see any problem with that just
file a bug the other case is non
constant message handling location
invoke exact invoke generic
so let's look at the following example
so here we construct a local guard with
test and then invoke it in an infinite
loop of course jeet can't inline it so
we here we see method colonel invoke
basic receiver not constant okay we
tried our best but we can't do anything
better how does the the native method
which is compiled for that particular
method handle looked like look looks
like so here we see for customized
version we compiled pretty good native
method we in line all three methods test
true and false branch good pretty good
we pay extra only for initial invocation
but but then we are fine but in a shared
version it doesn't work as expected so
all since we compile a native method for
shared lambda form we don't know what
method handle isn't it is invoked on so
we can't extract anything from the
method handle and we had we have test
true and fallback method handles there
that's the problem it significantly
affect each performance so the problem
is that in a customized case we compile
a single native method for the whole
metal handle chain but in case of shared
lambda forms its place into numerous
small native methods which in the
overhead of invoking it them is quite
high so I observed over head up to
twenty percent each performance
regression on some octane sub benchmarks
due to that problem so how it can be
fixed so the interesting observation is
that we don't need to customize the
whole matter handle chain we can just
customize the root method handle chain
and that's enough for a treat to know
everything about the method handle chain
so if we customized only the root method
handle but leave all the atom lambda
forms intact shared we get what we want
and there are only three ways to invoke
a message handle invoke dynamic where we
already know everything about it right
because the invoke dynamic is linked to
a call site instance and gvm
aggressively optimistically optimized
through that so it reads the value from
the coal side and in lines through for
invoke invoke generic and invoke exact
that's the case is where we want to
customize so infomatics handle is
invoked through that reflection like API
we just customize that message channel
if it it is invoked enough time the
customization is trivial we can just
overwrite the since the lambda form is
customized it means that the it is used
only with the some particular method
handle so we can just override the
method channel which is passed and
inject the correct method handle right
into the bytecode so GBM knows what is
what what to use so lambda from sharing
went into a 240 and the performance
fixes i talked about will be released as
part of a 260 which should be quite soon
at least on open the decay pages the
release date is somewhere in august so
when a 260 released if you are
interested in in method handle
performance give it a try or use GD k 9
it already contains all the fixes i
described both its say you can get you
can take early access it went like in
be 50 I integrated them long ago so so
let's switch to the future work what can
be improved further so from the
implementation perspective I I included
five items let's go through them so in a
240 there was a extensive effort to do
lambda from sharing but it's limited the
two basic types so we arrays types two
basic types and then use lambda phones
so we don't do any boxing or varargs
collection of the arguments so all the
basic types are left intact on the
bytecode levels and lambda form is
compiled it's left as is there the main
reason is that to avoid any performance
regressions so we want ms candles to be
optimized about to be a highly highly
optimized bowl so we want method handle
change to go away in compile code and
just so just ends up like argument
shuffling and actual invocation so here
how it looks like on the in the current
limitation so we have a cow side with
the basic types and we don't change that
basic types we pass them as these
through the method handle change down to
the actual invocation if the argument
started as int it it it it stays int for
the whole message handle chain how it
could be improved why don't both simply
box them right and the further erased it
erase them to object and do boxing and
unboxing at the the different ends of
method Kendall chains box at the call
side unbox before actual invocation of
direct method it will allow us to
specialized pro artists so like have a
lambda form for one argument two three
four that's it much smaller number of
lambda phones needed in that case and
why don't we go further and just come
just collect all the arguments into an
array and pass it disease wow just a
single lambda form / behavior right it
should further significantly reduce
amount of lambda forms needed and I
think it should allow to persist lambda
forms and from ahead of time compilation
it's much more feasible than generating
some per basic type specializations on
the fly right so so the trick here is
that we actually don't rely on profiling
data here we heavily rely on the fact
that method handle chains are fully
lined that's the key point to get
reasonable performance for that so we
don't rely on profiling we have all the
necessary information in our hands so
you do boxing and unboxing as part of
the same compilation
yeah so the key point here is that JIT
compilers should be should be able to
reliably and box I eliminate that boxing
and unboxing and look through a race so
here we load the boxed values into our
way here we unload arrays right now
aren't I immutable so there is no way to
say it's a Ravus final element so
compilers are compilers are conservative
with optimizing our race we introduce it
stable annotation how many people heard
about stable okay so a stable allows to
constant fold loads from a race but it
works on their own field declaration so
if your load array reference a a rare
reference from a stable field jit
optimizes it but here we construct some
something new local instance and there
is no way right now to annotate value
like a stable value so more GBM work is
needed to make that happen plus from
this startup and the warm-up perspective
during the interpretation these the such
sharing is quite quite heavy weight so
we need to to to to to measure how much
does it say yes done yes okay
the motivation for frozen arrays is to
allow array array array sure array array
explosion to be more reliable even if
the array escapes along some pads
because then you can prove nothing bad
happens on the past payload of the array
also I want to say for the record that
our colleague Frederick horse from
prototyped a rainy explosion and box
explosion in j rocket several years ago
and it was the basis of his
implementation 292 so we we have good
hopes that it will work in our world
never the main problem is that it's
really hard to splice in sub I arson to
see you'll notice I are we try to get
the same ok look a lot of stuff they're
totally was we have reliable boxing
boxing nation was very straightforward
to add hope they would explode through
arrays if there were new allocations and
you just didn't constant Iranian cease
to fill and devote out that just over a
far as well but I still think you're
going to have to yeah let's let's talk
about that offline yep so the next item
is method handle s-type so right now so
s type is a the adaptation which is used
to implement generic invocation so
exactly like vacation invoke exact
method handle more exact requires that
types of coal site type and method
handle types are are equal equal in the
sense that it's the same time exactly
the same but sometimes we want to adapt
it slightly like doing a cast like if
you have a object on coal side but
method handle has a java if you have a
object but you pass a string
method phenotypes differ in doesn't
differ too much don't differ too much
s-type turn transformations allowed to
invoke method handles using in exact way
when types doesn't match exactly so
constructing such adaptations are
expensive and we want to cash the result
right now there is a single element cash
on method Kendall so when you do invoke
generic on a method handle by imogen
eric generic invoke I mean method handle
desh meta Kendall dot invoke that's
called method generic invoke so if
method types don't match it tries to
adapt the method handle using s-type
transformation and the implementation
cashes the result the problem is that if
method handle is shared and you try to
adapt from different call sides with
different types you simply pollute the
cash you must you most of the time you
will recompute the adaptation so we need
to fix that and most notably to hear you
feedback do you see this as a problem
how important in that is it that how how
frequently do you do generic invoke when
you share method handles that's the data
we would like to hear from the users
from you also it's interesting to
further reduce the consumption right now
it's almost so after lambda from
recompilation the amount of stack
consumed is proportional to the
depth of the message Kendall chain since
we have a we have a stack stack frame
consumed per message handle but for a
loan for deep enough method Kendall
chains it still can consume quite a lot
so one of the solutions could be to a
customer I to do to compile a whole
method handle chain and generate a
single method for the whole method
handle chain some kind of in lining but
on a lambda form level I did some
experiments on that front and yeah it
was easy to implement no additional work
from all on vm level but the problem is
that it almost completely defeats the
lambda from sharing benefits so the
amount of space needed is almost as much
as we head there with before lambda from
sharing because we have too many usually
too many indie cool sites in organ amico
sighs and we need to split specialized
for them and also vm anonymous classes
are still classes I'll talk about that a
little bit later but don't we need a
more lightweight way to load code
sometimes we don't need the don't need
it yeah go go get implemented right so
the other direction could be tail calls
most of the lambda forms they are tail
calls they have tail calls so for
example in that particular case if we
could tail call it would end up with us
see
with no stack overhead no additional
stack frame for the method handle chain
since the old lambda forms are Taylor
they did they day they have a tail call
and then so next lambda form they reuse
for example as an exception God with
test isn't tail recurs doesn't have a
tail call for test method handle so in
that particular case if you have a deep
guard with test method Kendall chain you
will have to preserve the stakes range
for the tests but other than that in
most of the cases tail calls would help
a lot here there was a prototype in ml
vm the problem was the security x proper
access checking right but i think we
have a solution to that ok so there is a
we have an understanding how it could
look like in the actual implementation
so it's possible to do lightweight code
loading so right now we have vm
anonymous classes it relaxes some
limitations of classes like when so the
idea is to remove the name so you can't
look of the class it allows to
disentangle the class loader and the
clay loaded classes so loaded classes
can leave on their own so if you don't
use a class you can
that vm can just collected for the
ordinary classes the class lied lifetime
is tightly coupled to the classloader
lifetime which loaded it removing the
name aloud the to relax that but as i as
i said it's still a class do we only
need a class know if we wanted to low
the code we don't instantiate that class
for example but it's possible for
ordinary class to be instantiated if
there is a constructor or we use some
unsafe mechanisms to instantiate a class
don't really want something like that
construct a method handle which points
to some bytecode we provided something
like a method with a constant pool
attached it constant pool something like
that does it sound appealing mm-hmm yeah
yeah also other rainy or here is john
john from DC team so how does how do DC
engineers feel about vm anonymous
classes
yeah thanks so classes a vm anonymous
classes were a problem for GC guys
because with the permian elimination
there was a meta space introduce it to
stir classes and the idea was based on
the tight lifetime relation between
class loaders and classes and for it
doesn't work for vm anonymous classes so
for GC there wasn't special treating or
vm anonymous classes introduced which
complicated of course that all expands
into 64-bit pointers once
yeah so the actual benefits the actual
benefits should be measured for sure but
dead it's most likely we can further
relax the limitations so we don't need
the class most of the time we just want
a code to work with yes always have
access to yes yes yes yeah yeah yeah
yeah and another aspect of the changes
to prep produce a public API for such
functionality yeah and the regarding
profile pollution in addition to the
optimization counts profiling pollution
I talked about so dissolution richer was
implemented for God with test isn't
ideal Soto we are fixed pollution on
lambda foam level if you share a method
handle you can end up with the profile
pollution in that case so the profile is
collected on per message handle basis so
for every method handle there is a
special profile collected but if you
share method handle you don't get a
specialized profiling for every use site
probably if you do method handle are
sharing you should be aware of that not
sure it's important to be fixed but
there is another problem profile
pollution causes problems for other
parts of the system for example the most
notoriously known is streams API they
suffer a lot from profile pollution John
calls it
customization problem so you can google
up the not a good right abs by Joan
about that problem so we have to find a
way to fix that so we'd like to
encourage you to think about that
because it's a problem which which is
present in many different places it
manifests in different scenarios so if
you see that in your code please think
about how it can be fixed both on vm
level and another level as well yes yeah
it could so for four streams it won't
help I think because you don't get
inlining what Roland did it allows to
flow the profiling information from top
to down if you have a polluted profile
underneath when the code is heavily
shared you the edge it can overload it
with the the most specific profiling on
top yes cliff
here at t1 t2 I but all possible profile
minus t1 and I greatly Courage's in
mining too expensive performance and
then t2 i make it sexy one profiling
decision test potholes and so c2 is
always a superset of C one but it would
have liked specific profile on kodak so
very specifically i was looking to clone
the profiling sites to pick up like
specific role models yeah anytime you
have time like that I would tell c12
damn well in line all these things and
then you got site-specific profiles and
then see to it okay yeah yeah that's a
viable idea I think Igor various of did
a prototype of color color color
specific profiling like in 2012 or
something like that probably yeah
probably that's a way to fix part of the
problem a moment maybe most of the
problem different execution context from
the original request because then the
handle and use all of the customizable
lighting profile information during its
a lighting in other words complicated
reactive or work stealing frameworks you
might have your original custom request
very far separated from the intern
great let's do let's discuss it offline
thanks cliff so and regarding further
extensions so Paul Doug Leier opposed
and the ball has been working hard for
the past couple of years on the Vark
handles as they are designed as a safe
performant alternative to unsafe part of
unsafe methods so initially they were
did they relied on the method java.lang
invoke framework so they were different
flavor of method handles war handles not
sure it will end up that way but they
initially they look like a natural
extension of the API to provide a
fine-grained access on a memory model
specific aspects of field reads and
writes regarding a pixs enhancements
Charlie Nutter initiated a discussion
while ago like half a year regarding
what would be a good candidate for
inclusion in java.lang invoke and it
seems we are pretty sure they're a set
of point enhancements in API which will
help a lot to the users so for example
let's look at tri finally so right now
there is a way to work around the
problem you can implement a tri finally
with try catch and just append the
finally a logic on both embossed
exceptional cases in normal case the
problem is it's for the user it's too
much bola preta played code and the
Charlie all wrote a special library
to ease the pain of working with massive
candles and from the
implementation-specific we can't
generate it or it's almost impossible to
generate an effect efficient byte code
for that tri finally shape so on
implementation it blows into a set of
lambda forms method bound method handles
and it's really hard to extract the
initial intent if we have a tri finally
Combinator we could we could compile it
down to the efficient bytecode with
try-catch final that's it so also on a
candidate list method kinda loop
Combinator's and a couple of blue cups
so if you want more details feel free to
contribute to the original mailing
thread so we are working on so Michelle
hoped plans to make some progress on
that front so stay tuned with that on
that so we hope I I don't promise
anything right now but probably we can
do something in a near future for that a
native calls are also natural extension
John did a really good writeup a while
ago about how native calls I can look
like on as part of java.lang invoke
framework and yesterday I did a demo of
the prototype we have been working as
part of a project Panama ffi effort it's
from my experience it feeds it fits
really well the framework so for Java we
have all the separation ready and to get
a native calls i just added a bunch of
new entities like native method handle
native entry point on vm level it's just
a new linker linked to native and debt
seed so if you are interested in Courage
you to download
the Panama workspace and play with it
there is a test case with a bunch of
posix calls implemented so if you're
interested in better ffi for java feel
free to join to the effort try it send
feedback we will be grateful of that so
that's it thank you very much you could
but but yeah vm does a lot of things to
get it right so we have at least right
now the the inlining decisions design in
lining in realistic in the compilers
doesn't work well with messican with the
deep deep invocation chains and we tweak
did them 4matic handle chains so if you
just implement that yourself you won't
get as much performance as you could so
it should be fixed on let's discuss it
offline so I ran out of to almost run
out of time so I don't want to really I
i I'm almost over my hour ok if ok if we
have time let's let me know when we I
yeah yeah so regarding it then regarding
the question so the question was why
don't we simply generate if you're a
user why don't you simply generate
bytecode yourself the answer is
it will it most in most of the cases it
won't me won't be optimized as much as
if you use method candles for that
because we had to tweak the compilation
pipeline to work well with method
candles for example we have an exception
for final field final fuels optimization
so right now final fields aren't
optimized lowes from final fields aren't
optimized in normal code but there is a
special express exception for Geralyn
invoke classes just one case there are
they stable annotation yeah inlining
heuristics also if you have too deep
invocation chain it will be pruned at
like nine
any other questions Charlie your kid run
ahead I had one so a lot of them a lot
of the profile pollution that you talk
about goes away when we find things to
indeed call site we we have a lot of
shared method handles throughout JRuby
we do some invoke exacts but generally
if you're running for peak performance
we're finding everything into any call
sites at some point so it seems like one
of the big things that would be helpful
the Java side is a way to do and invoke
dynamic call site to a method handle so
that we can have the same routing of the
handles that you get from generated by
code okay Brian any take on that so we
can asked for a way to to inject invoke
dynamic in Java code right did I get it
right here are wondering why it's taken
us so long to do this there's some
completely non technical of a tricky
issues
we think we have a way to a story that
you can generate you can express invoke
dynamic in Java and Java source those in
the context of avoiding the profile
pollution because we can actually hanker
it to a false ID rather than having to
do the generic education trace all over
yes I would like to do that any other
questions great thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>