<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Immutable Collections | Coder Coacher - Coaching Coders</title><meta content="Immutable Collections - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Immutable Collections</b></h2><h5 class="post__date">2016-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pUXeNAeyY34" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi guys I'm I'm Michael and today I'm
going to talk about immutable
collections first I want to give you
like some background about myself for
people who don't know me I'm a PhD
candidate at CWI which is the Dutch
national research centre for computer
science and mathematics in the
Netherlands and they're mostly doing
research about the efficiency of
immutable collections but also like in
how to speed up programming language
runtimes
and also like how to speed them up with
data structures and vice versa
so let's go yeah today's topic is
collections so what what are collections
collections are actually like a group of
objects and how those objects are like a
arranged but can be categorized like by
like in various categories like what
kind of data type it is is it like is it
a list I said a map data type how it is
stored like a in like yeah insertion
order is it like a unordered etc etc so
the whole domain of collection is like
quite like huge and has a lot of like
variability like inside so in today's
talk I'm not going like to cover all
kind of like different collections that
you might like have seen already but I'm
focusing on a specific subset and that
subset is like mostly concerned with how
to efficiently like get immutable and
order collections like an order like
hash stats or hash maps on the JVM well
now that you know like what collections
are and what kind of collections I'm
talking about why would you want them
immutable and they're like plenty of
reasons like why you would for example
care about immutability but just a few
to mention is that once like from
virtual machines perspective for example
or once you have a key mutable data that
unlocks like a lot like a whole range of
like new optimizations because you've
like stronger guarantees like about your
heap graph and like what changes and
what doesn't change but more importantly
from a user's perspective it gives you
like a simpler like mental programming
model because you're just coding with
values and the values you have they
don't change so there is no kind of like
you're leaking a reference to
some other part of your program and that
probably like corrupts your state of
your collection and last one released
its it's also like in terms of like a
concurrency it's like a very robust
because your immutable collections dear
like thread-safe
per se so you can easily share them
amongst different threats
without having to fear that your state
gets corrupted well I would like to
quote Joshua Bloch on his book like
effective Java and he's an item there
where he says that actually classes
should be mutable unless there is a very
good reason to make them mutable and I
would like to make it like more precise
and say like the same is true for
collections so you actually want like
immutable collections the quote also
continues and says well immutable
collections provide many advantages and
the only disadvantage is the potential
for performance problems under certain
circumstances let me paraphrase that I
would say that immutable collections are
challenging to optimize but they
constitute the more sensible default so
that's my opinion on the topic and I
would like to exactly focus on these two
points in how to get them efficient and
show you like some things you can do
with them that you can't easily do for
example with mutable or rabies
collections so to do so I'm going to
break my talks down in four parts first
of all I would like to introduce you to
the core principles of what does it take
to have efficient immutable collections
and some of their for example more
well-known encoding stare for second to
talk about how to get the memory
footprint down of such data structures
and then I'd like to continue in talking
about okay once you have that how can
you get those data structures even more
efficient and more expressive and last
but not least I'd like to talk about
okay what does it take to probably make
them even faster in future on the JVM or
in general so let's start with the core
principles
well immutable data structures yeah if
you code with your mutable data
structures that doesn't mean that you're
stuck with like like your data and you
can manipulate it in fact you can but
like you're deriving a new instance new
copy for example that has like values
added or removed from them and the yeah
the important thing is how you implement
it behavior under the hood so what you
don't want to do actually is that you
first have to like copy like your whole
data structure okay
probably like allocating like more space
for a new element you insert then
actually mutating the clone you created
and then saying oh yeah from now on it's
like immutable again so here you would
like have to copy the whole data
structure well what does make like our
constitute of the core principles for
efficient immutable data structures so
the first one is actually instead of
having like a large chunk of data you
try to split it up like in smaller
chunks and those smaller chunks are
usually actually aligned in a sort of
tree structure once you have such a tree
structure it's very or you can devise
and update the data structure as a sort
of like a delta where it just encode the
parts that changed so in that case you
would just have to like clone a fraction
of the tree and link back to everything
that didn't change so int imagine like
your previous data structure was a
really huge had a few thousand millions
elements you can like implement it very
cheaply in fact such data structures
they already exist on the JVM and
they're implemented like in plenty of
languages there so jvm languages like
closure rascal Scala all of them like
have one thing in common so that they
have like strong immutable collection
libraries or yeah collection data
structures that are implemented exactly
upon that paradigms and they're usually
implemented as hash array of tries or
let's say
successors to that data structure so
what is the hash your own tribe hashing
remapped tri is exactly such a tree data
structure and one of the interesting
things about that is that each node is
just simply purely an array so you
implement notes as an array and you
encode the prefix of the hash codes in
the tree structure itself well in this
case particularly we will like encode
like five bits each per level and in
order to make it more illustrative and
how that works I would like to continue
throughout the presentation with a
simple example like a set of integers
and if I would encode that set of
integers in the encoding I showed you
before I would end up like with dead
three bits it's like nested arrays like
were you have like segments of a fixed
size these segments here for example
like 32 bits long and those segments
actually like mix data and like
references to further sub trees so I
told you I'm going to talk about the
efficient immutable data structures and
for me that doesn't look a fission does
it it's like there is a lot of like
empty slots that you're wasting there
well therefore hashirama tries for
example use bitmaps in order to signal
which of the array slots is actually in
use and which isn't so by doing that we
can compress that seed it conceptually
asserted too wide long arrays to just
like arrays that are as tight as we need
them to represent the data on the left
you see like a Java like a code skeleton
that tells you exactly that with like a
32-bit integer bitmap and like an array
that can grow to that size what can you
expect like performance wise from such a
tree data structure well first of all
they constitute like shallow and wide
search trees so that the trees that have
a branching factor of up to like 32
and with 32-bit hash codes that means
like we get trees that are only like up
to seven levels deep and by having like
that shape of the trees we have actually
like performed character characteristics
for lookup insertion and deletion that
are like in a very small log number and
the scala community for example coined
the phrase that they are saying like
it's effectively constant because it's
like really you have to traverse under
like maximum amount of like a path of up
to like seven notes to get there one
strong point of these trees compared to
the array data structures for example
are that you can implement structural
operations on top of that so you can
merge structurally to trees for example
in implementing the Union operation
instead of like having to iterate over
one and performing an insert for all the
others so that was a quick review of how
what hashirama tries are and aside those
are already like present in a few
languages out there and the question is
like how can you make them like memory
efficient because we are having three
data structures and those three data
structures actually like have by nature
a lot of like in directions you have
like notes that point to note and for
the example to illustrate in how to make
the memory efficient I would like to
make their code smell even simpler and
remove the bitmap because essentially
what we are having here it's just like a
compressed ArrayList
that like gets up to 32 elements wide
well we have a nested array here or like
a field which is of type array and as
you might know on the JVM
like arrays are objects too so they live
in the heap they cost you like a memory
indirection
probably like cache misses you have to
allocate like extra headers like for the
objects you
you have a length field there you might
have like padding or alignment issues so
that gets back like to Brian's talking
like you weren't just like to to
optimize like a simple property of your
program but yeah so there's a lot to do
behind the scenes so conceptually what
we want in order like to have the
deficient is not to have the memory in
directions and in that example we might
to think about like compile time
specialization but we just say ok hey
let's enumerate all the possibilities
that we have and in this case we can
just get up to 32 different like note
configurations and just emit like
specialized layouts that contain the
number of fields like in lined in the
class just as much as we need them okay
that sounds great from the pearl-like
perspective of like footprints so we get
probably like better cache behavior we
get some more memory footprints but
there are also like two properties that
we get that we actually do not want to
have and that's like well we're getting
megamorph across and also like a lot of
code blot and why is that because we not
only have to specialize the memory
layers we also have to specialize the
operations on the memory layout so we
are effectively like substituting a
dynamic like structure for a set of like
static structures and desn't is like a
lot of like performance overhead on the
runtime as well well in the optimal case
we would like to have that picture like
where we are always happy and that's
like we have small footprints as I
showed you before but we get like the
benefits of a dynamic structure of like
having monomorphic on sides instead of
like having to dispatch on the type and
being kid-friendly like not having to
generate like old specialized code just
for purely accessing my specialized data
layout and that's possible to achieve as
well and the answer lies simply in
decoupling like the data layout from the
operations that operate on the data
well and in terms of our code snippet so
we just substitute like the
specialization with something that looks
like much more dynamic again so in this
case we could for example as one of many
possibilities creating a very view out
of like subsequent fields of the same
types that are like in an object array
did anybody here see that like such sort
of like attempt already on the JVM well
essentially that's also like what Paul
sandals were presented like last year
here at a at the jvm language summit
when he talked about the VAR handle API
that's are coming in JDK and I and not
only that you have like the opportunity
to get handles like two variables or
like fields you also like have the
opportunity to get array handlers for
different kinds of arrays but one part
is missing like from from that proposal
that's exactly like having an array view
on subsequent like fields of the same
types so Paulson was referred to that as
some of the exotic features of the
upcoming very handy API and to my
opinion as far as I'm no it's not going
to ship like in the subsequent release
well once you would have such an
abstraction you can really like code
with a specialized data structure as you
would code with an array so you have
like you're a copy operations like your
clients or side and yeah and you can
implement and code it as you would like
with a regular dynamic data structure so
that concludes a part of how to get rid
of memory interactions in your tree
nodes for example so to have specialized
memory layers but not having the
overhead or like our extensive overhead
for the JIT compiler or the runtime at
the same time like compared to the state
would shipped like in Scala or enclosure
hash tries in the recent years also
became more expressive and more
efficient
and Dara would like to go back to the
example like that you showed you before
so just like it's simple like note
layered and how the tree looks by side
oh by the way I forgot to mention if you
have any questions like throughout the
talk feel free to raise those questions
otherwise I take questions also like at
the end so if you look at the data
structure you see color coded
well we're interleaving data like
payload that we want to store actually
and like the administrative overhead so
like further like references to sub
trees and that actually can get like
more performant as well so just by
manipulating the data layout we can like
get speed ups in iteration for example
over like established data structures up
to 6 sx x 6 x + 6 6 x of us and improve
equality also like by not a magnitude
and that at the same time while also
like reducing the memory footprint well
sounds too good right how how is it
possible but first of all if you have
the interleaving between data values and
sub nodes your data structure actually
has to check at run time what type it is
so you have to do like an instance of
check or some kind of dynamic check to
decide am I now having here a data value
that I have to return to the user or do
I have to do I have to go deeper and
like recurse while I like looking for
wellness and what we did is also like we
change the compression function to not
only like get rid of the empty slots but
also like to permute our elements along
the way and what that gives you is like
an encoding where you have on the right
hand side you have them all the subtle
references grouped together while we
have like all the payload there as well
and in terms of the duration for example
well iteration becomes much more simpler
you just need to know the
like how many like notes how many data
values I have per note and then how many
sub nodes I have so you can yield all
the data values in one go and then just
like once recurs in the subtree instead
of like going back and forth all the
time and yeah so essentially like all
kind of like treat reversals or tree
processing operations like you profit
like from that sort of like reordering
how did we do that
well essentially like on the data layout
side we added like yet another bitmap so
instead of like just having one bitmap
that tells you like which slot is in use
we now have like two 32-bit bitmaps that
tell you as well like what type it is so
we make it explicit are we like are you
having like a sub node on that position
or is it the data where you well how can
I save memory in just permuting for
example some elements and adding yet
another bitmap and there's like a lot of
like detail to that but it becomes more
obvious once if you look at a mapping
holding for example so maps would use
like two slots like for the key and
values and because previously which has
said one bit to like say if a slot is
used or unused also like sub node
references occupied two slots so you're
the mixture of like key value pairs and
then empty slots and sub note references
once you do the permutation you can
actually get rid of all these empty
slots as well so you save memory but you
also like to get better cache locality
and you can like exploit that layout in
the code and keep in mind as well is
separating like or regrouping the
elements that you have like all the
similar types elements together it's one
of the key ingredients for later on
exploiting different memory layers and
that's also like what I want to like to
talk about Mouse how this data
structures got more expressive as well
well just imagine you have like a
a program that processes like America
datasets and you want to store and
process that like you with collection
data structures let's say well if all
your data fits in a 32-bit range yeah
like you have like specialized like data
structures for that you can use like
specialized primitive collectors so like
the Java ecosystem offers you a wide
range of libraries that specially
optimized for that and you get the
benefits of like having the data stored
unboxed and or the past access passes to
the data also like don't need boxing
well let's assume on the other side you
have a data that's like that's more
complex or like a bigger numbers that
don't fit in into the 32-bit region so
you might have to represent them as a
big integer object or something like
that well there we are already like
covered at least like four immutable
collections because you can say you can
just use generic collections as they are
in the JDK so they can be parameterize
by type and well here you go but what
about if you now have like your data
contains maybe a lot of numbers that fit
into the 32 bit range and some of them
did the bigger
well can you express that with a
primitive collection well obviously not
because it's really like specialized in
just storing integers Long's or whatever
kind of you specialize for so you're not
able to express that with a primitive
collection what what's the other ways
can you express that with a generic
collection and of course you can but
you're blowing up your like your memory
footprint because suddenly all your
primitives get boxed so what hashirama
tries are like successes to that
encoding allow you is also like to
generally like store heterogeneous
collections are heterogeneous data
efficiently so you can store efficiently
unboxed integers or any kind of like
value types in the future probably a
long to reference as
and you're not consuming much more
memory so with such an encoding is
actually possible to reach the memory
footprint of specialized primitive
connections while still having the
ability to store references alone how
does it work
well on the background like you here see
like a simple set as before we're from
the right like the subtle references on
the left you suddenly have data mixed so
you have like like here numbers and also
like references to big integers once we
exploit the similar similar tricks that
we did before in like once again
permuting the elements that now or the
primitive data is grouped together and
or the reference data is grouped
together on the right you can once again
like exploit the differences in
differently like specializing memory
layers
how does it work like move the
permutations well in principle speaking
or I showed you before is we had two
different bitmaps that made it explicit
what kind of data type it is and you can
see it also like s we have the state of
like what type in note is distributed
over two different bitmaps
well once you change it encoding and go
back to a state where you have like one
single bitmap but like longer like bit
sequences the tell you which state it is
you can also like dispatch on like
multiple style article states there and
so that boils it down to how much
heterogeneity you would like to support
so for example the same amount of bits
as before we can support like two
different types but you can also like
grow that and say like hey I want to
support many more types so that's like a
trade-off and definitely entails also
like trade-off decisions there but
that's how it works
conceptually I would like to say that
actually that kind of encoding that I
presented use orthogonal to like
different approaches so one of them is
like to union types are in Scala that
approach is like that sort of like
heterogeneity like on the type love
that you can express it at least but
it's also like orthogonal to primitive
generics that are like coming up in
Valhalla because what the encoding gives
you is like a separation and the
regrouping of the values so like that
all of them are grouped together and
then you can like make it explicit for
example with generic types in and
specialize the later on so that
concludes how to get more like how
hashirama trys got more expressive
performant recently but what about the
overall performance and performance
challenges with the data structures well
assad here we're working with arrays
with tweets those trees are like very
shadow but still they're trees and what
we would like to have is like that for
any kind of operation like that we have
trees that are as fast as a race and one
way to tackle that or like at least to
reduce further like the penalty that you
have for the three data structures is
improving the locality of like are
between different nodes so the
specialization approach i showed you
before
deals are like is concerned with
locality within a node but if we have a
tree data structure just also like all
the locality concerns between the nodes
if you have a symbol or a single tree
you can imagine something like laying in
out like a flat in memory and then just
having like four good jumps like a log
and forward jumps to find your data and
that's something like the man like the
java memory management subsystem
or the garbage collector for example
could take care of in like being more
aware of like what data you dealing with
but the story is not that simple because
as i told you before
like once we modify those data
structures we actually get all these
deltas and deltas are actually linking
back to the original state so instead of
having a single tree what we actually
get is a forest of trees
and then it doesn't become that obvious
like how to represent that efficiently
or locality vison in memory so that's
like I would say very open-ended
question like in engineering and also
lacking research in how to for example
allocate those debtors or in generally
how to deal with those forests let's say
on a like virtual machine level but I
would say personally that is definitely
possible like to still reduce further
the performance gap between those trees
and erase and there are many many
aspects with trees already like shine
and outperform like mutable collections
and also like array based
implementations but there is still like
a performance gap for some of the
operations so to sum up I was first
presenting let's say the state of the
art of hashirama tries as they are like
implemented several languages that
already exist on the JVM then I was
talking about how to get those three
notes more efficient by specializing
them and what kind of like dynamic
exercise it would need in order like to
treat specializations still as a dynamic
data structure furthermore I was telling
you in how that how hashirama tries got
more efficient and that's mainly that
was mainly achieved by permuting how the
elements are stored in the tree nodes
itself then I was focusing on how to get
more how to got more expressive and more
expressive in the sense of that you can
also efficiently store heterogeneous
data there so for example like unboxed
primitives or value types along to
references and in the end I was
sketching like some of the still
remaining performance like parts were
like a issues that we need to address in
order like to make them yet faster well
as a takeaway I would say that Java
already caught up in the last year's for
example with Java aid and the stream API
in providing functional
fractions in Java but what's still
missing are what we still have is like
we apply the functional abstractions of
mutable data so we are missing effective
and efficient immutable collections on
the JVM or in the JDK so some of the
more details about what I was talking
today you can find for example on that
github repository and there are also
like links to scientific papers and also
like implementations where you can dive
into Orlick which you can use and with
that I would like to close and thank you
for attention so I'm happy to take
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>