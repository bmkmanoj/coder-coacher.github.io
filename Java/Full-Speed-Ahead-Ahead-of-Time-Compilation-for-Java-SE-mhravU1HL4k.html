<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Full Speed Ahead! Ahead of Time Compilation for Java SE | Coder Coacher - Coaching Coders</title><meta content="Full Speed Ahead! Ahead of Time Compilation for Java SE - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Full Speed Ahead! Ahead of Time Compilation for Java SE</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mhravU1HL4k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right well I welcome everybody glad
that you could all make it looking I see
a few familiar faces out there us ooh my
son for coming
so today yeah I'm sure you've seen the
safe harbor statement a whole bunch of
times but I kind of just want to
re-emphasize it for this talk because
everything I'm talking about in today's
section or today's session is
experimental as of JDK 9 and not
officially supported in any way so
usually we just kind of pass right
through these things but in this case it
actually is kind of important so please
don't build your entire production
system based off of the OT technology
you're gonna talk we're going to talk
about today so a few people in the
audience I think know me but just for
those of you who don't I'm a JVM
sustaining engineer which means I fix
post release hotspot bugs that's my day
job I'm almost of the JDK updates
project maintainer or one of them so you
can find me sometimes arguing with
people online about what kind of changes
should and shouldn't make it into update
releases I've done Java one before
I'm co-author of Japan's best-selling
WebLogic book and I do Twitter and have
a blog so we don't have a whole lot of
time to talk today so we're gonna kind
of try and keep things fairly simple
this is one of three different IOT talks
that I'm aware of
at JavaOne this week and fortunately the
other two talks are all tomorrow so if
you weren't planning on necessarily
attending them tomorrow maybe you can a
put that into your schedule because some
of the upcoming talks look pretty
interesting and good too and I have some
pointers to them at the end of the slide
deck but for day this is kind of an
introduction to the feature itself we're
gonna start off at the very beginning
and kind of introduce what IOT is and
what the goals are that kind of the
design goals for this new feature and
what you might hope to get out of it so
we start off with some kind of boring
warnings just to make sure that there's
no misunderstanding and that everyone
takes everything I say with a little bit
of grain of salt so the first thing is
that like I said before it's not yet
officially supported son and then later
Oracle kinda has a long history of
introducing new features this way other
examples might be tiered compilation
the gg1 compiler the car I'm the the g1
garbage collector garbage first these
are all features that originally enter
the product as an experimental feature
that was not officially supported and
then they became supported and then
later they became the default enabled
right out of the box and so while we
don't know for sure whether whether a OT
will will take a similar path there
there is kind of room for optimism but
as of now it's not something that you
can really affect expect official
support from Oracle from that said if
you do find a bug please file a bug
report we want to know about them we
want to fix it we want to make this as
high quality as possible as of today
it's only supported on Intel 64-bit for
pro Linux and also just as another
requirement you have to have lib e.l.f
installed that's no biggie you just have
to get it before you build or whatever
but it has to be there and so hopefully
we're planning on adding other other
platforms as well in the future but for
today this is the only one that you can
use this on there's also no official
documentation which hopefully I'll kind
of fill the gap a little bit with
today's session the only official thing
out there really is is the jep document
and again there right now we don't know
when or even if official support will
follow or in what timeframe certain
platforms may or may not be entered
that's all the boring boilerplate I had
to get out of the way let's let's
actually jump into a ot and what it's
all about
so given all the the things I just said
why would you bother learning about it
if you can't you know really use it in
production yet well for one thing it's
pretty darn likely that this may be
supported in the future and so just in
the same sense that you want to get a
hold of EA builds early access builds
and play with them to learn about the
direction the platform may go in on this
is your opportunity to get a jumpstart
on on what may be a very important and
exciting technology being introduced to
the platform the other thing is that
like most of my talks we're going to
jump off on a few random tangents that
brush up against other features of the
hotspot JVM and
so it's kind of a good excuse to just
kind of cover some interesting material
and also if you're anything like me you
just think this stuff is really cool and
a whole lot of fun so it's always good
to kind of explore and get a peek under
the hood so everyone's really interested
in this because this is a fundamental
change or addition to Javas execution
model and that's been kind of in place
since the beginning of time immortal so
originally we had you know you had your
java source code you can pile to Java C
you get a class file and then you run
that on the runtime this should not be
surprising to anyone and then of course
in the the java 1.2 time frame we
started adding JIT compilation into the
mix and so while you were running your
application on your runtime the JIT
compiler would emit actual machine code
that would be stored and run from memory
and JIT compilation has many advantages
the idea here is that methods do get
compiled but compilation is expensive
compilation is always a very resource
intensive exercise and so both CPU
cycles and memory are being consumed and
your applications execution threads are
basically competing with the C 1 and C 2
compilation threads for limited
resources or finite resources in your
application but once we do get over that
hump and we get the method compiled the
machine language versions that we admit
run very very fast and gives us the peak
performance that we expect from Java
today but one reasonable question is is
well is j compilation really always
superior or better than just peer
interpolate in a peer interpreter
implementation and we kind of have as a
possible case study of that J rocket so
I didn't mention in my self introduction
but I actually come from from the J
rocket side of things originally and we
had a pure jet what kind of like a
classic jet implementation where we
didn't have an interpreter like hotspot
does to fall back on so everything just
got jet compiled by default and so does
this really give you better performance
this kind of of militantly trying to
everything to machine code and the
answer is well what's your definition of
performance so kind of an extreme
example and no this is profoundly unfair
to Java but on left here I'm running
just a native HelloWorld you know that
was written in C you know this this
comparison means nothing because
obviously something written like that
versus a full runtime of garbage
collection and everything else Java
brings to the table
you're not going to compete but the idea
here is that that people really do look
at this these types of use cases and are
asking themselves you know like like
well why does Java take so long so
there's always been this this pressure
to try and make Java start up and run as
fast as possible and so the point of
using HelloWorld here is that just out
of the box even executing anything you
know the JVM is kind of behind the
native world in terms of startup time
and people noticed that and so whether
the comparison is fair or not people
make that comparison and it's something
that that we have to be aware of and and
that kind of motivates us to try and
prove on these times as much as humanly
possible so check compilation isn't
always fastest and that's why we have
the hotspot way of thinking or
philosophy where we don't J compile
everything like J rocket did
so like I said the couple the overhead
for compilation is is extremely high and
so we're consuming resources like memory
and CPU time but also another issue that
people are not necessarily always aware
of is that there's also a latency
involved with JIT compilation and what I
mean by that is that even though we have
the interpreter that can can start
running the code immediately and we can
asynchronously do the the compilation
and a separate compilation thread there
there's a delay before we get that
compiled version so there's a delay
before we hit the peak performance for
that particular method and so in the
sense you know we get to run it
immediately but we're running it slow
we're running it in the interpreter and
what we want to be doing is we want to
be running it the fastest version of
that method the compiled version of that
method as fast as humanly possible
so in hotspot what we do is we start off
every method begins with being
interpreted that's the default you know
behavior that everything gets and then
we're constantly doing what we call
hotspot profiling which is basically
just collecting you know doing a stop
the world and seeing what all the
threads are running and keeping track of
all that and then we have statistics for
for you know invocation counts and
things like that
the interpreter actually gives us most
of that information for free anyway so
we have all this information and then we
identify which methods are high and we
only just compiled those and then the
interpreted ones just remain interpreted
you know so things like static
initializers and and other stuff that
you know just there's no value in paying
the overhead cost of compiling we just
avoid by compiling by not compiling at
all but even with hotspots JIT that
there's still some some very bad kind of
weak points that that we have to deal
with application startup time is still
an issue it takes you know longer we
have a warmup period so even after your
application starts responding to
incoming requests it still may take a
while before enough of the hot methods
get compiled to machine code and we
start seeing peak performance so startup
time is one measure and then time to
peak performance is another measure and
they're both things where there's
possible room for improvement in the
current compilation model and again
we're consuming a lot of resources most
notably memory and as kind of a side
thing not so much of an issue with
Oracle's product offerings but there are
some platforms out there that for
whatever reason disallow the dynamic
generation of machine code at runtime
and so if you wanted to run the JVM on
them you're kind of stuck with an
interpreter unless you can find some way
around that so this is something I
talked I talked about him yes in my
session yesterday and I think those
people are there with us but there's two
separate compilers that are built into
the hot spot a VM a c1 which is for
client use which kind of just does a
quick and dirty compilation the idea is
to kind of finish the act of compiling
and get that that quicker machine
language version of the method available
for execution as fast as possible then
there's C 2 which is the server compiler
and that one spends a lot more time
ie resources on on doing very heavy
optimization to give you the most
performant code as possible and that one
of the key differences here is that c2
can't just run out of the blue c2
depends on having a whole bunch of
profiling data that was collected by the
runtime before it compiles a method and
so without that data we can't really see
to compile methods and that will come
into play later and in a few more slides
so that's the current situation as it
exists kind of today or before JDK 9 so
what does a Oh to bring a ot bringing to
the table here well the idea is of
course you know because it's ahead of
time is to generate machine code before
the runtime before we actually run the
application so this is kind of just like
like traditional static compilation and
so the JVM hopefully can find and load
that code during runtime and then we
bypass the latency in the overhead of
compilation work while we're running so
this is the the kind of standard model
that we have today again we just saw
this in a slide a few slides ago and the
the addition here is that now we have
this new tool this.j a OTC Java ahead of
time compiler and it's going to give you
an e LF shared object or shared library
that you can load into the runtime and
use that pre-existing machine code to
startup as fast as possible but there's
some other benefits here and so in a
traditional model where you have
multiple JVM running and they're all
running the same code like say some sort
of of redundancy or clustering we've
been the same OS VM you each JVM kind of
gets its own copy of it so not only
you're paying the performance cost of
compiling it all those times but then
once you have the output of that act of
compilation once you have the actual
machine code that has to be kept in the
code cache and the code cache takes up a
lot of space especially with the
introduction of tiered compilation our
code cache size expanded by about five
times and so if you run a lot of JVMs on
the same OS instance
that can actually be a lot of
unnecessary redundant overhead so Ã¤Ã´t
allows us to create these read-only
segments in memory that have the the
machine code in them and the commission
that can be shared across multiple JVM
instances and so we have a density
improvement where if you're deploying
all these different gems to the cloud
the total consumed memory is going to be
less so there's a lot of advantages here
it's going to give you faster startup
times this this is just you know as soon
as your your application is ready to
start actually taking and processing
requests that's going to improve then we
have beyond that faster warmup times or
less warmup time and so even though it's
taking requests like I said a few slides
earlier it takes a little while before
you reach peak performance but with
Ã¤Ã´t because a lot of that those hot
methods are already going to be
available we can reach peak performance
much faster and of course because we're
not doing that compilation at runtime we
also have less memory footprint we have
a less overhead for compilation to
compete with your execution threads for
your application that are doing real
work for you and of course we're getting
the space savings the density
improvements if you're running multiple
JVMs that are running the same code and
now we can run on on you know platforms
that for whatever reason might disallow
runtime generation of machine code so I
have little stars next to all this so
what I'm saying is your mileage may vary
and what I mean by that it's like a lot
of options like like tiered compilation
or different different GCC's garbage
collectors you will find that some
applications benefit from these changes
and improvements and other ones might
even have performance regressions from
it it's very hard to say there's no
silver bullet that kind of improves all
cases and that's one of the reasons why
this technology is is not really you
know officially supported yet and and it
might be even longer before it would be
on by default and that's because really
there's there's cases where where this
actually doesn't doesn't help but for
many applications we've seen
improvements
so I'm sure there's a few people in the
audience they're going you know quicker
quicker warm-up times quicker startup
times
sharing data among different JVMs all
this sounds pretty darn familiar like
why do I think I've heard all this
before and reality is there's actually a
number of other pieces of functionality
that have been added to the platform in
the recent years that kind of have very
similar design goals to what we're
trying to do with a ot and so I just
want to cover those really quickly to
kind of put everything in perspective
and see how IOT kind of fits into the
larger picture of startup time and
performance improvements that we're
trying to do so the two big ones are
tiered compilation and class data
sharing and also app class data sharing
and of course with the the announcement
that we're basically taking all of what
are currently the commercial features of
the JVM and making them open source for
the first time open JDK users will also
be able to take advantage of app class
data sharing in the very near future so
tiered compilation it's a pretty
complicated topic and of its own so I
don't want to dive into too much detail
here but basically idea is that we no
longer have to make the choice between
using the ce-1 compiler or the c2
compiler we actually use both and so
traditionally the server JVM would just
have the interpreter which would run for
say you know 15,000 executions of a
particular method and then it would
compile it using all the profiling data
and with tieran enabled we now have a
mid-weight point where we use the c1
compile a compiler to generate
instrumented code that collects the
profiling data needed by c2 but also is
much quicker and easier than a full c2
compilation so we kind of get the best
of both worlds we get a really quick
compilation that happens very early on
in the life cycle of the process but
it's it's fast enough that it's it's
dramatically quicker than interpreting
the the Java bytecode directly and that
kind of makes things faster until later
c2 can come in and give you the real
peak perform
it's a nice happy middle ground we also
have a class data sharing so in a
traditional situation this should look
very familiar to a few slides ago we
have masane class is being loaded by
multiple JVMs on the same OS instance
each JVM is going to go through all the
effort of parsing the the Java bytecode
and turning that into the internal data
structures that are used by the hotspot
JVM traditionally those restored in the
permanent generation now that that
information has been moved over to what
we call the meta space in JDK 8 but the
point is is that it's consuming genuine
memory so with class data sharing the
idea is that you would create an archive
that would sit on your file system and
that would actually have kind of pre
parsed or predigested versions of the
class data and so the JVM s could share
that and that would give you a faster
startup time because you're not actually
parsing the Java bytecode and it would
also give you some density improvements
because you're sharing that across
multiple address spaces so there's
actually two different forms of class
data sharing as the the standard class
data sharing that's been around for
quite a while I'm not actually don't
remember when they added add something
somewhere on the 5 or 6 era and and
that's kind of free to everybody to use
that's never been a commercial feature
but it is only works with the JDK
classes themselves ie what used to be
part of Archie jar so the Java SC class
library was really the only classes that
were more compatible or that worked with
the traditional class data sharing then
there's apt class data sharing which as
you can probably assume from the name
means that it's the same class data
sharing technology but applied to your
own classes and within your application
and so with app class data sharing you
can actually do the same thing with your
classes and share them across just kind
of pre parsed versions of them across
multiple JVM instances and today that
this is a commercial feature that you
don't that's not part of open JDK and
you don't you know get to use this
without paying
Oracle money but in the future in the
very near future
just with java flight recorder and all
the other great stuff we're actually
bringing this into the open source and
so this would be part of open JDK in the
in the very near future
so just to make clear what that what the
differences here are so the reason this
whole sounds probably really familiar
for you is because a lot of these diet
design goals are the same tiered
compilation that the killer apps so to
speak for that a faster startup time and
faster or at least a less noticeable
warmup gap and then class data sharing
app class data sharing is also giving
you quicker warmup time and some some
better memory efficiency by removing the
redundancy that would otherwise be there
for having multiple JVM load the same
classes but what's different from all
these what makes a OT unique is that
none of these involve actual the output
of the JIT compilation process so a OT
is all about what would be within the
JVM code cache this is actually the
machine code that's generated all of the
class data sharing an app class data
sharing saved the metadata about the
classes but it didn't it didn't actually
do anything with generated code so Ã¤Ã´t
is really a first for the platform and
all of these technologies work together
it's not a mutually exclusive situation
where you have to choose one over the
other
and we found in our performance testing
that they actually really work best if
you use all of them at the same time so
this is a ot represents another
incremental improvement in our goals of
faster startup time and and better
memory usage so it was suggested before
I did the this particular talk for Java
de Tokyo that I could play better
humanize my presentations by having more
pictures of people and I was given this
slide so this brings the total of
pictures of people to 10 entire
presentation which is a pretty good
average I think most most of my
co-workers have only like 5 or 6
pictures of people so I don't think
anyone's going to complain anymore usage
so this is where we get into the meat of
things so as already mentioned we have
this new tool J a OTC and it's sitting
there just right next to Java C and
everything else in your bin directory
and using it is pretty darn simple
they because you have to actually
explicitly label the output otherwise
you get like a really horrendous default
name well I think it's like output dot
soo or something it kind of reminds me
of GCC but anyway you just tell it what
the name of the library that you want to
generate and you feed it your class and
it will it will give you it will give
you that that shared object file and of
course there's rather extensive online
help that will be useful especially if
it's a new tool and you'll notice if the
default invocation kind of follows the
UNIX philosophy of you know don't say
anything unless stuff goes wrong and
that's to be compatible with all your
standard tools that assume that any
extraneous outplay represents some sort
of error but you can change that with
the info flag and that kind of visit
it's like a verbose setting and it that
will actually tell you what it what it
found in the class file and and what it
decide decided to try and compile so you
can kind of prove to yourself that it's
working as you expect it to do which is
kind of important to do the sanity
checks when you're using a new tool so
there's there's a few gotchas that are
kind of interesting one is that by
assertions which most people you know
never bother to enable actually has to
be known at compile time and normally of
JIT compilation this isn't an issue
because the the JVM knows if you have an
assertion is enabled or not but one
thing that could happen is if you
compile without assertions enabled ahead
of time and then you run it in your JVM
you think during the runtime that just
by enabling assertions there that you're
getting all of your assertions turned on
when in reality anything that you
compile ahead of time with with this
tool will not have the assertions
enabled so if you want to have those
assertions on you have to make that
decision when you do the Ã¤Ã´t
compilation and this flag this compile
with assertions lets you do that I think
we have the compiled commands flag and
this is simply just a way to kind of
bail out of compiling specific methods
in the future we might expand this we
might add other options or settings or
whatnot but as of today the only thing
we support is excluding individual
methods
or you can use wildcards to to say
exclude an entire package and so because
this technology is new and we haven't
exactly worked out all the kinks there
are cases where you you'll have problems
where a particular method might cause
the Ã¤Ã´t compilation itself to fail or
perhaps even worse the method might be
compiled but it might be compiled
incorrectly and so the generated machine
code might have some sort of problem in
it and so at runtime that problem
manifests in some scary way and so what
you can do is if you find that there's a
particular method that's causing a
problem well first thing is I'm hoping
that you'll send us a bug report if we
don't know about it already but the
second thing is that you can use an
exclude line in a compiled commands file
so you can just tell a OT to compile
everything but that method and then
continue using it so it's not just
classes the the kind of hello world
example is just you know doing a class
but in practice what we usually want to
work with is either an entire module at
a time or an entire jar file at a time
and Ã¤Ã´t has no problem doing that
it'll just suck the entire thing up one
thing I want to point out is that
compilation as I've said several times
already is extremely resource intensive
and Grell which is the compiler that we
use for IOT compilation definitely uses
a lot of a lot of memory and a lot of
resources and because this actually
happens in Java this is you know this is
all Java software that does the
compilation for us this is a native code
we actually have a heap that you can run
out of and so for example if you're
compiling the entire java base module
I found recent builds that I need about
somewhere between a five and a six
gigabyte heap just two to compile Java
base so if you pass this command line
today to the Ã¤Ã´t compiler it'll
actually thrown out of memory or on most
systems because most systems you're not
going to have a default heap maximum
heap size big enough to handle all this
so if you're going to compile something
big just be prepared and this is like
you know a huge chunk of the JDK so it's
not that surprising but
be prepared to give it lots of memory
and a lot of time I think I think this
usually takes about somewhere between 20
and 30 minutes depending on how fast
your machine is it's pretty intense so
another thing is that and that the
syntax is not unique to the Ã¤Ã´t
compiler but you should be aware how to
pass options so of course like for
example if you want to set a larger
maximum heap size this would be the way
that you do it and this is the standard
format that of a C and a whole bunch of
other tools use that allow you to pass
things to the JVM running whatever the
whatever it is that the tool that you're
trying to execute there but this is
actually way more important than then
you would normally and I'm going to
explain why in a few more slides but
just know so it's the standard - J and
then whatever whatever JVM option you
want to pass I know it's only gonna it's
only going to be what you explicitly
hand it on the command line
it won't go behind your back and compile
anything for better or for worse so
there's another option this ties into
the tiered compilation that I would
introduced a few slides ago and there's
a compiled for tiered option so it turns
out that you know internally the the
Ã¤Ã´t tool does not use the same
implementation for compilation that the
hotspot JIT compilers use like I said we
had to jet compiled we have C 1 and C 2
and a lot of people assume that well
obviously they ot tools going to use
that that code base or that
implementation to compile but it
actually uses growl growl which is part
of the Oracle labs work and study into
JVM technology and growl produces code
that that is uniquely different from C 1
C 2 and performance wise it's actually
somewhere in between a lot of testing
that we've done shows that to somewhere
be somewhere about maybe 10 percent
slower than than C 2 which is pretty
darn good
but you're not getting the full peak
performance that you'd get from from a
legitimate C to compile and probably the
missing piece there is the fact that we
don't have the runtime profiling
information that c2 can take advantage
of when it JIT compiler stuff so the
actual ideal situation for for peak
performance is as I said earlier to use
Ã¤Ã´t as just one component in your
strategy for getting the best
performance possible and you do that by
saying compile four-tiered when you have
ot compile and so what that does is it
doesn't generate the default growl code
it tells growl to include the the
profiling instrumentation that c2
requires for it to compile so you
actually end up with the same kind of
scenario that you do of tiered
compilation except the the middle step
is no longer c1 it's the the Ã¤Ã´t
compiled growl code and that will be
just a precursor to what will hopefully
eventually be the c2 compiled code and
so you don't have to you basically get
to have your cake and eat it too
here as well you get the the immediate
performance advantage right out of the
gate of having a pre compiled code there
but then eventually the JVM on its own
time can can loop around and see to it
to give you that extra 10% bump so what
does actually do when you run it when it
when actually first proposed to this
talk I was like well we're going to do a
whole bunch of demos and it turns out
the demos are like not horribly exciting
because well this is it works right and
there's nothing here that would tell you
that you know the library that you
actually created was used so what can we
do if we actually want to verify that
that was actually used well there's an
option there's a new option that's been
added to the JVM called print Ã¤Ã´t and
it's disabled by default obviously but
if you put that in and you run it you'll
actually get some output that's going to
look a little similar to the print
compilation output that we're all pretty
used to seeing and this is telling you
two important pieces of information here
one that it was able to find the library
and load it
and the other two things is that there
were two separate methods that were
identified and loaded into or kind of
acknowledged as being part of the code
cache and that's the constructor for the
class and then the actual hello world
main which you guys don't need to see
the source code for because it's as
boring as you're imagining so let's look
at let's try the same thing so let's go
and I'm gonna disable compressed
oops so just just to get an idea how
many people here are familiar with
compressed oops compressed references
okay so about two thirds so good we're
I'm gonna go into a little bit of detail
time permitting a bit later in the slide
deck about those but you know very
common thing is is that you might want
to you know disable compressed
references for compressed oops
for whatever reason so here we're
starting the JVM and I'm just disabling
that and everything else is just like it
was on the command line earlier and
you'll see that I actually get an error
it tells me that hey you know you you
wanted to load this Ã¤Ã´t library but
it's actually was compiled with
different settings than what the JVM
runtime is set for now notice it does
actually run the code nothing actually
broke so even when we fail to load the
Ã¤Ã´t library and use it for whatever
reason your application will still
rotten you'll just get a little warning
if you have the print Ã¤Ã´t if you don't
have the print Ã¤Ã´t listed it'll
actually silently fail to load the AOA
ot library which may or may not be the
ideal behavior but it does kind of
follow along
the philosophy of not breaking anything
that currently works so the idea here is
that you need to match your your runtime
with the compilation so the options that
you use during compilation have to match
the options that use during the runtime
and here I'm just pointing out that if
you if you took away I'm reiterating
basically that if you take away the
print Ã¤Ã´t thing and you run it this is
still failing this is exactly the same
command line that I was just passing
except we don't have the verbosity
output and so it's silently failing to
load that for you but we have to we have
to match these these options and the the
current kind of documentation so to
speak the the JEP document which is the
closest thing we have to to any kind of
official documentation for a OT
currently says it just simply says that
you need to match the options but it
doesn't really say which options you you
really need to to match although it's
implied and in practice there's only two
things you need to worry about you need
to worry about which garbage collector
you're using and you need to worry about
whether compressed oops is enabled or
not and it's kind of interesting the
thing about why that's the case but just
to summarize here so you can think of
every single compile Ã¤Ã´t compile and
every single run as falling into one of
these four quadrants so you know each of
these is can just be thought of as a
binary thing and it's either on or off
and I'll give I'll stick you into one of
these things so as long as you match
your a ot compile with your runtime
settings then the library should be
loaded and we make sure that that it's
sanity checks it when it loads the
library so that you don't ever end up
loading something that is incompatible
with the runtime additionally we also of
course check that the class hasn't
changed so there's code in there that
basically fingerprints classes that this
isn't we don't really intend for like
malicious users to try and do anything
sneaky so it's not like a cryptographic
sound hash or anything like that but it
should be enough to prevent you from
accidentally making a change to your
class so the bytecode doesn't match the
the library that was produced from a ot
compilation of that same bytecode so why
do we need to synchronize VM options
that seems like a really arbitrary kind
of pain in the body dang right like that
it's a usability issue and there
certainly is room for improvement here
and that is kind of on the internal
roadmap to hopefully make this work a
little bit easier or better or at least
be a little more intuitive but today you
you really can't do what we referred to
as cross compilation and what I mean by
cross compilation is I'm not talking
about from one CPU type to another but
even
like from we've compressed references
using a JVM running the OT tool that
uses compressed references to a runtime
that doesn't use compressed references
we don't currently support that so why
don't we support that well if we look at
the compressed referent or compressed
oops and oops oops by the way is
everyone familiar with the term oops sir
okay so oops just means a reference it's
a pointer to an object on the heap that
it stands for ordinary object pointer
and it's kind of a strange name but it
has a lot of historical precedent so
small talk and self apparently used this
term and and that's why we've inherited
it and of course the hotspot JVM kind of
evolved from a self runtime so there's a
lot of history there for free using that
term but we knew here oops just think of
object reference it's it's a pointer
into the heap somewhere so compressed
oops is well a compressed object pointer
of some kind so what the idea is is that
originally when you make when we made
the move from 32-bit to 64-bit there was
a performance penalty that you were
paying even on a platform like Intel
where you were getting all these
additional registers and stuff and so
you'd normally expect kind of a speed-up
when you go to 64 bit because the
register pressure was relieved to a
great extent but the slowdown was a
result of the fact that all of a sudden
all of our references are 64-bit and of
course this isn't Java specific I mean
all software kind of has the same
problem when you move from 32-bit to
64-bit is that all of a sudden you you
you have this bandwidth cost between
memory and the CPU because all of the
pointers are now twice as big so for the
heap maybe we could do something to
alleviate that overhead and to kind of
make that regression just magically go
away and the cool thing is the the guys
that son did find a way to do this and
actually actually I think we did this a
j-rok at first and they copied us but
I'm not entirely sure what the history
of there is but um anyway maybe maybe we
both came up with the idea at the same
time but the idea is pretty simple it's
what is is if the heap is actually
smaller than four gigabytes
then you don't need a heap address space
that's larger than 32 bits so what you
can do is just you know stick the heap
somewhere in your address in the
processes address space and then just
add the offset of wherever that heap
starts to every he preference and oh you
basically can address an entire four
gigabyte of heap without having to store
64 bits of every address and this has a
tremendous impact on the performance not
just of speed but of size because all of
your objects any object that has a lot
of references in it to other objects
pointers to other objects become
significantly smaller so it can actually
make your heap consumption smaller as
well so this is great
but how does this actually get
implemented well I said there's you know
we break the 64 bit pointer into kind of
two halves and so there's a 32 bit least
significant bits and then there's 32 bit
most significant bits and the idea is
that this these are not going to change
right if you have a heap that's under
four gigabytes and you align it
correctly so it's not sitting on on a
border between two different you know
kind of 32-bit address space pages in a
sense then this whole this will be the
same for all of your pointers and so we
can just like throw that away we don't
need to keep that information handy and
so this is what actually gets stored in
every field of an object on the heap
that's pointing to another object and
that's all fine and dandy but then how
do we actually use that and so in the
code that's generated when we're
actually looking at a reference because
obviously on the 64-bit platform if we
have a pointer that's in a register on
the CPU or somewhere in in this the
threads execution stack it has to be a
real 64-bit pointer right so how do we
expand it or uncompress it and basically
all we do is just we add the the base
address of the heap to it so it's very
simple and really easy but it's
something that we have to do and
compiled code has to do this so this is
one we have to know ahead of time when
we Ã¤Ã´t compile that this is actually
pointing you know this is actually
compressed reference not a full
reference and then we have to actually
manipulate it but
or we can use it in its raw form this
really cheap and easy to do but a more
complicated example is what if the heap
is somewhere between 4 and 32 gigabytes
well surprisingly we can actually do
compressed references they are too and
the trick is that on a 64-bit platform
there's alignment requirements right so
we have to basically start everything at
a multiple of 8 the address always has
to be a multiple of 8 and so that means
that in the address the least
significant 3 bits are always going to
be 0 which means of course if it's in
variate that that that's redundant
information we don't need that so that
kind of expands our are the ability of
the largest amount of address space that
we can reason about to 8 times what it
was before so that gives us actually a
32 gigabyte chunk of space that that we
can reference with just 32 unique beds
and so we just kind of take that out
then you know the kind of the only part
that matters so to speak and this is
what gets stored in the heap and then
everything else we sort of just throw
away but again there's there's a cost to
this not so much in performance at run
time but but in complexity so when we
compile now we need to be aware that not
only do we have to add some some base
address where the heap is located but we
also have to shift the the address three
three digits three binary digits to the
left or right depending on if we're
compressing or uncompressing the
reference interestingly enough this has
this shift is basically free because the
the CPUs we support allow for move
instructions there's a scaling factor
that's built into the instruction so we
can go up to we can multiply by up to
eight when we do these moves and so
there isn't actually a shift instruction
that's added to the outputted code it's
all compact and in the same move
instruction so we get this for free but
the Ã¤Ã´t compiler has to be aware of it
it has to
to do this and this is why we have to
match our or VM options the other thing
is is garbage collection strategy now
garbage collection is a huge topic with
a bunch of very complicated details very
you know none of which we really have
the time to cover but just to give one
example of why you need to be aware of
what garbage collector is going to be
used when you you when you are compiling
for it whether it's chicken pot
compilation or IOT compilation is the
the necessity of right barriers so right
barriers are basically a way that we can
use to avoid having to do mark for the
entire heap and when I say mark I'm
talking about mark as in mark-and-sweep
woman mm we want to look at the heap
only I want to identify which objects
are live versus which ones are dead
obviously for a very long time we've
used a technique called generational
garbage collection based off of the
generational hypothesis and that's where
we divide the heap into an area where
it's young and then there's old and I'm
deliberately simplifying this
dramatically obviously g1 looks nothing
like this and not even CMS or anything
you know there's scavenger spaces and
and the role implementation is much more
complicated but in general sometimes we
have parts of the heap that we care
about that we want to collect and you
know I'm very frequent we have other
parts of the heap that we don't
necessarily want to have to collect or
mark very frequently and so we have we
have some division we want to handle
them differently so if I come in and I'm
trying to do some sort of concurrent
collection for example and I'm you know
an executioner mutator thread comes in
and creates a new object and then has a
and then modifies some field of some
other preexisting object on the heap to
point to this new object
well this is okay and we don't really
need to keep track of this because if
this is the the young space of a new
space of the nursery or whatever it is
in the actual garbage collector we're
using we're gonna find this when we mark
this area so this is a this is actually
you know fine in and of itself but what
if we have something in the old space or
you know whatever region that we don't
want to have to mark you know this is at
the part of the heap that we don't
frequently go in and try and collect
well if
the code modifies an address pointer or
an OOP that's here in the non collected
the the region we're not going to
collect next we need to keep track of
this somehow we need to do bookkeeping
and so what we do is we have something
called a card table and a card table is
basically the data structure it's used
for our write barriers so what is the
write barrier a write barrier is
additional code that we admit when we
JIT compile or that the interpreter
executes that says that if you if you
write to something on the heap if you
change an address anywhere on the heap
you want to mark a little bit in this
card table somewhere to say hey there's
a change here so each little chunk of
the card table is just basically like a
map that corresponds to as it happens to
be 512 bytes of of the heap and so if
there's a since each of these can be
thought of as a flag if that flag is set
that tells the garbage collector that
hey there was a change you know whatever
chunk of the heap corresponds to this
one flag there was a change there and
you should go look at that
so now when we modify any object aught
you know any object field or pointer
field in an object we just mark that
there and then here and the not young we
mark that there - and then when we do a
GC we can you know do the young
collection like we always would and you
know mark the entire thing and then we
can just check the the individual pieces
of the part that we're not collecting to
see if there's any incoming pointers and
this is tremendously faster than having
to mark the entire thing so that's in a
nutshell how we do generational
collection and it's also how we do
concurrent collection at the card table
the right barriers in the card table are
integral to both of those parts of the
GC and I think knowing that now it's
pretty easy to understand that when we a
OT compile classes they ot compiler
needs to know what garbage collection
algorithm we're using because we need to
know whether right barriers would be
necessary or not if you're using
parallel GC we actually don't need the
right barriers necessarily you know if
it was if it was not
generational collector it's not the I
mean there's some cases where we didn't
we need different types of right barrier
there's different data structures that
the GC uses that do or do not need to be
updated and you know it all depends on
the individual garbage collector that
you're using but the key takeaway is is
that we need to know what the garbage
collectors expectations are but when we
compile so anyway we have these four
different kind of scenarios or four
different types of Ã¤Ã´t compilation
that we can end up with as a result and
each one corresponds to a set of GART a
choice of garbage collector and
compressed
oops is enabled or not enabled and
there's a there's actually a known
library names that if we stick these
these libraries under the Lib directory
of our JDK directory the the JVM runtime
will actually find these automatically
and use them but it's only for a small
subset of logic modules that are
actually part of the JDK proper itself
right now so if you want to use your own
you know if you want a OT compile your
own classes the only option you have
today is to pass then the library file
name on the command line but if you are
doing one of the one a chunk of the JDK
itself a module from within the JDK
itself and it's one of these five
packages or I'm sorry about one of these
five modules one of these four or Java
base if it's one of those five then you
can just stick it in library in the libs
directory and the the JVM may pick it up
automatically so we're out of time but
just to wrap things up so the value that
that is added by ot compilation here is
that it really gives you a faster start
up you reach peak performance quicker
you have a smaller footprint if you have
multiple JVM a smaller total footprint
if you have multiple JVMs running the
same code and we can even run on on
platforms that would otherwise disallow
jet the risks here the the caveats are
that it's experimental it's not
documented
it only runs on Linux amd64 today and
custom class loaders invokedynamic or
india are not supported and of course
your mileage may vary so there's other
Ã¤Ã´t sessions this was this one was the
introduction one this one was just how
to use it whether you know the
information you would need to consider
whether you might want to use it or not
to make that decision and hopefully
you'll walk out of here with feeling
that you've learned that but if you want
a more in-depth talk I'll look under the
hood
these guys are doing a talk from that
they also delivered at the JVM language
summit and this was this was excellent
at the language summit you can actually
catch it on on YouTube if you are unable
to see it live today but if you can fit
it into your schedule I strongly suggest
going and seeing them live tomorrow and
then also there's another session I
don't really know the details about but
someone is doing a kind of more I guess
fundamental talk about about some of the
principles involved with static versus a
ot compilation so it should be an
interesting talk so thank you very much
for your time I very much appreciate the
oh sorry oh it's your talk okay Wow yeah
yeah so I'm sure it's gonna be a great
talk okay thank you very much for your
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>