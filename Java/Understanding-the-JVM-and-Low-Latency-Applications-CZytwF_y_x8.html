<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding the JVM and Low Latency Applications | Coder Coacher - Coaching Coders</title><meta content="Understanding the JVM and Low Latency Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding the JVM and Low Latency Applications</b></h2><h5 class="post__date">2012-06-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CZytwF_y_x8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to this session on
understanding the java virtual machine
and low latency applications
my name is Simon Rita and I work for
Oracle as a technology evangelist
firstly I have to put in this slide
which is basically our safe harbor
statement when we look at applications
people always want fast applications but
what does fast really mean there are
different ways of interpreting what
people mean by the term fast firstly
there's I want the answer as fast as
possible and then secondly there's I
want all the answers as fast as possible
so in one we want a single answer as
quickly as possible and in the other we
want lots of answers as fast as possible
these are very different approaches to
the idea of what we mean by fast the two
goals themselves are somewhat orthogonal
or different in terms of a programming
perspective if you want to achieve one
answer as fast as possible
your approach is going to be different -
if you want all the answers as fast as
possible if you want just one answer as
fast as possible
then we're talking about a low latency
application we want the response time to
be as fast as possible
if we want all the answers as fast as
possible we're talking about high
throughput because the latency for a
single answer in amongst all of the
answers could be higher but as long as
all of them are processed in the
shortest possible time we're not
concerned about how quickly a single
result takes to be processed in the case
of wanting one answer fast then we're
talking about low latency and that's
really what we're going to be talking
about in this presentation is how to
reduce the amount of time that it takes
to get a single answer how can we make
things as low latency have this low
latency as possible one of the first
things we got to look at is the fact
that in Java applications we have a java
virtual machine
virtual machine will have a number of
performance considerations associated
with it probably the biggest is the fact
that it is by its very name a virtual
machine we're not dealing with a
physical machine in the sense that we're
interacting directly with the hardware
of the machine when you compile Java
application what you're generating is
byte codes which are for the virtual
instruction set of our virtual machine
they may match in some ways to the
native instructions that are used by the
platform that you're working with but
the chances are that they won't all map
one to one so there has to be some
conversion from the byte codes that are
generated by the Java compiler into the
necessary native instructions for the
particular processor the particular bus
architecture and so on that you're using
and similarly for the operating system
there will be a number of library calls
that you need to use and again the byte
codes need to be converted so that you
make the appropriate library calls for
the operating system that you're using
how this happens can be in one of two
main ways you can use fully interpreted
where every single byte code is
processed as it's used and is converted
to the necessary instructions every time
you do the same thing you have to
reprocess the byte code and make the
conversion so if you're in a loop every
time you go through the loop you have to
make those conversions every single time
the other approach is to use
just-in-time compilation
rather than compiling everything ahead
of time where we would compile all of
the code into native instructions and
then run it as a native application what
we do is we allow the applications start
running we interpret the byte codes as
the application starts working and then
we identify hotspots within your
application where you start running a
loop a number of times the virtual
machine can identify that and say the
instructions within that loop can be
compiled into native instructions such
that as you iterate through the loop a
number of times
rather than converting the Black Codes
into native instructions every single
time you get to the point where they're
compiled into native instructions and
then for subsequent iterations through a
loop they'll be executed as native
instructions giving you a much faster
processing time
the other big thing with Java is that
unlike a lot of native code it deals
with automatic memory management if you
want to create an object in Java you use
the new operator so you instantiate an
instance of a class and create an object
the java virtual machine is responsible
for allocating the space for that object
you don't have to do that you don't have
to call malloc you don't have to
allocate the space for your object so
Java will take care of that the java
virtual machine will do that for you the
other thing is that the java virtual
machine has a garbage collector and what
this does is eliminates the need for you
as a developer to programmatically free
up to the memory that you're using so
unlike other languages like C++ you know
explicitly have 2d allocate space
associated with objects garbage
collector will keep track of all the
references that you have within your
application when you stop referencing an
object and so you're no longer using
that object the garbage collector can
detect that during a garbage collection
cycle can then reclaim the space how
that happens is down to the
implementation of the virtual machine
what particular algorithm you are using
we talk a lot more about this as we go
through the other thing with the idea of
automatic memory management is that by
doing it this way we also don't have any
explicit pointer manipulation again
unlike C and C++ you can't actually talk
about a physical or even virtual memory
address and manipulate that you can't
store things in a physical memory
address or a virtual memory address and
this makes programming in Java a lot
safer because you don't have the issues
of incorrect pointer arithmetic freeing
up space that you don't own
ting areas of memory that you don't have
access to and so on Java right from the
very beginning has been designed to work
in a multi-threaded environment and we
provided the capabilities within the
libraries to enable you to do this so
you have the thread class you have the
runnable interface you have the ability
to instantiate a new thread that can
allow the java virtual machine to create
a new thread and have your application
have multiple threads all running at the
same time in order to allow you to
protect data so that you get consistency
of data even when multiple threads are
potentially trying to access the same
piece of data at the same time we have
the idea of locking each object has a
single monitor associated with it so
that when you use the synchronized
keyword for a method or a block of code
then you only you have the acquirer
monitor and you can lock that particular
object some of the more recent
inclusions in Java C 5 and Beyond have
provided higher level locking constructs
so that you have concepts like
semaphores you have things like
readwrite locks
mutexes and so on what this allows us to
do is to provide some programmatic
locking some automated unlocking so you
can provide locking on objects you can
provide locking on data structures and
so on and then depending on what
approach you use the unlocking can
either be automated so if you use a
synchronized block or synchronized
method then you automatically release a
lock when you exit that piece of code or
if you're using some of the libraries
then you explicitly have to unlock your
code as well all of these things have
performance considerations because they
introduced changes to how the system
works which can affect how your
application will run let's look at the
idea of an application stack because as
with many things in computer science we
can take a layered approach to how we
see an application running at the bottom
we have our physical Hardware the CPU
the memory that we have the bus
architecture
that's being used caching all of those
very low-level system specific Hardware
pieces that are in our applications
platform on top of that we have an
operating system and the operating
system is the software that enables us
to not have to worry as developers about
which physical memory address we need to
talk to in order to access a particular
device it enables us to be isolated from
all the complexities of how we
communicate with things like the network
card how we deal with the virtual memory
address translation hardware in the
system all of those things are isolated
from us and the operating system
provides a lot of other functions a lot
of higher-level things that are very
important from the point of view of
running applications you will have
potentially many applications running at
the same time on a single system and
it's the operating system that handles
the scheduling of those applications and
the threads within those applications
and by using different reg link
algorithms we can use different
approaches to how to make things as fair
as possible so that different threads
get access to the system they're not
blocked continuously we can implement
priority systems to give certain threads
higher priority and get more resources
than others and so on so the operating
system allows us to do all of those
types of things on top of that we have
our java virtual machine we've already
talked about some of the things that the
virtual machine does in terms of memory
management thread synchronization and so
on I'm really the key point about this
is that the virtual machine is isolating
us as the application code in the
developer from the operating system and
from the hardware so when you write your
Java code you don't need to worry about
whether you're implementing it on Linux
whether you're implementing it on
Windows or SPARC whether you're
implementing on Intel Hardware ARM based
Hardware whatever so this is where the
the famous write once run anywhere
approach has come from so you write your
application in Java and it's the java
virtual machine that handles converting
that into the necessary instructions
actions to run on the platform that
you're actually using on top of that if
you're using something like an
enterprise java application you may well
have an application server something
that provides containers the ability to
manage resources the ability to provide
connections to databases to transaction
monitors to security systems all of
those types of things that are required
for enterprise class applications this
is optional obviously because many Java
applications don't require an
application and server and then finally
at the top we have our java application
code the code that we as developers
write what we can pile into our Java
classes create our jar files if we're
developing enterprise applications then
we'll be creating war files or ear files
but it's what we're actually developing
in terms of the functionality of our
application so the important thing about
this stack is how we can look at it in
terms of the effect that we can have by
tuning different parts in changing the
way that our application will run and
effectively what we can say is that from
the bottom upwards we'll have more
impact by making changes the further up
the stack we go if you think about it
it's very difficult to make changes or
have an impact in terms of hardware yes
you can add more memory yes you can add
CPUs but beyond that you can't really
change anything in terms of the way the
hardware actually works operating
systems have the capability to change
certain parameters about how they work
but mostly from an operating system
perspective there are very few things
that you would need to change and if you
do change them it won't have a
significant impact on how your
application actually runs within the
Java Virtual Machine there are many many
options that you can use to change the
way the garbage collector works the
algorithms the way that the just-in-time
compiler works threading all those types
of things there are lots of things that
we can do in terms of tuning performance
within the java virtual machine
similarly for the application server
there are various things that can be
changed to tune the way that connections
are handled pooling all
things like that and in our java
application clearly we can change the
way that our application works we can
take a different approach to algorithms
to make them more efficient or do things
in different ways and that will have the
biggest impact on our application works
from the point of view of our
application we can have the biggest
impact in terms of application code
beyond that we're looking at tuning the
java virtual machine we don't really
need to worry too much about operating
system and hardware so we want to look
at application code and we want to look
at the virtual machine in the rest of
this presentation I'm not going to talk
about application service specifically
we're going to be talking about how the
java virtual machine works rather than
enterprise class applications
specifically so first of all let's talk
about memory management and how this
works and the impact it can have on
latency for our applications what we
need to do is look at the the layout of
the heap space that is being used by the
java virtual machine now what we're
seeing here is the layout for the heap
in the hotspot virtual machine there are
other implementations of the virtual
machine which may use slightly different
approaches but from the point of view of
the Oracle hotspot implementation this
is what we use at the moment so you can
think of the heap being broken up into
certain regions and we use a
generational approach to the heap space
so where objects reside is based on how
long they've actually been in existence
at the bottom we have this purple area
called the purple permanent regeneration
and effectively you can ignore this this
is used for virtual machine class
metadata objects things which you don't
access directly from within your
application any way you would access
things in the permanent generation is if
you're using reflection and you need
access to cloth or class objects or
method objects things like that
and in fact for JDK 8 the plan is to
remove this generation completely
so what we really need to focus on is
the heap space as would be used by your
application directly and this is broken
up into two regions you have the young
generation which is the light of blue
section at the top and the old
generation the darker blue section at
the bottom within the young generation
we actually broke this up into three
regions which are effectively two
regions because you have the Eden space
and then you have two survivor spaces
what this allows us to do is to change
how we allocate space for objects and
also how we reclaim space from objects
that are no longer used so if we look at
what actually happens when you allocate
an object this will most of the time in
fact almost all of the time will be
allocated in the Eden space and so you
will allocate space by the virtual
machine for your object and then you can
write whatever values into your object
and they will be stored in Eden space
when Eden's face fills up because you've
created a number of new objects and more
space is required we do a minor garbage
collection minor garbage collection
requires moving or copying of all light
data that is still in the Eden space
into one of the survivor spaces all live
data that is in one of the survivor
spaces it's also copied to the other
survivor space unless it has been in the
survivor spaces for a predetermined
length of time when objects become old
enough they are promoted into the old
generation so they then become part of
the old generation and space is
allocated for them there the important
thing about this is that by using this
approach we can take the different
approach to the algorithms who use for
allocation we can take the different
approach for how we do collection and so
on especially in the old generation but
really the key point here is that it
requires us to move or copy objects from
one area to another with
in the heap space so during garbage
collection cycles objects are being
copied and effectively moved between
different areas of the heap space the
impact on us as an application is that
we can't from the JVMs point of view
allow an application to continue running
whilst we're doing this copying of
objects from one place to the other
because if we have an object in even
space and we're in the process of moving
it to a survivor space if application
code starts changing values there how do
we know whether we've copied the right
values into the survivor space or are we
making changes it's a survivor space as
we're copying the object it becomes very
complex so what we do to avoid that
problem is to introduce a pause on all
the or called mutator threats or the
live threads that you have in your
application are paused during a minor
garbage collection cycle this is where
we get the problems of latency because
the way that that happens is
non-deterministic we can't predict when
a garbage collection cycle is going to
start because we don't know how quickly
you're going to allocate objects in your
application so we don't know what point
that will will kick in this non
deterministic nature is what affects
latency because we can tune things in
terms of how our code works and we can
know specifically that it takes a
certain amount of time for us to execute
a certain number of instructions and we
can say that for a particular task it
has a certain number of instructions so
it should take a certain amount of time
to execute but if garbage collection
happens during the execution of those
instructions we introduce a pause which
could be variable in length and that
variation in length is what introduces
non-deterministic behavior in our low
latency applications then can make
latency longer than we would expect if
there was no garbage collection involved
and no pauses so this is one of the
things that we need to understand and
one of the things that we need to then
approach in terms of how to tune both
our virtual machine and our application
code
to minimize the impact of that so
important concepts of garbage collection
first thing we need to understand is
that the frequency of the minor garbage
collections which is where we need to
move objects from the Eden space into
Survivor spaces and from Survivor spaces
into the old generation is dependent on
the rate of object allocation faster you
allocate objects the quicker you're
going to fill up the Eden space and
therefore the more quickly you will need
to have a minor garbage collection makes
sense similarly it's dictated by the
size of the Eden space biggie you make
you eat in space the longer you'll be
able to allocate objects for before you
have to mine a garbage collection cycle
if you make even space smaller you'll
take less time to fill it up and
therefore how frequently minor garbage
collections occur is dictated by the
size of the Eden space as well as the
rate of object allocation how frequently
we promote objects from the young
generation into the old generation is
dictated by several things firstly it's
dictated by how frequently we run a
minor garbage collection cycle and this
is effectively how quickly we aged
things because we copy objects from one
survivor space to another we do that a
certain number of times the more quickly
that we have minor garbage collection
cycles happening the more quickly we'll
get through that number of copies
between the survivor spaces and
therefore the more quickly we will start
promoting objects into the old
generation similarly the size of the
survivor spaces will dictate to some
extent how quickly we aged objects and
the tenuring threshold which is the
number of times that we will copy
objects back and forth will dictate how
quickly objects get promoted into the
old generation by default that's 7 so an
object will be copied seven times
between the survivor spaces before it
gets promoted the idea behind that is
that it gives objects a chance to become
garbage before they get promoted into
the old generation we'll come back to
why that's important in a moment
so ideally as little data as possible
should be promoted into the old
generation what you really want is to
keep everything in the young generation
and promote as little as possible into
the old generation anything involves
copying either from Eden space into the
survivor spaces or from the survivor
spaces into the old generation requires
input and output and must by virtue of
the fact that we have to protect data be
stopped the world so it involves pauses
introduces non-deterministic behavior
affects the prediction of latency in our
application code other important
concepts of garbage collection is that
the object retention impacts latency
more than object allocation so garbage
collection only deals with live objects
if we don't have live objects in our
young in the Eden space we don't have to
do anything with them we can effectively
ignore them so we only deal with live
objects
similarly when we're collecting in the
old generation we only need to look at
the live data that we have so however
many garbage objects you have makes no
impact on how long it takes to actually
analyze how much garbage you have you're
simply looking at the live data that's
involved so garbage collection is a
function of the time of it is a function
of the number of live objects that you
have how many live objects you have to
look at in order to detect all of the
data in the application as it's running
at that point and also the complexity of
that graph what you will have in your
application is references directly using
the variables in your code to objects
those objects themselves will then have
references to other objects the graph is
the relationship between the variables
that you have in your application
through the different objects through to
the other objects and all the the
continuous collection of objects that
are live data how complex that graph is
in terms of the relationships between
those objects
whether they're cyclic references
whether they're complex cite cyclic
references and so on can have an impact
on how long it takes to run garbage
collection and therefore will affect
latency now one of the things that we
have to understand is that in terms of
object allocation object allocation is
very very cheap for the hotspot virtual
machine we're literally talking of the
order of 10 cycles in common case which
is most of the time to allocate an
object the reason for this is that we
treat even space as a stack and we have
a stack pointer and when we want to
allocate space for an object all we have
to do is say put the object where the
stack pointer is at the moment
increment is that point to by the size
of the object and we're ready to start
allocating again that's very very quick
and very very cheap even in the case of
multiple threads what we do is we use
thread local allocation blocks so each
thread can have its own allocation
pointer and we don't have contention
over different threads trying to access
the same pointer to allocate objects so
we're talking about literally 10 cycles
for common cases and if you compare that
to the way that malloc works you're
talking about roughly 30 cycles even for
the fastest mammal algorithm so it was a
three to one ahead just in terms of our
object allocation with short-lived
objects reclaiming them is very cheap
and this comes down to the fact that we
simply ignore them when we're doing a
minor garbage collection so all we do is
we look at the eaten space we look for
the live data in there and we copy those
objects to a survivor space so anything
that's garbage we simply ignore we don't
have to do any processing on it at all
when we copied the live objects we reset
the pointer to the beginning of eden'
space and we've effectively collected
all of those objects without doing
anything to them there is this thing
called the week generational hypothesis
what that says is that for typical
applications most objects are very very
short-lived literally of the order of a
few million instructions before objects
become garbage
literally another megabyte is allocated
and your objects become garbage
so most objects aged 98% of objects
become garbage very very quickly and so
if they die within the Eden space it's
very cheap to reclaim them so what does
this mean in terms of rules of thumb for
approaches to low latency well the first
thing is don't be afraid to allocate
objects if you're going to be disposing
of them quickly if you're doing
intermediate results don't worry about
allocating objects and using them if you
know that you're only going to retain
them for a very short space of time
you've got a loop and the object you're
allocating is scoped within that loop
you know that it's only going to exist
for a short space of time so the garbage
collector can reclaim that space without
any significant overhead because we know
that allocating the space where that
object is very easy again we know that
it doesn't impact greatly on performance
of the application the other thing is
that the garbage collector likes small
immutable objects and short-lived
objects so anything that's immutable
can't change it like strings is also
very good as long as they don't survive
a minor garbage collection yeah that's
that's all good because you know we can
use these things and we can take
advantage of that in terms of the
performance of our low latency
applications try to avoid complex into
object relationships now this is this is
a rule of thumb and it's one of those
things that's easy to say but is it
really easy to implement the thing is
that because garbage collection has to
analyze the full graph of live objects
and the references between them the more
as I said the more complex the graph of
relationships the more time it will take
to do things so it's something really
just to think about and bear in mind
when you design your application is how
complex you make the relationships
between the different objects in your
application code if you can simplify it
rather than using very complex set of
relationships then that will help you in
terms of latency
however on the flip side of that let's
not go completely overboard
so don't allocate objects needlessly yes
it's cheap yes it's easy to reclaim them
but the more frequently we allocate
objects the more frequently we're going
to do garbage collection simply by
virtue of the fact that's the way it
works similarly the more frequently we
get to do garbage collection the more
frequently we're going to be copying
objects between the survivor spaces and
so the effective change is that we will
be aging objects faster
similarly faster object aging means
faster promotion with the older
generation and what we don't want is to
really get a lot of data into the old
generation so more objects in old
generation means that you're going to
have more frequent collections more
compacting collections or concurrent
collections depending on how your old
generation algorithms are configured so
ultimately it's better to use
short-lived
in mutual objects than long-lived
mutable objects again this is just sort
of rule of thumb so it's not something
to sort of think of right I must never
use a long-lived mutable object that's
clearly not going to work and trying to
use only short-lived immutable objects
is not going to work either
but it's something just to bear in mind
when you're designing your code and
you're thinking about how to approach
different parts your application code so
what we end up with is the ideal garbage
collection scenario what will be the
best possible situation that we could
have in terms of garbage collection and
what we're really looking for is an
initialization phase where the
application starts doing the work that
he needs to do and builds up the set of
data that it's going to work on and what
we don't want to have happen is for that
set of data to really change a lot so we
want that data to be promoted into the
old generation and then that be a fixed
size set of data that stays there and
doesn't really grow very much and we can
use lots of objects in terms of doing
the work around that data so long as
they never get promoted beyond the young
generation so what we're ideally looking
for
there's no garbage collection on the old
generation and only collection in the
young generation because minor garbage
collection pauses are on the whole the
fastest giant garbage collection that
you'll get in terms of how we can
approach that what we really want to do
is start with parallel garbage
collection so I've used the plus use
parallel old GC or plus use parallel GC
options and what this will do is it will
enable us to do the fastest possible
minor garbage collection that we can
clearly that's limited by the fact that
you would have to have multiple cores
and multiple CPUs because if you've only
got one core one CPU trying to do things
in parallel isn't going to help you only
works to your advantage if you've
actually got the capability to do things
truly in parallel in terms of doing that
what we've done is to really try and
improve the efficiency of the algorithms
that we use we use techniques like
work-stealing we have made sure that the
way that the work gets divided up
doesn't have contention for data
structures so there's no locking
involved and those types of things so we
can make the best possible use of
multiple processors when it comes to
doing minor garbage collection cycles if
you are experiencing old generation
collection which is not unheard of
then for low latency applications what
you want to be doing is using the
concurrent mark-sweep collector the idea
here is that what you do is you actually
do some of the garbage collection in
parallel with the application code so
you're not creating very long pauses in
order to do the old generation
collection now that has a knock-on
effect because the way that the
concurrent mark-sweep collector works is
it doesn't compact the data in the old
generation because it doesn't do that
you end up with the need for free lists
in order to determine where the spaces
are that you can put objects that are
being promoted from there young
generation into the old generation the
effect of that is that some of the minor
garbage collection times can be a little
bit slower because now you're having to
manipulate the free list you haven't
searched for an appropriate size block
take that block off
list use it and so on however because we
can hopefully avoid having to do a full
compacting collection on the old
generation the downside of having
slightly longer minor garbage collection
pauses is very much offset by the fact
that you're not having significant
pauses for old generation collection one
interesting aside on the concept of
concurrent garbage collection is that
concurrent collectors require a right
barrier in order to track potentially
hidden live objects this is because with
the fact that you're having code
executing at the same time as the
collector we need to track changes which
are being made to objects as the
collector is running when we get to the
point where we need to actually complete
the garbage collection we have what's
called a remarketing phase and that will
then take into account any changes that
have been made whilst we've been doing
the concurrent work but from the point
of view of tracking changes as we're
doing the concurrent garbage collection
we need a lock so that we can track
changes in terms of hidden life objects
anything that involves a right barrier
is going to introduce a performance
overhead because you've got the concept
of locking involved you need to acquire
a lock you need to release the lock and
so on
obviously the overhead involved in that
depends on the implementation if you
have a very efficient implementation of
the locking mechanism then you can have
very low overhead things where the
system is able to track the fact that
the same thread is acquiring and
releasing the lock on a particular
object then we can do things in terms of
changing that into a no op and that
becomes a very efficient way of doing
things so the impact of using concurrent
mark-sweep and the right barrier does
depend a lot on the implementation if we
use a stop the world garbage collector
you don't require a right barrier
because you're pausing on the
application threads you're pausing the
mutator threads and so there's no
problem with objects being modified as
the application is actually running and
as the garbage collection is running so
for an ideal situation
we used parallel burr
collector or parallel or garbage
collector avoid old generation
collections amass avoiding full garbage
collection
let's talk a bit about how we can be
friendly in terms of our programming
towards the garbage collector
we know the garbage collectors there how
can we actually help it to do its job by
minimizing the impact of some of the
things that we do in terms of our
programming the first thing to consider
is the concept of large objects now
large objects are used very logically in
a lot of applications but what you need
to do is consider how these things
actually work internally large objects
can be expensive to allocate if they're
very large it may not be possible to
allocate them in Eden space may not be
possible to allocate them in a survivor
space so for very large objects they get
allocated straight away into the old
generation and again if we're using
concurrent mark-sweep we have free lists
we need to allocate and find a block of
the right size and so on so we're not
using the fast path for allocating
objects we're not getting the advantage
of there's 10 cycles 10 instructions to
allocate an object the other thing is
that for large objects and can be
expensive to initialize the java
specification requires that all of the
bytes in a particular object is zeroed
when it's created so you've got a very
large object there's an overhead in
terms of actually going through and
making sure that every bite is 0 another
thing is that the use of large objects
can cause fragmentation in our heap once
they get into the old generation because
we're having to fit them into gaps
between my data as we allocate space for
them the heap can become fragmented
where we can have objects dotted around
the heap space not contiguous in memory
the gaps between the large objects and
other live data can become small enough
that we can't fit another object into
that and so we lose the ability to use
that space efficiently so there's a
number of impacts that using large
objects and a large number of large
objects can be can actually have on your
application in terms of its performance
with the JVM so rule of thumb really
here is to say
avoid large object allocations if you
can don't use this as a hard-and-fast
rule if the algorithm that you're trying
to use and the problem you're trying to
solve requires large objects don't think
I can't use them in Java
just think about whether it's possible
to use a smaller number as possible
whether there's a different data
structure you can use which maybe
doesn't have so many large objects and
so on certainly it's not so bad during
application warm-up if you want to
allocate large objects that are going to
be used for the length of the
application that's not such a bad thing
it's when the application to reach its
steady state but you don't want to be
allocating large objects so here the
concept pooling can become important you
create a pool of large objects that you
want to use during the applications run
and those can be allocated at startup
time and that way you have a steady
state of data as the application has run
is running beyond its startup time next
thing in terms of garbage collection
friendly program is thinking about data
structures and resizing there are a
number of collection classes in Java
which are backed by some form of data
structure something like an ArrayList
something like a vector which has
storage underneath it when you create
one of these collection objects if you
don't pass a parameter to the
constructor a default size will be
created for you and what happens is if
you start adding objects to your
ArrayList and objects your vector and
you need more space than it was
initially allocated the storage
underneath needs to be resized typically
will double in size and more space will
be allocated for you now that's
invisible to you as a programmer but can
have a significant impact on the way
that the system works because it's
having to happen invisibly to you the
programmer so if you know that you're
going to be using a large number of a
large number of spaces in a collection
which is such as an ArrayList and so on
then it may well be better to size it
accurately when you create it use the
size parameter in the constructor and
set it to be
good size so that it will hold all of
the objects that you need again you need
to be careful with this because
allocating lots and lots of objects
where you're not necessarily going to
use them all all the time can be
detrimental because you're suddenly
allocating space for a large number of
objects and you're possibly not using
all of that space until you're wasting
space in terms of memory resources so as
I say resizing can lead to unnecessary
object allocation also contributes to
fragmentation this is a problem if we're
using a non compacting garbage collector
such as concurrent mark-sweep object
pooling is a very good technique it
works very well for certain types of
objects if you're going to use things
like threads if you're going to use
things like car and JDBC connections
then pooling of those objects is very
sensible because if you can't use them
frequently what you don't want to be
doing is constantly creating those
objects and then letting the garbage
collect tidy up after you so it's much
better to use object pooling but there
are some things that you need to be
aware of there which is that if you have
a large number of objects in your pool
and you're not necessarily using them
all you're adding to the complexity of
the graph of live data you're adding to
the number of objects that the garbage
collector needs to visit every time it
runs through its garbage collection
cycle access to the pool requires
locking so if you're going to be using
lots of threads all trying to access the
same pool then you're going to be having
lots of contention there may be blocking
involved and you might have a degrading
performance in respect of that
scalability in terms of number of
threads becomes a significant issue if
you're not using large number of threads
then it's not so much an issue and if
you can manipulate those threads in in a
way that means that they're not in
contention for the the collection pool
at any particular time then you can
minimize the impact of that so all you
got to do is weigh up the the benefits
of large objects allocation at startup
versus the downside in terms of
scalability for multiple threads and the
contribution to the complexity of the
scene graph so it's really a balancing
act
third thing in terms of garbage
collection friendly programming is about
finalizes and finalizes the simple rule
is don't use them that's not a I mean
it's it's a rule which is a good rule to
go by but obviously there are situations
where you might think well I really do
need to finalize it for this well yes
there may be some very rare occasions
where finalizes are actually valid and
really do make sense and you can't do
anything other than use a finalizer
but really really do think very hard
before you use finalize them because
finalizes have a really significant
impact on performance if you use a
finalizar
you have to go through at least two
garbage collection cycles in order for
the object that has the finalizar
to be collected because the object has
to be placed on the finalizar queue and
the final izq will not necessarily run
every time a garbage collection cycle
runs it also slows down the garbage
collection cycles because we now do have
objects on the finalizar queue and there
needs to be processing of that and so on
so if you can avoid using a finalizar
in terms of having that happen
automatically it's much much better to
use an explicit method that you create
in your code and which you explicitly
call when you no longer want that object
to be used so before you remove your
last reference to that object you make
sure that you manually call the method
which cleans up the resources that way
you're not then forcing a finalizer on
the garbage collector it's treated just
as if it was a normal object and so
there's no impact from the concept of
having multiple garbage collection
cycles involved and the finalizer queue
involved reference objects where you're
dealing with soft or phantom references
and those types of things are a possible
alternative to finalizes but Alma has an
almost last resort an important note
about soft references as well is that
the referent is cleared by the garbage
collector and so this is
how the garbage collector treats the the
references that you're declaring is soft
how aggressive it is in terms of
reclaiming those objects is down to the
garbage collectors implementation so
you're really placing your trust in the
way that the garbage collector works you
don't have strict control over how your
soft reference is actually get used and
how they will get garbage collected so
the the aggressiveness of the garbage
collector will dictate the level of
object retention and there's no easy way
of doing that within the the virtual
machine so certainly these are things to
bear in mind very carefully when you're
interested in very low latency because
these are things which again introduce
this concept of non-determinism because
you can't predict what's going to happen
when the soft reference if you know in
great detail how the virtual machine
works and how the implementation of the
garbage collector is going to handle
these references it might be possible to
kind try and model that but on the whole
you're really introducing
non-determinism
into your application through that there
are other things around subtle object
retention so going back to finalizes
let's look at this class here I have a
class which implements something and
then extends a class which has a
finalizar so because we extend it we get
the finalizer as part of this class even
though we haven't declared it within the
class we actually are subject to having
a finalized of run on these objects if I
then declare a variable within this
class which is a large buffer 2 megabyte
buffer what's going to happen is that
that buffer is also going to be
subjected to the garbage collection
impact of the finalizer
in the superclass we can't avoid that
because the superclass has the final
answer and therefore it must be called
we hit this minimum of 2 garbage
collection cycles to free up that 2
megabyte array in non determinism and so
on so what we could do is we could break
some of the the pure object-oriented
approach and say okay
rather than sub classing the class with
finalizer
will encapsulate it
a reference we can use that within our
particular class we can have our buffer
within that class and now because we're
not sub-classing the class which has the
finalizar when our class my
implementation is ready to be garbage
collected only the reference to the
class with the finalizar will be placed
on the finalizar queue the buffer that
we have of two megabytes will still be
able to be garbage collected straight
away because it's not being affected
directly by the class with the finalizar
a couple of other things about object
retention this is a fairly minor point
inner classes have an implicit reference
to the outer class or outer instance so
this has an impact in terms of the
object retention to grow more
specifically the graph complexity
because now we've got extra references
the garbage collector needs to follow
all of the references from different
objects within the heap that are still
valid
so inner classes will have reference the
outer class the garbage collect has to
follow that reference to ensure that the
class is still ref point the reference
to that class is still recorded so
effectively what this can do is lead to
a potential for increased garbage
collection so increased latency this is
one of the nice things with Java SE 8
with the future of Java C 8 a lot of
inner classes will be replaced by lambda
expressions and because lambda
expressions not going to be implemented
simply as a syntactic sugar on the
compiler to simply implement those as
inner classes we'll be using the
invokedynamic bytecode instruction in
the JVM and so we can avoid a lot of
this problem and we'll reduce some of
the overhead of using what is in effect
and in the class within the application
so let's talk about another approach to
garbage collection this is one that was
introduced in the hotspot virtual
machine a while ago and what we saw was
that there were some limitations in
current garbage collection algorithms so
concurrent mark-sweep is very good in
the sense that can run concurrently with
the application threats
in a large part of what it's doing but
it has some issues in that it doesn't do
any compaction so there's no compaction
data which means we have to use free
lists we do have fragmentation of the
heap we also have need for a remarketing
phase which introduces another pause
which is a stop the world pause and so
we can affect the determinism and we can
affect the latency of our application if
we use parallel old we can do full heap
compaction but the downside of that is
potentially very long stop the world
pauses certainly for low latency
application most full heap compaction
it's going to be way way longer than any
latency that we're looking at in terms
of our goals now we can potentially set
a pause target for the concurrent
mark-sweep collector but it really is a
best effort there's no guarantee and
there's nothing that the collector can
do internally that can really affect it
beyond sizing of the the heap space and
the different generations there's
nothing you can do as it's running to
try and reduce the time that's required
for particular parts of the garbage
collection so as we increase the size
and heap as we increase the throughput
as you increase typeset data then these
problems will start showing themselves a
number of people within son of the time
and subsequently Oracle came up with the
idea for the garbage first collector
which is a different approach to how we
do collection of the old generation and
in fact young generation as well so I'll
talk through how that actually works so
from a kind of high level perspective
it's a replacement for the concurrent
mark-sweep collector it's available as a
fully supported useable collector from
the Java Runtime version 7 update 4
onwards prior to that it was part of the
hotspot virtual machine but we were
using it as an experimental option now
we actually support it as a runtime
option and it can be used in production
systems it's very much a service style
garbage collector so it is aimed at
server long-running applique
it works in parallel using multiple
processors it runs concurrently with
application threads it is using a
generational approach in the same way
that we've seen with the hotspot heat
layout so we have the concept of young
generation we have the concept of an old
generation it is designed for good
throughput so we're not minimizing
throughput or not changing throughput
expense of latency however the
differences are that this is actually
compacting or strictly speaking I
suppose you could say semi compacting it
has improved ease of use so we're not
looking at as many variables tune in
terms of changing what can actually be
done and it has more predictability
around the pauses that you have with the
collector although it's not hard real
time so even though we can give better
predictability we can give better
approaches to the targets that we're
setting we're not dealing with hard
real-time where we guarantee that
certain things will happen at specific
times from a high-level overview of g1
it is a region based heap so we still
have the concept of the young generation
we still have the concept of the old
generation the sizing of the young
generation is dynamic so it will
actually change as the application runs
and then we do partial compaction of the
heap space using what's called the
evacuation and I'll explain that as we
go through the way that it works
internally is it uses a thing called
snapshot at the beginning and what this
does is it voids the idea of the
remarking phase that we would have in
the traditional concurrent mark-sweep
collector again how this works will
become apparent when I talk through the
the diagrams that I'm going to show you
in a moment in terms of having a target
for Ana latency and the maximum pause
that we want to live with what we can do
there is we can actually tune things as
the system is running so we can look at
the the number of regions in the heap
space that we're working with and we can
decide how many of those in order to
collect in order to hit the target that
we're aiming
so rather than just being able to change
the sizes of the parts of the heap we
can also adjust how much of the heap in
effect we're collecting as the system
runs and as the garbage collector runs
and that way we can adapt to changes so
that we can more precisely hit the goal
but we're working for so the idea of
poignant garbage first is that we always
look for areas of the heap space that
have more garbage than anywhere else so
by picking the areas that have more
garbage we can be more efficient in
terms of the amount of space that we can
actually reclaim so that's that's all
very good and we can also use things
like minimal work for maximum returns in
the amount of work we do which again
will become apparent when I talk through
the diagrams so for the next few
diagrams what you need to understand is
that the the color scheme represents
certain things so here's the color
scheme that we're going to use gray
represents non allocated space green is
the young generation blue is the old
generation like green and like blue
represent the old generation young
generation and then dark green and dark
blue represent recently copied objects
in those generations as the diagrams
shown it'll become very easy to see how
this works
so let's look at the young generation
collection minor garbage collection that
happens using a concurrent mark-sweep
collector so you can see that at the top
of the diagram here we have the green
areas which are the eden space and one
of the survivor spaces the other
survivor space is empty there's no
allocated objects there the old
generation has blue areas in it which is
the live data and that's dispersed
through the heap because we don't
compact what we need to do now is
because we filled up the Eden space we
need to run minor garbage collection so
we need to do some collection we need to
move some objects around what happens is
that the end of the collection we've
moved any of the live data from the Eden
space and any of the objects that's
to be retained in the survivor spaces
into the second survivor space any
objects that needed to be promoted have
been moved into the gaps that we had in
the old generation so we can see now
that we've got some dark blue objects
which represent the objects that were
just promoted from the young generation
into the old generation if we look at
this from the perspective of garbage
first what's going to happen here is
that rather than dealing with physically
contiguous areas where we can say well
this is the young generation that's an
area of memory that we have this is the
old generation that's an area of memory
we have what we deal with here is
logically regions so we say okay we'll
break up all of our heap space into
regions and those regions can either be
young generation or old generation and
they can be eaten space or survivor
spaces but their one megabyte in size
which seems quite small but there is
logic behind that and we'll explain that
as we go through so we split the whole
of our heap space up into regions and
then we allocate those regions to be
either part of the young generation or
part of the old generation and so we
deal with a logically continuous area of
memory rather than a physically
contiguous area of memory so here we can
see we've got some green areas which
represent the regions being used for the
young generation we've got some blue
areas that represent the regions that
are being used for the old generation we
need to run garbage collection because
the effect is that we've reached the end
of our Eden space so we need a mining
garbage collection what happens during
the mind of garbage collection is that
we go through and any objects which are
still live data in the in space and any
objects that are still that need to be
any objects at the live data in the Eden
space need to be moved into one survivor
spaces and the objects that need to be
promoted from the survivor space in the
old generation can simply use another
region within our heap space and we know
which ones are empty we can simply take
one of those empty regions and start
using that to promote objects in - it
may have been used for the young
generation before now
I'm saying this is now part of the old
generation and so we can add objects to
that
so we're evacuating objects from the
young region into the old generation the
net effect is that we have the same
thing happening so now we've got eaten
space empty so all of the regions that
were being used for eaten space now are
empty our survivor space has switched to
the other survivor space in the same way
we saw in the earlier diagram and that's
got some live data in it and the objects
that have been promoted have been
promoted into one or more regions which
are now being used for the old
generation now in terms of the the
sweeping and the marking thing what
happens here is that if we need to do
some concurrent work in terms of running
the garbage collector on the old
generation concurrent collection has a
number of phases to it so what we have
to do is we have to do an initial
marking phase where we look at all the
live objects that we directly reference
from our application we then have to go
through and follow all of those
references to map out the graph of live
data and we can do that concurrently so
we have an initial marking phase where
we pause the application track all of
the application direct connections to
objects then we allow the application
continue we do the concurrent marking
when we finish that we then have to go
back and remark because there will be
some changes that have happened during
the concurrent phase and then we can go
to and go through and do the sweeping of
the objects that are no longer required
so net effect is that now we end up with
number of areas being added to our free
lists and we clear out the data that's
no longer required from our old
generation from the point of view of the
garbage first collector things are a
little bit different and they might life
a whole lot easier
because what we do in the evacuation
pause so our minor garbage collection is
we don't just go through and copy
objects from the young generation the
old generation we also look at the look
at the regions that we have for the old
generation and we look for
or something we do some initial marking
on that so that we can build up a list
of regions that still have live data in
them and then we can sort that based on
how much live data they have and the
idea here is that when you do the
concurrent collection where you're
looking through say okay we need to
collect data or you need to collect
garbage from our old generation what's
going to happen is that by using small
regions one megabyte lots of those
regions will be completely empty because
the data in them will no longer be valid
and so we can instantly reclaim those
spaces we don't have to do anything we
don't have to move any objects around we
can simply add all of the regions that
no longer have any live data in them to
our list of free regions that we can use
either for young generation or the old
generation the other thing we can do is
where there is still live data we can
sort those based on the percentage of
live data that's still there that way
when it comes to collecting we know that
where we do have to convert and we do
have to copy objects from one region to
another we can pick on the ones that
have the smallest amount of live data so
the amount of data we have a copy from
those regions is as small as possible
and then if we pick on let's say ten
each of which that have 100k of live
data we can copy the hundred K of live
data from each of those ten regions into
one region which is one megabyte in size
and have that region completely full so
we've effectively compacted the data
into one region we then free up ten
regions which now don't have any data in
it and that goes to our list as well so
at the end of the remarking phase we
simply say okay now we've collected all
of our regions that don't have any data
in and we've actually then compacted so
pick on the live ones the ones with
lower amounts of data and we end up with
ineffective compaction into a smaller
amount of space so we look at the
comparison between concurrent mark-sweep
and the garbage first collector the
layout of the heap is logically the same
so we still still dealing with young
generation even space survivor spaces an
old generation
but from the point of view of the layout
of the heap is very different
by using regions in g1 where we're not
dealing with contiguous memory we can
then choose where to put data and do
that dynamically as the system is
running we can also pick on regions that
have small amounts of live data to
maximize the efficiency of the garbage
collection as we're going through now
latency is a key goal for Oracle and so
our engineers are very much looking at
this in terms of okay we've got g1
that's one approach but what other
things can we do in terms of improving
the response time reducing the latency
associated with a virtual machine a
number of areas of research that are
going on at the moment we're looking at
lots of different ways of doing this and
at the moment we're looking at all these
different ways with the idea that we'll
pick the most efficient efficient ones
and we'll we'll implement those at some
point in the future so we're looking for
requirements we're looking for what
customers are actually interested in
from the latency perspective so having
spent a lot of time talking about
virtual memory and memory management
within the virtual machine let's talk a
little bit about adaptive compilation
and some of the impact that we can have
in terms of latency around using the
adaptive compilation now basically what
we're dealing with here is the concept
of a just-in-time compiler where pieces
of code are being compiled as they're
required by the virtual machine so we
have a set of byte codes that need to be
interpreted and at some point the
virtual machine is deciding that a
particular region of code is being used
sufficiently frequently that it makes
sense to compile those byte codes into
native instructions so that we don't
have to continually convert those byte
codes into native instructions every
time we use them this would be the
typical situation of using a loop many
times now the thing is that with what
the virtual machine can do is make
decisions about optimizations based on
what is happening in your attic
so the only data that the virtual
machine has available to it is what is
actually happening from when the
application started running to this
point in time so it knows exactly what
classes have been loaded and it knows
what code paths have been executed so
far but it does not know about all the
code in the application because it
doesn't know about any classes that
haven't been loaded it doesn't know
about all the possible code paths that
could be used by this application
because many of those haven't actually
been executed at this point so unlike a
traditional compiler which will look at
the whole application every single piece
of code that's possibly used by that
application the JIT compiler only looks
at the code that has been run so far the
optimisation decisions of the JIT
compiler have to be based on the runtime
history so it can only use what has
happened in the past to decide what to
compile has no ability to predict the
future so it doesn't know what's going
to happen moving forward what that means
is it can make some decisions which are
not going to be great because if you've
been running a particular link a number
of times and it gets the point where the
JIT compiler is up that loop has been
run enough times it makes sense to
compile that's that set of byte codes if
you then immediately stop using that
loop you get to the end of the loop then
the compiler has compiled a set of byte
codes for no benefit so that kind of
limits some of the optimizations that
can be used as the profile changes then
the JIT needs to react your application
may change the particular paths that are
being used different clients may use
different options within the application
different things may need to be
processed you go off into different
parts of the application so JIT compiler
needs to then say okay I don't need this
particular bit of compiled code anymore
I hasn't been used for a long time so
we'll effectively drop it out of the
cache and weari optimize based on the
new profile of where the application
code is going now from an internal
profiling point of view what the system
needs to do is come to term which
methods are hot on which methods are
cold so which ones are being used Lots
which
one's not being used and there's
different ways of doing that you can
either do it in terms of looking at
everything that happens where we're
looking at invocation counting so what
that will do is it will simply say every
time a method is called then we record
that as a count so we know exactly which
methods are going to be the ones that
are being used most and ones that being
used least we can do that by adding an
add instruction into the native code and
we can simply say just increment a
counter every time we call that the
other way to do it is to do it on a
periodic basis where we do thread
sampling so we say okay every 100
milliseconds or every 50 milliseconds we
look at all the threads that are
executing and we say which piece of code
is each thread executing at that point
and we record that detail so we're doing
more of a statistical sampling rather
than a full set of information by doing
that we can minimize the impact that it
has on the actual code because we're not
looking at every single method call
we're doing a periodic check and so it's
minimizing the impact that we have the
third approach is to use Hardware based
sampling and a lot of platforms nowadays
have program counters and different
statistical counters that can be used by
the hardware and that again will
minimize the impact because you're using
them as at the very lowest level and so
we can use instrumentation mechanisms in
order to do that so there are some
assumptions that the JIT compiler will
make the first is that methods will
probably not be overwritten now that's
not to mean that you never override
methods but on the whole you can make an
assumption which says methods will not
be overridden and by doing that you can
improve efficiency yes many methods will
be overridden
but that's in effect it's an exceptional
circumstance rather than the commons
case so by doing that we can improve
efficiency the other thing is floating
point numbers so we actually make the
assumption that a floating point will
never be not a number so an
unrepresentable number and to do that or
to process floating point numbers we
actually use hardware instructions
rather than the floating point library
so where we can use the floating
we processor within the hardware we can
do things more efficiently rather than
doing in software so we make that
assumption and less a floating-point
value is set to not a number exceptions
are the exception so what we say is that
for a try-catch block every catch block
is very unlikely to happen because we
hope that exceptions don't occur so what
we do is we always mark catch blocks of
being cold we may well compile the code
in a try block but we will always say of
the code in the catch block is going to
be marked as cold because that is the
exception to the rule not the normal
situation locks will probably not be
saturated as in used excessively so what
we do there is we say ok for a lock if
we want to use a lock then we'll start
as a fast spin lock because the chances
are that by spinning the process for a
number of cycles the lock will be freed
and so we'll actually be able to acquire
that lock rather than changing context
shifting out registers writing out the
context of the thread at that point and
then loading another one what we do is
we say okay give the Locker chance to be
unlocked by spinning the processor and
chances are that will happen the other
thing that we do is we we work on the
principle that most locks are acquired
and released by the same thread and so
if you've got a thread that's processing
something in a collection where the
collection has synchronized methods
chances are that when you iterate
through that same thread is acquiring
and releasing the lock on the same
collection and so we can replace that
with a no op because we can say well ok
if it's the same thread releasing the
lock and acquiring it slightly later
there's no point in releasing it and
then reacquiring it we can simply just
ignore that unless another thread comes
in at which point we have to do extra
work but from the point of view
efficiency we can simply assume that for
the majority of cases most locks will be
released and acquired by the same thread
in a particular number of situations in
lining and virtualization these are
competing forces so the most effective
optimization that the compiler can do is
method inlining rather than the compiler
having to resolve methods call a method
via a lookup call a method via reference
and then put things onto the stack
make the method called pop results off
the stack and so on what it does is it
simply takes the instructions that are
in that method and places them where the
method call would be makes the code
bigger but it makes it run much much
faster problem is that by virtualizing
methods which happens in a lot in Java
the the way of method inlining becomes
problematic so from the the good news
point of view JIT method or JIT compiler
can D virtualize methods so only sees
one implementation of a particular
method then it effectively said well
it's not a virtual method so we will
treat it as being monomorphic because
there's only one implementation
therefore it can be treated as a single
method bad news is if the JIT compiler
later discovers that somebody and you
know there is actually an additional
implementation due to sub classing and
the reference that you have is to the
superclass then it needs to do mais the
code it needs to undo the method
inlining that it's done potentially
Arion lining the code and so it wouldn't
need to reoptimize it to make it a
violent fit call because there's two
implementations that can be used and
certainly if you go off into the idea of
having the third or more implementations
then you end up with a multi morphic
call where the JIT compiler is quite
limited in what it can do from a
performance point of view getting some
important points about inlining and
virtualization implementation is during
steady state so what you don't want to
be doing is to making changes to the way
things work from a class structure point
of view once your applications reach the
steady state so you don't want to be
using different subclasses from the
point of view of virtualization once
you've reached the steady state should
you write get friendly code we talked a
lot about an idea of garbage collection
friendly code should we write get
friendly code
well actually no I mean in this case you
should really let the the JIT figure it
out because the JIT is pretty good at
analyzing what's going in your
application there's not really many
situations where you could make a
significant impact on what's actually
happening with your code by writing your
code differently from the JIT point of
view so do beware premature optimization
don't do it
code naturally and let the JIT figure it
out for you if your application has
performance problems you need to profile
it and then you need to find out where
those problems are and then you modify
code but only further problem areas in
order to improve performance so don't be
thinking about how the JIT compiler is
working when you're writing your code
wait until you've got your code working
you've got the algorithms all sorted out
and then if you've got performance
issues work on those in terms of
improving efficiency so basically to
conclude this this session the Java
applications use a virtual machine we
have the JVM so we need to be aware that
there is automatic memory management and
we need to be aware there is adaptive
compilation of byte codes now how these
things work does have a significant
impact on the performance of your
application how your application
interacts with the JVM from the point of
view of memory management and of
adaptive compilation can radically
change the performance profile of your
application having said that of course
what you need to do in terms of tuning
an application is profile profile and
profile you've got to know what the
application is doing in order for it to
be able to in order for you to be able
to understand how to improve the
efficiency of it avoid premature
optimization don't be thinking about the
fine details when you're writing the
application yes be aware of some of the
issues certainly around memory
management use of finalizes and so on
but don't be thinking at the very lowest
level in terms of JIT compilation and so
on as you're writing your code place to
go for some more information there's a
couple of references on the oracle
TechNet work some good articles about
tuning
the virtual machine and there's a nice
nice document that's available from
middleware magic that was written about
WebLogic and talked about some of the
the J rocket side of things I've spent a
lot of time talking about hotspot there
was in this paper they also talked a
little bit about some of the hot rocket
no J rocket specific things that are
going to be integrated into the hot spot
virtual machine moving forward so with
that I hope that you have enjoyed hope
that you have enjoyed this session and
that has been useful for you so thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>