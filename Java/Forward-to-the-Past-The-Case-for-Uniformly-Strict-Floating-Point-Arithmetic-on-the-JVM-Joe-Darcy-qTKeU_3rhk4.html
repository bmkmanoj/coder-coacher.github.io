<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Forward to the Past: The Case for Uniformly Strict Floating Point Arithmetic on the JVM - Joe Darcy | Coder Coacher - Coaching Coders</title><meta content="Forward to the Past: The Case for Uniformly Strict Floating Point Arithmetic on the JVM - Joe Darcy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Forward to the Past: The Case for Uniformly Strict Floating Point Arithmetic on the JVM - Joe Darcy</b></h2><h5 class="post__date">2017-07-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qTKeU_3rhk4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you for coming here today my
name is Joe Darcy I work in Oracle with
Java platform group on a floating point
and a variety of other topics I'll be
tweeting a URL to the PDF of the slides
and have a URL to the slides on the at
the end of the presentation this is
Oracle's safe harbor statement you'll be
seeing it at number of times throughout
the conference however I must let you
know that if you're interested in safe
harbours grab me during the break
session I have some other
recommendations of interesting safe
harbors in the area and over he will be
talking about today originally Java had
very straightforward floating-point
semantics both in the language and the
VM however to allow better performance
on certain popular processors at the
time namely the X 87 the floating-point
coprocessor back in 1998 there were
subtle changes made to default
floating-point semantics so if you've
come across the strict FP modifier
that's when that change come in and
there's a corresponding extra bit in the
classifier level and the vocal endpoints
mentor actually redefined now I think
today in 2017 these loosened definitions
are no longer useful and I've recently
published a proposal to restore the
original floating-point semantics
requiring strict of floating-point all
circumstances if you're interested in
reading more about that that's Jeff JDK
enhancement proposal 306 now here in
California we like to accept and
accommodate the sensitivities of many
people so this is your final warning
that the rest of this talk is going to
be about floating-point arithmetic so
for a number of years I felt that many
people feel about floating-point the way
some people feel about spiders something
unpleasant that's best dealt with by
someone else so as part of the
preparation for the talk I thought I
should get some data to back back up
that assertion so I did a quick survey
spiders fear loathing indifference or
fascination and as expected there was a
fair amount of fear but mostly
indifference same questions about
floating-point
it's even worse floating-point is both
more feared and more loads than them
spiders so hopefully hopefully it's
based on fear at the end of the talk it
won't be quite as what quite as
frightening after a bit more background
will have a condensed floating-point
primer we will talk about the cue era
tease of the x87 architecture the
benefits of restoring strict FP and some
for looking statements and I would
probably have time for Q&amp;amp;A but I'm happy
to talk during the break or elsewhere
throughout the conference so I think
there are three C's of technical
communication ideally you can be
simultaneously concise clear and correct
this is extra difficult with
floating-point because of the number of
special cases so sometimes the concise
leanness will have to be sacrificed for
accuracy so why does floating-point have
a bad reputation so if you look back far
enough a few decades ago there was a lot
of variation of floating-point semantics
across different architectures but
starting in 1985 there's the I Triple E
floating point standard and I was very
quickly adopted by many processor
architectures it's a essentially uniform
there is a backwards compatible update
to the I Triple E floating point Center
back in 2008 so things are basically
haven't changed and it's very common to
find a 32-bit single float format in a
64-bit double format across platforms
and the default semantics of I Triple E
floating point is fundamentally very
simple you take the exact mathematical
result of the operation and you return
the floating point number
closest to that exact result so that's
kind of the locally optimal change you
can't really ask for more than that at a
local level but of course as we know
from the poll and perhaps from the
feelings in the room as well
floating point still has that bad
reputation so what why why is there so
much fear for any binary floating-point
system one of the problems we run into
is that one tenth is a repeating
fraction in binary that means it's not
stored exactly so this leads to a lot of
surprising anomalies with binary decimal
conversion and things that are
surprising as well like one tenth as a
float is not equal to one tenth as a
double because double has more precision
and the repeats only four bit so you
have money more bits so they're values
are equal
also the usual arithmetic properties
don't hold namely associativity so it
matters what order you add up
floating-point numbers in now may be
surprising but of adding up
floating-point numbers is actually still
a research topic over in the linear
algebra community they're working to get
more reproducible summations despite
having non associativity and that's
that's an ongoing effort now even
working past the associativity and the
dietary version there's a question of
getting from your language to the
processor from the language to the
intermediate format you might have one
set of semantics go to see that on the
next slide so in the Java ecosystem we
have very explicit and precise
specifications to cover the
floating-point semantics going from the
Java level to the class file level and
for the Java Virtual Machine
specification constrains the mapping
from the class file to the via now if
you're working on a different language
environment that's targeting the VM
there's some other specification force
that may be less precise or may be not
specified at all and likewise if you're
outside this ecosystem you can have some
completely different mapping that
doesn't go through the VM at all so
variations in the mapping at the levels
is another reason floating-point has a
bad reputation or people are have a fear
of it this is a quote from the beginning
of the preface of the first java
language specification and i think it's
a very important summary of the java
philosophy except for timing
dependencies or other non determinism
and given sufficient timing sufficient
memory space a java program should
compute the same result on all machines
and in all invitations now if you were a
skeptic in java you might read this and
say well this is kind of a vacuum back
you silly true statement of any platform
except for all the non determinism it's
deterministic that's true for anything
but there really is a big difference
here especially compared to comparing
java to something like c we have to
worry about the sizes of instant
pointers and you know side-effects
during during expression evaluation and
this philosophy I think led to with a
reasonable amount of discipline you can
get predictable programs
if you look very closely they weren't
exactly reproducible in terms of timing
effects and like and this philosophy
also extended to the floating-point area
for numeric also for this conference
when I was doing a research this is a
quote from the introduction of the
virtual machine specification noting way
back in 1997 that even at that time most
the language implementers were targeting
the VM I think it's been very heartening
to see the work that's been discussed
this conference over the years to see
this vision fulfilled over time the
original VM specification also covered
floating-point basically it says for the
floating-point types you have to use
exactly the I Triple E types these float
in double 32 and 64-bit and for the
operations on those types you have to do
what I Triple E says you have to do for
those types in particularly you also
have to support the sub normal values
those small values close to zero so all
this is very straightforward and allows
a pretty direct mapping from language to
VM to Hardware instruction if you look
at the virtual machine specifications
since 1.2 these are some quotes from
Jesse 8 it's not quite as simple there's
the discussion of value says well what's
the value said well let that's the spine
and specification here it's start
slugging out the extended exponent so
sometimes you can use more exponent
range and just the floater doubled and
it makes the important point that these
value sets are not types there this this
is very very strange so the first
sentence in a course about type theory
is uses on some variation of a type is a
set of values so here in the virtual
machine spec a type is instead a set of
a set of values which of course is not
the same so so this is a looking a
little odd now we have these different
value sets so there's a conversion to
find among them and sometimes it's
permitted on other times it's required
so this is allowing more wiggle room in
this back and we're used to seeing for
this sort of aspect of the platform so
what happens when you have this value
set conversion goes on here to discuss
it
this is the important form here is that
you're converting from one set of
numerical values to another and there
can be a difference so you have some
rounding operation that's going on so
some changes being made to a value
computed by the program so what
difference can that make in practice so
I was going to try to do a demo this I
did manage to make a hotspot display
this behavior here if you multiply these
two carefully chosen floating point
values together you can get two
different answers but just showing that
on the screen is it very helpful but I
can demonstrate it if needed that I it
is actually possible so why why is this
allowed or what what's happening here to
explain why these particular values are
leading to multiple answers that's what
will answer it in the rest of the talk
so let's start looking the details here
so we start out with some Java source
code we want to compile that down to
class file so if we have a product
method that wraps a multiply at the
class high level this turns into a
wrapper mounted email instruction which
is what we'd expect
now let's declare it strict f-p that
modified that says you have to use the
exact semantics that were defined
originally all right so now we have a
strict product instead and we compile
that down to class files and we get
basically the exact same thing except
the act strict bit is set on the method
now this is JVM LS so just like so it's
not low enough so what does this look
like when we further compile down to
assembly using enough options OOP you
convince hotspot to not inline and show
this to you so if we look at the
assembly generated on a x64 machine for
the non strict case again we see in we
get a wrapper around an SSE multiply
instruction which is what we we did the
back if we take the strict FP version of
this method and we look at the assembly
we get for it it's essentially the same
it's a wrapper around the same SSE
multiply method so we're not going to
get any difference in behavior here
because the same hardware instruction is
being executed now if we go back and
instead use the 32-bit x86 or team point
instruction
using a few more options to convince
hotspot to do that we get a wrapper
around a different double multiply
instruction here to the older x87
instructions and this simple instruction
sequence is the one that can return a
different result from the strict case so
the code is the assembly code is very
understandable but it's actually doing
something subtly different now if you
take the strict FP method instead and we
compile this down to X 87 and we look
what we get we get something a bit
surprising we don't get one multiply
instruction we get a bunch of multiply
instruction so first we load this
extraordinarily tiny constant to the 2
to the minus can the 15,000 we multiply
by that we do another multiply then we
multiply by I to do plus fifteen
thousand which is seems like undo the
first one and then we have a load store
so what's going on here this is rather
confusing maybe it even looks kind of
buggin because after all why would you
store something to one address and then
load it right back in while it looks
like there some of these instructions
here might be extraneous these are
actually all necessary and this is the
best known technique to implement strict
double floating-point semantics on the
x87 so now we'll transition to the
condensed floating-point primer to
understand what's going on here so this
will be as much as we need to understand
the strict FP defaulter P and a little
bit more because a floating-point is
just that much fun so if we want to have
a condensation of the different
mathematical systems we've seen
throughout our education we start out
with counting numbers then we have
addition once you have addition we want
to invert it with subtraction now once
we have subtraction we define new kinds
of numbers zeroes and negative numbers
the ones who didn't have before
something similar happens with
multiplication and division once we have
division then we get rational numbers so
a whole different kind of number and
this happens a few more times we want to
start taking roots of polynomials so we
can talk about things like the length of
a diagonal of square we get them to
algebraic numbers
and once we want to start talking about
circles and their circumference --is we
have a more complicated thing going on
and we get full full real narrow real
numbers now algebraic numbers real
numbers and rational numbers are very
useful kind of math not mathematical
object called a field the field
satisfies these field axioms and of
course these are also properties that
enable a lot of optimizations that the
compiler writers like to do and from
this point of view you can think of
mathematics as sort of the same way as a
game there's a set of rules
there's objects living within those
rules and you can explore the results in
that framework and just like you can get
expansion Paks the games we've seen you
can get expansion packs to the
mathematical things we have and a lot of
things like negative numbers in in
fractions might peer strange or
different at first as we come to work
with them they become familiar and
perhaps something similar happen at the
computer arithmetic because the same
trick has been done before in terms of
completing operations and having to add
new kinds of values so what is floating
point arithmetic floating point
arithmetic is a systematic approximation
to real arithmetic and it approximates
real arithmetic in several dimensions it
both approximates the set of values you
can represent and it also approximates
the properties of the operations over
those values so in particular the field
axioms here in orange do not hold for
floating-point arithmetic so you see
many of them don't hold in particular we
don't have associativity and in terms of
being able not being able to say things
that are both concise and accurate
commutative 'ti only holds if you use
the right sort of equivalence operation
you have to use kind of a bitwise
equivalence or semantic equivalence
rather than the equals equals operation
from the floating point standard so it's
hard to make simple statements the
revision of the I Triple E standard at a
very helpful conceptual diagram to
delineate different levels of floating
point so up the top the mathematical
system being approximated is extended
reals real numbers with plus and minus
infinity then we can talk through a
rounding process about the
floating-point data there's
representable floating-point numbers the
way those values are represented in the
floating-point system in terms of the
fields of a floating-point number and
finally at the lowest level the
different bit strings used to store that
and we usually we can say at the top
three level to not have to worry very
much about them coding what are the
values look like we have assigned time
significant time space to the exponent
and you can fully characterize the
values you can store in a format based
on the width of the exponent of a range
of values that can take on as well as
the number precision digits you have or
precision bit so for but for binary we
have eight of exponent bits and 20 for
precision bits now if you're saying that
adds up to 32 that is correct and there
is a sign bit in there but with some
clever encoding we can get one of the
bits back so it all works out similarly
for double we have a larger exponent
range with eleven bit and we have a 53
bits of precision in terms of how the
floating-point numbers are structured
the significant uses a normalized
notation usually that means the leading
bit is one and then it can take on all
the other values so number of
consequences of this representation
floating point finite floating-point
numbers are sums of powers of two where
the distance between the powers by two
is inaccurate it's bounded by the number
of precision bits you have so floater
double or too large to fit on a lot a
slide so instead we're going to start
with a toy format so let's say instead
of a 24 53 bits of precision we're just
going to have three bits of precision
and three exponent values and we can
write down the values like this there is
a special case but now we can see the
normalized representation we have the
leading bit as one in all these cases
and if we start by looking at the cases
where the exponent is minus one we just
have all the possible combinations of
the significant values from 0 0 through
one now there's a few things we can
observe here for the values were the
exponent is minus 1 the distance between
successive values is the same it's one a
and in each case when we change exponent
values the distance between the numbers
doubles but it's again the same within
that exponent range it can be helpful if
we graph these values so here's the real
number line and we can start filling in
the values with exponent of minus 1 0 &amp;amp;
1 now you'll notice even in this toy
format there's a very large gap between
0 and the first representable number
then between the first and second
representable numbers so that's that's
kind of a problem on numerically so we'd
like to fill that in instead and we can
do that with what are called sub normal
numbers sub meaning below the normal
numbers and you'll notice unlike the
normalized value they're leading bit is
0 instead of 1 now there are other
format of tweaking of lines so this is
kind of the base
taury format but we can also define say
an extended exponent toy format over
here the sub more values extended
exponent toys format where we go down to
minus 2 instead of minus 1 so we have
all the normalized values we had
previously then we get another set of
values with the exponent of 2 and a
different set of sub normals
now you'll notice that all the sub
normal values we have in the base format
are representable values in the extended
exponent format however some of the sub
normals on the first format are normal
values in the extended exponent format
because we have more exponent range so
the representation of a given value
differs depending on the details of the
format and we can graph that as well we
can get the toy format back we have the
extended exponent so they're the same
now we fill in the values where the
exponent is minus 2 and the new sub
normals okay so floating point is
imprecise there's only so many answers
we have so there are only 16
non-negative finite values so we can
write them all down here so these are
the choices we have to return
so what should we return for adding 1/2
to 1 to 1 1 and 1/4 the exact value is
1.75 we can look here off on the right
and when you see 1.75 is one of the
representable numbers so that's the most
reasonable thing to do is return that
exact value and that is indeed what the
standard requires how about if we add 2
plus an 8 we can look here again that's
not a one of the answers we have but
it's between two and two and a half so
the most reasonable thing to do is
return the floating point number nearest
to the exact result in that case that
would be two and that is in fact what's
done by default now how about this case
one plus a in a we can look here at the
representable values and that's exactly
halfway between two representable
numbers so we're going to need some rule
to break that tie in a special case and
we'll see that in coming slides can help
a little bit to graph this so let's
first consider the region of the number
line around two and a half to three and
we're going to talk about a rounding
mode and a rounding mode here is a
mapping from the real numbers to the
floating point number so they make make
it clear that you can only return the 40
point numbers we'll get rid of the
number line on the bottom here so if we
look at the region between two and a
half and three until you get to two and
three-quarters you're closest to two and
a half right so we should round down
like this this is how we get to the
nearest I presume about now let's look
on the other side let's say we're
between two and two and a half once we
get to two and a quarter we're closer to
two and a half right so we should round
up in that case like so so the it so
this kind of drainage Batian on the
number line represents how the closest
value is computed now we can go through
the same argument for all the other
values here and that gives a graph that
looks like this so we have all these
little Skip isosceles triangles scalene
triangles rounding down to different
values here now what happens when we get
to beyond three and a half well it seems
reasonable just a few examples here so
what should this how should this value
round she drowned down to two right if
we're in this
range here and to explain how the ties
work this is the value halfway between 1
and 1 and 1/4 the rule that's used is
called round to nearest even so we look
at the two values at bracket 1 and 1 and
1/4 so we go off on the right here and
we see that the last bit of 1 is 0 and
the last bit of 1 recorders is 1 so the
last bit of 1 0 is the even value so we
will round down in this case okay so we
still haven't answered all their
questions though what happens if we're
on 3 and a half we've run out of numbers
well if we all the other values have
these symmetrical triangles around them
so it seems reasonable to extend a
little bit more to get down to 3 and a
half but now we really have a problem
because we apparently don't have any
other value to return so this is where
we turn to that trick of defining new
kinds of values new kinds of numbers and
we actually overflow to plus infinity
and this is the same thing that happens
with floater double but much farther
down the number line now this isn't the
only possible rounding mode this round
to nearest even there's other rounding
modes you can define one that you
sometimes is called rounder 0 or
truncation so we can look what that
looks like graphically here again we're
rid of the number line and instead of
having symmetrical triangles now we have
in sprite angle triangles because you're
rounding down in this case
so let's look here so we're going to
round down in both cases in particular
even though the value between 2 &amp;amp; 2 &amp;amp;
1/2 is closer to 2 and 1/2 we're going
to round down to 2 okay
and we also round down to 3 and 1/2 so
we don't overflow in the same way under
different round notes so what are the
benefits of using round to nearest even
by default instead of something like
truncation it is closest to the
grade-school rounding in a statistical
sense reduces the round off error and
the little detail about managed nearest
even avoid drift in certain kinds of
summation loops so here's a particular
example to show how floating-point
associativity doesn't work from the
format we've seen before we know that if
we add 1/4 to 2 that will round down to
2 all right so let's say we add 2 plus
1/4 plus 1/4
if we first add two plus a quarter I
will round down to two and then we add a
quarter again and I round down to again
so the interval will be two let's say
instead we add the two one quarters
together for one half we add that to two
that will round up to two and a half so
floating point addition is not
associative this is true for this toy
format it's true for the real formats as
well a few fun facts about the floating
point if you get a subnormal result from
addition or subtraction that's actually
exact is there's no rounding involved as
how the math works out all float values
are exactly representable as double
values in between every two finite float
values there are about 500 million
double values because of the extra
precision that means from a certain
perspective double isn't twice as good
as float it's 500 million times better
than float because it puts you that much
farther away from any kind of numerical
problem like running into and
surprisingly over 45 percent of finite
double values are numerically integers
well why is that we have a limited
exponent range 53 bits so if the
exponent is bigger than the number of
precision bits we have when you multiply
it out you're just adding integers
together so this is another reason
floating point shouldn't be so fearful a
lot of its just integers and no one's
afraid of integers there are very very
friendly integers are very friendly you
just have to go talk about just a quick
point here
completing the floating point arithmetic
we've already saw infinity earlier in
the talk there's a few other values that
come up there's also man and minus zero
there are rules given in the standard
for operating on these that once you
recognize the need for the value it's
pretty straightforward to work through
what the rule should be but we won't
have to thankfully we won't have to
worry about this too much more in the
rest of the talk so that's the end of
the primer and we can move on to x87
floating point arithmetic
now the x87 has been around for a while
the design was done way back in 1977
maybe some people in the room weren't
even around in 1977 and their transistor
budget was 40,000 transistors and with
that budget of 42,000 pounds this is you
got correctly rounded arithmetic
operations but that wasn't all you also
got pretty good rounded trends in all
functions like sine and cosine and
exponent and that wasn't just for the
float and double precision but also a
double extended precision there was a
full 80 bits long with 15 exponent bits
and a full 64 significant bits and it
also had a stack base register set
although that wasn't as good in practice
as it turned out now this work predated
the I Triple E standard and informed the
I Triple E standard and the optically
standard actually dropped a number of
features that were found in this chip
such as an alternate way of dealing with
infinity for the floating-point
instructions there there was a control
word you could set the processor to
round to float or double precision but
using the larger extended exponent range
of the double extended format so that
means if you even though you around you
the 53 bits of precision you're still
getting 50 Keuka 15 bits of exponent and
of course 15 bits of exponent is more
than 11 bits of exponent that you would
get for flow now if you were concerned
about the exponent bits as well the way
you can get that surround was doing a
store to memory so you could store to a
64 bit valued around the exponent so you
might think ok we can round the
precision with this weight around the
exponent if I want strict double maybe
all I have to do is do the operation
that gets the right precision bits
assuming I have the a precision control
step store it down to round the exponent
and then load it back in and that does
actually work for adding subtract when
they will see whether or not it works
for other operations like multiply and
divide and and of course you should be
suspicious that the answer is no it does
not work for the other
operations and to be equivalent you have
to get the same answer for all inputs of
course and for all possible values with
a floating-point fields here and you
also have to make sure that the handling
of the special values Nan's infinities
works out too all right so let's go back
here what we want to replicate is the
semantics we see under single rounding
so we'll use the toy format again so
we're starting the real number line and
then for a single rounding say around
two and a half we want to get get down
to that value okay
so let's say we're going to do an
intermediate rounding first this is
analogous to rounding on the exponent
stack with the extended precision so
instead of just having the base values
we also have the values of the extended
exponent range would soar in the sub
normal range of the original format so
you can start our case analysis the
special values in this mode work out
fine and overflow actually works out
fine as well so we don't have to worry
about that now let's start work looking
about the normal range for both formats
say around two and a half again so we do
the first rounding to the extended
exponent format and then we'll round
again to the second format now in this
case the rounding doesn't actually do
anything because the values go to say so
that's exact and we'll see here that the
range being rounded with the second
rounding the range of the original
number line is the same in both cases so
this is equivalent for the normal range
here and we can do a similar analysis
for all the other cases where the values
line up with the same density now that
leaves of course the sub normal range
where the values of aren't lining up for
the same density so we'll have to look
at what happens there so first we'll
look at the identity first we format
with out the extended exponents and
they're rounding looks like this with
the triangles it's all very nice and
regular are they triangular each 1/8
wide on the number line and it all looks
very simple so now or is use the
intermediate rounding with the extended
exponents and it looks like this will
have the number line and we have
or more answers to go here and we'll
focus on the transition between the sub
normal range and the normal range so
around 1/2 in this case now when the
values are exactly representable in the
intermediate format there's no problem
we can just copy them over like this now
that leaves the cases where they don't
line up we have these extra midway cases
here so now we have to use the
Imagineers even rounding rules for these
other values and that looks like this
now if you look from the top to the
bottom now you might start noticing some
asymmetries and how the rounding is
working now remember there's a symmetric
region around 1/2 that should round down
to it from both sides of round to it
however if we look carefully here
there's this little extra region it
first rounds up when you round to the
extended exponent range and then it
rounds up again when you go from the an
exponent range to the final one so this
means there's a larger portion of the
number line getting converted to one
half than there was originally so this
is a problem because we're not getting
the same answer back anymore so this is
at least a potential problem like what
if we don't actually have any values
that come up in this narrow region to
cause problem so we can start going
through the operations square roots not
a problem if you had a non toy format
square root doesn't overflow underflow
so that's not usually okay since
subtracts an add of that have some
normal results or exact
we don't have rounding going on so Adams
subtract are fine within the situation
and for this this particular toy format
multiplies not a problem either the
format has so few bits of precision we
can actually get values into that
particular range so we kind of dodged a
bullet there then there's divided
divides always problematic so the rajah
troublemaker here if you go through the
case analysis there are multiple
quotients where the exact value is in
that narrow range we can't have the
exact value to be so that's going to
cause a problem we're going to get a
different value under double
rounding with divide in this format than
we would if we had a single round
alright now we can construct the same
value with double but before we do that
we're going to have to take a quick
detour to pick up a special-purpose tool
hexadecimal floating-point literals have
people seen hexadecimal floating point
literals and for a few oh good good I'm
surprised probably so many there's been
in the platform since 1.5 it's very
helpful for writing floating point tests
in the life and it's analogous to the
little binary representations we've been
using as a textual representation of the
significant and the exponent fields or
the floating point value it's a somewhat
human readable at least more than the
raw bits of the long so the one way you
could write 3.0 is a hex literal is 0 X
3 0 p 0 that is three times through to
the 0 that's not how the valued actually
stored as a floating-point number it
would be stored as one point eight in
hex or one and a half and decimal times
two to the first okay so we saw the
value in the toy format between the
transition from several numbers for
normal numbers so let's look at that
range for double this is the minimum
normal value here so we can construct
the max subnormal so we get 52 bits of
zeros as a fraction and then the leading
bit is one with the same exponent and we
can go to the range of interest so we
have the all the bits of F we want the
next bit to be zero because we're not
quite going halfway and then we have a
few bits of one now we can treat this
value as an integer all right we can
write it out here and it's an integer we
can find its prime factors and now what
we can do we can try to construct a pair
of double values whose product is that
exact set of bits in the right exponent
region so that's not too hard once we
have the factors we can put the two of
them together like so and we can see
copy the bits over into the hex floating
point values and now we just have to
have the right exponent and when we
multiply these together there's two
results we can get we can get the strict
result
which is the smallest largest subnormal
if we do strict rounding or if we we
also allow the legal nonstick grounding
where we get the smallest normal value
instead so one multiply two possible
answers so how do we actually get the
strict results on the x87 so what's
going on here so the key idea is to try
to get the process to do the hard work
for us in computing these the
significant bits so we want anything
that would be a sub normal in the final
format to be a sub normal in the
intermediate rounding format instead
that way the processor does the hard
work and to do that you have to scale
down so that it would be a sub normal in
the intermediate range that's why we're
multiplying down by this huge factor 1
times 2 to the minus 15 360 which is the
difference in the exponent range between
float and a between double and double
extended so we can see that looks like
here we scale down one argument by the
small value we do the intermediate
operation and then then we scale it back
and usually the scaling up and scaling
down doesn't have any effect on the
value and this allows the intermediate
operation have the right significant
bits we want and on the x87 we do the
store reload to enforce the overflow
threshold we can see that what looks
like the toy example to make sure it
works there's one of the problematic
arguments we can scale it down in this
case the ceiling factors only 1/2
because the difference between minus 1
minus 2 is 1 and this will get the right
intermediate results here if we didn't
do this we'd round up twice so we get a
final result of 1/2 instead of 3/8 a
brief in history of Java floating-point
proposals back in 1995 it was all strict
all the time in the first few years
there were a number of a people doing
work on alternate proposals including a
James who had won back in 1997 there was
a concern to address the impedance
mismatch between the x87 semantics
and Java strict semantics that led to
among other things this proposal for
extension of Java floating-point
semantics from 1998 this had not only
strict F P but wide F P and wide FP
allowed broad use of wider precision
which was a problem with predictability
so within a yfp expression the VM would
have wide latitude to on a expression on
the floating-point stack to use not just
full precision but float extended
precision and one valid float extended
precision is double so you can imagine
you have your float expression and the
VM can randomly arbitrarily decide in
the ground to float here I'm going to
use double for this one and you have no
way to protect against that and you know
this would make it very difficult to
have a predictable program because you
don't know if your values are going to
be stored in float or double which is
going to affect how they want also
you're not required to have subnormals
and y2p expressions during the waning
days when I was a full-time student
myself and others same feedback on this
proposal once it was made public exhaust
of a Java grande document proving Java
for numerical computation published to
respond to this and acting on the
feedback instead of doing the page
crisper proposal something more modest
was done in Java 1.2 that was strictly
FP and craftily redefining the default
floating-point semantics so strict efi
means use the original all strict all
the time but by default we are now
allowed to use extended exponent range
but not more precision just for the
results on the vm stack there are some
good features of the redefinition for
default FP it was enabled by Roger
Tolliver coming up with a more tractable
approach to get exactly reproducible
results that's the approach I've been
discussing here today and if overflow or
underflow doesn't occur you get the same
results in default floating-point as
well as strict filling point so that's a
very helpful property if you're trying
to semantics if you often want to avoid
overflow and underflow anyway
and for your name storage elements your
fields and your variables you're always
going to have strict semantics
if you need to get exactly strict you
can do a store now there are all some
problems though so how do you print out
one of these values with extend exponent
range basically you can't because if
you're going to pass it as a parameter
once assassins parameter can only hold
this stripped off loader double step so
all the extra information goes away and
there's no really no ready mechanism to
force the extended exponent range to be
used if it was available and there are
subtleties with the code generation so
what's done here what was done here to
allow simple code generation for x87 no
other platform benefited from the
flexibility allowed allowed by this
change there rather complicated
semantics introduced in the floating
point standard for all possible
platforms so I this was a pragmatic
compromise for its time but times have
changed and at this point I think it's
more important to have simple semantics
for all platforms and at least I for one
would never would like to never have the
thing about to call floating point
semantics again you someone at least one
person agrees with me which brings to
JEP 306 to restore the strict floating
point semantics everywhere so we
wouldn't have to worry about these
extended exponent value set so the value
set conversion strict to XP would be a
no op and potentially the axe trick bit
would be position we up on the market
and I know Brian has been very looking
with lust ly after this bit position as
since I mentioned the idea for this
project for him now the practical
concern that led to this change in the
floating point to Mattox was allowing
the x87 to run more quickly at a
reasonable pace but we don't have to
worry about that anymore praten x87
processes for many years have had sse2
or later an SSE 2 or later Forks both
float and double an SSE instructions
which matches java semantics perfectly
fine so there's no performance cost from
being strict all the time because people
are using strict instructions anyway so
on for example on default a 64-bit
hotspot it doesn't use the x87 registers
at all it only uses sse because that's
always there so that's about Jeff 306
and strict FP people are also often
interesting able to make fast a fee
instead you know strict that's kind of
dull
fast fast FP is more interesting that's
certainly technically possible
there's a known wish list of items maybe
we use views multiply add maybe we use
the algebraic translations that only
approximately hold and so forth however
I think adding faster P to the platform
in a way that's consistent with the
spirit of Java would be a little more
challenging than people suspected first
one reason is it has few precedents few
other platforms have cared as much about
the precise floating-point semantics as
Java have and I think to do proper due
diligence or tacitly investigation of
the numerical accuracy versus speed
trade-offs one particular example of
interest is a fuse multiply add this is
a increasingly common hardware
instruction that as the name implies
does a multiplying and add as a single
cycle and it's fused meaning that
there's one rounding error instead of
two and our hardware that implements
this instruction it's faster than doing
a change multiply now there's some
clever tricks you can do with this to do
things like correctly rounded divide in
software and it helps certain loops run
faster in jdk 9 we added a method
support for this in the platform as well
now there's three possible semantics you
might want to have for fuse multiply out
that you have to use fuse multiply on
the way to specify that most
straightforward things just call the
method if you want the semantics that
you must not use use multiplied you can
just use the you know multiply and add
instructions on the platform but let's
say you want to have the semantics cells
like I'm really careful use choose
multiply add but if it's convenient and
fast use it there's no good way we have
to specify that third option so if we go
back to the diagram here the the
interface
a language compiler would have to
specify that third option is really
lacking so options would include a new
hardware new JVM instruction that would
be owners to add so maybe we could get
by with more code generation entry point
instead to capture the sort of semantics
that a fast floating-point mode would
require so I think the most expedient
way to support faster floating-point or
looser semantics would be something like
at a loose FP bit to the method and let
the VM guys do whatever they can figure
out to the method that would be fun for
the VM engineers but it wouldn't be very
good for the people who'd have to debug
what's going on so I think a more
nuanced API is needed but we've seen
examples of the platform where that kind
of additional API is added for example
in jdk 9 there's this string can carry
added as a way for java c or other
compiler to more easily concatenate
strings together so Java C doesn't have
to worry about how its buffered up there
all that and the VM doesn't have to
worry about recognizing multiple coding
patterns from different compilers that
can change over time so maybe we could
add some sort of cogent entry point like
fuse if using this path it's okay and so
forth and this wouldn't be an API that
end users would be connected to see
likewise for summing things up maybe you
don't care exactly how things are summed
up so you could have an API entry point
like that maybe this looks similar to
the vector API that we hear we'll be
hearing about later in the conference
maybe one is built on top of the other
pbd some more more thought that are
required there's some other
floating-point formats that might be a
potential interest there's a some use of
hash precision 16-bit in different
contexts for GPUs and machine learning
I've heard machine learning summarized
is a low precision matrix vector
multiply so this is where the 16-bit
floating-point comes in there's also
support for quad precision 128 bits this
is part of the 2008 update to the
standard various architectures have
supported quad even predating the
standard but a generally doesn't have
direct hardware executions so it doesn't
go very fast now one of those documents
from back in the late 90s about
improving Java program numerical
computation had
wish list that features to come into the
platform and some of these features are
still under discussion in the Java
community including here at this
conference and it for example for
example none of these would be satisfied
using value classes which we'll be
hearing about later
so in conclusion evolving the Java
platform is a long-term effort I think
it is important to clean up and retire
unneeded complexity so we can make room
for some new complexity instead that's
more more helpful and probably out of
time but I'll be happy to answer
questions during the break or I know the
times during the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>