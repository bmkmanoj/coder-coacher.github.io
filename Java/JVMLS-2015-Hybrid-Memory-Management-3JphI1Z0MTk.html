<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JVMLS 2015 - Hybrid Memory Management | Coder Coacher - Coaching Coders</title><meta content="JVMLS 2015 - Hybrid Memory Management - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JVMLS 2015 - Hybrid Memory Management</b></h2><h5 class="post__date">2015-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3JphI1Z0MTk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome everyone to the last talk
for today I am Christian Wilma from
Oracle labs and most of the work and
presenting was actually implemented by
Coast and Sue he was a student at UC
Irvine and now joined oracle apps full
time I want to talk about safe and
efficient hybrid memory management for
java or in other words how do we get rid
of our garbage and all the dead objects
without running the garbage collector
yeah the usual disclaimer everything
that I say is more less my personal
opinion and not an Oracle product yet so
if you like it come to me but don't buy
Oracle stocks yet I have to give you a
little bit of a context where we
implement this the project so that you
understand more less why we are doing it
and how we are doing it and the the
bigger context is at oracle apps we have
a project called Trafford we have
implemented words of dynamic languages
so we have implementation of JavaScript
our Ruby there's an implementation of
pison so we implement all these
languages in Java as an ASTM to a
Beretta and then we have our
just-in-time compiler a cold grawr to
give you good performance for these
languages so if you want to implement
your own language you write them into a
Beretta for it and we automatically give
you the good good compilation
performance and that whole system runs
on the hotspot vm actually our
modification of the hotspot vm but
hopefully coming with java 9 that's a
JEP to book the these hooks also into
the openjdk so if all goes well then
with travie nine you can run these
languages also before performance
without our only em but for now you need
our own build of the hotspot TM that is
all working very nicely but if you want
to have a standalone implementation of
these languages or you want to embed
that in other larger applications that
you have legacy applications written in
C or C++ then using the hotspot vm is
not the best solution because the
hotspot vm has quite a large footprint
and more less behave
I think a little bit like an operating
system and every big application I guess
wants to control everything so we
developed this thing called substrate vm
which is a very low footprint of yem
that we can run all these languages in
an embeddable setting and by embedding I
don't mean small embedded devices I mean
embedding it in large legacies to other
applications so the substrate vm is an
embeddable vm since we like java we also
implemented in Java and it's a low-level
version of Java you could say so we
don't need features like dynamic class
loading and reflection but we need
extended low-level support like a
low-level pointer type and we really
optimized and targeted to run our
truffle languages so our goal is not to
make a full java virtual machine our
goal is to make as small as possible
runtime environment that has just enough
to run our truffle languages and we
ahead of time compile the vm and all the
truck for languages using our compiler
of course and since we ahead of time
compile things you get then a standalone
executable or shared library so you can
debug that with a standard gdb you can
profile it with standard VT on so all of
your native development tools just work
on the executor this ahead of time
compilation first requires aesthetic
analysis so we have a closed world
assumption we know all of our Java code
from the substrate vm we know all of the
jdk codes that we use we know all the
java code from our traveler languages so
we run a very aggressive static analysis
over this code base so we find out for
all methods for our fields for classes
are they actually used and only the part
that's used goes into our executable and
once we have finished aesthetic analysis
we start the head of time compilation
and all the reachable methods are
compiled and then you get an executable
that has machine code it has an initial
java heap that starts the execution and
you have or the supporting debug
information that
if he needs for examples which generates
the end of two of debugging information
and you really get elf lip binary on
linux or mac o binary on necklace does
this work and what's the benefit well I
have a little JavaScript micro benchmark
and this community I don't have to warn
you about micro benchmarks and what they
say and what they don't say but in that
case its receive e we have a couple of
interesting things that we can get out
of this benchmark so we run this method
actually with a fixed number of 50,000
iterations of this loop and the x axis
on the on the next slide is how often
they call this method so we call it from
zero x 240 thousand times and forty
thousand times you can say that some
sort of peak performance and calling it
0 times of course means how fast is your
vm in starting up so what do we get and
actually have three different groups of
charts the first thing is really a the
startup perform an Excel fifth 0
executions to 10 executions then a 0 to
200 so you can say well medium level
warmup period and then you have peak
performance and the top charts are
execution times solo is better and the
lower charts are memory footprint so
also hello s better and when you have
today high-performance JavaScript pm's
like Google VA to a Mozilla spider
monkey these are all implemented in c
and c++ they have the advantage that
their startup performance is of course
very good so these are the red and blue
dotted lines so they start up in a few
milliseconds so when you actually don't
do anything then you don't spend any
time versus when you have a java-based
implementation then you have to start up
your travel vm you have to load all the
classes so you spend a half a second or
longer just to run 0 execution of the
loop because you have to do
the startup and the first thing we see
here is that the substrate vm you also
get the very excellent start of behavior
because you don't have to do anything
for the startup you haven't ahead of
time compiled executable the same way
you have the executable for v8 so now
there's nothing more to do here and also
the memory behavior is similar menu more
or less do nothing and then on the
medium-term you see how that the
java-based VMS more or less one up and
then reach clip performance and then on
the real long run you see where how good
is the code generated by there
just-in-time compiler and in that case
we go back to the slide if you look
closely what are we actually
benchmarking are the same via
benchmarking is this how good is this
Cape analysis of the just-in-time
compiler because we're allocating an
object here and be accessing it in the
loop so that means he performs is more
or less is the compiler able to
eliminate the object but that's not what
I want to talk about here what they
really want to talk about here is that
even though on the substrate game we
have a very good start up performance at
some point we have to invoke our
just-in-time compiler and our
just-in-time compiler is written in Java
to it's very aggressive compiler so it
does all the optimizations it does a
very aggressive escape analysis for
example so it locates a lot of temporary
objects and you see that here and
there's memory charts that actually we
don't have background compilation yet so
we have a blocking compilation so you
see when we have four iterations we
suddenly have this big jump in the
memory consumption and so we allocate
about forty megabytes during this
compilation of this method well you can
say why is it that much could it be less
of course but in any way it's a
just-in-time compiler which means it
locates temporary objects that are
necessary during compilation and we are
very sure that none of these objects
actually survive compilation because
compiler finishes it creates a bite
theory for measuring code it creates a
meter data that get installed installed
in the code cache but all the temporary
objects are dead but still they stay in
the travel heap until at some point of
the garbage collector runs and then the
garbage collector finds out where they
are dead so they go away so for us the
question is how can we make use of these
execs known execution patterns we know
that the compiler only has temporary
objects and they are evening compiler
faces so we have every phase as
temporary objects so how can we use this
domain knowledge that we have about our
application to optimize the memory
management so we want to free the
objects as soon as we know that they are
no longer used but we don't want to do
it the unsaved C or C++ way that you
have an arena based a location that you
just do a location allocation and at
some point value say well I free
everything and if you have a dangling
pointer into that memory region well at
some point you're being crashes and then
everyone is unhappy so we want to avoid
that so we want to have the same memory
safety that Java gives you and we have a
static analysis which we use to find out
what is actually reach over and we
extend this static analysis to also find
out what's the lifetime of objects and
can be safely reaching a locate them so
this is basically the system
architecture in one slide so we have the
standard points to analysis the static
analysis that we have for substrate vm
which normally creates the information
about regional classes methods and
fields and we extend it to also do a
region analysis and doing this
compilation ahead of time compilation
process a build up which meet the data
mapping that we savor every region every
allocation site belongs to a region and
during code generation we use this
information than to emit a slightly
different allocation code so instead of
allocating in the normal garbage
collected heap
we allocate in a region heap and in the
runtime data structures we have the a
locator and the region manager and
that's a very simple stack based model
when you enter region you push something
on the region stack and then you can use
this region and then you exit the region
then you you do a pop of the region
stack and the pop freeze their memory so
everything that was allocated in the
region is automatically gun when you do
the pub we still have the garbage
collector so when we actually ran out of
memory then we just take the whole heap
together and run a normal garbage
collection so garbage collection is so
infrequently so infrequent so we don't
really care about the region stuff when
we do the garbage collection it just
take every object in all regions and say
well that's the young generation and all
the objects that survived go into the
old generation yeah I'm all set
everything of that we opted for a manual
description of what are our regions so
we have a simple annotation for vision
scope that you can get if the name for
debugging and that means that this
method is the start of a region and
everything that this method calls can
possibly be allocated in this region and
of course regions can be nested so in
this compiler terms you say well your
main compile method is the outermost
region and then every phase is also
annotated as a region and so you get the
nice region ah yes think yes the
question
what do we do about finalizes if you
have a message that a class that's
actually finalized ever then it must be
conservatively located in the global
regional so we just say all the
difficult stuff like finalizes things
like if you would do reflective stuff
you would just always put that into the
garbage collected tip I don't think
there's any it doesn't make sense to do
any special thing to optimize that
that's the nice thing you always have a
garbage collected heap as a safe bet so
you don't have to support everything yes
what do we do with shared code like JDK
code actually this method that annotated
methods are the region stats and
actually have a slide here so we have a
that normally does the points to
analysis is a context-sensitive points
to analysis and for those of you that
know something about points to analysis
we use well as one of the standard
abstractions that's the published
literature which means it's to object
sensitive and with one context sensitive
heap so you can say well as the context
is you look at an object in the context
of who is referencing to it and who is
referencing that so you have two levels
of object references as the context and
we extend that to add the heap region as
another context and that means we just
have a when we analyze a jdk method we
analyze it in the context of the region
it is in and if you call it IDK methods
from multiple different regions then we
analyze that context sensitively in the
inner separate context and what that
means is that from the point of the
first point of view of the analysis you
really see this mess up multiple times
and that means in one context it might
say well I can allocate it on the top
region stack and in this other context I
can relocate it more or less three or
four levels down from the top regions
back and in the another context very
escapes completely you can say well it
needs to be in the in the global region
but I'll tell you then later that we are
going to merge this information so that
we can actually have a much better
location performance there'll be more or
less be then comes where we are
conservative in terms of this this
information now yes another question
yeah so I have something about that in a
few slides how we do that but first a
little example and we have a small call
graph with full message AP and see and
for simplicity we assume that all
methods are annotated s region
boundaries because you will do that in
reality but it makes the example nice
and what you then get is this region
graph and since we don't like graphs we
like trees better be more or less
converts the region graph to actually
reach entry by cloning the region's from
the point of view of the analysis if
it's reachable from Mike diverting
points and that's exactly that we then
have that the different if this is it
this is a jdk method then we analyze it
in the context of the regions that are
actually are on the region step and the
region and analysis then iterates over
the program statements and for every
location it says well if you have an
allocation then you have the initial a
look a region where the allocation site
is in and you have to find out in which
out the region do you have to allocate
and for that you have to track or loads
and stores and invocations so it's
really you can say well i have an
allocation site as the source of of an
object and i follow it through our
method course when
parameters you follow it through all
load and hands to us so you say well
it's stored in another field which means
this the object is stored in and the
object that you are storing there now in
a relationship that's saying well the
object that you store in needs to live
at least as long as the epitome needs to
know that the object you're storing
needs to live at least as long as the
object you are storing in because you
never want to get a dangling pointer and
so what we get here is for example if we
have an allocation site in our method d
and it's used the object is used summer
than in method a then we built this
mapping function where we have the
allocation side a we have the definition
region which is a d in that case and we
know we need to allocate it at least in
rich region a and when we look at our
region three what we know now is if we
are allocating in this mess OD here we
need to allocate it at least two levels
down at the region stack so this offset
between the region where the allocation
happens and where the allocation site
released and the region where we need to
allocate in this offset here is what we
really need for a location and that
allows us actually to do the efficient
allocation because what we have done at
runtime is we have a multiple T lips
that are open concurrently every region
has its own thread local allocation
buffer and for a bum pointer location
and we select the T lab based on this
offset so we savor and this here we have
the region top here and we go down to
levels and that's our T lab where we are
allocating if we have a bit more
complicated example where we have the
location side x for X Ian method f and
it's a locate the method is used the
object is used both in an sob and the
message see then we have actually
the object escaping to be via the
methods D and E and to to see and we
have a depth of 2 foot for these cases
and we have a depth of one for this case
because here we are only one level down
so we look at it from our actual
location in code so that's a stylized
code from our grad compiler let me use a
snippet based lower ring for locations
of is able we implement the new instance
operation as as a method that more less
is you can say conceptually is in line
for a new for a new instance bytecode so
what we get in as a parameter is we have
the hub which is a constant we have our
region mapping table which is also a
constant are built a constant object
graph built during this aesthetic
analysis and then we have to look up in
this table but it's the real offsets
that we want to allocate in because as
we saw in the previous example based on
where we are when we're coming via this
region the meso d or e we need to get an
offset of T of two and we come here when
the method called VRC then we have an
offset of one and this function is more
less the table lookup in this result
Taylor here we are picking look up our
offset you can say that's already
reasonably efficient because it's just a
table lookup and it then gives you a
constant to look up your car you you're
T lab but of course you get a table
lookup and it's never a good thing
especially in Java to make you a
location even a little bit slower and so
this table lookup is actually makes your
your travel code run definitely five to
ten percent slower because especially
when you have a location heavy coat so
we don't really want to have a table
lookup done for every location
how can we get rid of that it can be
conservative and do a thing that we call
normalization because we say well yeah
from this context now you need to have a
offset of two and in this context the
only would need an offset of one for a
locating the object but you don't lose
overly much if you're just
conservatively say i always use the
offset to because you only lose a little
bit in this context here where you
locate the object in a region that lives
a little bit longer and then this inner
region but then we get a constant for
this allocation offset and now what we
have as the result of this data canal is
this is really a compile-time constant
relocation site in which region we are
allocating based on the top of the
region stick which means that our
allocation snippets down here is
actually much simpler and not really any
anything slower or any longer than the
original originality lab a location
because we get the constant offset in
and we just select the T lab based on
this constant offset so for the real vm
implementers if you say well you have a
tea lab which is more less a bump point
to a location where you have a
threadlocal value of the current top of
the t left and the end which is more
Lester gives you the range of memory
that you still have available for a
location we really just have these data
structures replicated a couple of times
and we know how often to replicate it
because the static analysis also gives
us the maximum depth of the region stack
so instead of having vonti lab we're
having NT labs and this constant offset
just is the index into this T lip you
can see really a location is not going
to be any slower when you have
region-based a location so in summary
what do we have for our memory
management that's a little bit internals
of the substrate vm we decided to have a
chunk cheap anyway where you have memory
chunks of a fixed size one megabyte or
whatever chunk size you
decide to have and every chunk has its
own card table and off the table so that
you create so that we can do a
generational garbage collection and the
normal young generation consists of a
couple of these memory chunks but and
then we have the region chunks which are
are the same chunks but they are
maintained by this region enter and exit
and they are usually freed without the
garbage collector but as I said before
if we really have to do a garbage
collection then we just treat them as
part of the young generation so you say
that oh you have the an average region
sighs that's reasonably small something
in the area of kilobytes hundreds of
kilobytes maybe megabytes then you still
can locate a push and pop many regions
before you reach a normal young
generation limit of hundreds of
megabytes or gigabytes like you run
normally huh and you run on the hotspot
TM and yes of course we could do an
optimizations like doing a just
region-based garbage collector so we
could say well we have one region that
actually grew a lot and we want to do a
region local GC of just the region lots
of things that we could implement but we
haven't implemented yet how does it work
we don't have it working for a yet so
what we have it working for a spec jvp
2005 and i have comparison of the
performance results when running with
our normal garbage collected heap which
is a generational stop and copy where
affair 256 megabytes young generation
and I compare it to this hybrid region a
location where they have a spec chibi be
instrumented to use our region based
allocation and for those of you who
knows back gbb that's a very nice
transaction-based workload so it really
runs a couple of transactions over and
over again so there are only this six or
so in transaction methods that you have
to end up
with a region boundary and that's all
you need to do to to teach our system
about the region's inspect EVP and we
run it for a fixed number of
transactions to molest gated
deterministic number of executions and
we run a hundred thousands of
transactions and that means we we get
about 300,000 a region enters that's the
number of transactions that are executed
and the number of reach India locations
that's the number of region exits that
we really do without a garbage collector
disturbing us and you see a very few
garbage collections but we are still
region free in two hundred ninety nine
thousand two million 9999 thousand nine
hundred fifty-two so there's the few
garbage collections really don't matter
and the maximum stack depth is also very
reasonable that's only three and we
allocate a couple of gigabytes so fifty
six gigabytes in that case and when we
run it with our hybrid collection then
we can actually reach in a locate a bit
more than three-quarters of all the
memory which means seventy-five percent
of the memory is never seen by the
garbage collector it's just a freed by
the region exit and the maximum regions
text size is actually a half a megabyte
so the regions are really they're very
small especially for aspect gbb but we
can see then is that when the number of
garbage collection of course drops down
to about the quota because quite
obviously if we have three quarters of
the objects freed by the region memory
management and only one quad of the
object is in the garbage collected heap
which means we need only a Claddagh the
number of garbage collections which
means we need only for the nine
incremental cheese instead of the usual
219 which means we also don't need a
full pc for the whole run so we save a
lot of
time but the real interesting thing is
when we designed that the system we
didn't really design it in a sense that
we want to save a GC time what we really
want to do is of course avoid GC at all
or to be able to run with a smaller
young generation with smaller heap size
so the goal is not to reduce the number
of garbage collections the goal is
actually to remove reduced in the size
of the young generation so the to reduce
your memory footprint and you see that
quite nicely when you actually run with
various heap sizes so we have heap sizes
from one megabyte to 256 megabytes and
that's just the young generation size
the old generation size always has the
old generation always has the same size
and since they don't really do any old
generation GC there or cheesy old
generation size doesn't really matter
but what you see here is so that the top
lines are total execution times and the
lower lines are just the garbage
collection times what you see here is
that of course if you go to a smaller
young generation size then you spend
much more time in the garbage collector
and a very unreasonable size of one
megabyte for the young generation means
you spend really a lot of time in the
garbage collector but if you compare the
blue and the red line then since we are
only allocating a quarter of the objects
are in the young generation they can
wear less run with a quarter of the
young generation size and still get the
same performance so instead of running
if 256 megabytes young generation you
can run the 64 and you get the same
performance well if you run with 16
megabytes instead of 64 you get about
the same performance in terms of cheesy
so that's that's the real goal for that
so yeah it's working nicely for a spec
GBP and we are very close to getting it
running or also for our two more or less
allocate all the temporary objects the
growler locates to that in the region
based setting but as I said we of course
in
our substrate the end setting where we
have a closed world static analysis so
the option of obvious question is could
you do that also in the hotspot VMware
you have an open world setting and we
haven't done anything but I believe we
could do it and it's on the to-do list
of things to try but because you
actually don't need to do a static
analysis of your whole application you
only need to do a static analysis of the
region based code you can say well I
have a region enter method and I do a
static analysis that's open world just
starting from this region method and see
what is reachable and if you have stuff
like reflection like Jane I'm a source
like finalizer like weak references
whatever complicated features that Java
has you just say conservatively that's
all in the young generation so that's a
normal heap allocation and only the
things that you can nicely analyze and
statically guarantee that they are not
escaping that part these parts you can
really put in the in the region and a
garbage collector like chivonne which is
also already region-based is of course
I've quite a good fit for success in any
way so we hope to try it out on the
hotspot vm too okay so more details on
this hybrid approaches we had a paper
this year at is mmm so if you are
interested in all the details you can
look at the paper and read all the
details or you can talk to me over the
next two days and I'm happy to answer
any questions okay that's all that I had
are there any questions for now oh yes
this monkey is that one right so when
you ran with the hybrid am i right in
seeing that
no for Jesus yes that mollusk robster
and full kisses from 220 so I think we
have a old generation size of about 500
megabytes but if you look at the full
cheesy time yeah it drops from one
second to 20 seconds obviously but the
incremental cheesy times actually is
dominates the cheesy time anyway so most
of it is actually and if you look at
what what numbers get published for spec
jbb then usually people try to run it
with really huge young generations and
in the gigabyte range so too to avoid or
generation our generation collections as
much as possible which is when you know
from a positive number of folding sees
204 G seasons what what really is
happening is you haven't run the program
for long enough to get to a full JC so
my question is it kept running it to
where you know you actually got you know
all you see is there an effect on the
full GC time because and I see cliff is
nodding because ool did this experiment
10 years ago yeah with escape detection
where they managed to avoid a tremendous
number of allocations by helping them on
the stack but by detecting which objects
are prone to escape and what they found
was the exact same results here which is
you eliminate a lot of GCS but when you
hit one Floyd is it suck and so you get
to the point where you do many fewer GC
as you put off the time where you get to
a full GC for much longer but when you
get there you're so unhappy and I'll let
quipment finish my question yeah so so
good what I found was that is old GC was
so good at doing full G CDs that didn't
matter that I was allocating three
quarters this way and rid of them the
full GC for soldiers just remains
they're cheap it's not that's not the
issue and in
everything else just it was right I even
got about that much memory done with
stack allocation all using runtime
detection and then updating allocation
sites is to allocate Anna people the
stock instead of doing a static escape
analysis but roughly the same amount of
memory roughly the same amount of
savings in terms of Jung Jin gc's go
away full gc's would happen at a reduced
rate but it didn't matter whores all
because of the different GC technology
yeah well our old change GC is very
stupid so we have definitely a lot to
gain but what won a comment on that is
it's also about what use case do we have
and is that our main use case are these
truffle languages and their its if you
start up your Chi Ruby and where do we
start up you have all these compilations
going on you don't want to do any
collections bring this startup period
and if you implement your runtime
carefully that's hopefully you halt so
your runtime doesn't locate too much
garbage that's well as long living then
what you're more or less eliminating is
you're eliminating all these temporary
objects that just fill up your young
generation it more less bumps after your
use use memory size of the vm to
hundreds of megabytes yes you can very
cheaply de locate them so our young
jeezy of course for all this stuff is
also very efficient it only takes a few
it only takes milliseconds but you
really want we really want to get to a
completely moreless pc free setting so
it's it's valuable for us from that
point of view and yeah the comment is of
course writes that if you eliminate your
you're short lift objects then the old
generation fills up with the really hard
stuff and yeah your old generation
collection might take longer
in the app generation that objects was
nothing to collect so young generation
is a copy collector on the business to
live objects yes we have a lot of that
generation exactly but you still have to
take a safe point if you have a stop the
world collector and even if a
incremental collector you still usually
need a safe point you stop all threads
so you can say this let's definitely
still some overhead even if the actual
collection is is very fast yes no we are
not there there in in this there's my
slide so this 256 megabytes is the young
chain plus the region but since in that
case of our maximum region size is
actually half a megabyte it doesn't
really show up here so you can say in
our trunk tip that we have this ball
megabyte chunks that means that actually
12 with a maximum regent depth of 3 you
can say make a maximum of three min of
three megabytes are actually for the
region and then if you go really to a
one megabyte then where you have to
collect more or less after every region
nearly anyway because you have a bond
megabyte reaching size which doesn't
leave you much more for young objects so
no we are not not treating in a debate
as they give the system more memory yes
no regions are all single threaded
because which which means that in some
sense this region abstraction is also a
threadlocal memory abstraction so the
static analysis if you say your run
method of the threads is a region
boundary then you find the static
analysis finds you all objects or
locations I
they do not escape the thread so in that
sense you automatically get a
threadlocal a location and thread Locker
more less garbage collection if you want
it yes about 10 years ago I worked a lot
with real time job for a hard real-time
system and back then was a son real-time
Java system I think it's been
discontinued sense but there is a
Stanwell Park escape and that has an
actual API region allocation and so what
line said that eventually if you're just
running any Java program so it
eventually get a full GC which can be
more terrible as terrible as any other
case so what real-time Java does it says
that for specific threads that do region
allocation at least for them you know
that they will never be caused by
folding seat so if latency critical
threads you say this is a non heat
reddit and you know that red no matter
how long the GC pause is that fred is
never going to get caught so at least I
mean of course it complicates a program
or at least you know that if you have
some lazy sensitive code that's always
going to be responsive no matter how bad
yeah but we are not targeting any real
time setting because that's the even if
you have is read that would be
completely reaching allocative and
everything that's red do is doing is in
the region another thread would still
fill up the young generation trigger HTC
and block your that's red so we are not
solving any real time problem and yeah
but we're also not making the
programming model more complicated so in
real time in Java if you really want
this real-time guarantees then you as a
program I have to to do a lot to start
this region's make sure that you
allocate everything in the region and we
just say well that's a region start and
hopefully you'd pick a good region start
because if you pick
bed region start them static analysis
might say well there's nothing I can do
in this region but if you pick a
reasonably good vision starts then you
get a lot of benefits automatically
without thinking much more so the vm
work going for four simple use and but
less guarantees yes how expensive is the
region enter the region enter is very
cheap because you just have to increment
your Regent depth but the region exit is
what is expensive because the region
exit has to go through all the allocated
chunks and return them so the region
exit has to go through your trunk list
and hand them back to a global free list
but it's also reasonably cheap in the
sense that you need to iterate the
couple of linked lists and as the chunks
to another linked lists but you have to
be careful to have regions that are not
too fine granular so via our model is
you want I think something in the area
of hundreds of kilobytes tens of
kilobytes at least allocated in a region
the goal is not that you more or less
annotate a leaf method as a region
because it locates two or three objects
that would be too fine granola and if
you have such a case that would also
likely be handled by escape analysis so
we definitely want something that you
have objects like compiler intermediate
representation objects that live for the
whole compilation or objects that live
for is to a compilation face so you say
well I built up a temporary map with
information that my compiler
optimization phase needs it's clear that
escape analysis cannot handle that
because it's a complex object graph and
it lives four milliseconds for many
method course but it's also clear
doesn't live
for an extended amount of time so that
that's the model that we are going for
yes original just had a single region
when you compile code you allocated
memory in da one region and then free
the region at the end of the compilation
yes yeah you can say that that's your
out the most region of in a compiler
just one region well as soon as we have
food running for for the compiler then
we will certainly measure that but from
our knowledge about growl is we know
that we have a couple of phases like the
register locator which also allocate a
lot of temporary object so it makes
sense to have to have nested regions yes
so cash cash improvements are definitely
something that data it's on the list
that a web server doing transactions and
jev and in to the extent that you kept
it fitting within the l2 it showed up on
performance and that he was always
turning the same yes if you fill it what
happens if you feel a region that
depends on how do you define filler
region and currently we don't have a
maximum size of a region we only have a
maximum size of the young generation so
that means in if we assume we have a 256
megabytes young generation and you have
one hundred percent a location rate into
the region then you can you could really
have a region with 256 megabytes and
then you say well the region is full and
we do a garbage collection in reality it
means that the
the normal young generation grows and
grows grows grows but in the meantime
your regions get pushed and pop and so
memory gets freed and as soon as yeah
the young generation reaches the maximum
size then you do a garbage collection
but you always have the iphone think
that simple answer your question is do a
garbage collection yes so yes so first
we rely on the quality of your region
boundaries that you placed and what we
can give you is we can give you steady
clearly a list of these allocation sites
can be placed in this region and the nme
Kelly then at the end of every
application runs we can tell you how
much memory got really dynamically
allocated in every region and from that
you can get feedback of 0 is the region
placement actually useful or am i doing
something wrong and second question is
can you automatically infer the region
placement yes it should be possible we
haven't really looked into it but what
you're more less need to do is
conceptually you could say you treat
every method as a region start and then
find places they actually many region
exits more or less end up in so that way
you say well you have many many rigid
starts but it turns out that oh yeah at
the end of the computation is the place
where all how long all these objects
live and then you can more or less
reconcile that oh I only need one starts
then too so you can say from the static
analysis first starts with this am
that you have an unbounded number of
regions and then you look at the result
and see we are actually the objects then
freed and that these are then the
boundaries where you also start the
region so that would be a way to do it
automatically but for now the manual
thing works well enough yes of course
there are some challenges and the
biggest challenges always is this son
mis unsafe stuff where you right into
objects and the static analysis doesn't
really know where your writing and what
you're writing and so since Morales
probably every bigger piece of software
uses some sort of unsafe operations we
have to handle that so that's the piece
that's currently missing so we need to
treat unsafe in a sense that we we don't
know exactly where you're writing but we
know you're writing into an object that
isn't this region so that we can say
well then the objects can stay within
the region yeah one last question I
think before we run out of time
problem with two fine-grained region is
that you have to do something when you
exit the region and if you have to
traverse a couple of linked lists and
return chunks to a free list and you do
that on the two fine fine grained level
Rach I only have for example one object
in the chunk then it means you do this
for one object allocation which more
less is more expensive than the object
allocation was so this cost of exiting
the region of returning the junks need
to be amortized over thousands of our
locations and then it doesn't matter for
static analysis no the static analysis
doesn't really care how many regions you
have and how fine-grained they're like
every context sensitive analysis you
have to be careful with yeah how deep
your nesting is but that's careful
implementation of the static analysis to
solve that okay then thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>