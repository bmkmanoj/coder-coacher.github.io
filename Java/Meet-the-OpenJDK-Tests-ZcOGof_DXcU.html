<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Meet the OpenJDK Tests | Coder Coacher - Coaching Coders</title><meta content="Meet the OpenJDK Tests - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Meet the OpenJDK Tests</b></h2><h5 class="post__date">2013-05-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZcOGof_DXcU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay and good morning my name is Alan
Bateman I'm working in the JDK team in
Oracle and this set of slides in this
presentation is about the tests that are
in openjdk it's called meet the openjdk
test because it's very much introduction
to the test that we have in in openjdk
at this point and I want to go through
and explain to people where the test
live what the tests do at a high level
how they're wrong and a few other little
things that that are interesting about
the tests okay so will we get started by
talking about where the tests actually
live so the JDK 8 and we're talking
solely about jdk in these slides and
consists of eight repositories the top
level and repository and then seven
others with the vm libraries and
compiler and other code so when you look
at the eight repositories you will find
that there are tests in only four of
them in the hotspot repository you'll
find some tests for hot spot on the vm
in the jdk repository you'll find the
tests for libraries and EP is and many
other things in the language depository
we have tests for the java c compiler
and other line tunes ap is a recent
addition to JT cake is the dinaswyn
JavaScript engine and that is in its own
repository and comes along with its own
tests so eight repositories and testing
for those apposite orys clearly there's
tests missing from other repositories
and we talk about that as we go through
the slides okay so here's a quick just
inventory just to give folks an idea of
what tests are I'm in terms of numbers
we have this snapshot is based on JDK 8
build 82 at the time of this recording
we're up to build 88 so the test numbers
have moved on a bit since then but this
is just it gives you a good snapshot us
as to where we were at this particular
point in time so in the hotspot
repository there's there is for this
particular bills 290 tests in the line
tools repositories 2700 a GD k
repository 4850 and with a comment there
to point out that there's 50 to 100
excluded tests and i'll talk about those
as to what excluded tests are about
later on in the slides we also have been
a swan chests and and did they have
about 750 tests at this time and they
run in a slightly different way the ant
and and we talked about how tests run
later on now one thing just to say is
the test counts on this slide are
completely meaningless what what we
talked about for test count is does not
correlate with desertion count or
anything like that we can think of this
almost as a test source files rather
than broader than assertions so how the
tests organized and so the organization
is it depends on the repository which
for the hotspot repository at the high
level tests are organized by area and so
you if when you look in the testtree
you'll see that there are the tests
there are tests therefore for the
runtime area for the garbage collector
for the compiler for the serviceability
and so on and within that then they tend
to get organized by bugs the line tools
repositories at a high level is also
organized by my area and then under the
Java Sea tree in particular which is
where the majority of the tests are they
tend to be organized by books as well
the JDK repository is is at least on
first look it looks a little bit more
organized and because everything is
organized by my api api package and
however when you look at the detail
you'll actually see that some areas are
better organized than others and
sometimes it's not obvious where to put
tests and often we have discussions and
you'll see that on somebody openjdk
lists and periodically where there's
fusion sometimes as to where to put a
test because it's not obvious we talked
a bit about that later on in the slides
another thing to say about the test is
they are not precompiled so this might
be different from from what people are
used to seeing before and in the line
tools are positively then you're
primarily testing a compiler so you're
compiling from source code into class
files so you don't want the test
pre-compiled in the hot spot and JT in
jdk repositories then and we are
compiling as we go what and people often
wonder what the overhead of that is and
it's been measured so many times now I
think it's been concluded for a long
time but the compilation overhead during
the test execution is not significant
and clearly the lot a lot of tests in
those areas you think could be free
compiled and that is true but because
it's because the overhead is so
insignificant we don't bother doing that
okay so what are the tests written in
now this is this is an interesting thing
so the tests themselves are primarily
written in the Java language themselves
and most of the tests are standalone
tests meaning caveman public static void
main tests and they're also typically in
the unnamed package so that's a little
bit different maybe from what people are
actually used to and that's the
historical reasons for this one of the
advantages of the testing standalone and
runnin is that it makes them really
really easy to debug outside of the test
harness and because you don't require to
have any kind of setup a configuration
to run this test this is very useful if
you need to SSH into a machine for
example and to run the test on the
command line and you don't require any
setup or any test harness so that's
really really useful that way now
although the tests are standalone there
has been a trend in recent time to make
use of libraries I'll talk a bit about
libraries later on in this presentation
and that come makes it a little bit more
complicated to to run the test of course
because there's a little bit more setup
and we have within the testtree a number
of MJ unit tests they're very small and
and one of the things that needs to be
done at some point is get those
converted to something else testing g
probably and a lot of the newer tests
that are going in particularly in jdk 8
are using the testing g framework and
both individual tests these are just
individual source files making use of
testing g and also blocks of tests and
particularly the jsr 310 the date and
time and tests and also the lambda tests
are all using testing g and they're in
blocks of test organized in a tree of
purely testing g tests and so more and
more will see it test engine test being
added to the jdk repository now most
part s are written in the java in the
Java language we also have a number of
shell tests shell test being and tests
that you need to to be run and I via the
shell and the test harness will look
after that but they're tested typically
we do some special setup in the
environment before running the test so
if you want to test the run with a
special you limit or you wanted to set
up other other variables for the test or
set up some particular odd scenario or
condition the poor for the tests to run
and they will stable sometimes we've
written Michelle tests and now one thing
to say and we've another flight on this
later on is is that she'll tests have a
cost and there's that there's a
maintenance costs associated with them
there's also other issues with which
I'll test that we'll talk about later on
the common thing with all of the tests
is that they're wrong in via the JT reg
test execution harness and and there
thurs there's a complement slide deck to
this that goes through and introducing
JT reg how to write tests and that that
and some else will will have been
present ok let's talk about the
tests that are in there so and one thing
to say is is that a lot of our tests
would be what we would regard as
regression tests and many of those
regression tests would be almost stress
tests now when I talk about regression
tests what I'm talking about is is a
test that's testing some particular
scenario or bug that has now been fixed
and so when we fix a bug and anywhere
within within the within the JDK then
the default rule is that you must bring
a test to actually demonstrate the bug
and then prove that the the bug has been
fixed so the test will typically set up
the conditions to ensure that that which
we're testing for that particular bug
and but by having it in the test suite
it ensures that that particular bug
never comes back sometimes it's very
very hard to write a regression test and
particularly writing a regression test
that is both reliable and also runs very
fast so sometimes what happens is that
you write tests but because it's so
complicated to set up the scenario or it
becomes maybe a maybe too fragile to
demonstrate that scenario that sometimes
is it it doesn't make sense to add the
test or sometimes that it comes back to
bite you because the test is not
reliable going forward and but we have
to be pragmatic about these things
sometimes and em and try not to and try
not to enforce this rule all of the time
so for example if you're testing a for a
particular crash that takes 48 hours to
duplicate then it's point is trying to
put a test for that into the into the
JDK test because nobody's going to run
that in their local environment because
I just don't have the cycles to be to be
running the test for that amount amount
of time they want folks want to be able
to run tests in minutes not days and
there's other other scenarios where it's
just way too complicated to set them up
that require special configuration on
the machine that you can't do in an
automated way so there you have
be pragmatic sometimes but the default
rule is is bring a test when you're when
you're fixing a bug and we call these
regression tests and many of the tests
in the Indian the JDK repository in
particular are what we call regression
tests and they're also tend to be stress
tests in some cases because and the
particular bug or issue that they're
that they were created to test only
arises under very stressful and
conditions all feature work then that's
going into the JDK in addition to bug
fixes requires tests and now this wasn't
always the case in the early days of the
j th s the feature work would go into
the JDK repository or equivalent without
the test the test would go somewhere
else but in recent years the in region
releasing releases is the tenth and
going with the features so these would
be what people would traffic leak all
unit tests and they're testing the API
or the feature or whatever and now what
I can what I should say is this is that
although they are unit tests they may
not be using a standard test framework
like test engineer or r RG Eunice
they're often written in styles that are
unique to that particular area and
that's actually one of the interesting
our challenging points about the tits
and the jdk repository in particular is
there is varied style in there just
because of the breadth of the java jdk
and unit unit s force for a new API is
going in test tend to be and test ng
based at least at least in very recent
times and I've touched on this jdk is
big and as a wide variety of tests and
the result is that the frameworks have
actually gives up in particular areas
that tend to help writing tests in
particular areas so there's areas for
example jdi to java debug interface or
our am i where you're you're trying to
test multiple vm so you've got the vm
running and you've also got the are my
registry maybe one or two other
processes running as well are the VMS i
should say in order to coordinate those
and
startup and shutdown of these tests
frameworks tend to build up around them
and so you will see this in many areas
of the JDK and so they said testing g is
what we've been moving to for some of
the newer tests and what many many of
the existing tests because they've been
there for some time are not using a
standard framework ok so just a few
other points about the types of tests
and I think for the most part you and we
will see we could say is is the tests
are black bot tests there and they're
testing particular api's and sometimes
they're testing JD case jdk specific
api's and not just java SE AAP is and
sometimes they're testing implementation
highly implementation specific API such
as on Death Star api's and another point
about the tests is that they're often
testing behavior that is defined well
beyond the API specs and service
provider specifics for example or very
much implementation specific behavior
you can see that in a lot of a lot of
the tests and in the in the hotspot
repository and there's a they're
starting to appear a number of our white
box tests so there's a white box API now
I'm for those tests and the line tools
repository which is been leading the way
in a number of different areas and has
both white box and and black box testing
and as John Gibbons call them great
great has gray box tests as well all
right
bit about test coverage okay so I mean I
think one of the first slides we wish I
showed you that it was tested only four
of the repositories there's the other
four where we do not have tests are
things there are areas such as the jacks
p jax-ws koerber and repositories so
these are these are areas where for
historical reasons the tests were
developed elsewhere other other
repositories and didn't migrate into
OpenJDK and so that means that if you
were to generate test coverage data for
the test sir and openjdk then you may
spot some holes and these are some
examples that are very very obvious so
the XML parsing API is the web services
API is Corbett jndi jdbc and so on
there's a number of these areas where
where the tests do not exist in OpenJDK
so that creates an opportunity for folks
if they want to contribute to test and
in those areas now when I say that there
are tests that we don't have tests in
OpenJDK for those for those areas it's
important not to conclude that they are
never tested they are of course tested
Oracle and other partners and licensees
all of course will have to have tests in
these areas as well as just they have
not been opened and and made available
in OpenJDK and so areas will just a
couple other points about air else out
there if the two of tests is is that in
general is the in terms of test coverage
is the line tool suppository has been
setting the bar there in terms of the
test coverage and the quality of the
tests and in the jdk repository which is
where the api is and libraries are then
there is some really good coverage in
some of the newer api areas where there
has been a conscientious effort to
develop and open unit tests and as part
of the part of the development and and
there's also reasonable coverage in
areas that have a lot of regression
tests so there's many areas many many
many API areas that and that are the
relatively mature
and what they didn't have unit tests
when when they were developed and pushed
into the JDK repository now over time
those areas would have ever have
obtained a lot of regression tests that
cover all the bug fixing that was done
over the subsequent years and releases
but they don't have the unit tests that
you might expect and forth for those Oh
guys the hotspot repository is is has
only a small number of tests and it's
very very clear that if you're touching
code in the vm area then you need a lot
of tests and so it's it's clear that the
tests are in the hospital repositories
at the moment are very insufficient if
you were working in that area and it
would be nice to publish some test
coverage data and there's also tools out
there that people could actually they
want to and get more details as to what
the exact test coverage is particular
areas okay so how are the test run I
mentioned in one of the slides earlier
that we use jg reg test harness and and
the reasons for that are covered in
another another presentation but just to
say is is that that the JT rig existed
long before some of the standard chest
harness is that the stash I better used
by people today and so there's there's
historical reasons why we're actually
using using this openjdk specific m test
runner JT reg itself is very very easy
to use when you're used to the command
line and it's just a single command to
run that run from the tests and there's
a whole slew of options that you can do
to configure things how they run and so
on and which tests are run the hardest
point pop the hardest part of running
tests and particularly within the jdk
repository when there's so many tests is
knowing which tests to run so if you're
working in something in the security
area well which tests to do a run when
do they do they provide to the tests for
the api's and i'm working on or do i
need to be running tests in other areas
as well for for example and
so and when you're working when you're
when you're working on the jdk echo then
it's important that you have have some
knowledge where the tests are for that
area so that you can work efficiently
and run the tests us are applicable to
that area in addition to running the
running the tests via JT rig then then
we also have a make file targets set up
and in the build and so at the top level
may test will run tests and are you can
specify particular areas where we have
test targets that translate our map to
test for particular areas and some
people do working on the JDK do use the
makefile targets but it's problem was
primarily set up this way to support
builded test systems and particularly
within with an article where where the
build and test system will run the tests
be in make targets and the useful thing
about that is is allowed for and
partitioning of the tests across
multiple machines so when the tests are
split up into this 20 or so make targets
there then they can run concurrently on
different machines to get through the
test faster and some of some of the work
around the make targets is still ongoing
and is this a bit of cleanup there I
think to make it a bit more accessible
accessible to people and much more
obvious as well so that that is clear
which make targets and how that and what
the right ones are for for the area that
you're working in so how long do the
tests take and there I'm not going to
answer that question in this slide
because it depends very much and how you
run the test and which tests you
actually run and some tests and are
slower than others some some areas have
a lot more tests and lot more stress
tests to take a long time than other
areas the there is a complement to this
presentation that will go through GT rig
that will cover a lot of the end what we
call execution modes and that will
explain how to run the tests in
in more efficiently and the main the
main point though is is is that if
you've got the memory and you've got and
you've got em the right hardware then
you can get through the tests and much
quicker then then might be initially
apparent and because the test can
actually be set up to run at least so in
most areas can be set up to run in a
whole pool of VMs running on on a big
machine with lots of memory and lots of
course and you can get through the test
very very quickly and this is it this is
this is very much the the the the most
efficient way to get through the tests
and for the the non client Finton on GUI
tests if you like because they there
they are set up to and be able to run in
this mode okay now this is an
interesting topic how how stable are the
tests and well the goal is very simple
all tests should pass on all platforms
all of the time and the case of the line
tools test which is Java C Java age and
so on the line tools api's then the test
for those areas have a very good
reputation and they usually meet this
goal they always pass and which is which
is really really good we have let we it
left so under jdk repository and where
most of the tests live and the reason
for that is is is that we have a
long-standing issue would test that path
that fail intermittently and that might
sound scary when you hear it first but
when we go through the reasons for that
it may become a little bit clearer so
the JDK is very very broad we've API is
covering a huge number of areas and one
reason what tests can fail and
periodically is is because they're just
overly sensitive to the machine
configuration the networking test is an
area there that I would point out that
that is really really sensitive so and
there's some wacky networking
configuration setup and it causes test
to fail because o interfaces are plumbed
with with with strange addresses or
there's some other configuration related
to dns or proxies or any number of
others others
can cause tests to fail it's really
really hard to develop tests in that
area that are robust enough to be able
to detect that they've got awards got
some strange configuration and and it's
so that's just what with just one
example and we've this kind of thing
although all over the JDK it's really
really hard sometimes to write tests
that and that have a lot of concurrency
in there and if bogut bugs can slip into
the test and they only surface under
specific lenders on specific machines on
under specific load and really really
hard so you might run this particular
test might run several thousand times
might be running for years and then you
get some random failure on some
particular machine after after some
upgrade or adding new hardware or
whatever and it's it's possibly the
first occurrence of a test failure above
a failure with this particular test and
those kind of things could be really
really hard you often have to go work
through whatever is left output of the
test and try and figure out why why did
it fail in addition we're not we're not
standing on a on a solid platform and
the tests are exists along with a
changing vm changing library changing
everything so everything is moving as as
as these tests are running and bugs in
etive lately creep in especially when
you're working on a major relief with
release where there's significant
changes and a lot of code turn going on
there can be a lot of lot of em and bugs
in the system if you like as as as as as
the release is being developed and they
can cause intermittent failures we also
have operating system bugs and it's it's
it's not unheard of to to have and
crypto chest failing because the native
crypto libraries how about have a bug or
there could be any number of other
reasons of other operating system bugs
that can actually cause problems we also
get
have potential for interference between
tests so when we talk about execution
modes and in the other presentation and
often you're running tests in the same
vm as the previous test so rather than
starting up a new vm for every test
you're running you're running the tests
sequentially in the same vm are in a
pool of the ends what can happen is is a
test can leave things behind it can
leave it can change some global settings
that cause problems for the test that
runs ultimately not necessarily the same
test the the exact subsequent tests it
could be mini-me test later where the
problem actually surfaces those kind of
problems are very very hard to find it's
possible to get interference and between
tests that are running in different VMS
you've got a pool of VMs running and
something changes one test changes
something on the machine configuration
and and it's not properly synchronized
with and some other tests running in a
different vm then you can get
interference and those kind of problems
are really really hard to find it's also
possible and this is this is totally
bizarre but it is possible in some areas
to get interference between tests that
are running on completely different
machines the what one example that we
have to track down one time was an
interference between multicast tests so
in in Iraq of machines you've got and
the networking tests running at the same
time and they're using multicast and
what happens what can happen is is that
one test can actually receive something
that's actually sent by another test on
another machine on the network and it
can cause testa fail so identifying
those problems is really really hard and
and fixing those problems is not as not
as hard as identifying them is where
that were the big issue is and what you
can see when you add up all of these
type of problems together is is and with
the number of the number of platforms in
10 times the tests are run the potential
for for intermittent test failures is
high and it's a very very difficult
problem and the good thing is this is a
lot of progress has been made over the
last two-plus years those that have been
watching the
and the commits to m2m OpenJDK over the
last few years we'll have seen a huge
amount of fixed two tests and
significant improvements to tests going
by but there's a lot of work to do and
one thing that people often often
describe the process of keeping test
reliable is is it's a game of
whack-a-mole and there's personal
signups after of the mole surrendering
because there's a lot of other things
that people can say about this as well
some people would actually say the moles
are armed and it is a hard problem and
and having more people running the tests
and having having and more people
looking at the tests and making them
more reliable and would go a long way I
think okay moving on to when the tests
go bad and test you go bad so we often
have to exclude tests and the and as
many major reasons for this it may be
that fixings a test problem is is found
what the fix for is not imminent and if
it can't be fixing the time you matter
then we sometimes have to exclude tests
it may be that the fix for particular
issue is coming via different routes so
we're waiting for a vm fix to come in or
some other other other than geist achill
them reason like that and sometimes and
changes in one area can cause chest in
another area and and it may take a bit
of time to actually track down what that
is so there's many many reasons exclude
tests and and within OpenJDK we use
these two ways to and exclude tests that
one is we're the Attic nortec so tests
have the app when they have the attic
door tag then they're just ignored by by
JT rig by default it marks them as error
and you can pass options in there
together to not report them as errors in
the JDK repository then we have a
separate mechanism which is an exclude
list and the reason for that is because
the exclude list is more power
full and not ignore it allows us to
exclude tests on a per platform basis so
we might have a test that's only only
unreliable or problematic on windows so
we can exclude it only on that platform
until the issue gets fixed and so at any
one point in time there there there will
be tests that's but that are excluded
and there needs to be an ongoing effort
to to liberate tests and liberate test
meaning room fixing the issue and
removing them from from the exclude list
so folks that are watching the the
commits on JDK 8 will we'll see
periodically and happens almost every
week test being added and removed from
the problem that's it is constant turn
on that ok shell test I talked to a
shell tests earlier on the presentation
and mentioned a few issues and i just
want to mention a few more issues about
shell test now in general shell tests
and what cover shelter sam and i'm
typically talking about and bourne shell
tests is there typically slower than
equivalent java tests and but in
particular they're slower when you're
running with in tests in the same vm are
in a pool of VMS and the reason for that
is because they're needing to start up
at the end / test so you're right the
test harness is running the shell test
and then the shell test itself then
we'll typically and spawn our start up
one or two vm so they're slower because
of that additional start up and they're
also maintenance problem every time we
add a new platform we need to modify the
shell tips folks might remember when we
when we added support for Mac then there
was several hundred shell tests that
need to be updated in order to work and
on the mac and on windows we use an
cygwin and software and as the shell and
there's a whole bunch of issues and
differences between versions of cygwin
that cause a lot of problems and it's
really really hard to keep all of those
versions and i keep tests working on all
those versions at the same time
sometimes we shall tests you're not
testing
you think you're testing that's because
the shell test may not be passing the vm
options through consistently so we might
be you might be trying to run the tests
with the client vm but if the vm options
aren't being passed through when you're
on the service class machine it may
actually using the server vm remember
client vm so it's really really hard to
diagnose those those issues we fixed
several hundred of those in 2012 and but
it's possible that she'll test says have
changed since then and and and issues
like this me have come back really hard
to diagnose and she'll tests are not
always reliable and on many occasions
we've found shell tests that were
passing but when you look at the details
in the log you realize that some
intermediate step in the shell and
script was actually failing and and it's
really really hard sometimes to write a
shell chest that that is really really
reliable so a wonderful opportunity for
people is to go in and replace as many
of the shell test as possible I think
long term we need we need to do this
great effort has been made in the line
tools repository over the last few
months to remove and replace shell tests
and we need to do the same for for the
JDK repository and what opportunity for
someone if they're in tricity is too and
create a library for the things that
she'll test need to do and write it as a
pure Java library and we we have we have
API is in the platform now and for
several releases that means that many of
the older shell tests can be replaced
some of the rationales are on the
reasons for why shell tests were
introduced several years ago is because
it wasn't possible to do some of these
things in pure Java now it is possible
so those shell tests can be replaced
okay so fixing tests and so and when you
and on the last slide I talked about
replacing shell tests and and and and
I'll in previous slides I talked about
the problem less than our excuse list
and these are opportunities for fixing
test liberating test from the exclude
list or a play
tezz are just fixing buggy tests when
you're fixing a tests are modifying a
test it's really important to understand
what that test was created far and on
the top of each test there are typically
two tags in there in that bug and an apt
summary tag and that have links to
useful information the bug tag is the
bug number and on books of some com
where you can actually find out if you
can and the original issue and summary
then is is a description of what the
test is as far now sometimes the summary
is insufficient to explain how that what
the test is doing but it provides you at
least some crumbs the really important
thing is when seeking a test that you
don't change the test so that it's no
longer testing what it was designed to
test its really really easy to go in and
modify a test and it no longer tests
what I was originally designed for and
you got to watch out for that and okay
so couple other points just to say about
Tess's is so what 11 we fix test so we
add new tests then they regularly need
time to to bed in sometimes when when
tests are changed or new tests are added
it could be a few days or a few weeks
before you realize oh this test actually
fails only on lenox and only were under
certain conditions and I never spotted
that in my own environment before I push
the test but now I understand why so
there can be a whole bunch of reasons
that test fail and so often for test
particularly tests that are actually and
try out exercising api's or features
that have platform or operating system
specific implementations and are
sensitive to certain configuration you
need to test them on a wide variety of
platforms before you push them and just
because and there they may cause
problems later on ok so when adding new
test so I talked about or earlier in the
slide we add new tests when writing new
features we had
test when we're fixing bugs and the rule
is that when you're changing something
adding or fixing something then you try
to include tests that have coverage for
the area that you're that you're
changing sometimes it's not possible I
think I mentioned earlier on of about
and fixing a bug that takes 48 hours to
to reproduce its it wouldn't make sense
to include an automated test for that
that's something for elsewhere and when
you're adding tests and just be aware
that that the maybe test for this area
already so what it may actually be
better in some cases to add additional
coverage to an existing test squadron
duplicating and are right in completely
new tests it's important when adding and
modifying are adding tests that they be
killed reviewed to the same standard as
the JDK changes so folks that are
watching the OpenJDK lists will see that
we do when we're changing something we
have a code review we're discussing the
changes on the mailing lists and the
same quality of review is required for
the tests in order to keep the tests
high quality and high quality tests mean
a high quality JDK okay so where do you
discuss test issues so and when tests
are tend to be associated with the area
that the test was created far so you're
changing something in the libraries area
then the best man us to use for that is
the cords disk you're changing so the
core libs dev mailing list changing
something security Ares you copy the the
the security dev mailing list and so on
so typically you're you're using the
mailing list for the area where that
where the test belongs there is also an
OpenJDK a general quality mailing list
called the quality discussed list and
this has been there since the beginning
of OpenJDK and what at this point did
the number of subscribers is is low and
that needs increase with a lot more
interest
and in the community on on quality right
so where do we go where we now and well
clearly am from from the slides is that
the test that we have an open JDK are
not sufficient and for in the area of
unit tests for example is unit testing
to or not don't do not exist for many
areas that they tend to be only in areas
that more and more recent because that's
just historical reasons for that there's
also a lot of other tests that are that
are not present in OpenJDK folks are
testing out the build typically want to
be able to do smoke tests after they're
built to make to give them at least some
assurance that the build is successful
and they haven't broken anything
significantly and so smoke test would be
clearly something that will be would be
interesting to add and we don't have a
repository in OpenJDK for performance
tests or micro benchmarks and we do have
performance tests that people add into
the into the into the GDK history that
are not wrong automatically and what it
would be nice if there was some place
two and two to push micro benchmarks in
particular because there's always
there's always the following fixes and
performance changes going on and we want
to regress there and there isn't there
isn't big stress test a long-running
basher chests in OpenJDK that's that's a
whole big other area that that and that
could do with attention and there's also
many chess many areas that do not have
tests I I highlighted a couple of those
and more more obvious ones and another
thing is is that openjdk isn't em
doesn't yet have a community focused on
quality and we need to change that we
need to build up a lot more more
interesting in quality of the quality
side of the JDK so that and so that more
people can actually contribute to that
and
another another thing and this is really
important is that M irrespective of any
test that we have in openjdk are are in
or in Oracle's test suite are other
companies test wheezes is that often the
best tests are not unit tests for the
JDK themselves but rather real-world
applications and libraries and and often
folks testing out preview bills or doing
bills themselves and running their the
test for their own application or
library and they're often the best tests
and bug reports time your bug reports i
should say are often much more valuable
than contributing and bug fixes
themselves and because a timely bug
report or good bug report means that to
the right mailing list or into a bug
database or whatever means that that
folks can season it folks that are
experts in that particular area can
identify and fix the issue in it in a
timely manner okay so this is the end of
the presentation it was intended to give
people an overview of the the test in
OpenJDK and and how old tests are on the
type of test and so on and if folks have
questions and are interested in this
topic then please follow up on the
quality viscose OpenJDK list okay
goodbye for now</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>