<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ahead Of Time (AOT)  Internals with Vladimir Kozlov and Igor Veresov | Coder Coacher - Coaching Coders</title><meta content="Ahead Of Time (AOT)  Internals with Vladimir Kozlov and Igor Veresov - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ahead Of Time (AOT)  Internals with Vladimir Kozlov and Igor Veresov</b></h2><h5 class="post__date">2017-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n5DCg6M2MDM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Reggie Merkel's loaf and here you
girls of we are from hotspot compiler
group this presentation is about ahead
of time job a compilation which was
introduced at a new experimental feature
in gdk9 we will present you serial growl
changes phases of UT compilation and let
us performance results 5 main motivation
for G is essentially provide faster top
for Java based JIT compiler which we are
playing to replace in the future see
tools currently we were using growl for
the head
it also helps preach picture-phone
faster because you skipped in georgia
tyranny executed compile it code EG code
could be shared because conceptions
read-only and in shared libraries and
coda cache size could be reduced
especially when you have tight ejek
compiled code which don't need don't
trigger compilation so you can use
smaller the hip four-digit code new GT c
tool for UT completion was a lead to
genic installation is used growl a java
compiler code generator it's only
available on linux existing foreign gdk9
where ug supported and he is examples
instruction how to use it
essentially it spits 5 model names list
of model names classes or jar files and
you also can add compiled command file
where you filter out its file explorer
to compile only methods
two types of fatigue world could be
generated depending on compiled fatigue
flag by default not yield code similar
to what the one generating client
regenerated is it best to use as it
doesn't get profile collecting code so
it's best used for short running
application and good for start-up and
second type is deals gold which similar
to what teal to see one generated
profile invocation back branches it
should be used if you want to triggered
high performance it recompilation to
reach peak performance currently there
is limitation how you should be a eg
code should be use the same run Java VM
runtime configuration cataleya
configuration should be used during Eid
compilation and execution it is because
when you do it ET compilation Iranian GB
Yemen used runtime for particularly
system you're running on to generate to
use instruction available this machine
and also called specific to for example
use GC so when we execute Aug code you
have to use the same flags and the same
setup code we're set up otherwise for
example you can generate code which use
a weak instruction but when we try to
run at another machine which wk with you
crash in the future we want to introduce
eg cross compilation ways with five
targeted
runtime consideration and also since we
are in the job update you have to
recompile unity libraries for the same
reason
that rat to waste to load 30 library it
specify using the AG library flock and
another one we ET aware of the
well-known libraries installed in Java
installation Lib directory it look for
particular names which contains
following modern names you're currently
using schemes we'll have to compile and
create this library installed and
another one you teachers looking for
well known librarian giant pollution
directory and only their own libraries
its final flux so if you want to some to
test new UT library with the same
classes installed one you have to remove
installer to run first
it also recognized some name runtime
configuration coded in library names
there is for configuration which fits 5g
one opera little GC used or complexity
loops used or not so all these four
configurations and a century with
Angelique in nine we support only
parallel G C and Java actually a Chris
Salinger tried CMS in Twitter and it
works too so
more detailed eg when UT library loaded
code is not moving Code section digital
or digital library treated as additional
code Akash hips and when we traverse
compiler that we do it the same load
with with reverse roll the skoda hips if
you could follow the same invocation the
optimization and loading rules as normal
JIT compiler and we may not remove it
you I mean the rink is from method
descriptor to the AG code separate field
we may keep it even when we switch to GT
code because when we do my zip record
instead of going back to interpreter we
want to go to your ticket instead but
there are cases for example glossary
definitions or some other cases where we
cannot use anymore jakotsu a marketer's
not when terms we have seen the print
recorded for each ad class and we
compared where class is loaded and if it
does not match we don't use this class
so now we talk about Gao changes hello
so in order to make Braille work as a
general purpose ility compiler we had to
make a few changes to handle constants
class initialization add to your
compilation support and do some tweaks
to the aligning policy the biggest
difference between the code that is
generated by a JIT and the code that
comes out of the a 80 compiler is how
they handle constants jets are usually
able to just embed constants into the
code with UT we want the code to be
mutable in order to be shareable so for
runtime constants that change from run
to run
we cannot seem to attach the code we
have to use in directions
at a high level there are three kinds of
consonants that we have to deal with
there are performance sensitive
constants that actually can be embedded
into the code and speculatively assumed
to be never changing those are things
such as field offsets GC barrier choices
etc we validate those constants one
below the library we depend on class
fingerprinting keep layout parameters
command line flags basically the state
of the VM on to verify those then there
are global constants of things such as
heap top and pointers code table based
pointers etc those are eagerly
initialized when we load the library we
abstracted them away with the new nodes
that basically folds into constant and
jitmoud as you would expect and is
lowered into an indirect load in a UT
and then there are local constants
things such as classes objects method
counters that we infer to your
compilation to handle local constants we
use automatic constant replacement which
is a phase that is run late in the
compilation pipeline after all the
constants have been exposed we place
classes and objects with result
constants node that does lazy resolution
we replace method counters with resolve
method and a lot of counters now that
results in methods allocates the counter
structure if needed and returns it
there's some well-known constants that
are eagerly resolved and automatically
replaced for example this happens for
primitive array classes we also have a
phase to replace class mirror constants
with in directions for class constants
a bunch of optimizations have been
during constant replacement currently
there is it at most one resolution of a
constant four method it is placed in a
in a blog dominating all the uses and
although it's optimal for code size it
may be too eager and we plan to
experiment with but a better scheduling
approach in node replication we try to
reuse dominating class initialization
nodes more about those in a minute we
also do simple optimizations for example
we avoid full-blown constant resolution
for the class of the road methods holder
and any of its super classes because
it's guaranteed but that by the time the
methods around those are resolved in
initialized in the ants constant
resolution node lowers into the
following construct which is basically a
load check if it's null and if it's not
a cauldron our currency BT
implementation doesn't do any kind of
keep serialization yet so we have to do
all class initialization at runtime we
added a special plugin to grow parser to
be invoked at class initialization
points at which we insert initialize
class nodes we try to reduce the number
of nodes that ends up being in the graph
in various ways during parsing we avoid
inserting nodes for method holder class
and its super classes there is also a
separate phase that we have that does
data flow analysis and removes redundant
class initializations it's also good in
combination with loop feeling and it
allows us to get initialization nodes
out of the loop
in the end its initial is class note is
lowered into the same if no than coal
runtime diamond similar to what happens
with resolution in order to fully
integrate with a hotspot runtime we had
to support tier compilation basically we
profile method and vacations and beg
branches similar to what happens at
level two profiling profiling the eggs
are inserted in the parser using a
special plugin later it is now is a
process in a page that assigns a leading
education frequencies and random sources
more about those random sources in a
minute there are two styles of profiling
and that we support there is the normal
way of profiling which is just what he
would expect its load the counter
increment store the counter back mask
ative the result of masking is zero
color and time to may be invoked JIT
compilation we also introduced another
way of doing profiling which would call
probabilistic profiling and it's an
experiment that has a goal of minimizing
cache line being funny consider a
scenario which is very common in real
life when they're a bunch of threads
running pretty much the same code see
serving web requests if they're running
the same methods they would use the same
counters as a result the cache line that
contains the counter will have to move
from one core to another when the story
happens and it is very problematic
especially in numerous systems so the
idea behind the probabilistic
profiling approach is not to touch this
cache line every time but to do so only
once in a while with some special
specified probability to do that we need
branch and random wedges
implemented as follows here probably uh
is a part to log of the universe of the
probability so having branch and random
the profiling code looks like this
essentially nor no profiling wrapped in
branch and random there seemingly a lot
of arithmetic but it actually all comes
in folds to pay attention to this random
function on top it is what we would need
to implement to make it all work to
chiefly implement random we start with
introducing random seed nodes dead
flowers into our DTC on Intel although
it's a reasonable random source for our
purposes it's too expensive to use in
loops so for loops instead we insert LCG
pseudo-random generators which
essentially mullen add turns out that
some arithmetic is way better than
touching memory it pays off a great deal
and this approach results in about 30%
speed up on several benchmarks on Numa
systems lastly we had to just inlining
policy a little bit growls and lining
defaulted mining policy relies pretty
heavily on profiling information block
frequencies in particular and we haven't
made a teen compiler to consume such
profiling information yet so we had to
reduce the lining depth and do other
tweaking we also don't report CH a
information 480 methods so we'll replace
some t chain lining decisions with men
time checks otherwise inlining only
happens for exact types which actually
was a surprise for me that it covers
about 80
changing the cases so it it's reasonable
so that's all for now changes vladimer
let's talk about the decomposition
phases which happens in JTC tools so we
have initialization collecting method to
compile method compilation converted
generated compiled it called the key
code and generate finally shared library
after classes of the based on common
line loaded we look through all methods
and filter them useful use bits five
compiled commands file after the list of
methods is created we fork up to 16
threads of growl and start composite
this is most memory and time-consuming
faith because we have to keep result of
all a compilation until the next phase
and for when we combine Java based model
with all Nestle compulsory but more than
50,000 methods so you have to specify up
to a 4 gigabyte Jacob in this case and
it's may be time consuming to if you
have very large methods with a lot of
backwards for example we have trouble to
compile of the magically generated
system models jigsaw methods we cannot
use it is good as it is
we have to modify for that we copied in
local buzzer updates compile it and
replacing for example calls with scores
two trampolines these trampolines low
destinations from readwrite sections
which is initialized during runtime
lazily by runtime we also replace
constant types of pointers to classes
methods and strengths because also we
cannot combine it as you said before and
we initialize pointers with zero during
stuff up during generation and when with
the cute little hit this code when you
see for example loaded a class pointer
from memory and zero we jump to runtime
result was and store results pointer to
this memory location so next time we
just loaded in continue execution
without Anton call we store names of
classes methods and drinks in utf-8
format in eg library and these names are
used during class resolution depth
resolution and also slink allocation
after we modified code and collected
data we also call to runtime call to run
time to get fingerprints metadata about
methods after all this data collected
with quadrant I'm configuration via
inversion and we start generating we
generate object files and then we use
local linker to generate library usually
Linux has linker by default MGC but mark
walls and windows is happens
make sure it is installed some system we
are taking missing them so refer me to
Justin 80 libraries standard shared
library so you can use existing tools to
lock on them you can use for example
readers to look on section you can use
db2 luw call to bring compiler to Gmail
to look on them and you can use object
down to them disassemble code in GT
library section names listed for example
here could be changing in future we
change them into the key ten already to
shorten them we can add new sections so
not rely on this output too much we have
the SS script the script in DG in hot
spot just directory which built DG
librarian tells them so you can use it
as example and they will read me the
explains how to run case now let's talk
about performance this wise described
setting which I used to run it collects
this data the main point is that we use
dutch methods to compute e compilation
we did several experiments compiled of
methods for example we create for John
base it creates something like 300
megabytes eg library in this case and it
affects the top and execution because
you have a lot of method there and we're
loading classes have to search
correspondent Netta Java method and it
takes time so the best results of the
top and execution we found that it's
better just use touch method it means
method to achieve the uses during
testing run
this slideshow execution time real-time
running Java HelloWorld
so time is milliseconds and smaller
value is better when observations that
improve on average improve stuff up
about 5% and close data sharing about
20% and combining we creep up to 25%
improvement first column first column
shows data collected for Sivan which is
imported by Jill stop at level one work
second column is default event loyalty
to your configuration and certain fourth
column within other set its event love
growl Street yes absolute it's much
better result
yes it's actually says that it's
combined it java-based both growl CDA
generated which you can do with up CDs
but notice it ears so third column and
fourth column rates gravis
okay second set about et set is for
savant case study Quixote is used so no
profiling for the savant was see to was
geared UT and for savant was ground both
cases as used the certain fourth column
in this shows that cheered UT have
penalty bow 10% because you have to
profile this wise difference there for
third set forty years
if you look on civil for growl result it
shows that when you use Java based plot
graphs in this combination performance
and water it's the same ability
essentially the es archive size and
number of classes it almost triple so
you have to spend a lot of more time to
processing this data this is the same
run but user time it's combined in CPU
time it shows that et is much less CPU
there is a power because he does not
regret compilation during this time so
essentially if you look on CPU person
it's running just one thread javathread
execute application is in EEG code and
if you look at normal case you see that
second and third thread also active
doing JIT compilation next data it
slightly longer running application
about two seconds is Java C compiler and
JVM CI files but it is
is very similar to previous data third
column or in the second dataset shows
that Egypt God foreground helps a lot
this is my our main motivation so if we
compiled relativity and use it it's on
the pair visual and c2 combination this
is again user time which again shows
that eg God helps with severe very
resource consumption and next is guru
Garron it's also about small hello were
three pitches of the round two seconds
this one shows that CES help
what up CDs of the load is up to 30%
better for default case C 1 plus C 2 and
for the one case is up to 40% employment
in all this CES cases java-based plus
curtains job with corolla classes
together and again the same story with
user time now about peak performance how
ingenious effect big performance I use
big GBM 2008 is fronting like quite
simple similar reasonable and set up
very simple I reduce it warm out time
interation time and number of iterations
experiment shows enough time to get
compiled during world map so reach this
peak performance I use combines
score advance to compare results and
that stuff similar for start-up
benchmarks run high on this light higher
value is better main point here is that
e key which is pointed by arrows guilty
guilty in default case of adversity to
shall help to reach the same peak
performance and default case so
imperialize you saw that it helps faster
startup and as you show it does not
regret big performance or even you get
slightly maybe better peak performances
in default case and also yield
equal graph performance is about 10
percent water than C 1 plus C 2 which is
expected it gives better results in some
ditch marks such as XML transform but
there are some worse results which small
sigh marks benchmarks
general derivations about performance
data show travel application give better
results with c1 + obsidian sand Pat
Geraghty we got best deg result when you
compile your et only touched methods
because in the library
it's really small and all executed in a
satirical the PRTG village the same big
performances currently we have started a
et court especially static eg could
consume much less speed resources code
generated by growl it's about 10% slower
currently Templeton's law the code
generated by c2 this is work in progress
we planning all in replacing on c2 when
we get the same or better performance
yeah it seems to have compiled graph
even by just tight Achilles enough to
get results we want the years up TDS
improve a lot much better than eg
startup performance and benefits of eg
and CVS incremental improvements adapt
we have some restrictions 4 gdk9 how and
when you can use UT and we working on to
address them currently we added support
for Mac OS and Windows sonic 64 in
judicata we remove the dependency on the
bells tool and use Java code to generate
object files it was done because we need
to support also Mecosta and windows we
investigating
using data from TDS archives in a key
code for example we experimented to use
class pointers initializer annuity
because it's not moving when during
execution cfrm is loaded so it's the
same pointers but we found out that we
still have to do class resolution anyway
to construct or plate system dictionary
and other VM table so we don't see much
benefit from that we also experience
populate string pointers essentially to
do care serialization object
serialization up CGS allowed to do this
with g1 which rises and it seems it's
helped but not in default case we're in
only several hundred strings store it in
years archive but if you store all
strings in classes then it may help
so some result shows that it not help a
little but still can help and we working
also on in what dynamic support we have
some problem to compile music using more
dynamic and that's all question ok
the question how many method I have to
compile for hello world
vdd yes so from my head out say
something like mm do you just prepare a
compiled everyone memory is there any
heuristic that no for all this
experiment we do training run and the
risks locked in vm3 InTouch method of
like that which has then print recreate
output with old method which is the key
to doing run every matter yes which is
executed I see yeah it's a natural yeah
but you do have to do training runs for
that it's actually the same for
obsidians you have to do training around
to collect use classes otherwise it's
all about hundred system class which is
not enough so I have done exactly the
same thing as you which is if you try to
compile everything it's really too big
you have to
I mean compiled doing a training run and
then compiling what you are running yeah
should be the default things
yeah but it should you have to have to
get somewhere somewhere a list of
methods yeah well but and now what I we
have to specify this with using some
options flag for JTC by default you do
tool it can just give class on it
compile method there this is difficult
it when you have to spit five flax I
don't think it's you form this if you if
you compile everything it's not going to
be like super bad but it is it is
because it's really big
it is in the startup you have a price
that starts up just to load everything
yes yes a good recipe is recommended but
and and the other thing is because you
are doing training run you have more
information you are the providing
information yes
you Goodman we we don't do that you
would yeah we should but we gone for
experimental purposes so gdk9 the simple
instruction that I sent for people to
try and essentially let alone people
understand what's best to do and we do
it in the commentation to describe a
question plus single fingerprinting you
use clinking different to prevent
execution of code of course as a change
suppose this code is in mind what you're
going to do if it's online the the outer
methods will be made not entrant there
is we record dependencies on anyway
knees and feel fields that static fields
that I use as one question for Igor
randomized profiling
he says a thirty percent performance
improvement with that compared the
randomization compared to the hardware
randomization or was it the 30 percent
improvement it was over over normal
profiling don't oiling be normal mean
you you increment every time so if you
if you don't increment every time that's
that's the difference
basically um yeah so I think I think the
results were for maybe startup
benchmarks in JVM 2008 or something and
yeah thousand eight waves where the the
number of 30 percent comes from the
difference between the the code with
normal profiling versus probabilistic
profiling is probably even bigger but I
don't have exact numbers if you were on
a difficult boat probe ballistic you can
turn them off on growl dot probabilistic
profiling equals true false if you're
running in a single-threaded mode
perhaps no normal profiling is can be a
little better sometimes but if you're
running something multi-threaded then
probabilistic profiling is way way
better so it's sort of a hybrid between
sampling profiling and exact profiling
it I got here a little late so I
apologize if I asked whether they get
covered in the talk but I was curious as
to how the Ã¤Ã´t dealt with
visualization semantics particularly the
lazy visualization that fella has what
was done there we as I mentioned we
insert special nodes that do
initialization at initialization points
so when that happens I don't know
allocation static field access static
methods spec method calls yeah we have a
special check whether the class was
initialized since a runtime check so
essentially we talked that we replace it
immediately class constant in what you
usually see injected caught with load
from memory and now check if it's not
which is means it's not insulated we
jump and call runtime which data
resolution
pilatus memory Follette on it so in the
future if we maybe do keep serialization
you can run static initializers that
static compilation time and then you can
emit initialization checks a quick
question for Eagle the probabilistic
profiling what was the problem
value used is a dependent upon the
number of CPUs and if so how does it
vary which proctoring the or when you
know prophetic profiling how do you how
do you initialize the problem value it's
a compile-time constant yes but what is
it why I ran a few things and decided on
the value so what's the value using I
remember maybe maybe eight so each or
Angela run okay I think 711s X so every
hundred twenty eight time or okay and
you were running on an HCP machine for
CT machine there was probably something
bigger
okay I think it was a two socket until
maybe I don't know eight cores each word
thanks
Thanks it's early and I'm under
caffeinated so apologies if this is a
stupid question with respect to the
probabilistic sampling it seems to me
that like why why the one could imagine
bumping fed local counters and
periodically flushing them back
and in so doing you wouldn't have liked
the sloppy counter you would have exact
values so long as you plus the fed local
one back to the global one and you would
have less in terms of cash ping ponging
I'm curious about the intuition behind
like not going with that but instead
doing the probabilistic sampling
approach um well it doesn't have to be
exact in fact even with normal profiling
it's not exact because the increments
are not synchronized so if you do it in
parallel in many threads you will
increment the same value essentially and
stores so there will be a lot of missing
counts even a normal case then if you
you can certainly like you setting
accumulate increments and then maybe at
the end of the method or in some like
pose dominating blog flush them we
actually plan to do that to optimize
profiling even further but I don't know
it just seemed like a fun thing to try
and paid off that's totally fair ever
last question well a 5/32 um so I like
jazz now fingerprints do you only do
class level fingerprinting like if you
have to invalidate are you able to
invalidate only on a method basis or do
you have to invalidate the whole
compiled code for a whole class Oh God
for whole class so essentially
fingerprint generator calculated
enrolling full question constant ball
and my code I sued so even if you change
slightly by court it will trigger remove
YouTube so there's only a single level
of fingerprinting yeah and so what if I
just redefine one method in a whole
class where you throw everything away
if the famous JIT compilation
essentially if you do some redefinition
all in methods will be thrown out
yeah I think it's a lot worse with just
Cowboys yeah it's just compulsion
removes okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>