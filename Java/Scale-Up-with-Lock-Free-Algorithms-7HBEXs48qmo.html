<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scale Up with Lock Free Algorithms | Coder Coacher - Coaching Coders</title><meta content="Scale Up with Lock Free Algorithms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scale Up with Lock Free Algorithms</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7HBEXs48qmo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello everybody let's talk about
scaling up with luck for algorithms
today so I let me present myself I and
therefore the things interesting here I
teach concurrent I serve you as a
program in a st. Petersburg IT mo but
we're not going to go into deep theory
of concurrent computing today today
we're going to talk about scalability
issues and what we'll see some some some
stuff I also work in Kaplan for
jetbrains so if you excuse me my coding
examples will be in cotton but I
hopefully they'll be easier to incent
for everybody and I'm really clueless
about you guys so who is first of all
who has obviously if you came here
probably being writing some concurrent
code please raise your hands if you did
okay okay so I see so so the most of you
didn't so in a year you come to see
what's going on so I'll help you today
so today we're going to talk about share
it mutable state and that's kind of
three words that really dread people
really know what it's about because when
people who programmed for for for many
years when they hear the word share it
will stay they they get different
associations maybe something like this
you know when it all crashes and blue
screens or maybe it sounds like this you
know that's kind of you know association
with people here because share instead
was three things you you should never
ever put together you know you can have
mutability you but separately you know
sharing separately but when you do it
all together
it's it's it's it's become a really hard
so if it's that bad then why people is
doing this so why we have this shared
middle state and why Java supports it
when many other modern languages do
support it why not why don't we go like
and forbid it like there are some
languages that do completely forbid
you're from Harry
Oh state and there was this actually
really simple like if you look at the
morning world and you have this you know
 word week data so I put a big
big data and you have to do something
with it you know many times the your
problem that you're trying to solve is
you can't represent it with this pattern
you have this big data you split it in
two batches then you do some processing
with camapign the data then you collect
the results in getting answer that's
most of the problems I like that and the
problems I like that are called
embarrassingly parallel that's actually
the actual technical term the problems
in calling embarrass in parallel and
because to solve them I mean the easy to
realize and you don't have to to go in
this scary world of shared mutable state
because you know there's nothing shared
here you can scale it upskill diode into
cluster you can write your regular DNA
processing code let's framework that's
it's the bottom take care about
arranging only operation that's it if
the if your problem is like that you're
a happy guy you you don't have to be
here however sometimes you have this big
big data but it is also like real-time
or semi real-time oh it changes all the
time like for example stock quotations
change all the time you know your
inventory in a big company something
that you know status of users in social
network like there's a lots of data it's
constantly changes and you have lots of
simultaneous requests coming to do
something with this data
you know compute it your portfolio you
know yet
real-time statuses for users in your
social network do something like that
and when you start trying to solve this
problem in a both scalable and
performant way there's simply no way to
to solve it which is in most performant
and scalable without actually sharing
this big data which changes which is
minimal between all the current
concurrent processes then you have
I mean if you take the other approach
you just okay let's not share anything
you know then it gets a massive
duplication later
I mean SEO and if data changes in real
time you have to deliver it in all the
copies etc a huge waste of resources so
many problems of this kind can be only
salt if you actually take this beta data
put it in some memory and access it
concurrently for all the requests that
you have and this is of course duties
stuff like this is a really dark side
but I mean we have to embrace this dark
side when we when we work with it I mean
there's simply no other way so let's
let's come forward to this dark side and
work with it in order for this
particular presentation we have to I
have to
it will straight you with some examples
so we'll have to pick some toy problem
and that's the hardest part
I mean peeking into a problem to
showcase concurrency because to a
program never never real problem and
will never show you the two problems
that you'll encounter real life but for
those this particular problem I've built
a stack there isn't simple most people
no know how stacks work and you know it
will let us showcase some core concepts
they will be working with for sake is
simple will get the Lincoln stack so
we'll have a node in a stack that has a
next reference and the value of some
type T and I'm doing my as I said my
examples in Kotlin just because a cotton
is you know special English were
invented so or code fits on slide you
know and because if I do it in Java you
know it's a full side of code and you
know in the Wisconsin III can fit
something else in addition to that and
that's what I need so oh I won't be
scared it was Java anymore
so the sec is simple so how it works
anyone starts with an empty stack that's
some top variable with their null
pointer then we add an element to it and
it stopped once to attend its next is
now then we can add another one element
to
then switch top tweet and that's a stack
contains two elements in me so that's
that's that's real easy so push elements
the stack is easy and we can easily
write this code I mean we can just
declare class for a stack we can declare
top variable and I'll push is just one
liner easy not not a video so that's our
top that's a push so how do we pop an
item from the circuit
so we examine what the what the current
element is the top and then we flip the
top to the next one and we read the
result and get an easy and just how it
looks
that's the way it works so this which is
especially you know concise and coupling
because you know I can I can fold my no
checks into this nice syntactical cancer
so it's really really like everything
fits on one slide that's very simple
second plantation so does it work of
course it works if I do it serially like
if I if only once threaded time works
with my stat if I don't share it right
everything works perfect but I said
working if I actually share alright if
it's shared middle state if the stack is
a partnership maybe I'm doing maybe I'm
doing kind of for example a pool of four
big objects or for my data and pulling
objects is easy to do with stack stack
stack is nice
absolutely nice data structure to do a
pool of objects so suppose I have a pool
of some buffers or a seller put in stack
and now I have this big parallel
processing and my many threads needs to
take items from the pool and put back
items into the pool so so what happens
if two threads try to push into stack
right so one of them comes you know
weeds the top tries to add value B then
the other comes with it up tries to add
element C but now only one of them will
succeed in updating the top and the
other one will rewrite the top from the
first one so we'll get in the end will
get only
two threats concurrently push will get
only one of them added to the stack and
this is a common problem this it's I
think you called it conflict and it's
easy to solve in both Java ya Kotlin you
just market synchronizing job it's a
keyword you know in in the cotton it's a
annotation but the concept is the same
by marking it synchronize you you
ensured mutual exclusion between
different threads that are trying to
push and pop and this way it prevents
the problems that we saw but the
question we came here to understand it
does it scale okay so I saw the problem
now works but does it skill in order to
figure out if it scales we have to
understand what's called bilities and
some who measure it remember we'll just
run a benchmark and we'll use this this
nice gem a framework that's I would
recommend anyone who is writing
benchmark is open source on open source
and I will have all the links at the end
it's open source on D on the Java side
so and it's part of what man GDK so I
can just declare my benchmark which I
have a share stick and I will have
multiple threads just trying to push
item set popping making sure they pop
and one so the work my workload is real
simple just the sequence push pop I make
sure that issue is pushes and pops OH
stacks doesn't grow indefinitely it's
just limited in size and to measure how
it scales
I want this benchmark by wiring the
number of threats enter on it and in
order to make it realistic I've went and
rented this the largest Amazon instance
I could get my hands on with some extra
large compute instance with you know
Xeon processors 32 Hardware threads so
like I think that's how you run your
wheel application right you'll if you
have lots of data you have this big
machine lots of memory and lots of
Hardware threads you'll load your data
inside
machine and you will start all your data
processing threats will start working
with this data concurrently so the in
this whole model that's basically this
wheel tour examples on model of that
kind of real-life processing if so I'm
running this of real-life looking
machine and that's what I get what I get
is that it's it does like 20 millions of
these operations per second in one
thread but then if I add more threads it
doesn't become better it's actually
become worse you know it's especially
working two threads which kinds of
interesting effect when we have two
threads is like the worst of the worst
but as which number of threads kind of
stabilizes and but still it does not
perform as well as if it were running
just a single thread so differently this
kind of problem in the solution to it is
we're adding more threads doesn't make
it doesn't make it better it doesn't let
it push more operations per second so
it's what's called doesn't skill why
doesn't kill because because I think
that its technical contention so because
when one threads perform said for
example a pop and the other thread in
the middle of the pop comes and tries to
do it it can't do it has to wait because
synchronized is a mutual exclusion
concept you know we'll have to wait
until the first pop completes and only
then we can pop an item from the stack
so that's that's one one problem with
synchronization which all usual solution
to the concurrency is that under
contention we're starting to wait we
cannot do things concurrently like
really seen synchronized keywords and
locks they limit our concurrency because
we have to since have to be done
serially more more so it adds overhead
because every time I stumble into the
log by other thread I have to contact
tsuge that's why it falls down under
when we have multiple say it's not just
stays flat like we can only push more
operations per second we actually under
contention when we have lots of doing it
we're pushing less operations per second
because there's X overhead involved in
managing
but that just one problem with locks the
other problem is ox is dead locks when
we start to writing a rake in our
application we synchronize with locks as
we add more and more locks in our amps
we add the contents risk intron and
daggered locks in the situation where
you know I cannot take a lot because
it's taking but some other thread who
turns out to be waiting for for the log
that I keep and that's that that's
that's the other scalability problem
with locks were scalability not in terms
of how many Hardware course I have but
skull built in terms of how large my
project is the logic my project becomes
the harder it becomes to maintain if I'm
using locks in it so that's why we're
for either of those problems were
turning our sighting to lock for
algorithms algorithms that don't use
locks to solve this same problem and the
to understand how lock for algorithms
work how they don't don't run into the
problems with concurrency but without
using locks let's consider what prevents
us from pushing two items concurrently
into the stacks the problem is that when
we've prepared to push an item we expect
that the top is still pointing to the
old item and we want to update it to the
new item wearing short leave this deck
if only we can do this operation
atomically making sure nobody else can
interfere and add their element faster
than we did and that's exactly this
atomic update from expected reference to
the new one is is called
is there is there's an A for search
operation the name is compared and set
and there is a class in Java called
atomic reference that provides us such
functionality it's a it's a class
actually you know it's it's a part of
GDK so we can use it from any given
language on this example we'll be in the
following slides using it from code line
because it just a class we can use and
inside it contains two volatile variable
you can get it or you can compare and
set it you can prime said is that's
atomic update I'm updating variable
under condition that nobody else has has
changes is still equal to the expected
value it's available since ages since
1.5 and I mean it's real prison a long
time ago 1.5 and job was the first
language major language who kind of
standardized such operations compare
said like in c plus plus he used to be
forced to use like vendor specific
extensions some libraries it wasn't part
of the language in became partners
language in c++ only recently and that's
why when you start reading literature
and concurrent computing on logarithms
you often find examples in java because
because java has me has standardized
this kind of code way longer than than
any other language so i mean that's easy
so we have yeah we'll have well you may
compare and said that's always how we
use it we use it by replacing our top
variable instead of being a variable
that we're going to change is going to
be value we're not going to reach the
top itself instead it's going to contain
atomic reference and we'll be using this
atomic reference it's one change itself
but we'll be updating it and the way you
write lock for algorithms is by
following this pattern so we read the
current value and we're using yet we
decide how we want updated in this case
we want to create a new node with
pointing to the well you were just read
with our value and we'll try to perform
this atomic update through comparing set
and if it's successful we return if it's
not successful just loop which is slow
passing or successful we just retry
until we successfully push it and this
pattern is you'll find in almost every
lock tree algorithm it's really powerful
pattern because it lets us solve the
problem
concurrently modified data structure
without using any law whatsoever I said
that's the the real power of walk free
you know and but the pattern is simple
we can similar right pop I mean it's
it's I won't go step by step here but
it's the same idea we with the current
value we try to updated necks of
successful return well if not you try
but notice that here to write this we're
using atomic reference class and I mean
let's remember why we started doing
lock-free so our end goal in this
presentation is to make a scalable
application and then scale with somewhat
implies it's going to be fast but by
using atomic reference we can create
another layer in direction instead of
just working with the top variable
directly we've created yet another
object atomic reference and we're
constantly indirectly going there by
reference into another object which
might be a key for this small example
but in larger application we'll have a
lots of concurrent options it might be
might be a performance problem so so
it's a trap here that then we need we
need to solve it somehow there are two
solutions first of all what we would
like to do is we would like to be able
to directly modify node if there's
actual volatile might've modifier in
Java or volatile tation cotton that you
can use to specify that I'm going to
modify variable from multiple threads
and it's is going to ensure all the
query memory semantics for you but how I
actually do compare and set I can read
and write it but how do I comprehend
operations that they need for this
atomic update one solution is a class
called atomic reference updater which
also defines this Java in five and john
1.5 and it's it's pretty ugly class in
the sense that you know it's even its
signatures look scary and when you use
it you have to earn a living example in
Java because there's no ugly code and
that's why they're in
Java so you'll have to defy
multi-variable
and you'll have to define static-filled
and that's using this new update or get
your first job data which is pretty big
amount of boilerplate doesn't even fit
on my slide horizontally and then in the
code when you scamper and said instead
of writing it naturally like I want to
update top I'm writing very in natural
style I have using this constant to call
compare and say degree so it it just
doesn't look like it looks like
boilerplate not a real code but it works
it lets me avoid this extra extra field
and the other solution has appeared just
recently it's built in Java 9 in Joanna
has the thing called war handle it's a
new new abstraction Joe and I it's it's
somewhat similar to atomic reference
update in the sense that you also can
compare set through it and use it very
similar way just the code even more
argue later you have to write even more
boilerplate to use it so I mean
unfortunately that's kind of progress in
this respect but it you have tried
mobile play to initialize it looks
similar more boilerplate but then you
use it again in the same ugly way it
just doesn't look natural since I write
lots of lock-free code in this
concurrent code and part of my day job
and I can't say when I start coding and
calling you you start you start to
notice all this boilerplate you know
because you're so much less boilerplate
in coding code so you kind of try to
issue it as much as possible so I wrote
this very simple library called atomic
phone so instead of going through this
boilerplate I can just write
top as if it was an atomic reference I
just have a nice function called atomic
that that trusts me like it's real
compact way to declare the fact that top
should be atomic variable that contains
note maybe now and it's initially now
the inner just use it just like I did
with atomic reference so I you stop
comparing set and that's it this but the
trick is that I code it like atomic
reference and I compile it into Java
byte code just like Road but then there
a bytecode pass processor who actually
chose this code into coded using atomic
reference filled up later so behind it
since it's so I don't have to write this
boilerplate and I let the to write this
boilerplate for me and that's that's
that's kind of what what kind of the
progress in you know computer
programming what's about your as you
know you go into a high level of
assertion you let tools do boilerplate
for you they don't write yourself the
good thing about it that without
changing my source I can turn flip and
switch and have it emit the code that
using more handle so I don't have to
change a single line of my code for that
of course the question is is was it
worth it
okay so we did all the step will learn
how to write lock free stack even would
even learn how to write it so it's nice
you know and can work around different
api's but addy like why what what did we
win anything so let's measure it and
when we measure it you know the results
want look they kind of mixed so for
single thread case there's a definitely
win and that's one advantage of our
algorithms that they just have more more
they're more performant when there is no
contention a single thread case but
unfortunately it's kind of in contention
case we're losing a lot as you became
we're working if not working much worse
than it worked with logs so why why is
that wildly one third case faster is
obvious because we don't have locks it's
just much less overhead but why we're
having such worse results like several
times less operations per second in the
ministry s case the reason is it that
water algorithms also suffer from
contention but they suffer in a
different way because when lock free to
log preparations overlap the second one
not just wait until the first one
completes it wastes the whole work it
tries to do it because it tries to
update but then encounters that the
other thread want the race and did it
did it before so it has to try the whole
operation so the
fact of contention on log preparation is
much more pronounced and there of course
there are a lot of techniques to olivey
the problem there are lots of papers on
how to make a lot for algorithm
basically by doing tricks like you know
exponential back-off a proper spinning
ceteris better others worse combiners
lots of papers that apply these are that
techniques in certain situation well you
can actually get performance of
logarithms on par with synchronous all
then while still keeping this good case
of being passed in knock non container
case but we're not going to go into
there I mean because usually practice
you'll never need this advancing
techniques and now I'll show you why
first of all let's see whether we had
our problem is to toy like we first of
all we've looked at it to a problem to
start with the stack is not something
you typically work with typically your
data structures are way more complex and
they're just a stack you have some maybe
you know code database that's your
update concurrently or something else
that that sits in updating it it takes
much more time so let's try to simulate
it let's let's add let's take it more
real but you know artificially inflating
the time of our current operation by
just spending some time and site we'll
do this both for our algorithm wheels
locks and the same change for a lock for
implementation and let's see how they
behave that's what we see because we've
our operations became longer we don't we
we don't see much difference in there
single thread performance anymore
because this over here of locks is now
small compared to the cost of operation
itself but we still see that lock three
algorithms don't shine here under
contention the same rule kicks in you
know under contention locks degree but
now they degrade like there's no this
strange d-pad to thread case but
and they stay pretty constant degraded
with a pretty constant and lock-free
algorithm degrade under contention
because again the same problem they have
to retry operation which is now became
more complex but there's by the way less
less difference in their performance
someone so I promise here that's or
lock-free should be good so what's
what's the reason
let's try to lay a workload whether this
workload really representative of what
we do with our data structures and of
course is not because usually when we
have this big data problem that's
updates in real time
usually it's updates occasionally
compared to the number of queries and
reads there - you're doing against it
and that's that's really common pattern
when you face this ability in big data
problem because if the data is big you
know only small bits of pieces of it
update so every piece of data updates
relatively rare compared to the number
of virus reports and read operations
that have - that have to see what the
current value is so we'll workloads are
mostly dominated so in order to simulate
this will add we'll add in our operation
will be twisted push and pop will do
some weeds will peak at the top of the
stack measure it's one there will do ten
times
so in this going to ten to one basically
ten Whisperer perturb dates actually in
our workload in the peak operation for
synchronize implemented in this is easy
we just we just reach top well your war
if it's null return now for lock free
implementation that's also easy we do
the same change and you know withers and
implement peak for lock reads especially
simple implantation so let's see what's
going on now the picture is totally
different
first of all the throughput of the lock
from finish now significantly
outperforming the
simply significantly second they now
catch up under increasing number of
threat threats and that's that's clear
demonstrates were locked free algorithms
chime Laufer algorithms shine in weed
dominated workloads when you read your
data much more often than you update
them because with lock-free weeds don't
never there's no loop there's no reason
Locker algorithm never suffer from
contention they could just weed the
current value while weeds of the
synchronize data structure that's
protected by logs they have to suffer
they they have to wait until the writers
change their modifications and it's not
you see it's not that you know some
people think oh no problem I'll just
remove synchronize from Maurice it's not
that simple because ability to remove
this sin question or it it possibly
depends what data structure it is the
basically depends on whether you can do
it at least semi log free and again can
you shoot the logs for weeding for
example if you take the regular hash map
that's in the GDK and you think oh you
know synchronizing on every hash max
getting expensive let's listen
translation on when I put into hash map
and leave all my hash map guess not
surprise for pure formats because you
know because it's doesn't let me scale
and you'll find that it just doesn't
work I mean I've seen instance in
production where people forget into
synchronization read and you know hash
map just hand to some moment when you
starting to read without
synchronization you end up in some crazy
states where the read that is not
synchronize just just hand once into the
infinite loop so not every structure
unless the data is designed to allow
lock for weeds you can adjust say okay I
don't need locks on reading it see but
let's see interest to see what happens
if it's even more read emanated if you
have hundred reads you see as the
lockrey becomes even better like it's
now clear clear outperforms
limitation but see I'll have this deep
is still one thread the faster more
threats
you know slower but where's the
scalability so the whole topic of the
talk was Caleb and here I show you the
most as you add this slower everything
works so then what was the point of
having this huge 32 Hardware threads
machine to run this code if just any
more stress just makes everything floor
the reason is that again the workload we
have is artificial
in reality you never have like all your
threads badger in to the single day
distortion that never is a case well
know if you have a shared data that's
shared mutable data in your code you
usually with something from it then you
do something with it you do some
computation processing it's not like you
read it read it read it constantly it's
never like that and you don't update it
console it's never like that you you
receive some data from Network parses
that update you moves you some requests
read some data do some processing reply
you know it's never Unicode never better
sin to the single you know single day
search all the time some work law that
we'll be measuring so far it's
completely non representative of the
actual were close you'll encounter in
practice in practice your clothes will
look something like this
that's the best thing we can model the
real-life workloads in this in as a
micro test so we'll just consume some
CPU between you know every we'd to
simulate the fact that in real
application you're doing something
instead of just reading and of course
we'll do this for all our both you know
lock free and Link it implementation and
let's see what happens and that's that's
where we clearly see the scale of this
i've initially promised you because you
see here that now actually both for link
stack and for lock free as i increase
the number of
when I run my code I get more privations
per second because you know they can do
more of those processing concurrently
and you see that in those more will
worsen errors scalability of lock-free
clearly better than scalability of links
if they start now they start at some
point because compared to the cost of
the other CPU expense of the other
operation it is the same their minor
compared to the you know to the spin
that we've added but as number of firsts
increase we see the squabbles of La
falourdel's clearly clear bed and
actually yes
how many Hardware threads our machine
has because you see it's increases in
who's locked up until we have 32 threads
and says flats after that because I mean
there's no reason to have any more
threats than like with the Machine I was
running that on has just 32 threads so
adding physical you know Hardware
threads so anymore
you know threads because all our threats
are consuming CPU they need anymore does
not do any good but the good thing is it
doesn't harm either on the real size
number of threads we don't see any
decline when we over subscribe our
machine and that's very important for
practical applications because you know
in practice if you especially if you
program with threads in deviant world
you might for virus reasons might have
way more threats than you have CPU cores
for example of bein when working on
versed in the rest applications have
seen applications that run hundreds of
threads in much on machines that support
only 64 Hardware threads and that's
normal because you might be might need
those hundreds work with database or
with other blocking operations but the
same time in my application I have some
shared data structures like the
constantly update and I want to read
them to do
baris analytics like figuring out what's
the current stock prices and for those
you know I I wanted to scale
want access to the shared data to slow
down the rest of my work and I wanted as
little as little barrier as possible and
that's that's good that's you know it's
number I think you did it doesn't become
worse so the the kind of a lesson here
is that we need when we measure
performance we need to make sure we ask
the ask the right question is the most
challenge actually task into measuring
performance especially concurrent code
and especially if you're trying to
assess such fleeting characteristics
like scalability is us there are
questions and like modeling your
workload is is really exercise it pays
off so if you are into into the task of
making sure you got scales
I mean don't never you know don't think
like you can just you know um eat this
exercise of trying to model your
workflow I mean modeling workflow is
important because if you don't model
your flow correctly you are not getting
the right answers out of your
measurements and then when you have a
great model for a workflow you know you
can then you can compare what's
implementations and see that if you have
a reader manator workload and the real
life condition lock-free
algorithms often outperform the
velvetiness who has logs and if you want
to learn more about that this is true
great book I would recommend the one is
Java concurrency and practice by Brian
it's really good book for practitioners
with the only downside that it has
examples in Java this is slightly too
verbose but I mean there's they easily
translate it in Catalan II and you know
save you a lot of typing but and the
other book is more from the theoretical
standpoint they are the most processor
programming by hello here's a bit and it
goes deeper into the theory of
concurrency
explains lots of interesting in
non-trivial
lock-free algorithms in a deep way
so for anyone who is interested or -
that's two bucks in a third-order you
can start with either one depending how
your mind works like you're more
theoretically or practically inclined
there - good complement each other
really well and really nice read and
some links at the end so I mean I hope
the slides will be posted so the gym age
that I've used for benchmarking the the
reference to content that I use my
examples and atomic food that lets me
write it all in a nice you know concise
way and I'm open to questions
yeah the bench managed image you run it
with the number of threats you want and
it will just runs as as many threats as
you ask it for you know because my state
is called per benchmark there is a scope
benchmark annotation so it will share
this state between all the stress and it
just they do well use operations per
second so in vacations of my this is
this is my benchmark method and the
metric I show is up operations operation
is one invocation of this this function
number of methods successfully completed
medicals per second that's that's a
measurement that's just default in Gmail
you can configure what you want to see
you can see like how many nine seconds
or microseconds into but default is
operations per second
yeah there are lots of no of course
there are lots of ways I mean actually
if you read this book there's a whole
chapter on virus techniques to make it
better there's a whole chapter I mean I
just just this prison is too small to go
my goal was to if you get interested if
and if you want to know how to do just
read this book excuse me yeah yep there
is a lot of libraries first of all Java
library contains some of the lock for
algorithms in the standard library like
concurrent hash phase map and and and
others concurrent skip list so but the
the well I found from a practical
experience and if you face a real life
problem the problem is locked algorithms
they don't compose so somebody gave me
you know ready to use concurrent hash
map and I have a slightly different
problem like some operations it doesn't
natively support I cannot unlike I
cannot just extend it there's no way I
have to rewrite the whole data structure
from scratch so in practice when you
face the scalability challenges in real
life you almost always find that no
existing controller solves your problem
the way you want you still have to write
it from scratch and again it doesn't
happen often because most of the damages
write the code just works I mean in any
type of complication most of the code
just is not performance sensitive at all
there's at most one percent of your code
that needs this attention to scalability
performance and usually writing
something specifically if if again if
your business domain is where
performance matters if you care bus
codes if you want to analyze this real
term big data work clothes then it's
usually pays off in writing in writing
your own customized invalidation
oh that's that's a really hard question
I mean there's it it's a thriving area
of research and there's lots of our
formal methods to produce those proof
the problem is all the formal methods
that first of all they don't scale to
beacons complex algorithms the second
problem is like you write in some
special language your code then you have
approval that would prove your code but
then the complex problem how you
translate this proven algorithm into the
actual code how you make sure you have
not introduced any backs bugs while
while making the switch so so what's
more a practical pragmatical question is
how do I test that is correct and for
this purpose there was a bunch of
framework sexually there is I don't have
a link there is if you google for
example there is link check to open
source ELISA for example check veneers
ability of your data structures and a
multiple threads it's not it doesn't
prove you anything but it lets you
quickly find common common bugs that you
did and oh I'll prove is the work in the
rest of us I've been I've been involved
in its design you know and creation of
this tool so but not tools there is
nothing entry and that from you know
design to unfortunately and that's still
open question hopefully somebody solves
it some point in the future
yep
it's a is Java if you if you replace the
synchronized with whistling words my
sink right so you pee yeah you can
replace synchronized with javis rented
lock you'll get approximately the same
performance I mean because inside
they're very similar I mean it just
synchronize it's just less code and with
reinterred block actually in cotton
there is a nice extension so you can do
it with lock model so it's it's actually
using interlocked is way more
comfortable in cotton because of the DSL
nature of cotton so you write much less
code because in Java just very very
broad boilerplate so in Java you would
usually use synchronous just because
using your entered Locker just produces
lines and lights a boilerplate in cotton
is not a big problem but from the
scalability standpoint there is no win
the reason why people would use rented
lock is totally different the reason why
people would use your ended lock is if
they want is they having very complex
data structure with lots of locks which
is a challenge to design because you
have to make sure that you don't try to
deadlock and they want to hand over
locking they want to take one lock but
then another then it really is the first
one there's no way you can do it with
Java synchronized but with your
interlock you can do it because you can
control input your invitations of lock
up and lock but from performance then
where there's there's no wind from skull
podium
yeah you can do we dreadlock in there is
there is approach yeah there is there's
actually there is thing called to reach
dreadlock
so you can use a separate you can do
read lock but the the problem is do
dreadlocks it's it's it's a source of
over here so we still get this lock in
over here even though you in crystal
currency by dreadlocks it's another
techniques
so basically when you're running to
sculpt with your problem since locks one
thing is go lock free the others go read
read lock so there's two different
sometimes you know if it's data
structure is very complex it compensates
search a hard to implement lock for your
way I mean I mean there's some limit on
how a complicated structure can be to
make it lock free so usually the rule of
thumb of this if your structure is
simple enough so you can make it look
for just make it lock free if you cannot
make it for walk free then but you still
have to scale and matzo first then you
would go with ruh dreadlocks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>