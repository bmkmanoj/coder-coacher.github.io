<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JDK 9 Hidden Gems | Coder Coacher - Coaching Coders</title><meta content="JDK 9 Hidden Gems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JDK 9 Hidden Gems</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G7pfr2XyLfI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello welcome everybody this is JDK
9 the hidden gems
I'm Michael vid stet until recently I
used to be a JVM tech guy and then I
moved to the dark side and I'm out now
managing the DBM but hopefully I will
stay technically enough for this to be
interesting
hi I'm sandy sonatine senior software
engineer and technically Intel and we
work with Oracle and the open JDK
community to optimize Java for Intel
platforms cool so we'll be talking about
hidden gems in data k9 this is the
Oracle version of the slide that says
that everything I'm about to say is a
lie I believe that Sonia will have her
corresponding slide at the end of the
slide deck so stay tuned for that
this being a presentation on JDK 9 at
JavaOne shortly after we launched
galvanize clearly is two dimension
jigsaw there's no way around that right
so in theory this could be a
presentation of data on jigsaw
specifically so all the great stuff
we've done to modularize the JDK the
cool tools we built to improve on that
so you can create your custom theories
you can start out with this module graph
that we have in the JDK you can
streamline that and only include the
modules that you actually need for your
job application I could talk about how
you end up with runtimes they are very
much smaller
I could also touch on support we've
added to have multi release jar files
and in a way make sure that you can run
your job application no matter which JDK
you're running on and stream seamlessly
a lot of people to do that and I can
also touch on support we have added to
Java C to make that even easier so that
you can compile for certain JDK release
these are all jigsaw related things so
we're not going to talk about that this
is the hidden gems of JDK 9 we're going
to try to touch on the other stuff that
has gone into data k9 so all the small
things that tend to sort of end up in
the shadow in the shade behind dat
canine jigsaw I'm gonna start dawn by
touching on tools and libraries quickly
I'm gonna say something about hotspot
and JVM and Sanjay is gonna touch a lot
on the performance improvements we've
done
if you go to the JDK 9 page you'll see
something along the lines of what's on
the right-hand side here so we have you
know some schedule and a bunch of
features that we've added that we worked
on and it doesn't look much until we
realize that the page is significantly
longer than that thank you are rarely oh
by the way for this slide and the
animation I don't have the management
skill sets yet to create it myself but
if you want to go look at what's in JDK
9 you can do exactly this you can go to
the JDK 9 page and have a look hopefully
you'll get some inspiration from this
presentation and when I count the depths
we have on this page I ended up with
something
jep's are our JDK enhancement proposals
therefore relatively large features are
very visible or things that are very big
impact or large features and in many
cases its large features so in JDK 9 we
have 90 really big features this is just
counting the Japs on top of that we have
all the other improvements so this is a
big release for us it contains a lot of
different things and again if we're
gonna touch on a few of them in this
presentation so starting with the the
the tools and libraries one thing we
have added and I don't think that's not
anything like no presentation of the
other one here is complete without at
least it touching on Jay shell because
it's such a cool tool so other runtimes
have had support for rebel based playing
around with code for quite some time and
now it's the turn has come to Java so
Jay shell is a read eval print loop tool
that you can use to try out things try
out code and experiment with things
without having to create your Java class
file and use Java C to compile it and
deploy it and all of that so I'm running
Jay shell here hopefully you can see
well maybe not the leftmost couple of
pixels but at least GHL you can do
obviously simple things like adding a
couple of numbers why not assign a
variable and why not say you know take
that variable and add four these are all
sort of standard simple things you
should recognize most of them
that's J shell in itself but J shell
also has other cool functionalities I'm
gonna actually demo another thing that
we have added in jdk 9 the first thing
I'm gonna do is to say list ov and then
I'm gonna tab and J shell actually has
support for printing out the Java Docs
for methods it has tab completion and
and can also print out the Java Docs you
can get help in the J shell tool itself
in case you're not sure what parameters
the function takes method takes I should
say in this case it may be a bit you
know mysterious when you look at this
but what I'm gonna say next is that
we've added a func a piece of
functionality called convenience methods
for collection convenience methods for
collections Factory whatever I have a
set of functions methods that make it
easier to create collections with a
smallish number of elements so you can't
use this to create collections with a
hundred elements but you can use it for
single element or up to eight or so ten
in this case I guess and the nice part
about these things is obviously takes
less typing what you'd normally have to
do is to first create your list and then
go add and add and then you can start
using the collection yeah with this you
can instead create a list of let's say
I'm gonna have three elements to this
list and all of a sudden I have a list
of three elements what you can't see
here I I guess I could force it but the
collection here is actually immutable
it's it's non-modifiable so you get back
something that is read-only you can't
actually add elements to it you can't
remove elements to it and there are some
nice parts to that that we can leverage
inside of the JDK in the JVM to optimize
the implementation of this collection so
if we for example find a better way of
representing the data here in the
background without the the full-blown
collection that we'd otherwise have to
allocate to potentially support you
adding an element later or removing an
element ATAR we can optimize something
that is targeted for exactly one element
without any modifications to it and that
that does can help a lot with footprint
but also with the performance
that's another example I guess I could
create some we all love strings I'm
gonna create a hello world list here and
then just because I like streams from
Java 8 I'm gonna say for each system
print line and you'll have your your
string piece top or obviously I what I
also can do is put this together
collectors meaning and you will have
your food hello world string another
piece of functionality with additive
streams while I want we're on the topic
is a take wild operator so you can say
for example that pick elements from this
dream while this thing is true or well
the thing isn't true I guess so I'm
gonna say while this the component here
doesn't equal comma and I'm gonna
collect that again
and you one will get the hello there's
also a drop dropped while so two new
methods armed streams we like streams we
like optimizing streams and we've
optimized streams of Arden JDK nine as
well so improvements have gone in in
that general area
speaking of concatenating strings and I
couldn't find a good way of showing this
because it's very sort of low level and
back-end ish in nature but one of the
things we've done is to change the way
we generate bytecode for concatenating
strings so for piecing strings together
in the path so first of all there's a
lot of string manipulation in Java so
it's very important for us to get that
right and optimize it correctly and in
the past what we had was a series of
concatenation append operations that the
the JVM and the JIT compiler had to sort
of trying to pattern match and scrape
out and piece together in order to make
sense of the fact that in the end is
just concatenating two strings what Java
C does starting with nine is to output a
bytecode sequence that is much more
obvious for us to work with it's making
use of the invokedynamic instruction
which is an instruction we added very
recently in the first and only
instruction that has been added to Java
and it's using this instruction to to
make it easier for the JVM to detect
this the other nice part about it is
that that same pattern it can week or
more of the implementation and the
optimization opportunities or then moved
into the libraries so if in the future
release of the JDK we find a better way
of optimizing this we have that same
pattern we can detect it and again the
update the implementation of
concatenating the strings in the
background and you will all get the
benefits of that without having to
change your code and so back to the
presentation I walked through J shell
I've touched on convenience factory
methods for collections that was the
order of the words I was looking for and
in the five string concatenation which
is again the concatenation of strings I
had backup slides in case the demo
didn't work out
if you want to know more about J shell
there are plenty of sessions during Java
one I recommend that you go to at least
one because J shell is extremely
powerful I have a slide at the end where
if you if you forget about what was on
that slide
moving into the VM one of the key things
we've done on the JVM side is to switch
the default garbage collector it used to
be the parallel garbage collector but we
have now switched over to use g1 our
garbage first garbage collector as the
default the g1 garbage collector has as
its goal to provide both good throughput
and low latency or low cost times it is
region based so the way this works in
the background is that the whole heap is
split up into individual regions and
then it's the g1 collectors choose very
carefully which regions it's going to
try to evacuate to empty and and clean
up so that we can later reuse that for
new allocations and basically the way it
works is that it picks the ones that
have the most garbage first where the
fewest objects are actually live still
and then it removes all those live
objects out to somewhere else and what
you have in after that is obviously a
clean empty region that you can start
allocating in so their name comes from
the fact that it's taking the garbage
first as if you know the regions with
the most garbage first the default post
time target for g1 is 200 milliseconds
but this can be tilled so if you want
more throughput for example and can
tolerate more post times
then you can continue it one way or if
you want low post times but you can
tolerate and throughput overhead
I think it's unit the other way and as I
mentioned g1 has been in the JDK for a
while it's been made before now and we
have made some improvements to the
implementation of do you want to make it
more performant this is an ongoing
effort so as a matter of fact if you
would pick up JDK 10 take the code today
and compile it you would find that we
have already improved on what's in 9 so
there are a lot there's a lot of
activity going on here and
of improvements but the end goal here is
really to have AGC in the default DC be
one that that doesn't only care about
throughput but also takes low latency
into account and we believe that a lot
of applications going forward will be in
one way or another latency-sensitive
it's not just bulk processing of data if
you want to know more about
jg1 there are at least a couple of
sessions that will touch on that so
again recommend you to go to those on
the topic of GCS one of the other things
we've done is to deprecate and remove
some of the the code we have in the JVM
specifically we have deprecated the
concurrent mark-sweep collector the the
CMS were short the CMS collector CMS has
traditionally been used in low latency
use cases where you don't have very
large heaps and where your application
is relatively well behaved if I put it
that way the challenge for us on the JVM
side has been that the code base the
implementation of the collector has been
very very hard to maintain and it has
been very hard to evolve the other
collectors while still making sure that
CMS works the way it does so what we've
done now is to deprecated it the goal is
to remove this in a future release we
haven't decided which one yet and the
hope and goal here is to have the other
collectors fill the void device if you
so will there will be cases where you
may have to tune things differently for
example and all that so if you're using
CMS again be aware of the fact that this
is going away and that we very much
appreciate feedback on d1 and the other
collectors help us improve those that's
that's the future where we want to go
the other thing we've done which I think
effects fewer people is that actually
remove some of the GC combinations that
we have in earlier JD case this is
something we deprecated in JDK 8 and
that we have now removed completely in 9
there used to be something called ICMS
and there was also a set of different
combinations of various DC's that seemed
to be not very well used let's say
were again hard for us to maintain those
are now gone other stuff we've done to
improve things both sharing footprint
and performance is work on something we
call CD s CD s is short for class data
sharing this functionality has been in
the JDK for quite some time as well if
you used to be limited to the Java base
classes the JRE classes themselves but
as of I think one of the eight you
updates or eight updates I should say or
nine we also now have support for
application classes as well so this is
not limited to the GRE anymore it's
something that you can use for your your
application classes as well and the way
this effectively works is that you have
a sort of training run where you run
your application once and what the VM
will do is to read all the classes
format them and put them in a way that
the JVM likes internally in a more
optimized format for the JVM and then
store that into what we call the
application class that were CVS archive
for short it's effectively a shared
library much like you defined on the
operating system level but but but it's
not an elf it's not in the native format
of the operating system right now at
least and then that's that's your
training run and then for every run
subsequent run you do of the JVM after
that will point where you can point it
to these this archive and it will make
use of the information that is in the
archive and leverage that one of the key
things we've done in 89 year is to
improve on what is in what goes into the
archive so it used to be that it was
basically just the class data itself the
bytecode and the side structures but now
we can also store in turn strings and
this is also something that is evolving
is where we're looking at adding even
more to this going forward you have two
major types of improvements here the
first one is startup time so by four
since the VM no longer has to read the
class data from file from your jar files
for class files and massaged that and
make sure that it's in the right format
and again all of that instead now we can
just pick up this shared archive and
it's already on the right
you get startup time improvements and in
this specific case we're looking at
WebLogic server based domain and we can
see that the improvement is ballpark
25-30 percent something like that this
will clearly vary a bit on your
application but I think we've seen that
typically you'll see this ballpark
number 25 30 % and startup time so it's
pretty significant and the other aspect
of this is footprint so the fact that
now more of the data can be shared
across instances means that you have
less waste less footprint for the
individual instances and obviously the
more instances you run the better this
does assume to some extent that you're
running similar applications that that
it's likely the same exact stack of
software or at least that they share
some common frameworks and libraries but
I think from what we've seen that that's
a very typical deployment environment
and I think it's even more common with
environments like doctor where you
actually have the exact same image and
you stamp out multiple different
containers from that so the nice part
here is that this works together with
doctor as well as a matter of fact if
you place the the shared archive in a
layer that gets shared across doctor
containers then it will work very much
like operating system shared libraries
and it may not look like we're saving a
lot so the the bars here are the unique
data per instance with and without app
serious it's the shared data with and
without the absolute Lea's and it is the
total footprint with and without the
absolute yes and it may look like the
bars are similar you should note that
the y axis is logarithmic but in the end
this actually does add up over time so
this is in this case 10 different
instances if you run even more you get
more sharing and what we're seeing here
is approximately 10% which may not sound
like a lot for like my laptop but if you
if you stamp that out across a data
center with hundreds of thousands of
instances then it suddenly starts
turning into real dollars
I want to mention that app CDs will be
open sourced this is functionality that
the CD s aspect has always been open but
the app CD s aspect has been closed but
we're working on open sourcing this I
cannot promise which exactly release it
will go into but suffice to say that we
were very eager to get this out so as
soon as possible
on the on a similar topic we have AB CD
s that takes care of Java class data and
the other thing we worked on the last
few years is functionality called
alt or ahead of time compilation so this
is doing with AB CD with JIT compile the
data there were Serge it compiled code
the same thing that AB CD s does with
the class data so you know in a normal
operating system shared library you have
the data aspect then you have the code
aspect and you package them as a as a
live something else probably this is
doing the same thing with code so
there's a tool called J OTC you give it
a class file and it will spit out a
shared library in this case it actually
is an elf you can't use it apart from in
the VM obviously but it's still stored
in that exact same format so you you
basically do again sort of a training
run but in this case it's a tool that
you run and then for subsequent JVM runs
you point it to that shared library and
the shared library will be reused the
code itself is not as efficient and not
as performant as you would what would
you what you would have with the on line
jet the the cervical pilot we have
inside of the VM but the nice part here
is that the code that is stored actually
contains a lot of the information and
profiling code that is needed to drive
the optimizations later during runtime
and the server compiler will actually
kick in and improve on this code on top
of what's in the archive so basically
you get like the first tier of
performance using the Ã¤Ã´t code and
then during runtime will optimize on top
of that the functionality is
experimental in 9 and only currently
available on
x64 that said we're adding platform
support over time and similar types of
benefits can be seen from a OT here I am
slide on it actually next slide isn't
that so I'll wait with that but you have
startup you have time to peak
performance instead of having to go
through the full cycle of first
interpreting code and then probe
compiling code with profiling capturing
code in it and then optimizing the code
fully in the end we have already sort of
shortcut the first part of that so you
have faster time Northshore shorter time
to peak performance or reduce time to
peak performance and the code can
obviously also be shared across
instances you also get to footprint
savings in addition to being something
that is useful to end users this is also
something we're planning on using
internally in the JDK to be able to move
our JIT compiler up into Java to
implement that in Java instead still
working progress but this is
functionality that is sort of a
prerequisite for that I won't go into
exactly all the details of this slide
but I will say that it is showing
basically that you have the default case
you have the case where you have only a
OT you have the case where you have CD s
and you have the case where you have
both and basically what it's
highlighting is the fact that they're
both improving things in themselves but
together is where we get the most
benefit out of them
so together CD s and alt together is a
very happy combination if you want to
know more about a OT it turns out that
we have at least two sessions on that as
well so again I'll have a slide on this
later at the end of the the presentation
in case you want to look at it and I'll
go through the rest of the slides really
quickly I'm sorry for that Java flight
recorder I want to highlight that this
tool has helped us in term we find a lot
of bottlenecks this is functionality
that captures information in a very
fine-grain way but in a very fast and
efficient way so basically the goal is
to have it default on and always collect
all this data and we also have some
visualization help to show that and we
can
we're using this internally to drive
development JFR is also being open
sourced again where we want to try to do
that as quickly as possible but no
quicker and if you want to know about
more about flight recorder or for that
sake Mission Control which is the
visualization tool there are sessions on
that as well and I'll skip through these
slides I do want to say that we've
unified our login framework we have a
unified logging framework in the JVM now
there used to be like four or ten or
twenty or something depending on which
subsystem you had in JVM now we have one
so you can control all the logging using
this one framework and I'm personally
super happy to see that this was a very
fast snapshot of functionality here are
other things that went into JDK 9
grouped together and the whole week
obviously you will have a chance to
catch up on these this different these
different aspects but hopefully that
gave you a quick insight into the
functionality aspect and with that I'm
handing over to Sandia for the
performance part thanks Michael so last
couple of years we have contributed to
and collaborated on many optimizations
in open JDK towards Gerry canine so some
of the key optimizations include the
vectorization and has been mad crypto
and compression acceleration compact
strings and the new API support and
optimizing the new API for the Intel
platforms so the Intel recently released
skylake platform in july and unless
otherwise stated most of the performance
runs are done on the top pin skylake
platform that is the Intel Xeon Platinum
8180 at 2.5 gigahertz with turbo at 3.8
gigahertz so with 384 gb of ram and we
are comparing the latest JDK 8 release
the JDK 8 update 144 with the JDK 9
which is just released September 22nd
all the runs were on
on the next platform so on the
vectorization enhancements we extended
the vectorization to 512 bits the JVM
Jade as in 38 already supports out of
vectorization they do supervise our
vector ization so that has been now
extended to 512 bits to benefit from the
avx-512 that is supported on the Skydeck
platform we to super unrolling of the
vectorized main loop there is reduction
support is added vector loops with
reduction in them are Nova Christ and
also there is support for additional
operations that get vectorized now so
I'll go into that details next couple of
fights
so avx-512 is introduced in the Intel
Xeon Phi processor and is available on
the Intel skylake processor it basically
extends
simply to 512 bit wide that means 64
byte operations can be done in a single
instruction or 30 to a short 16 in tower
16 single position and 8
double-precision floating-point
operations can be done in a single
instruction there are 32 CMM registers
which are 500 called bit wide so what we
have done is to add support for
cogeneration amick so I went to
cogeneration in the JVM jet and extended
the similar vectorization we have also
optimized the array string intrinsic
methods to benefit from avx-512 so this
is a small Java example that we have
shown that we have used to show the
benefit between Gerry cage
nine this the loop colonel does get auto
vectorized here there is the operation
being done on elements of two arrays and
the result is stored in a third array so
this particular example with different
data types and different operations is
what we showed to ensure the benefit so
the we are basically comparing the 32
byte vectorization that was available on
the previous platforms to 64 byte that
is 512 bit Elektra ization that's now
available in geni k9 you need to use the
use of X equal to 3 JVM command line
option to get 512 bit Twitter ization
and we also show the benefit of
alignment JVM has an option called optic
alignment in pipes which you can use to
align the data type so this chart here
shows the performance gain for
floating-point operation from 32 byte
vector to 64 byte vector and the
performance gain is up to 30 percent
without alignment and goes up to 70
percent with alignment and here we show
the performance of integer arithmetic
and logical operations here the
performance gain is up to 40 percent
without alignment and 80 percent with
alignment so the auto vectorization not
only benefits the classic Java code but
it also helps streams that was
introduced in G decade so okay there is
a lambda somewhat more complex
expression than the previous example
and this lambda gets Auto vectorized and
if you would have used parallel along
with this this lambda then you would see
again a cross multi-core and as well as
you'll benefit from the vector unit on
the processor so here this run is done
on a single code and you see there's a
gain up to 60% with alignment and 64
byte vector so as I mentioned before
jelly canine has vectorization and has
made over JDK 8 so in dedicate given a
simple loop like I showed in the
previous example the there is a pre loop
main loop and the post loop generated
the pre loop does the destination
alignment and the main loop is the one
that gets auto vectorized and the scalar
post loop is for tail processing so in
jdk 9 we now super unroll the vector
main loop since the vector main loop a
super unroll there is a vector single
iteration post loop which can do as much
operation remaining operation using
vector instructions as possible
so here is an example of reduction so
the highlighted portion of the code is
doing the sum reduction of
multiplication of area elements and
prior to JDK 9 reduction was not
supported basically if there was such a
reduction in a classic Java code the
loop won't get vectorized and you won't
see benefit from vectorization angelica
9 and the reduction is supported and you
can see that up to 2x gain is seen for
this particular code here I show the
vector post loop to generated code and I
can see that the multiplication VP ml LD
that is a vector instruction and the
rest of the code is for the summation
the reduction operation and summation is
used extensively for example in
convolution for image processing or
k-means algorithm matrix multiplication
so those all can get now out of a closed
so we also recognized additional
expressions in the vector loop and
basically vectorize the loop for example
the math dot square root if there is a
square root performed on every element
of the array that now gets recognized
and we generate a vector square root
instruction there we square root PV and
you can see going from dedicate to JDK 9
we see about greater than 5x performance
for this loop
so in Cherokee 9 there's another feature
introduced called compact strings so for
this we collaborated with the Oracle
charlie hunt dimension Tobias Hartman
and many others so basically what
compact string does is it tries to use
one byte storage for characters of a
string in Java by default it supports
the Unicode characters that is there are
two bytes per character but s key and
English the upper body is always zero so
you can use single byte for characters
so without changing what's visible to
the user internally the internal storage
is changed from character array to a
byte array and if all the elements of
the string fit within a byte then the
string elements are stored in a single
bytes and if not the string is inflated
and two pipes are used for element so
what this does is strings are
extensively used in any of the Java
programs so it reduces the underlying
storage that is used by the program and
not only that most of the interesting
operations are done at the back using
vector instructions so now because the
string is the string elements are half
the size you can do double the
operations behind so so there is a
performance improvement is also possible
there is some overhead to check each
element and inflate the string if
possible so both this the the memory
size which improves the garbage
collection performance and the ability
to do double the operation is benefit to
the applications
here this is an example we had run
spectively be 2005 this one was done
on'y has well a desktop and this is a
peak run so we saw about 5% performance
improvement in the peak throughput first
picture baby 2005 and this is a heap
dump so you can see that the the total
number of objects instances is about
similar between JDK 8 and JDK 9 the
number of bytes that is used is reduced
by 20% and if you see the highlighted
portion which is showing the number of
instances of character array and the
number of instances of byte array so
whatever was character array before is
now by today with JDK 9 so angelica 9 we
also worked on optimizing the math
library some of the key trigonometric a
transcendental methods like the sine
cost an exponential and log they were
optimized and you can see gain up to 5x
for this math libraries these math
libraries are extensively used in big
data machine learning the financial of
option pricing algorithms and HPC and
here we show three micros for financial
option pricing the Monte Carlo box meter
and black Scholes and they see gain up
to 5x and for example and these are
Gotham see use cos sine log square root
power exp etc that's why the game
JDK nine
we also worked on optimizing the key
crypto algorithms to benefit from the
internal architecture instruction so
Intel Architecture provides in yes and I
catalyst multiplied and you know avx2
also you can do 64-bit multiplies and
get 128-bit result
so all that used at the back in the in
the optimized dubs benefits the the
crypto Java provides all the crypto
through a provider framework the Java
cryptography architecture and sunjai C
is the default provider so we have
optimized the Sun JC provider methods
through optimized ups and you can see up
to 10x gain for a yes counter and a yes
GCM decode so the the SHA optimization
here is through the AVX instructions the
AES counter and the AES GCM his
expressions are three a yes and I GCM
also has a G hash component that
benefits from the catalyst multiplied
and the crypto RSA is through 64-bit
multiply giving a 128-bit result at the
ADC X 80 or accessories so JDK nine for
compression acceleration JDK nine also
added a feature prior to Cherokee nine
interrogate the the sealy was bundled a
lot as part of the lip zipped or so so
now in jdk nine uses the system Celebi
instead of the bundle select so that
gives the capability to the user to
replace the systems you live with the
software accelerated were not a hardware
accelerated
one and here I have given two links the
software acceleration can be done either
through the Intel performance primitives
or there is a github project which is
listed in the document here that could
be used and for hardware acceleration in
Intel introduced quick assist as part of
the skylake the chipset Louisburg
chipset so the quick assist also has
compression acceleration capability
through a driver and the library support
that can be found at the link given him
so this this one shows the performance
improvement through software
acceleration I used open source the leap
from the link given here and the
performance shown is for the Calgary
corpus data set the compression portion
and the only thing I had to do was to
get the source code built a library and
set the LD library path to the built
library and we can see em you know 1.5 X
2 up to 3x gain in compression so that
was very encouraging stuff on Freebird
added the crc32 C to Java 9 and Intel
has the instruction called crc32 C which
can be used to implement the crc32 C of
the Java 9 API and this is the basically
the I scuzzy CRC polynomial and prior to
JDK 9 and in dedicate as well as
Cherokee 9 both have the Joey tells if
crc32 which is a different the CRC
polynomial and and that was the the JIT
zip crc32 is optimized using the
catalyst multiplied so the crc32 see the
new API gives an additional 2x
performance improvement over a
previously optimized version so and we
know that the checksum is extensively
used wherever there is a lot of data for
example Cassandra HBase they all use the
checksum API in lexicographic array
comparison was introduced by Paul
Sanders and we worked closely with Paul
Sanders to optimize the area compare
using the same D instructions at the
back and array compare is used in
Cassandra and HBase for key comparison
so for example Cassandra uses blocks of
size 8 K or 64 K so you know and there
is a checksum calculated for each block
so this is going to benefit that and we
have the underlying algorithm a team is
using the similar instructions
and last but not the least there was an
MMA API added by Joe Darcy which does
the multiplication and addition and
after addition the truncation part has
turned after addition so Java spec does
not allow us to you know optimize if
there is a multiplication and addition
we can't optimize that using FME because
it expects us to do the the rounding or
the truncation part after each operation
so with this new API now we can optimize
FME using the underlying architecture as
affirming instruction and FMA is used in
convolution for image processing it is
used in matrix multiplication in class
algorithms so this is beneficial for HPC
and machine learning so yeah I think
what others have shown a lot of new
features that are available in j9 so
please download it try it out and give
us feedback and this is the legal
disclaimer and optimization notice from
Intel</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>