<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lightweight Relations with Michael Steindorfer @loopingoptimism | Coder Coacher - Coaching Coders</title><meta content="Lightweight Relations with Michael Steindorfer @loopingoptimism - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lightweight Relations with Michael Steindorfer @loopingoptimism</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/D8Y294vHdqI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone I'm Michael I'm a
postdoctoral researcher at the Technical
University in death and today I'm going
to talk about lightweight relations I'd
like to start with a disclaimer here I
know that you guys at Oracle you all
know the data bases but this talk in
particular it's not about relational
database management systems rather I'm
going to talk about how we can equip
collection libraries which sort of
lightweight relational data structures
and that by not adding another type and
by saving the matically memory and
that's like five times over comparable
approaches let me reiterate some of the
properties that we expect from
collection libraries first of all
collections contain commonly used data
type abstractions like lists set or Maps
and for all these abstractions
you usually have handful of
implementations that are highly
optimized that are typically generic in
general purpose and they are not
tailored to do with any particular
workload and in order to fulfill that
they have to carefully balance memory
and runtime characteristics so to be
versatile and yet scalable so that you
can throw like a Verity of different
workshops at them coming back to
relations well graph and many-to-many
relations they occur in many areas in
computing for example in compilers
random programming languages or program
analysis and one example that many of
you are I guess familiar with is control
for graph analysis where you have the
basic dogs in the code encoded as nodes
in your graph and the possible flows of
control in are encoded as edges while
talking about relations
that's nothing else than a binary
relation from nodes to nodes and how
would you go about and implementing that
with a collection library
well we've mapped and we've set at our
disposal and they're kind of universally
present in all these libraries so what
we can do is we can just nest sets as
the values of the map to then model or
the successors of one node in the graph
so while maps insides are universally
present in all these libraries there are
some languages or libraries like
language wise contact closure or
libraries like in Google guava that
provide nice API abstractions for that
so that you have for example a dedicated
multi map data type instead of having to
do the composition yourself well and in
Scala for example you can use building
traits in order to lift a map with
nested set to the API functionality of a
multi map and idiomatic lean protocols
you could use something like enclosure
you can use something like protocols
well that's just on on the API side
because all of these examples that I
gave you
they still internally use an exclusive
nesting of sets that are nested as the
values of math and there are some
particularly painful properties of that
explicit composition so one of it is
that the random performance is
suboptimal and I would like to just
flesh you some coded you're not supposed
to read it all in how you would
implement for example insertion in the
multi map with closure data structures
so there's already some of the major
optimizations involved a lot of like
dispatching case distinctions in dynamic
type checks and it doesn't get easier
with delete another thing that bothers
me for example about that explicit
composition is that they come with a
really high average overhead so
inclusions color for example you have on
average as 65 byte overhead per stored
key value here and if you think about it
quite a lot if you just want to store
two references like key in the value you
end up in you end up with having to
store eight times more in accidentially
encoding and immerse the explicit
composition can lead to severe
algorithmic limit
and in order to illustrate that I will
use immutable collections as an example
but what I'm going to talk about later
on like in the solution space
likewise applies to mutable collections
well in immutable collection libraries
you often find some sort of builder
structures and build abstractions are
used to efficiently stage stage
linearization or creation of immutable
object instances so here we see for
example a map builder on the slides
where you can in a mutable fashion use
several update operations like separate
operations here before you freeze the
state of the instance and then use an
immutable instance well some
implementations out there they're
particularly clever in how to do the
conversion from the mutable build
abstraction to the immutable data
structure and in particular for example
enclosure you have atomic constant time
conversions from one to the other so
that's the case for regular map data
structures for example but we're
interested in multiple multi maps aren't
we so if you want to efficiently stage
the creation of the multi map well how
do we do that with builders we can nest
set builders in met builders as we do
net sets in map but here you see the
abstraction breaking down because
suddenly instead of having like or one
conversion you have a linear time
conversion and that is because we first
have to iterate or F over all the nested
set builders close them before we then
can yield our final multi map so what
I'm saying here is why you can compose
immutable Maps and set with each other
the build abstractions they don't
compose everything so far that I was
mentioning well speaks about for why
don't you provide a dedicated maizena
implementation right so like just
engineered from ground up and provide
the data type that eliminates all these
issues that we had well that's something
we could consider
but the question is should we do it that
way should we add yet another data
structure to our collection library
because what it entails is that you have
invest a lot of like time and effort of
the money in engineering optimizing the
data structure and then also like
maintaining that over a long long time
span so well maybe it's not a way to go
like to add yet another data structure
and as I said before collections you
have some certain properties that we
expect from that so that they have a
small set of highly versatile
abstractions and as you saw we can
already compose multi maps also like
they have some problem so what would be
alternative solutions to tackle that
problem well let me take a step back and
just look at the differences between
maps and multi maps and if we ignore for
the moment all the API differences that
are definitely there like just from the
memory representation the only
difference is that for a regular map we
need to store one-to-one correspondence
between keys and values well it's like
for multi maps we need to cater for
multiple values that can be associated
for a single key so another possible
solution we could think about this well
instead of having two implementations a
dedicated hash map and a dedicated has
multi map why not having one kind of
like hybrid data structure that can
adopt to its content and like what we
hash map and mighty map at the same time
what could be feasible but what we would
expect from that is well if we use it as
map and like we purpose it with somatic
map interface that it then behaves the
same as a mess like in terms of runtime
characteristics space characteristics
and well what would you expect like for
me to being a multi map
well that is at least better than doing
the explicit composition of nesting sets
in map
and that's exactly what I'm going to
present in this talk the Exim cash by
NAB data structure which is a two-in-one
data structure so instead of adding yet
another data structure to the library
well you could think about replacing a
hash map for example to get the best of
both worlds and the core idea behind a
tree structure so on the slide to be
coming back to our control flow graph
example is to canonically store in an
efficient way
all the data in its most effective form
so if you have want one correspondences
with just sorted the key value table and
if it's really like if you really need
imagine that capacity we can associate
for example a nested set to it so let me
talk a bit more about axioms design and
in order to do so I would like to start
with the data structure that we are
hopefully know a jolly good attachment
in a Java util hash map well you have
internally one large potentially large
continuous array of key value tuples and
how would we for example start out and
add a multi map that's a feature to the
data type and that in a type safe manner
well one way we could do that is adding
yet another array like that differently
types so that we can associate keys with
sets of values and then we can make like
the case distinctions in order to store
in one or the other well but the data
structure I'm going to talk about
detects in treasure map as the name
already suggests is not a flat array so
instead is it's a very shallow prefix
tree and in my tour last year started
the jvm language summit I covered some
of the basics of the data structures so
for now all you need to know is instead
of like having one large continuous hash
map we chunked it up in small fixed size
hash tables and because we arranged that
in the tree while we also have to
maintain
Frances two subtrees and we can add that
by for example adding yet another typed
array to our notes all right everything
we did so far is just like extending our
basic data structure and adding
differently typed arrays and then
creating a tree out of that so that
doesn't look efficient at all and I
completely agree that's not efficient if
we would like have that many
interactions and they're well but from
literature in and from research for
sparse precincts three data structures
and hash tables we actually know how we
can efficiently optimize that for spans
later as a hash table so instead of the
complex structure you saw before we just
turn all the different arrays into type
slices that just store the same space so
how's that done like to give you some
intuition we have to associate some
metadata with that nodes and that's here
in the form of a bitmap of type lawn
where we use chunks of two bits for
indicating the type of a slot and in our
case that can be either like it's empty
so we use it later for compressing away
like the empty slot then we can
distinguish one to one and one to n
mappings and last but not least we can
reference sub trees and with that
information we can permute all the
elements that in the end what we get is
a data representation where with type
slices and while still maintaining the
oh one local properties of a hash wrap
per node well that's about let's say a
glimpse about a tree representation
internally but the connection is much
more than just its internal
representation so for example you also
need to maintain global properties and
track that like the size of the
collection so how many elements are
stored there and the hash code problem
and because we are like in the multi map
setting if
need like the MIDI map Kappa capacity we
have to store nested set and that method
does typically follow like the same
structure so with a container that has
decides and the hashcode properties and
internally we have done some structure
that maintains all our entries by
looking at the that schema are on the
slides you can already spot
some sort of redundancy there because
well as said before if you interested in
flat mod map data that like one single
abstraction for like on the API level
and for the consumer we are not
interested in nested set aren't we
because that's rather an implementation
video and instead of storing size and
hash codes like redundantly in all the
nested set why not just get reading or
getting rid of that because we already
know it on our top level type so we know
like the size like how many multi map
entries we have and also like the hash
code of that and once we do that we can
already go one step further so now we
have like an almost empty objected just
for example redirects to the set mode so
we can actually get rid of that as well
indirectly point from the mighty map
node to the set nodes so that gives
effect more space savings well that
optimization does become completely for
free because at least it requires one
relaxation in the API so the group by
operation on a multi map our relation
would need to be relaxed from returning
except like materialized set to for
example an iterable of values and that
is because we throw away like the size
information that used to be a collection
before so let me talk about some
performance characteristics of that
hybrid data type and we've to
distinguish actually two cases while I
said like it's supposed to be an
efficient map and it's supposed to be
inefficient Margy map so let's look at
both well the first thing if we use
axiom
as a map so we repurpose it with a map
API what we get is that random
performance is yes often the same as
with like regular try this map it
sometimes much better for particular
operations and adverse 25% slower and
the memory footprint is exactly the same
also we added like more flexibility to
also represent multi Maps there is no
overhead that we pay well look at how it
behaves like for multi maps random
characteristics usually speed up because
with a more integrated approach and
that's due to the bitmap encoding that I
showed you before so it's on average 25
to 50 percent faster than having the
explicit composition composition of
nesting sets in maps and what's
particularly interesting is that we gain
tremendously in footprints so like we
gain tremendously in lowering footprints
well how how can we split it up so if we
compare that to idiomatic naughty maps
in closure in color as I mentioned
before axioms design already like
delivers 1.7 times improvement over the
attitude and in the beginning I like our
briefly like flesh you like the
insertion and deletion code of closure
and if you're a creek you could already
spot the closure tries to do something
similar in order like to distinguish
singleton sets from like nested sets but
they do it like with having like a
dynamic type check and yeah and
essentially like the map in the set data
structures are not as efficient as I can
get for example by providing a dedicated
data type the does that so the design
alone already pays off tremendously but
once we then drop the sizing the hash
codes of the nested sets we get even
more some memory savings so that's like
around 2.4 X and if we then consider
something like memory layout
specializations we finally get to it
five times improvement over what ships
enclosure is cover well so far are the
design of the data structure itself but
let me talk a bit about
support so what would you require in
order like to develop such a data
structure or run look smoothly on the
JVM as I showed you before is a mixing
business and I'm not using I'm mixing
bitmaps in a very particular way that is
like having two bits aligned sequences
that kind of like tag the type of a slot
well if we have a look at what the GDK
for example has to offer in terms of
like bit twiddling operations we find
that there's already a lot and that's
present for example in the integer and
long data type with operations like a
bit count etc but all of these
operations so it just assumes single bit
they don't have any kind of awareness of
is kind of like longer bit sequences and
that's actually something that makes it
more painful to develop such a data
structures just let me give you an
example in order to see what kind of
operations are involved in order to
maintain the internal like state of DXM
data structure so on the slide you see
is 64-bit bitmap so like just they can
be stored in the long and just assume
for this example that we would like to
filter and tag all the theories your
sequences well you see them in red on
the slide what I can use it like first
applying bitwise operation or a sequence
thereof in order to highlight all the
zeros you get in Co Sirdar sequences and
cancel out the rest and once I did that
effectively reduce the information as I
need in a bitmap from two bits to 1 bit
and then with that I can subsequently
like use the for further processing well
because I reduce it I can also think
about compacting the bitmap and then
going on and especially like such
compacting operations they're not
there's nothing like present in the GDK
for example to help you with that but
especially for these use cases modern
CPUs already support operations or like
instructions that help you with more
complex masking operations in big
processing you're not supposed to read a
lecture
in particular operations like very
little bits of extract and parallel bits
deposit that lets you like do this sort
of like shrinking expanding and much
more and for the sake of experimentation
I added that like to the long and
integer datatype in the JDK and also
like edit intrinsic supporting the JVM
and I would like to appreciate like
having that support maybe in future
versions in order to like facilitate
developers who want to do more let's say
complex bitmap charismatic switch data
structures alright that's about platform
support let me switch to a different
topic and that is screen processing or
more particular parallel screen
processing as I said we have a hybrid
data type that can be like in map or
like a map our multi map and essentially
also gracefully convert between both
representations and so the particular
example that I chose for for stream
processing is a simple parallel word
count and that operates in a MapReduce
fashion will refers to mid toppers of
like form word and it's count and we do
that in parallel and then aggregate the
like the count later if you look at the
signatures and all the clauses involved
you will see that first of all we expect
as a return type and map from type
string to long and if we then look at
the collect Clause at the end we see
that we first of a group by operation a
group by key actually the turns
essentially our intermediate
representation of stream of tuples to
imagine map representation and then does
a reduction step in order to return a
map so we have met 19 maps involved and
we convert from one too near to the
other well let's do it a bit more
visually so let's assume our input
stream is like a is a character so we
have twice the word a in it and let's
assume we are now processing that on
four different threads in parallel so we
would like map the tuples on this rest
and essentially get maps with
entries so we just have one entry per
map something that I didn't mention so
far but it's also like very crucial for
a tribe is data structures like axiom is
that those data structures are very
efficient in merging partial States
because you essentially sift together
the trees in and that's much faster than
combining for example to java.util hash
map well let's do that
so let's merge together like two
intermediate results and what we then
get is two Maps already what with two
entries there's no multi mapping
involved which are set like two unique
keys per map if we then go on and merge
it further what we finally get is that
our multi map gracefully converts or our
map gracefully converts Amati map with
key a present twice and in this case
twice maps to the integer or long
constant one so now we need two parallel
mapping and merge the intermediate
results and well but our returns are
that we are looking for is actually a
map so what's left is actually doing a
reduce operation and if we actually a we
could even just do the reduce operation
on a fraction of the tree because if you
look at the sums all the unique key
entries they have the right time already
so we just have to reduce in this case
like one node where there's like one
particular multi map entry and once we
do that with the right count there and
we'll a we're done so that's just like
one example in how you could use such a
data structure like tribe a state
structure internally but you could also
implement operations like hash joints on
binary relations and so on and so on so
let me summarize what I was talking
about so far well first I was
introducing a I am a data type that's a
tuning one so that can be repurposed
like from a map interface from a multi
map interface and that efficiently
behaves as a map or MIT map regarding
the data you throw at it and they can
gracefully convert
between that you can use such a multi
map the basis for implementing at least
binary efficiently relations on the JVM
and I was also showing that that kind of
data structure can use up to five times
like memory improvement over working for
example get out of the box with Scala
furthermore I was talking about some
platform support that would be nice in
order to engineer some data such data
structures in the future and I was
highlighting like what kind of CPU
instructions are involved there and last
but not least I was talking about a
screen processing example where it's
simplified in that case but it just
highlights what you can actually do if
you have such hybrid data types that can
gracefully convert from one to another
representation so in total well what I
presented today is part of a larger
effort like a research effort where I
for example I'm are in particular
interested in different like in the
future variability of collection data
structures and also like solutions and
implementation choices in how to achieve
a certain goal and moreover also I'm
interested in to explore the design
continuum between collections and also
in memory databases because both of them
like like share a lot of things together
especially if you consider prefix tree
data structures and there is a large
continuum of trade-offs to explore well
if you're interested to see some code or
also like an have done in two research
papers that go a bit more deeper and
like back the performance result of the
app that I was talking about have a look
at the capsule Hedgehog collections
library on github and that's it from my
side so I would like yeah like if you
have any questions I'm happy to take
them and would be nice to have a
discussion going
I just saw immune thing no I mean in the
definition representative of a taco when
you have the the multi map you're
talking about I said but then the
example you show in the case of a you
have consensus of one so I mean yeah
right okay I know what you're getting at
ah
so I think probably the in the
implementation you actually have at
least notices well what you're saying
what you're saying is I was talking like
mostly about maps with nested sets and
well and then like the example that the
chose gives us a list in ethics it's
actually highlighted as like a nice
margin up there you arise like I was
focusing on another thing in my talk but
on the implementation side there is no
difference between that because it
essentially just have to know which sort
of like which data types to allocate for
nested set so all what the data
structure does the hybrid one that I
show to you is to effectively dispatch
between signals and instances of a
collection and like collections of size
larger than one and a singleton like
list can like be more or less unbox as
well as can as a singleton set can be or
any kind of likely different nested data
structure so it's this like the data
structure design is likewise applicable
to miss semantics and set semantics I
mean how you are adapting the deployment
ation depending on what you are having
at this equation chain that's the on the
long run that's the overall idea that
you really have like flexible adaptive
data structures that really can like
regardless of what you throw them like
internally transform to the most rivers
representation well if you look at how I
like how you have to like coated in
interfaces or in any kind of like
typeset representation that gets like
much harder so in that particular case
is I don't I would have like separate
implementations for like set semantics
and like list semantics but essentially
what's the only difference is internally
is secretary methods for what kind of
container you allocate and that's
depending on on the semantics
I was also showing you before the
separation between the auto collection
for example that stores the size in the
hashcode property and internally the
tree structure and that's actually also
like how I can repurpose one data type
of different API I reuse the tween data
structure like from a map API but also
from MIT map API and they can be like
since they can also like theoretically
reuse that from it like list multi
method API from Seth Moulton web api etc
and internally I have the dispatching
between singleton containers and like
larger containers
have you done any performance some
iteration or look up yeah cache
coherency properties of these data yeah
so can you get a bit more concrete like
what kind of performance yeah well I
tested I tested the performance of all
these operations and because I'm
predominantly doing research in in
persistent immutable data structures
like my mock my to go charges to compare
two explosions Carla and I mentioned
before is that like if you use it as a
method like most of the health
operations are the same some of them are
dramatically better and like others are
for example up to 25 percent slower the
dramatically better bodies actually is
actually duration as a use case and that
happens in how I can internally
effectively dispatch between different
data representations and in earlier work
that I also presented last year the jvm
language summit I was presenting a
successor data structure to the hash
room map try that improves on on galaxy
cache locality and all that comes
together here and that the data
structure like axiom is more or less a
further generalization of the gem data
structure I presented last year okay so
you made a trade-off not stuffing the
hash code into the into the data
structure to save on space yeah that's
like when you do a lookup from a map
you're going to have to do when you get
your get your value out you're going to
do how to do a comparison and often
rather than comparing all all of the
values themselves you compare the hash
code of the values you you may have to
do a calculation that way it's sort of a
trade-off between space and time that's
all right if an offender correct your
question correctly is your thing that
I'm not storing the hash code offer the
elements there let's write like for debt
performance numbers that I showed you
here so for example if five times memory
improvement is like without storing the
hash codes or memorizing the hash code
call it like that but in the research
paper about the champ data structure I
was going into detail in like different
approaches in like what does it take in
like memorizing the hash code or not per
element level and
also the second level is like if you
stored it and memorize it on a
collection level so there are different
trade-offs involved and all these
different traders can lead to different
numbers of invocations of hash code and
also equality as you already like
mentioned that like if you don't store
it then you have like probably more
comparison of values that you can rule
out otherwise and but the nice thing is
that we actually at the gem data
structure for example it's likewise
applicable to storing hash codes or not
and you can even do it like more
efficient because on a note level like
aggregated I can store the hashes for
different elements like in one
integrated for example and in my point
of view it's the best combination of
what you get like from muscala in the
closure data structures because scholar
for example memorizes the hash codes in
leaf nodes so like in that Java util
hash map you have like one entry let's
say class that stores like a key value
and the hash code and that already like
adds up to a lot of like memory
consumption whereas like a closure
doesn't do it like similar to what I
presented here and you get array of the
getter essentially rid of like one layer
of the tree like for the leaf layers
because the inland one level up and at
that level you can actually also like
store all the hash codes of like the
aggregated least there so it's adaptable
I'm not using it like myself in that
configuration but like it's described
and in the paper that I mentioned thank
you
right more questions hesitations comment
otherwise thank you for attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>