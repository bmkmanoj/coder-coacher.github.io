<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JVMLS 2015 - Java Goes AOT | Coder Coacher - Coaching Coders</title><meta content="JVMLS 2015 - Java Goes AOT - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JVMLS 2015 - Java Goes AOT</b></h2><h5 class="post__date">2015-08-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Xybzyv8qbOc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm really happy that I can present this
here because I talked to some people
yesterday and they were asking me what
are you doing you disappeared and that's
the reason I disappeared because of that
and since you probably never heard of it
so that means that we are doing this
enclosed and it's confidential thing and
we couldn't talk about it until today so
I'll present that I'll talk a little bit
about it why we're doing it now there
are some people in this room who have
done this before it's definitely not a
new thing right everyone knows how it
works and we have other Java
implementations that do that and there
were other companies before is that did
that so this is important because this
is still experimental and we don't know
exactly in what java version we are
going to deliver their send so on so why
why why are we doing this I'll briefly
talk about that then a lay out a little
bit how our alt compiler works and how
your alt compiled stuff then I talked a
little bit about the container format
with which one we chose and why we did
that then where we are right now what we
support what we are planning on
supporting in the future again it's not
tied to any chapa version at this point
so and then I have a bunch of numbers so
different slides I try to do as many as
I could and I hope we can we can talk
through all of them so why did why did
we do that customers are asking for 80
forever right there there are kind of
three different forums the question is
asked and the first one is how am I you
know command line start up fast and we
are at a bunch of times from Charlie so
and then last year or the year before
nasan was always complaining oh I want
to reach my my peak performance faster
rate
and then there's this other group they
don't like the optimizations they don't
like when their application gets
interrupted when they want to do a very
important trade so these are the reasons
we and over the past years we always
said well you know we have to your
compilation and c1 is really fast and so
we try to avoid doing it because what
most people requested was well if I if I
start up my application and it took me
20 minutes to get there why can't we
just take that code that's c2 produced
and sterilize it down to disk and use
that that's kind of the naive approach
people out set outside as a chest and
it's it sounds easy but its not position
is always slow no matter what well that
but the main problem is that you have
different phases in your application
startup and c2 you know it it emits a
lot of uncommon traps and the
optimization for things that haven't
been executed or are not hot and and so
if you would technically I mean we never
tried it but that's an assumption if you
would see realize down the code you have
and you start up your application you
would be up like crazy all the time so
all the the improvement to get quicker
to peak performance is gone so you need
a different approach here and and we
think that the solution is static
compilation basically well one thing I
want to mention here is also if you
would see realize the kokesh down to
disk you have problems with you know the
class loading order and things like that
because it all needs to be loaded in the
same order and that's pretty impossible
so
we decided to try to write everything in
Java because it's easy we we basically
done that except the part that writes
the container and we did that we're
using labelled now because we started
out we didn't have a lot of time and we
started out using the valve because we
knew okay it's is producing the right
container for madam we don't have to
worry about that and we wanted to
replace it later but we didn't have time
anymore maybe we'll do it in the future
and we're using graph for the code
generation and that's that has also a
couple reasons one reason is that our
existing compiled is a very jet centric
you could do it with c1 right because c1
is is compiling the whole method and
it's but it's doing a lot of co patching
so you would have to replace that with
something else and the one downside is
if you would use an existing chicken
powder you you would have to or it would
just happen that you destabilize it
right and so this way when we know okay
our chick compiles do their work and we
have this out a compiler which by the
way is written in Java which is really
nice and we can just use it and the
other thing and you've probably heard
that in a different talk by Christian
move it wimmer they've they're using it
already for a ot compilation in
substrate bien right so we are just
taking it and we we enhance it a little
bit what we needed and it worked pretty
fine the the ultimate usage will be
through CH a link so che link will have
a plug-in interface and then whatever
chain link step you do you can say oh
okay but if you you know package these
classes together I will I want you to
also alt compiling we don't do that
right now because tailings not in katy
canine yet but that will be the yen
solution and
and we're in right now it's just a
standalone tool and we're doing all the
optimization that growl does except
we're not using profiling data because
we're doing static compilation we don't
have any and if we are we could but at
this point we don't and we compile all
paths possible right and that's
something I said on the previous slide
is we want to avoid the optimizations
and uncommon trips and stuff like that
we want to compile all the paths and we
want to be able to run them it's not
always possible most time and we do very
very limited in lightning and I want you
to remember that when we look at the
preferment slides later we only in line
leaf methods that are static a final and
the main reason for that is not because
we're limited by by the in learning of
gras or because we haven't implemented
anything that that that wouldn't work
it's because we didn't have time yet to
figure out a good inlining policy and if
we turn on the e-learning policy that
growl has compilation times shoot
through the roof in my you waiting hours
because you don't have profiling data so
it's in lining everything and that just
doesn't work so we're kind of studying
profile that you could use to achieve we
could yeah of course yeah I mean it
depends on on what you are compiling if
you do a training round or something
like that yeah sure you can in check
profiles so the LT compilation will have
two different modes one we call tiered
aot and the other is non tiered alt so
the tiered aot generates simple
profiling code it's it's it's the same
as a as a in a tiered compilation see if
the matter now we have three four
different tiers right and tier to
compile is a is a c2 compasso c1
compiled with simple profiling it's
basically the same so we produce that
code and then when we when we hit the
threshold invocation counters or back
Chicanos will recompile with see one
first which sounds weird but the reason
is the simple profiling does produce
type type profiling we only have
counters but we also need type
information and so we compile with level
three first get full profiling and then
we let see to do its job and if we would
generate a tier 3 type of code which is
with the full prevailing and tie
profiling in it would be just a too big
of a performance hit because in a tiered
system you run any interpreter first not
very often it's it's like a hundred in
vacations or something like that but
it's still enough to know yeah this is
kind of warm hot right but in the air T
will you run everything compiled and you
run this one myth this method that only
is executed once and you're producing
all that profiling data that you don't
need so we stick with the simple one and
the non tiered mode is you're running on
a OT code only no sono profiling is
going on it's basically what see one
would produce right you don't have too
many uncommon traps you try to compile
all the paths and you run on that and
that that's interesting for the high
frequency trading for cloud because you
can share code and for embedded where
you can for example jet compile or
you're restricted by memory did compiles
jittery compilations can happen if for
some reason you had an uncommon trap
would be optimized for whatever reason
we've seen that so it happens it you
could you know you there are ways to to
get rid of that if you really can't do
jet compilations but this is what you'll
see you later so the container format
we decided to use shared libraries a
while back John and I were talking about
this and all cha know we said we should
use shared libraries and I said well
it's really a lot of work to implement
that and so on and so on to for example
class data sharing right they're using
their own proprietary format their
serializing the thing down to disk and
then they load it back in up the same
address and everything's laid out and
perfect and it works on all the
platforms and but it's it's really a
trade-off of on of a well-known format
versus you spend your own the pros are
don't do what other people have done
before we don't need additional loading
coding that in the JVM that's that's a
plus right we just elope Amit and deals
in MIT and there we go all the existing
tools just work right you can inspect
the library you can use gdb suddenly you
really have stack frames in gdb Wow and
the last point is the point that John
always pointed out its operating system
engineers have spent years and years in
optimizing loading DS and and we see
that later libraries can get really big
and we need them to load fast if you
coming back to AB CD s 2 2 CD s it's
definitely faster to map it to an
address right because you're just
mapping and it's there or you don't have
to do any relocations but the problem
with that is it doesn't always work and
this has to work all the time so we need
pick code we need relocations we need
that stuff working and so we would have
just done the same thing that jet
libraries do today without help from the
operating system so we chose that the
cons is we have to do it for every
operating system that's really annoying
and it might not be as flexible as if
you do your own right with your own
format you can do all the hacks all
there you want with an existing format
you try to find ways to make it work and
so far we didn't hit anything that
didn't work so
we were pretty happy with the with the
choice so what's the current status
mainly because of the container format
we're only supporting Linux at this
point and and so we as I said earlier
we're losing using the belv Christian
just asked me so what are you doing in
Windows is it yeah so we only support
Linux 64-bit at this point and what we
support we try to compile other things
as well but what we have tried to really
support is to chop it out base module so
because that one's used by everything
that starts up right so if we can if we
can compile that and get some
performance improvement out of this this
would be perfect is done every
application kind of has an advantage and
it's a new compiler after all right so
growl has been around for a while and it
has been used in in different scenarios
and so on but it's it's never been a
production compiler and we're using it
in a different way grog was tested it
sits around for a couple years right and
it has been tested for a couple years
but only in a chip configuration in it
would never really tested side substrate
vm and an aot setting so testing is
really crucial here that we you know
when we compile Java debays we have to
run basically everything again on an alt
compile java web based library compiling
custom applications will probably be
experimentally in the first version
because we can't we don't have
experience with it right it's it's the
same as if you would introduce c3
tomorrow you wouldn't come out with a
with the version and say oh yeah every
customer can compile everything that we
need them an experimental period what
people can try it out and say yeah it
works for me or no it doesn't you
planned so on its own we have many
applications inside and testing
frameworks and stuff inside Oracle but
it's not it's only a tiny fraction of
what code is up there
so but we want you to try it right so
we'll find a way to get the code to you
and and there so as I said right now
it's it's a commercial feature I think I
don't know run probably yeah
he was
we have specialists for these questions
so in the future what we were planning
to do I mean yeah we need more platforms
if if we want to make that a standard
feature of the Java platform and when we
compile Java the base we wanted all the
on all the platforms right we want to
compile more modules I was looking
around a little bit and you know Java
basis the obvious one I've seen that
Java dot XML is used quite a lot for
starting up the application right you're
reading in XML files configuration files
things like that I don't know about
logging maybe and the first two you will
see later in the slide so I've compiled
them they work but I don't know if we
are going to support them in the first
version jdk compiler would definitely be
helpful if java sea is just a tiny a
little bit faster would help it would
benefit everyone nasco and yeah why not
but we'll see we appreciate it because
the initial is so when you start to when
you start the mass from a JavaScript
programs from a swim there is always the
site from initializing the JVM you also
need to initialize all the great ass one
runtime infrastructures yeah we can we
can a little bit i have one benchmark
with mass or and so we want to support
custom applications right where one
custom was to be able to compile their
own application that's that's the goal
and then as Marcus mentioned we want to
feedback profiling information because
that makes it faster especially in the
in an entier case and I'm again thinking
about the high-frequency trade our guys
right they know exactly how the
application works they do with training
around this they say that they save
their profile and recompile it in a non
tiered way and that would be perfect but
we don't have that yet okay numbers
so these are it's a fork or machine and
the interesting thing is that there is
one so you want compile a thread and to
see to compile threads so it is
basically if you start up something that
compiles a lot you only have one thread
left for your application it's really
unfortunate I don't know if you can see
it we have one millisecond slower that's
really annoying and when we started out
with that working on it I I haven't even
looked in a long time but back in March
or something hello world was 500,000
bytecode instructions executed and
interpreter right and so we we were
working on this and working a lot it
should get fast it should get faster
right and he never did and it's it's and
that and the really annoying part was
the more we compiled all we won with
with making up not executing it an
interpreter we lost the time because the
library got bigger do you have more
methods in the library when you search
my method it takes long it takes longer
to load it in so we want it to be fast
and hella well it would have been
amazing but it it's not the case on the
other hand Chava see it's this one is
compiling helloworld dot java basically
right it's a small thing it does
something so we have t or t one it would
just see one you know with tiered stop
at level 1 then this is a regular tiered
system non tiered is both Java dot base
and the application compiled in a non
tiered way and tier it is both Java
debates and the application compile
integrally so as you can see the non
tiered is faster than the tiered one
because you are not executing the
profiling code but this is a thirty
percent improvement which is really nice
and on a next slide and the important
part is the tiered aspect
so we you can still hit peak performance
with that because you're producing
profiling information and we see the
same with Nassau it's also a thirty
percent improvement that's a I think
executing empty javascript file which is
really nice I wanted to have that on
hello world but you know so this is Java
Sea compiling things that I found so we
are jvm see I don't know if you've heard
of this this is something that we're
integrating soon and it had 200 classes
I thought okay that's cool i'll try that
we are faster than a tiered system but
slow at NC one here its 357 classes so
and it got worse and the thing that's
really annoying is this why I see one so
fast I don't know I haven't haven't
benchmark Java Sea in a long time I
don't know if that was always the case
or it's a regression or something I
don't know so we're getting slightly
slower here and we're not exactly sure
why this happened and you'll see that
later on another slide as well I think
it's because of the of the heuristics
we're using so we tried it especially
for for this presentation here we try to
optimize startup performance for
applications and then it comes down to
what are you profiling and we on eager
he he he he made a change that we are
not profiling very simple leaf methods
because we thought all right if they're
probably in line somewhere else are
called from somewhere else and so we
don't have to profile them and that
that's I think that's it but we're not
exactly sure so at one point yeah
okay good so but still so this this is
annoying but we can get rid of that
especially on a later slide I I know
that we we hit exactly the same peak
performance with with a tiered aot so
it's just a matter of finding out why
that's the case and then yesterday I got
I got an email from one of our
performance engineers and they have
their own they know doing benchmarking
way better than I do right and so they
have set up all the assistants and so he
sent me this one where Chavez I think
it's a Java Sea of chicken 9 is
compiling the source code of a seven
chalice II and then suddenly cheered was
faster than see one so I I don't know
why I don't know anything about this so
I have to talk to him why that's the
case it might be Landers or I have no
clue but I'm but I'm compiling jdk nine
chava see here yeah that's that's the
JDK campana module here 360 classes I
don't know if javis Chavez see from Java
7 was bigger different yeah
okay no java ce9 to compile java c 7
yeah you should know why why that's the
case right you should not okay so yeah
it's we have to look into that I mean
it's it's definitely encouraging that
this is faster it's just a matter of
finding out why the other ones not and
then you know jruby it took me a long
time to collect all these numbers so so
the leftmost one is there def thing they
are doing and then turn off their JIT
compiler and things like that so that's
the fastest you can get today and then I
have 31 2 tier to tier 2 1 C 2 basically
and then to non tiered once and this is
interesting because of what that said
earlier in the first version will not
officially support compiling custom
applications right so I'm showing this
number two what could you gain by just
compiling javed up base and this is the
tiered aot thing so Tom and Charlie they
told me oh yeah try disable chance
because it's basically doing noche Ruby
stuff it's only Java less so you can see
with the non tiered version we are
faster than a deaf one this that's
actually encouraging that the tiered one
is exactly as fast as as the deaf one
yeah and then you'll see on the next
slide why so here again the non tiered
one is faster we're getting slightly
slower here oh yeah I probably should
point out that's that one as well so you
get some you know some improvement
compared to two regular tiered run it's
not that much a little bit so
the first version will help you in a way
but what you really want is to compile
your own application and that one's nice
so this is how many gems were there like
yeah okay so they said that's important
I don't know why but that's really nice
compared to this guy and the reason why
this is nice is this slide so I took
mention 9000 then I run the classic
benchmarks and and you can see that the
first bar is there def version which is
kind of not so fast because they turn
off their compiler and the yellow one
which was always faster in their
startups compared the yellow one to the
first one the way about you know 2x
sometimes more faster in in the peak
performance then and if you do tiered
aot with both both java based on JRuby
you you always hit the peak performance
of which color is that the third one
yeah no its core I this the fire hi is
better I've never used spanish 9000
before so it's it's kind of prints the
time when it runs and then it calculates
the score i don't know how high is
better yes sorry so we want to compare
the the third column this guy to the
last one because that's that's what we
add a loose here in that case but
usually a gain compared to a regular to
a regular tiered run and and you've seen
before I think the improvement of a
regular tiered startup vs vs this guy is
fifteen percent if I remember correctly
fifteen or sixteen percent so you start
up fifteen percent faster but you reach
the same peak performance
okay that was that yeah it looks pretty
good and and and Charlie said that the
for development the peak performance is
not that important so that the yellow
one would be sufficient for developers
yeah so that's that's pretty nice so I
and then I did a spectrum 98 run mainly
because it's a very short running
benchmark and I wanted to see are we
fasted on the first iteration and where
are we after the fifth so I just picked
one so you can see the first one is a
tiered c1 c2 and then we are interested
mostly in the yellow one which is tiered
aot we are except one jack we are fat I
couldn't run Java C because it just
breaks all over the place we are faster
in the first iteration and then when we
go to the fifth one it's a we are
slightly slower which is about the same
as I was talking before it's this five
to ten percent that we lost somewhere so
I'm pretty sure if we can fix that
problem that we have we hit the same
performance as anything else yeah maybe
not in this one but with respect
benchmarks it gets interesting to also
look at the non tiered a thing to get an
idea if you're if you're not recompiling
what performance can you expect so you
see yeah sometimes you lose a little bit
or a lot but there's a better one just
one so spec gbb 2013 this is ten percent
and this is 5 so this these are the ten
percent we're losing somewhere but I
remember and I could have pulled out the
old numbers but I didn't I remember
doing this in tune and in tune we had we
had to
p performance so i remember that the
interesting thing here is non tiered so
how much compared to a tiered system are
you losing if you were if you're only
running on tier dlg.code this might I
didn't look into it this might recompile
something because it d ops could be the
case but on the other hand as i said
earlier you should remember what
inlining we're doing now so these i
think its twenty or twenty-five percent
you're slower that's what you can expect
what we are we are in lining basically
nothing and with that we can see the
later slide it's as capable now is it
basically that doesn't work we are in
lining nothing so that's what you can
expect high frequency trading eyes
that's where you and so I also looked
into I used back jvb because everyone
knows it it's not it's not a perfect
example because it's not a lot of code I
can't remember how much it is but the
char files I mean there are many char
fans like 20 or 30 char files that you
compile and actually when we compile oh
yeah the previous slide was that was
with them I think I didn't state that it
would was Java debase and spec aot
compiled we compile everything so we
didn't go through the trouble figuring
out what do you actually need to alt
compile so we're compiling just
everything because it's easier and so if
you if you do a jbb 2013 run that's the
that's the code cache usage and it's let
me see nann and methods that angular
thing down here that's basically in the
interpreter and some adapters and then
you have the profile and methods which
are c1 compiles and then these are your
C to compile the ones you want that are
fast and if you if you use an entier de
OT you get down to this I if you do
tiered it there
not a lot of change here then we
basically end up with the same a little
less but so I didn't put it on it
doesn't make any sense but so one thing
you can see is we're definitely
compelling something and this can be the
optimizations or what I actually think
it's some reflection stuff because we're
spinning back codes and right yes yeah
some some with some methods we can't
compile for one reason or the other and
so that might be the case as well so so
you save about I think it was 22
megabytes of your Kokesh if you if you
run one instance and this is just a
number of blobs so again this done here
is only adapters and these are your
compiled methods it is what 8000 and you
get down to two thousand and this is
these are the libraries so lip job
without basis so that's what it's called
it has 44,000 methods compiled in it it
clearly spec gbb doesn't need all the
44,000 right but we didn't we need some
kind of analysis that figures out which
ones do we actually need to compile to
get that size down but it's about a
hundred megabytes that one library and
the dark part is the read only part
that's the one you can share and the
other thing is you know all the G ot
relocations and all that stuff and this
is spectra baby with all the
dependencies that are in the lib
directory it's combined it's 40,000
methods it's really a lot i don't think
everything's needed from the especially
from the dependent see libraries and
this one is i think like 85 megabytes of
most of it can be shared so if you or if
we have a way and analysis to figure out
what we actually need to compile to get
you to the speed you want
you can and you run multiple instances
of the same application in on the same
machine you can share all that code save
all the you know 22 megabytes of your co
cach and then it you know at X amount of
instances you definitely save memory it
has more methods than Java dog base it's
like 50 thousands or so yeah so it's 120
megabytes something like this but again
especially with you guys I didn't know
what what to compile at there's work
yeah I'm sure there's maybe only some
packages that you need to compile it but
I compile everything you get millions of
exceptions when we when we try to
compile JRuby because of all the chip
ffi stuff and and that static class
initializer look for files that that
don't exist and things like that it's
funny but so I want to quickly do show
you this as well it's a it's kind of an
odd use case because no one of you will
ever do something like that but I think
it's interesting to show that we that we
can speed up the bootstrap of gras and
for you who don't know what a bootstrap
is so job is written grout is written in
Java right and if you if you turn off
tiered growl is the only compiler in the
system and so basically when you
schedule a compilation with Gras it it
starts doing it it starts doing it
interpreted but while it's doing it it's
getting hot and it compiles itself right
and so a bootstrap is basically
scheduling all the java.lang.object
methods for compilation and waiting for
all the compilations to be done so that
means it mostly compiled itself and a
couple methods you scheduled so this
takes on a machine the one machine i
said like with four cores and it uses
all the cores to compile it takes 28
seconds to do
if you do a non tiered mode which is C
one's kicking in first compiling growl
itself and so on and so on you get down
to nine and then if if we alt compile
that jvm CI which is the interface and
aunt growl we get down to 263
milliseconds so and we compiled three
methods these are the java.lang.object
methods why 263 should only be like only
541 the reason is you have to initialize
some stuff and that's not going away so
what we are planning to do with a OT is
we won we want to have a static class
initialize analysis some that we can
actually not only EOD compile but
completely foldaway but we don't have
that yet so you you have to wait for
this and you also have to wait for class
loading and I think that's the major
portion of this there's also some time
involved in the growling the
civilization itself but we could
technically get rid of that you know
fire off when you start up the vm fire
off a compiler thread and let the
initialization do you know on the side
while the main thread is doing something
but it's all the compiles are waiting
for something and that's the
initialization part if you do it with a
growl only bootstrap you know teared off
and all that stuff you're waiting the
same time it's just a little longer
because class initializes need to be
interpreted while we have a day of tea
compiled so they run faster and I'll
throw this up and it's only you know
this this graphing is we're just
experimenting with it and we see how far
we can go out and again we are not in
lining a lot that's a major issue
especially for growl so I took a random
dacapo benchmark and looked at the first
couple compilations that are happening
off the bench mark itself and and this
is where we are so this is the growl one
is the with the 28 second bootstrap
right so you get the best code that you
can prove produce foreground and the
other one is the non tiered air to you
which is important because what you want
is if you have something like growling
your system you don't want it to
interfere with your application startup
so you actually want to compile as non
tiered so it's not recompiling itself so
I did that sometimes it's embarrassing
then these are really small methods like
they only produced 100 and 200 bytes of
output code so they are in one
millisecond to millisecond they are okay
but as soon as it gets a little bigger
and I took the seven because there was a
one milliseconds in between the eleventh
as soon as it gets bigger and it has
more work to do it allocates more memory
and that's because we are not in lining
enough so the escape analysis doesn't
kick in and then it's just slower yeah
but it's it's encouraging and I think
that was it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>