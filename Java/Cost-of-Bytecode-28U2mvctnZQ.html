<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cost of Bytecode | Coder Coacher - Coaching Coders</title><meta content="Cost of Bytecode - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Java/">Java</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cost of Bytecode</b></h2><h5 class="post__date">2016-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/28U2mvctnZQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Duncan McGregor I'm going to talk
about the cost of byte code in a dynamic
language where you have large amounts of
code and we'll see how large large is in
a couple of minutes first in honor of a
Jay creep that's going on at the same
time over in Europe all language
developers are liars and everything i'm
going to talk about is experimental it
may never land in a product you
shouldn't base any like purchasing
decisions on this everything i tell you
is a lie except that it's a very old
logical paradox but it's a good one as I
said I'm Duncan MacGregor I work for GE
Energy connection i think when i called
in cambridge in the UK we change your
name fairly regularly hard to keep up
sometimes and and I lead the platform
group for small world which is a product
of hours and I'm hard work 179 on
twitter if you want to follow me what's
small world small world is a
geographical information system it's got
the Corbett's core data store that's
written in c it's got all the
application logic on top of that written
in a proprietary language called magic
and it's focused on topologically
connected networks and it runs really
large applications for modeling
electricity gas telco that sort of thing
we've got lots of applications they can
be led on top of each other in all sorts
of ways and lots of our customers just
shovel in absolutely everything they
possibly can magic is a dynamically
typed language it was inspired by small
talk unless
things like that so it originally had a
basis of using images and a few years
ago we decided to port it from the old
vm that we had to the JVM so but we
didn't have to maintain our own
infrastructure didn't have to marry
right around garbage collector and so on
and so forth and could use the
facilities provided in the JVM and
that's been quite successful we've had a
first two releases out with that
technology the second one came out you
know just to cut a couple of months ago
and we're keeping on doing support work
and exploring new features how large is
large well I tried a a a typical
application not even pulling in multiple
things together and when I looked at the
counter of methods in the method tables
where we're over a hundred thousand
methods in there so not small thousands
and thousands of classes we have I mean
some of those methods are going to be
tiny kind of field access of things
stuff like that some of the others join
bits of this this logic hundreds of
lines long nobody's ever going to
reflect them they're never going to go
away because frankly maybe nobody
understands them if you wrote something
as a customization for a customer it's
just going to stay there and it's just
good at prime B performance as possible
and so it's not all bad though it's not
as bad as many other dynamic languages
for some of the facilities it's got
objects do not change their shape so
it's an exemplar based language
exemplars have a set of slots when
you've created an object that's it it's
not going to suddenly acquire new
properties we don't have the JavaScript
or Ruby problem of having to track the
shape of every individual object some
things are
are well defined and can't be changed so
one plus one is always going to equal to
you can't redefine your way out of that
so we can do constant folding and things
in in the compiler if we want to yeah
what else okay yes so so standard
library it's almost all in magic itself
there are some a small set of primitives
relatively which have to be implemented
in Java and the ovm they were
implemented down in the sea but they're
fairly small be almost all the libraries
in written in magic itself and well
because we control the language and it's
for our stuff if we want to change
something we do have the ability to do
that and I'll talk about that a bit
later as well we just have to make sure
we're not going to break anybody's
existing code or if they if we do we've
got to be quite careful and you know
tell them in advance and that sort of
thing so quite a good position to be in
our approach so far has been everything
gets compiled to bytecode in advance now
whether letteth on whether in advance is
at the point where you hit return is
your when you're at a ripple compile it
compile what you've done to bytecode
then and then run it or more commonly
we're building a product and we're
compiling everything down to bytecode at
the point where you build a product and
then our source code has to be shipped
it's all just a bunch of jars and some
extra resources and things like that so
that's that's what we've done so far we
rely on budget for optimization and it
does an okay job of a lot of things but
it does mean we are limited in what we
can do for optimized
erations we've got to do things that are
universally going to be safe and that's
a bit limiting and so what of other
languages done to optimize themselves on
the JVM well there's been two so real
approaches oh sorry wrong slide yeah so
what costs me incurring from this
approach one we've got a time cost of
how long does it take to to generate and
run bytecode for you know small small
chunks of code for aren't going to be
run multiple times and an opportunity
cost what can't we do because we can
only make these really safe
optimizations see the lesson don't
reorder your slides when you jetlagged
you'll forget okay I'll have other
languages done this sort of stuff on the
JVM there's been two real approaches
that have been done one is to try and be
optimistic this is the sort of thing
that nice ones done start by assuming
everything is an int back off two
doubles or objects as you go as you see
things don't fit into that model that's
got some costs you need some
infrastructure to cope with that Marcus
has talked about that previously here
and you'll have some bite cogeneration
going on the other approach is to have
an interpreter or something as a first
stage that lets you gather enough
information to make more decisions again
that's got some warm up costs and it
means you've got to be able to recompile
your methods from from whatever
serialized form ver in which we haven't
had before so why not be optimistic
approach can you imagine what would
happen to try and regenerate the
bytecode for a hundred thousand methods
at startup as you go through running
them especially when you're big methods
you're going to try and make as many
optimistic type assumptions as you can
it's going to be really really painful
we fought about it and we never did that
approach and Nathan's hit problems with
much much smaller code bases so I think
we were we were justified in in not
doing that one so what about an AST
interpreter well this whole project gret
of a little prototype compiler and
things that we put together I sort of on
the sly it's very simple it built to
bytecode as quickly as it could but that
meant it was really only suited for that
very hard to manipulate it and turn it
into something that was going to be
interpreted lots of stuff that's all
fixable but it was it was it's a fairly
long road to fixing all of it so we
didn't start by taking that approach so
plan B no Machiavellian port here to get
towards an interpreter at all I swear
again I may be lying so I introduced an
IR into the into our compilation chain a
while ago is really to fix a couple a
couple of important issues some of the
constructs commonly used in the language
were very hard to get right when
compiling just from an AST it was quite
quite hard to analyze those correctly in
that form and it was a bit painful and
we wanted to do optimizations that
crossed AST notes and people started to
put those in as special cases of looking
if I've got the following types of
children then I'll do something special
but it was instead of doing a normal
compilation but it was starting to get
painful and I really wanted to get us
away from that and
it's quite an easy soe quite an easy ir
to work with it's fairly thin sort of
started as a fairly thin layer over
bytecode and that was because it was
easy that was to get it in quickly and
to prove that it was useful but it
proved very useful because it gives you
much better error reporting than ASM
does if you've ever had a bug in in
using amusing azum then you may get an
obscure error message and you may find
you get a method with no error message
but somewhere in there there'll be an
eighth row followed by a bunch of no ops
and that's probably not what you wanted
but you don't know about it at compile
time and and it turns out it's been a
really useful platform from which to
build other bits of experimentation and
start to clean up the compiler so we've
been doing a lot of that work and so
what about an IR interpreter well we
have an IR how mysterious how could that
possibly have happened and it turns out
to be very easy to turn the IR graph
into something interpretable very
quickly building the interpreter is
quite an easy job in itself as a little
bit of finessing around monitor enter
and exit type nodes if you don't want to
use on safe but that's entirely
handleable and there's a little bit of
optimization you can do round exception
handling and things like that but you
can get something up and running even if
it's not completely optimal and you can
do start to do some experiments so we
can start to compare everything going
through the interpreter everything going
through the bytecode and doing a mixed
mode of compiling all the methods to
bytecode because you expect them to be
long running and compiling all the top
level stuff to the interpreter because
you don't expect that to be long-running
but it obviously be a fourth mode here
of interpreting stuff and when it gets
hot turning it back into turning it into
my code but haven't got that implemented
and working yet but it's close so what
sort of time costs are we seeing on this
stuff well the old vm for interpreting a
large number of tiny bits of of code you
commonly see this in database creation
scripts and all sorts of areas like that
and indeed in because our database is so
tightly coupled to the system movie data
invert the information in the data
dictionary on default values for fields
and things like that stuff that has to
be ingested at some point as quickly as
possible I did a small benchmark of just
evaluating hundreds of thousands of
effectively random numbers and a little
bit of other stuff and measuring the
performance so on the old vm this sort
of this benchmark took 24 seconds or so
on the new vm with on java with bytecode
took 16 seconds so nobody's been
complaining about the speed of this
thing in general because it is faster
but if you try running this through an
interpreter rather than generating
bytecode it's four seconds or so and
that's on a bad day it's not been
optimized at all most of the time of
that is Potters in the parser which is a
very thin layer in lowering to vir and
lowering to the interpreter graph and
the bits of analysis that happen at that
stage most of the time in the bytecode
case is spent in ASM and then app and
then once you've output the bytes
loading valve validating and then doing
the reflection required to run the
method on the class what you got so
certainly for for small cases it's
looking very promising and I think I
haven't got on this graph is what
resources are getting used
if you look in something if you look in
visual vm or whatever to look at how the
garbage is being handled the
interpretive case is really working well
stuff is mostly getting used and then
discarded entirely you needin so nothing
much is escaping at all and the bytecode
case you are building up a large number
of temporary class files and even if
you're using tricks to with class
loaders to to make sure that can be
collected you build up a huge pile of
them in meta space before it all gets
collected away again so looking pretty
good let's try some more experiments so
cost of bootstrapping the Corbett's of
the system from source so this is
something you commonly do in development
but not something you're doing in
production so bytecode is taking about
sixteen seconds interpreter is down to
11 seconds so this is all looking
looking very encouraging what happens if
we try a more production sort of
environment so booting the entire core
application code not from source so the
way it would be packaged up so either
from by coding class files stuffed
stuffed into jars or from a mixture of
bytecode and interpreter graphs graphs
stuck into jars well if we look at the
bike code it's about 17 seconds or so to
boot the entire system there's a lot of
stuff getting loaded we'd like to reduce
that time we look at the interpreter not
so good so bit disappointing if we go
for the mixed mode we're actually pretty
much on the nail with what time taken on
the on the bytecode side so it's not all
loss but it's not really winning us in
time in particular
so so what went wrong well profiling
start up as I'm sure any of you have
done it will know is an absolute pain if
you use visual vm and eats friends you
will find that you're getting bias
because of safe points so you can't
really trust your results too well if
you try to try using jfo the likelihood
is you don't get enough samples in new
threads you care about to form a good
picture so JFR doesn't handle the native
stuff and it doesn't manage to collect
enough sample points in the time on the
threads you care about makes it hard to
tell if you use intel vtune or purple
things like that mostly what it tells
you is you are in the interpreter the
bytecode interpreter or your own one so
not very good at giving you information
however having said that the profiling
from pure byte code is almost impossible
to find stuff from easily if you try
doing a flame graph of it it's just a
towering inferno because you've got so
many different classes getting loaded at
the start and trying to remove those
parts of the stack and find the common
elements and add them up is a really
tricky operation to do I've never
managed to get it to be entirely
satisfactory and and okj visualvm and
things will give you biased results but
it'll often give you some clues and you
can try doing experiments to to validate
how that stuff is going so here's a
flame graph of bootstrapping entirely
with the interpreter and if you can read
the frames at all which probably quite
hard there's no frames to do with
serialization and there's no interpreter
frames on this because did I say
profiling was easy it
wrote for flame Graaff generator
completely fell all the important bits
of a graph for off the bottom of the
page and so you have to filter out
everything to do with Java serialization
I said it wasn't optimized we've put it
in as a temporary thing and everything
to do with the interpreter frames in
order to get anything watch out for
growth but we do get some useful stuff
so loading all of the code for the
entire for the entire thing as as
serialized java stuff it's taking five
seconds and actually we don't really
need to load all the methods until later
so we could write a specialized
serializer we could also delay loading
some of that deserialising some of that
stuff until we actually need it later so
yeah we haven't optimized anything so
it's not great but it gives us some
things we can look at what about mixed
mode mixed modes far more interesting we
didn't have to really filter this graph
at all it's except for the lambda form
frames over the thing I forgotten this
is the interpreters still using a bunch
of stuff with Method handles and it's
using that because we want to be able to
interoperate between interpreter frames
and compiled frames but an obvious thing
to do would be to have a representation
of methods fertilized the interpreter to
just call an interpreter frame if there
isn't a compiled one which might help
with the optimization a bit and start up
and the mixed mode stuffs got a bit more
interesting things we didn't have to
filter it and it's got some obvious
obvious lumped and so again we can see
the DC realization of the Java I are
graphs into something we can ex
kit but it's only 1.5 seconds more
interesting is we've got this horrible
spike of auskey stuff but is the Osby
framework for some reason wants to copy
jars that you register with it that when
at the very kind of opening config over
into another directory takes stupid
amounts of time and it's not necessary
in the way we're using it so maybe that
can be optimized away we have to use all
ski for various reasons so we can't get
rid of it completely also the linkage of
methods is taking quite a while mostly
it would appear from going and resolving
the Java classes that the methods are
compiled into that's something again we
could be much much more lazy about we
just haven't done it before because it's
been so hard to pull that data out of
the profiling you can get so so yeah
there's good stuff we can do on the on
the side of of of optimizing our startup
and maybe we can build something more
later with the promoting stuff from the
interpreter to compile things but it's
probably not going to make a huge change
to the startup in itself what about the
opportunity costs so I said we compile
stuff to bytecode in advance and we can
only make really safe optimizations at
that point so what would have been we
what have you been missing well we're
not doing anything clever with tight
bands because I'll talk about why in a
second we're not really doing our own in
liming though we could get some
advantage of that and the potential for
removing boxing in the language for a
couple of reasons i'll talk about has
been very limited so we've mostly just
had to rely on the JIT removing removing
the boxing where it sees it can
and so why not do anything clever with
tight pants well here's here's a method
integer dot foo return the self dot bar
that's integer we've got a tight band
for self so self should be something
that extends integer we should be able
to make that assumption right now we
can't somebody could easily do string
defined method method name and take the
value of a method we had over on integer
and pull it over to a completely
different class or just use it as a as a
raw function so we can't make those
tight band assumptions in our in our
compiled class unless we've got a way to
fall back and recompile it at runtime if
they get broken this has a quite alone a
knock-on effects you often see it a long
chain of methods in an application we
have deep class hierarchies a lot of the
functionalities all written on the base
and lots of methods calling each other
and a lot of those end up with a mega
morphic dispatch but it all resolves
down to the same method this is an
optimal it adds quite a bit of cost to
the to the execution but it's been hard
to get rid of well you might say why not
do class hierarchy analysis and we have
tried it it works brilliantly if the
method is defined on the root of our
class hierarchy so on object if we find
that and it's not overridden we can
remove all over class checks and it's a
big speed improvement and we can do it
on things too aren't at the root of the
class hierarchy and it works really well
in micro benchmarks the problem is if
you do it in a production system either
you have to have really large caches of
results of its just a subtype of this
because we can't use the Java type
system we have to model our type system
separately as a series of method tables
so we either have to have large caches
or we have to do with the heavy lifting
every time to check whether whether
something really is a subtype of
something else and that becomes too slow
so it eats up all the benefit so we've
disabled it in in running sessions I
wasn't getting us a real advantage we
did find some sites in the core library
which really would have to take have an
advantage from this so we took advantage
of the fact that we can change the
language we introduced a keyword for
doing a early bound method call we'd
already got this effectively because you
have to be able to call super
implementations of a method we
introduced a way to do to call the
implementation of a method on the
precise method table on which you are
defined it doesn't survive being pulled
off and used on a and the method being
pulled off and used on a different on a
different class but neither did super
method call so we haven't really broken
anything that wasn't already broken but
it's it's fragile if somebody owed
overrides our method in a subclass they
are they're going to kind of be puzzled
and come to me and say why isn't this
method being used and also it requires
that we identify these points by hand so
it's not been used widely we've just
done some very careful checks in the
debt of a core data store library
because it's so heavily used and that
sort of thing so it's limited and we'd
like that optimization to be much more
much more widespread and we could almost
certainly do it if we compile in type if
we compile type bands in then we check
at method resolution time and hopefully
never install a class check
and in lining jit does a pretty good job
of this but it's not perfect we've got
really heavy use of passing loot bodies
about around we've got some bookkeeping
that goes on in there and budgets never
going to be able to pass the to narrow
down type bands by doing enlightening in
the way that we could because it just
doesn't know about our type system so
doing our own inlining is something
worth investigating look at a loop as an
example this is a iterator method so
it's one that can be called as part of a
loop and it's doing sort of 4i over the
data slot of this object and calling
elements on that and it's every time
that an element is found it's actually
going to call the loop body that's been
secretly passed in unlike unlike Ruby we
don't have the loot bodies passed in as
explicit things they're they're hidden
so that's good in some ways we could we
know they can't escape it's bad in other
ways because it means it's harder to
optimize away intermediate loot bodies
you can't just say I'm going to use the
block but was handed in to me so um when
this gets compiled these outer parts get
compiled to one method the loop body is
compiled to a separate method which is
going to be passed to the iterator that
it's calling and what's not visible in
the source code is varies a bunch of
bookkeeping that has to go on in the
outer method and in the inner method on
every iteration in the old vm loops were
implemented as co routines and the
language has dynamically scoped
variables if you really want them
since we're doing this on the JVM by
passing a method around a loop body
method around there has to be some
bookkeeping of Fred local that tracks
for your dynamic stack so every time we
go into the loop body we have to swap
out the dynamic stack that the inner
iterator might have created and swap our
own back in and then reverse at the end
of the loop body so that's that's
relatively expensive even when things
get gitty dan inline properly just
because the vm doesn't have enough idea
about language semantics if you look at
a stack trace of this so thing in an
application it's very common to see lots
of chained iterators because something
will offer an iterator method and it
really just calls the corresponding
iterator method on some field or all
that sort of thing and although these
things look really really hot to ridgid
the things at the bottom don't and in
order to analyze the wave of bookkeeping
it's not enough to work at the top
you've got to take the two loop but
you've got to take the loop bodies and
their outer methods in pairs and deal
with them as one so to get rid of all
the bookkeeping you have to take the
outer two methods that are not at all
hot from the boy of viewpoint of idjit
in line those together and produce a new
loop body that they can share I don't
think the hot spot cheats ever really
going to do that for us but we know
enough semantics about the language to
make that sort of decision this bit this
loop model is why the boxing problem has
been so hard and so so few opportunities
to really deal with it it this goes deep
although you can write plain loops for
loops that just iterator counting
integers nobody does everybody just uses
integral number mixing up to n passes in
a loop body if you want to do something
that isn't entered
there's a function called range text our
tenth step and so the only opportunity
to really do that the unboxing you on is
in the occasions when stuff gets in
lined and because this stuff is so mega
morphic in the core library that doesn't
happen as often as you would like it
does happen for some really hot bits but
as still bookkeeping and so we want to
be able to investigate stuff and we want
to be able to prove to our to our bosses
that this work is worth pursuing and
what we can get something out of it so
we can use the via rebound method
invocation to simulate in benchmarks
what it's like to take out mega morphic
dispatch and to just know tight pants we
can do that quite effectively I've got
tools so that i can write I are graphs
by hand to say if we could in line these
things and they optimize down I would
get this I up off and I can measure the
performance of that and we've got some
pretty good results so just removing the
method in VV mega morphic dispatch case
from tight loops where it's a common
thing but that it that it happens it's
where you're going to care about it sped
up those loops by over two times and
that was without doing any optimization
on the bookkeeping of the loop if you
actually go attack V bookkeeping on
loops in in the best case widget can do
we can get a free time speed up in
longer chains that the jet has an inline
we get about 10 times speed up over what
budgets doing on its own so this is
pretty promising but I said language
developers always like micro benchmarks
do as well so this is a good indication
that it's worth pursuing what it's like
in a larger application will be
interesting to see
but it's worth pursuing so we've got
some next steps of them to do so we've
got a bunch of sort of technical debt
and cleanup to do in the compiler in the
IR a DST and ast and ir level and we're
doing that at the moment so we're
putting in more aggressive constant
folding we're putting in a better
structure in the st2 to deal with stuff
to to reduce things down so we get
better I are at the IR level we're doing
a bunch of work required to implement
inlining some of the things you need for
that our stuff like saving and restoring
the stack because now try catch blocks
and things may appear in the middle of
an expression where they didn't before
so there's a bit of there's a bit of
stuff going on in that I've got an
intern who's busily doing stuff with SSA
dir so we're reducing that down the size
of the methods we've produced by by
optimizing away a bunch of a pointless
variable initialization that came down
from the AST and things like that we're
hoping this is going to make a lot of
progress but we're going to keep
tracking the results as we go on and
yeah that's everything</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>