<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web App Penetration Testing - #2 - Spidering &amp; DVWA | Coder Coacher - Coaching Coders</title><meta content="Web App Penetration Testing - #2 - Spidering &amp; DVWA - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/HackerSploit/">HackerSploit</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web App Penetration Testing - #2 - Spidering &amp; DVWA</b></h2><h5 class="post__date">2018-03-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pA_kTQSNthk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey guys hackers Floyd here back again
with another video and welcome web
applications penetration testing series
alright so in this video we're gonna get
started with spidering more specifically
spidering with web suite and you know
the purpose of this video or this
tutorial is to help you understand the
spidering process and how to go about
doing it
with web suite alright so there's gonna
be a little bit of theory here but I'll
be explaining a lot of things so again
this video is really focused on
understanding spidering now before you
get started with that I just wanted to
let you know that the target or our web
application that we're going to be
targeting we're going to be attacking is
the damn vulnerable web application now
if you don't know what the damn
vulnerable web application is that's
fine you can just google it and I'll
probably make a video on how to get it
installed on called-- next but what I
would recommend if you're you know begin
or even if you're a professional in
hacking probably one of the best things
that you need to have you know in your
kit is Metasploit about - alright and
for the simple reason that it contains
first of and foremost a vulnerable
operating system and secondly it
contains all the vulnerable web
applications that will be using at one
stage during this series okay so we're
going to be starting off with the damn
vulnerable web application as I said it
comes pre-installed with Metasploit able
to so all you need to do is get the
local IP address on your mat exploitable
- virtual machine which in my case is
192 point one sixty eight point one
point one or two so what I'm gonna do is
I'm gonna open up my browser and I'm
just going to open up that web's that IP
address
point one point one or two and just give
it a few seconds to load up as you can
see there we are that's Metasploit able
to and it's going to prompt us to select
what vulnerable web app we want to use
in this case we're going to select the
dvwa which is the damn vulnerable web
application so just click on that and
it's gonna ask you for your admin and
password in this case for your username
and password sorry about that in this
case the username is admin and the
password is password alright so just hit
login and it's going to log you into the
damn wonderful web application now we'll
be looking at this at a more in at a
later video and the reason is we have to
go through all of these options but for
now if you go to the if I can just
remember where it was if I can just go
to the security so to the dam1 world web
application security at the moment it
was high because I was actually
performing some tests on it but just
change it to medium or low but for now
you won't be using it I was just letting
you know what web application we're
going to be using all right that being
said let's move on to burp suite all
right and we can I can start explaining
the spidering process all right so let
me just open up web suite so I've
updated it to the latest version I think
I'm running catalyst next now in the
previous video I was running parrot so I
think they should be an update but I
could be wrong so let's just give that a
few seconds to start up yeah there is an
update so I'll do that later and we'll
just click on create our temporary
project and use the web default and
start that okay so what's that starting
let me explain what spidering is right
now the purpose of spidering is to
identify our scope all right or what
what we want to scan now this is not
exactly scanning and we'll be looking at
scanning but essentially a spidering is
the process of mapping out our web
application and is very very useful for
finding links and and web forms which is
also very important because it will
allow us to then furthermore attack the
web forms manipulate headers etcetera
etcetera all right now when you talk
about automatic spider ring with burp
suite it's essentially when when burp is
spidering it follows links and I it'll
it'll start following links and to start
identifying for files folders and forms
from the web application and it'll the
the great thing about this is it will
record all the requests and responses
while it's performing the dhole
spidering process okay so once you have
a web suite opened up here you can let
me just expand it so we have a greater
picture of what's going on exactly
sorry if my virtual machine is a little
bit slow I need to configure it
correctly anyway what you want to do is
we've looked at the proxy check a
section let's look at the spider section
and in here this is a very very simple
menu and to understand it you
see that we have two tabs available we
have the control tab and we have the
options tab alright
the control tab essentially if we just
click if I just look if we look at it as
you can see these settings are used to
monitor and control the bed spider so it
allows you to stop to start and stop the
web spider ring and you can also clear
the queues alright when you look at the
options which is right here sorry about
that when you look at the options there
are a lot of options we'll be looking at
them and we'll be looking at them in a
second sorry about that I actually got
an email apologies there let's get
started now with the control section so
the control section if the lot we are
able to control the spider status where
we can stop it and start it and you know
furthermore we can clear the queues that
already exist there all right
we then have the spider scope where we
can we can define our own scope and
depending on what we want to spider
we'll look at that in a few seconds and
finally if we look at the well if we
look in the options section here we have
the crawler settings which allow us to
specify the way the spider is going to
crawl for the web content on the web
application we'll be looking at the
maximum link depth and what that means
passive spidering
that allows us to essentially spider to
continue spidering when we are looking
through or we're going through the web
application we're performing requests
and responses when they're performing
requests as for the form submission this
is probably something that we'll be
looking at in the next video and we'll
be doing this practically we will be
actually performing the we'll be
performing this by drilling process but
for now let me see what else yes the
request headers the request headers are
used to you can manipulate essentially
the headers if you've learnt about HTTP
headers by the way I really want to
cover HTTP because it's very important
that you understand how the headers work
but we'll be looking at this all in
advance but now let's start off with the
spider status not really the spider
status but looking at the control tab if
you look at the spider scope you can see
that you can it'll use the default suite
scope which is defining the target tab
if
you just click on use a custom scope you
can see that okay first you just click
on this little poke here you can restore
the defaults you can load your own and
you can save the options so that's just
to do with that now when you talk about
using the advanced scope here's where
you can essentially this is where you
specify what you want to map so you can
specify a host deport etc etc okay again
we'll be looking at all of this as we
move along
but for now edge is going to use the
default suite scope we can just if once
you click on that is going to stop that
the spidering process but we don't need
it right now so I'm just gonna come just
gonna pause it and if we move on to the
options now the options tab has a lot of
stuff that we need to look into first
and foremost we have the CRO the crawler
settings alright so when we're talking
about the basic options so for example
we can specify what the spider will
crawl for so it you can choose to select
for robot the robots.txt file which is
very important because it shows you you
know exclusions you then can detect you
can ignore the links to non-text content
you can request the root of all
directory it's very important stuff but
again you can customize this to your
liking now one of the things I would
recommend that you do not touch with if
you do not know what you're doing yet is
the maximum link depth rate the maximum
link depth is essentially the number of
links you want the spider to to
essentially to crawl or to to map now by
default v is in my in my in my situation
or in my case what I like doing is
alternating between three to five
anything higher than that will usually
overload every web application and it
will cause it to lag or to respond very
very slowly and you know again that
might not mean a lot right now but trust
me when you'll actually be performing
the penetration test on the web
application you really need a good
response otherwise you have your time to
live that cetera et cetera okay so let's
look at what passes spidering is all
right so specia spidering and set is
essentially just it allows you to
continue scanning you know or going
through
or actually performing your requests as
as it it essentially allows you to us
continue the spidering process as you're
performing any other task so as you can
see buses battering monitors monitors
traffic through the proxy to update the
sitemap without making any new requests
right so passively spied as you browse
you can also select the link that's
associated with proxy requests now this
I would recommend keeping it at zero to
two and that's because again you do not
want a very very deep link depth in the
sense that you're also going to be
performing your own requests and you'll
be doing many other things you could be
looking at the decoder or you could be
looking at you could be focusing on the
target and you don't want it again to to
slow down the web application all right
so form submission again this is
something that I said we'll be talking
about in the in the next video because
it is quite advanced and we'll get
started with the damn vulnerable web
application there moving on to the
spider engine we'll be looking at
application login as well but for now
just we will just skip over that when we
talk about despite the engine alright
these settings are controlled the engine
use for making HTTP requests one
spidering all right so this allows you
to change the number of threads you want
to use and as I said using more than you
can see right now it's at ten what I
recommend is still keeping it between
the range of two to five oh you might
you might cause the web application to
slow down and these are more advanced
settings that you can use it dependent
on timing all right and we've talked
about the request headers they allow you
to modify the way the spider will will
look towards the web applications for
example you could you could edit the the
device that is being used and you could
change it for example into a mobile
device and you get the idea you are let
you essentially allows you to to change
the request headers and from that
obviously you would get a response back
dependent on what you changed alright so
that was this spider ring or actually
the theory revolving on spidering now
we'll be looking at how spidering really
works in the next video I know some of
you may not like this that I actually
went through air theory and I haven't
talked about doing anything but remember
it's very very important to understand
what exactly is happening behind
spidering and in the next video we'll
actually get started with the spidering
process so thank you so much for
watching this video if you enjoyed it or
you found value in it please leave a
like down below if you have any
questions you can hit me up in the
comment section you can hit me up on my
social networks on and on my site if you
have any video suggestions you can leave
them on the website as well and thank
you so much for watching and I'll be
seeing you in the next video peace</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>