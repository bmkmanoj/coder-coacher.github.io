<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Making Android sensors and location work for you - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="Making Android sensors and location work for you - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Making Android sensors and location work for you - Google I/O 2016</b></h2><h5 class="post__date">2016-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OEvycEMoLUg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome I'm truly excited to be here
today to talk to you guys about location
and sensors get the clicker my name is
Steve malkos I'm the technical program
manager for the Android location and
context team I was amazed earlier today
to hear sundar talk so much on sensors
and context being part of the location
and context team we touch every part of
Google and and I'm truly excited about
that so without further ado okay let's
get into it
today we carry the world in our pocket
we carry more computing power and faster
data speeds than we did just nearly half
a decade ago from our desktop PCs one
item that has remained the same
throughout a computing history has been
building these innovative software
applications and you guys are
continuously shaping this world what we
need to do more we can do more we could
build deeper and richer experiences in
this talk will focus on what it takes to
build an awareness application will go
through sensors and location focusing on
bringing richer experiences by using
them then we'll take a deep dive into
the Android sensor hub and go through
the cool new things that we're building
for it awesome ok so what does it take
to build these highly contextual apps so
what does it take to build these high
these these high quality apps today
sorry the notes are not right let me
take care of that for you
our phones are very personal and
interactive they have transformed the
way we compose our day we use them to
get traffic info health and safety and
notifications and much more so it's
understandable that your biggest
concerns are on how to make your apps
more contextually aware so let's focus
on what it takes to build these high
quality apps when creating these
awareness application there are
typically three main layers that we need
to look at that's sensors algorithms and
user experiences think of an awareness
application like a human sense for
example your inner ear acts as the
accelerometer and gyroscope of the
device your eyes act as the camera
sensor that raw data from your ears to
your eyes gets sent up to your brain and
processed like the algorithms on the
device that's then classified into the
context and outputs as the user
experience like I'm running or I'm
running with a buddy MEMS sensors allows
us to take a deeper programming
perspective on comprehending the
environment around us so when building
these contextual applications we're
gonna focus on these four main pillars
that's coverage accuracy latency and
power will refer to these pillars
throughout the entire talk these are the
building blocks for creating
higher-quality applications let's walk
through examples of each of these the
first pillar is coverage for location it
must work all the time indoors outdoors
and in every country at Google were
continuing to improve our coverage maps
with our crowdsource models and Google
databases to make location better for
activity recognition it has to support a
wide array of different types of
activities like I'm running or I'm
walking or I'm in a vehicle and and
nearby notifications needs to work
across a wide array of different types
of devices
finally sensors have to work seamlessly
and uniformly a
across the entire ecosystem the second
pillar accuracy from the macro view GPS
must be very accurate users don't want
position jumps they want the positions
to be under five meters all the time
indoor and outdoor transitions also have
to be as smooth as possible from the
macro view that we saw on the previous
column to the micro view on position
accuracies it must work perfectly for
our virtual world examples of better
virtual reality can come from sensors
sensors that output less drift at lower
noise our user also wants very accurate
activity detection for example it's not
okay to classify biking activity if
you're shaking your leg in a moving car
the third column that I'd like to
discuss is latency from the macro view
users don't want to wait for their
positions location should show up
instantaneously and activity detection
needs to happen near real-time so the
users entire activity could be tracked
with almost no latency from the macro to
the micro on Layton sees our sensors
cannot lag when we have sensor and
display lags like this your virtual
world can go from making you from making
you a beautiful scene to making you
nauseated very quickly we want the
experience to be crystal clear we take
great care and ensuring that our sensor
latency requirements are met this
ensures that when you write your
applications you could be certain that
they'll perform to your users
expectations our final column power
that's battery life one of the most
important columns when building your
applications we all know that if your
application sucks up the users battery
life they'll quickly uninstall it from
you their device we need to make it or
you guys don't have to worry about power
we can do this by avoiding too many
knobs simplicity is key
power has even to subtly NC and accuracy
it means we could run more often
improving on Layton sees it also means
we could get access to more signals thus
improving on accuracy so in the theme of
building better user experiences by
bringing higher quality applications
focused on coverage accuracy latency and
power let's hear from Ashutosh on
Android sensors hi thanks T for setting
out the lay of the land for the rest of
the presentation
I'm Ashutosh and over the next few
minutes we shall be talking about how we
set about achieving these four pillars
of context for sensors I hope that gave
you some insight into the challenges we
face and the power we derived by work in
working with the ecosystem I shall show
you some of the new and shiny things we
have in store for you for Android n and
finally we share some of the lessons
that we've learned in developing Android
experiences which will probably be of
value to you as well I challenge that we
face is the constant tension between
increasing coverage but formally
defining new sensors in Android and
ensuring that we can keep a consistent
user experience across the ecosystem let
me give you an example we have received
requests to allow accelerometers that
only support two axes these are required
for low-end markets where the z-axis is
not required if the only thing that
accelerometer does is detect landscape
and portrait detection on the other hand
we have been asked to mandate a higher
dynamic range for accelerometers because
it allows some safety applications to be
developed which can potentially save
lives the problem is that these two
goals are mutually contradictory a
sensor that only supports two axes will
not have a high dynamic range the
economics of the industry work against
it the example goes to coverage we must
ensure that all Android sensors can eke
out the maximum visibility from the
hardware whilst ensuring that the
application developers seek consistent
API is across all of em Rock over the
years which steadily added new formal
sensor definitions
while the major physical sensors like
accelerometer find very early on we've
continued to define new capabilities
like sensor fusion rotation vector
geometry
geomagnetic rotation vector and new
sensor types for completely new form
factors like Android wear these new
capabilities increased coverage to
develop awareness applications because
most sensor types mean that we can get
more data about the world around us and
derive richer inferences we are adding a
few things this year for the Android
enemies let me walk you through some of
them let's start with something really
exciting all orientation sensors thus
far was centered around orientation they
provided the orientation of the device
in terms of roll pitch and yaw with
reference to either true north of
gravity or an arbitrary initial point we
are adding the three axes of
translational freedom by introducing the
sixth off sensor type apps will not only
know the orientation but the precise
displacement of the device in the world
this means that motion control in games
can go just beyond tracking where the
user is looking now they will be able to
move the player in the virtual world
just as the player moves in the real
world you can walk and not just pin in
place
augmented reality experiences will be
even more immersive now that they can
mirror the users movements just to set
expectations this sensor is not likely
to be widely pervasive across Android
but we believe that the introduction of
a formal sensor type is required to give
the system a push towards augmented
reality applications on the variables
form factor we extending the
capabilities of what it is what is
available through the heart rate sensor
thus far you have been able to get an
average snapshot of the heartbeat
measurement which is great but it leaves
a lot of information on the table we
will expose a much more finer grained
event for every heartbeat which should
more or less more or less correspond to
the systolic peak for each heartbeat
this will enable a whole new class of
fitness and wellness applications like
reporting and monitoring mental stress
physical stress / training exercise
recovery and sleep analysis etc and you
guys will be writing all
one of the most often repeated requests
from app developers to us was to expose
the hardware capabilities to determine
the motion state of the device we adding
these sensors to the Android sensors API
as one-shot events will we shall expose
202 of them one shall fire if the device
enters the stationary state and one that
fires if the device starts moving we
hope that this will allow the
applications to be more judicious in the
use of system resources and developers
will use this ability of motion
determination in low-power hardware to
get the resource usage finally this year
we realized that applicant processors
and display controllers are now smart
enough to enter low-power States if only
we did not keep pinging the application
processor with the accelerometer data to
compute the device orientation so we're
formalizing the screen orientation as a
new sensor type that can be implemented
by the OEMs to get the low-power domains
to compute the device orientation
however developers can and should
continue to use the Android activity and
window manager API is Android will
ensure that regardless of the underlying
sense of support your app will continue
to work seamlessly across all
addressable devices and if this sensor
is supported you get all the power
benefits finally we are adding
capabilities to 'allah applications on
an Android device to discover and
subscribe to external sensors through
the sensors API by external sensors here
I mean any sensor that is not present on
the device when the device boots up now
this can be an accessory for the device
that physically attaches to the device
or a truly external device that connects
wirelessly to your phone this is a nod
to the central rolls their phone have in
our lives and the realization that a
single device cannot be all things to
all of the people all of the time device
capabilities can be augmented and we
want to provide the standard way to
access such data what is coming up is
exciting and I cannot wait to see what
cool things the developers will come up
with these new sensor types having
decided how we want to increase coverage
how do we establish accuracy and
consistency how do we ensure that when a
new API version is released what makes
it to the app developers is consistent
across Android this is a complex process
and involves the hard work of hundreds
of thousand people across the Android
ecosystem
let me walk you through some of the
challenges
Android is diverse just on the harbor
side there are 300 plus OEMs 200 plus
carriers and operators hundreds of
component vendors dozens of CPU or SOC
vendors with a range of products there
are dominant players that have huge
market shares and there's a long tail
that caters to very specific niche cases
somehow we have to ensure that we
listening to everyone and helping
everyone in the ecosystem succeed and
then there's a sheer range of devices
that under can support all of the
devices on this slide are Android
devices the same specifications rule
them all
more or less there are few restrictions
on Monday that Android stipulates as an
example it's completely okay to make an
Android device without a single sensor
type not even next kilometer that sounds
bizarre if one thinks of Android purely
as a cell phone platform but as
immediately obvious once you think of
Android TV products and convince
yourself that determining the device
orientation of a TV would be an edge
case at best so what does it really mean
to support a sensor what sensor should a
device have what guarantees if many are
made explicit to the developer and how
these are difficult questions that keep
us busy and what is agreement on these
goals how does its diverse ecosystem
take excuse what tools do we used to
make this happen in a nutshell we drive
this process by creating standards for
Android producing exemplary Android
devices at Google and working in tight
collaboration with our partners the very
first step is the definition of Android
compatibility and under device is
supposed to meet 100% of the
specifications as defined in the Android
compatibility document the Android CDD
it goes into lots of gory details about
what kind of sensor should be on device
how they should be announced and they've
announced what bars they must meet with
tune the CDD language to advise the
ecosystem on what is coming down the
road and mash them into more
aspirational requirements the CD is
enforced through the CTS a compatibility
test suite all Android devices must pass
the CTS there are no exceptions or any
waivers for any device we keep adding
CTS tests to patch holes in our testing
note however that the CTS is an
enforcement mechanism only it cannot
possibly cover the entire specifications
exhaustively the artifact of record is
the CD what do we expect of developers
please take a look at the Android CD if
you're looking to deploy an app across a
large user base it will help set you
expectations on what is standardized and
what to expect Google devices are
another tool we use all Google devices
are great devices they also serve to set
a bar for the rest of the OEMs a nexus
device is subject to a lot of scrutiny
outside moving the OEM scrutinize it to
make sure that their devices in that
class are as good as or even better than
the Nexus they also use that opportunity
to figure out the holes in our
specifications if something is ambiguous
a reasonably good assumption would be to
go along with what the Nexus is doing
the implication here is obvious do teste
applications on Google devices I think
most of you follow this practice already
but please however go beyond just the
Nexus that launches this year do teste
applications on previous Nexus devices
and extend your testing to say Android
one devices on the sensor side we go
even further last year Google wrote the
center stack completely by itself and we
realize we are releasing they the
complete sense of stack source for the
Nexus devices in open source you can get
the code for the sensor sub software at
the repository on this slide we have two
primary reasons to do this the first is
to make life easier for the ecosystem
they now have access to the source code
that runs all the Nexus devices and
should be able to comply with all the
requirements we are putting forth and
hopefully even do better
the second is guidance we hope that
simply by releasing the source code we
convey our intents and expectations very
loudly in fact if you look at the source
code released you will find the
beginnings and the genesis of some of
the future work that we shall release in
Android n we have not written a ton of
formal documentation yet but we are
finding that our partners are reading
the release source and coming up with a
very good idea about what to expect
finally we must continue to build
internal expertise to keep abreast of
the developments in the space we learn
constantly and continue to invest in
internal capabilities to learn more
about sensors and algorithms let me show
you how here at Google we are innovating
new groundbreaking methods to test our
solutions and improve the quality of
essential Gardens can you play the video
please
in this video you'll see Matt
programming one of our robotic arms to
simulate unfriending swinging for
pedestrian dead reckoning solutions in
this example you see two things
happening first Matt will start by using
a motion capture system the motion
capture system is able to records
mash.sam swings with centimeter level
process precision we then translate the
exact motion of that capture system into
a robotic arm the robotic arm then
repetitively tests a specific scenario
on an automated basis again and again
that gives us the ground tools for us to
test our algorithms and sensor solutions
let's watch
that was cool
and Matt seem to be having way too much
fun on there
the final the final tool in a chest is
collaboration with our partners we work
very closely with our partners
throughout the year to make sure that
they are aware of what we are doing this
is a symbiotic relationship the partners
want to make sure that their product
roadmaps are aligned with ours so they
are not caught by surprise by any of the
announcements we make we on the other
hand want to make sure that we are not
shouting into thin air and have
realistic expectations about what is
achievable we meet them constantly
throughout the year every year we hold
an Android boot camp and give them a
glimpse of our plants on the census side
last year we conducted many summits with
sensor vendors Assoc vendors and the
OEMs if we are successful nothing that
I'm saying today would be a surprise to
any of our partners they should be
sitting back and saying we've got this
taken care of she's the cool thing for
you
why do you think they listen to us other
than our charming personalities the
reason they is they do is because they
want to reach you
the developers some of the best and the
brightest on this planet are developing
incredibly exciting technologies and
they want them to be exposed to you we
are merely facilitating this meeting of
immensely creative and smart people from
both worlds the ecosystem is what we
derive by inference from the ecosystem
is what we serve take away reach out to
us let us know what you're looking for
what is missing what needs improvement
what is truly exceptional and you want
more off you'll find plenty of ways to
reach us at the end of the presentation
the animal sense of the API has not only
become richer by adding more sensor
types
it also gives great control for the
developers to optimize system
performance in the next few minutes I
would like to talk about some
recommendations that'll make your apps
scale across different tiers of devices
and should under the hood optimized for
the hardware that your app is using I
will use power as an example as to the
extent possible you want to spend as
little power for your apps let us look
at some ways that will help you achieve
this
on many devices there's a multi-layer
approach to processing at least as far a
sense of the concern at the very source
there are sensors like the
accelerometers and the gyroscopes
sensors today have come a long way from
years ago and themselves consume very
little power they're really power
efficient accelerometers and gyros today
take less than a milli watt to sample
data they they embed smart algorithms
like gestures and motion detection some
even have rhodium entry programming
frameworks for customization then
there's the application processor this
is your big honking chip that you're
using when you're running apps or
playing games running the application
processor takes a substantial amount of
power so if every time you try to
process a single measurement you wake up
the application processor it becomes
limiting very quickly all the power
savings from the sensors are dwarfed if
the application processor wakes up too
often it consumes a lot of power and is
incredibly powerful competition but it's
also very power hungry it has to stay on
for a few seconds even if it needs to do
a little bit of processing it needs time
to wake up and sleep if every time you
get a sensor measurement you wake up the
application processor it never gets a
chance to go to sleep so getting the
application percent of sleep is a key
power settings technique modern Hardware
adds an additional layer for sensor
processing we show that as a sensor hub
in this diagram this low-power computer
domain may be part of the sensor itself
a special power domain on the SOC or a
discrete chip its goal however is the
same to offload computation from the
application processor and save power
there are a few hooks on the sensors API
that lets you achieve this code for
example batching you can use the sensors
API to specify the maximum delay your
application can tolerate to receive
sensor data this means less frequent
updates to the application processor the
events can be grouped and processed
together the bigger the storage buffers
on the hubs the more power is saved
another design pattern is to specify the
goal that you want to achieve and then
we've woken up when that situation
becomes true then the low-power hardware
can monitor for the decision to become
true instead of the main application
processor this allows always-on
monitoring of events that would have
been prohibitive by looking at the raw
sensor data all the time and then as an
example if you wanted to change the app
behavior when it looks like the user has
moved from one place to another you can
use a combination of geofences
the significant motion sensor type to
get you're processing for the N release
we have added new sensor types reject
motion and stationarity by using the
right triggers you will consume power
only when needed and not miss a moment
of interest for you a less obvious
design pattern is to use the highest
level of abstraction possible while the
api's may look like they are all exposed
at one level if you become a little
familiar with how the system is built
you will be able to see where in the
system the API is are tapping into take
for example the sensors accelerometers
gyros and magnetometers if TX is the raw
data the system must ensure of its end
of the contract and give you accurate
reliable and timely data it cannot and
should not put in any smarts however if
you ask for the rotation vector the same
sensors are used and data is fused it
will be tempting to roll your own sensor
fusion algorithm by processing the raw
data directly however you will lose
Hardware optimizations on the table as
an example most devices today if they
computing the rotation vector in
Hardware sample the underlying heart
sensors and an extremely hurried
sometimes in the order of kilohertz
there is no way we can support that
frequency of updates on the application
processor and if by rolling your own
sensor fusion algorithms you will be
leaving power and performance on the
table so this is pretty complex I hear
you I hear we have we have been trying
to come up with a specification the less
the application developers know that the
sensors on a device are tough notch in
the MNC release we exact we added
exactly such a signal for the developers
we call it high five sensors if the
feature is defined you will know that
the sensor are really accurate there are
stringent requirements on the resolution
range and performance of all the
expected sensors you will know that a
large batch size is supported allowing
you to conserve a battery you will know
that the timestamps on the sensors will
be very highly accurate for many
applications that integrate sensor
values over time for example dead
reckoning and error in the timestamps is
the same as an error in the sense of
value itself and often worse finally you
will know the sensors are low latency
Wi-Fi sensors have stringent latency
requirements to support all possible
interactive use cases that we have run
into checking for the high-five sense of
support is very easy this code snippet
shows you how to do it we're simply
looking for a feature string to be
declared so with the quick query to the
package manager you can be assured that
your device has great sensors and and
will support all the fundamental pillars
of contextual awareness sensor
capabilities are improving getting more
ubiquitous and we're seeing a huge
demand for these additional capabilities
we are the location and context team
we're constantly working on integrating
sensors into everyday use experiences to
make Android devices useful and a
delight to use location is the other
huge and complex sensor that a services
consume I will hand it over to David to
walk you through that part of the story
thanks Ashutosh hi I'm David and I work
as a software engineer on the Android
location team you've just heard about a
lot of the exciting capabilities we have
with low-level sensors and now I'm going
to talk about our higher level location
api's and how they do a lot of the
sensor fusion work for you the fused
location provider is our primary API for
producing locations it combines many of
the sensors Ashutosh just talked about
including GPS Wi-Fi cell accelerometer
magnetometer and gyroscope it's used by
Google Maps to power the current
location blue dot and by Google now to
provide invisible assistance like
reminding you where you parked your car
Android apps provide input hints on
accuracy requirements and frequency and
the SLP decides which sensors to turn on
in order to manage both power and
accuracy within the constraints of user
settings in the next few slides I'll
dive into the details of some of these
sensors and show how the SLP uses them
GPS is one of the most mature location
sensors that we use and when it works it
usually works great it shines outdoors
when you have a clear view of the sky
but it has some trouble in cities with
tall buildings and when you're indoors
it often doesn't work at all it's
relatively power hungry and eats the
battery pretty quickly when active the
SLP will use GPS
when app requests the highest accuracy
location available it will also
sometimes use GPS for balance power app
requests on newer devices that support
GPS batching Wi-Fi is another staple
location technology that has become more
and more useful as Wi-Fi access points
have proliferated around the world
unlike GPS it works well indoors where
people typically have Wi-Fi the accuracy
usually isn't as good as GPS but it's
good enough to tell you what part of the
building you're in and often what floor
you're on like GPS the power isn't great
but it can be improved by reducing the
location frequency and knowing which
cell tower you're connected to also
gives a very coarse estimate of location
it can usually tell you what city you're
in and sometimes which a neighborhood
unlike GPS and Wi-Fi it works both
indoors and out and the battery life is
great
given that the cell modem is continually
reconnecting to cell towers anyway so
GPS Wi-Fi and cell are the foundation of
location sensors surface through the FLP
but one of the best parts of using the
FLP in your applications is that you
continually take advantage of new Google
developments in both Hardware adoption
and algorithm improvements most of these
improvements become active in your apps
without any code changes and the next
few slides I'll highlight some of the
improvements we've recently made and
show how to take advantage of them in
your apps as I mentioned before dense
urban environments are very challenging
for GPS when driving near tall buildings
it's common for the location to drift or
jump to the wrong block at Google we
test GPS on a variety of phones and
environments this image shows a set up
for driving tests that we perform in San
Francisco which is one of the harshest
GPS environments on the west coast we
use a military-grade inertial navigation
system that provides centimeter level
accuracy in order to collect ground
troops location for comparison we also
take video imagery to compare position
jumps and legs when all devices are in
navigation mode this gives us the
automated ability to count and measure
each device's position jump or leg
and then produce a report listing the
exact moment when a device experienced
these legs and jumps this is an example
test drive in downtown San Francisco you
can feel a lot of issues with this track
positions clump together poor latency
and even some positions inside buildings
in order to solve some of these problems
we've worked with our GPS chipset
partners to incorporate their sensor
fusion algorithms that combine
accelerometer gyroscope magnetometer and
barometer with rod GPS pseudo ranges the
chips that can fall back to these other
sensors when the GPS signal is weak
providing a much better user experience
this image shows a before-and-after
result of this GPS chipset fusion
working on a nexus 6p you can see the
blue line provides a much more stable
position resulting from this fusion as
the user drives in an urban environment
we've also tackled accuracy in our Wi-Fi
location models over the last year we've
improved indoor median accuracy by 40%
in Google Maps this can mean the
difference between knowing it you're in
a shopping mall and knowing how close
you are to the actual store you're
trying to find we've added significant
use of the inertial sensors
accelerometer gyroscope and magnetometer
these sensors don't tell us where the
user is directly but they describe how
the user moves so we can use them in
conjunction with GPS and Wi-Fi to
improve location one example of how the
SLP uses the accelerometer is by
activating it when the device is
stationary in order to save power rather
than performing costly Wi-Fi scans over
and over again we write rely on the
low-power accelerometer to tell us when
the device moves at which point we
resume Wi-Fi scanning we also use all
three of these sensors to improve the
accuracy of Wi-Fi location if you open
Google Maps indoors and walk around you
may notice the blue dot follow you
around the building as I mentioned
before Wi-Fi accuracy alone is not that
great but by using sensors we can count
your steps detect when you turn and
combine this information with Wi-Fi
dehonian
more accurate location now Ashutosh
already talked about sensor batching
where we can accumulate sensor data on
low-power hardware and process it later
likewise we've added support in the FLP
for location batching where it collects
location data at lower power and
processes the batch of locations after
the fact if your application doesn't
need immediately immediate location
updates you can save significant battery
by requesting batching this allows the
SLP to utilize sensor batching under the
hood and also to consolidate network
queries an example of this use case
might be a sickness app that tracks the
user's activity the user will
periodically want to check their
progress throughout the day but doesn't
necessarily need constant updates unlike
the other SLP improvements I've
mentioned location batching requires
code changes in order to activate the
power savings in your app here's a code
example to activate batching you use a
location request object to provide your
apps constraints to the SLP the set
interval method tells the SLP how
frequently to compute locations to
activate batching the magic wait call is
set max wait time which tells the SLP
how frequently due to deliver locations
to your app so in this particular
example the FLP will compute a location
every minute but will deliver them to
the app every five minutes
typically in batches of five locations
now back to the fitness app example
let's say it's batching the users
locations and then the user opens the
app and wants to see their progress
immediately the app can call flush
locations which tells the FLP to
interrupt a batch in progress and
deliver the most up-to-date locations as
soon as possible the user will see all
locations collected in the app and then
batching will continue as previously
requested so by making a few code
modifications the fitness app can take
advantage of location batching with
significant power savings while still
providing the same responsive user
experience now that we've seen the
latest features of the fused location
provider I want to talk briefly overview
another set of location api's geofencing
the geofencing API is notify your app
when the user enters or exits predefined
areas of interest such as their work or
home it's built on top of the FLP so it
inherits all the new features we just
talked about that improve accuracy and
conserve power
in addition geofencing uses activity
recognition to dynamically manage power
based on the user speed and proximity to
the geo fences of interest this code
sample shows how to incorporate
geofencing into your apps you define up
to a hundred circular regions of
interest and register for notification
I've highlighted the set notification
responsiveness call because it's very
important for minimizing power
consumption it specifies the delay you
can tolerate and receiving notification
when the user enters or exits a geofence
so your app should set this to be as
high as as reasonable for your use case
so I've shown a brief overview of our
location api's and how they do the work
of fusing the latest and greatest sensor
technologies for you now I'll hand it
back to Steve who's going to talk about
her upcoming hardware features and how
we plan to incorporate them into our
api's great awesome thanks David we just
heard how we're moving more and more
algorithms down into our low-power
processing domains or how we just want
to offload as much as we can from the
main application processor next I'm
gonna take a deep dive and talk to you
about the Android sensor hub and the
cool new things that we're building for
it ok our mission with the Android
sensor hub is to use all sensors in all
wireless radios on a mobile device so we
can have a better understanding of the
state of the users context and location
we want to simplify our users
interactions for example pickup to wake
the device we want to augment human
memory and knowledge like we did with
Google Google now with where's my car
and we want the users to have a better
understanding of themselves like google
fit so what did we launch in marshmallow
we launched a standalone microcontroller
runs always on processing we did this on
the Nexus 5x and 6p and we open sourced
it to the ecosystem why did we need this
separate processor today the cost of
computing activities in an always-on
fashion for location and activity
recognition is really cost prohibitive
because of power by introducing this
tiny 3x3 millimeter chip we could reduce
latency x' improve accuracy x' without
hardly affecting the users battery life
I'd like to highlight though that sensor
hubs are not anything new they've been
around for many years
however at Google we wanted to
standardize on this output so that
there's more consistency in the
ecosystem this gives us better
assurances that when you write your
applications they'll work the same
across all Android so what did we do how
did we do this we connected the
following into the Android sensor hub
accelerometer gyroscope magnetometer
barometer proximity and ambient light
sensor by establishing these connections
we were able to achieve the following
I'm not going to read through this whole
list but I want to highlight some
examples it's running everything in
sensors dot H this includes things like
game rotation vectors gravity we're
running significant motion detector
which is the primary use case for
Android doze mode all of our activity
recognition models are in this hub and
we introduce new gestures like double
twist which is a private sensor that you
could register your application to today
so what are we savings in terms of power
it's quite simple when you run activity
recognition on the application processor
it consumes quite a bit of power when we
introduce sensor batching we were able
to cut that number down in half running
that same activity recognition model on
the android sensor hub costs us just a
fraction of the power and this is just
the start so here's a sneak peak on what
we're working on this year I'm gonna
highlight some of the big items we're
introducing GPS sudo ranges this is
groundbreaking as is the first time in
history a mobile application will have
direct access to the raw GPS
measurements
this is beneficial to many but
especially the phone makers because they
could use these measurements to help
them in their GPS performance testing
and if you ever had a bright idea on how
to use GPM at GPS measurements now's
your time to shine we're also
introducing personalized activity
recognition models because we all know
my step lengths different from your step
length by having a personalized model
we'll be able to better our accuracies
for activity recognition with the
Android sensor hub will introduce
downloadable code in marshmallow when we
launched activity recognition we lost
the capability of updating our
algorithms frequently but gained the
power savings by running those in the
low-power domain in Nexus n or Android n
will have the best of both worlds
low-power activity recognition with the
capabilities are continually updating
our algorithms will also connect GPS
Wi-Fi and cell directly into the sensor
hub which I'll talk about in the next
slide so by adding these additional
connectivity signals we're gonna move
our entire location engine into the hub
I noted earlier that power has two sub
pillars those were latency and accuracy
by moving the location stack down into
the low-power domain will improve
geofencing because location engines
running there so well latency will
become better because we could run more
often and accuracies will become better
because we will have access to more
signals so those are the main themes for
this year let's touch a bit in the
future in order to truly understand the
state of a user's context and location
we need to answer the following three
key questions where's the user examples
GPS latitude longitude geofencing which
is within your circle or not and
semantic location like where am i I'm in
Starbucks for activity we need to answer
what is the user doing are they walking
running on a bike and
for nearby we need to answer which
devices can I connect to which
chromecast devices are nearby so in
order to achieve or answer these three
key questions we need to push down
everything listed here into the
low-power compute domain for a location
we're already working on the fused
location provider and geofencing this
year for activity recognition we'll
continue to add new sensors and new
activity recognition algorithms down
into the low-power domain and for nearby
we'll work on adding nearby connections
and messages into the low-power domain
so by answering these three key
questions the Android sensor hub will
bring the best of Google's databases
machine learning algorithms and Google
infrastructure at the lowest power this
will allow us to bring you always on
location and personalized context we'll
continue to add more and more signals
down into these low-power domains we've
already achieved activity recognition
and and sensors in marshmallow and soon
are about to launch location and
dynamically downloadable code in n the
future will become more and more
contextually aware and by adding these
signals will help make you deeper richer
experiences in your applications we're
truly excited about these possibilities
and are looking forward to uh shirring a
new wave of api's for your applications
thank you
so I have one last slide if we could go
we have a new developer site this is a
nice refresh site go to
developers.google.com slash location -
awareness we've placed all of our api's
in a fresh new look for you guys to
acces our location and context
information we also have a quick survey
that we'd be really interested in
getting your feedback on our sensor API
so please go to the second link and and
fill out this quick and easy survey you
could do it at your leisure or do it
right now and then the lastly David
Ashutosh and I will hang outside for a
little while if you guys have any
questions or we have office hours on
Thursday and Friday at these times thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>