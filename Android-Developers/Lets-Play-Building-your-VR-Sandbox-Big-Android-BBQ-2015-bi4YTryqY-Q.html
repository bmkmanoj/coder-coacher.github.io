<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Let's Play! - Building your VR Sandbox (Big Android BBQ 2015) | Coder Coacher - Coaching Coders</title><meta content="Let's Play! - Building your VR Sandbox (Big Android BBQ 2015) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Let's Play! - Building your VR Sandbox (Big Android BBQ 2015)</b></h2><h5 class="post__date">2015-10-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bi4YTryqY-Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome everyone thank you for coming
to this talk my name is a 10 I'm from
Montreal I'm an Android gde and I
enjoyed the GDG organizer so I like a
lot of Android related things as you can
guess and I'm actually pretty passionate
about VR so this kind of the story kind
of started when we went to a hackathon
in san francisco after i/o for Villar at
the adobe offices you have to play
around with tango and like different the
Roofus dive helmets for the tango and
was quite fascinating so I started
basically thinking well I have to start
making apps for this thing this is
interesting i want to make apps for
cardboard you know right greater
coverage for four people thing is when
you start making uber app and i wanted
to do it with java because i like
challenges that way i had a lot to learn
I didn't actually know all that much
about OpenGL i mucked about with it like
ever since the 90s but it kept changing
you know I'd be lost with what was going
on so you know things get breaking every
time I change a small detail so that was
really painful so not being a very
sensible person I've been doing this
since last May and i'd like to share
with you some of the things i found out
and some of the pitfalls you could
possibly avoid all right so a few
sections here so this one's about OpenGL
theory so to be aware of the ecosystem
you have OpenGL which is for desktops
and you have something called OpenGL ES
which is for mobile out there right now
opengl es one is sort of non-existent
anymore it's what you had on the first
version of the devices they had a non
programmable pipeline so I'm going to be
saying programmable pipeline a lot
during this talk I'm just fair warning
with es tu it brought the programmable
pipeline to the mobile world and this
was a good thing so now we're in the
2005 515 thank you and we have OpenGL ES
3.1 and something called the Android
extension packs so these add a ton of
features so you get compute shaders
stencil textures
accelerated visual effects high quality
et Cie to EAC texture compression
advanced tech chure rendering
standardized exercise render buffer
formats more and more and more and the
Android extension packs allow us to do
things like this when you're looking at
an Android device such as Android TVs
which might be running you know very
powerful hardware so these extension
backs you install on a specific device
and if the devices are dwyer supports
that you get almost desktop equivalent
performance so this was introduced that
I oh I think 2014 actually and it's
based on the enemy Unreal Engine five
right and for the future we're going to
have something coming up called Vulcan
so Vulcan was announced this summer it's
a new initiative if you've heard of
apples metal for iOS it's fairly similar
so the idea is that you're even closer
to the GPU they strip away parts of the
framework that would make things
theoretically easier for you this will
likely lead to more complex apps but for
developers it will give them far greater
control so we still have to see a bit
more about that it's just been announced
it's not really i haven't seen any specs
yet so i think it's being worked on
currently at various you know
manufacturer officers and things like
that so you might be a bit dizzy at this
point because of all the alphabet soup
and acronyms I've been throwing around
and that is normal the key takeaways
from the previous slide our plane opengl
for desktop yes is for mobile devices
es-tu is where you started getting the
promega programmable pipeline yes 3.1 is
what we'll be looking at today things to
keep in mind it's supported say on a
nexus 5 it's supported since lollipop
but some phones won't support it due to
limited hardware on these phones so
you're going to need to programmatically
check if you get into production kind of
scenario and yeah so let's look at core
opengl context concepts we're going to
be playing around with today so we're
going to go for a minimum effective dose
right i'm going to try to gloss over a
lot of stuff because we only have so
much time and yeah time is precious
right right so we're going to look at
geometry math and core 3d cons
steps and my idea is that not a lot of
people here in the audience have a lot
of previous experience with OpenGL so
I'm actually going to ask this right now
who here is actually wrote an app using
OpenGL ES two or three right so a few
people good like share notes with the
others if they're you look confused so
core concepts we're going to be talking
about 3d space so you're probably
familiar with the idea of Euclidean
space right x y&amp;amp;z I had the honor of
choosing a graph where the axes are sort
of wrong for OpenGL so just to clarify
something on OpenGL this is X this is y
and z is depth right so right then we're
going to be talking about the vertex a
lot or something called vertices for the
thing to note though which might be a
bit confusing is that we're going to be
talking of the vertex of course as
points and say an object right as part
of like a triangle that composes the
faces of the object that we're going to
render but we're also going to be
talking about attributes hooked to that
vertex so it's not like the pure math
vertex that we're used to talk about it
will have other concepts associated to
it like normals color things like that
so vertices assemble into primitives
this is like you're really the old
school graph from like the red book
OpenGL red book from the 90s or even
earlier probably the one we're going to
be playing with a lot is triangles to
make a distinction between triangles and
triangle strips or fans or all these
other things you might be wondering why
this is this even in the spec and just a
quick like we're going to be talking
about performance a little bit one of
the things with OpenGL is that the more
data you have to shove into the pipeline
the slower it gets so you might notice
here that a triangle strip for example
uses up less vertices than plain old
series of triangles right because they
share vertices so let's talk about
models so these are wireframe models
as probably a lot of you know models are
actually usually well in this case we're
going to be talking about triangles as
our primitive so all these models will
be composed of vertices that form
triangles that form the model right the
important point I want to make with this
slide is that when you create a model
say we are going to talk about a cube
the the coordinates of the vertices are
going to be around the 000 origin right
so everything is always going to be
relative to the origin more for
management reasons as will see a bit
later so another concept you have to
sort of get your head around these
transforms so we're talking about
translations moving an object in a
different point in space rotations
twisting an object and scaling is also
one making things bigger or smaller what
you might want to note from this slide
is that the order that you apply
operations is important the good news
here is that the example code that comes
with the talk all this has been taken
care of for you you don't have to
experiment and sort of go oh why is this
moving all around so wrong and I I'm
confused so a lot of that will be taken
away which is good when you want to
experiment just to keep that in mind
when you're going to get a bit deeper
into the rabbit hole all right linear
algebra right transforms imply matrices
multiplication of matrices so most
people their math courses are far far
away and you know these are fuzzy
concepts and you don't want to mess
around too much with that the good news
is that this is all taken care of mostly
by the SDKs and the API so you don't
need to worry too much about it you do
need to understand that all the
operations I've talked about usually end
up in one big matrix and you multiply
your vertex by that matrix and the
vertex will be placed into its its
proper space its proper coordinate space
we'll come back to that all right
another big concepts so projections and
cameras right so when we're going to
configure our OpenGL pipeline we're
going to have to set up rules of
projection right so if you look at the
first figure the idea is we're telling
GL how to project things so it involves
telling it what are the bounds of our
view the top the bottom left and right
and also involves giving it some
configuration so it kind of knows what
is far and what is closed so when it
comes time to project the vertices we
have in 3d space onto our 2d screen or
to the rendering area it'll know how to
sort of proportion these things right so
this is through configuring like that
again don't get too worried about it
you'll see that line of code you
probably won't even notice it it's a one
liner but it's a fairly simple thing but
you need to be aware of all this theory
sort of fix things when they start going
wrong the other diagram is just here to
illustrate that we also need to place
our models but we also need to put the
camera in there right so we have the
concept of objects in space projected
onto a screen but we also have the
concept of this camera that's moving
about and looking at things so that's
also an important thing to remember
because that's going to be another OPP
mathematical operation and we're going
to sort of shovel a bit under the carpet
but that's going to be there and that
you'll want to be aware of all right so
lighting lighting is super exciting so
we'll learn how to implement lambertian
reflectance lambertian reflectance is
named after Johan and Lambert who
introduced the concept of perfect
diffusion in 1760 book photo maitreya
now I actually have a few scans from an
early edition yeah all right let's start
that over my lame attempt at a joke
today simple lighting right we want to
keep things simple and lighting to be
sure is actually a super important
subject in OpenGL and it gets gnarly
very quickly so we want to avoid that
for today we just want to have like the
base minimum lighting model that we can
use to sort of start experimenting so we
do need to understand two concepts again
ambient lighting which is just a light
that reaches an object after bouncing
around everywhere in the room so it's
sort of your background radiation
lighting it's the minimum amount of
light that a point on a surface is going
to get by default and then you have
something called diffused lighting will
be playing with which is the light that
reaches your eye after a source of
lights sort of the Rays bounce off the
surface and reach your eye right so here
the the amount of light reaching your
eye will be sort of directly
to the angle between the surface of the
object and the source of light and
that's what this whole Lambert cosine
law is it going to be about which we're
going to see in a few frames but again
very easy to calculate all that stuff
with OpenGL it's just right now I'm
giving you contact so when you see that
line in code you'll be like oh yeah it's
that thing he talked about earlier all
right another important concepts related
to the lighting is normals so a normal
is a perpendicular vector pointing
straight out of a face or the facet of
shape right but in our case we're going
to have normals associated to verte de
verte asees all right and that's
slightly different because if you look
at the last diagram here we're seeing
normals coming out of a bent shape so
what happens is that you can sort of
make approximations with normals where
even if you are actually playing with a
triangle that triangle is part of a
sphere right you could make it so that
the normals are sort of pointing out
from the center of the sphere so that
helps the lighting model like this thing
as if it was a sphere and help you get a
better simulation of these shapes
everything so the other thing I want to
quickly say is that normals so you might
all know this already but it's worth
mentioning and almost are called normals
because they are normalized vectors
meaning that they have one unit length
we might remember that from one of the
your old on my case old math courses so
just to make it a little reference
alright so we've talked about vertices
and how in OpenGL they're actually a
combination of things so in this case I
just wanted to show quickly that you
have basically we're going to have
position the normal the color and
texture we're not going to play around
with today but these are like all
attributes that are hooked up with this
bundle of data that is a vertex in our
pipeline and the on the right side I
just wanted to point out and sort of
give you a hint that if you never done
glsl before how that language looks like
glsl stands for our graphic language
trading no wait GL shading language
language which is kind of redundant
maybe its graphical I appreciated
language rather so it's not redundant at
all all right so just be aware of the
vector evac for in vac to that really
represents an area of floats in this
case all right so you know all right one
of the few last complicated gnarly
diagrams i'm going to show today i'm not
actually going to go over this i just
wanted to point out if you go to the
chrono site you can get the full picture
of what is the OpenGL pipeline in big
detail for 3.1 this is actually good
reading once you've gotten familiar with
the basic concepts and you want to start
playing around configuring your own
setup but let's skip over that right now
and look at at it at a very high level
view you can think of the OpenGL
pipeline as a client-server relationship
a very simple one at that you have your
application which is running off of the
CPU right talking to the OpenGL ES
framework client which is also running
off of the CPU and you're talking to
this to configure this whole pipeline
right you're going to be feeding data in
there and that's all happening on the
cpu and GPU land is kind of slow and the
transfer from CPU land to GPU land is
almost equivalent to a web call so
that's that's really the key idea with
this frame just slide that you should
sort of take away right the other thing
is like I had to have a perf matters
slide it has nothing to do with the
numbers I couldn't find a way to shove
it in there but this magic number point
Oh 16 milliseconds in this case if
you're rendering goes over that you're
going to make your users nauseous
especially since we're already we're not
talking about the vibe here right we're
talking about cardboard on phones which
might not be all that great so you're
going to need to start thinking about
performance pretty early in the process
so the better way of doing this you'll
see that we have it in our examples is
to have a frame rate indicator that you
can keep an eye on so that when you're
experimenting if ever you see that's
that frame rate drop dramatically under
60 frames a second which you can't go
over anyway right because the refresh
rate of the device is pretty much 60
frames a second but when you start
dropping those frames you have to ask
yourself ok I should start being careful
right like it should probably change my
approach read a few optimization
articles and start fixing it so early
optimization I'm not super fond of as
most people aren't but early warning
signs that's pretty good alright so the
simplified graph
for the OpenGL pipeline so the thing to
know here the color code so the blue is
mostly happening in userspace right it's
happening in your app in Java so that's
where you're going to take all these
vertices that you got from somewhere and
shove them into the pipeline that's
where you're going to configure the
pipeline and actually pass it the
programs that it's going to execute at
different steps this is where most of
the Java code you're doing is happening
after that the vertex step is one of the
first programmable stages that you're
going to give a program to the pipeline
to execute this thing is running over
each of the vertices being fed into the
pipeline I know it's very theoretical
right now that I hopefully start coming
together in a few slides I just want to
get good basics so those vertices right
you're going to be calculating on them
and you're going to be sort of basically
projecting them onto the screen which
will allow them that the green part so
the yellow parts are programmable stages
green part are fixed stages you don't
get to program them they're part of the
OpenGL framework and in this case with
its what's going to happen it's going to
take the vertex data and it's going to
decide ok so we're on screen are these
primitives so it's going to map those
out and once this is known and it goes
on to the fragment stage the other
programmable stage fragments are
basically programs that are there to
paint a pixel so for each of the pixels
you have on screen the fragment shader
is going to run once so let's we'll see
how we can do a lot of fun stuff with
that later finish up the chain there
frame buffer it's just there to sort of
give you the hint that you can make a
lot more complex builds with the OpenGL
pipeline but you don't need to you can
basically see these two steps and up to
the GL surface view as I'm dumping a
bitmap to the screen all right so this
slide is to reinforce the idea which I
don't find is very intuitive for most
people that you first have vertices that
are being computed on which are sort of
projected onto a screen and once that
that's sort of a relationship between
the model and space and the actual
screen is done then each of these pixels
the
system knows which shape they belong to
so it now knows what computations to
accomplish and what parameters to pass
through your program for it to calculate
the color of that pixel a small slide on
the power of parallel processing so what
we're looking at here is I couldn't find
a good video of it sadly it's an nvidia
event where the Mythbusters were there
to actually sort of show why is parallel
processing much more awesome than linear
processing or CPU so all they had set up
is a paintball gun that was drawing the
Mona Lisa so they had the first one
instance at a time the robot arm would
paint a wall with paint ball pellets
which was kind of cool and then they had
the massive array representing GPUs a
lot of them running on NVIDIA hardware
presumably painting a picture really
quickly right so that's the other thing
to think about is that if that program
is running for each and every pixel in
your frame it's actually all running in
parallel which is how this can be
actually a fast process okay so some
practice some actual code get ready it's
going to be heavy maybe not that much
but anyway we're going to start with
vanilla Android Open GL so again we have
a few people who played around with at
least OpenGL so you're going to be
familiar with this you're getting I'm
going to sell the punch line a little
bit you'll see that with cardboard you
don't need to add that much code to what
you've already learned to actually get
the VR app running which is a good news
so this class diagram the takeaways here
GL surface view it's an actual view
let's inherits from the view and the
thing that actually draws to that view
is a renderer so that's a subclass of GL
surface view you are the person that
will implement that's more of an
interface or an abstract class I'm not
sure I should have checked I will for
the next dog but basically you're going
to be implementing on surface created on
surface changed maybe most of the time
mine is empty and on draw frame so the
action all happens in the surface
created and draw frame so if is created
to initialize this whole big pipeline
we've been talking about draw frame just
to take the data that needs to be in the
pipeline for now shove it down their
press draw we're done all right
threading quick note on threading i made
this nice little diagram and i want to
really go quickly over it though
so it's on its own render thread the
renderer is running on its own render
thread it's got its own dedicated GL
rendering thread so you have the same
kind of challenges that you have with
the UI thread if you want to communicate
with your renderer you need to go
through something like Q event runnable
or some other threading mechanism ok and
this is so that you can get like
reliable draw calls at all you know
you're 60 frames per second alright
cardboard SDK I promised it wouldn't be
much more difficult it's not there are
however a lot well a few new methods in
the stereo renderer which is the
equivalent of the renderer we just
looked at so you have a few more calls
that come in to play when cardboard is
there and we'll see which those are in a
few seconds you might notice at the
bottom here that we are still using jars
in the libs folder a somebody talked to
that team and get them on Gradle thank
you so setting up the GL pipeline the
first step that we quickly glossed over
in the renderer so on surface created is
being called with an e GL config I'll
say it right up front this might be
called more than once during the
lifetime of your program if you get
called in etc right so you have to be
ready to sort of think about
reinitializing everything that being
said the steps here most of the
complexity of the steps is abstracted in
the class i called geometry which you
can see in my actual sample code that's
available on my github with a link at
the end of the talk i just wanted to you
know give you the basic so you get
started so in it buffers yeah so that's
step one then step two is going to be in
a GL program so those yellow steps in
the pipeline we're going to configure
those after setting up the buffers and
then after we've set up the programs
we're going to initialize handles so
let's look each of those steps and break
them down what they actually do right
step one initializing buffers this is
basically initializing a chunk of memory
so that we can shove floats in there
that will be our vertices and our
vertice attribute vertex attributes all
bundled in there so that we can then
pass it to the pipeline
so as you can see it's fairly
straightforward code not much to say
here except you're sort of almost in
cland but that's so gonna have to get
used to it OpenGL of it it's not that
complicated right so step 2.1
initializing the jail program I even
bothered putting this here just to
remind you that we're talking about the
yellow stages programmable stage of the
pipeline and that to mention that vertex
shader code and fragment shader code are
actually big string blobs they're
actually this source code that's what
we're passing in there and so yeah we
won't dig into the load shader method
that you said the top because it's
pretty much the same process as what's
happening below here so the first thing
you do is you call a GL so notice
they're all like static method calls
like every time you interact with the
pipeline it's that kind of call in this
case we're using GL es 30 you might have
been using 20 before or whatever so the
number changes depending on features
that you're using so we're creating a
program we're getting a handle to a
program so we're basically telling the
pipeline create a program object for me
and it's giving us back the pointer
pretty much then we can take we've used
the same kind of process to create
handles to vertex shader and fragment
shader up top right so at that point we
take those two handles that have been
initializing the pipeline and we pass
those two shaders into the program to
finish with a link call so the fun part
and we'll see that later is that when
you do this you're going to actually get
feedback from OpenGL as to whether or
not you have compile errors and they're
going to be a bit verbose so that's
going to be interesting in a few slides
alright so step 3 so I've talked about
the handles right so in this this is how
you get them literally give the string
that's going to be then they will see in
the shader that's the actual variable
name so it's through a string might seem
a bit clumsy other way but yeah all
right so this is our first shader the
vertex shader it's not well it's a
fairly simple semi pass through vertex
shader so this means that we're not
doing much here except a
assigning transforming our current
vertex into its final position as I
talked about earlier right so we apply
Model View and projection
transformations that are stored in this
matrix 0 1 matrix can be a bundle of
transformation that you can apply it in
one shot and that's what we're doing
here right so that lets us know where
that vertex is going to end up on screen
for the fragment shader is going to come
along afterwards to get its list of
pixels right so yeah while we're here
these are so yeah we have to mention
these are the inputs that we just used
in our Java code so I'm just want to
point those out right so you make a
direct mental link between the two and
the outputs are kind of important and
this syntax is actually slightly
different than es tu for people who've
done es-tu before like in and out i
don't believe we're in GL es tu so it's
good to know these things because you
might your old things might break if you
start doing 3.1 poetry alright and the
output basically are things that are
going to be passed on varying variable
buryings that are going to be passed on
to the fragment shader so varying as
well I won't get into that concept right
now but we can talk about it later if
you're interested maybe after the talk
all right so last big point here is that
we're going to set our color or V color
that's going to be passed to the
fragment shader right 2 1 1111 which
means like white pixel or a rather white
vertex attribute I should say and then
next up in the pipeline as illustrated
here we have our fragment shader with
the color being passed into it and we
see that all it's doing is taking the
color that was given to it by the vertex
shader and just saying ok that's the
color of my pixel that's it not doing
anything that's another pass through
shader all right so what does this give
us so once the code once the steps in
the pipeline have executed right
following a call to draw i'm calling it
simple draw here because it's more of an
example so i just wanted to show you how
that works so first step right we're
going to use the program so if you
remember when we initialize that's where
we basically tell up in jail ok those
traders I've configured time to use them
now
and we pass data to those shaders so we
already have our handles right we have
our variables into which we're pushing
the data so we're bridging the gap
between the two again alright so we pass
our projection matrix or transform
matrix sorry and then we also pass
vertices and normals in a big bunch
right and then the last call here draw
arrays so we're just basically drawing a
number of and then we're giving it the
number of vertexes in their vertices in
there and you might notice that the
first parameter of GL triangles is us
telling OpenGL so I'm passing all these
vertices and they're triangles it could
have been triangle strip if our
information was configured differently
they're not we were trying to keep
things simple well I didn't mention this
but obviously at the bottom here you see
the result of this it's a white square
white cube actually what we were facing
in our demo program all right oops yeah
one of those sorry all right basic
lighting so we're trying to fly through
a lot of this stuff so can come and ask
me questions after if you're kind of
like you might a bit faster or what the
hell was that so basic light model if we
use the shader as we've configured them
right now this is what we're going to
get when we're going to rotate our cube
in space not exactly great what we want
is this on the right so we need to have
a minimum of lighting in there to our
scene for I seem to do any kind of sense
and we're not going to go beyond minimum
because there's ways of that rabbit hole
is pretty deep and pretty awesome but
you know we want to get somewhere today
so how do we get to this we need a light
source we're hard coating it in the
shader here but and then you can see
that at the very top right so the you
like pause right there but you can also
know how we apply the you MV matrix here
not MVP so there's an important little
distinction here that you'll see in the
example code it's because you want to
put that light in 3d space not projected
on screen right because you're still
sort of working in the theoretical model
you want to put all the points in space
at the right place right so you're
taking that light you're putting it here
and you want it to be sort of moved
relative to the camera etc but you don't
want it project it on screen because
you're still at the
that were you can calculating
theoretical stuff right you're not yet
in screen space so that's why the
projection part is not there alright so
our point light source point light
source one point emits raised in all
directions right all right transforming
the vertex into ice space so we're
applying the same model view transform
to our vertex and our normal because all
these three things along sigh mean with
the light source right are going to be
used in in the calculations to find out
what color we should be giving our
vertex right then the way to find that
out we get a light direction vector from
the light source right to the vertex so
we find out what that vertex is that
sorry that vector is then we calculate
the famous Lambert factor so that's
actually very very simple right we just
want the angle between the normal and
the light vectors and having that angle
allows us to determine what kind of
shade we're going to give to that
surface the point one at the bottom
actually you might wonder so the actual
finding out of the angles the dot
product we see there so that dot method
being called with Model View normal and
light vector that's the one that
calculates the angle and the max part
with the second parameter being zero
point one that's the minimum amount of
light that's going to be on the surface
but I talked about ambient light this is
ambient light and the dot product is the
Lambert factor it which is diffusion
light all right and then we multiply
that with the color of the object right
so far our object is white we multiply
it by white it's going to give us a
varying factor and we're done all right
so that running that gave us the the
screenshot we saw earlier so let's
enable the cardboard SDK now enough flat
3d we're done all right so a very simple
first thing this is your layout you just
want to full on cardboard view that
fills out your whole view space then
your main activity in this case VR talk
activity so these three lines
highlighted here or what you need to
actually configure your view
so you set vr mode enabled to whatever
in this case I set it to false just to
show you you know what happens when you
don't have you are enabled you're just
seeing a plain magic window type thing
and turning it on just gives us
stereoview so at this point what's
happening is we're calling the draw
methods twice for half the screen with
different camera perspectives right
those two eyes and this is the result at
this point we can just slip this in a
cardboard unit and we're good we're
apster s copy stereoscopy wait I'll get
it one day right so what does this look
like there's two parts now instead of
just having one draw command we have to
kind of draw command on new frame well
not at all it Waka man i should say but
it's being called at every every frame
and the idea here is that anything that
you wouldn't want to set up twice for
both eyes because it's the same for both
eyes you do that job here so in this
case if you remember we talked about the
projection cone earlier right so this is
where we set it so we do set look at em
and we pass all the parameters so now
wait no that's not it it's a camera
position I'm sorry I confuse things okay
anyway so at this point what we're
saying is basically yeah actually yeah
we're setting up our camera based on
well it is the projection cone of sorts
but anyway you'll see in a second all
right oh yeah and the before I miss that
the set light position here would be a
place where you could set and moving
light source but we don't actually have
that in our current scene but if we
wanted our model to dynamically update
we could do those operations here
because you don't want to run them once
for each I obviously all right so the
actual Andhra I command where things get
a bit more interesting so first thing
that's good to do is just to clear the
buffers so just basically clear the
screen so that you don't see the results
of the previous operation the previous
frame being rendered having been
rendered and this is where we set up our
view quemar frustum so we find out what
the ratio is for each I just so that our
perspective we won't get a squished
image right we do that one call here and
that's what's going to set up our whole
cone
view that it's fairly simple we define
right left right bottom top near and far
and then yeah oh yeah I'm just keeping
the nice little things I put highlight
everything alright and yeah okay so
calculating the view transform matrix
this is the part where we take the
camera position and the I basically and
turn it into a view matrix so that's why
we need both right so this is the part
where you basically the viewers camera
think of the V matrix as the camera
matrix of sorts so at that point we have
our projection matrix wherever camera
matrix we know the the height and width
of our viewport so geometry for reasons
of it own uses that don't pay too much
attention to it the important parts are
the updating of information coming from
the renderer you know so that your you
can draw stuff based on the dynamically
changing position of the head of the
user right and then the drakaa man at
the bottom so we'll get back to what
actually happens in the draw command and
earlier okay so this is the part where
things get a little bit more exciting so
what's the point of all of this right at
which point am I starting to experiment
and having fun gives to be honest the
part I'm talking about all of this it's
kind of boring bit like you want to see
things animate on screen you want to
sort of create a world right you want to
be like creative and fast and active and
you want to do stuff you just don't want
to poke around so the problem that we
have is that well okay I'll talk about
life coding so we want to set up an
environment for life coding what is life
coding it's not like coding on stage not
today I'm sorry within Wi-Fi and
everything and you'll see I need the
Wi-Fi for my little demo that would not
have been a great idea life coding is
when you have a code build run cycle
that is so fast it's basically
instantaneous so that you get instance
response as you type out your program
you see what's happening so this is a
basically it's fragment shader editor
online so we're basically live editing
the shader you see as I comment out
stuff like we can see it in action so
here we're just basically varying the
amount
of color depending on our X and ry on a
simple sine wave so just poking about
very very quickly we can experiment and
find out how all of this works and get a
very good feeling for oh so that's what
a shader fragment shader does right and
you can come up with ideas and so that
instant feedback loop is super important
and on Android everybody's painfully
aware that an apk doesn't build
instantaneously right so if you want to
get into that world of quick
experimentation you're in trouble it's
not going to work so that's this is what
we're gonna well this is what I'm
basically showing you today like how to
set that up guess it's not that hard
I'll touch really really quickly oh I'm
kind of short on time so I'll skip on
those and ask me questions after I will
be able to sort of plug those in so the
basic life publishing tools that we have
I use IntelliJ with nodejs and it runs a
small JavaScript program that I can edit
that will actually push my geometry into
the viewer unit so what happens is i
have my pipeline can figure on my phone
and it's getting live data not from
itself but from the internet so using
firebase that way if you play with
firebase you know that its response time
is almost instantaneous so I can push my
geometry and push my edited shader code
straight into firebase which broadcasts
or however many viewer units are
actively subscribed to firebase that's
fairly interesting way so you might be
wondering how to do real-time
reconfiguration the sliders here to show
it's basically the same thing we did at
the initialization stage you can just do
it between frames you can just create a
new shader and compile it and run it it
also means well we're going to get the
shaders at this point right because
we're talking about sure again I want it
to be very very clear that vertex
shaders near they run a few times for
each vertices and then the pixel shader
or the fragment shader is running a ton
of times right what can we do with this
so the geometry I've been talking about
is iterative geometry so with opengl es
3 you get you get a feature where you
can define your shape and you can
actually make a ton of copies of it
changing some attributes of it as you go
along so this is the data structure i'm
skipping a bit over cause i want to get
to the good bits faster so
this is me live editing the geometry in
IntelliJ so i hope i did that fast when
i recorded it i can see it's blinking
right so what I'm doing is every time
and I'm commenting a line I'm rerunning
the JavaScript program which is pushing
new geometry to firebase which is then
coming back to the unit it's actually my
physical unit next to my desk that's my
laptop in this case so we're just adding
new shapes as we go and we're just on
commenting lines rear right here so what
you can easily do at that point start
putting for loops in there and creating
cubes and creating your own spaces
that's a part we won't get to today but
you can see the possibilities and you
can start experimenting very quickly so
then the other thing that we can start
doing is deformation so I'm just
mentioning this to sort of let you know
what's possible with the shaders in
which ways you can use and abuse the
pipeline so in this case what we're
doing is that it's an example the link
is at the bottom again I'm going to
share the slides later but this is an
experiment where you take a sphere you
apply noise and to the distance of
vertex to the center of the sphere and
you get a deformation like this you can
come up with really really nice effects
this way like that demo ends up being
like an actual exploding ball of fire
with like really nice effects and
there's about 20 lines of shader code in
there and this kind of stuff is for
WebGL but it's easily applicable to all
I've shown you today on Android right
quickly the fragment shaders so extra
mapping you probably know what that is
you have a texture you're mapping it
onto a shape right so you have to sort
of think oh who's doing the job of that
it's the fragment shader fragment
shaders getting all that texture data
and since it's responsible for painting
the pixels it's the one that's going to
be looking up those pixels right in some
cases and taking them straight from the
bitmap and dropping them into the right
projected space on those triangles so
where does that become interesting so
you can abuse the fragment shader to do
kind of interesting things so here I'm
going to show hopefully quickly how to
abuse it to start rendering geometric
shapes that have nothing to do with the
actual rendering that we're going to be
doing oh yeah and the quick note i
wanted to show here is that while I'm
editing this so this is
the ACE editor running in a web view
pushing every time i type i change it
pushes the firebase comes back to the
viewer unit and what we were seeing at
the bottom here was like pit get logged
logcat so we were seeing the Pitkin
output of the compile errors that I'm
putting into the the web view right here
the chrome pain so this allows you to
play around with fragment shader and
then in this case the times the FPS is
read the signal that there's a compile
error can look at your logs you see
which line what exactly you did wrong
you start editing and you fix it really
quickly so that allows you to start
experimenting with shaders in a way that
you probably would have never done if
you were just rebuilding the APK every
every 10 minutes hopefully we should see
the results of this so I'm going to yeah
just I'll just let it run until it's
done so now all of a sudden we just have
like this flat polka dot pattern
appearing here so what what's going on
so it's really more of a thought
experiment but I just wanted to
introduce the audience the idea that
there's something called distance
functions that allow you to sort of play
around with flat space so in this case
we're actually ignoring anything 3d
we're just rendering these circles based
on where we are in the in the rendering
pass because the fragment shader knows
we're on screen it is so this allows you
to do stuff that's kind of fairly
interesting and this is an example of
such valve has a really nice research
paper where they explain how to use the
texture system instead of actually
applying and sticking a texture onto a
triangle what they do is they take a
high res texture here like 4k by 4k they
extract out of it a distance field patch
which is another texture really but they
pass this to your fragment shader and
this is only basically telling the
shader okay if you're at this point
you're this far away from inside or
outside the letter right and using that
they basically managed to implement
something that looks like vector
graphics but is actually stored in a 64
x 64 bit map so that's or texture I
should say so this is just to introduce
you to the idea that you can once you
start programming your own pipeline you
can do very fancy things and there's a
lot of people have done a lot of fancy
things and research papers like these
you can
look up on the internet and they are
very very self-explanatory like you can
apply all this to your programs in
virtual space this is a quick
explanation of the full screen fragment
rendering trick this is I'm going to not
explain it to too much but the idea here
is that if you want to draw in flat
space and only use the fragment shader
you just need to put up a full surface
on screen like you put up two triangles
that fill up the whole view screen and
you can start playing around with the
fragment shader you might be saying well
what does that do like what what can I
do that's kind of interesting with that
and here I'm going to encourage you to
go and look at shader toy shiratori is a
fragment editing website basically with
the WebGL WebGL AP AP is they abuse
fragment shaders in an awesome way and
it's not just one awesome way if you go
there you'll get the source code for all
these nice examples if you actually
visit this webpage you'll see that all
these things are animated and you can
just learn from the best and reuse that
stuff in your own examples like you can
see that there's actual worlds being
built here you might be wondering how
the hell can you do geometry if you're
just drawing pixels like how does that
work like they're not even putting
geometry in these examples it's like a
two triangles were looking at so well
you're going to have to explore that
explore something called ray marching or
come see me after i'll give you the
links to the papers because we're short
on time and we won't talk about that
actually over time oh my god am I good
well kick me out if I'm rambling alright
so one of the things I want to point out
too is they do a search for something
called to TC in shader toy this is just
to impress everybody they organized what
was called a to tweet contests so the
concept of the to tweak contest is you
have our many characters fit and two
tweets and that's how big your shader
can be and text wise so they all these
shaders fit within two tweets white
spaces and comments don't care but and
these are all again animated if you just
do that search and go look that up and
all this stuff is online and the shaders
are public
so you can learn really interesting
tricks and if you start thinking this
way it's not actually so hard and I've
done a few proof of concepts where you
can creates a 3d fractals and apply
apply the VR angle to it as well so when
you do that you end up creating spaces
are really interesting to explore and
the major point here is that that's not
so many lines of code once you grok how
this stuff works you can be hyper
efficient and come up with like
incredible effects and you don't have to
have a production crew next to you
that's paid like hundreds of dollars an
hour to come up with all those nice
assets right you can do it yourself all
right I'm almost done here i just wanted
to actually no we're skipping over those
all right your challenge is I'll take
two minutes just to sort of overview the
small problems with the platform so
cardboards it's awesome right but you
might see where I'm going with that
slightly problematic that can be a
drawback it's not necessarily only a
drawback but you know the other thing to
you have to think about when you're
designing apps for this you only have
one clicker in some cases you don't have
one at all and you have to sort of
design for that that's that's slightly
problematic as well you can get creative
and start doing tilt to exit right so
that's not a bad idea so you can take
examples from the best apps out there
and start implementing them in your own
you have to start thinking about head
tracking it's a super rich source of
information you can do all sorts of very
interesting experiments with that I've
heard of a startup that recently got
financed in Montreal and all they do is
run that and that it takes to see where
you're looking at in the scene like
where the head tracking is going what's
interesting and they're going to resell
that to advertisers there's fun stuff
you can do there I'm most of audio if
anybody's went if anybody went to the
HTC vive a tryout you know what I'm
talking about it puts you into a scene
so much more it's like con there is that
it's cardboard we're talking about not
everybody is going to take the time to
put on headphones just for your app
right so you have to be a pretty
compelling app to actually get somewhere
some pitfalls of the platform so its
food for thoughts for designing your own
apps again so some of the things that
don't work all that grates centering and
drifts
so you can try any cardboard app along
while enough especially if you have a
unit that's strapped to your head you're
going to notice that you start off the
game sitting this way at some point
you're like wait I'm sitting to the side
I'm still looking forward in the game
what's going on and nobody seems to
think that maybe there should be some
sort of research center switch because
again you're sitting in your sofa right
you can't your motion your range of
motion is basically well not know but
it's very very limited so what's the
solution you suggest the people to sit
in a swivel chair I guess that works
although it kind of gets tricky if
you're alone at home and where did I put
that glass again or where's my coffee
standing up with cardboard actually this
is why I think it's kind of more clever
in a way that they put it then the way
they have to hold it because when you're
standing up you can't just flail around
and whack people in the head right so
that's kind of interesting problem there
though is that your reflex to move is in
grains like I've like if you standing up
I am sure that you're gonna drift for
having tried it I can't help myself at
some point I just take it off an oh crap
I'm like three feet this way right so
what else I guided experiences so you
have since your audience is captive you
can guide their experience so these are
screenshots from a lot of well-known
things i think but just to go over them
again so this is from a not field trips
with expeditions so over here ed phones
no distractions so that's kind of
interesting if in the case of
expeditions for example the students
wear headphones so that they are not
distracted by the others and sort of
follow instructions more on the the
instructor has like the microphone that
can feed into everybody's head basically
right you can guide your group you can
point big arrows that you know let's say
where to look at and what I've just
shown you you should see that I've just
given you tools that allow you to
publish live to a huge group of people
anything you basically want so that's
that's there's a pretty powerful
combination in there and again it's with
tools that don't require a big
production crew behind you to get to
somewhere interesting the advantage is
with cardboard back to reality is really
easy
you don't have to strap the thing off
your head scripting so like windy day
with a little mouse there everybody's
probably tried this it has scripted
actions right so it's being clever and
its use of okay the user is actually
looking now I can do something that
these are all considerations that you
have to look into when you're designing
a nap another thing I'd recommend is
there's an actual design pattern app
from Google that you should really look
at that's very compelling and has some
very clever ideas of where you should go
all right that almost done two slides
yeah I'm a bit over but nobody hates me
so that's good time for an upgrade so
this is the point where you go ok
cardboards nice but I'd like to take a
proper sort of set right so what can you
do so we have i NVR i sponsor at the BBQ
I tried their unit really nice the
production units I think should be very
promising like I'm really looking
forward to what they come up with
probably it's going to be soon that's
when I lose my voice yeah the oculus
gear is actually super compelling for
having tried it the big advantage here
is that they put the touchpad here so
you can interact with your world so
that's that's very much a must and
something very much lacking in cardboard
like this default sort of platform I
want to interact with the world the
homie dough is actually very good
headset as well and they come they have
an offer for like a bluetooth remote
that can be used so that that for users
can be interesting Android wear I've I
I'm not sharing this with you yet but
eventually on the example code side
there's going to be a proof of concept
I've built with Android wear where
instead of having to touch your temple
you're going to touch your watch right
and you can easily feed events touch
events into your cardboard app when the
Watchers hooked up it's actually super
compelling it's got like some
interesting you see like interesting
dynamics because your hands are sort of
trapped when you do this right so you
sort of lose the risk that the users are
just going to flail that hot cup of
coffee into the dog or whatever right so
those are like thoughts another thought
here is that well unless they change it
in recent months as far as I know if you
sell physical goods from an application
nobody
cut that's true i believe from google
play and the play store the Play Store
the apple store so that can always be
interesting if you want your users
support cardboard first and if you want
users to have the full experience and
you can actually upsell them on it like
you could make money that way so that
could be interesting because we all know
selling apps is not necessarily an easy
proposition sometimes so that's one
factor you could consider which is what
this slide was actually about so i went
ahead there all right I'm done thank you
so much i appreciate you me
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>