<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mobile machine intelligence: today &amp; tomorrow (Plenary - Playtime EMEA 2017) | Coder Coacher - Coaching Coders</title><meta content="Mobile machine intelligence: today &amp; tomorrow (Plenary - Playtime EMEA 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mobile machine intelligence: today &amp; tomorrow (Plenary - Playtime EMEA 2017)</b></h2><h5 class="post__date">2018-02-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YiBx-oO347Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone my name is Rishi and one of
the engineering directors in the Android
and and the Play teams I honestly feel
like I lucked into one of the coolest
jobs at Google and there's a lot of
activity going on in the space I
promised myself a beer every time
someone mentioned AI or machine learning
and it seems like at the end of the day
I'm going to be very very drunk at the
rate of things things are going today so
it's no secret that if you've been
following our narrative in the last
couple of iOS a few Hardware
announcements that Google's heavily
banking on AI and machine learning is
one of the core pillars of a product
development and and there is a reason
for that
you have heard our CEO mentioned before
that we used to be a mobile first
company and now we are an AI first
company and I wanted to give you some
context into how we evolved into that
vision so come to think about it in
retrospect it is easy to to imagine what
it meant to be a mobile first product a
mobile first product that it is a
product that is fundamentally designed
for a small screen it is fundamentally
designed for a for a product that has
always on connectivity it is
fundamentally designed for low power
usage and so on and so forth so we
believe that products of tomorrow will
will take some of these other things for
granted
so products of today for example it's
hard to imagine an app that is not
designed with some of the principles I
mentioned in the mobile first column
five years down I think products are
tomorrow will have to have some of these
other columns other factors in the AI
first column so products of tomorrow we
believe will be required to learn and
adapt will be expected to learn and
adapt from from user behavior
we believe that products of tomorrow
should assist the user in their
in a day better we believe that products
of tomorrow should be conversational and
assistant so products of tomorrow should
be able to figure out if they don't know
anything and they should be able to ask
the user for the right information
products of tomorrow should also be
context aware they should be aware of
the state of the environment they should
be aware of the state of the user and
they should be using that information to
make that product better and last but
not the least products of tomorrow
should be trustworthy in a fundamental
way and that's not just about safety
security and privacy but also being able
to deliver performance that is reliable
on day one and undo it day hundred so
with that in mind machine learning is a
fundamental set of algorithms that help
our products and our platforms get
closer to that vision I see here on this
slide that machine learning is the
science and the art of making machines
learn your product shouldn't just be
great on day one but should be even
better on day hundred and machine
learning will help you get there so as a
running example I picked a picton app
that a hypothetical app that will take a
picture of an animal and lets you
classify it as a dog or a cat and in in
the world of yesterday you would develop
this system by designing a bunch of
heuristics based on human knowledge of
what separates a dog and a cat and you
would just encode that knowledge in a
bunch of rules but real world is much
messier than that and very unpredictable
so your sort of rules might work for ten
images twenty images a hundred images
but as I as you start to gather hundreds
of thousands of images all kinds of
corners cases pile up and it becomes
really hard and intractable for a
rule-based system to evolve so before I
think this took this new gig on Iran
searching discovery for the Playstore
for four years and even our abscess tag
started with a rule based way and as our
product started to grow it became harder
and harder to
over time and so machine learning comes
to the rescue so compared to rule based
systems machine learning systems take a
completely different approach to solving
some of the same problems so here I show
a simple schematic of something called a
neural network it's called a neural
network because it mimics the processing
our brain and and try to achieve it with
software systems so the way this works
is given a picture and given millions of
such pictures it will take the pixels of
that picture and then follow through
with layers of computation computation
to allow us to predict at the end
whether it was a cat or a dog it doesn't
start with hard-coded set of rules that
embed our knowledge about cats and dogs
he said it learns or relies on millions
of samples and data and with every
sample it gets better and better and
better at doing doing their job so back
in 2001 a 2011 on some of the standard
data sets around computer vision around
recognizing objects in images we had
around 26 percent error rate compared
that to error rate with humans about 5
percent so you could imagine that at
that time we would be lucky if we could
classify a picture as indoor versus
outdoor but we weren't too good at
reliably recognizing what's in the
picture so today these systems based on
deep neural networks are even better
than human level performance it's a
three percent error rate and
perceptually it feels like we are now
able to see the world through machines
and that enables a whole lot of other
things other cool things we could do
with software systems it is not just
about labeling pictures is a cat or a
dog but we can automatically caption
them it's really human sounding captions
so I show some of the examples here the
left-hand side table picture says a
close-up of a child holding a stuffed
animal so if you think about it there
are many primitives that goes
to a label like that we not only detect
that there is a young child and a
stuffed animal in the picture we detect
that the child is holding the the animal
and that it's a close-up so there are
multiple layers of intelligence built
into a caption like that there are many
you know cool examples here let's really
showcase how the technology has evolved
over time this scientist in me would
cringe if I hadn't put even a single
example of a mislabel you can see that
the bottom row says a man flying through
the air riding a snowboard and it's
clearly not that so AI systems today are
much better well compared to where they
used to be but they're still not perfect
and any product you design with AI in
mind has to be aware of that so the
types of problems Americans saw like I
want to give you a quick flavor for the
types of use cases you might be able to
power with ml and AI so the first
example is classification and it's
similar to what I showed before into a
picture label it as a cat or a dog take
a song label it as a pop song or a rock
song or take a nap and label it as a
harmful app or not these are some of the
examples of of classification take an
item and label it within a namespace
another example is prediction so here I
show YouTube next recommendations for
videos to watch if you watch this video
so machine learning systems are getting
really really better at predicting user
activity if you watch this video given
the knowledge of other users who have
watched the same video before are you
able to reliably predict which app
they're gonna which video they're gonna
use next and the third example sorry how
do I go back yeah is it's perception so
machine learning systems are getting
better at not just labeling and
predicting but also being able to
understand the world through images
through sound through audio and -
natural language processing here I show
an example of assistant integration and
I know
you'll be hearing about that through the
day so few examples of where Google's
used ml technology over the few years so
Google has a wide array of products some
of these products 7a products are used
by billion plus monthly active users a
day and many of them fundamentally used
machine learning an example here is
google lens this is one of the you know
the recent examples of machine learning
has fundamentally changed the way
products behave today so you are able to
not only detect landmarks but are able
to do that in real time in the
viewfinder of the camera and in perceive
the real-time imagery through machines
next example is machine learn
recommendations Kobe mentioned that ml
is used heavily for recommendations on
the app store
here I show example from YouTube so
given your video watch history what
videos make the most sense for you
another example is Google Translate
where you can look at the picture in
real time detect not only the text in
the picture they take the language it's
in and able to recognize that all in
real time
another example is smart reply in Gmail
and inbox we can process the text of the
message and are able to predict what the
likely response is going to be so for
someone like me running from meeting to
meeting trying to respond to image
through messages on the staircase this
sort of feature is super useful and last
not but not the least this is an example
where it's not a user facing application
at all but our ability to to optimize
back-end processing in data centers
using deep mind AI from our office in
London we are able to reduce our power
usage in Google's data centers by up to
40% so Google not only builds some of
these products but we also have some of
the most used platforms in the world
today and it's not sufficient for our
own products to get better with
IML we also want to empower all of you
to use AI and ml in your own products as
well so to do that we open sourced some
of the fundamental parts of our ml stack
through tensorflow
a couple of years ago and this is a
repository in github that any one of you
can you know download and use it's
getting huge traction in the developer
community already just in two years this
graph shows the github stars for
tensorflow compared to other such
repositories out there and it had just
you know it's been exceedingly popular
and getting used more and more we are
not going to stop there though so
tensorflow
when it was open sourced a couple of
years ago it was fundamentally designed
for server use case and at i/o this year
we announced that we are taking it to
work on device so tensorflow Lite which
is going to be a fast and efficient
version a pencil flow going to be open
sourced this quarter will be instance of
tensorflow that runs on android devices
and last but not the least we are not
only making it possible for use to you
to use ml libraries but we are also
making it possible for you to directly
access Google's intelligent api's so
example here is detecting faces and
emotions in faces through Google's cloud
ml api's and some of the same api's are
available on device as well so that's it
I'll be available for taking some of the
questions later down the day and I know
we have a deeper dive on ml during the
day thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>