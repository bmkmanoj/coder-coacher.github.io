<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Android NDK performance in an ART world - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="Android NDK performance in an ART world - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Android NDK performance in an ART world - Google I/O 2016</b></h2><h5 class="post__date">2016-05-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ok97X9Z4Si8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right thank you to everyone who got
up so early this beautiful Friday
morning after the fabulous party we had
last night and for everyone who is who
is attending virtually you know I'm
sorry you missed out out of it I'm sure
there's plenty of footage and video and
all this kind of stuff from that because
it was pretty fun and I'm sure people
are a little bleary-eyed so thank you to
the P to those so it's hardcore native
developers who came out here and let's
see this works I get lucky all right so
any capers in our world how many people
here are NDK developers all right this
is good this is good I'm excited to see
so many people out there who are doing
this this is a labor of love for me this
talk so I'm really glad you guys are
here and you know there's a lot of
Android development we get to just
blissfully ignore a lot of what's going
on in the operating system you know I
take the blue pill you don't really need
to know what's going on and how the
matrix is formed you just get to go and
code in a nice beautiful environment in
managed code but today we're actually
going to take the red pill we're gonna
get in a little bit inside to help
things work and I hope it's fun and and
it may whether or not it's useful or not
you can tell me but at least hopefully
we'll all learn something from this and
of course I'm talking about native code
I'm referred for furring and
specifically the unmanaged code does not
use the Android runtime and one of the
primary reasons that people end up
creating this code is if they already
have a bunch of working C code you know
when you have a lot of code that works
you create new code you just create bugs
no one wants to do that now of course
your legacy code might not include
support for really you know awesome
multi-processing or might have you know
not have support for neon intrinsic so
even though it's awesome that you have
this legacy code you're not always
guaranteed to get the best performance
with it that being said performance
there's a lot of people who write NDK
code for performance reasons how many
people are doing it for performance
reasons out there
how about how about cross-platform
compatibility Lemieux says to show of
hands okay so it's about about the same
and a lot of that means to get the best
performance though you need to know
about things like the number of types of
cores you know thermals
in the design like there's a lot of
different variables to actually making
sure you have you know the best
performance but oftentimes native code
is your only option
and of course reach you want to build
and hit multiple platforms and C++ is
just getting better and better at
isolating the operating system for you
which is awesome
and finally the NDK has a few
capabilities aren't yet available inside
of the runtime now what I'm excited
about is that actually if you look you
know up when I wrote this talk you had
to actually to get low latency audio you
had to use open SL and if you want to
get the absolute lowest latency audio
you still need to use that but I'm
really excited that and then in Android
n we actually are now allowing people to
use Java to get pretty darn good latency
on their devices and of course it's the
only way to access Vulcan the graphics
API right now and that just makes a lot
of sense and University however Vulcan
is you definitely don't want to be
making that many J&amp;amp;I calls if you don't
have to not that it doesn't work so this
is probably review of you for you a lot
of you since you're all indicate people
what exactly is it you know it's a
combination of tools and headers that
allow and Android development you know
all applications are eventually tied to
manage code running in the Android
runtime and the android framework itself
is written largely in managed code your
live arrays are ultimately loaded into
this framework typically from within
Java code and they all get to talk to a
stable application binary interface
which then of course talks to the native
system libraries and this is important
there's a lot of people are like well
why can't I just use all those other
libraries that are there and people have
done this and and that's why
compatibility breaks and we're actually
one of the things that we've added to
end is if you actually try to use any of
those internal libraries they will not
work anymore we actually have separate
link spaces so there's good and bad news
to this the bad news is you're actually
to have to put things like Lib PNG into
your app the good news is when you put
them into your app you're guaranteed to
get your version of Lib PNG on Android
and above while on previous versions of
Android that was not always the case
so I think in a way
solves a lot of problems all right so
it's grown over the years and you know
it started off in cupcake and we added
you know media futures and gaming
features and the ability to do you know
fully native applications
you know it's lots of cool stuff
recently however we've actually you know
gone even further we've we've actually
put renderscript finally into the NDK so
they're bindings for that we've
continued to expand graphics with Vulcan
and we've hadded a couple of really
interesting ones in the latest versions
of the NDK we've added trace
choreographer and multi network and at
least I'm going to talk a little bit
about trace and choreographer in this
one so in addition the NDK toolchain
which is separate from platform releases
has its own set of 12 major releases and
many minor releases 12 just came out
this week if you haven't seen it and
here are some of the highlights we added
STL pretty early on and then we fixed it
yeah there we add more platforms like
x86 and MIPS we added the claim compiler
in 8c and the misunderstood arm 7fp and
90 which we are actually taking out of
NDK level 12
so 64-bit in 10 and 11 we finally
transitioned to clang as our primary
compiler we've actually deprecated the
use of GCC and a middle there we moved
to lldp as our primary debugger but
again this talk isn't just about the NDK
it's also about art so what I did was I
was really curious like art is a totally
different beast than our previous
runtime so how does it affect the NDK
and so what I did is I went through the
perf Jan I article I mean probably
everyone's read this perf j'ni article
right like yeah or if you haven't you
really should because the real question
I wanted to know was do these tips
actually make sense in an art world and
so we're gonna keep a scorecard for both
are involving for each one of these tips
and so we and so let's let's go through
them and and we're gonna benchmark these
I actually benchmark these with Google
caliper which is pretty cool it's
actually what we use inside of our CTS
tests for all of our benchmark marking
so it's about as good as you can get we
use it a lot internally of Google for
doing all sorts of benchmarking so if
you ever want to set it up it's actually
pretty cool you'll learn a little bit
about Android just setting this thing up
hopefully we'll make it more user
friendly
at some point in the future and to the
basics when you're using j'ni to
reference manage objects you're actually
taking advantage of reflection and named
of course because it allow it describes
the ability for code to inspect itself
at one time now to access a class field
we take the object reference for the
class and use reflection request the
field ID my name okay and from our
developer training here's what it says
it is important to class do cache class
references field IDs and method IDs
because they are guaranteed valid until
the class is unloaded and that really
happen rarely happens that it's really
easy to catch it it does so let's look
at our first performance suggestion and
how it holds up under art so a nice way
to catch these IDs which nicely solve
the unload problem is just to put it in
the static initializer so there you go
you know you go you called me like this
one needed the NIT inside of that you
get all of your field IDs so as you see
I'm making a native call inside of my
static initializer which is kind of
slick right after loading the library
and now those IDs are good forever so
it's really nice if the class gets and
loaded the static library initialize
it'll get calls again and here's how it
benchmarks and it turns out and these
now this is the Nexus 5 running KitKat
akin marshmallow but if we can see both
get field ID and get in field are
actually slower in art in other words
the advice to cache field and Method IDs
is actually more important in the world
of art than it was in the world of
Dobbin and we'll see we'll see a trend
here and the difference in the cost of
get it fuel is a little surprising both
one of the both runtimes actually
changed the state of the internal thread
and that's actually what costs costs
about it for a bit of performance here
although art actually clarifies what is
doing we scoped object access rather
than scope j'ni thread state and what
we're seeing is a couple of things one
is it art is pretty complicated by
comparison to doll Vic and it's got a
lot more going on the garbage collector
is much more sophisticated it's much
higher performance and the other thing
is it's just being really defensive
there were definitely times in dalvik
where you could cause it to deadlock and
it's very very difficult to make art do
that so that's that's the beautiful
thing about all this work they did but
it does cost us a little in actual
performance so this is something to know
also I love this use of the preprocessor
to prevent all this duplicated code okay
as far as cache from field method IDs
ago it's a big win on dalvik it's even a
bigger win on art so let's look at our
second performance suggestion and we'll
see how it holds up if possible that is
usually faster to operate with utf-16
strings Android is usually not require a
copy and get string cares whereas get
blah blah blah okay you heinel have to
read that to you basically it makes
sense if we don't have to copy strings
it's gonna be faster right so here are
the two calls that get a string you can
either get the ucs string which is a
16-bit you know standard format for the
manage side of things and UTF which is
of course what a lot of our native code
expects and we would expect them since
there's no conversion going on that
actually the UTF would outperform the
UCS okay so here's let's take a look at
how this actually benchmarks and this
was a little surprising to me because
this was a short 15 character string and
it turns out that it was actually faster
to send the 8-bit decoded string across
instead of the 16-bit string on art I
don't think you can see it actually
benches the way you designed it expect
it to but it's even then it's very very
close and it was a little bit of a
surprise and and the reason and so I was
like okay I've got to understand why
this is happening so basically I was
like let's try let's first of all try a
longer string and see if it actually
behaves correctly so the longer string
we actually see yes this actually dwarf
benches out the way we expect but it's
surprising so let's look at the code and
see what actually is going on so you can
see inside of art there's this line
called is movable object and it
sometimes gets to return a coffee and
sometimes returns the original string
and is movable object it's actually kind
of an expensive call so what ends up
happening is that this is actually it's
a for loop inside of here that goes
inside this fine continuous space or mob
Jek and that's actually what takes the
time so it really ultimately on on art
for a lot of normal string sizes you
might actually be better just sending in
the utf-8 today now of course this is
all subject to change and maybe they'll
watch this and be like oh god we've got
to fix that but at least today that it
doesn't matter all that much as it turns
out so that was a little bit surprising
and again this is a micro benchmark so
it certainly is gonna help prevent
garbage this is working you know right
fortunately for all you people on the
livestream I've got a microphone so on
our on our Jan I tip scorecard it's
probably a decent idea to do it on both
but it's definitely definitely better on
dalvik finally let's look let's look at
one more tip here and this is this is a
this is actually me paraphrasing because
I couldn't come up with a quote that was
even close to being good enough this is
this is actually a very very good thing
that actually paraphrase because this is
important because you want to reduce
j'ni overhead and actually using region
calls to copy data if you're going to
end up copying it in the end of you know
in the finality is actually saves you
quite a bit of performance so here are
the two native functions that copy data
from string into a buffer one of them
does it using kind of the old-school way
of doing of doing get copy and release
and the other one does it using
getstring region which is all done at
once
which is cool you're eliminating some
J&amp;amp;I calls and as you would expect oh the
other thing I was going to point out
here is that I actually passed the
length of the string in as well now I
could of course just pass the object
from the runtime and query it but
obviously that would be another Jan I
call that'd be a huge waste of time and
it turns out that it's 20 to 30 times
faster just a passing additional
parameter rather than making another Jan
I call so as a general rule you're gonna
always want to have more parameters into
calling into your native functions if
you can if you can possibly avoid ever
making a call back into the runtime ok
so here's how to actually benchmarks for
copying string data and and you know
once again this is kind of what we would
expect you know that the the getstring
region is extremely close actually
between the two viens on the same
hardware and and getstring cares is is
definitely slower than both and so
region is definitely a good one we'll
put that on our scorecard so you know
sharing structured data between managed
and unmanaged code in the fastest
possible ways and as easy as sharing a
soft-drink you basically have two
options you either use arrays or direct
byte buffers
and it really comes down to your use
case you know if a razor useful if
you're primarily going to be accessing
the data from managed code but here's
what our tips say about direct byte
buffers okay this seems like a rather
uncommon
like you think we could do better than
this okay is it actually slow like on
our on our honor both of our runtimes on
a real device well this is why I was
like I've got a benchmark this one too
and as you and actually you know it's a
it's not terribly surprising but we see
about two times the Raj anti cost in
order to get the direct buffer address
access rather than just using a the
standard stuff that's built into the
runtime but you notice something really
interesting here and about this one and
that if we actually go to read back from
the byte buffer it's actually like three
times as expensive to read from dalvik
compared to using art and like like this
makes no sense like we've seen in every
other call we've seen that that that
dalvik does this much much faster than
art so what's going on here let's let's
let's take a look at what's going on
actually inside the code so basically
here's what this looks like as you'd
expect this is our this is our byte
buffer direct byte buffer we can see
that a regular byte buffer is actually
backed by a java array while the direct
buffer is backed by a memory block and
so far that's exactly what we'd expect
and here's how we start actually reading
an integer we can already see the direct
byte buffer as well and it has an
additional level of indirection and
based on the profiling I've done that
probably cost us about five nanoseconds
and now we're somewhere here's our byte
or implementation in our memory blocking
implementation both conveniently in the
same class and our memory block version
actually uses J and I so that explains
why direct is so much slower it's
actually doing a full jan i call just to
access this memory but why is our it so
much faster well here's what that native
code calls into but it still doesn't
tell us why art is faster the real magic
actually happens where the j'ni function
is declared
because jan i overhead is pretty high
the wrong time actually kind of
shortcuts it and into a fast mode in
certain cases
so inside of arts native code we declare
the je and I call with exclamation point
to make it clear that this is a
potentially dangerous function and now
the thread gets to stay in running mode
without having to switch to native so
looking back at our graph our direct
call is about half the speed of our byte
buffer call and this is partially
because the the runt the version in the
run time is actually using another trick
it's really an inline intrinsic so we're
never going to get the kind of
performance out of out of our native
call as we will even without most of the
overhead compared to what's going on
inside the runtime and partially because
we have a little bit of overhead even in
this kind of fast mode and this is about
60 nanoseconds of overhead on a nexus 5
so it's it's not huge I mean these are
tiny tiny amounts you have to remember
that almost every single call you make
in Android ends up going through one of
these eventually so the question is is
there anything we can do to avoid having
to make a call into J&amp;amp;I for each end we
want to read and it turns out there is
we can copy the entire array at once and
then get the end individually and now
we're gonna do some pretty expensive
stuff here so it only makes sense if
you're going to be reading a lot of
structured data from the byte buffer as
always we want a profile just to make
sure but once again you can see that
that that this is pretty awesome
in terms of art alright so that was the
really really scientific part where I
did lots of benchmarking let's get into
actually doing some fun stuff if that's
maybe more practical so this this slide
mostly demonstrates why I'm not a design
advocate but there are lots of choices
to be making when using the NDK as well
as some updates I'd like to highlight so
we have a couple of samples here and I
wanted it I wanted to show you them
let's see on this on this on this phone
I have up here and this is this is
actually running the n Developer Preview
and there is a really cool NDK sample we
have up here called more teapots and you
can see a normal mode it runs at 60
frames per second this thing uses
geometry instancing
to get actually pretty decent
performance running even with you know
fairly fairly a lot of teapots flying
around basically and if we switch over
to this to the laptop you can say I've
actually take
this trace of this now how many people
here have actually used this trace I
want to see a show of hands because this
trace is like my favorite tool ever
almost it is it is pretty awesome and
it's kind of hard to understand
completely what's going on when you're
just looking at it here so so basically
you can see that there's this inside of
my more teapots application I've got
thread 94-91 and inside that I can see
egl swap buffers with damage khr and a
queue buffer and then if we look also
inside of our surface flinger we can
actually see that there's a business
there's a thread that's tied to that as
well that's that's responsible for
updating that surface but we can do a
lot better and let me show you a little
bit of what what can what we can do okay
back to the slides for a second so oh I
I didn't want to go over a few more
things here but I'll do that later okay
so what we can do start off is let's get
rid of that ugly name we can actually
name our threads in this case the code
is actually attaching to the VM to the
runtime and so you can see here that we
actually are using this attached current
thread with attach args and if we do
that and let's see if we can switch back
to let me see back to the laptop and
I'll show you kind of two things here
one is that I've actually added
something called too many teapots so
this is this is like instead of 512
which the regular demo has we're now
looking at one that's rendering 4096
teapots which is actually way too much
for this poor sad Nexus 6p to do and
you'll see a couple of things
interesting one is that we now see that
CPU for here is actually active and
that's really really bad because CPU for
it turns out is one of the high
performance CPUs that just absolutely
will cause the device to thermally
throttle relatively quickly especially
if someone has it inside a nice
insulating case which of course a lot of
people do in fact I did this with and
without cases and I got about 10% better
frame rates once I took it out of a case
there to give you an idea of how these
things actually matter
and but the really cool thing is as you
can see if we go down here should be
here maybe it's not in this one it'll be
in the next one this is supposed to have
a name threat Oh mmm this is this is
this is the the challenge with with
getting systrace and sometimes I miss
things but any case this will actually
show the threat name I swear okay let me
let me show you a couple of other things
on the on the actual device itself so so
what what is why this is the too many
teapots demo as you can see it's running
at about about twelve frames a second
which is really not very good but let's
say we absolutely wanted to put 4096
highly including teapots into our into
our game running at you know some
reasonable resolution okay how would we
actually use what's built into the NDK
to help us with that so we can switch
back to the slides I'll talk a little
bit about that and what we can do first
of all we want to know what's going on
with performance so one of the cool
things we've actually done is we've
actually created a native trace API and
this has actually been in the platform
ever since M so we've always had to
begin and end sector with it always
we've had beginning in section for
several version which allows you to
actually annotate these sis traces but
now we actually have a native API to do
it for you so we can actually see not
just like the khr swap buffers thing but
actually individual sections of what's
going on in our engine which is really
really awesome and you just do and a
trace begin section a trace end section
it's pretty lightweight it's a lot
lighter weight than doing a log-log
statement and you'll actually see all of
these things show up in the graph really
really nicely now one of the things we
can do really really easily is to reduce
the size of our render target now what's
cool is that actually using the asset
manager API is that are built into the
NDK we can actually get the density of
the screen and that's useful because
obviously we you know we might want to
say our application doesn't need to
render in anything higher density than
hdpi but that's nice because it still
actually preserves the screen size it
means that we're actually really truly
giving the giving the user a decent
experience by using density as our
target so let's let's let's switch back
to the the phone here and I can show off
the first thing we can do here so this
is just scaling the target obviously
that's not a very good user experience
here but that's actually scaling it
based on density so I'm now use I'm
actually now rendering the screen at
old-school hdpi just by using just by
using the density metric there to scale
but obviously that's not very good but
we can do something better and you see
we're getting a good performance game
we're about 25 frames a second here
however we can actually make it even
faster and actually make it a better
user experience all at the same time and
by taking advantage of the compositing
engine inside of surface flinger to do
some of the work for us and actually
save some memory bandwidth in the
process so we can we can actually let's
see did I yeah let me switch back to the
slide here for one moment okay so this
is what we actually do so in addition so
once we've gotten that we know you know
the ratio that we want all we have to do
is call a native window get out get the
right format so we don't want to change
the format of our native window when we
do this and then we can call set buffers
geometry and this is pretty cool because
what it'll actually do for us is it'll
automatically scale that render target
for us and make things a lot happier and
a lot more friendly and actually when
you're when you're looking at this on
the device unless you're looking
carefully it's really really difficult
to notice what's going on here so we can
switch back to the phone and you can see
what I'm talking about so now this is
once again this is using the internal
compositing engine to actually scale
things up and you see we're actually
doing better than we were even rendering
in just a little corner of the window
because it's actually using less fill
and the other thing that's really cool
about this is the the fps is actually
being rendered using a pop-up window and
that pop-up window is actually running
at the full device resolution so that
text that you're seeing there is
actually rendered at the super high DPI
stuff and only the background is
actually being rendered at our lower
resolution and the compositing engine on
a device like this can do enough windows
that we're actually getting the benefit
of that so we have a HUD running it
really high-res
background running at a lower res and
you can barely tell because our eyes
seeing characters with a bunch of
jaggies it's really noticeable to us but
seeing a background it's a little lower
resolution not so bad this is actually a
cool trick I don't think I've ever seen
anyone use this in a shipping app but it
can be done and it works pretty well
the one trick about this one is it
really helps to be an immersive mode
when you're doing this because every the
the pull downs for the notification and
for the the bottom of the system buttons
at the bottom actually also take Windows
so by going into immersive mode you
actually free the system up to have more
windows to use for your app in the
compositor okay so we can actually go
back to the slides now okay so another
thing we could do is actually say let's
try to cap the frame rate and we
actually added an API called
choreographer and choreographer is great
it actually gives you a callback
whenever there's vsync coming in and up
until then it was only available inside
of managed code but inside of N we're
now opening it up for use in all of your
code inside native code and there's a
great example of this actually but let's
let me let me actually show you what
that looks like well so we can switch
back to the device here where is this
one called
oh maybe I don't have it on this on that
screen yeah there we are and so now this
is actually using choreographer to lock
things as close to 30 frames a second as
possible and you can see there it's like
28 29 we're still getting some jank and
you can see that it's probably running
on the thread so we're overheating
pretty quickly there but that's another
great thing you can do what's awesome
about choreographers a lot of people are
like their their app or game doesn't
need to run at 60 frames per second and
they'd much rather save the user battery
life not have the phone overheat in
their hand you know users actually
respond on comments like alright I
played this game I played this game and
I was burning up and I had no battery
left an hour later and one easy thing to
do is say you can even make a slider
saying how many watch what are the
frames you want to limit the frames per
second of your game and for a lot of
games that actually is huge and
choreographer it allows you to do that
in an extremely extremely powerful
way I mean what we're actually doing in
the code we can switch back to the
slides here is is basically we're
getting a frame callback and then we're
simply every other frame callback worth
swapping so it's pretty straightforward
this is not the most intelligent way of
doing this because you will get jitter
if we're not actually able to render at
30 frames per second this is kind of
awful but as long as we're actually able
to do this this actually works pretty
well and and it's actually not that hard
to make it more complicated this is just
the sample that we ended up writing
what's cool is if you do go to that QR
code that sample is live today and it's
cool it actually uses choreographers
around the latest version of Android it
uses egl extensions for on a couple of
older versions it actually goes down to
the job of the java version of the api
on earlier versions so you actually can
see how to do it all the way back to I
think 16 or something like that
which is pretty nice so the sample is
cool it's a little complicated and it's
not using more teapots it's just a
single teapot alright so I showed you
that and there we are we got a little
bit of performance clawed back we're
getting around 30 frames per second now
and we're not overheating under the wolf
vision here and this is even better than
we did with reducing just reducing the
render target and we haven't added had
it had to add a single non-native call
to do any of this mind you every single
thing that you're watching here has been
entirely inside of the NDK which is
which which I'm very pleased about and
I'm sure that you know our developers
are too and let me show you one or the
little thing that we can do and that you
might you might want to do and why you
might not want to do that on n so I have
heard about what we've done with
sustained performance mode so sustained
performance mode as a feature mostly
intended for things like VR that were
could that really really require you
don't want to drop frames you don't want
the device to overheat and so basically
what they do is they actually try to put
the device in a mode where it's never
going to overheat
now the problem is that at that rate
under the conditions they test on you're
not gonna get the highest performance so
what I've done is I've actually done it
taken a version of the app that actually
uses that API this has got all the other
optimizations
I put in other than the choreographer
one because it just gets too horrible
and and you can see we're now getting
sustained performance it will never
overheat running this you can run this
for hours but we're about 15 frames a
second so you do give up a lot but if
you're if you absolutely need sustained
performance you can do that especially
you're doing because like Pro Audio
where you're really really worried about
glitching this actually does pretty well
so that's another option for you again
there's just it's just a new window
called this is the first actual call
through Jay and I that I've had to make
in all of this but as you can see yeah
you know you lose a lot of performance
so you have to you have to do you really
do have to make a call there okay we can
go back to the def now alright so there
are a lot of big choices to also make
when using the NDK do you use clang or
GCC do you use hard FP 64-bit you know
and I wasn't sure so I went and did more
benchmarking this was like a bear to
prepare for this thing um so let me
let's talk about performance if you look
at clang versus GCC in 32-bit mode it's
pretty much the same like it's it's
going to be at some times it's exactly
the same literally the Westone number is
identical between these two and they're
within a percent or so any and the other
two which is kind of what we expect and
if you actually run your code and run
benchmarks on and you see anything
different report it to our to our
compiler team there's actually a place
where you can file issues on our NDK
site let us know because our the team is
really really very very aggressive about
trying to make the compiler perform well
in 64-bit world it's actually also just
as close GCC sometimes outperforms
claying playing sometimes I'll performs
GCC but it is completely not noticeable
these are very competitive compilers
however we look at build times and this
is our zoo XI application and this is
kind of crazy cuz zoo she actually has
tons of assets that gets get built along
with it so and I didn't even pull them
out I simply just said alright I'm gonna
do this the dumb way I'm just gonna
build it from scratch have it build all
its assets go through all of its
resource stuff and it was 1.4 times the
speed of GCC and you can see why
internal Android team really wanted to
switch the clang because I don't know if
you have you've ever heard of the kind
of machines that it takes to actually
build Android the whole platform but
it's a lot of code so just if you can
improve your compile speed by 1.4 1.5
1.6 times that's a huge productivity
gain but also there's great sanitizers
that are built into into clang that the
team loves that actually the error
messages are kind of cool
although GCC is making massive massive
improvements there so I wouldn't want to
but this is still honestly is huge for
me now there's a big thing about hard f
p on arm7 how many people have actually
heard of this how many people actually
use it okay all I can say is every time
I've tried to use it I've run into bugs
it's made Mike I've run into things that
don't work it's a really really really
happy thing and are it's not used
internally by the platform at all and
because of that our tools team made the
decision to eliminate it so it is not an
NDK r12 basically if you're making a
whole bunch of calls to math libraries
you this is where you'll actually get
hit by this you could you know and
because what ends up what this is
actually doing it's not actually not
using hard with hardware floating-point
it simply has to do with the way
floating-point parameters are passed in
function calls so if you look at the
assembly that gets generated it's a
little it's a little more expensive but
the truth is if function call overhead
is your problem in your application in
your floating-point intensive
application you've probably done
something wrong you know like like
that's you know hopefully you aren't you
aren't actually making a call to do
something extremely extremely simple
that you could just inline and that's
and that's the hope and most of the
important math functions will get in
line
however we know there are some there is
some code is gonna lose performance
because of this but honestly once we
actually started looking at the problems
with bugs that people were having and
the fact that the platform team wasn't
using it and when the platform who
doesn't use something on Android
it generally doesn't get nearly as well
tested as it should be so it's really
really important for us to do that
that's what's so great about what we've
done with clang is it clang is actually
used you're getting the exact same tool
chain for almost the first time ever
that suggested the platform and and it's
being very very well maintained it's
also the same version of the tool chain
that we're using
like Google Data Centers if you know
what we had way we test stuff in data
centers as we roll it out to 10% of see
what's going on see if we hit any bugs
pull it back James you know do fixes and
so we actually get to take advantage of
all the stuff that they're doing to make
the data centers Rock so it's it's
actually really really good I think that
we're actually all kind of United around
making one compiler work really really
well and not that GTC is dropping away
if you do need it if your code doesn't
compile first of all let us know if you
if you have a problem compiling with
clang and we'll try to fix it and second
of all if if you are you know we are
going to keep GCC around for a while so
it's not going away we're just not
updating to new versions if you're
expecting to get GCC 5 you know four-pot
4.9 is pretty much it so one final thing
is if you really want high-performance
definitely look at doing a 64-bit ABI it
and and really you know I was actually
not a hundred percent sold on this
because you do end up using a little
more memory and that does of course mean
you're using a little more memory
bandwidth but the extra registers that
you get that the compiler can optimize
around for 64 bit completely makes up
for any memory inefficiencies it has
almost all of the time and we ran the we
ran these benchmarks a lot we print a
whole bunch of other benchmarks in
general if you have something that's
really really performance sensitive
consider doing a 64 it a bit ABI
consider at if it's not too big just
include it of course you can include
multiple abis
into multiple libraries into any android
apk or you can use multi apk to
distribute multiple different versions
of your app and and again it it just it
just works better and the best thing
about it is you all already get hard
floating-point when you're running in
our v8 finally x86 native the x86
emulation that is in a bunch of Android
devices that allow them to run hard arm
code is awesome the fact that it
performs this well on on admittedly
meaningless benchmarks is pretty good
but you can see like with whetstone you
know you can be giving up easily two
thirds of your performance by not having
an x86 build and so if you want to
target devices like the Nexus player and
like the zenfone
which are which are both pretty awesome
devices you know definitely consider
actually doing a native x86 version at
least test it because it really makes a
huge difference in terms of battery life
okay so the Android runtimes a lot of
things to make everything run fast on
Android LNM it uses actually ahead of
time compilation to bring the
performance of bytecode much closer to
native performance and on end of course
it uses a blend of Just In Time and
ahead of time compilation and and once
again
it deals much much better with garbage
and what we're seeing with art is that a
lot of times you don't actually need to
even go into native or into native land
again if you want if all you're
concerned about is performance art is
actually doing amazing amazing stuff and
you don't have to worry about all of
this kinds of thing so it's complicated
as I said before that means that it's a
little more expensive to actually do a J
and I call and ok and we have to do
synchronization and that's what's
causing us to actually have these delays
so what's next
basically here here are two links you
can go to one of them of course takes us
into our NDK page we actually have NDK
docks online which is amazing and we're
updating them we have people who are
actually assigned to this project to
make it awesome secondly we have great
in any case samples and we're putting
more online all the time so there's
anything you want that we don't have a
sample for an API that's not covered let
us know we'll build samples</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>