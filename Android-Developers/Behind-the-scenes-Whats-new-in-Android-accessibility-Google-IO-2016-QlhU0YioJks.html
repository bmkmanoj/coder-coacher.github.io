<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Behind the scenes: What’s new in Android accessibility - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="Behind the scenes: What’s new in Android accessibility - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Behind the scenes: What’s new in Android accessibility - Google I/O 2016</b></h2><h5 class="post__date">2016-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QlhU0YioJks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone and welcome to behind the
scenes what's new in Android
accessibility my name is Maya Benari I'm
a product manager on Android
accessibility and with me today are a
couple of people from the accessibility
team we're going to show you a couple of
new features come a couple new demos and
a unique look off behind the scene so
this is our agenda for today we'll give
a brief overview about accessibility
we'll talk about what's new in
accessibility in Android and we'll talk
about the gesture dispatch API and show
a couple of very cool demos also we'll
look through voice access and new
service new accessibility service and
then we'll get give a brief look of
behind the scene in UX research so let's
start what is accessibility
accessibility is about created is about
creating products that are usable by
everyone including people with
disabilities such as motor impairment or
visual impairment but I'd like to
rephrase this definition as slightly
different and I would say that
accessibility is about challenging the
assumptions we make about our users for
example can the user see the device or
distinguish between colors can he or she
touched the device or hear back the
sound that the device is producing and
can the user speak back to the device
now this is not a small number 20% of
the US population will have some sort of
a disability during the lifetime
according to the u s-- s census but it's
not only that challenging the assumption
get benefit all the users why because
some of the technologies that were
developed in the past such as speech
recognition or word prediction started
as techno
oh geez for user with accessibility
needs but this is not all we are all
accessibility users some time this is
called situational disability when we
drive our car we cannot look at our
phone when we are in a noisy environment
we cannot hear the sound of the device
and we cannot speak back to the device
and when we carry heavy bags from the
supermarket we cannot touch the device
so we want to design an inclusive
experience for all users regardless of
fraud restriction the user might have
now let me briefly mention some of
accessibility service and features
available on the platform today so first
what is an accessibility service this is
a long-running privilege service the
change the interaction model with the
device in one of two ways
one it can change the way the user
interacts with the device or it can
change the way that content is presented
to the user
now the first accessibility service we
have on the device today is talkback
talkback is targeted for people with
vision impairment or blind and it's
basically a screen reader now the user
can interact with the device using touch
gesture and content is presented and
content is spoken to the user through
text-to-speech the second accessibility
service is Braille back so can we switch
to the demo so belbek is similar to
talkback just in this case the user can
interact with something which is called
a Braille refreshable display the user
can type through these keys or interact
with this joystick and then the content
is presented using well in those Braille
cells with the dots raising and lowering
the next service
is switch access switch access is
targeted for motor inferred user who
have trouble interacting with the touch
display in this case we have something
which is called adaptive switch this
adaptive switch has two buttons and the
user can configure one button is next
and one button and select and using only
these two buttons the user can interact
with the device note that the switch can
be with more or less buttons can we
switch back to the slides and the lies
accessibility service is voice access in
voice access the user can prove perform
low level interaction on the device
using only his voice this is a new
service that we recently launched and
Patrick and Scott from the accessibility
group will song soon tell us more about
this one we also have couple of
accessibility features already baked
into the platform for example large text
magnification gestures color inversion
or color correction for people with
light sensitivity or who are color blind
- tracks test and caption support in the
next section we're going to dive into
the latest and greatest feature for
accessibility in Android and we and we
had a lot of very exciting things to
share with you the first one is vision
setting on the welcome screen this will
enable visual impaired users to
independently set up their device now
can we switch place through the dumb
of course okay so this is the main
screen the welcome screen on Android
then here on at the bottom we have
vision settings that flashes every ten
seconds you go so if I tap on that I'm
presented a couple of options
I have magnification gestures font size
display size talkback so for example I
can select magnification gestures and
then I can triple tap and I can increase
the size and magnify the UI now another
option that I want to highlight is
display size this is a new feature
launched in Android M so if this is the
regular screen size I can increase the
size of the overall UI to be bigger now
the nice thing about all those settings
that whatever settings I'm selecting
it's going to be reflected both through
all the welcome screen but also as the
user setting throughout the device back
to the slides
another cool feature is mono audio
support this is this one is intended for
people who have a hearing loss in one
ear and this enabled them to listen to
mono audio stream and we do that by
combining the left and right channel
into a single mono audio scream stream
and we also have couple of new features
in talkback including improved tutorial
clarity improve gesture detection to
work better across different devices in
hardware and also we added a new API for
accessibility service to tone turn
themselves off so if the user
accidentally turn on talkback in on the
welcome screen you can easily turn it
off now the next section is about the
gesture dispatch API and this is
something that I'm personally very
excited about it because now we will
allow app developer to build services
such as point scanning face tracker an
eye tracker so for the next one I'd like
to invite Anna and engineer on
accessibility team to demo the api's
thanks Maya I'm Anna and I'll tell you
about the new jester dispatch API that
lets accessibility services mimic touch
on the screen first of all what's a
touch well it has three parts placing a
finger on the screen drawing a path or
gesture and lifting the finger the new
API lets you as a developer specify that
middle portion the path taken by an
imaginary finger or fingers now why is
this important for accessibility let's
take a look at switch access an
accessibility service that does not
assume that you can touch the screen
rather it lets use a switch or a set of
switches to move highlight across
actionable views and select the
currently highlighted view now this
leaves off some functionality when you
don't want to interact with the entire
view but rather a small part of it the
example that I'll be using is Google
Maps specifically the maps area which
supports complex gestures things like
zooming in and out and having the
visible area so with this new gesture
dispatch API we can add points canny to
switch access and that lets us perform
these complex gestures so let's take a
look at how we can use one switch to
operate Google Maps
we'll start in the Maps app and you'll
notice here I have a switch already
connected to my device I'd love to get
walking directions to the three
buildings just west of here I don't
quite remember what they're called but
fortunately once we make a selection on
the map we can get directions that way
so here it seems like I don't see those
three buildings on the map so I have to
pan to get to see them I'll start point
scanning by pressing the switch and the
first thing I'll do now is select where
I want to perform that gesture first the
y-coordinate then the x-coordinate and
here I'll choose the swipe right
so now on the lower left we see the
three buildings that I'd like to walk to
I'm going to place up in there by
long-pressing so again I'll start by
choosing the location first starting
with the y-coordinate again and then the
x-coordinate and here I'll choose long
press from the Actions menu this gives
me a button at the bottom of the screen
asking whether I like walking directions
that's exactly what I want I'm going to
select that button so again I'll choose
the location first
and here I'll show you select
and here we are directions to the point
we just chose from the map using just
one switch I haven't liked back tamiya
now he'll tell you about another really
cool application of the gesture dispatch
API thank you Anna for the demo and the
next demo is an extremely exciting one
for a hair tracker using the dispatch
gesture API by a company called sesame
enable sesame enable is an Israeli
company that was co-founded by two
people
Oded who is a computer vision expert and
gira
who is in high current engineer who
became quadriplegic due to spinal cord
injury
now sesame enable utilizes the front
camera of the device to track head
moving hand movements and move a mouse
cursor now I'd like to invite bloddy the
head of development from sesame enable
onto the stage to demo the technology
hey buddy how are you doing ready for
this so the first thing that Vlade will
do is he will can we move to the demo
the first thing the bloody will do is
enable the service now this can be done
through voice but because of the
acoustic here vlogging will just tap on
the sesame enable notification now the
service will calibrate Vlade face after
it lock it down buddy can control a
mouse cursor using only his head now if
lodi would like to tap on something he
just twelve
and he can tap now once bloody is into
his email he can swipe up or down
think we're a little bit excited
shaking
it's okay
it's okay
would you like me to help you
do you want me to like to help you let's
write it out so the first thing that I'm
going to do and we didn't rehearse it so
I'm going to try to help here I'm going
to calibrate my face okay now I can't
control the mouse cursor it's a little
bit hard here because the the overall
stage is shaky
it is hard
yeah it's hard to to calibrate the
overall stage is shaky and it's hard to
do but this is what these demo is all
about so in general after we move the
mouse cursor we can also like lock the
sensor this is by looking right
yeah
yeah sure Trigon just go to candy crush
yeah sorry
so in general these memories involve a
mouse cursor using the device it's a
little bit hard to do it on stage
because they all should stage is a
little bit shaky that's right
and
so the thing is basically you can do
everything using a typewriter on the
device including tapping and touching
and swiping and also downloading any app
from the Play Store and basically
interacting with us app only using your
face
so if you only try it out in a little
bit of a less shaky stage ah you are
more than welcome to come out to access
an empathy sandbox to try it out and one
last note about this API that while this
API is a very powerful it doesn't
diminish the need for app developer to
make their app accessible such as adding
content labelling or increasing
stuttering its size because while the
API allows you to interact with
different elements on the screen it
doesn't know which element it interacts
with so following Android best practices
is still very important so thank you so
much buddy for your help and with that I
would like to hand it off to Patrick and
Scott to talk about voice access
all righty thanks Maya I'm Patrick Clary
product manager on accessibility at
Google and with me is Scott Newman
software engineer on accessibility at
Google and we're here very excited to
talk to you about a new accessibility
service for Android called voice access
and this is an accessibility service
that is meant for users with a motor
impairment that find it hard to use a
touchscreen with their hands but before
I want to talk about that I want to tell
you about one of our testers who's name
is Andy Andy is a 65 year old male with
a central tremor and for Andy he really
likes to be able to communicate with
friends and family and send them
pictures through email and messages and
updates and he generally does this at
home on his desktop PC utilizing an app
like Dragon NaturallySpeaking which
allows them to dictate by voice however
Andy would really like to be able to do
this son on the go from his mobile
device but due to his tremor
using a touchscreen is very problematic
now if we take a step back here we
realize that Andy's experience is not
that unique in fact there are millions
of people in the US alone that have some
form of motor impairment that impacts
how that you can use a touchscreen so
this can range from people with
essential tremor like Andy to people who
have Parkinson's amputees people with
arthritis even people with spinal cord
injuries just to name a few
now in addition to that there's many
more people who have what we call a
situational disability like Maya
mentioned before this could be a
temporary impairment that affects their
use of a touchscreen something like a
broken hand or wrist or maybe their
hands are occupied a common use case we
actually here is that someone might be
cooking and they their hands are dirty
they don't want use their hands and
they'd love to be able to still control
their device so this is really the
motivation we have for voice access and
our goal with voice access is to provide
someone this complete control of their
device through use of their voice alone
in essence we want to be able to allow
users to click by voice or to put in the
words in one of our testers use your
voice and you're able to access the
world so you might be asking how does an
accessibility service like voice access
differ from a voice assistant like
Google now or ok Google where you can
say this hot word and then perform a
search query which is more
conversational or you can perform a
voice action like you can set a reminder
you can create a calendar event an
accessibility service is different what
we're looking for is to empower people
to be able to use their device and have
full device control now the high-level
voice assistant is really lacking the
fine green commands needed to do this so
for example I mentioned Jeff likes to
send messages to friends and family so
from his mobile device if he wanted to
do this he might have to open the camera
app take a photo by pressing a button
then open an app like hangouts by
clicking on the icon scrolling and
swiping to the contact he wanted
clicking on attach the photo and then
tapping on the text box typing in text
and then clicking send so with voice
access our goal is for each of those
touch actions to enable a corresponding
voice command that allows users to chain
together a series of these commands to
perform a complex text task so just see
it in real life
I'll hand it over to Scott we'll give
you guys a demo thanks a lot Patrick so
my name is Scott Newman I'm a software
engineer on the accessibility team and
I'm excited to give you a demo of voice
access today before I go ahead and get
started I just wanted to mention that as
some of you may be aware speech
recognition demos can be fickle in front
of live audiences so if we run into any
issues just bear with me and we'll get
through them so with that let's go ahead
and get started so I remember that
Patrick sent me a message a little bit
earlier and I'd like to read it but
before I actually read the message I'm
gonna go into accessibility settings and
enable large text so it's a little
easier for those people and
back for the room to see what's on the
screen open settings scroll to the
bottom tap accessibility tap large text
go home go home let's try one more time
go home alright there we go
so that's voice access in action so what
actually happened right there
so you may notice that there's this
persistent blue button that appears on
the top right of the screen and so
that's a voice access activation button
so from any screen you just press that
button and then voice access starts
listening and then from there you can
issue commands to globally navigate the
device interact with individual elements
on the screen and basically control your
device entirely by voice so what I did
there was I opened the Settings app by
saying open settings
I said scroll to the bottom to actually
scroll the screen down as if I were
performing a traditional tap gesture
then I said tap accessibility to tap the
corresponding button on the screen that
was labeled with the text accessibility
and then I did the exact same thing to
press the switch label large text so
this actually uses the same
accessibility api's that underlie the
other accessibility services that we've
seen so far
like talkback or switch access so as
long as you follow accessibility
development best practices your app
should actually work with voice access
right out-of-the-box so with that let's
actually see the message that Patrick
sent me can we go back to the gas tanks
open hangouts
Patrick Clary stop voice access again so
I used voice access to open the hangouts
app and then when I said Patrick Larry
had actually found the clickable button
on the screen with text that was labeled
Patrick Larry and then clicked that
button on my behalf so one other thing
that you may be noticing is that there
are these numbers that are drawn on the
screen whenever voice access is actively
listening and so what that is is kind of
a safety fallback so if you want to
interact with something on the screen
that's clickable or scrollable and
there's no text associated with it let's
say a clickable image then you can
default to just saying the number and it
will click that element on your behalf
so with that I'm gonna respond to
Patrick's message and then send it type
the party is at 7:00 I actually noticed
that I made a mistake the party is I
think at 8:00 instead of 7:00 so let me
change that really quickly replace 7
with 8 so in addition to navigation
interacting with individual elements on
the screen we actually offer a full
suite of text editing commands and so
that was just one of very many that you
can use to edit text you can replace
about parts of text with other texts you
can copy you can move elements around so
just a really fast way to actually
interact with whatever is on the current
screen and the hangouts app actually
didn't need to do anything special to
work with voice access it just works
right out of the box so what I'm going
to do now is send the message you'll
notice that at the bottom right of the
screen is this green send button and
there's no text associated with it so in
order to click that button I'm going to
reactivate voice access and then I'm
going to say the number that's
associated with it that appears right
next to the button then it's gonna click
that button and send the message so
let's go ahead and do that
tap 11 stop voice access
undo stop voice access so that's voice
access for you so notice that you can
start voice access by pressing this blue
button actually if you want to do it
completely hands-free if you say the ok
Google command there's actually another
way that you can access it completely
hands-free by voice we offer a whole
host of activation methods so you can
choose what's most efficient for you
similarly you can stop voice access by
saying stop voice access or just by
touching the screen which is kind of a
fail-safe option to leave voice access
quickly so that's voice access in
practice can we go back to the deck
please Thanks so let's hear from some
actual voice access users about how it's
impacted their lives role of video
please
my name is Stephanie and I just finished
my master's in advertising and the c4 c5
split-level quadriplegic I have no
feeling from my collarbone down so it is
absolutely vital that I'm able to use my
voice
okay Google start voice access my name
is Jeff and I have a neurological
condition called the essential tremor it
is incredibly difficult to use my hands
and fingers it is very easy for me to
use my voice
okay Google start voice access after
using this product for probably about 10
seconds I think I'm falling in love with
it open camera use your voice and you're
able to access the world shutter
share hangouts
Astrid Weber
send I cannot tell you how excited I am
tuck this product open calendar new when
you don't have the ability to use your
fingers and hands family movie night
it's really all about voice
so that's voice access we're really
excited for you to try it out it's an
open beta right now so if you come over
to the access and empathy sandbox here
you can try it out yourself with that
I'm going to hand it over to Astrid and
Jin who are going to talk about how you
can use UX research to improve the
quality of your product with voice
access as a case study thanks a lot
thank you Scott
hi my name is Jen and this is Astrid we
are inclusive design and researchers in
Google and we wanted to share with you
today a little bit about how UX research
influenced and drove the design of voice
access but first what is UX research the
general definition is that it focuses on
understanding user behaviors needs
motivations through observation
techniques task analysis and other
methodologies the key here is that why
we might think we know what users wants
because maybe we are users of our own
product or maybe we are really close to
it and develop it and design it it's
critical to get out there and get
feedback from external users and
understand their unique perspectives
insights challenges and pain points in
fact the products that we demoed here
today all underwent many iterations of
design and development and all of which
were inspired and driven by UX research
the process itself is about a five step
process and it can be cyclical if you
want to keep refining the way it was
applied to voice access it was first
looking at what are the objectives of
the product at home one key objective of
a voice access was that they wanted to
ensure it was easy to use easy to learn
how to use right out of the box in a
hands-free manner and so from there they
developed hypotheses okay so based on
that objective
they thought that contextual help was
the best way to to allow people to learn
how to use the product in the moment
from there they define what the Beth
myths methodologies were to use
conducted the research synthesize those
findings and reported those back to the
team which further iterated on the
design of the product so now Ashley is
gonna take you through the specific
methodologies used and how those
insights actually impacted the design of
voice access Thank You Jen I master
drebber I'm a UX researcher working on
voice access and I'd like to show you
how we accompany the design and
development process with a few research
methodologies and I'd like to get
started with formative research
formative research is if you go into the
field in order to understand about your
users from the daily lives so you go and
see how their work how they live their
lives at home and we did that in
particular with users with severe motor
impairments as you've seen before with a
user no video what we learned out there
is that at the moment there is no
cohesive strategy for people who cannot
use their hands in order to access their
mobile devices completely hands-free
after we knew that we went back and
developed the first prototype of voice
access as soon as we had something that
was testable we did test it and we did
so with internal testing internal
testing is probably best known to tech
companies because you all you need is
yourself so all of us within the team
install the application people outside
the team and we try to use it in as many
circumstances and contexts as possible
what we found was a lot of bugs so we
dress those bugs we made the product
better we iterated it until we got to
the point that we felt the product is so
stable now that we can actually put it
in front of real users and that's what
we did
we ran usability studies usability
studies they are when you invite people
all track she come into your lab those
are people from the core audience so in
our case people with severe motor
impairments and they test it and you are
in front of them you can see exactly
what they're doing and they're running
through a couple of tasks with you and
you see the limitations of the product
you see where it's failing but you also
see where people are especially lighted
and what you see actually here in the
picture is one of our labs it's designed
to be very comfortable and welcoming to
our users and at the same time it has
cameras to capture audio and video so
afterwards we can actually go back into
the data analyze and see exactly how we
can improve the product usability
studies are great but they have their
limitations and I think the biggest
limitation is that it's a very short
period of time that your user is
interacting with a product and it's in a
lab environment so it's not the real
life so we went one step further and
conducted diary studies diary studies as
the name already indicates uh studies
where your user is going to keep a diary
about the usage so what did we do
we sent voice access home with the users
on their personal devices installed and
they could use it in whichever context I
wanted to the only thing they had to do
once you had two premises was that they
would report back about the experience
so you can do that over the phone you
can have them read emails or just fill
in a quick survey and in the end of the
study they would come back into the
office with us and give us a report how
they use the product what they liked
what they didn't like what we learned
from that process is that for one voice
access really needs to be polished and
be visually delightful you've seen in
the video the latest design that we have
we didn't start off on that I have a few
more examples on that in a minute for
you and the second thing if you use it
on the go you need a really easy way to
activate and deactivate it because
otherwise it's too much hassle for the
user to actually dive into the software
and then again when you don't want it to
list and deactivate it so let me show
you the
examples that I just mentioned the first
one that I would like to talk about is
how we improve the contextual help you
see on the left-hand side the before and
on the right-hand side you see the after
so what is that on the left-hand side we
see a long list of all the voice
commands that we are offering everyone
who's using or developing on voice based
software knows that the biggest
challenge is how can you actually teach
your user what they can say and how can
they remember that so it is essential
that we're offering a place where the
user can go back and check again what
they can say in order to get to a
specific result we thought it makes
sense to just offer one long list
everything in one place people will find
their way when we observed mmm up
usability studies we found that this is
actually not the case people get lost
along the long list they even forgot
what they were looking for because they
were so overwhelmed by all the stuff
that we offer in there and also the way
we presented the information without any
examples was not intuitive to understand
for them so what did we do if you see on
the right hand side the first screenshot
we based we have categories now so we
have basics of navigation gestures and
text editing and with those the users
deep dive into a smaller subset of
commands which you see then on the next
screen shot within basics of navigation
that the user actually gets specific
examples of the voice action that they
can take and by doing so we actually had
much better results that users were able
to find what they were saying and also
for remembering later on where to find
that command again the second example
I'd like to show you is our tutorial
again having a really good tutorial is
crucial especially if people have not
used voice interactions before in order
to navigate the interface we wanted to
make it as real as possible so we took a
screenshot of the interface and
explained it to the users what we didn't
expect was that users would want to
interact with this screenshot and
wouldn't distinguish between a
screenshot versus the real product so we
took that learning back
and we designed the experience as you
see it on the right-hand side to be text
explaining the functionality so this is
how to use the numbers what God had
demoed before and we did have numbers
now on the screen but those are real so
you can interact with them so the user
is learning about it immediately and
they can apply the learnings and with
that we actually made really good
progress and now Jen is going to talk
about usability testing and how you can
use it for your way on application
thanks Eric
so in the few minutes remaining we
wanted to leave you with a quick starter
guide of how to do your own usability
studies because we do find them very
effective and you don't need anything
fancy to actually execute these
usability testing is really useful to
understand how people use your product
and why and that's the key point is the
why before you jump in and just bring
somebody in and have them run through
some tests you want to have a plan and a
strategy first starting with what are
the key questions you want to answer
with this research write these down
review them with your team make sure
everyone is on the same page these are
your research questions and without
solid research questions the end result
might not meet everybody's expectations
so based on these questions then you'll
identify tasks that will help you answer
these questions so for example if a
question was our usual users able to on
board without any additional assistance
you would bring in the new user and have
them start the app for the first time
and maybe run through some of your key
fundamental tasks and see if they can do
it without asking for you assistance
from you looking in the help or online
once you're happy with your your study
and your tests questions and tasks you
want to think about who to bring in and
actually conduct the study on now you
can certainly start with friends and
family ideally those people will
actually be using your product or do use
it and we also highly recommend having a
diverse set of participants as much as
possible and you can actually reach out
to various organizations that lobby for
accessibility needs and they're more
than willing to
accommodate you and get some people to
help run their study on now when it
comes to actually running it and
figuring out where to run this you don't
need a fancy lab like we showed you can
just find a quiet comfortable space and
you want to make sure that there's room
in there for yourself the the
participant obviously ideally will have
somebody to help take notes and you'll
have some video recording equipment and
also it's great to have a developer
designer somebody that's very close to
the product there to observe as well and
there's been tons of books written on
how did you actually conduct usability
studies effectively it they all kind of
boil it down to these four key steps
asking the users to perform the tests
and then taking a step back and just
observing them seeing what are they
actually doing other than completing the
task are they hovering in interesting
spots are they using a keyboard what
keyboard shortcuts are they using and
that leads to the next point which is
digging into why I noticed you were
hovering over that button can you tell
me why what did you expect to happen or
after they complete a task what did that
meet your expectations and this is the
magic of usability studies being able to
actually understand the rationale and
the the meaning behind their behaviors
and then I think the champ most
challenging part of this especially if
you are close to the product is
remaining neutral you might have the
urge to once you kind of defend or
explain while we tried this but we
couldn't do that so we tried this this
is not the time to do that you're just
there to listen absorb the feedback and
move on oh and then practice definitely
practice at least once before bringing
in real users during this study you want
to ensure that you write down the most
interesting findings right away and then
review those findings with your team
after each participant this will greatly
help at the end just kind of summarizing
your findings you want to if you had the
ability to actually videotape it cut
video small video snippets to help
support your findings again not
everybody will have been there with you
observing the study so when you're
presenting your findings you can show
look this is how the users actually
interacted with the app
and then one of the most critical pieces
is it would be a real shame to do all of
this work and have it just sit in a dock
somewhere unused you want to have
somebody in charge ready to drive the
process to to address these issues you
know of course it doesn't have to happen
right then in there but you want to be
sure that somebody is taking the charge
to prioritize and figure out what are
bugs
what are feature requests and when they
will be addressed so we have on our on
the i/o app and our spaces a great link
to give you more details on how to
conduct usability studies if you're
interested and overall we hope that you
found that this brief overview of the
research methodologies used for voice
access inspiring and hopefully can bring
it back to your team and now back to
Patrick all right
thanks Jen thank you everyone for
attending so you know as you can see
it's very important for us to hear from
users with accessibility needs so that
we can learn how to improve the platform
and for all of our developers to take
into account Android development best
practices so you can all make apps that
are great for everyone who has different
accessibility needs so if you'd like to
try out voice access it's in beta now
you can sign up for the beta at G dot Co
slash voice access also listed on the
screen here some other talks were doing
that our accessibility related please
attend those they're all great come
visit our sandbox it's access and
empathy it's actually right over that
way you can go there to do a demo of
voice access you can go see the e demo
of the sesame head-tracking learn about
many more things and also attend our
code lab so thank you everyone
take care</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>