<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Evolution of ART - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="The Evolution of ART - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>The Evolution of ART - Google I/O 2016</b></h2><h5 class="post__date">2016-05-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fwMM6g7wpQ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right good afternoon everyone thank
you for coming to this talk you'll hear
about art together at one time and all
about all about new selling features
we've worked on for Android n I'm
Nicholas Jeffrey a software engineer in
the art team and I'll be presenting
along my colleagues Matthew chachi and
Kalin traveler so art is the software
layer between the operating system and
the applications it provides an
execution environment to run Android
applications that environment
essentially consists of two things it
executes text byte code that's the
internal Android format imagine
applications through a mix of
interpretation and computation it also
manages memory of this application
allocating it and reclaiming it what is
so long I used why should you guys care
about art well it turns out art is that
the forefront of the user experience art
needs to be fast so the applications can
run smoothly it needs to start
applications really quickly to give a
snap experience to the user it needs to
ensure the UI can render at least 60
frames per second to make the user
experience jank-free
it needs to be power efficient and not
do too much extra work besides executing
the application and needs to be savvy in
terms of memory use less memory being
used by the runtime the more
applications you can actually run so if
you remember dalvik which was the first
Android runtime released in Android
phones it was sort of efficient on some
on some of these metrics and sort of
okay on a few at the time memory
footprint was pronounced so we
constrained the compiler to not use much
memory and that explains not so great
performance
dalvik also had a relatively
unsophisticated garbage collector I'd
like to lead to long application pauses
leading to a janky experience so in 2014
we introduced art art shifted the
padding of doing interpretation and
just-in-time compilation at one time to
add time computation when the
application is being installed so when
the application is being installed art
will compile it directly to optimize
code that gave us a great boost on
performance an application startup art
did not need a just-in-time compiler to
be warmed up before it could execute
optimized applications were running out
of optimized code code directly and
because this code is directly loaded
from disk you don't need to pay the cost
of a JIT compilation code cache the
garbage collector has also been
completely revamped where we implemented
state-of-the-art
concurrent garbage collector algorithms
and we make sure that GC pauses were to
a minimal so active experience was
jank-free
since lollipop with most improving
performance so we have evolution on the
compiler lollipop when we shipped had a
quick when we should nollie pop it had a
quick compiler that was a fast Dex to
machine code compiler it was ported from
the dalvik JIT at a time at the time
were really eager to ship art given all
the benefits and we just were mostly
focused on shipping a well robot well
tested a robust compiler
however quick was not structurally meant
for more sophisticated optimizations
such as aligning and register location
so in marshmallow we introduce the
optimizing compiler that's a
state-of-the-art SSA form based compiler
infrastructure SSA in Kampala jargon
stands for static single assignment and
that's a well-known format for doing
optimizations in the compiler we
implement all sorts of optimizations
such as inlining bounce elimination
common subexpression elimination and so
on
we also included a linear scan register
allocator a state-of-the-art virtual
location technique for four compilers
and um we iterate on that compiler and
make more aggressive optimizations like
more aligning and lots more
lots more optimization this graph shows
the performance of marshmallow and M
preview normalized to performance in
lollipop its benchmarks one on Nexus 9
the tablet where we at a bit worried
lollipop and the first relief of art and
you can see we've causes the improvement
performance of a time we measured on
three different kinds of benchmarks
deltablue enrichers our well-known
object-oriented benchmark in the
arbitrated community that stresses how
you have the runtime implements calls
try stone is about how the run time and
the compiler generates integer
computations reversely bench test bench
and ribs are actually adaptations of of
Android applications perversity and
chess emulate dai of an actual game that
can get download from the Play Store and
reads emulate spreadsheet emulation so
we feel very pleased with the
improvements we made here ranging from 1
point to X - 5 XP Docs
and the performance boost isn't specific
to one architecture because most of
optimizations are CPU independent all
platforms benefit from them
so running the same benchmarks unarmed
study too will lead to the same
improvement trend across measure across
the board so that's great improving the
code generated by clean power is a great
win for performance faster frame
rendering and faster startup great but
I looks like you guys run into this so
you probably familiar with that dialogue
now it's pretty lucky right only twenty
one to go so that dialogue is what is
shown when you take a system update and
what happens there in case you don't
know is that we're redoing all the
optimizations we've done at install time
of an application why because when you
use all application art will optimize it
heavily against your platform and it
will put hard-coded dependencies in the
compiled code that will make your
applications run much faster but when
you get a system update all those
hard-coded dependencies become invalid
because you get a new system so we knew
to redo all the work when we shipped art
that was actually a trade-off we've made
because OTAs or system updates were
usually once a year so for once a year
update you'll get really better Android
experience
but Android ecosystem has evolved and
security being at the heart of Android
our security team worked hard on making
sure security fixes could be sent to
hundred phones as soon as possible so
when our lead security engineer and
director of Nexus products announced we
are now moving to monthly updates it was
sort of clear that this initial
trade-off we made will not work and our
team to adapt so we bought back a
just-in-time compiler in art so no more
optimizing app dialogue thank you
it turns out moving the dialogue is not
the only benefit of having a JIT we now
get much faster installs around 75%
faster and because they would he could
not know when it was compiling which
parts of the app is being executed which
it turns out that we were compiling all
the code of your apk and I just sort of
a waste of your storage if you're gonna
use just 5 or 10% of the code but
letting legit has some unknown like
clearly keeping on compiling all the
time like you start the app Egypt you
kill the app you start it up again
Egypt but has some implications on your
battery that aut compiled code didn't
have having a copulation code cache
could be wasteful if not managed
properly
so what we did in M is introduce a
hybrid just in time F time copulation
system that combines the benefits of
both worlds the idea is that
applications start running with a JIT
and when the phone is idle in charging I
will do profile guided optimizations and
ahead of time heavily optimize the parts
of the application the G has the JIT has
executed already so later in this talk
Macaulay clean will go into more detail
about how this hybrid äôt JIT profile
guided compilation works what I want to
focus now is how is the unknown with the
JIT so let's go back to the five metrics
I mentioned earlier in the talk for
runtime performance the jet is based on
the same ehe compiler that brings the
same high performance so we're covered
but the other metrics are actually down
to how we tune the JIT a couple of
things we made when starting this
project a color of design decisions we
made were based on those metrics like we
implemented a March faster interpreter
in n compared to the one in marshmallow
and ones like up to 3x faster then you
have to appear in marshmallow it's very
important to have a fast interpreter
because when you start your app you
don't have any ahead of time compiled
code it's the insurers gonna run and
later on the JIT second we do a JIT
compilation on a separate thread and not
on the application threads some
compilation can take very long and you
don't want to block the UI thread just
for doing compilation
for saving power what we did is most
what we do is mostly focusing on the
hottest methods of an application and
filing for memory footprint we
implemented a garbage collection
technique that ensures that only the
method is that indian matter over time
are kept so if a method is being
compiled and i'm not being used anymore
we'll remove it from memory
all right so let's focus on application
startup now I'm going to walk you
through some systrace that will explain
some of the implementation decisions
we've made if you don't know what this
race is it's a great tool for both app
developers and platform developers to
analyze what is happening on Android
system so bear with me
there's a lot of information on that
slide but we'll focus on the things that
matter for us so here's that way she
strikes looks after launching Gmail on a
device that had Gmail ahead of time
contolled so applications start up for a
user is actually when the first frame is
being rendered and this trace here is
someone's wall that's what you would get
on marshmallow and lollipop is that for
starting Gmail it would take half a
second for the first frame to be drawn
so our first implementation or alpha
implementation of the JIT we do the same
measurements and reserve results were
not great you can see now the startup is
around one second so we increase the
startup around 2x
and you can see that the jet thread here
executing is initially idle and that
becomes very busy so what's happening if
you take a closer look at what the
application is doing there's around 200
milliseconds for doing just the apk
extraction and doing apks fraction
extraction is a blocking information so
you need to do it before executing any
code similarly there's lots of things
happening after the AP construction that
don't have to do with your with a
zucchini application and that's
verification art needs to verify the Dex
code in order to run it and optimize it
so we'll fix this problem we'll start to
move
extraction and verification out of every
application startup and move it back to
when the application is actually being
installed so let's play the application
startup two times faster than our
initial and JIT implementation and quite
on par with compiled code so I've just
talked about application startup about
junk for janked for jank
we looked at the frame rate of scrolling
within the Google photos application and
systrace gives you this nice list of
spray of frames are being drawn a green
frame is when the UI I managed to render
it in time orange in red is where you're
dropping the frame and it hasn't been
rendered now John can be attributed to
many factors and artists it's best at
executing the code of the application
but if the application does too much on
the on the thread obviously we're gonna
miss a frame so you have for that
specific experiment we have around 4% of
frames that are being dropped
during our first chin bringing up we
made the same same experiment so the
application wasn't compiled and we're
just running with the JIT and the
results weren't great we're dropping
around 20% of frames if you take a
closer look at what the gene is actually
doing here you can see those long
compilation activities where echo power
is actually waiting for methods or for
requests to pop out methods those
methods haven't reached the hotness
threshold that we've set and the heart
that special is hired because we want to
save in battery but that doesn't save on
saving junk the UI thread you want the
code that the steps where is executing
to be hot as soon as possible
so the solution was to increase the
weight of UI thread requests for
computation so the methods it could it
runs would be compiled almost instantly
so if you've seen since to trace there's
no more long pauses of computations and
we only dropped around 4% of miss of
frames which was the og level we had
initially all right battery usage
we've measured the power usage of
starting Gmail pausing for 30 seconds
and then scroll around the emails and we
can see here that the JIT is being the
high cost application startup prepared
to air of time population but one
startup is done the scrolling actually
doesn't has has no difference between
aut and jet the reason for this for this
difference is that as the startup the
JIT is very busy compiling a lot of
methods and Gmail seems to be very
aggressive in executing code at startup
which is not necessarily the behavior of
all apps so we've done the same
experiment with other apps we've looked
at Chrome camera on photos common camera
are mostly based on native code so here
that doesn't really is not really
useful for all the things you mentioned
and the power usage is very similar
whether you're in a OD set a ot setup or
it should set up photos on the other
hand does have to have a code but
doesn't have the behavior that Gmail
heart and you can see that the
difference is again very little that was
battery let's discuss about the final
metric memory footprint so we looked at
the overall usage of our beta testers
within Google and we're quite happy to
find that the main memory use of the JIT
is failure isn't reasonable the maximum
we've seen on a heavily loaded system
that have lots of application being
executed was around 30 megabytes
system-wide but in average what we've
seen is that in some general 10
megabytes for individual applications
big server applications will thanks some
memory like 4 megabytes but in average
most applications have a reasonable Java
code size and the code cache is fairly
small around 300 kilobytes
so to wrap up on how it affects these
metrics when you well it's being
executed we've seen that performance and
jank are on par with the quality level
we had with art ahead of time population
and I've shown you that it does have a
relative impact on application startup
battery and memory footprint so now I'll
be handing it over to my colleague clean
and is going to explain to you how we
were covering from those small
regressions compared to äôt by doing a
long time profile gallon computation
Thank You Nicola hello everyone I'm
Colleen and I'm here today to give you
more details on a profile guided
compilation and this is a new
compilation strategy that we introduced
in M as my colleague Nicola mention is a
combination between GT and ahead-of-time
compilation and it's mainly based on the
observation that the percentage of the
applications code which is actually
worth optimizing is very small in
practice and focusing on the most
important part of the application drives
a lot of benefits for the overall system
performance and not only limited to the
trick happening the slight regression
that we'll have to enable his system so
the goal here was to have a full
five-star system and this is what
profile guided compilations help us
achieve so let's take a look a bit up on
the idea in a nutshell we want to
combine execution profiles which were
ahead of time compilation and that will
lead to a profile guided compilation
what that means is that during the
application execution report profile
information about how the app is
executed and we use that information to
drive offline optimizations at a time
when the device is charging and idling
so that they don't take resources out of
our users
so let's take a look in into more
details how this works how it affects
the life cycle of the application and
how it fits together in the jeat system
that my colleague talked about the first
time the application is executed the
runtime will interpret it the JIT system
will kick in and optimize the hot
methods and eventually disciple will
repeat in parallel with the
interpretation and if the JIT
compilation we record the profile
information and this profile information
gets dumped to disk at regular interval
of time the next time that we execute
the app the same process starts again
and the profile files will eventually be
expanded if new use cases based on the
how the user used the app but the later
point when they use the device is not in
use anymore it's idling and charging and
at the state that we called maintenance
mode we kick in the service and what his
service does it takes a look at the
application
it looks at the profiles and it tries to
optimize the application based on its
use the output of the service is
actually a compiled binary and this
compiled binary will replace the initial
application in the system so now the
next time the application is launched
after this service will execute it the
application will contain different codes
different states of the same code so it
you may have code which is interpreted
and eventually be cheated and you also
have code which is ahead of time
compiled if the user for example uses a
new part of the application that haven't
been explored before that part will be
interpreted in GTD and will generate a
new profile information and so the cycle
begins again so what's important here is
will improve the application performance
as the user executes ative new use cases
and we'll keep recompile it until we
discover all possible cases so let's
focus a bit on the profile collection
and how that impacts there our
application performance and other
factors
as I mentioned we do collect them in
parallel with the application
institution and what we focused on to
make sure that it has the minimal impact
on the performance then one factor that
we put a lot of attention into is to
have an efficient caching and I'll
totaling so that we minimize the write
operation to disk we also have a very
small file footprint and the amount of
data that we write to disk is actually
very very small another point which I
mentioned before is that they keep
expanding these profiles as the app
executes and obviously it depends on the
application
it depends on the use case our test
shows that the largest part of the data
is actually captured during the first
run
subsequent runs add the profile
information it obviously depends how the
user used the app but the largest chunk
of the information is actually captured
in the first execution and yet that
gives us important data to work on the
final point which is worth mentioning
here is that all the application all the
users get their own profiles and if that
in mind let's take a look on what
exactly we record in this profile
information
the first thing are the hot methods and
what constitutes a hot method it's a
metric which you have internal to your
runtime and factors that contribute to
it are for example number of invocations
or whether or not that method is
executed on the UI thread so that you
can speed up requests that will impact
the users directly we use this
information to drive offline
optimizations and to dedicate more time
to optimize those methods the second
data that we record are the classes
which impact the startup times how do we
know they do so it means that they are
loaded in the first few seconds after
the user launch data and my colleague
Matthew will go into more details on how
we use that we improve startup times
even more a final piece of information
that we record is whether or not the
application code is loaded in some other
apps
and that's very important to know
because it means that the application
behaves more like a shared library and
when it does so we'll use a different
completion strategy to optimize it so
let's focus now on the compilation
daemon on the service that actually does
the compilation and let's take a look on
what decision makes this is a service
which is started at boot time by the
system and is scheduled for daily daily
run its main job is to iterate to all
the apks install in the system and
figure out whether or not we need to
compile them and if we do need to
compile them what sort of strategy we
should use the service wakes up when the
device becomes idle in charging and the
main reason for that is that we don't
want to use user time when the device is
active and we don't want to waste
battery time so it delayed this until
the user the device is not in used
anymore when the daemon wakes up what it
does it iterates with the applications
and the first questions it asks is
whether or not the application code has
been used by some other apps that data
that I was telling you that we report in
the profile if that's the case then we
perform a full compilation to make sure
that all the users benefit of the
optimized code if it's not and this is
the case probably for the largest
percentage of the apps it's a regular
app you don't get used by some other
apps we go into and perform a much
deeper analysis on the profile
information if we have enough new data
if we collected enough information about
the application then we'll profile a
guide compile that application we'll
take a look at the profiles and you
optimize only the methods that were
executed so that we focus on what
actually the user used from that
application if by any chance we don't
have enough information let's say you
only know data we only have data about
one single method from the top then
we'll just keep it because probably is
not worth optimizing and an important
thing here is that we do perform the
profile analysis every time that we run
the demo and that what it means that we
might end up recompile the app again and
again
we no longer have any new information
about it and here you can see that
actually I was talking about different
use cases and how we apply different
completion strategies shared up shared
libraries get a full compilation whereas
the regular apps prefer a profile guided
compilation in em we generalized on that
and different use cases from the life
cycle of the application have different
compilation strategies for example it
install time we don't have profile
information yet we still want the
application to start as fast as possible
and as my colleague Nicola mentioned we
have a strategy where we extract and
verify that app with minimal running
time and which will ensure that the
application starts fast when you update
the app we had the same story we no
longer have a profile because what I
reported before was invalid
so we repeat this verification procedure
when the completion daemon kicks in
we'll do a profile guided compilation
where possible therefore and for system
and shared libraries we're going to do a
full compilation so that we make sure
that all their users are properly
optimized with that in mind let's take a
more closer look on what benefits are if
when we do profile guided compilation
and all the benefits shared the same
root cause we only optimize what is
being used and what it means when you
first start the app after the
compilation happened previous hot code
is already optimized we no longer have
to wait for the JIT for the matters to
become hot so the digit can compile them
so the applications will start faster
we have also less work for the JIT and
that means we use less CPU and we
increase the battery life overall and
because we are much more selective with
what we optimize we can dedicate and
spend more time there and apply more
optimizations
besides that we get a smaller size for
the compiled binary and that's a very
important thing because what we do if
this binary map with into memory smaller
size translates to reduced memory
footprint and the important difference
here is that for example this memory
that is not
into this binary that we map into memory
will be a clean memory compared dirty
memory digital generate we also have to
also use far less disk space because the
binaries are much smaller now and we
free a lot of space for our users how
much space let's take a look at the
numbers in this chart we compare
different applications which is google
plus play music and hangouts and it
tracked how the generated binary for the
compiled code performance across
marshmallow which is the blue line and
preview during the first boot which is
equivalent of fresh install of the
application which is the orange line and
the Green Line is how much it takes for
the profile guided compilation as you
can see the reduction is more than 50%
and obviously the green line will go up
over time and our tests show that
actually stays around 50% most of the
times and you may wonder how come we get
so so great reduction in terms of size
well when we analyze the profiles we've
realized that only 5 to around 5 4 to 5%
of the methods actually get compiled and
as you use the app more obviously this
percentage will go up will generate more
code but as I said in general will stay
below 50% or around that area now what a
natural question here is if i'm mollie
compiling 5% of job how can i reduce the
space only by 50%
why not 95 well those lines contains
also the application raw code that dex
code and that's a line below which you
cannot go because we need to run
something and here's how we compare to
the application size you can see that in
marshmallow we generated more than 3x in
terms of code size whereas in an we stay
below 1.5 X and these are all cool
benefits but it's not the only thing
actually that we use profiles for we
also use them to further speed
system updates as my colleague Nicola
mentioned because of duty don't need to
recompile the app again and that
basically gets rid of the long waiting
time for the optimizing app we still
want to do some processing of the app in
particular extraction and verification
to ensure that those apps get executed
as fast as possible and they are first
lunged and in M we actually know how
those apps were executed before so we
can use profile only guide to guide the
verification and that saves around 40%
of the time extra we also added new
improved usage stats and compared to M
we can now track precisely how the
application was used then how it was
executed and you only analyze what
actually matters for the users
what is that application which has a
user interface and the users can
interact with them those are the most
valuable for our users and we focus on
them during system update however when
you take the update first update to n we
don't have access to all the goodies we
don't have profiles and I don't have
improved enough accurate usage stats to
realize how the app was used and what we
do we do a full verification of most of
the apps this is still much much faster
than we used to do in M and how much
faster let's take a look at the numbers
you can see here three different lines
and these numbers are obtained on the
same device which have roughly 100
application installed and the first line
which represents an OTA a system update
from M to M Prime where we took a
security update we took around 14
minutes to process all the applications
and that's the time that the runtime
spends processing them when you take the
OTA to M we reduce the time to about 3
minutes which is branch up to 5x
reduction and there we verify most of
the applications now the next step a
security update will kick in for an and
what we do already have profile
information we have already had improve
usage stats and you can use that to
drive the time even lower
take a security update on this device we
take less than a minute and compared to
M that's more than 12 X improvement in
terms of speed up with that I like it to
pass it to Matthew and he will explain
how use profiles to further even to
speed up application even more Thank You
Colleen why new feature an end that
reduces application launch time as
application images an application image
is a serialized heap consisting of pre
initialized classes this image is loaded
by the application during launch during
wash most applications end up loading
many classes for initialization work
such as creating views or inflating
layouts
unfortunately loading classes is not
free it can consist of a large portion
of the application launch time the way
that application images reduces this
cost is by effectively shifting work
from application launch time to compile
time since the classes inside of the
application image are pre initialized
this means they're able to be accessed
right off the bat by the application
application images are generated during
the background compilation phase I think
was colleen referred to as a maintenance
phase by the IOT compiler leveraging the
JIT profiles the application images
include and serialized only the set of
classes that were used during prior and
washes of the application using the
profile is also key to having a small
application image since it only includes
a small fraction of the classes inside
of the actual application having a small
application image is important because
the application image is resident in RAM
for the entire lifetime of the
application as well as larger images
take longer to load as you can see here
application images have a very low
storage requirement this is mostly due
to the profile for the four apps here
the storage requirements were less than
two megabytes per app as a comparison I
put the application to compile the
application code with the profile so
this is already reduced compared to what
application code sizes would have been
on M the loading process and application
images begins with the application class
loader creation
when the application class loader is
created we load the application image
from storage and decompress it into RAM
for dynamically loaded Dex files we also
verify that the class letters supported
type since there is no dependency on
from the application compile code to the
image it means that we can reject the
image so the class loader is not
supported simply reject the image and
resume execution here are some results
for application image launch time
improvements for the four apps that I
have just displayed as you can see here
there's a round of 5 to 20% improvement
compared to profile guided compilation
only and now to the garbage collector
the garbage collector has not changed -
Martians for last IO presentation as you
can see here we were already in pretty
good shape back then there's still only
one short GC pause and the garbage
collector was high throughput since it
is generational there have been a few GC
changes mostly relaized application
images and class unloading however these
changes do not have our DS GC changes do
not have substantial performance impact
one thing that has improved each release
is allocation time with L we introduced
a new custom thread-local allocator that
reduced a lot of reduced allocation time
substantially by avoiding
synchronization costs in M we removed
all the compare and swap operations in
the allocation common case finally in n
the allocation common case has been
written and hand optimized assembly code
this is the largest speed up yet at over
200 percent start to M combined all
these improvements mean that allocations
are now around 10x faceted KitKat which
was dalvik class unloading also
introduced an N is a way to reduce ram
for modular applications basically what
this means is that classes and class
loaders can be freed by the GC when
there are no longer used in previous
Android versions these were held live
for the entire lifetime of the
application for class hoping to occur
all the classes in the class order must
no longer be reachable as well as the
class letter itself when the GC notices
this it freezes them as well as the
associate
this chart demonstrates just a bit of
how the class loader interacts other
components and retains them with that
off the nicolas for the wrap-up Thank
You Matthew
alright so to wrap up we've shown you
all the features we've worked on for n
mainly a fascicle pattern up to 5x
compared to previous releases a faster
intrapreneur up to 3x compared to
marshmallow a new JIT compiler and proof
our guide computation just remove that
optimizing Apsara log and provide
fastest or installs applications image
for faster startup and faster on
occasions you can actually if you're
interested in all that sort of low-level
stuff follow it on the AOSP website
where we actually do all development
with that we'll take your questions
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>