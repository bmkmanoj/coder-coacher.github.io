<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Best Practices for Android Audio (Google I/O '17) | Coder Coacher - Coaching Coders</title><meta content="Best Practices for Android Audio (Google I/O '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Best Practices for Android Audio (Google I/O '17)</b></h2><h5 class="post__date">2017-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C0BPXZIvG-Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone
I'm Kelly and I work with music and
audio app developers to help them launch
and build successful businesses on
Google Play we're really excited to be
here today to share with you both
technical and business best practices
for developers that require high quality
audio so our session is divided into
three parts I'm first going to share
with you tips from leading audio
developers who have successfully built
businesses on Android then Don is going
to join me on stage to share its common
technical hurdles that audio developers
face and ways to overcome them then Phil
will be announcing our latest a audio
framework that we think will help
developers reach your performance needs
then to wrap things up we'll have rollin
rollin CEO join us on stage for a really
cool live music demo so stick around so
I grew up in Los Angeles California in a
very musical family and I remember the
first time I stepped foot into a music
studio it was really exciting but I was
also a bit overwhelmed by the amount of
professional tools and expensive
equipment and also mentioned that
renting the music studio itself wasn't
exactly affordable as a teenager these
DJ turntables were my favorite piece of
equipment because I thought it was
pretty cool being able to mix my
favorite beats for my friends but as
cool as these turntables are having
access to expensive equipment or an
expensive studio isn't really feasible
for most people so luckily for the
billions of people around the world who
want to produce music music creation has
changed drastically
now with the accessibility of mobile you
can simply download an app and have
pro-lite quality music instrument and a
music studios directly in your pocket
making these DJ turntables accessible to
everyone on their phone and that's why
we've seen tremendous popularity in apps
like edy Jiang with over 45 million
installs but of course it's not just DJ
apps that have grown in popularity its
apps that allow you to sing karaoke such
as smil Singh
with over 52 million monthly active
users and apps like cord ke oscillators
that recently released on Android and
while high quality audio is important to
music creation apps it's also important
to a whole host of other apps such as VR
or voice reliant apps or games and the
market for high-quality audio apps is
only expanding so that's why I'd like to
now dive deeper into the first part of
our agenda which is sharing with you
three best practices from leading
audiences developers who have
successfully launched and built their
businesses on Google Android's these
messages are launch smart sync global
and everyone favorite make more money so
first launch smart so we know that
device fragmentation on Android can be a
major challenge for developers that
require a high performance audio in
order to deliver the best user
experience it's important to understand
what device work well for your
performance needs by performance here
I'm specifically talking about a
device's CPU and latency with the lower
the latency the better the user
experience so let's take a look now at
how joy Tunes and apps that requires low
latency approached launching on android
in order to maximize the reach but still
achieve a good user experience so joy
tune is the creator of simply piano a
subscription-based music app that helps
users learn how to play piano
in fact my daughter here has learned how
to play piano almost exclusively on this
app so we've had a piano in my household
her entire life and it wasn't until I
introduced her to this app did she think
that playing piano was all of a sudden
fun so what she does is she simply
follows along to the guided lessons on
our piano and the app immediately
recognizes what she's playing and then
provide feedback to her to help her
improve so the key here is that when my
daughter presses down on the piano keys
she expects that simply piano will
immediately and accurately hear what
she's playing and provide feedback but
in order to have this experience simply
piano requires low latency of playback
and recording of musical tracks
so to figure out what devices meet their
low latency and computational
performance needs they first launched a
beta version of their app in our early
access collection so early access is a
collection on Google Play for new
selected apps that are still in beta
inclusion in this collection enabled joy
Tunes to build a beta audience and to be
able to collect private user feedback
this enabled them to identify both
problematic devices where users were
having a poor quality audio experience
but also problematic regions that they
were then able to exclude from their
production version they also developed a
way to have their app work on lower-end
devices by making automatic adjustments
of certain features such as changing and
optimizing the graphics so that a user
on a particular device would still have
a good experience so with all of these
learnings joy tunes was able to then
launch to a public audience with a very
high user rating of over 4.3 and to also
launch to a larger number of devices
than they had originally thought
possible so second you want to thank
global when you think about launching
your app or growing your user base an
Android has tremendous strength here and
we continue to see enormous growth of
music creativity apps in both developed
and emerging countries when expanding to
developing regions so you want to
optimize your app for its specific needs
for users that are in that market for
instance you may find that there's a
higher number of users that are on
low-end devices and you may find that
more users have lower bandwidth
constraints but as mentioned with simply
piano you don't have to limit your app
to particular devices there's a number
of technical things that you can do to
optimize your app for both high and low
end devices so let's take a look now at
another leading audio developer that has
been distributed globally but it's seen
enormous success in emerging market thus
mule is a leading developer of mobile
music apps including small sing that
allows users to sing along to their
favorite songs in a karaoke cell fashion
so for instance if I wanted to sing or
if any of you wanted to sing
frozen's let it go the app would match
up what I am singing in real time with
the song and then I'd be able to overlay
audio graphics to make my humble voice
sound like a pop star if Disney
sing-alongs aren't popular in your
household they have a ton of other
genres too so small has seems phenomenal
growth but last year the apps all over
10x active user install growth in the
Southeast Asia region with over 40% of
their user base now comes from this
region with Indonesia being one of their
fastest growing countries but not only
have they seen enormous active user
install growth they've also been
monetizing well they're so with over 7x
increase in revenue over the same year
in the Southeast Asia region so in
addition to some of the technical
optimizations that they've done another
reason for this viral uptick can be
attributed to some Yool offering locally
relevant content for instance a user in
Indonesia pictured here on the left with
the headphones can sing along to one of
the world's top hits or she can choose
to sing in a duet style karaoke with one
of her favorite regional artists such as
PETA pictured on the right who's a very
popular Indonesian singer so as you
think about expanding your reach you
want to identify areas of growth and
then create a localized experience for
that market I also encourage you to
check out our building for a billion
guidelines online if you're interested
in more tips on building for emerging
markets lastly you want to ensure that
you're testing your monetization
strategy to achieve the best business
results so many music creation apps have
historically required that you just pay
a premium price in order to access their
app but because of the variety of
payments that are available on mobile
user consumption habits have changed and
in fact on play our fastest growing
business model it comes from
subscriptions where we've seen both
subscribers and revenue double over the
last year so while the last few apps
that I mentioned simply piano and Smule
allow users to download the app for free
and test it out and then sign up for a
subscription the developers of ultimate
guitar tabs' have tested an interesting
hybrid model so
Ultimate Guitar Tabs allows you to learn
how to play guitar through in-app
lessons or you can just jam to your
favorite song so ultimate guitar started
off as a premium app but rather than
charging a high price point they
experimented with a lower price point
for a paid app allowing users to
download the app for $2.99 as you can
see on the left
this essentially lowered the barrier to
entry then they up sold their users once
they were in the app allowing them to
download the full version for $9.99 as
you can see on the right so this hybrid
approach of using paid and in-app
purchases turned out to be an effective
monetization model for ultimate Guitar
Tabs they not only increase their
revenue overall but in app purchases now
account for 65% of their revenue which
is a pretty striking stat given they're
already a premium app so while a hybrid
approach worked for them I encourage you
to test out different monetization
models beyond premium and see where you
have the best conversion results so
these are just a few examples of audio
duct developers that are seeing
tremendous success on Android there is a
tremendous appetite for music creativity
apps and so we think if you follow the
business tips that I just gave and also
some of the technical tips that Don and
Phil will be sharing that there's a big
opportunity for developers in this space
I would now like to introduce Don to the
stage to discuss common technical
hurdles and how developers can overcome
them thank you hello hello
Thank You Kelly hi I'm dawn I'm a
developer advocate and I lead our
developer efforts for the Android high
performance audio framework what that
means is I help you guys create amazing
audio experiences on Android what it
actually means is I spend virtually all
my time listening to sine waves outside
of Google I do a bit of DJing and in my
head I like to DJ in places like this
but the reality is actually probably
closer to this more people definitely
definitely turned up later I promise
ok so today I'm going to give you two
best practices which you can use in your
apps to create amazing audio experiences
these are obtained low latency audio
paths and meet your audio deadlines so
you don't give your users headaches by
putting audio glitches into their ears
so so starting with obtain low latency
files I'm going to talk about two signal
paths through the group through the
Android system number one recording and
number two playback now one of the first
questions that I often get from
developers is what is the latency of
this path and this is the thinking
behind the audio dot Pro Hardware
feature flag and if the device reports
support for this lag it means that this
particular path is less than 20
milliseconds over the headset you can
use this flag in your app to either
enable certain features such as live
monitoring or you can only distribute to
these devices these Pro audio devices
and there's now tens of devices in
market which is supporting this
particular standard so it's not just
pixel and Nexus devices we're seeing
good uptake from OMS as well so audio
recording this is the past through the
Android audio framework when you're
recording you have an analog signal into
a microphone goes through an analog to
digital converter through some effects
to kind of clean up the signal this can
be things like noise cancellation and
code cancellation and then the digital
data is delivered to your app now effect
can add latency so if if we're talking
about low latency apps what we want is
the lowest possible latency and there is
a route through the system which allows
us to avoid adding this latency and this
is a plane using the voice recognition
preset the other thing we need to
remember here is to use PCM 16 format
and this essentially allows the audio
framework to not do any format shifting
which potentially could add latency so
that's all I'm going to say about audio
recording audio playback is a little bit
more complicated so every phone that
produces audio in the world has digital
to analog converter in it this takes one
the noughts and converts it into a
voltage which is used to drive
headphones or a speaker now I like to
think of this as kind of a character
which is chomping down on this audio
data and producing a signal if I even
have a name for him DAC man
now that man has very specific
requirements for his food he wants it
served to him at a certain rate and he
also wants it served to him in
bite-sized chunks of a very specific
size now for this analogy to work DAC
man also includes DMA controller and all
the other hardware required to consume
audio so just bear with it so this is
how DAC man fits in to the Android audio
architecture so your app is at the top
here and it's your job to get your audio
data to the the output as quickly as
possible the default path through the
system is to go through a resampler
through some effects again to improve
the acoustic quality of the signal and
then through a mixer and outs of that
man now as with the recording pass the
resampler and effects will add latency
and we can obtain a lower latency path
called the
fast mixer path if our app conforms to
certain requirements so number one we
need to obtain the correct sample rate
so remember I said DAC man once it's
food at a specific rate we can use the
audio manager API to find out exactly
what that rate is and that will enable
us to create an audio stream on this
fast path the other thing we need to
remember is not to add any effects so
once we've created this audio streams to
DAC man we need to start supplying audio
data and we need to do it in this
specific chunk size and again we can use
the audio manager API to obtain this
optimal size so after this first chunk
of audio data is consumed by DAC man he
sends us a call back he's basically
saying I've run our food feed me more
and we get this callback on a high
priority thread and this allows you to
do your audio processing work without
being preempted by other parts of the
system now this is a fairly critical
part of any audio app so let's take a
closer look at what happens inside this
callback so every callback have a
deadline remember that you have to send
these chunks of audio data a very
specific intervals so the amount of time
you spend in this callback is going to
vary based on the computations that
you're performing like the complexity of
the the audio data but also CPU
frequency and you know the device they
are running on if you miss this deadline
that man is going to be very unhappy and
he's going to output silence in protest
so it's very important that we don't
miss these deadlines so for the next
part I wanted to talk about some common
reasons why you might miss these audio
deadlines so starting with blocking so
inside your callback there are various
reasons you might block and here I have
a code sample which does a whole lot of
bad things things you shouldn't do in
your callback so
one logging instead of logging inside
the callback you should use a trace and
use systrace it's a much better tool for
debugging the callback don't do memory
allocation if you need to use memory
inside the callback which invariably you
do you should allocate the memory
upfront when you instantiate your audio
stream and then just use it inside here
rather than trying to allocate new
memory don't wait on other threads bear
in mind you this is a high priority
callback so if you're waiting on a lower
priority thread you have priority
inversion don't do file i/o if you need
to read from a file use another thread
and then use a non-blocking queue or a
circular buffer to transfer data into
the callback and don't sleep there
should never be any need to sleep inside
here so so we've dealt with blocking and
the next reason why you might miss your
deadlines is core migrations now when
you create an audio app on Android this
CPU scheduler will assign your audio
thread to a particular call and here
here I have a cyst race which is showing
the audio thread running on CPU 1 and we
have 4 callbacks marked in those green
rectangles there are other callbacks the
other row of interest is F ready one
here which shows us the state of our
audio buffer and we have 4 callbacks and
then the CPU scheduler shifts our thread
over C to CPU 0 now this core migration
can incur a slight time penalty in the
order of a few milliseconds and this can
cause I'll call back to the start late
and therefore run over and sure enough
we have an audio glitch occurring there
so the solution to this is to set thread
affinity which means that we can bind
our audio thread either to the current
core which were assigned that's like an
OK way of doing it or we can use get
exclusive cores on API 24 or above in
order to get the cords which are
reserved for the current foreground
application
right lastly CPU frequency scaling so
this is a process which is used to give
users great performance and great
battery it's like a power performance
trade-off CPU frequency is high when
users need good performance and low when
they don't need good performance but
they do want to conserve power so this
is great for most applications but for
real-time audio applications it can
cause a problem so imagine you have a
synthesizer app and every time you press
a key the synthesizer app generates the
voice this is how the computational
graph might look for an app like this so
we start off we have 10 fingers down on
our keyboard and at the app bandwidth
required is fairly high now we take our
fingers off that keyboard our bandwidth
required drops down and the CPU governor
sees that actually our app doesn't need
as much bandwidth so it drops down the
CPU frequency everything's fine so far
now we've put our fingers back on the
keyboard our bandwidth rises to its
previous level but the governor takes a
while to ramp the CPU frequency back up
to the level that we need so
unfortunately during this time glitches
are occurring so the solution to this
well this the title of this talk is best
practices for Android audio but for this
section alone let's just call this Don's
practices for Android audio and this is
from working with top partners like
Raleigh this is what actually works in
the real world so what you can do is you
can use something called stabilizing
load now the idea here is that instead
of having a varying a varying amount of
time spent in your callback you have a
fixed amount of time and this
stabilizing load can be things like
gating voices on and off or you can use
assembler no operation instructions
basically to keep the CPU spinning so
the result of that is that you basically
have fixed load fixed CPU frequency and
you always have the bandwidth you
require in order to generate audio data
this is best used with sustained
performance mode on API 24 as this will
avoid you running to thermal throttling
issues
so in summary obtain low latency audio
and always make your audio deadlines I'd
now like to hand over to Phil who's
going to talk about a fantastic new
audio API in Android welcome Phil ok ok
hello my name is Phil Burke and I work
in the Android Audio group mostly on
MIDI and pro audio applications my
background is in experimental music so
my personal goal is to make the Android
platform really a great platform for
making strange kinds of new musical
instruments so that's sort of what
motivates me the what I'll be talking
about is a new audio API called a audio
which very excited about and I'll show
you how to do callbacks using that API
and I'll also show you how to optimize
your latency on any particular device
you happen to be running so a audio is a
C API so this is a native API you may be
wondering why a new API we already have
open SLES and Java and the reason is
that a audio is we think easier to use
and if you've done used opensl es and
compare them I think you'll see why
also the it's a platform where we can
make improvements and this will show you
how we do that the these three api's can
all go through the existing audio
flinger framework but if we make radical
changes in the audio flinger we
potentially could break thousands of
apps that are already existing so what
we do is we add a new audio service
where we can do some pretty radical
things and not have to worry about
breaking existing stuff so we can do
some big performance enhancements in the
a audio service so a audio use the
concept of streams audio flowing for the
mic to the app back down to the
headphones so how do you create a stream
using a audio we use a builder design
pattern so in the Builder you can set
your parameters that you want you could
leave everything just the default and
you'll probably get a stereo output
string but if you need a specific sample
rate or a specific format you can set
that once the Builder setup you can use
it like a rubber stamp to create
multiple streams so this is what it
looks like in the code so we have a
Audio create screen builder pretty
straightforward and if you want your
here's how you set different parameters
on the stream builder once you've set up
the stream you call audio screen builder
open stream again pretty straightforward
and then if you didn't specify things
like the sample rate or the format then
you'll need to query it to find out what
you got don't just assume that it's 48
thousand Hertz because some devices
particularly like USB devices it might
be at 96 thousand Hertz or something so
it's important to query to find out what
you really got have to open the stream
another important value is this frames
per burst and this correlates with the
chunk sizes that DAC man was consuming
IND on slides so what is a burst
whereas what is a buffer is this can be
very confusing so we say buffer na audio
we're talking about the whole array
where the audio data is stored for a
particular stream and in that buffer
there can be multiple bursts so this
case stack man has two bursts that it
can consume and we're writing in the the
size of a burst you have to start your
screen you can pause it you can flush
the stream stop it these are
asynchronous calls and normally you
don't have to worry about that but if
you have to synchronize we do have a
function that will allow you to
synchronize with a state machine inside
a audio the reading and writing so we
have to get data in and out of the
stream
so there's two ways if if your
application doesn't need super low
latency the easiest thing is just to
read or write using blocking writes and
so here we're in a loop and we're doing
a write and you notice we have a timeout
there
as a last parameter when we do a
blocking right we'll get back either
error code or the number of frames
written and if it times out or if we use
a timeout of zero and it may not be we
may get a partial transfer
okay the second technique is when you
need the lowest latency and to do that
you need a high priority thread that may
be running with a sched FIFO scheduler
and hopefully in a higher priority as
well so the way to do that is to write
your own callback function so this is a
function that you would write and we a I
do will pass to you a stream parameter a
user data which could be an object or a
structure pointer and then audio data
pointer which is a pointer to your array
and the number of frames and then you
can render directly into that audio
buffer and then return once you have a
defined that your callback function and
you know what data you want to pass you
give it to the Builder you set the data
callback on the Builder and then when
you later create a stream it will use
those values sometimes people need to
combine multiple inputs maybe you're
taking two input sources and mixing them
and sending them to an output so how
best way to do that with a audio we
recommend using one stream as a master
and doing your callback from that master
stream which ideally should be an output
string and then what you do in the
callback see here we're being passed the
output stream corner so what we do in a
callback is we do a read from the input
stream
and we set the timeout to zero so this
is a non-blocking call as Don mentioned
you don't want to block inside the
callback now initially you may not get
all the data that you're expecting but
pretty soon these two streams will
synchronize like very quickly within a
couple buffer calls and then you'll have
nice back and forth between these two
streams and you can um do echo or guitar
effects things like that
the other topic I want to discuss is a
dynamic latency tuning so if you it's
very difficult to predict ahead of time
what the exact number of buffers that
you need and the number of buffers
determines your latency if you have too
few buffers the too few bursts I guess
in your buffer then if you if your
thread is preempted you may glitch so
what you want to do if you look at this
diagram from before right now we only
have two bursts that are valid in this
very large buffer so that's our latency
is two times this burst for this buffer
so if the if we are unable to write to
the buffer DAC Matt will run out of data
after two bursts so if we have a glitch
we may wish that we have three bursts in
the buffer so we have a little bit more
cushion if we get preempted of our
thread gets preempted so we can adjust
this value the way you do that in code
is that you can query to find out how
many overruns are under runs you've had
on that output string and if it's
changed since the last time you check
that means that you just had a glitch so
what you can then do is you can query to
see what the size is of the buffer how
much basically how much of the buffer is
being used which determines your legacy
and then bump it up and say well let's
just add one more burst in here so
instead of being double buffered I'll be
triple buffered and then you set that
back in reset your size so this is a
sort of a simplification you may find
that you want to you know if you could
do timing analysis and maybe lower the
latency again later if you haven't
glitched for a long time but that's up
to the application to do those those
kind of smart smart analysis but this is
the basic technique so in summary to a
minimal a audio program you create a
builder you open a stream you start the
stream and then in a loop this case
we're doing the blocking writes
synthesizing audio and writing it to the
stream and then we close it when we're
done so pretty simple just for
comparison
this is sort of an equivalent opensl es
program I probably little hard to read
but as you can see the a audio is a
fewer lines of codes will more
straightforward if you want to use audio
so now a audio you're probably thinking
that sounds great but it's only in the
OH release so how does that help me if
I'm writing for marshmallow or new god
or lollipop so what we're doing is we're
developing a wrapper which has is
basically like the a Audio API and a
audio features but it's in C++ so it
just looks slightly different and what
we do is we dynamically linked to the a
audio library using runtime linking so
your program can run and link on
previous versions of Android but the
audio won't be there and so then what we
do is we just dynamically switch over to
using opensl es so if you write your
program to this new API which will be
like an open source thing or it's not
quite out yet but we'll be out soon then
if you write your program to this C C++
wrapper then you'll be able to use a
audio or opensl es transparently and run
on old or new platforms okay I'm excited
about what's coming up next this is a
roll he's going to give us a demo and
Roley the company has been taking
advantage of a lot of these tricks
they've figured out a lot of stuff and
I've been a great partner for us so I'd
like the other CEO of rolling roll Alam
to come up and give us a talk about some
of the programs they've been developing
on Android thank you
good morning everybody
it's such a pleasure to be here as Phil
was saying I'm Roland Lam I'm the
founder and CEO of Rollie a company that
is developing new musical hardware and
software and I'm very pleased to have
Marko and Jack Parisi with me who are
virtuoso musicians who are kind of on
this cutting edge of new hardware new
software and expression so just to give
a little bit of background you know
starting out I felt really passionate
about creativity and about you know the
joy that comes from creation and in
particular we thought we want to empower
everyone to be creators but particularly
in music and the reason for music being
kind of the center point for us
is it there's this huge opportunity for
expression that is untapped and the way
we think about that is that musical
instruments are tremendously expressive
but they're still quite difficult to
learn and on the other side
electronic music has such versatility
associated with it but then it's
relatively technical still and
complicated to set up your own home
studio so we thought what if we could
create instruments that were deeply
expressive but also easy to learn had
the versatility of electronic music but
then didn't have all of the extra
technical setup but to solve that
problem we thought first of all we need
these high resolution control devices
for digital so you know we couldn't if
you just have like simple
one-dimensional electronic controllers
you can't get to that depth of
expression that you have with all of the
physical gestures you can create with
acoustic instruments so I invented this
instrument called the sea board and the
sea board is an evolution of the piano
keyboard you can play it just the way
that you play a piano but then you can
modulate all of the sounds in real time
using very intuitive gestures so as
marco will show you could you could play
the sea board first of all just like the
piano
I think maybe if the guys can bring up
the audio sounds like maybe they just
did Marco so I were running the Seaboard
now on pixel take
kind of like electronic piano patches
just playing like piano but if he wants
to he could play it for example like a
guitar and he would just you know be
able to bend these soft silicon keys
left to right as you'll see
so those kinds of ends that unit usually
would associate with another kind of
acoustic instrument you can create in
this context and there's many many sonic
possibilities with something like the C
board so we thought wow this is awesome
we have this new physical technology but
we want to make it as accessible as
possible you know to reach many many
more people around the world so we built
a new product called blocks and blocks
takes the technology of the C board and
puts it into a format of a small like
pocket-sized music controller and you
can you see you see it there you can use
it to just play beats or a play
expressive melodies and when we launch
blocks we initially launched on iOS but
the idea was always to make it you know
go far and wide and so the issue for us
was really about the latency all the
stuff that Don and Phil have been
talking about because to power these new
expressive instruments we developed a
professional-grade synthesizer called
equator and with equator you know you're
running many many different channels of
synthesis at the same time and you're
controlling them with all of these
different multi parameter gestures this
is it's a professional audio application
that's used in studios all around the
world so to run that on a phone you know
we had to do quite a bit of work but the
recent developments in the last few
versions of Android have made a big
difference and all of the you know stuff
that's just been discussed has actually
made it so now we can run all the sounds
in a crater on Android devices noise the
application is available in early access
in the Google Play Store and Marco and
Jack some of you may have noticed opened
up Google i/o two days ago with the
performance that was performed just on
four pixel phones so they're going to
just say a minute from that so it's
seaboard plus block plus four pixel
phones three instances of noise and
they're also using DJ pro 2 so let's
take a look
thank you so much so one of the reasons
why we were able to do this was that we
develop a coding framework called juice
juc II and it's a C++ cross-platform
framework it's built for audio and it's
really built for speed and so we've been
working not only you know using juice
for noise but we work with thousands of
developers around the world who are
creating audio applications that are
cross-platform
and what we're finding is with these
recent improvements in Android it's not
just you know for our applications but
for many of our developers they can take
applications that were audio
applications that were developed for iOS
for example and now port those over to
Android and we're also seeing this is an
interesting opportunity for a lot of
other developers out there who want to
create low latency audio applications
but don't necessarily have the resources
to learn all of the different systems
associated with a different platform so
that's something to check out if you're
interested at juice comm we also
organise something called ADC not the
ADC that Phil was talking about but it's
called an audio developer conference in
London which is on the 13th and 15th of
November which is deals with all of
these issues so check that out at juice
calm but just you know thank you for
tuning in and we thought we'd leave you
with one more little performance from
marco and jack
you
so Marco and Jack breezy everyone and
also check out check out their work
breezy is doing some amazing things and
releasing some great work so I believe
that's all for this session and there's
a sandbox that will follow so you know
come check it out and thank you all so
very much for coming today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>