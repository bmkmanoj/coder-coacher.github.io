<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Designing for driving - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="Designing for driving - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Designing for driving - Google I/O 2016</b></h2><h5 class="post__date">2016-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JiwiTDXTO78" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Craig I'm a researcher at
Google and I work for the Android auto
project and I'm gamin Bella I'm going
designer on the Android auto team how
are you guys doing and we are going to
talk about designing for driving so
Thurman will explain what that actually
means so what we want to do is we want
to give you a little bit of an insight
of how Android auto came about the
considerations that went into making it
and how we're supporting developers who
might not know even about automotive
design in bringing their Android up to
this new platform and then time
permitting we want to talk a little bit
about the future as well ok so Android
auto does everybody know what it is
anybody not know what it is ok I think
everybody knows but for those they don't
know it's an extension of Android
bringing Android into the car so you get
the apps the functionality the features
that you want from your phone but you
have it presented in a new manner that's
fit for the context of driving why are
we on automotive area as well if you
heard sundar this morning talk about the
whole larger platform of Android we're
really a big part of that android is all
about having many touch points into
people's lives across devices not just
phones not just tablets but also watches
cars and TV and for this is all for over
a billion of users and a whole ecosystem
on top of it that you developers help us
build as well and together we make it
really like a really big thing and as of
today as at i/o 2016 we're talking about
Android out of itself becoming a
platform that has two new form factors
in addition to what was already on the
market for about a year spanning a bite
a range of automotive types and actually
you know really becoming a system on
itself and the first one the first form
factor I want to talk about is the one
that we already saw in the photo which
is the by plugging your phone into a
comfortable car and then your phone
becomes project
directly onto the cast display and use
the cost control to interact with your
content and so this is powering the
phone is powering the entire experience
in this in this form factor and as of Io
2016 as of today Android auto is also
previewing a brand new form factor for
those cars that are not yet comfortable
with the plug-in version of Android so
we're extending it to these older cars
for example if you have a classic and
you want to put Bluetooth in this is a
great and your audio experience directly
on the phone screen so you don't need
anything else there and the third thing
we want to preview is really a concept
of Android as an embedded platform built
directly into the car this is you can
see this outside and in the red
maserati's there's a preview of that
concept there and it's that common
effectors can actually build their
native system on top of Android and so
you're going to get a great experience
directly built into the car and what we
are super stoked to see as as you know
having built the system is finally
realizing this bigger vision of Android
auto being running across all these
different platforms and translating the
UI depending on the screen form factor
depending on the input devices that you
have and I think this is something that
we are super excited about and Greg is
now shedding more light on why driving
is really so different than using
regular phone in regular context right
so when we go when we move into new
spaces it's really important to take the
context of the environment into account
when we look at driving today we see
that we're spending a lot of time inside
the car so from a fairly recent study
they found that the average person in
the u.s. spends around 45 minutes in
the car per day for those of us living
in the Bay Area it's actually probably
higher given the traffic volumes that we
have out here but either way that's
somewhere on the magnitude of around 280
hours per year inside the car it's a lot
of time and it's a lot it's a big piece
of our life and what we find is that
during that time people don't want to
necessarily stop their digital life so
they want to still be able to be
entertained they want to communicate
they want to be productive and if we
can't find out if we can't figure out
the right way to convey the information
then it leads to problems so as an
example texting we we all know that
texting isn't good a studies found that
or they noted at highway speeds when you
text and drive it's approximately
equivalent to driving the length of a
football field while not looking at the
roadway so it's not good and we know we
that we shouldn't do it yet we also know
that people are still doing so the
question is why well one reason is that
we tend to overestimate our driving
abilities so as drivers were generally
overconfident and we we tend to try and
we don't understand the true dangers so
basically we were overconfident with our
abilities and we also underestimate the
true dangers of driving this leads to a
pretty bad mix because when we want to
engage with when we want to be
productive or communicate or be
entertained it means that we we start to
we start to engage in with our phone or
do things that we shouldn't necessarily
do and the problem is when we're driving
if we screw up if you do something
stupid you don't just put yourself at
risk but you put all those the others in
the car with you at risk and you put
those around you risk as well so it's
really critical that we figure out how
can we re-examine the technology to
better suit the environment didn't you
in fact have a statistic well you said
that the
of drivers think they are better than
average drivers which is actually
mathematically impossible yeah yeah it
was somewhere around i think it was
ninety-three percent of those sampled
thought they were above average drivers
in the West and West was published
statistic expert that realized that it
wasn't possible so from this point is
really really clear that the phone
designing for the phone is not the same
thing as designing for driving really
two different things i think greg has
made abundantly clear with the three
points that you just made that it's a
very different situation and this
different environment need to be thought
of as such when we start about designing
apps and services for this environment
right and one thing to note is it's not
necessarily the phone's fault right if
you go into a car dealership today and
you look at the technology that's going
inside the car it's a lot of them
resemble tablets and phones right now so
it's not necessarily the technology but
again it's the way that the information
is constructed and ultimately conveyed
that's the critical elements and so far
android auto and this is what this talk
is about the experience needs to be
designed specifically for driving and
what this means we're going to start
with the next few slides so for one
thing one easy way that we can think of
this is that really drive driving and
keeping your eyes on the road is task
number one and dealing with the phone
should be tossed number two and this is
very different than then phones were we
design apps specifically for whole
engagement for full engagement and here
we actually want to do it differently
because we don't want you to spend this
much time with the eyes of the road
right and another way to think about
this is if you substitute primary task
for driving and your secondary task is a
distraction so all other activities
inside the car that don't pertain to
driving are in fact distracting and we
find that people engage in distract
activities quite a lot so there's a
recent study that looked at naturalistic
driving data and found that over fifty
percent of the time people engaged in
distracting activities these this can be
anything from texting so we talked about
44 but it can also be ed or conversing
with the passenger or even dancing in
your car seat so all of this if it
doesn't pertain the driving it has some
element of distraction for the driving
test when we look at distraction itself
we can break it down from the human
perspective we see that ultimately can
be thought of in three main fundamental
ways so we have visual distraction this
is of course when your eyes are off the
road looking at other things we can have
manual distraction this was here when
your hands are off the steering wheel or
off steering controls fidgeting with
other things like the climate control
then we can have cognitive distraction
this one's a little bit harder to assess
but it's basically when your mind is not
actively thinking about the driving task
and it's attending to other things in
the real world we we don't we rarely see
a task that takes just one form of
distraction most often tasks have some
element of all of these types of
distraction so if we use an example
something as simple as rolling up the
window in order to determine if you need
to roll up the window you might have to
glance at the window so there's some
visual you have to reach over to the
button of course so there's some manual
distraction and then you have to think
about maybe do I need to push or do I
need to pull on it so this is a really
simplified example and of course the
levels of distraction here aren't really
critical but it's a good representation
and it's a good it helps us to
understand when we're designing
experiences we need to figure out how
can we focus the experience how do the
experiences affect each of the different
channels of
fashion ultimately how can we minimize
the overall distraction and what we know
is that we can't fully eliminate
distractions always going to exist in
some form inside the car but what we can
do is we can mitigate it through
responsible design so then how do we
fundamentally rethink how we design the
UI for driving away from the interaction
model that we know from the handheld and
what I want to do is actually share a
few principles with you that have come
about as we did a lot of research and a
lot of design iterations and let's talk
about those so 11 simple principle that
is sort of one of the foundations for
Android auto is that really trying to
biasing things towards action what does
this mean so when you're on your phone
and you're trying to play music for
example you have a selection of multiple
apps you select your app then you have
maybe a grid of like different sisters
suggested things the thumb the song you
have might not be in there you scrolling
down a list maybe possibly another list
and that's a lot of noodling before you
actually get to play a song compare this
with the old-style autoradio if you
still remember those from way back when
we just press any of the buttons in
outcomes music it's really simple where
you put in a CD or cassette and just
works and this is sort of the gold
standard in some ways because you
there's very little load on you you just
started and it immediately is actionable
immediately set up for consumption and
then you can still change from there you
know the channel whatever but it's it's
gives you some instant gratification
right and then we also want to try and
reduce visual and cognitive load by
overall simplifying the interface making
layouts and content predictable so
making things consistent and then
flattening hierarchies as much as we can
and to make two again surface actions as
quickly as possible so this really
overall just means a reduction in
complexity and really focusing on core
activities that make sense to do while
driving and keep in mind for other act
these those can always be deferred to
pre or post drive states such as playing
with your playing with settings all of
this is done to keep interactions
efficient so that you can get the
content you want but then get back to
driving as quickly as possible and for
those other tasks where you're looking
for a very specific content voice it can
be a great shortcut they can be this
great deep dive so if you're looking for
that one song that you really want to
play you can simply make a voice command
and this allows you to re-route away
from the complexity of going through an
interface visually and just make a
simple voice command and get it so we
really want to enable voice in all of
our apps so the users can take advantage
of these shortcuts and the great thing
is about voice is it really doesn't
require any visual elements or manual
elements for distraction we're also
talking today if you go over to the
booth later on you'll see that we're
working on hot wording so that ok Google
will work in the car so you don't
actually have to press a button as well
to start your voice session now the
other side of this is though we can
alleviate visual and manual distraction
we do have to be aware that voice comes
at a cost through cognitive distraction
because every time you think about what
you're going to say every time you start
to speak and every time you listen it
takes some element of cognitive
processing so we want to carefully
design the voice voice interfaces and
the voice flows to try and mitigate
cognitive processing and overall
cognitive load so people ask us often
why don't you just use your phone people
are using it today right and the phone
had ten years and last ten years is
really evolved to almost like a
computing platform of smartphones and it
really hasn't had a long time to
optimize its UI but think about it when
you're driving your attention is
primarily on the road and your phone
will be an arm's length away from you
not close to you and then finally if you
ever driven on the American highways
around here you know there's a ton of
potholes and you know what that does to
your touch accuracy you know if you
tried this on the screen it's very very
tricky and then you throw in non-ideal
lighting conditions like today when the
car turns the light comes from this side
then from this side sometimes the phone
is full of glare and you can't really
see anything and you can't change any of
these things right and so it becomes
clear that we don't have a regular
handset situation here so first of all
the typography needs to be significantly
larger to see at a distance the touch
targets needs to be much more forgiving
and bigger and then finally the contrast
needs to be maximized to work in all the
lighting conditions including night
driving the good news is with every
building blocks that we're making for
the OS we're also keeping the developers
in mind as we're building this so you
don't have to necessarily spread all
these details you don't have to be car
and driving experts as you're building
an app for Android auto so a few
examples are on the left side we have
font styles that have been extensively
tested and optimized for driving use
case that developers can call upon we
have layouts and component sizes build
for automotive touch target sizes so all
our layouts are actually built on this
sort of metric that allows for these
large enough touch targets and then
finally we have this icon conscience
which are mechanism built into the
system where you can provide an icon as
an app developer that's one who one
color and then we switch the color
accordingly to what happens with the
background and this this happens
automatically you don't have to worry
about that yes so this is the whole
field of legibility in glanceable
environments is actually a super super
interesting area what you find is that
little tweaks and things like font
weight or font size or contrast can
actually have quite significant impact
on the overall readability and time it
takes to understand or read text and so
you know what we're talking about
driving we're not talking about a matter
of seconds we're really talking a matter
of milliseconds here and every little
bit counts so we're really interested in
this type of research mit age lab and
monotype have both been conducting
research in this area for quite some
time now and so we're collaborating with
them to conduct collaborative research
to again invest
these areas of font legibility dynamic
lighting environments font colors
contrasts font size all of these things
to better understand basically you know
how does this really affect legibility
in a driving context and how can we
further optimize our system to again
even if it's shaving milliseconds off
let's let's make those improvements and
make driving better one of the big
learnings that we had over time was that
we understood that we needed to make our
system throughout noble and familiar
throughout all its applications what
does this mean well remember how Greg
previously talked about cognitive load
and one of the biggest loads is actually
that what we refer to as mental loading
and unloading of an application model so
that basically refers to the user is
switching from one app to another they
need to learn and relearn a new model
sometimes they remember it of course
when they use it frequently but they're
still unlearning an unknown learning
going on and if you're driving the new
mental capacities already sort of shrunk
because your attention again is on the
road and so so it means that for us it
meant that we met we wanted this system
UI to organize in such a way that the
apps are all self similar there's
recognizable elements in all the apps
that even if you switch to another apps
it sort of works in the same way and
this is very different from phones
obviously where every app tries to
distinguish itself and Spotify is a
completely different UI then say google
play music but for Android auto we want
to keep this at a minimum so that people
don't have to relearn in the situation
of driving so let's look at some wire
frames here this this is a navigation
application on Android auto and the bio
frame is the bottom right this is the
phone application you can see the same
system takes place where there is these
recognizable elements and we're going to
go through in a second and this is an
audio music application again with very
very similar elements and in the next
slide we're going to talk a little bit
about these common elements that are
used throughout the system so we we
basically did
the elements in such a way that they
have common functionality no matter
which app you're in and these anchor
points are the menu on the left top
corner where you get access to
alternative content and compared to what
you're currently playing the microphone
on the right side which access is what
we call the demand space which is
basically you can query the system like
a google voice to either do actions or
find a specific piece of content then
the application has its own actions and
this is the white area and the whole
system has an activity strip at the
bottom where you can switch between the
core activities of driving which we
identified as navigation communication
audio and music and entertainment and
then cause specific activities now let's
look at the case of music applications
here so the first one we saw was google
play music and you can see sort of a
very familiar structure with a
play/pause button look forward and and
rebind and you have additional menu on
the right side in this here this other
screen is a different application this
is an audio app that happens to have a
30-second revine in 30-second forward
and even though they have slightly
different functionality and they express
their brand in a different color it's
still fundamentally the same structure
that the driver can benefit from even if
even if they've never seen is up before
they can reuse that and you know when
you at night driving at night has its
own specific challenges and we do a lot
of automatic switching of some of the
panels and things like that for them to
reduce the overall lighting intensity
but you can still recognize the exact
same structure and we actually giving
the application developers a way to
notify them then when they are in a
low-light situation so they can switch
to a different color palette this is one
of those things that we're doing to make
it easier application developer so they
don't have to redo everything themselves
and so today we're introducing this new
form factor directly on the phone screen
and unifying the story for the
developers in the overall experience
because what you can see here is these
exactly the same structure from a user
experience model even though the form
factor is very different and you may not
see as many buttons in the white area
because frankly the screen is smaller
and and you know is usually in a mount
but it's exactly the same structure
otherwise and this is what this new form
factor looks like in for example for
google play music during day time in the
next screen we're seeing the same
application that we looked at before the
audio app which is right on that next
screen and again you recognize the same
structure over and over again and
finally at night you can see this here
again very similar building principals
and even in different applications like
the phone application make your phone
call this looks immediately familiar
because it's the actions are groups
that's the most important actions are
directly front and center and you're
immediately in the experience rather
than having to navigate through a bunch
of menus to actually get to the
consumption experience and then finally
the navigation screen which is also part
and parcel of this experience here on
the smaller form factor and you know
this functions in exactly the same way
okay then we also want to try and be
naturally integrated into the car so
what does that mean exactly it means
that we want to use existing buttons for
common functions when we can so that's
things like controlling the volume
skipping track muting or parsing the
voice button so when we can use these
buttons that helps to provide a more
seamless experience to the user helps to
reduce learning costs and reduce
confusion so that when the driver steps
into the car he or she doesn't have to
guess or relearn they can just start
using it and it just works and more than
integrated we also have to adapt to the
different technologies inside the car so
if you go to the dealer today you'll
find that technology widely varies
across manufacturers makes and models
and we need to be able to provide a
solution for those so in the screen that
you see we need to have a solution not
only for touchscreen but we need a
solution for
or see if it switches there we go for
when it's down a touch screen so when
it's a rotary knob or maybe it's a
touchpad or some other interaction
method that hasn't yet been released so
this gets to be supremely challenging
and ultimately what we see is that this
field is only getting more complex you
have different screen sizes and those
screens are now you're seeing different
form factors so some are doing long
portrait some are stretching wide in the
landscape you have different control
types of course and the layouts for
where technology is going inside the car
has also is also changing so you're
seeing shifts of where information
conventionally used to be just in the
instrument cluster or just in the head
unit announcing migration of where
information is change or moving across
the inside of the car and basically they
just go back this is a lot of complexity
it's a huge amount of complexity but
it's really important that we pay
attention to this complexity so that use
developers and designers and makers
don't have to worry about it yeah I want
to iterate on that point I think that
what Greg just showed is that you know
you can go outside and look at some of
the cars and their differences in screen
sizes and all that but this you don't
have to worry about this because one of
the things that we're doing virtually is
translating the UI across these
different input and output devices and
you know of course it's designers we
could say well these guys are crazy why
are they doing this you know we want a
uniform UI and it should all be the same
right but when you look at cars they're
very expensive they're very emotional
person purchases people pay a lot of
money for this stuff and brand
differentiation is really that
competitive edge and you know they're
doing this because they want to leapfrog
each other they want to sort of say
we're this brand this is our
characteristics and our system basically
owes them to adapt and integrate well
with that system and be a natural
Potter's as greg was showing before like
it needs to feel like it's part of that
system and so the good news is that
Android auto really takes care of that
as far as absent services are concerned
so you didn't necessarily have to be an
automotive
expert and one of the other interesting
challenges is that as an ecosystem of
course we're in many countries in
Android auto rolls out to many countries
and there are many different locales
regulations but also driving habits you
know left side right side driving and
things like that that all have to be
taking account so when you look at this
overall complexity it's actually a
pretty multimedia mention ilim where on
one side you have an ecosystem of all
apps and services that all want to get
an android auto on the other side you
have many different cars and form
factors and in the middle you have many
different look cars where these products
are being used that's hard right but the
good news is again is like Android auto
put many many people hours into
engineering components that can be
reused across these different form
factors so hopefully as a makeup apps
and services you're shielded from that
kind of complexity and so you know this
is this speaks to that point that was
just making basically it's you know the
common platform is important because we
can't ask you to create an application
or a different application or design for
every different car type of there there
wouldn't be nearly impossible right and
so we're trying to stand help with the
standardization across these things and
so finally as designers of that system
we have a lot of responsibilities so
first to you guys the developers and
makers of things that the apps continue
to work on the system no matter what
future car con-ui configuration the
manufacturers will come up with and then
finally to the driver that they get to
use their stuff their content on all
these different cars and doing so in a
responsible manner and then finally to
the car manufacturer that they can and
they get an ecosystem of apps that they
can use to augment their car experience
right as your mom just mentioned we have
this responsibility of the driver and
part of that responsibility is
understanding that humans simply aren't
as capable inside the car when they're
driving we're just not as good at it
interacting with technology because we
have this competing safety critical
activity that we have to pay
engine too so we have to be very careful
and weigh things very carefully every
design decision against the implications
of the effects and the effects that it
might have on human performance and
there's this critical balance here
essentially one of the crux a--'s of
really developing experiences inside the
car is how can you deliver a positive
user experience while at the same time
making sure that we're not over taxing
the driver and that leads us to this
very typical process of research and
design so this is a pretty common
iterative cycle that you see in most
development cycles but I pointed out
because it's really absolutely critical
for our team to have this close
hand-in-hand process and we really
overly invest in research in there in
our research because distraction is such
a critical issue so we need this tightly
coupled process between the two because
it really helps to facilitate rapid
design and helps us to head in the right
direction early on rather than having to
course-correct later on when it's more
costly and that leads us more or less to
trying to test everything so a lot of
the principles and the patterns that you
see from android auto aren't a random
set of ideas and theories that we've
just arrived at their the result of a
highly iterative process that's
predominantly led by vast amounts of
user testing so we rigorously test again
and again as much as we can and we have
a lot of tools and methods that we use
both on the qualitative side and the
quantitative side but rather than while
on this slide I'll go to the next one so
we have this quick video which is from
natin low and whose is anybody familiar
with NAT and low so okay a few people so
NAT low is this YouTube channel it's a
couple Googlers that go around Google
and they film kind of behind-the-scenes
action of different projects and things
happening at GU
and then they posted on their their
YouTube channels so they recently
stopped by Android auto to check out our
research lab I'm not going to play the
whole clip but I recommend everybody go
and check that out but instead I have
some quick clip so that I'll talk over
so we can go ahead and play the video
and this is just to show some of the
different research activities that we do
so there's their startup and then
there's not a--not a it's really low and
not in that order okay so we have
driving simulators so this is one of our
driving simulators and basically what
you see is it say you know it's a
representation of a car inside of our
lab and allows us to put participants
inside a fake car but give them this
really immersive environment as they
drive and as they do this will give them
tasks in Android auto to perform and so
as they try and drive and complete tasks
we then look at their driving
performance and evaluate how much does
their driving performance to grade and
that's an indicator if we need to
redesign or not if something's overly
distracting beyond the simulator we also
have we look at I tracking as well so as
I mentioned for visual distraction is
really critical so here's one of our eye
trackers it of course tracks the eye and
it gives us a quantitative assessment of
natural glance behavior inside the car
so we use this again paired with the
simulator to have them engage with tasks
in Android auto as they have to also
drive and that helps us to determine how
people make glances and if that we find
that people have to make too long of
glances that's an indicator that it's
overly distracting and we need to go
back and redesign and we also look at
cognitive measurement this is a little
bit harder to assess but this is
detection response task DRT and
basically she's putting a micro switch
on her finger that's a little button
that she can click and then she also has
a Tector thats gets taped we tape it
around the neck area and that vibrates
on a random frequents a random interval
and once she feels the vibration she has
to acknowledge it with the button and
basically this is a really rudimentary
way to black box the brain and look at
the phase delay of cognitive processing
we can start to evaluate relative to
other tasks how distracting something is
from the cognitive aspect then we have
occlusion goggles these are these really
goofy looking goggles and they cycle
between fully opaque and fully
transparent and it's just a really quick
easier way to simulate a dual task
environment for evaluating whether a
task can be completed within a
reasonable time and whether or not a
task is Chung keable thanks big that was
really great we have a little bit time
left to talk about implications for the
future in the areas where interested in
in terms of future and also you can't
have a presentation about cars with our
talk about the future I guess we're
really at the beginning of this journey
where we're starting to understand that
bringing two industries together us sort
of from the internet and and and you
know develop a community and make off
apps and Google and on the other side
the automotive industry which has much
longer development cycles you know
between car cycles sometimes seven years
and more but we're really understanding
the thoroughness that they put into the
process is actually essential because
they're building huge machinery it's
actually a very non-trivial problem and
at the same time they're learning from
us as we are sort of doing going through
more rapid iteration two things and both
of these things coming together is is
starting to be a really interesting mix
and a new potential for developing
things that were in here yet before and
we are starting to see also that the
automotive you exclude we're building
we're starting to apply this to other
areas within Android because we're
having some very interesting learnings
one areas they were going to look at in
the in the near future is that we're
very interested in new types of
applications that haven't yet been
covered were you know there's innovative
areas like car health driving monitoring
new forms of car ownership you know or
even professional drivers that require
new types of apps so in this case for
example an EV manufacturer might be
really interested in talking about
how the batteries are doing and things
like that so we're starting to think
about apps in the car also as something
that goes over to Android as morphable
apps morphable systems that can take
different shapes and that's something
we're learning as we're building Android
auto that might apply to future in other
form factors different shapes depending
on what the context is and what your
cognitive contact your cognitive ability
is and so this is something that's
completely new and other things that i
love about designing for cars is that
it's really sort of a greenhouse at a
very defined environment everything has
its place it's very heavily constrained
and so we can we can start experimenting
and learning from things as people are
sitting in these positions there's
clearly a driver a passenger maybe
passengers in the back they all have
their roles and so we're starting to
learn how we organize the system
optimized for these specific roles and
we can do this in a car much better than
we can sort of do this in freefall
environment like the phone or the TV
another area that we're very interested
in is kind of thinking beyond just the
in cabin experience because when you
think about it as greg was mentioning
before there's really a pre drive
experience and there's a post drive
experience that's very important to us
that really makes it this the whole
holistic experience but also think of it
as a scale thing you know you have your
phone but the few phone is inside a car
which has multiple phones and then the
car drives inside an interconnected
system of cities and highways and each
presents a new connected orb ingenuity
innovated one example is that for
example in maps you see the green red
yellow traffic and this is something
we're sort of small units of information
get put together to form useful
information for all the drivers and we
believe that every major change in
transportation has fundamentally changed
the infrastructure you know trading
notions of scale the way we build in the
way we organize our cities and we
believe that this next thing the content
connected entities that cars are will
change that yet once again and cass is
interesting because when we look at ten
years ago cos we're really
super optimized they have the engine
optimizing all this other stuff but as
people are using phones in different
ways in the people using connectivity in
different ways they have as great
pointed out expectations of how they can
also use this in the car as they are
spending more and more time in the car
and so this is very clear that this is a
brand new platform and we think about
computing in a very different way and so
as as we think about this is become very
clear to ask that cause connected cars
do become a thing onto themselves you
know as in the past when mobile forms
can trust came out people try to
shoehorn the pc interfaces into mobile
by just shrinking it right what's a bad
idea so the phone has come a long way to
sort of develop its own interface and
real change will happen to the car you
is there not just smartphone interfaces
that are low bit blown up but they will
happen whereas cars become their own
connected thing in themselves and so we
almost at the end of this talk and if
you remember only two things from this
talk we want you to take away this first
thing is we really learned that we
needed a completely different model for
the UI to get designed for dr designed
for driving optimized and the second
thing is that android as an operating
system and especially specifically
android auto helps you with building the
sum of this expertise directly into the
building blocks you get to use for
making applications and this is a long
process because of all the safety and
driver distraction implication we have
two very thoroughly test all this stuff
but the future is very exciting where we
as an operating system also starting to
learn to sort of almost morph and adapt
the UI to new constraints and to future
form factors that we haven't even
thought about and believe me there's
some crazy stuff out there that we see
it auto shows with their screens like
you know hugely wide and and normal
computing environment normal adaptive
responsive design doesn't even follow
this and wouldn't have a response to
that so we're only at the beginning of
this evolution and we see this as a
starting point and we can wait to see
what together you and
us will build from this going forward
thank you very much for coming thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>