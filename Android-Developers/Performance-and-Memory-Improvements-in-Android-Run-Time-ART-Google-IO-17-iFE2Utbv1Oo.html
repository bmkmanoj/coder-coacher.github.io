<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance and Memory Improvements in Android Run Time (ART) (Google I/O '17) | Coder Coacher - Coaching Coders</title><meta content="Performance and Memory Improvements in Android Run Time (ART) (Google I/O '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance and Memory Improvements in Android Run Time (ART) (Google I/O '17)</b></h2><h5 class="post__date">2017-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iFE2Utbv1Oo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone I'm David Cyr I'm the
lead of the Mountain View Android
runtime team we're the folks that build
the code that runs on your phone and
loads your applications runs them and
manages their memory so so last year our
team presented the evolution of art and
we describe some of the techniques we're
using at the runtime and how you can
really see the goodness that we bring
this year so last year we talked about
the profile got it up compilation
how you can make your experience better
by understanding how programs run on
your phone we talked about how important
it is to keep memory usage down and to
make memory allocation blindingly fast
and we talked about just-in-time
compilation and how we can get great
performance from your applications
without the annoying optimizing dialogue
so you heard last year about how great
things are but it was just the beginning
now that your phone knows more about how
your applications execute we can focus
on loading only the parts that are
important to you so apps load faster and
use less memory and speaking of less
memory we've made your applications
spent less time reclaiming and even less
time allocating memory and of course
with a just-in-time compiler we can
focus on making your applications run
even faster
nicolas and art will be talking to you
in just a moment about that you'll
notice I mentioned using less memory
twice so let's start by talking about
that I'm going to begin by talking a
little bit about how we rearranged your
application to use less memory and then
matthew is going to tell tell you about
how we improve our heap sizes reduce the
jank reduce the pause times and have
even better allocation types first a
little review of how Android
applications work on your Android phone
an application comes in an apk file
which contains one or more Dex files
these contain the instructions for
executing your application art uses the
Dex file to get information about
instructions to run strings class
relationships and lots of other things
and the Dex files are loaded into memory
by art loading these files has both a
ram and a startup time cost so when you
run for instance naps one of our
favorite application your phone reads
the Dex files into memory and you can
see amounts of thousands of pages
multiple megabytes worth of information
are loaded into memory
now the Dex files were produced by a
developer and the typical developer
tools don't really know about the use
cases that are going to be important to
you on your phone so the Dex files may
have unimportant things on the same
pages as important things like for
example the way you use Maps you may
only use some of the methods that were
in the Dex files or some of the strings
that are there are not seen in your use
case as I told you before in Android an
art introduced profile based compilation
in Android oh we have clever new uses
for this several clever new uses in fact
but one of the ones I'm here to talk
about is improving the Dex file locality
the idea is simple use the gyp profile
information to move the important things
the things that have been used by you
and your usage of the applications
closer together and this is all done
seamlessly and transparently to the user
as we compile applications on the device
so after we run your application once at
least to collect the profile information
we use information about what parts of
the application we're important to you
we gather the important methods together
and leave the unimportant methods
together as you can see on the right
they're a lot fewer pages are accessed
especially the methods of when we use
profiling information now let's take a
look at what type of RAM improvements we
see after launching a few apps a few of
our favorites again as you can see
there's a significant reduction
typically around a third in these
applications
it's less member used by decks after
layout this Ram production also improved
launch time on devices that have slower
flash because you're pulling in fewer
pages and have spending less time paging
applications off of the flesh so that's
it for my partner over to Matthew who's
going to tell you about our new garbage
collector thank you David
another thing that saves RAM is the new
concurrent okay you could current a
coffee and garbage collector now Android
has had a compacting garbage collector
that runs for background apps since
Android L but unfortunately since this
collector is non concurrent I meant that
there was a GC pause for the entire
duration so basically the problem with
having a GC pause for the entire
duration of the GC is that it can last
hundreds of milliseconds and this would
very likely cause jank for foreground
applications in Android Oh art now could
currently compact the heap of background
and foreground applications this enables
compaction of many long-lived
applications such as the Android system
process and Google Play services since
these processes were long-lived
they tended to have a high fragmentation
over the time that they were executing
let's take a look at a GC process unlike
its predecessor the new GC is region
based starting out the GC does a brief
pause to identify which regions we are
going to evacuate these regions are also
known as the source region threads then
resume from the pause after having walk
their stacks next up is the largest
phase of the GC the copying phase in the
copying phase reachable objects are
copied from the source region the
destination regions finally in the
reclaim phase the GC frees the ram for
the source regions
starting with the pause phase the pauses
are pretty small during the pause one of
the key steps is identifying which
regions to evacuate the goal of
evacuation is to copy all of the
reachable objects out of regions with
high fragmentation after this is
accomplished UC can release the RAM for
these regions in this example the GC
picks the middle two regions as the
source regions because these both have
more than 20% fragmentation next up is
the copying phase in the copying phase
the GC copies all reachable objects from
resource regions to the destination
regions with the goal that no objects
will reference the source regions after
collection is completed the GC also
updates the references to these regions
to point to the new addresses of the
object now this application threads are
running concurrently during the garbage
collector the GC needs a way to make
sure that these stores don't end up
reading a field that points to a source
region to accomplish this the GC uses
technique called a read barrier a read
barrier is a small amount of work done
for every field read the read barrier
prevents the threads from ever seeing
references to the source regions by
intercepting the reads and then copying
the objects of destination regions in
this example there's a thread attempting
to read foo X this is a reference to a
bar object in the source region the read
barrier intercepts this read copies the
object the destination region and also
returns the new address copying process
continues copying moving objects as well
as doing repairs if necessary until
there are no longer any references to
the from reach to the source region at
this point the GC begins the reclaimed
phase as you can see here there are no
longer any references to the source
regions so the garbage collector can
free all the ram for these regions and
while we are left with is a heap that
has much less wasted RAM
- when the collection begins now you
might be wondering how much RAM can we
save by compacting foreground
applications as well as background
applications well the average heap size
is 32 percent smaller occured to the
Android n concurrent mark-sweep garbage
collector and RAM lost the GC overhead
is also a little bit smaller one
important factor about concurrent
garbage collectors is how long
application threads are suspended every
millisecond that the threads are paused
or suspended is one less millisecond to
prepare the next frame for the UI with
the new GC the average pause times 0.4
milliseconds compared to 2.5
milliseconds in Android N and a 99%
worst case for large heap is a 2.6
milliseconds instead of 11 milliseconds
in under 10 finally always compacting
the heap has enabled Hart to switch to a
new thread local bump pointer allocator
this is simple this alligator is simpler
and faster than the free list alligator
it replaces overall this means the
allocation is around 70% faster compared
to Android n and if you go further back
allocations are 18 times faster than
KitKat on a device adjusted basis and
now off the Nicolas for other
optimizations
Thank You Matthew so besides managing
application memory are is also
responsible for encoding executing
application code and for every hundred
release our team spent a significant
time optimizing wherever we can so in
this talk we want to share with you two
major accomplishments we've made for
this release the first is a performance
if city and the second is about you a
new optimization framework for loops let
me start with the performance city and
an r2 let me talk about loop
optimizations so to validate all the
optimizations we do we have our own
benchmarks where we look for regressions
and improvements but we also want to
make sure that optimizations do matter
for real Android apps so for the old
release we have invested our
optimizations on one of our Android
applications sheet over the years the
our team and the sheets team I've worked
on benchmarking the core computational
logic in the cheats app or sheets
benchmark suite is composed of three
kinds of benchmarks benchmarking
low-level runtime capabilities Corsi's
formal evaluation and benchmarks that
shuffle ran themselves and we're very
excited to share with you that in
android do we make significant
improvements to our runtime and compiler
up to the point that we are now running
eight of eight out of the nine
benchmarking sheets from two times -
three times faster compared to our
previous release Naga
you may remember a similar graph and
this one that we presently the keynote
years ago this is actually a snapshot of
our benchmark monitoring tool where we
track over time the impact of each evil
change we do in art on our benchmarks
for this graph where we see that the
sheets aggregate score keeps on
improving month after month between the
two Android and ad with all release so
let me pick al explain to you the major
improvements we made for o so one major
change is of course on your garbage
collector that Matthew just introduced
overall and improve the performance of
sheets by 40%
thanks to thread-local allocation buffer
and faster collections
the next major influence we made is our
in liner which gave us another 20% boost
and this was what this optimization was
only the easiest optimizations we've
made because we already had it in liner
in EM but what happened in the end is
that now we're doing all the computation
in the background and we only compiled
important things I mean we can avoid the
code bloating when we in line with those
two improvements implemented in the end
it was a lot easy for us to do a more
aggressive aligning for oh so for
example we now in line across textiles
we in line methods which could end up
throwing and we give a much larger
aligning budget so more methods get in
line now let's move to an optimization
that did require some work and helps us
increase the sheets code like 15% this
is called that optimization is called
code thinking it's an optimization that
it essentially moves instructions next
that to instructions that actually use
them so typically what you want to do is
the instructions are that are really
being used you want to move them closer
to where they are used so the
instructions are not in a regular flow
of your
want to move them to the intestinal
cases that's fairly abstract so let me
give you an example to make it more
concrete here's an accept of sheets code
there's a method called who ate range
takes two integers and returns a new
range the query range method shows a
helper method called check condition
that will check the required conditions
for creating a range if those conditions
are not met we'll throw an error
notice how the check condition is a very
first method and this is important to
notice because it will affect the code
that is being sent to our runtime here's
how so you start with a simple method
the Java compiler actually did sugars
the varargs call to allocating a new
object putting that allocating a new
array putting in an array boxed versions
of start and end and then passing this
array to check condition and this is an
Android specific this is what the Java
compiler does when compiled to bytecode
and these instructions are not
completely free yes we made allocations
extremely fast with the new garbage
collector but having to create this
array the boxed versions of storing and
all the time the method is executed is
not really ideal so here's how we avoid
executed completely first we can easily
inline the call to check condition so
int only this is how the code is
transformed in art
notice how arcs now is only used in the
in French so where the compiler can do
is move the allocation and the boxing
closer to where arcs is being used in
the exceptional flow so at the end of
this optimization what the compilers
made sure of is that only the require
instructions will be executed in the
normal flow the last optimization I want
to mention here is class your key
analysis class search analysis is a
pretty common technique in object
oriented language what a runtime will
try to infer classes or methods that can
be made final even though they aren't
markets final having this information
internally gives a lot of room for the
compiler for more optimizations for
example you can do more inlining and
because Java has dynamic clustering we
will bail out of this optimizations
if sudden you would suddenly a new a new
class gets loaded and all the
optimizations we did by inferring final
classes and methods become invalid
so this sums up the optimizations I will
instruct you about for the record we'll
also did a bunch of other improvements
for this release such as faster type
checks faster access is pop-out code for
class of strings faster gni transitions
more compiler instances and a lot more
well did not have the space to list here
so for the interest of time I'm not
going to go into details on these
optimizations instead I'm going to head
over to art we're going to tell you
about the whole new set of loop
optimizations Thank You Nicholas so if
you don't look at virtual method
overhead or general runtime overhead
programs tend to spend most of the time
in loops so besides all the good stuff
that Nicholas already talked about it
can really pay off by optimizing loops
specifically and I will discuss some of
these in this part optimizations really
always consist of an analysis part where
you look at the program and an
optimization part where you actually
perform the transformations and you can
see here like a list of all the work we
did for all but I'll only touch on a few
of those in the interest of time but
before I do that let's first look at
what loop optimizations can do for you
so here you see a graph that quotes
Android all versus Android end
performance so higher is better and the
color encoding shows you specifically
where loop optimizations in general the
blue stuff has helped and the red stuff
where factorization which I'll talk
about briefly has helped and you see
that benchmarks on the left like loop
and some really get unrealistic highs
speed ups and it is really as a result
of that we broke the benchmark so we
were able to transform loops into
closed-form expressions that execute
really fast and although that's always
nice to have the ability in your
compiler it's not very realistic but as
you go to the right of the graph you see
more realistic speed ups
like LUN Linpack obtained like 10%
improvements by factorizations so
central to all loop optimizations
is always induction variable
recognitions and that consists of
finding expressions that progress in a
regular and predictable way in your loop
and the most common example is a linear
induction so here in this example the
basic loop counter I is a linear
induction but also the expression J
shown in blue is a linear induction
every time around the iteration its
increments by a loop invariant constants
there's many more induction variables
and you see some of them depicted here
on the graph and detecting as many of
them as possible is always good for the
next phase the actual optimizations so
let's look at what loop optimizations
can benefit from induction variables so
here you see a somewhat synthetic
example where the compiler can easily
see that in the innermost loop the
increment to sum actually is very
predictable since the loop only iterates
hundred times it can actually replace
the sum hoisted out with a closed form
expression that just adds hundreds at a
time
after you've done that you can actually
hoist it again out of the next loop if
you take a little bit care of the fact
that the loop may not be taken when n is
negative so after that the whole
computation has been hoisted out of the
loop and the whole double loop can be
eliminated so in this case the whole
loop is replaced by closed form and
that's one of the examples that I showed
in the beginning very broke the
benchmark obviously your code you'll
probably not benefit this greatly from
optimizations but having this ability
can really kick in like after you inline
library code not to the same degree but
still nice to be able to to optimize
your induction variables
another example where induction
variables can help you is it bounced
check eliminations so when you access an
array you always need to test to see
whether the subscripts can go out of
bounds and if they do you throw an
exception but if you know the exact
range that induction variables will take
you can often eliminate those tests
completely so here you seen an example
that both the XS - a and B will never go
out of bounds so the compiler can
statically remove those tests and the
program as a result will execute a lot
faster induction variables also tell how
often looks iterate so if you know that
it's only a few times you can actually
completely unroll the loop and the
advantage of enrolling is that you
completely removed the loop control
overhead it reduces the code size
because you don't have to control to
iterate and it often enables like
constant folding so you see an example
here where the blue part shows that the
loop only iterate from 10 to 10 so it
actually only iterates once so you can
replace the whole loop with a single
statement and as a result you can also
constant fold to multiplication and
already do that at compile time so the
program will run a lot faster again
typical code Volt's optimized right away
but of the inlining or other forms of
specializations these situations are
cure and they can help improve your
performance so the last loop
optimization I want to talk about is the
ability of Android all to act to take
advantage of Cindy instructions so Cindy
instructions are instructions that
perform a single operation
simultaneously to multiple data operands
so you see an example here where one
instruction actually does four additions
at the same time and all our target
platforms are Intel mitts they have such
instructions and if you take advantage
of them it can greatly improve the
performance of certain classes of
programs
so converting sequential code and we
typically focus on loops but it doesn't
have to be restricted to that into code
it exploits to send the instructions
that process is called factorization and
it's illustrated here where on the left
you see a loop that iterates
sequentially and on the right you see
the vector loop that actually goes by
for and in order to do this
transformation this vectorization you
need somes very specific analysis you
have to detect and resolve memory
conflicts between the loop iterations to
see if you can actually execute them in
parallel you may want to be a little bit
more strict about alignments because
seen the instructions often require
stricter alignments on the memory and
you want to detect idiomatic construct
you'll see an example of that shortly
stuff that you cannot really express
with single operators in your sequential
codes but if you detect them you can map
them onto very efficient sim the
instructions so let's just explore this
whole factorization with a case study
suppose they keep on a like display a
string of pictures like these paintings
here on the graph on your display and
you want to transition smoothly from one
picture to the other you don't want to
just show one and on the other you want
to sort of have a transition in the tree
and you want to do that real time you
don't know the stream in advance and of
course you want to do it in an efficient
way so one way to get a smooth
transition is actually performing a
crossfade so what you do is it's called
rounding housing act so basically you
take the average of two pictures and you
showed it in between the two pictures so
the example here you see the shipwreck
on the bar on the top and the quiet fire
on the bottom and the picture that you
show in-between will sort of be an
average of the two pictures showing them
at the same time it forms a nice smooth
transition so how do you code it and you
don't want to go to a native solution
you want to don't want to use the NDK
you don't want to use GPU code you just
want to stay at source level
so here you see a method that could do
such a crossfade so you have two
incoming byte arrays which represented
pictures X 1 and X 2 and you go for like
an 0 extension and an averaging
operation and then you computes the X
out so keeping this at source level is
of course a much easier way of
expressing such an algorithm it's easy
to write as debug maintain etc you don't
have to use the the NDK you don't have
to use GPU counts so Android n will
actually translate the loop that I just
showed to the sequential code on the
left and Android all will now generate
the simply called shown on the right and
I don't expect you to dress this whole
assembly right away but just focus on
the parts that have been highlighted so
first of all in orange you see that the
sequential loop goes by one it does one
byte at a time before it goes to the
other one
in contrast the same decode goes by 16
it does 16 bytes at the same time the
other thing that you notice is that the
loop body is a lot shorter for the same
decode and that's a result of the yellow
highlighted instructions on the left
there's a lot of instructions required
to do the zero extension and the
rounding housing and on the right
there's a singles idiomatic instruction
that can takes care of that so all this
combined makes sure that the loop runs
about 10 times faster if you just look
at the loop however if you look at the
real application where this loop is
actually part of a larger thing where
there's a like a handler requesting new
pictures etc you can see that
vectorization made the difference
between rendering at a slow 20 frames
per seconds and the fact of the
factorization we vendor at 60 frames per
second and that's actually as fast as
you can go like you cannot render much
faster so that really shows that
factorization has given you the power to
render at a very acceptable
and you actually have CPU cycles to
spare because it doesn't need the full
power to go to the 60 frames per second
and all this was made possible by the
vectorization you don't need to write
like T DS NDK code anymore you can just
Express the loop at source platform</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>