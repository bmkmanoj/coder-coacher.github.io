<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introducing the Awareness API, an easy way to make your apps context aware - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="Introducing the Awareness API, an easy way to make your apps context aware - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introducing the Awareness API, an easy way to make your apps context aware - Google I/O 2016</b></h2><h5 class="post__date">2016-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/37ia7S4Lsv4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Bhavik Singh I'm a product
manager on the awareness API and my name
is Maurice true I'm the Anjali for
awareness and today we're going to be
introducing a new API that makes it
super easy to make your applications
more contacts to wear but first I want
to be on the same page about why we're
talking about this in the first place if
you think back to the keynote just a few
hours ago
sundar talked about how mobile has taken
off like a rocket ship and what this has
done is pretty fundamentally change the
way that we as users use our mobile
phones our wearables and even our IOT
embedded devices in the desktop world I
would open up a computer and have these
long prologue sessions with multiple
pages of content I would input my
intention into the computer with a
keyboard and a highly precise Mouse but
with devices that are much smaller and
that would come with us everywhere we go
we now glance at them in the bus quickly
to check the calendar while walking to a
meeting and maybe even to play a game
while bored at an i/o talk or something
like that so we as developers understand
these changes and we've actually made a
lot of you know updates to our
applications to accommodate them new
user interface patterns have emerged
like material design that are much
lighter and simpler than their desktop
counterparts we have new ways of
inputting intention into our devices
like voice stickers and emoji that allow
users to express themselves without the
need of a large physical keyboard but we
at Google believe that we can actually
do more today we're still treating these
phones like tiny computers in fact
they're quite different from computers
though because they're jam-packed with
sensors and with powerful algorithms
these sensors allow our phones to be
aware of the context around them and in
turn the phone
tell the applications where the user is
what they're doing and what's around
them you can use this information to
build more assistive and aware
applications that can help users in
their day-to-day lives so to dive a
little deeper I want to give you a few
examples of what a more aware and
assistive world would look like we all
have a morning routine and for me
personally it involves waking up to an
alarm and usually at this point I'm
incredibly groggy because I slept way
too late last night and I'm pissed off
with my phone because I don't have my
first meeting until four hours from now
so why am i awake your alarm can be more
aware of these signals it can understand
when you went to sleep and when your
first meeting is to adjust the time that
it wakes you up so at this point I'm
having a pretty decent morning and the
next big thing for me to do is to get
ready for the day so usually this
involves reaching from my phone to check
a weather app or go to some weather
service and now I've become distracted
by this swarm of notifications I have a
snapchat I need to look at and my days
become pretty distracted what if my
phone found a nearby TV through
chromecast and projected onto it the
day's weather so now as I walk from my
bed to my closet with one easy glance I
can see the day's weather and know that
I got to get a rain jacket finally at
this point I'm pretty relaxed I'm
sitting at my breakfast table having
cereal and while I'm doing that an
assistant application is looking at
driving and location patterns across
thousands of users and realize it that
there's a lot of traffic so it wakes up
a nearby speaker maybe my google home
device and says hey Bhavik you should
leave one in the next five minutes so
you're not late for your first meeting
this morning that is usually this
chaotic storm of applications and
services and questions is now this
elegant easy experience just because a
few applications were more aware let's
walk through another example we in the
Bay Area here love our parks and for me
personally I love running through Golden
Gate Park so very often on a weekend
morning I wake up I'll strap on my
and new Android wear device and head out
the door into the California Sun half an
hour into the run I'm like oh I
forgot to actually start tracking my run
what if your wearable was smarter and it
automatically detected that you were
running and launched the fitness app for
you it could then track your distance
your cadence and your heartrate so that
you can automatically keep track of
those fitness goals and finally achieve
those New Year's resolutions now music
is a big part of my run what if as soon
as I plugged in my headphones I got a
notification from my favorite music
application saying here is the best
sunny running playlist for today with
one tap I can listen to the right music
to keep my feet moving finally if you
guys know me at all
you'll figure out that I get distracted
very very easily so while I'm on my run
I just see this like cute little dog I
gotta take a photo so I reach for my
phone and the launcher application knows
that Bhavik often takes photos when he's
outside so it puts the camera app right
in the center of my screen and when I
take this photo it tags it with not only
the location but also the weather and
the activity so later on I can search
for photos that I took while running it
can find that tag and quickly show me
this dog again I'm happy now finally I
want to talk through one more example
driving is really really difficult to
multitask what if we lived in a world
where aware applications made it easier
so as soon as I got into the car and
started driving my Bluetooth turned on
and connected to the car speakers my
favorite navigation app opened up and
launched into driving mode now the user
performs one very simple action which is
starting to drive and his entire
situation scenario and applications are
set up perfect for that journey as he's
along his way I get a notification on my
phone that says hey you're actually near
a pharmacy and you need to pick up
medications be
sure to do this because the stores
actually still open and you're going to
drive by it these sorts of aware
experiences can really help your users
but also help you as developers tasks
like launching the Maps application
while the users driving or setting an
alarm are tasks that users perform every
single day they're highly critical for
them and if your applications are aware
you can streamline these critical tasks
in term your app becomes a part of their
habits which can increase your retention
suggesting the right playlist when
you're running or reminding a user to
buy medication is suggesting important
actions to your users ones that they
might even have forgotten and these
sorts of actions will mean that they're
more likely to click on your
notifications if those actions are
tailored which can increase your
click-through rate on the very very
important actions that you care about
finally when you tag a photo with
weather and make it searchable later or
you show me the rest of my day on a
nearby TV that's a moment of delight for
me as a user
it's something that makes me want to
rate your app five stars on the store or
tell my friends about it and these sort
of moments of delight can drive more
users to your application so whether
you're a music streaming service a
health and fitness tracker a local
recommendation app an alarm or driving
app or really any other sort of
application you can use the power of
aware experiences to help your users but
also hit your goals as developers now at
this point in the talk you guys must be
thinking well this is all really cool I
want to add it to my roadmap but like
this stuff is really difficult to build
that's where we come in we as a team
build Android awareness and location API
s and what we do is we bridge the
physical world where your users live
work and play and the digital world
where your applications and Android are
we focus on this so that you can focus
on building more aware and assistive
context applications
to dive deeper we think about the
signals that we provide in three main
buckets the first one is where you are
or location location is pretty
fundamental to the human experience
second only maybe the time and so we
fulfill this need with three api's the
first one is called fused location
provider and it provides highly accurate
latitude and longitude information by
combining signals across a variety of
sensors on top of fused location we have
a geofencing API that lets you specify a
latitude and longitude and a radius
which is called a fence when a user
walks into that fence your app can wake
up and suggest or perform an action if
you want to learn more about these api's
visit our talk at 6:00 p.m. today called
making android sensors and location work
for you don't worry if you didn't catch
that there's a link at the end with all
the talks that I mentioned on top of
that long we also have semantic location
what semantic location means is I never
say I'm at this latitude or longitude I
say hey I'm at the Starbucks or the
coffee shop and maybe your app wants to
get photos of a Starbucks or get the
open hours of someplace if you're
interested in that type of information
you should check out the places API and
learn more in our talk tomorrow
understand your place in the world we're
very happy that a lot of developers
enjoy using our location API s
and many of you in the crowd today might
have also been using them in your own
applications
within Google Google Maps is the obvious
example it uses not only a fused
location provider but also the places
API to help people find their place in
the world and navigate it the second big
bucket of signals that we think about is
what you're doing our phones today have
these tiny physical sensors that tell
the phone where it is with respect to
gravity how fast it's moving what it's
orientation is in the real world and we
build a layer of intelligence on top of
these sensors to provide you as a
developer semantic information about
what the users doing the first API for
this is called activity recognition and
it can tell you if the user
is running walking biking and we're
driving and we're excited to introduce
new types like push-ups sit-ups and
squats this year on top of activity
recognition we have a very powerful
fitness platform that allows application
to read and write fitness data so if
you're interested in data like nutrition
or how much of a person is run or what
their weight is be sure to go to our
talk tomorrow which talks about Android
wear and fitness finally underlying all
these api's is our core Android sensors
platform and it gives you access to this
raw data that lets you build powerful
games and activity recognition and
fitness experiences if you want to learn
more about the sensors API visit our
talk later today at 6 p.m. making
android sensors and location work for
you now a lot of applications thousands
of them are using activity recognition
but one of my favorites is google fit it
detects how much I've walked run or bike
everyday and combines that with more
advanced information so that I can keep
track of my fitness goals and get in
shape finally the last bucket of signals
that we think about is something that's
up-and-coming and incredibly exciting
and it's called what's around you in
today's world we have more and more
devices phones beacons TVs and they all
need to talk to each other the nearby
suite of api's helps you as a developer
do this
it's messages API allows you to send
messages between devices and it's
connections API allows you to maintain
persistent connections between devices
which is useful for things like
multiplayer gaming that team is also
very excited to announce a new API later
today at their talk nearby proximity
within and without apps so be sure to
check that out
chromecast is one of my favorite
applications that uses the nearby API
and they use it to power this amazing
feature called guest mode guest mode
means when you come to my house you can
actually use my chromecast device even
if you're not on the same Wi-Fi network
just because you're physically close to
it
that's awesome so that's it really we
have nine api's that tell you where you
are what you're doing and what's around
you now in building these api's and
building the products that use them
we've learned a lot about what signals
are important for people and where
problems can arise and so Maurice is
going to talk a little bit about those
problems and some solutions we have for
you great thanks Bobby
all right so we have nine api's these
help you sense basically where the user
is what they're doing and what's around
them and in my opinion kind of the real
power of these signals is not what you
can do with them individually but when
you combine them together and that we
can get kind of a holistic view about
what the users context really is now we
actually went back and tried to put
these together and we did run into some
issues and what we saw is that these a
guy is really looks something more like
this there are individual puzzle pieces
that don't quite fit together well
together so let me explain exactly what
I mean by how they're not quite fitting
together well all right so let's go back
to the example of getting a reminder
when you're driving near a store now
they implement this you know we have
basically a geofence API and that allows
you to set a region around the store
detect that the user is in there and
then we also have the activity
recognition API that can detect whether
the user is actually in a vehicle or not
now it's easy enough to just you know
invoke both of these API is call into it
get your callbacks and then try to
combine those signals together now at
this point we don't haven't given you
any tools or utilities to actually put
these signals together so you're kind of
on your own now I say okay that's not
too big of a deal right just have a
couple of callbacks put them together no
big deal but the big issue here actually
is system health and what do I mean by
system health so system health is
everything about how well the phone is
functioning right and the two major
factors for mobile devices is the
battery in the RAM now battery is kind
of obvious right if you start using too
much of the battery then it will drain
and at that point you have a phone that
doesn't work which is not good for the
user and the second was a little bit
more subtle in terms of RAM usage if
there's too many things running on the
right this can cause some cpu thrashing
and the issue with that is that then the
phone starts to get sluggish and that
also leads to a pretty poor user
experience okay so why should you care
about this now the thing is if the user
suspects that is your app causing the
battery drain order app causing that
phone to become sluggish you know they
may actually in the worst case uninstall
your app and actually that that's
probably the worst case scenario here
and that kind of the irony of this
situation is that you've been trying
your best to actually target those that
very specific situation when the user is
driving near the store and hence you
know make it very relevant to them but
if you don't do that right then you may
end up actually causing a worse user
experience alright so let me dig a
little bit deeper about system health so
in this case like I said probably the
first way you could implement this is to
call into the geofence API and the
activity recognition API get the
callbacks and combine them together
sure no problem but now that you have
two signals you're hooking it to you
actually have some other options to do
things a little bit in a more optimized
way so another way to do it is instead
just turn on the geofence API first make
sure that the user has actually is near
the store and in that case and only in
that case do you turn on the activity
recognition to determine that the user
is in the vehicle and of course there's
an alternative you can do the opposite
direction instead caunty the activity
recognition API first and then when you
detect that the user is driving then
come to the geofence API now the big
question is which is the best for system
health you know which is the ones going
to drain the battery the least which was
going to cause the least amount of CPU
thrashing all right so it's actually a
trick question right there's actually
many many factors that really go into
determining which which is going to be
better than the other things like the
sampling rate that you choose for each
of these and then there's also lots of
implementation details that you may not
be aware of things like how many times
the radio wakes up and how much is times
the CPU is using and there's also all
they down to the hardware how much power
that the sensor is drawing alright so
anyways if you have to you might say
okay that's not too bad and you can do
some work to tune it and then make sure
that system health is good but really
the power is again combining all these
signals so imagine scaling up to all of
these
now at this point you're actually facing
a pretty tough problem it's a lot more
code to handle and if you're really
going to try to optimize system health I
mean I are talking exponentially more
different combinations you have to
consider in order to do this well the
other issue is that the more API is that
you hook into that means the more your
app will actually wake up and this
actually causes some pretty bad memory
pressure and in the end could actually
end up causing a sluggish phone and
again we don't really want that so the
issues today again are in order for you
to hook into these API is you do have to
learn multiple of them for each type of
context signal and the issue is that
there are subtleties you have to learn
about things like how do you choose the
sampling rate and other things called
like priority level and these are pretty
subtle things to you to figure out how
to use properly the other thing is now
there's no support for combining these
signals together so you have to write
that code to do that and the furthermore
even after you've done all that you may
end up with battery drain and
sluggishness that can be pretty
difficult to solve so our challenge was
to figure out can we make these
individual puzzle pieces fit together
into a whole puzzle and our goals here
will say is it possible to arrange our
API so that we can make it very easy to
combine them together and so that you
can really target those specific
situations at the same time these issues
with system health you know is there
something that we can do to help you
guys all do that well I'm very excited
today to announce that we have a
solution for this problem and we call it
the awareness API thank you
all right so the awareness API it's a
unified sensing platform enabling apps
to be aware of all aspects of a user's
context while managing the system health
for you we've designed this so that you
can engage your users in very targeted
very specific contextual conditions now
it will be available shortly after i/o
as a Google Play services API now for
now let me give you a preview about what
we have to offer so far our first
release will be offering seven different
context types right off the bat so this
will help you answer questions like
where is the user via the lat/long
locations as well as the semantic
notions of locations that we call places
also what's around you things like can
we detect nearby beacons around you in
order just so you have an idea what's
there we also have some some code to
help you help basically combine time
with some of these other other
conditions as well and also the answered
questions like what are what is the user
doing via the activities and we also
found that there's some interesting
device States things like whether the
headphones are plugged in which have
some some notion about you know how the
user is using that device and finally
things like ambient conditions in the
environment things like weather which
actually do have a effect on users
behavior like today being extremely hot
and muggy outside all right so now the
biggest challenge that we have faced was
how do we simplify these 9 API into
something that's much easier to use and
also allows you to combine the signals
together and the way we approach this
problem was to consider well why don't
we think about the common usage patterns
of how app developers would want to use
these API s right what are the ways you
want to actually use and access these
context signals and we came out with two
of them that covers a good broad range
of these so the first one is called a
fence API and this is a callback style
API the idea is that you register
listener with the specific set of
conditions that you want and then that
gets called back and then you can react
accordingly
now the word fence may seem a little bit
mysterious but it comes from geo fencing
where the idea there was to set up a geo
fence and then the software will then
detect whether the user is in there now
we realize this is actually a
generalizable concept we don't have to
be doing just fences in location but
also
all types of users state things like is
the user walking or not are the
headphones plugged in is it hot and
sunny outside so all of these we can
consider a fence and that's what the
fence API gives you all right so let me
give you a concrete example of how we
can use this fence API to accomplish one
of the scenarios that we talked about
earlier now this one had two parts to it
the first was the user gets in the car
and then now your device just goes into
amazingly into this in car mode and the
second half of it was that you're
driving nearby the pharmacy you actually
get a call back to go and remind get a
reminder to pick up medication alright
so with the final first fence API the
first thing to do is to determine the
condition that you want to detect so
that first one was to detect whether the
user has started driving now in this
case is very easy we have built in a
subset of primitive fences based on the
context types and we have the detected
activity fence and just merely specify
ok I want to know when the user is
starting to be in a vehicle and this
condition is true when you first get
into the vehicle simple ok now the other
condition is a little bit more involved
probably want to be driving near the
store while that's open so in this case
we'll start off at the bottom things you
need are first kind of a geofence or
location fence around the store that's
what that first line is the second line
shows a condition to basically detect
when the user is actually in the vehicle
itself and that's what that detected
activity fences and the last one is we
probably don't want to show
notifications when the store is not open
so we can do instead is create a time
fence there is only true between those
open hours in this case we showed an
example between 10:00 a.m. and 6:00 p.m.
now one thing to to kind of think about
you know what is a fence here well it's
actually a boolean condition and you
know it takes a value of true or false
and actually that gives us our key to
how to combine these things once you
have boolean conditions you can combine
them with the boolean operators of an or
not so at this point for this specific
example let's actually do that so we
want to combine these and and the end
operators what's appropriate here so now
we have our full condition which is true
when the user is in the area around the
store the user is driving and it's open
hours ok so now you have your two fences
and
you need to basically register this with
the awareness API so you create your
offense update request add your fences
and then just call update fences and
while all your fences are registered a
couple things to note we do understand
that probably you want to key off of
multiple conditions of the user so we
had that our API is situated so you can
actually add multiple fences and in
doing so you need to also give us a key
that's what that first string start
driving and driving your store so that
you can know which of the fences are
actually calling back to you the other
thing to note is that the pending
intents can all go to the same callback
mechanism that will help simplify your
code to handle all the callbacks and the
last point is that where you pass in a
pending intent and the nice thing there
is that your app doesn't even have to be
running will be competing these
conditions for you and give you the
callback at the right time and this is
how we're helping with system health
your app can stay completely out of the
way of the system and yet you can react
when these conditions happen thank you
okay so now let's finish this off let's
write the callback in this case we show
receiving the callbacks via broadcast
receiver you get your data through the
intent we have a utility function to
extract that state into what we call a
fence state and now for the first
condition if you know that the key that
you had pass in in this case start
driving is true then based on the
condition of the state you can show the
Maps app in the in-car mode and for the
second one you can also key are for that
one which is driving your store check
the state of the fence and then show the
reminder so this is our fence API and it
allows you to react when the user is in
very specific contextual conditions that
you specify all right so let's talk
about the snapshot API now this is a
polling style API and the idea is that
your app while it's running can just ask
what the current values of these
different types of contexts are things
like the location the activity the
weather etc so let's go back to the
scenario so at this point Bhavik saw
this cute little dog so he wants to snap
a picture of that and on top of that
what do you want to do is tag it with
the current semantic location and the
weather okay and this is really easy
really we have a API called the snapshot
API you just call two methods one to get
the places others to get the weather
pull out the data they can tag it to
your photo and share it with the world
one thing we did do is that we added
caching underneath so that to kind of
basically not allow you to have to think
too much about what kind of cost there
is to to call into these api's okay so
the summarize the awareness API will be
releasing with seven different context
types and two simple API s we've
designed this so that in the future we
can add more context types and yet not
increase the complexity it takes for you
to incorporate the new signals as we go
forward and of course the other part of
it is that we try to handle system
health for you so that you don't have to
worry about it and by simplifying this
for you you can focus your efforts on
building that great experience for your
users all right so let me take a step
back and talk a little bit about where I
feel the awareness API kind of fits in
the grander scheme of things
personally when I first got ahold of a
smartphone the thing that really amazed
me and the relief kind of like excited
me was the fact that I had a bunch of
sensors and just having sensors it's not
enough the fact that people actually
carry these phones with them everywhere
they want and once you do that there's
like a real opportunity for the phone to
really know you know who you are
what you care about what your intentions
are etc and if the phone can know that
and your apps can know that then I think
we could build these kinds of magical
experiences that have basically never
existed before and we have a kind of a
new relationship with computing so that
was kind of the moonshot and that's what
we've been working towards trying to get
to now the awareness API is kind of a
step in that direction and what we've
done with that is to basically take kind
of these separate individual sensing
capabilities put them all together into
a unified platform and you know whether
that's a big step or small step time
will tell but it is a step to simplify
things so that now you can build better
experiences now this is incredibly
powerful and we'll be putting it soon
into your hands but really to reach this
moonshot it's not about the technical
capabilities as well right there's other
things that are of concern and one of
those things is privacy of course right
so we must be respectful of the user's
privacy and the real challenge here is
to build those experiences that really
simplify and delight the users in ways
that they have never felt before
and as far as I can tell in order to get
to that moonshot right I don't see any
path to success
that doesn't include respecting the
user's privacy and I'm sure you'll agree
as well
thank you all right so there is one
thing so part of the addressing privacy
is what we can do and the awareness API
is built in with a permission model
using androids permission model so for
each type of context we do protect it
against one of these Android permissions
so that we can ensure that the user has
given consent to your app to actually
access that signal now most of these are
pretty intuitive but things like whether
for example does require access find
location permissions and the reason is
because we're giving you the weather at
the users current location okay so
that's what we've done
but of course addressing privacy doesn't
end there
really has to be end-to-end and that
includes in your app and what kind of
experience that you're building now the
two basic principles we follow for
addressing privacy or transparency and
control and transparency is about
basically letting the user know what it
is that we're using what personal
information we're using about them as
well as what we're using it for and the
second half it is of course control you
know we have to let the users give them
the ability to actually activate or
deactivate these features so just to
give you a quick example I mean if if I
plug in my headphone and my favorite
music app starts playing automatically
starts playing music I mean that's
fantastic but only if it was transparent
to me that that was going to happen and
I have the option to actually turn it
off if I don't want it okay
I think one of the immediate uses of
awareness will be the post notifications
to the user and we do ask that you try
to be as specific and as targeted as
possible that way you'll basically hit
the user at the most relevant moments
when they actually want to actually take
action on that notification you send
them one thing to note though as an
Android end users can disable
notifications for your app and also the
notification shade is a shared resource
across all apps so of course you need to
give them a reason to turn your
notifications on okay and finally we've
done our best to be as efficient as
possible to address system health but
you know but please be mindful about
what fences you happen to register you
know make sure that you wait against
kind of the user experience enhancement
that you're providing now again to
conclude the awareness API is coming
soon and I very much look forward to
seeing what you guys end up building
with it
now at this point let me turn this over
the Bhavik you know tell us what our
partners have been up to with this
thanks Maurice whoa so as Maurice has
shown you we have a new API that makes
it super easy for your applications to
be aware we let this out to the wild a
little early and that a few partners
play with it and I'd love to show you
what they've been able to do Trulia is
an online real estate agent and one of
the big parts of their service is
helping their users and potential home
buyers find and visit open hours or open
houses something they've struggled with
in the past is when should I send these
users a notification to remind them to
visit an open house sure I could do it
when they're near the area where the
house is but what if they're driving
through it or it's a rainy day and
they're just not feeling it so with the
awareness api's fence feature they've
actually been able to create highly
tailored notifications you will only get
a notification for an open house if
you're in the right location if the
weather is nice and you're walking and
not driving or running through the area
they're very excited to see how this
more tailored notification will increase
click throughs on this very important
action for them one of my favorite photo
editing applications is aviary
it's a powerful editing tool that lets
you take and edit photos to really
capture a moment one of the big features
they have is a stream where you can see
photos that other people have taken to
get some inspiration a thing that they
realize is the way that you take and
edit a photo depends a lot on your
context the way I capture a rainy day in
Seattle is going to be very different
from the way I capture a sunny day in
Yosemite or maybe a sweltering day like
IO and so what they've been able to do
with the awareness API is use snapshot
to understand what your places which are
semantic location is and the weather to
show you photos that could inspire you
to capture and edit that perfect moment
finally music is really near and dear to
my heart and super player music is an
application music streaming application
that is very popular in Latin America
they have this amazing assistant bot
feature where you can ask it for
recommendations and it will return
recommendations to you what they've been
able to do with awareness and plan to do
is merge those functionalities with
contact signals so that now when I've
just finished running and I'm looking
for something to cool down while
stretching when I get to the gym or
maybe when I'm about to go on a long
journey they can suggest the right music
for the moment those are not the only
partners that we work with and we're
very lucky to have been involved with a
wide variety of applications in the
health and fitness space Ron keepers
thinking about tagging it's running
posts with whether we have local
applications like Trulia and Zillow that
help people find the things that they
need in the homes that they want while
around them GrubHub is also thinking
about how they can integrate weather
with their features and so is key kanto
we've got photo editing applications
like I showed you with aviary
but also pix art and even os-level
functionality like Nova Launcher which
is thinking about completely rewriting
its launcher to be more context aware
and show you the right apps at the right
time or xej which is going to allow for
customizations of ringtones and
wallpapers based on context with these
partners we're just getting started we
have nine api's across where you are
what you're doing and what's around you
and a brand new APR that we're launching
today called the awareness API that
merges all of this information in a
battery and system health friendly way
I'm very very excited to see what all of
you are going to do with it if you're
interested sign up for our preview so
that we can remind you when the API
comes out and get you off of the list to
get an early look at
developers.google.com/live ngey / into
any of the other api's I talked about
today visit GCO slash awareness IO to
see a full list of our other talks and
open hours thanks me and Maurice we'll
be around just outside for questions
after</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>