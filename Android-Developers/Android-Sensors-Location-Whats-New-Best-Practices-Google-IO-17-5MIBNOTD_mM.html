<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Android Sensors &amp; Location: What's New &amp; Best Practices (Google I/O '17) | Coder Coacher - Coaching Coders</title><meta content="Android Sensors &amp; Location: What's New &amp; Best Practices (Google I/O '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Android Sensors &amp; Location: What's New &amp; Best Practices (Google I/O '17)</b></h2><h5 class="post__date">2017-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5MIBNOTD_mM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thank you so much for coming
my name is Marx the goddess and I leave
the location and context client team and
with me our way Wang Steve malkos and
shauvik Sen and we're really excited to
be speaking with you today about what's
new with android sensors and location
now we'll start by discussing some of
the upcoming changes to background
location and how you can prepare your
application for the Android o release
well then discuss accuracy improvements
the location improvements activity
recognition as well as sensors so let's
get to it well I think we've all
experienced that terrible feeling when
your battery is just about to run out
it's a beautiful day outside to decide
to go for a hike and after several hours
you come up to a beautiful baby deer
it's super cute and you manage to get
really really close to it closer than
you've ever been
and you decide you're going to take a
picture now this is going to be the best
picture you've ever taken you slowly
carefully take your phone out of your
pocket push the power button and your
phone is out of batteries
so not only do you miss the shot but
then you realize you're not exactly sure
where you are and you're really counting
on your phone to help you navigate back
and all of a sudden the woods look like
a much scarier place so I think the
point here is that battery life is
extremely important to users in fact one
in three users say the battery life is
the number one problem on their device
the number one problem this is also
apparent in statistics we track about
location so a large percentage of users
have decided to completely disable
location on their device and when we ask
them why it's at power with the main
reason now this is a problem for every
developer in this room who uses location
since when location is disabled just a
lot of stuff breaks from navigation to
contact the where apps I mean even the
weather just doesn't work whenever
location is disabled but I think more
importantly your users turning off
location means that they're not happy
with the experience they're getting and
we don't want that we want users to
absolutely love location so we've taken
a really close look at power consumption
and here's what we found
well for one thing location is used a
lot the location API is currently allow
applications to request location at
virtually any time so every bar you see
here is the device doing some kind of
location related work now we can analyze
the cost of these using something called
Android power primitives that's
something I'm really excited to be
speaking with you today about that's the
first time we've talked about these
externally so you can think of power
primitives as a way to understand the
fundamental building blocks of power on
an Android device and we can use these
to really understand sort of how
location power is computed here are the
four basic operations that consume power
when using location we first have to
wake up the device to perform some work
we didn't have to get some location
signals like Wi-Fi and GPS then we have
to run some computation on the CPU and
then finally maybe go to the network to
get some more model model information
and we can understand the cost of all of
these by using the power primitives so
this is the cost of every single one of
these operations there's a lot going on
here so let me highlight the most
important ones
well first using the cell network is
extremely expensive this is why the
location stack has been heavily
optimized to cache data locally whenever
possible and try to avoid going to the
network by computing things locally um
also obtaining location signals is
expensive for example the first fix for
GPS can cost quite a bit this is why we
usually tend to prefer doing Wi-Fi scans
instead but unfortunately Wi-Fi scans
are not really cheap either
so what we've been doing is trying to
reduce the cost of these as much as
possible by limiting which channels we
look at when doing a Wi-Fi scan as well
as spending the least amount of time
possible on each Channel but we are
coming up to some physical limits so how
much we can optimize here so in summary
we see that power is extremely important
to users
unfortunately location is not cheap and
we're coming up to some pretty hard
physical constraints as to where we can
optimize and as well there are some
amazing features being built using
location
so with all of this in mind we're
introducing a significant change to how
location works in the Android Oh release
called background location limits this
is something we're really excited about
as it drastically improves battery life
for Android users while also allowing
great location use pages to be built and
that way we'll come speak view some of
the details of background location
limits as well as how you can prepare
your application for the Android okay
thanks a lot max my name is Lee Wong I'm
a software engineer in location and
context team I'm going to talk about
background location limiting Android
release the power improvements to locate
energize as well as how you can prepare
your application for these changes so
first let's take a look at the
difference between fog on a bandwagon
apps for location purpose application is
considered as fog on when it is actually
inheriting with users
which means is either running a fog on a
UI of running your foreground service
that the user occurs about otherwise the
application is considered as background
from location protective so our study
shoot programs are the major contributor
to location power issues now before
Android or release there were no limits
to background location usage which means
any app can request location at any time
now this call is served several
different problems as a result many
applications request location
aggressively when I run in background
all they leak a fast foreground location
when I switch from foreground to
background so beginning from oh we are
going to impose limits to the begginers
so I would like to outside these limits
apply to background location only and
would not affect your foreground
location your age so let's start with
location providers location providers
are the main entry for requesting
location updates we have two main
location writers the fused location
provider in Google Play services and a
location manager on a platform so start
standing from Android all four post doc
engine providers as running in penguin
another physical location communication
more frequently than a couple times
power so basically I infielder location
provider a few more location
communications are allowed but the
delivery time is going to be the same as
number energy I so so still looking
provider we're also going to take
advantage of Wi-Fi connection to reduce
location communication no no no new
location commutation will be performed
one advice is connected to the same
access point which in because the user
has not moved much from the previous
location so this provides significant
improvements when a user is at home I
always work with Wi-Fi connection so
with similar strategy in mind we also
applied very similar changes to defenses
the responsiveness of gfriend's will
increase from tens of seconds to a few
minutes but the power is going to be ten
times more power efficient than the
previous releases so we also use similar
Wi-Fi connection approach and like can
now provide a quick transition detection
when a user loop comb or lose the work
so all the changes I mentioned apply to
all underground apps running on android
Ulrich's regardless of the target SDK
version we believe this is a special
approach to provide users with
immediately battery improvements one
upgrade - oh now you may be wondering
how do the changes look like on a real
device so muxu example of location
activities before now let's take a look
at how to change after applying all the
background location limits so you can
see there are much fewer location
activities during the same period in
fact after around 6 p.m.
while the device connected to a 13 Wi-Fi
access point there were no location
computation but we were still able to
provide special occasions because we
know that the device is connected to the
same Wi-Fi so the user has not moved far
away so the paper consumption for
Belvoir locations is much much better
now oh okay so
of a few changes to background occasions
and the better improvements the budget
looks really good but as a developer you
might be wondering this means relocation
coming to my eyes what should I do
so let's talk about a few approaches
that you can take to make your
application ready for background changes
the first cause it's the only request
slogan location so now I really need
location when it is not interacting with
the others if it only needs you the
location to search restaurant nearby
like always request location after user
opens the app no background location
with limits would we're applying this
case and I can't get this frequent
location updates as default so we are
also working on reducing the current
location and latency to ensure a better
foreground location on the other hand if
your application needs to trigger when
the user enters or exits a giving area
such as automatically cleaning of air
conditioning when a user leaves home you
can use yourself a guy to detect the
user has entered the word or exited an
area of interest as mentioned before the
two-phase API in Android L is ten times
more power efficient than previous
releases if your application needs to
collect users location and display the
location history at a later stage such
as Google Maps timeline then you can use
basic API in field location provider
making allows the crane system to
collect the carrion signals such as
water screens at low power with a higher
rates than compute and deliver locations
to the client only when needed
this way we can consolidate all the
network to queries to cell battery so
let's take a look at an example of how
to enable location Beijing at zero you
will need to create a location request
you set the interval to ten minutes the
key here is to set the next word time to
a few times more than the request
interval so this allows location
provider to flex location signals every
10 minutes but delivers in locations
locations for the class
and every certain minute now finally at
the last result if you really really
need uncertain foreground occasions when
you're running in background you can
start a foreground service by showing an
ongoing notification application showing
angry notifications I counted that as
log on apps so background attention
limits will not apply to startup Foreign
Service create a notification and apply
a pending intent to be called when a
notification is clicked by calling
subprogram or if your app tag is Android
L call stands for Ghana stories the
stories become the foreground while
you're showing on Google applications to
your list now do not forget to stop
foreign service if photo on location is
not needed animal okay so so far I've
talked about background location limits
only Android device how about real
devices the system did not impose any
limits on real devices so we rely on
your pencil practices to make sure the
background location power is at a
reasonable step the optimization you do
such as request only for Ghana locations
you'll do fencing in that of frequent
location polling using Beijing if
immediate location is not needed now
whether it be applied to pro devices to
make it better on location you need a
battery your age at a reasonable sex
okay so in summary I've talked about
back on location limits and Petrelli
improvements for location zero and
several different ways to make your apps
ready for those changes now let's switch
gear to talk about location accuracy
improvements thank you
for the past years we have been kid
young improve alacrity accuracy
especially for in the environment our
destitute of 40 percent and 20 percent
accuracy improvement of our solid parts
of two years this year we are good track
of achieving another 40 percent but
accuracy improvement and another 40
percent accuracy improvements in a year
2018 in fact our accuracy would be good
enough for indoor navigation now let me
show you a couple of videos to
demonstrate our indoor accuracy
improvements so the left side video is
running on the current field record
provider the right one is running on
version that coming soon
the pollute 30 line should arouse the
user will take it the blood out Azure is
dual its current location now let's play
the first video so you can strain the
blue dot aligned with the doodles
relocation will normally wear at the
start but as user once tipping does the
location jump and divert a lot from
users relocation okay now let's play the
second video to see how things improved
so this time the plot out aligned with
the valve perfectly where even when the
user was deep in doubt now when a user
negative 10 you can change the algorithm
try to move really really well so we are
very excited about the accuracy
improvements and we hope this compromise
the opportunity to the application
another area we are truly excited about
is the support of the Royal Genesis
environments in Android release on
location manager
developers can now get more information
such as pseudo ranges toffler's and
cairo faces other than just resolution
the information can be used to build
value added apps such as much much more
precise positioning using carrier phase
measurement
we also provide tools to log analyze and
realize all the instances measurements
you can use this tooth when Pro tighten
your abs and also question like we still
eyes are you I envy you
how strong are the individual
measurements how accurate are the random
element we are very eager to see modular
cases of the Royal Genesis environment
in an ecosystem cave
so far we talked about accuracy
improvements to fuel economy rider the
new IP is for the ISIS environment we
hope developers can take videos of these
improvements to build better location
edge with that I will hand over to max
to talk about activity recognition
oh thanks a way so activity recognition
provides your application with
contextual cues as to what users are
doing in the real world and now these
are the activities we currently detect
this is something I'm extremely proud of
just a few years ago this list was much
smaller so you can break it down into
fist it Fitness type activities like
running or biking device context like
are you in a vehicle as well as some
gestures like the tilt awake gesture on
your watch and today I'd like to talk to
you about some upcoming improvements to
our accuracy some simplifications we're
making iterate to our API is to make
them much easier to use as well some new
activities will be detecting so let's
start with accuracy improvements well
the world of machine learning has seen
some amazing results from the use of
deep neural networks and activity
recognition is no exception we've
developed a deep activity neural net
which significantly improves the
accuracy of activity recognition now
this makes it much easier for you to
build contextual applications and it
also looks really cool we're also doing
some big improvements to our API to make
them easier for you to use now the
current activity recognition API
provides you with raw low level access
to activity signals now this is very
powerful and flexible but it also means
that simple things can be hard to do so
let me show you what I mean now let's
say you're building a car Sisseton app
that allows users to interact with their
device hands-free what
in a car now a simple way to do this
would be to write an if statement that
says if the activity is in vehicle start
my app and if it's not in vehicle close
my app so let's talk about some of the
pitfalls that you'll encounter do if you
do this okay so the user starts in a
vehicle with a confidence of 80% great
so far so good
you launch your application it keeps
driving the confidence of 100 everything
is working well so far and then they
stop satellite and you get a still
activity now that simple if statement
from earlier wouldn't have taken this
into account and the app would have
closed so you say okay okay let me
update my filter to deal with the still
while in vehicle use case so the user
continues to drive and then hits a speed
bump and we miss detect it as biking now
we work really hard to avoid
misclassifications but sometimes you're
unavoidable and this kind of raw
low-level API we're providing that has
very little filtering in it now you can
write a filter to try to deal with
things like this because if you look at
the history well there's a lot of
driving in the past the biking
confidence is very low so 55 is pretty
low and there was no activity that you
would expect between driving and biking
like something like walking but but
writing this is getting pretty
cumbersome and complicated ok so the
user continues to drive and now you get
a tilt event now tilt event is a strong
indication that the users activity is
about to change it tends to happen when
a user goes from sitting to standing so
you can take that into account into a
filter and then finally the user starts
walking and you can finally close your
application now writing a filter to deal
with all of this is pretty tricky and
requires a lot of training data to get
right oh we're making this much easier
with the activity transition API now it
takes a messy raw activity signal like
this and turns it into simple
transitions you get a vehicle start
event when the user starts driving and a
vehicle end event when they stop that's
it we take care of all the filtering
behind the scenes this means that by
using these things this is activity
recognition transition API you can write
that simple if in vehicle statement and
your app should just work
thank you now I'd like to quickly talk
about some of their new activities we're
working on detecting so we're really
excited to announce that we've developed
a road versus rail vehicle detection
algorithm that can tell the difference
between a car and a train for example
but we think this will lead to some very
powerful applications the help users
during public transit as well as improve
in-car experiences we can't wait for you
to try it out and I'm also really proud
to announce that we're now able to
automatically detect strength training
exercises this means that you can walk
into a gym and have your entire workout
automatically track for you this makes
it really easy to keep track of your
progress over time and make adjustments
along the way it's available in the LG
watch sport we'll be rolling it out to
more devices soon so in summary activity
recognition will be bringing you
improved accuracy simplified api's and
new activities in the coming month and
now I'd like to invite Steve to talk
about how we'll be making this as well
as other contextual use cases run at
ultra-low power
awesome thanks mark hi my name is Steve
malkos I'm the technical program manager
for the Android location and context
team before I start talking about
Android sensor hub and context hub
runtime environments can I see a show of
hands how many people here know what the
Android sensor hub is not that many of
you that's alright because when you
leave here today you'll all be experts
in this field all right mark just showed
us some really cool examples on how
activity recognition works we've
launched some amazing features in
Android things like physically tracking
or physically tracking where you're
parked your car tracking your activities
like running walking and biking and
providing you with relevant information
that's important to you at a specific
moment in time like pulling up a train
schedule as you arrive at the train
station but we found that bringing some
of these factors some of the biggest
limiting challenges here has been around
power consuming power that's your
battery so with the Android Center hub
and context hub runtime environment
we're able to leverage the capabilities
of a low-power processor to make it
possible for us to bring these
experiences and more we all know that if
you write an application and it sucks up
the users battery they're going to
quickly uninstall it from their device
when talking about power for activities
that happen in an always-on fashion
there's always two things at play
latency and accuracy improving power
means we could run our apps at a greater
frequency improving on the latency
improving power also means we could look
at more signals improving on our
accuracies so two years ago we
introduced the Android sensor hub in
marshmallow we use this tiny standalone
microcontroller that's depicted on a
dime and connected all the sensors to it
and it runs in an always-on fashion we
launched this on the Nexus 5x and the
Nexus 6p last year we worked on the
second generation of the Android sensor
hub this time in addition to connecting
all the sensors to it
we also connected all the wireless
connectivity components and we launched
this on the pixel and the pixel excel
as mentioned earlier the Android sensor
hub runs and then always on processing
and it hardly affects the users battery
context hub runtime environment provides
a common platform for us to leverage
these low-power processors so with the
context hubs common platform we created
many applications that we call nano apps
and all the exits so let's go through a
couple nano app examples all the
activity recognition API is like running
walking biking and vehicle have been
ported from the main application
processor into the Android sensor hub
we'll take a look at how activity
recognition both saves on power and
improves on our accuracy when running as
a nano app if you if you to run activity
recognition on the main application
processor it consumes a ton of power
about 5 milliamps when we introduce
sensor batching in the Android operating
system we were able to cut that number
down into half but it's still running on
the main application processor we took
it from 5 milliamps to 2.5 no amps
running these same algorithms in the
android sensor hub as a nano app cost us
a fraction of that power 300 micrograms
and because the power is so low we can
now run these algorithms at a faster
frequency improving on our latencies and
our accuracies another example of a nano
app that I'd like to highlight is
geofencing there are tons of geofencing
applications in the market today that
use the Google geofencing API we've not
offloaded these algorithms from the main
application processor into the android
sensor hub and we did this by connecting
the Wi-Fi chip gps and the cell modem
into the low power compute domains this
brings us a significant power savings
and it improves on our latency and
finally on the android sensor hub let's
talk about gestures we added tons of
gestures gestures like lift to check
phone DoubleTap to check phone flip
camera and more I'm going to highlight
one of the examples and let's focus on
flip camera if I launch my camera app
and double twist it it will change from
we're facing to selfie mode I have to do
this all right everybody on the count of
three say cheese one two three cheese
alright this applica gesture is only
possible because it runs in an always-on
fashion in the android sensor hub any
application could register and use this
private sensor type today ok to close on
the android sensor hub with thre
devices on the market that support this
automatically get a common platform that
takes advantage of lower power lower
latencies in an always-on compute
fashion and these these these nano apps
can overcome some of the new paradigms
that have been introduced with our
latest operating system like background
location limitations we're really
excited about this platform and the new
use cases that could be enabled from it
I'm going to switch gears now and go
from that to Android sensors
highlighting four key items and then
Sollux going to come up and talk to us
about the compass dynamic sensors this
was introduced in Android n it allows us
to expose sensors that are not
necessarily bundled with the device and
it lets us connect them via bluetooth
and USB for example you can now attach a
high quality inertial measurement unit
to your device
biological sensor environmental sensor
and your apps could natively see them
this year we're standardizing on these
interfaces for dynamic sensors so the
sensors that you build can now leverage
the Android platform and make it easier
for you as an application developer to
access them direct sensors channel
initially targeted for VR and camera
applications simply put direct sensors
channel is minimal latency between the
sensor driver and your application by
bypassing the sensor service
accessing sensors directly with direct
sensors channel the programs that you
develop will have the smallest amount of
sensor latencies ever experience of VR
application with the skewed horizon
similar to uncalibrated magnetometer and
uncalibrated gyroscopes we introduced
uncalibrated
last year we wrote our own calibration
algorithms for the accelerometer and we
open sources to the ecosystem with the
hopes and goals that a device
manufacturer would build better devices
and we're seeing results like this after
applying the VR after applying the
calibration algorithms the VR landscape
is straight final sensor feature that
I'm going to focus on is risk sensor now
called low latency off body detector
simply we have a reliable way to detect
when the watch is on the wrist or off
the wrist the targeted use cases for
this is low friction authentication to
your phone no pass codes for payments
and changing the UI behavior when the
watch is off your wrist like sleeping
the watch more aggressively when it's
off your wrist so before I hand the
microphone over I want to quickly note
that we use sensors today in almost
every application and our commitments to
them at Google is huge we'll continue to
invest in this sensors future these
features like dynamic sensors direct
sensors channel calibration algorithms
and low latency off body detectors are
just a few highlighted examples on what
we're working on we'll be adding more
sensor features in the future with that
let's continue on the sensors theme in
here from the Civic on the compass
thanks DS i'm shauvik cinema software
engineer in android the location and
context team and I'm here to talk about
heading and hearing so what is heading
and bearing so bearing tells you at the
context the direction of the user is
traveling and heading is the direction
that the users phone is pointing so
heading and getting a different like
heading is something that if you are if
you are standing at a stoplight and you
plan your phone around to find which
direction you should walk that's when
you're heading changes and your bearing
is not necessarily changing at that
point so how do you realize getting and
heading depends a lot on the users
context so if you're an application who
might be my gain by knowing the
direction of the user think very hard
about what context you're after
because based on that you want to select
between heading or bearing let's see how
we are doing that in Android Google Maps
so in Android Google Maps the blue dot
arrow used to always point to the users
heading essentially which direction your
phone is pointing now what we found is
that the users expectation from the blue
dot arrow is much more when they are
stationary like standing still or you
know above or panning their phone around
they want to see they're heading from
the blue or arrow perhaps because they
are trying to figure out which direction
to go next if they're traveling like in
a car or in a bicycle or even when they
are running but they want to see the
bearing that is the direction that they
are moving because they want to perhaps
are verified that the relation they are
going towards is actually correct and
more importantly they want us to
communicate the uncertainty in heading
or bearing because if we are not certain
if the heading or bearing information
that we are giving to the user is not
correct the user wants to look around
and orient themselves and be diligent
about this so how do we do this to
distinguish between stationary and
traveling we can use the activity
recognition API that matches talked
about you can know when the user is
stationary versus traveling and choose
between heading and burek based on that
now to determine the be uncertainty note
that bearing is based on location
so you can you can see what the location
speed is and what the location accuracy
is and based on that you can determine
if the building is correct or not but
how do you find heading uncertainty to
understand that let's see how heading is
being actually been computed so heading
today is estimated based on the phone's
compass and as many of you know the
phone compass can be very unreliable in
presence of metal objects or external
electromagnetic fields simple cases
where like for example if you're in a
car or you're charging your phone or
even simple things like you keep your
phone next to your laptop it's possible
that the phone's compass will become
uncalibrated and hence becomes
unreadable and all of these types of the
heading uncertainty that we are after so
in a nutshell what we found by looking
at a lot of data is that heading
unscented it really is about what is the
environmental uncertainty in which your
device is at now to understand the
environmental uncertainty we found
machine learning can actually help us
there is only as only a few kinds of
environmental uncertainty or animals
that we are after and if you have enough
data our machine learning can tell when
there is animal e and hence when the
compass is correct and when the compass
is not and that ties into heading
uncertainty we can tell the user or your
heading is strongly not correct because
the compass is unreliable and moreover
now because we know that the compass is
when the company's unreliable we can go
back to other kind of sensors like
gyroscope and this in return actually
improves the heading accuracy
significantly so now that we know the
heading on sentry let's see how we can
communicate that using and in an Android
Google Maps so before in Google Maps we
had this pointy cone which will always
fall into a heading but now because we
know the heading uncertainty we can have
a corny y where the width of the core
now tells you what is the answer and in
heading if it's too wide you know that
the heading is problem or correct and
hence you can look around and try to
orient your yourself if the cone is
narrow you know it is correct and you
can just look you know use it for your
own travels
beyond Android Google Maps if you are an
application who might be Venice might
benefit by knowing the users direction
and also the uncertainty ending in a
more correct fashion we are happy to
announce that this is something they're
going to bring in for all applications
to the fused location provider API with
this I will handle or to Steve to talk
about the future of Android location and
context thank you awesome thanks so Vic
I love talking about the future so let's
talk about the presence with regards to
testing location quality because that's
the key to making future improvements
over the past few years we've gotten
very serious with regards to testing at
Google we've built up massive labs and
real-time tests from around the world
measuring our location quality for
example we run running tests walking
tests Viking tests stationary tests
testing both indoors and outdoors when
driving near tall buildings it's very
common for your GPS position to jump lag
or even drift at Google we we use very
expensive military-grade inertial
measurement unit that gives us
centimeter level accuracies that gives
us the strong truth that's the image at
the top the San Francisco tests set up
the image at the bottom shows our
running and pedestrian test setups the
output from these tests is the graph
where we show a cumulative distribution
function on the position errors from
each device as measured from the ground
truth okay project elevation before
talking about this I want to quickly
note about a service that we launched
last year called emergency location
services in Android this service gives
us the ability to use the computed
location in Android and send it to
emergency responders during an emergency
situation so for example if you dial 911
to on your handset the computed position
from Android will go to carriers network
and public safety answering points for
those carriers who sign up for this free
service
all right with that in mind project
elevation took many places around the
world like downtown San Francisco when
you're deep indoors you're 2d Latin long
are just not enough GPS altitude works
great when you're outdoors but barely
works if at all deep indoors and we
especially need it when we're in
environments like this so with Google's
machine learning algorithms anonymized
crowdsource models will soon be
launching project elevation this year
this service will first be utilized by
our emergency location services in
Android and where we hope to see first
responders benefitting off of it and
then we'll also launch it as part of the
standard Android location altitude
outputs so you as a developer can get
access to this as well we'll also bring
a new class of 3d accurate indoor
location you heard part of this from
white by using Wi-Fi GPS cellular PDR
IMU altimeter we're just around the
corner from launching this Maps massive
improvement and we'll get there again by
using Google's machine learning
algorithms anonymized crowdsource models
to give us these better accuracies where
we can now navigate indoors then once we
have this all running on the main
application processor will port it down
into the android sensor hub because
that's the key 5 meters at zero seconds
time to first fix at ultra-low power ok
the GPS geek inside of me gets really
excited when we talk about dual
frequency GNSS receivers L 1 plus L 5
signals what does this mean to you
better GPS performances will bring this
into smartphones in the next couple of
years and will do so for the following
reasons better ranges for multipath
mitigation because we'll be using a
higher chipping rate 10x higher it will
give us unprecedented frequency
diversity using the l1 and l5 signals
together our Android o AP is already
have full support for multi frequency
GNSS receivers this enables many
cases I'm just going to highlight two
better downtown urban Canyon
performances no more positions on the
wrong side of the street post-process
decimeter level accuracies any
application that needs really precise
location will benefit off of this like
fitness applications traffic info and
more so at work we get to work on some
really cool amazing new stuff let's put
this all together the future will become
more and more contextually aware we'll
continue to add more signals into the
android sensor hub we've already
achieved activity recognition and
geofencing next we're going to add the
fused location provider awareness places
and more we'll do this with deep neural
networks machine learning algorithms and
our personalized models with the goals
of helping you make deeper experiences
to your users in your applications by
adding all these signals we'll reach a
point of location and contact
singularity where you'll be able to
write applications that simplify user
interactions help augment human memory
and knowledge and your users having a
better understanding of themselves I'm
truly excited about this future and all
these possibilities thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>