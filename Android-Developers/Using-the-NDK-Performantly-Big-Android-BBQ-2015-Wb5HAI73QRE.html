<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using the NDK Performantly (Big Android BBQ 2015) | Coder Coacher - Coaching Coders</title><meta content="Using the NDK Performantly (Big Android BBQ 2015) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Android-Developers/">Android Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using the NDK Performantly (Big Android BBQ 2015)</b></h2><h5 class="post__date">2015-10-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Wb5HAI73QRE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Android internals we writing performant
native code if you haven't heard enough
about me already I have spent five plus
years talking to developers just like
you around the world and it is awesome
to be here in Hurst and I spent 15 years
as a software developer before doing
that so I have a little bit of street
cred I actually started developing
Android right around Android 1.1
seriously actually I started developing
commercially then I was working with it
ever since one dot oh I wear a lot of
hats and these this is like one of the
smallest hats that I wear and I have no
shame and I think I'm kind of funny
sometimes especially with lack of sleep
like now all right performant is an
entirely invented word it is not a real
word how many of you knew the performant
wasn't a real word okay good it got a
bunch of English majors here that's
awesome so and actually according to
urban dictionary and most of the
research that I did because I do
extensive research on a talk like this
performant was actually invented by
software developers okay and and and and
there's there's some theories behind
this but the Urban Dictionary defines it
as having adequate performance and but
really this would not be nearly as cool
of a talk native code having adequate
performance so this is why the word was
created okay it just doesn't have the
ring to it we don't like adequate in our
industry we like awesome so so yeah so
rather than title at that I use the
invented word and we're going to be
talking about really really tiny
benchmarks because you know in order for
your app to actually perform well you
have to do everything and make sure
everything happens within sixteen point
six seven milliseconds that is how you
get 60 frames per second but most of the
benchmarks that I'm going to be talking
about in this lecture are in nanoseconds
so if we take this into nanoseconds
that's a lot of nanoseconds so this
stuff is really really fast so don't
worry when I tell you it takes five
times as long to do something on one
version as the other because it's really
fast still but it's good for you to know
all right
so again do not panic remember the clock
speed of my Nexus of the Nexus 5 which
is what I used to do most of my testing
because I wanted something that could
run KitKat as well as run LNM is about
2.3 gigahertz tops with 4 cores so
that's that's like billions and billions
of instructions you know I know I'm like
sounding like Carl Sagan here so we're
talking in very small micro benchmark
turns now Android internals is is this
kind of thing I'm working up I want to
see what you guys think of it so after
afterwards we'll have a quiz and it
really is about the unique voyages of
discovery we can take in an open source
platform like Android so the idea is not
just to understand how to code Android
but understand how it works so that when
you run into problems you have a better
idea of actually what you're dealing
with and and we're gonna do it kind of
this way we're gonna actually test our
assumptions we're gonna benchmark we're
gonna look at source code and we're
gonna debug even in native so this
voyage once takes us into the land of
optimizations and art if you saw my talk
yesterday I got I got a bit into that
but this time we're gonna be a bit more
pragmatic because really gonna talk
about how the world of J&amp;amp;I has changed
as we've moved from a world of delve ik
to a world of art but I'm getting ahead
of myself so let's talk about native
code most of you guys have actually done
native code but I'm talking about code
written using the NDK and we're talking
about primarily C and C++ code that
interfaces with the Android runtime
using j'ni here is a really really
abbreviated architecture diagram of what
this looks like
applications written with the NDK take
the form of these you know Dex classes
that execute on the Android runtime they
interact with system libraries by the
SDK framework classes an SDK application
code is written in a language like Java
that the runtime can support so the
Linux kernel was written primarily in C
and C++ and so are the system libraries
the framework and the Java Runtime call
into these libraries using the Java
native interface or j'ni now the a the
NDK essentially allows you to write I
dynamically linked native library but it
can't run
directly against the system libraries
because these a be eyes or ap eyes
aren't stable so the purpose of the ndk
is to give you a stable application
binary interface to run your own
compiled code against that provides
access to only the most critical OS
features so the platform can still
continue to grow and expand and change
how they implement things and be awesome
but your application code is talking to
this through this ABI it's all important
stuff and that's what it looks like boom
your application code now talks it's
your library which is going straight to
native let me talk to you a little bit
about the history of the NDK the
original first versions of android did
not even have it but once but we got it
in cupcake and and we've been slowly
expanding it ever since so you know in
the first versions of it you got a C
runtime really minimal C++ support z lib
compression logging networking dynamic
linking some math that's not not a lot
but enough we then added graphics so the
first version couldn't even talk to
OpenGL but we added graphics there and
you know what the longest part about the
slide was actually trying to find all
these images again it's like what if I
used to slide with these images and then
gingerbread really expanded things
gingerbread got much more serious about
gaming and and multimedia so we added
our native application API so you can
actually build that was the first
version of Android where you could
actually build a native application
without needing to use any java
whatsoever and then and also sound which
was really cool like open SSL it was
really nice to have we continued to
evolve it an Ice Cream Sandwich we added
them the openmax al media layer um not
many people know this but you actually
can access render script directly from
the NDK as of KitKat and it's pretty
cool stuff it was a long long request
and we also did a bunch of graphic stuff
here that's why these aren't in order
but in jellybean mr2 we added yes three
and in lollipop we added 3-1 as well as
64-bit support so that's pretty cool so
let's talk about some assumptions so
we're gonna star something since they're
basically to follow the suggestions that
the perf J&amp;amp;I
if you have not read this article it is
the gospel for looking at how to deal
with J&amp;amp;I on android but do they still
make sense today we have an update of
the article since we shipped art so here
are the basic things you have to do ok
absolutely critical when you're doing
trying to make j'ni performance one is
you are going to cache field and method
IDs and you're going to do it
intelligently to you're going to you're
going to get strings in a reasonable way
and you're going to copy things in a
these are the only three real tips we
gave but I'll go more into details so
how did we benchmark this we actually
use something called caliper now how
many of you actually have ever heard of
caliper in this room no one that's good
I'd never a one person sorry I had never
heard of it before doing this but I
would but I was interested in doing
benchmarking it turns out if you
actually look at a OSP we have caliper
tests checked in this is actually how we
benchmark our the VM ourselves and when
you use this thing called Bogar and if
you actually look at what's checked into
Vogue our it's a really ancient version
of caliper I'm hoping someday we
actually update that it would amount
look they would have made my life a
little easier but caliper is a really
cool framework for running micro
benchmarks all right so let's get to the
first thing this if you haven't used NDK
before it's how you access a class from
native code so once you have the class
and I'm passing this class in from from
Java you can see J class type that is
actually that is actually a class
information and then I could simply call
get field ID the name of the field the
type of the field so it's so in this
case integer and then finally I can call
get int field to actually pull the value
so that's how we actually access an
integer that's inside of a Java
classroom native code alright so the
first suggestion which is a really
really good one is to cache field and
method IDs and here's why those in field
and method IDs are just numbers they
don't actually change once the class has
been loaded and if you want to be really
really good about when you actually grab
them inside of the static initializer of
the class in Java you can actually call
awesome j'ni code in this case I'm
calling native Annette and inside of
native in it really should have been
named native in it now that I look at
the slide but that's okay you can see
I'm getting that field ID and that field
ID will be good as long as this class is
loaded so that's pretty awesome I don't
have to think about it I'm just storing
it into in a in a little variable there
that's associated with my native class
all right let's talk about performance
so here is how it benchmarks on a nexus
5 running KitKat and marshmallow and
you'll notice something art takes longer
that's that's going to be in general a
common theme art is more complicated
than dalvik in general and so it's even
more important today than it was
initially to cache these things because
devices are running faster and art is
already faster doing most things so
you'll even notice this may be a little
bit more than even these benchmarks
should say so let's look at the code and
try to figure out why this is faster or
slower I should say and the really key
thing is this thing here this scoped
j'ni thread state and scoped object
access this is why J and I actually does
not run at lightning warp speed and
that's because every single thread in in
Android can be in one of two states it
can be in running state that's when it's
actually can be more than that but two
states that we care about it can be in
running state that's actually when we're
in the Java Virtual Machine and we're
actually executing stuff and it has
access to all that great sorry I should
say the run time it's not we do not have
a java virtual machine in android you
can strike that from your memory the the
Android runtime that's when it's in
there or it can be in the in the non
running state or native state so when
we're actually accessing a variable like
this which is an INT field all of these
are our variables that are inside at the
runtime we actually have to switch the
state of our thread in order to do that
and that means that we're doing a whole
bunch of synchronization and that
synchronization is expensive it's
expensive on the order of about 300
nanoseconds now to give you some context
because theater nanoseconds is a really
small number in the average function
call an average function call in art is
about five nanoseconds and avec it's
more like ten so so once again we're
talking about something that is really
tiny number but it's still like sixty
times longer than a standard function
call so it's still some do you think
about so let's look at our first
scorecard and yes based upon our
benchmark caching field and method IDs
is great for both doll Vic and art it's
even better in art alright so that
looked look at the suggestion - of this
which was used get string cares now this
is this was kind of interesting so
basically as you probably all know the
standard for Java which art also follows
is to treat all strings as as double
byte character strings UCS - and this is
important because we are using we are in
a world that's highly international
single byte strings are kind of passe
etc etc
not to mention is it turns out the the
VM actually doesn't particularly have
great instructions for dealing with
bytes so it actually it's actually kind
of nice to have these things in in these
two word sorry these two character these
two by characters thank you so the
suggestion here is that rather than
getting string UTF characters like we
have there at the bottom we actually
call get string cares which actually
takes our string and gets us the closest
to being a native representation of it
that you can imagine and we would expect
this to always outperform the UTF
equivalent where it actually has to do a
copy of memory all right so let's look
at this
I took a 15 character string I ran some
benchmarks on this and I was actually
really astonished to see two things one
as we expect it art is actually slow but
it's actually much slower and two this
was a real shock as you could if you're
if you look at those two blue lines on a
15 character string
get string UTF cares actually performs
faster than get string cares how can
that possibly happen because we already
said get string cares doesn't have to
copy the string it doesn't have to
translate the string between utf-8
so something is happening to actually
make on this very short string it's it
actually faster to do all that copying
in translation so let's try a longer
string just just to see if I'm crazy
here so this is a string that's a
hundred characters long and we see more
of what we would expect
so get string UTF cares is now slower
than getstring cares so the question is
why was getstring UTF ever faster under
art so let's look at some source code so
you can see here the getstring UTF cares
always has to copy so what happened what
it does it goes through and it actually
just goes to that copy operation well
getstring cares actually has to go and
check huh well can I actually avoid this
copy so it actually goes looks at the
heap and checks to see if that's a
movable object and it turns out that's
actually somewhat of an expensive call
so you can see here there is this fine
continuous space from object that just
sounds dangerous and what is this
actually doing in real life well it
actually calls this which has a for loop
into it which looks defined continuous
spaces so you can see already here even
though the VM is doing all of this work
to try to avoid this little tiny 15
character 30 byte mem copy it's actually
failing to run this particular case
optimally and so some point in between a
hundred characters and 15 characters
happens to be the break-even point what
does this really really mean is that
unless you're passing very very large
strings around do whatever the most
convenient to you honestly it's it's not
a big deal do you have to do a whole
bunch of crazy stuff in native code to
actually make your code handle you know
tube to byte strings to buy characters
it might or may not be worth it you'll
probably want to look at actually
profiling it so here's our scorecard so
in general yes getstring you
TAF cares is going to be faster for
large characters but not always so I'll
give I'll give it 3/4 of a star for art
here's another suggestion though that's
they came out of there which is use get
string region this is kind of
interesting so here is what that looks
like so normally if you want to copy a
string into a native buffer in this case
and my native I mean you know just
literally a buffer of characters you're
going to call get string cares and
you'll mem copy it and then etc etc and
and you'll see I'm also doing some you
know memory deallocation here just to be
fair on both sides you can see it's
actually several several lines of code
and several more accesses because every
time you actually do something like get
string cares or get string region you're
actually talking to the VM as well well
this is actually kind of cool sorry
you're talking to native code as well
and to the VM so this is kind of cool
here and you can actually use gets
during region and guest string region
does the copy for you that's kind of
nice also one thing I'm doing here which
is a nice little optimization is I'm
actually passing the length of the
string into this and and that's kind of
cool because as it turns out passing
extra parameters into J&amp;amp;I is almost free
it takes literally on the order of a
couple of nanoseconds for every single
additional parameter you want to use so
that's awesome
and if I were going to actually query
this string and say give me that give me
the size of the string that would be
another 300 nano second round trip
through the machine so adding adding
additional parameters is a great way of
optimizing your J&amp;amp;I so i thought i'd
point this out this is sort of a little
minor optimization here but if these
things are what you're thinking about
again you're trying to avoid round trips
on both sides you're trying to avoid
extra calls into the VM or into the
runtime as I should say from from native
code and you're also trying to do the
other way you're trying to avoid extra
calls into native code from the runtime
all right what does this really look
like after all of this well it's kind of
as you would expect
get stream reason is way faster you know
you're avoiding doing an extra
allocation you're also and and so that's
going to in general be good inside of
and you can also see in art this is
actually a lot slower than in dalvik and
a lot is all relative again these are
all little tiny things you might think
after this talk that art isn't very fast
and I don't want you to give that embed
impression you at all in fact our art is
scary fast at doing almost anything but
this so in almost any other way it is
going to blow a Dalek so do do not do
not take this any kind of indictment
against art in fact you can also in one
of them when I was asking one of the
internal guys it is all about we know
why this is the case art actually was
written in a time when we had multiple
processor cores in the system so when
they started designing it and writing it
they were thinking the entire time about
deadlock problems and I would say that
art takes an incredibly conservative or
conservative approach to make sure that
you're not gonna have deadlock I would
and if you if you look in like the list
of bugs on AOSP
you will find deadlock bugs in dalvik
most of which have been fixed but I
think part of what you're part of what
you're seeing is the art team wanted
this to be incredibly robust and that's
why you're seeing a little bit of this
so maybe in the future we can actually
figure out how to make these even closer
together but that's what it's like today
alright so another another big win on
art and a big win on dollar back to use
get string region alright let's talk
about a problem that a lot of people
have which is sharing raw data with
native code and this is also part of
this now if you haven't figured this out
at the talk jan i calls are relatively
expensive and you know again this is
relative you know we're talking about
five nanoseconds for a regular call to
about 300 nanoseconds on a nexus 5 to be
fair of a jan i call so what is this
what are we really talking about the
overhead of a of a one-way call here or
sorry two-way call this is a two-way
call so on dalvik our overhead was about
was a little less than 130 nanoseconds
on art it's almost twice that
and a good thing that devices are
getting faster you can see I've also
been started next to 6p and a nexus 9
both in 64-bit mode and you see they're
actually pretty fast but even the Nexus
9 actually doesn't outscore Dahl back
running on the Nexus 5 for doing these
kinds of things so
J&amp;amp;I is expensive and the real goal of
all this and there's any take back for
this entire lecture is avoid chattiness
every bit of chattiness you add adds
extra time and a lot of that is stuff
you don't even think of so for example
let's say you're like you know what I'm
going to avoid riding a whole bunch of
code have you ever played with unity how
many people here have played with unity
so one of the way one of the ways in
which you talk to Android from unity is
to use something called Android Java
proxy and Android Java proxy is really
cool because basically it takes it in
and just proxy interfaces and it creates
a dynamic class essentially on the fly
that's used to fill out some interface
that you can then use to talk to a whole
bunch of internal systems but you
realize is that by doing that you are
getting the chattiest possible interface
into Android and so if you're trying to
do something over and over and over
again that's going to actually impact
your performance so for example let's
say you're trying to read bytes out of
some out of some class in Java you know
one at a time you realize this is this
is going to very very quickly exhaust
all of your CPU time on the main thread
so you really do have to be careful with
what you do on this and think about the
interfaces you have between your native
code and the VM all right let's go back
over to this this this thing so how do
we how do we actually deal with sending
big chunks of data between native code
and and the runtime and there is this
cool thing called a direct byte buffer
now how many people played with direct
byte buffers here before you pretty much
only ever want to deal with a direct
byte buffer if you're working in native
code there's really no other reason for
them to exist as far as I can tell
although the VM might choose to not
actually allocate this memory out of its
normal page pool so on some on some VMs
but it actually might get you memory you
don't have long way back to us - but in
our run times it does not so
and and you get you get this nice
allocate direct and then when you're
inside of native code you can just get
an address to that chunk of memory and
start writing to it which is really cool
and there's no like I want to free this
there's no like release address it's one
call so that's nice and fast right in
theory so this is what this looks like
when you're using direct byte buffer so
let's look at the performance again
we're looking at benchmarks there all
the time and these things keep me up
sleepless nights doing these and what
that actually looks like and as you can
see once again this is a very very slow
access because direct buffers actually
involve even more synchronization and
and and so but actually we're talking
about something on or running a nexus 5
is almost in the 600 nanosecond range so
you really want once you actually grab
this the answer is you really want to
use it for something if you're using it
to perhaps an integer not a good idea
you want to actually use it to pass lots
of data all right
but there's another side to this once
you're inside of code that's running on
the runtime what's the performance of
byte buffer reversible direct byte
buffer versus regular byte buffer so
let's take a look at that what okay now
now now now the other me let me just let
me just back up a little bit here
because you're seeing something really
really strange here you're seeing that
first of all dalvik Nexus 5 direct byte
buffer is the slowest call by
substantial amount compared to all these
other calls ok it takes it takes 300
nanoseconds the other thing you're
noticing is a direct byte buffer is way
slower than a standard one which is
backed by a standard byte array in Java
so once again two things that are kind
of weird and whenever we see really
weird stuff like this
other than scratching my head it is time
to go and explore some code and try to
figure out why that's the case all right
so here is what actually happens when
you do allocate and allocate direct you
actually get a different class we're
using polymorphism here it's awesome you
either get byte array buffer or direct
byte buffer one of the two okay and
you can see byte array buffer is backed
by an array and DirectBuy buffer is
actually flat backed by this class
called memory block alright so and
here's how and here's how we start
reading an integer we use the call and
get int and invite array buffer it's
pretty standard it actually goes into
another another class called memory
int and inside of inmate side of memory
block we have a little bit we have an
extra bit of indirection we actually
have to call into the block class which
calls into memory dot peak int but a
different call cuz that piquant is
taking a backing array and that other
one is taking an address plus offset and
yes you are actually looking essentially
at pointer arithmetic inside of the
runtime right here not something you see
very often so what does this mean well
when you're actually looking at how this
is implemented if you try to find the
source code this is what you'll see
you'll see probably the most classic
implementation of how to how to pull
data from an array and get it into an
integer that you see inside of the byte
array class and inside a memory block it
actually calls into J&amp;amp;I alright alright
so this is this is know now remember
let's let's go back to this this graph
here so we saw that that art is way way
faster than dolls I could doing this and
yet we just demonstrated looking at the
source code that it's actually calling
in to J and I so that's really weird why
is it so much faster alright so once
once again here's what actually happens
inside of that native code but really
doesn't matter because as we've shown
almost all of the cost of this operation
is going to be in synchronizing between
the different thread States what between
the two VMs so it turns out that art is
actually doing a little trick and that
is when it actually declares the method
it's declaring it with this little
exclamation point on it which is a flag
to the VM that says well this is a
dangerous function actually it's a
flight to the person coding it that it's
a dangerous function it's a flag to the
VM saying this is a very non dangerous
function it's not going to try to cut do
any
in Java it's not gonna last very long so
let's not actually go through and change
the state of a thread at all let's just
run this code as quickly as possible so
once again this is how how long it
actually takes to read that integer from
a byte buffer now it's still about half
the speed even on art of our of our of
our standard byte buffer college even
with all of that even with this fast
switching and that's because what you're
at you if you actually go in and throw
this into a debugger you realize that
that whole statement about where it's
using you know lots and lots of shifts
in order to do it it's actually not
getting run at all it's actually in
intrinsic and so that's how this is is
speeding it up and also even if it was
running that code it's just really fast
like it's really really fast and as it
turns out there is some overhead in
doing even this fast J and I call
because it still has to set up the call
stack and do all the other things that
it would have to do to do it to actually
switch from running in the runtime to
running native and that's that takes
about 50 to 60 nanoseconds according to
my benchmarks just to do that in fast
j'ni all right so is there anything we
can do to avoid having to make a jam I
call for every single int we want to
read it turns out there is we can
actually get it all at once using using
something like this sound so we can get
buffer we can allocate an array we can
wrap that in a new byte buffer and then
we can get that okay and we have to
fiddle with the position because
otherwise there's an overflow there's no
fast call to actually just give me the
contents of that of that buffer that'll
actually work so believe it or not this
is what you have to do and what does
that look like if you do all of that
allocations and this is even including D
allocations and stuff like that and the
answer is of course it's pretty slow
it's really really slow on dalvik you
can you can see like this is this is
where you're started getting into
multiple levels of optimization here but
if you're going to be moving a lot lot
lot of data around big big big chunk of
data and you're going to be accessing
that
from within with inside of the of the
runtime then yes this is a strategy that
might make sense for you if you're like
for example one of the things you might
want to be doing is using like
flatbuffers to move big c structures to
the runtime and how many of you are
familiar with flatbuffers first of all
when i when i say that
alright so flatbuffers are really cool
they're an open source project that that
my team created and and it basically
allows you to do really really efficient
translation from stuff that's coming in
either from disk or from network into
structures that you can use it is about
as efficient as you can get given the
amount of flexibility that it has it's
actually very similar to protobufs if
any of you have used that except that
it's designed from the start to run on
mobile and to run really really fast so
so if you're doing something like that
you might actually get some performance
out of this all right so now since we
have a little time on today show you
just a little bit of how you use j'ni in
Android studio all right so once again
how many of you here have actually tried
doing this in Android studio okay so
that's not not an enormous enormous
number of people but that's that's ok
because this is actually really cool
this made my life so much easier than
that actually trying to then trying to
actually deal with with the various
things that go on into and I see here
here's a whole bunch of native
declarations that are inside of my ji
Bank benchmark class and you can see the
kinds of things you would expect you
know like these byte array calls and
string calls and and so let's I want to
add another native method okay so I'm
going to type native I'm gonna let's
have it return an end I don't have to do
call it J and I but just for consistency
I'm gonna call J and I pass a bunch of
stuff to native and so we're gonna pass
let's say a string a byte byte buffer
a an integer Andale and a long etc etc
and you see a couple of things have
happened here probably the most useful
thing is it we actually now are
compiling the native code and the java
code all in one Gradle build which is
really awesome because we can do stuff
like say hey this function actually
isn't found we can't resolve this so you
see it shows up red it knows that it's
not in my native code so here is the
really really cool trick for anyone who
has done a lot of a lot of j'ni code to
be able to bility to do this is awesome
I can do create function here click on
this and now I have a native function
that's crew that's been created inside
of that C of that C file and this is
really cool not only first of all is
it's it's also done some helper things
for me it thinks I might want to get
this string interesting enough into UTF
rather than rather than double by
characters but hey you know it's
probably what your code wants and then
and it's also gotten the byte array
elements for me and it's released them
at the end because if you're gonna use
it's assuming you're actually gonna want
to use these things and so it actually
puts in that code for you so this is
really really cool and the best part is
now when I go back to my benchmark class
here you'll see it's no longer red it's
actually done the compile and we are
golden we are actually ready to now run
that inside of this class so if you
haven't had a chance to play around with
Android studio and its support of the
NDK I highly recommend it it's still a
little bit of work to get your Gradle
project up and running because you've
still got to use the experimental
version of Gradle but you don't actually
have to use the experimental version of
Android studio it is now in mainline so
go check it out play with it and make
sure that you're not making your
applications that actually use the NDK
very chatty if there's anything you can
take back from this entire lecture all
right so I'm going to switch back into
into non mirroring mode so I can you
know finish all of us all of the
exciting slides that are left in my that
are that are left in my presentation
which is really just this
if you need to get in touch with me this
is how you do it and I hope you've
enjoyed the talk if you learned a little
bit I have time for some questions if
anyone wants to to stump me this is a
really good chance to do it because you
most likely will and but other than that
you know again it's not that scary to
use the nu K it is really cool to use
the new Android studio stuff and you
just have to become cognizant of the
kinds of performance problems you could
create with it and I hope you've got a
little bit of it from this and also once
again in what's wonderful about a
platform like Android an open-source
platform like Android is you can go and
explore the code you can actually
understand how we solve these very
difficult problems in many cases and you
can learn something and take something
back with your engineering career and
and use it again and that that to me is
half the fun of working in an
open-source project I mean wouldn't it
be awesome if everyone can simply say
you know here's the reason why that
doesn't perform well let's go look at
the source code and I think everyone
should be able to do that so I'm super
excited to be able to work on a
development project that actually does
have an open-source back-end so that
being said thank you very much for
coming this morning
and I will take questions now okay I do
not know
it's a really good question you've now
stumped me I I'm so embarrassed that's a
that's okay and yes mm-hmm
so if it is like like let's say it's a
you're doing like Java gets string
critical like that which which would
which would mark that as being in use
that will never be moved that is fixed
in memory direct byte buffers are also
fixed in memory that can't be moved
there is a little bit of a little bit of
weirdness around that because if you
look at the way they're allocated there
is there is a little bit of code that
checks around around moving them but
once you're actually accessing them you
know getting that direct byte buffer
address it gets no it is it is fixed in
memory so it can be moved however
outside of that call so once that once
that call goes away my understanding is
that is that it can be moved so again
it's it's protected for the lifetime of
that call I think that's a really good
question I think I think that's what I
remember and don't quote me on that one
I might be wrong it might be always
protected but what we looking at looking
at the the allocator there are actually
two different kinds of allocation that
can happen and for very very small like
less than three pages that goes into the
movable allocation pool and for things
that are larger than that at least in
the current implementation it's not
movable ever so so so yeah you kind of
yes and no yeah
it depends it depends on whether or not
you so the question is is if you if
you're using a back-end to deal with
this data are you better off and that
and you're talking to C++ code
ultimately you need to get that data in
your C++ code is it better to just use
the networking services that are built
in to the NDK or is it better to
actually use and do everything in Java
and for there's kind of two questions
that I have about this is the
performance of your networking something
you actually even care that much about
that's the first thing if you're on not
on the main thread and you're processing
some stuff in in in native code you
might not care that's a little bit more
expensive because you're not actually
affecting the frame rate of your
application and you might be saving an
enormous amount of time by actually
using the implementations that are in
Java so as a general rule you really
want to look to see whether or not you
actually care about that particular
performance loss you know I think and
then and then weigh it yes for
performance you're gonna do way better
if you parse something in you know and
completely in native code especially if
you're not using it in in on on the on
the Java side of things then yeah you
know that would make sense but the real
question you have to ask is what's the
cost of that you know what's the cost in
terms of opportunity how much more time
is it gonna take me is it really
worthwhile and that's with all of these
things you know that's that's what I say
if it's an easy optimization like let's
throw a couple parameters into a J and I
call by all means do it don't don't
waste more time don't waste more battery
but but if it's gonna mean rewriting an
entire library then we really look
closely at it and say you know how much
am I really gaining out of this
so okay so the question is about j'ni
versus using renderscript you know when
does it make sense so what's really cool
about renderscript first of all is that
render script is actually LLVM byte code
that gets compiled on the device and
there's some beautiful things you get
from that one is that it can be actually
optimized for that particular CPU that's
running on the device to some degree
there are certain kinds of optimizations
you can't do from LLVM but there's a
whole bunch that you can there's there's
intrinsic that you can actually swap in
and out there's there's like people
often do optimizations that are specific
to actually how that particular device
works so that's one of the one of the
secrets of renderscript is that it
actually can generate better code than
the compiler can in some cases second
thing is it's also running in a kernel
it's actually running in its own little
tiny machine that has used to run
massively parallel stuff and it's really
set up to do that very very well so if
your problem space falls into
renderscript you know something that
that's really health paralyzation and
something that is and something that's
also helped by using these intrinsics
that you get from the LLVM byte code
then by all means use it you know but as
a general rule I would say that again
you're looking at opportunity time and
cost you know if you're if you're not
seeing that it's a performance issue
that's impacting you it may not make
sense to to go through that the mean
part of the reason we have j'ni is to be
able to reuse all this crazy amount of C
and C++ code that's out there and so for
me it's like you have to always have to
balance these things but from a true
performance standpoint it is very
possible that renderscript will be the
highest performing way to do certain
kinds of operations because it can just
do a whole bunch of things that the
compiler can't do because it just
doesn't know enough about the system
architecture and then it really depends
on how well the individual OEMs have
actually managed to or chip providers
actually managed to optimize the
renderscript compiler on their
particular on their particular chipsets
so there's there's a lot of variables
here I wish there was a cut and dry
answer but what's great about
renderscript a really cool reason thing
you might want to use it anyway it's
even despite all of that
is because as I said the LOV and bite
code gets compiled on the individual
system so you only have to ship one copy
of the byte code you don't have to you
don't have to use a dependency on the
NDK you don't have to worry about about
about it bloating the size of your bill
with a whole bunch of different
executables and that by itself might be
worth investigating renderscript
just for that one reason now with 64-bit
I believe you actually do need to ship
64-bit byte code so it's not completely
transparent to Architecture I think I
haven't actually tried this that's I
vaguely remember reading that somewhere
sure
so we've you make malloc if you do
Malick's and freeze its separate it's
actually using a different allocator
it's using je malloc when you and when
you're doing stuff from the NDK and
you're using roz Alec when you're in the
the virtual machine and that and the
reason is it and if you went to the talk
yesterday
ra's Alec is really really good at
garbage collecting in the background and
and and we're trying to avoid heap
fragmentation bye bye bye bucketing all
of our memory allocations je malik does
not trying to have everything cleaned up
in the background it doesn't have to be
as parallelized so it gets a slightly
faster allocator than then Roz Alec when
you're in native so so yeah they don't
they don't share space it's been a long
time a thing in Android that if you
desperately desperately need to run
something that couldn't run inside of
the heap space that we give you in the
run time you could always leave it at
native code there's other ways to do
that too you can run multiple VMs like
having it different by by launching a
different each activity into a different
process there's all sorts of ways of
getting around this but you've been
using ash mem as a last-ditch resort of
you actually are completely out of all
the memory we allow you to do that but
but realistically yes they're there
they're entirely separate heaps and yes
oh it's not that complicated it's just
that if you want it's just that we've
changed so the reason I say it on
placated was probably not the right term
to use there it's actually it's mostly
that we've changed the structure of the
way the the Gradle files look so if you
actually look at what we've done we've
we've added this the concept of model
into the experimental version of Gradle
oh sorry you can't see let me let me
mirror it let me mirrored the display I
just did like the dumb Californian thing
here all right so yeah now you can see
what I'm seeing so if you taking a look
at at the peeled Gradle file here you'll
notice that we've added this concept of
model so now Android is not at the top
model is at the top so basically you
need to go through and restructure your
Gradle build a little bit in order to
take advantage of this there's there's
there are some pretty good stuff things
online um you also see like all the kind
of standard things you'd expect to see
in the old NDK build is there you can
actually add libraries here and and also
turn on you know sorry you live bro
static libraries as well as dynamic
libraries here so you know pretty basic
stuff you can see I'm not using any of
this and in that this is also how you'd
build different product flavors so he's
saying building I'm building x86 arm
seven and our mate actually I'm building
all because I have all of these here
that's hilarious
but any case this is how you would do
product flavors but and and dependencies
again just like just like normal
normal Gradle stuff so it's a little
different in structure and but it's
really not hard to set up once you've
actually set it up you want I can even
show you debugging it's really cool I
think actually I mean I feel the shoot
debugging is I think we're out we're
basically out of time but if you want
I'll show you those you know anyone
wants to come to a table out there I can
show you how the debugger works
so if two options
we can google play you can either upload
each each individual variant as a
separate multi apk chunk basically those
are all separated by version codes or
your other option is you can put them
all into one all into one apk and you
can and it will do the right thing when
in that when it actually launches the
applications you've got you've got two
two options it really depends how much
the native code you have and what
percentage of your apk size it is for
some people even having six flavors of
their NDK libraries will only be a
negligible amount of their space for
others let's say you're running
something really big and heavy like like
unity you know it has its own runtime
and all sorts of stuff you're you're
definitely going to have to seriously
consider or distribute on Play so multi
apk is really the way to go if you want
lots of different versions and a native
machine like that and I highly recommend
doing it
you use very translator to actually the
arm code it's pretty fast but it's not
nearly as battery efficient just 86 code
so I highly recommend doing an x86 build
as well and being one of the big things
I hope that we do is make multi apk even
easier to use because right now there's
sort of a partitioning scheme we we
suggest and it's a little bit a little
bit more of a challenge to walk through
the first time on the Play Store so I'm
hoping we actually make that better I
think I think I'm out of time though so
you're more I can totally take questions
afterwards but thank you thank you all
for coming I hope I hope this was fun
and enjoy the rest of your barbeque
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>