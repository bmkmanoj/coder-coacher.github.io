<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Simplify Your Dataset Easily (LIVE) | Coder Coacher - Coaching Coders</title><meta content="How to Simplify Your Dataset Easily (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Simplify Your Dataset Easily (LIVE)</b></h2><h5 class="post__date">2017-02-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K796Ae4gLlY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the livestream is starting soon okay
there it is
Oh world it's Raj good to see you guys
welcome to this live session we're gonna
we are so live right now it's not even
funny
we are so I'm straightly started here I
mean eat myself okay great okay we got
people in the room we got a bunch of
cool people in the room we got Brandon
GE Oh a beer ooh advice Esteban yo hi
everybody
good day actually the pirate okay good
to see everybody
tip on wa radu okay Jon hi whoo Korean I
wish I could read Korean someday when we
have a neural lace and we can languages
not as much of a problem for us okay hi
guys okay we got a lot of people in the
room right now okay okay the society
okay so today we are going to talk about
dimensionality reduction okay we are
going to go deep on this we're going to
actually build PCA
and I've talked about this in my last
weekly video principal component
analysis from scratch okay where that's
what we're going to do and it's it's
very useful we have a lot of use cases
and we're going to compare it to to
other dimensionality reduction methods
we're going to compare it to TS n e and
L da so we are in acronym heaven right
now okay um that's what we're going to
do and we are definitely going to do
some math we are doing some math times
because guys I am the most excited when
we do math because math is awesome okay
and it's not taught well math is not
taught well so that's what I'm here to
do and let's start off with a
five-minute QA and then we're going to
dive right into the math in an ipython
notebook we're going to build PCA from
scratch to simplify our data set so we
can visualize it alright so hit me with
your best questions not meth
laughs okay here we go
come on guys let's see it thank you so
dark time how to do pruning on Alex net
or any deep neural network I am newbie
and no basic Python pruning for the
architecture or pruning for the data I
will do a freestyle um first of all Alex
net is great for transfer learning um
we're going to talk about transfer
learning in later episode are you indeed
yes you like game boy I mean when I was
10 years old when it was that was the
thing I want that hairstyle any tips uh
dye your hair silver right in the middle
take a picture tweet it I'll retweet it
agree the problem is not the math is a
teacher exactly if a neural net can
figure out the right weights for itself
what is the purpose of running PCA
the purpose of PCA is to reduce the
dimensionality of our data so we can
visualize it it reduces the I'll talk
about why and second actually what are
your thoughts on our use Python instead
more modular Siraj are you liable
tensorflow summit it's live right now
yes first of all they didn't even enable
chat guys I would be there if they
enable chat they didn't do that so I'm
gonna watch it later they're just
spewing facts without there being any
community aspect love tensorflow love
you guys but you guys got an enable chat
okay so how can I reduce complexity and
imperfect information games imperfect
information game reduce complexity in
imperfect information game uh please
clarify what do you mean by that
how do you think about generative models
how do you how do I think about
generative models um how do I think
about it I I think about it as a
distribution it's a distribution of
possibilities it's a curve of
possibilities and we give it data and
then we kind of generate we generate new
data based on that distribution and
possibility so it's all of it all of
what is generated is related to the data
that already exists it's not something
completely off they're off the rails
how can ml change a comment a common
thing in my life
how can ml change a common thing in my
life okay guys we are the most important
people
in the world okay we are the most
important community in the world the
people in this community are going to go
on to build the most valuable startups
to make the biggest contributions to ml
if software is eating the world as Marc
Andreessen of andreessen horowitz says
and ml is eating software that people
building ml are the most important
people in the world even if you don't
know that much about ml even if you
barely know anything about ml the fact
that you are in this live stream the
fact that you even watching this video
that you have the awareness to know how
important this is that you have the
desire to learn it puts you in the top
bracket of people okay
we are we're going to be the biggest
machine learning community in the world
okay we are just getting started so how
can it fix a common thing in your life
you have to start learning the basics
and then inspiration will come once you
understand the tools that that are at
your disposal you will find something
that you are passionate about and I
refuse to accept that you don't find
anything that you're passionate about
because you will okay I reject that
notion I found you today okay candy
bunny be used in timeseriesforecasting
type of application like in real time
tool breakdown in drilling oil yeah deep
one can be used for time series an LST M
recurrent neural network we'll talk
about that later where are you I am in
San Francisco what our challenges D
pointing faces facing pret presently um
we need to okay so the major problems
are learning with unlabeled data that's
the biggest problem right now how do we
learn an uncertain unsupervised learning
without labels and have it be useful our
predictions and okay so that's what that
is
the five minutes are over Thanks top 1%
for saying that I'm gonna do a freestyle
and I'm going to use music so someone
shout out a topic a one-minute for
yourself and then we're gonna get
started here we go
someone shout on topic just one word
no got a topic one topic and then we're
gonna go I'm waiting for topic first one
to say a word first first word I see
planets yo I go out to other planet
I leave my sanity when I go there it's
like I gotta be something else I go into
space it's like my face it's so fun
unlace with all these other things I try
to close I try to flow my stage face
until out of control yo when I don't
pack the solar system in the moon I go
Jack like I'm flying like I'm a little
do you new I go out into space when I
come back to earth to my machine running
stays that's what I love to do I do it
every day
I do it my way I do it ok not really I
do it great it's like I'm from Chile
not really I'm from San Francisco II
that's my place where I stay at yo cuz I
do it every day and I do it when I've
you tend to flow okay so that was it my
1-minute freestyle and I used music this
time so let's get let's get on with this
alright let's get on with this um I'm
gonna start sharing my screen and we're
gonna build the we're going to build pca
okay so here we go here we go let's get
started with this screen share and guys
I want you to code along with me so open
up an ipython notebook and start up
coding along with me okay so here we go
okay here we go
here we go so what are we gonna do today
guys what Mimi let me also have my self
up I'm gonna put myself up in a little
quicktime window like I like I do you
know how I guys how I do right guys okay
here I am in the little corner of the
screen and we're gonna get started with
our ipython notebook okay so let me make
myself a little smaller so I fit in
there okay so here we go let's do this
let's do this so what are we gonna do
we're going to build PCA okay and why do
we want to build PCA oh so why do we use
the mention of why do dimensionality
reduction what is the reason for this
what is the purpose there are three
reasons reason one reason one okay I'll
give you a minute to initialize I'm just
writing out some comments so go and go
ahead and initialize your notebook
we're going to do this from scratch
we're going to use numpy and later we're
going to use map plot live to plot our
data it's a reason one for doing
dimensionality reduction is space
efficiency we want space efficiency
right data is huge right there we could
have 500 gigabytes of data but if we
find those features that are the most
important we could reduce the amount of
space our data takes and because not
everyone has terabytes and terabytes of
space so that's reason one the other
reason reason 2 is computing efficiency
ok the less data there is the faster our
model will run right make sense right
the last we have the less data we have
the more the pass or model run and
reason 3 and my favorite reason my
favorite reason is visualization we
cannot visualize hundred dimensional
data when n dimensions are the same as
features they are synonyms ok so
remember that dimensions are features so
if we have a hundred dimensional data
set we can reduce the dimensionality to
2d or 3d and then we can view it and why
do we want to view it so we can analyze
by human eye and it just looks pretty as
well ok we can analyze it by human eye
there's a lot of insights we can make if
we can just look at the data so today
we're going to build PCA okay we're
going to build PCA
we will build PCA
and then compare it to t SME and LBA
I'll talk about these methods so so
three-dimensionally reduction methods
total okay
and so that's we're going to do I'm
going to talk about the math behind this
we're going to talk about the math
behind this okay okay so here we go
we're gonna start off with this okay so
let's let's turn up let me make the
first uh the first import so so we're
going to build PCA okay so here we go
the first thing we'll do is important on
pi we always want to import numpy for
any kind of map that we wanted to then
we're going to introduce AC can anyone
tell me why we introduced a seat three
seconds one two three think about it
okay because it's good for debugging for
debugging it's it makes it makes because
we're gonna we're going to randomly
generate data and for you and we want to
start from the same seed that same
starting point so whenever we test it
the randomly generated numbers are
always the same
okay so that's just for debugging so
it's just it's just good practice
necessary but it's good practice so now
let's go ahead and create two classes
we're going to create two classes and
now we're going to put on our math hat
guys and this is where I'm so excited
because we're going to put on our math
hat so step one is to create our data
set we're going to create this data set
okay and to do this we're going to we're
going to okay okay we are going to
initialize one variable first I'm going
to call it mu back one which stands for
a the sample meat okay so that so I'll
talk about why I'm spraying a meat so so
I'm going to use an umpire to do this
I'm going to say here's your zero and so
this is the sample mean of the data why
do we want the mean the mean is the
average of it that's what it's going to
represent that's what it's going to
represent so it's going to be just a
matrix of zeros and then I'm going to
I'm going to create a covariance a
sample covariance and I'm going to talk
about what these two things are in a
second okay so let me just write this
out and so the sample covariance is
going to be 1 0 0 and
let's see zero one zero so and then
what's the last one we want to fill in
that last blank spot just so we have to
attend beam zero zero one okay so this
is the sample covariance why did I
initialize these things what's the point
of that there is a very important point
because we're going to use both of these
values both the mean and the covariance
to generate our first data set and we're
going to call it class one sample how do
we generate this data set well we're
going to use one of them PI's functions
called random dot multivariate under
sporck normal and I'm going to talk
about what this does in a second but see
these are the parameters we use to
generate that data so let's talk about
what what the hell is actually happening
here and I'm gonna use a transpose of
that and let me print it out and I'm
going to talk about it here we go let me
print out make sure it worked okay is
not defined wait a second what line is
that that is on move back one numpy that
array import numpy as MP named MP is not
defined by about a second to non keyword
arguments are accepted for this okay
let's see let's see what's happening
here let's see let's see
numpy that are a covariance matrix and
pot I got array one 100k okay I see so
there has to be another one of these
okay so good okay so now we generated so
what the hell just happened here right
so let's talk about what this is let's
talk about exactly what I just did
because this is layer algebra okay this
is linear algebra so I generated this
multivariate normal what is a
multivariate normal distribution it's
the same thing as a Gaussian
distribution so let me um so this is a
Gaussian distribution this is this is
what are generated data looks like in 3d
space they're a set of points it's a
three dimensional
a matrix of values okay so there are so
what this is is it's a three by twenty
matrix it's only right that this is a
three times twenty matrix there are
three columns with with twenty rows each
okay so there with twenty rows at what
this is and a Gaussian distribution is
used to create a distribution of
possibilities and we use the mean to
define a Center for it what is that
Center Point right in the middle that
little red center point and we use the
covariance as a measure of how of width
of standard deviation how far off from
the means we want to stretch this data
this distribution of possibilities okay
how far do you want to stretch that and
by the way the word covariance and we're
going to use it a lot covariance is a
measure of how changes in one variable
are associated with changes in a second
variable the covariance how much does
one variable vary in relation to another
variable and why do we even use this
this value because we want to we want to
measure how these two variables are
related and the variables are features
what is that what is that
connecting value between these variables
and we're going to use this to create
our our lower dimensional data and I'll
talk about why in a second okay but
that's what that is
and so that's it it's a Gaussian
distribution curve of possibilities so
that's that's for our data okay um so
okay so this doesn't make much sense so
okay so so g.o we are we are just Jen we
are generating sample data and we are
general we're just generating sample
data and it's a bunch of so this is the
data as we see as we print it out and
it's just across a distribution of
possibilities so if we were to pick any
point off of this it would be like point
six point five point three right so it's
a three dimensional point it's a it's a
triplet of numbers and it's just a set
of those so it kind of looks like this
but that's all it is just it's a table
the word matrix is scary but a matrix is
just a table of values that's all that a
matrix is okay what is the difference
between a three dimensional matrix and
sir um a tensor is an N dimensional
array so a three dimensional matrix is a
tensor so a tensor is the most
generalized form of you know so a four
dimensional major to the attention of
five dimensional eaters okay let's keep
on going
so next that was it for our first sample
and guess what I'm going to do this
twice I'm going to do this twice so I
can basically just a copy and paste this
because I'm going to do this twice I'm
going to create two classes I'm going to
create Q classes and so I'm going to
paste this twice so I'll say two two one
and so this is going to be 1 1 1 so if
there are two different so there are two
data sets and the mean is going to be
different for this one okay so the
starting point and then I'm going to say
class to sample around the bar okay 2 &amp;amp;
2
and why do I want to data sets well we
don't plot them so it's 1 2 let's see
let's see
good okay so got two data sets now right
okay class 1 and class 2 these are my
two data sets and now we're going to
plot them so step two is to plot the
data so we can look at what this looks
like ok and it's going to be much more
intuitive when we plot plot the data so
let's start by plotting the data all
right so we're going to import that file
live so we can plug this data and we're
going to call it PL key and then we're
going to define our variables so bigger
width so the figure which is going to be
and then we're going to use the thicker
side there's going to give us the width
and the height
okay configure is going to be what is
the width and the height so figure size
is 8 8 okay so then we're going to
create a 3d sub block and these so
that's the figure okay so then we're
going to say these are sub plots great
parameters encoded as a single integer
okay so what am I talking about here so
once we have our figure we're going to
create a sub plot and the figure is kind
of a generalized form that we draw our
data in so hold on a second I'm just
going to be really pretty in a second
I'm going to print this out it's gonna
be awesome
so I'm gonna say let's add the subplot
and
the subplot is going to start up so
that's that's what I meant by that word
what is 1 1 1 these are the subplot Rick
Rude primers encoded as a single integer
so what this is saying is 1 1 1 it says
so it's a 1 by 1 grid and we'll talk
about the first subplot so this could
have been you know anything else but
that's what it is it's just defined the
rules for our our our plot what do we
want our plot to look like ok so it's
going to be a 3d plot and we're going to
say and you know what let's see yeah
let's keep going here so then the font
size is going to be us to find the font
size the font size these are such you
know little trivial things but they're
necessarily right we want to make sure
that our data looks good
right whether we're presenting it to
people whether we are looking at it for
ourselves we want our data to look good
and we're going to plot our samples now
now that we've defined the rules for our
graph now it's time to actually plot our
data so we'll say ok so let's plot it
using the plot function and we're going
to plot the first sample and you know
what this you know I can just paste this
this part in because it's not as
important at the machine learning that
we're about to do so let me just put we
just I took that advice before some of
them so so here's what we're doing were
plotting both samples class 1 and class
2 okay and we're going to label them one
is going to be blue and one is going to
be green and there one is gonna be level
class one and one is gonna be label
class - we're gonna have a legend and
let's show the plot let's see
hope this works boom okay so let's see
what happened here it said can't assign
to literal so what that means is TLT to
RC params equals legend dot font-size
can't assign to literal okay legend dot
font size let's see what's going on here
in a plot why I lab in line let's see I
want this to be in line by the way let's
to the top of me
so act stuck figure add subplot
projection 3d figure size 88 PLT figure
RC params
that what do we got going on here RC
parameter P LT can't assign to literal
interesting so what's uh oh yes a equal
sign yeah that's what it was okay
populating here from numpy and mom
taught live okay let's see what's going
on
uh none projection 3d unknown projection
3d why do we have an unknown projection
of 3d let's see we have an unknown
projection of 3d because see what we've
got here I will look I will look I will
look I am looking now RC params equals I
do look I am looking okay uh so let's
see bigger size and then also knows the
error again unknown projection 3d so
your dad's sub time let's see what's
going on so far
alright let's just go for this boom
unknown projection 3d let's see what's
going on but I didn't do port maps lot
like 3d in my earlier code so it's
interesting I mean what I would do at
this point is probably google it and
Stack Overflow but I'm going to see what
is going on let's see we have um capital
D capital D for what so oh is that what
it is is that what it is
no it's not no it's not capital D its
lowercase D ah so
so okay so this is actually there's a
lot of noise no that's cool
no it's fine it's fine it's fine so it's
just gonna it's a it's saying uh none
protection three let me just show you
this I'm going to keep going we don't
actually um so okay let me just show you
what this looks like
let me show you what this looks like
here's what it looks like check this out
check so when we do this this is the
projection in 3d this is what it looks
like we have two classes in 3d this is
what it looks like in 3d space okay our
data is valid it's just the it's just of
tools and that's what it looks like it's
in 3d space so we're going to get
started with the math because that is
the important part this is what it looks
like in 3d and now we're going to
actually take our data and we're going
to look at it all right so let's let's
keep going here okay so we have our data
the data is valid it's fine
okay let's let's keep going let's keep
going so now what so that was step two
so now we're at step three so step three
is we're going to merge this data into
one data set so would you merge the data
into one big-ass it is that I hadn't say
Asuna okay so that's what we're gonna do
so how we're going to say all samples
equals we're going to use numpy z'
concatenate function to do this contact
mate function to do this we're going to
say take our first class sample take our
second class sample and on one axis
we're going to merge it so it's going to
be accessed equals one so it's all on
one axis what does that mean one access
well let me let me write this out and
then we'll show what ship will show it
it works like we'll show it one axis
looks like this is now what our data
looks like it is now a three by 40 data
set it's a three by 40 data set and the
Suraj Raval company is not taking
investments right now we are focused on
our media division and I'm also focusing
I'm going to start building our research
division which includes you guys later
on but right now it is just media we
will be the best media world ok back to
this so all samples that's what it looks
like 3 by 40 I said ok so that's our
data
so let's gets and also this is what the
transpose looks like by the way because
we're gonna be using this T you're not a
dot T at P n we're going to be using
that a lot what does that feel it gives
us a transpose and what does that do it
just flips our data it just covers it
flips our matrix okay so starting
horizontal don't be vertical and that's
just good for looking it's it's good for
sometimes formulas require the transpose
which they will but right now we're just
doing it instead you don't have it look
aligned so we can without having to
strike our window so now that was step
close out of those step three now we're
on step four so step four a research
division okay so step four is to compute
the d dimensional mean vector what is
that why are we we're going to compute
why are we getting the V vector it will
help compute the covariance matrix so a
lot of of this process of dimensionality
reduction for PCA and in general is to
compute these little pieces they're all
little pieces of a puzzle and we use
each piece to help us find the answer to
the next piece it says process of
discovery it's an incredible process of
discovery each of these are pieces to
the puzzle and eventually it's going to
lead us to something that is amazing so
let us try to find a mean for each piece
right so by itself this doesn't seem
like it's that important right but when
we use we're going to use it to compute
our covariance matrix so the mean for X
we have three features right radius numb
me function to do this we're going to
say take all of our temples and just
that first row and that's gonna do
dimensionality reduction is taking a lot
of updated with a lot of features and
squashing it to just two so instead of a
hundred features we have just two and
that's great for a lot of things so
we're going to keep going with this all
sample so I can just copy and paste this
and I'll just replace the numbers right
to mean for X we for y v40 we're taking
a three dimensional data set and we are
squashing it to a two dimensional data
set the covariance
is the measure of I gotta keep writing
this so should one how to variables vary
in relation to each other right and so
now we're going to so now we have to
meet what is the mean to mean it's
average value from all of those values
so what is the average if we add all the
values together and divide by the number
of values it gives us the average value
so we want the average value or mean for
each of those features and I'm not
picking the uh I'm not picking which
features I want measure so it depends on
the method but for uh so for principle
component analysis it picks those
features that it finds to be the most
relevant and we're going to talk about
that I'm going to visually show what I'm
talking about in a second it's going to
make a lot more sense
stay with me guys okay stay with me we
are computing the mean right now so now
we're going to take this 3d mean vector
so we're going to create one big vector
out of this so it's called a mean vector
it is one array I'm going to print it so
we can see what it looks like I'm going
to print one array and mean Y and then
mean C okay so those are our three we
create one vector out of this and then
we can just print it so print mean
vector let's see what it looks like okay
putting Rama with this equals okay so
this is a oh right there we go boom come
on second NPRA just mean X right close
that close that bracket invalid syntax
on line eight what what is line eight oh
this one okay remove that yep this is
our mean vector see these three values
are what do I mean by squashing it okay
so okay so um okay so if we had a
hundred features if we had like if we
had a data set of pokemons okay listen
we had a pokemons data set and there are
a hundred features the length of it the
fire breathing capability of it the
blank of its tail who it loves its
gender all of these features by squash
I mean give this huge hundred
dimensional data set of all the
attributes of a Pokemon we give it to
this algorithm like PCA and what it's
going to do is it's going to squash it
and what that means is is going to take
those hundred features and find the two
most relevant features from that data
but it's not the same features that
exist there are two new features there
are two new features that they don't
they don't they're not cool they're not
like a word they're not like they're not
like oh these are the two features tail
and head size they're entirely new
features they are entirely new features
that they just represent and I'll talk
about this in a second so let's keep
going with this so now we've computed
the mean and now we're going to compute
the covariance matrix using this meter
so let's talk about the covariance
matrix with a little uh with a with it
with a image okay so I have this
interactive website that I found I'm
going to I'm going to link to you guys
so you guys can see this okay this is
the one right now if this is not the one
what is the one let's see what is that
what is that covariance covariance
matrix an R this is a great link for it
okay so let's see this so this is also a
great link let me let me paste this as a
pin as a little learning aid so check
out that link but okay so what are we
doing right now let let me get back to
this we are computing the covariance
matrix now the covariance matrix models
the relationship between our variables
and in our case those are features it is
a matrix of values that model it models
the relationship between all the
different features and what it's all
going to come together
remember these are pieces to the puzzle
it's all going to come together it
measures the the relationship between
all these variables the word variance so
a variance is the degree by which a
random variable changes with respect to
it's expected value that's that's
variance variance is a word for
describing a value in relation to itself
with nothing else in common just itself
but covariance is a degree
the other side is one so covariance is a
degree by which two different random
variables change with respect to each
other it it measured the relationship
between each features that's coal
bearings or how are they both going so
let me let me write this out now uh so
it didn't work let me uh okay yeah let
me just there we go yep so let's try
this again for now would it work it'll
work
I'm too gloppy laughs yeah eventually
not yet I'm not popular not enough but
works not great great awesome okay so
let's let's do so this is calculate the
covariance matrix so let's calculate the
covariance matrix so we're going to
start off with an empty array of zeros
okay it's an empty array a 3x3 array of
zeros and we're going to use them isay
respond to to do that so we have an
empty array of zeros now we're going to
say okay so for each value this in our
data center for I in range of all right
great of all samples dot shape we're
going to say take that covariance matrix
and now we're going to add our value
Strickland what are these values look
like so according to Emily I sent you
guys in a covariance matrix it's not
just a covariance it's the variance and
the covariance that's what it looks like
there is a specific formula as you can
see here for creating that matrix and we
are going to programmatically write down
this formula so it is the same set of
rules a formula is a set of rules that
is always the same so we're going to say
we're going to keep continuously add
value to our covariance matrix and we're
going to say for all the samples
starting with the ones in this row and
we're going to iterate through the row
start in this column we're going to
reshape and reshape there's a new shape
to an array without changing its beta so
it just gives us a new shape so it's all
in the same shape - a mean vector and
now we're using the mean vector so we
take those features and we subtract the
mean vector and then we compute the dot
product of it and then one more value
and so the dot product is
the dot product is the when we take two
matrices and we multiply them together
and why do we multiply matrices together
so we can so it gives us a more
generalized value and is actually
there's actually a lot of reasons why we
would multiply two matrices together but
in this case we're doing it to give us a
covariance matrix a mean vector is going
to help us compute that relationships
let me just cut Excel all samples okay
natural just paste this so then I can
talk about it please great okay
this is our covariance matrix it is a
three by three matrix okay we use our
mean vector to help us calculate this
and we computed the dot product between
every feature - the mean feature - a
mean vector with every other feature -
the mean vectors transpose why do we do
the transpose that is a part of the
equation as you can see here the
transpose let's mention see this T it is
the same set of rules every time okay
now here is my favorite part now we're
getting to my favorite part why do we
compute the covariance matrix now this
is the now the pieces of the puzzle are
going to start coming together now it's
going to start making more sense why we
computed the mean vector to compute the
covariance matrix to compute the eigen
vectors and eigen values that's what
we're going to do now we're going to
compute the eigen vectors and eigen
values okay so that's step six step six
is to compute the eigen vectors and
eigen duck eigen values why can we do
that what is the point of an eigen
vector and eigen value and what what
even is that word well first of all let
me just say I can vectors and eigen
eigen values are used throughout all of
engineering it's not about computer
science it's used in electrical
engineering it's using physics it's used
in the Google page rank system eigen
vectors and eigen values are used
throughout engineer and know so here are
these what the system let me let me give
you a quick summary of the service of
what
so this is actually a great interactive
link to definitely click on this link
I'm about to paste it into the chat and
it's also going to in the comments for
people who are not watching this live
check out this link okay
so eigen vectors make understanding
transformations easier okay
they are the ax axes the directions
along which a transformation acts by
stretching or compression and eigen
values give you the factors by which
this compression occurs okay so yeah so
basically there's a lot of problems that
can be modeled with a linear
transformation but it gives you a sense
of directions for so so that's it that's
a general description so eigen vectors
give you a sense of direction as to
where this data is going so for me let
me show you this so you can actually
move this around this is interactive
eigen vector is German exactly so see as
I move this these sets of points so this
is it this is a very this is a very
intuitive explanation on this page but
essentially an eigenvector is a
direction that some data is going and
what do I mean by directions direction
in this case is like so there's actually
a there's there's actually another um
link that that also helped me understand
this then I want to paste in here Shh so
hold on a second so um all I said so
principle but I think this is one no
there's another one so basically or I
think this guy I have it okay so this is
a great explanation so so check out this
point let me show you there I'm sure
this this length this is an amazing
amazing link this is an amazing one okay
check out the
points okay there's a bunch of different
data points they're a bunch of different
data points and we could just we want to
find the direction of this we want to
find the principal components of this
data what is the principal component
what do I mean by that well if we just
draw a random line in this data it's
gonna all the how far what is the
variance how far is this data from this
line and we can draw a red line between
every data point and this line and so
that that's cool
but if we draw a horizontal line it
shows them the if the data is even more
spread out and what why is that good
that that means that this is the
principal component of this data this is
the essence of this data this is that
point where if we were just given this
line if we were just given this line and
some and some distribution we could
recreate the data points that's what I'm
trying to say we can then recreate the
data points from this lower dimensional
idea this line and this line in this
case is the principal component and the
direction for where should this line go
where what should will be horizontal
should be vertical that's what the
eigenvector gives us it gives us a sense
of direction as to what is that point
with the most variance between data
points and we want the most variance
because that's what lets us create these
new lower dimensional features from this
data so that we can recreate the data
with these lower dimensional features
okay um so that's what that is so where
were we so let's let's compute these
eigenvectors and eigenvalues all right
values and eigen vectors eigenvalues and
eigenvectors and we're going to use
numpy linear algebra functions I've in
sub function to compute them a very
useful function to have okay so for
every value we have let's print out all
of those so that's actually good to get
close we could just print them out we
just print them out to print Augen value
and print eigenvectors so let's just see
sighs okay oh okay all right so let me
print this so there's a space between
them so let me say what this is so these
are our eigen values and eigen vectors
or eigen vectors give us the direction
and our augen values give you give us
the magnitude of that direction these
two values are intrinsically correlated
we need both of them okay and so we need
both of them so what are we going to do
to to combine them together what we are
going to uh we're going to create um an
eigen pair create eigen pair from both
and sort them so an eigen pair is if we
were to combine both of these odds
okay what do you mean by the direction
of what did I mean by the direction of
the data so so like like what is that
point where in the lowest dimensional
space we could then recreate all of
those dimensions it has to be this what
is the optimal what is the optimal line
we could draw what is the optimal set of
features we could have such that we
could given some formula from these
features we can recreate all of the data
with the most accuracy the direction of
where we should go negative positive of
of should you know what should it say to
look like that is what the principal
components that's what the eigen vectors
and eigen values give us and that that
that lowest dimensional point is the
principal component and we're going to
we're going to calculate those in a
second so let's let's let's create our
eigen pairs put our eigen pairs or eigen
pairs are going to be Rickey's num pies
absolute we're going to use the absolute
value of that and back
just just paste one in this one can be
pasted so we have just writing pairs
this should be onion values now let's
talk about what's happening here I now
use this see if it works so we get we
made a list of eigenvalue eigenvector
two bolts two bolts right because there
are three dimensions and we use the
absolute value function to make them
positive why do we make them positive uh
because it doesn't matter if they're
negative or positive is it just making
them positive makes it easier because we
want to deal with any kind of it's the
same as like the idea behind the mean
squared error the reason we square it
because it doesn't it's just a magnitude
a times it's not if it's positive or
negative it's just the magnitude that
matters okay so then we sort them with a
sort function and then we reverse it so
that it's going to be in descending in
decreasing in descending order and these
are the values of the eigen pairs if we
were to combine both of those values
together this is what it gives us and
these three values are the most
important values we are looking for I'm
going to post this code yeah let me let
me let me paste this code I'm pasting it
dad totally that's a great idea let me
let me do that um Evan on github um and
I haven't actually commented it yet but
I'm going to so let me just a second and
then I'm gonna so here's here's what's
happening guys okay let me paste this
code in and then we're gonna keep going
check this out boom all right so that's
my messy code I haven't written readme I
will in a second okay so those are our
ID in pairs so now we've done that so
that was stepped that was step seven
actually another that was step 7 step
seven was to sort of explore them okay
go step seven and now we're going to do
step eight now we're going to choose the
largest one so step 8 is to we're going
to stack we're going to choose
kiv vectors we points we want to be by K
eigenvectors so what is the
dimensionality of what these are
dimensionality that we want here so
we're going to create a matrix and we
have one more step after this so stay
with me guys we have one more step after
this okay so we're going to create a
matrix and we're going to create a
matrix and we're going to say for our
augen pairs 0 1 shape them 1 boom we're
going to stack our arrays horizontal
that's what we're doing right now we're
stacking our erasing sequence
horizontally that's what appreciate
every day we're gonna stack them
horizontally and how are you and why are
we doing that we're using how are we
doing that were using the H stack
function okay by the way guys all of
this can be implemented in a single line
of code in a single line of code which I
get to learn but we are doing this from
scratch with the Mac because we are
awesome and we just want to know how
this works okay
it doesn't never hurts to know at least
the general idea of how this works okay
because we are awesome so now we're
going to talk about reshape 3-1 okay
print matrix W I bet you there it's
going to be in there in a second but
that's okay because eigen pairs eigen
down Hue's let me just
it's that entire thing okay so this is
now H this is now a two dimensional
matrix it's a two dimensional matrix
with three values H we created this
using our our eigen
I get values tuples this can absolutely
be a data science interview problem yes
yes it can be so now we are on our last
step we're on our last step so step nine
is to transform our data is to transform
our data using this up again pair matrix
using this eigen pair major we're going
to transform our original data set so
now the pieces of the puzzle are coming
together if it the piece of the elements
will clarify McHale it's not that
they're going to ask you a CODIS all
from scratch but probably ask you the
high level idea like deep mind is known
to just lightning like lightning
interview like a hundred questions write
about machine learning just off the bat
like what is PCA like what is this what
is this okay so we want to transform my
data using this eigen pair so we're
going to say take our transformed with
our transformed data and actually the
transpose of it times a dot product we
do the transpose so it's going to be the
same shape and so remember are all
samples data that was our original data
set we are computing the dot product
between our original data set and this
new eigen vector or eigen pair a matrix
and by computing the dot product it's
going to give us a two-dimensional uh
two features instead of three features
because they must two features instead
of three features and we can use these
two features to plot the data we're
going to plot it in a second okay so and
then we'll print out the transform
that's it it's just one line of code it
was just one line of code one line of
code this is our new data sample we just
performed principal component analysis
we now have two dimensional data instead
of three dimensional data now instead of
writing out all this code unnecessary
code just to print out I'm just going to
paste it in so this is what it looks
like now this is our now this is our two
dimensional data instead of our
three-dimensional data for our classes
okay
it is now two dimensions instead of
three we just performed principal
component analysis by hand now instead
of writing out e now instead of writing
out GS any and Lda from scratch let's
just talk about them and compare them
because that's the really important bit
and then writing out the math for those
two methods is going to take quite a
while so let's talk about a comparison
okay so we're going to compare PCA
versus t-sne versus LBA these are the
three most popular dimensionality
reduction techniques out there these are
three most popular PR techniques out
there okay let's compare them PCA done
okay so what are the pros so why do we
use piece of PCA oh so basically I can
sum it to this so for the best
visualizations this is this is just
something to remember too if you were to
take something out of this section know
that so the best visualizations a chart
let me start off with this the best
generic dimensionality reduction method
is drumroll please PCA there's a lot of
reasons for this there's a lot of
reasons for this but the best generic
dimensionality reduction method is PCA
but if we have supervised data or sorry
if we have a supervised it but and that
that is for generally for supervised the
only for unsupervised data which is most
part it but the best method for
specifically for supervised data that is
data that has labels is drumroll Lda now
what is l ta l da Center Roselle da okay
is linear discriminant analysis linear
discriminant analysis now with another
apartment building but it's the same
steps as PCA so it's the same as same
steps as PCA except for one difference
we compute the mean vectors for the
different classes instead of take the
whole data set remember how we got a
mean vector for the whole data set
I'm doing that will compute mean vectors
for each of the classes will compute
mean vectors for each of the classes
it's the same except we compute mean
vectors for each of the classes we
compute mean vectors for each of the
classes so that lva now what is the best
for visualization me it is TSN
it is utilize a data set easily but
essentially there are three steps to
t-sne okay and there's also a great link
that I'm going to send you guys in a
second to learn about this okay uh t-sne
and it is an O'Reilly page this is the
best explanation for t-sne that I found
on the web besides me so check this out
okay so we compute a similarity matrix
between all the feature vectors and then
we compute a similar matrix for the map
points which are the projected points is
where we want to be is if we want we
have a hundred menschell data and we
want two dimensional data that'll those
are our set of map points and basically
we use gradient descent to minimize the
distance between two matrices and that
resulting matrix is our lower
dimensional data that is it at the
highest level possible now we need for
data visualization generally it
generally gives us more accurate data
visualization than any other method but
in general remember those three reasons
I gave at the beginning I mean I mean
let me uh stop screen sharing and go
back to me but in general we want to use
PCA that is our most as our most uh gen
gen Eric dimensionality reduction
technique okay so that was it let's
let's do a ending up 5min acuity and
then we're out of here hi guys and by
the way thanks for watching you guys are
awesome and also um I assume app uses K
nearest neighbor it does yes I so map is
another method there's actually so many
dimensionality reduction methods are so
many out there but what I've done is I
picked the three that I think are the
most important and the most popular okay
i don't--are Moy please increase the
frequency of live sessions per week um
I'm going to increase my video output in
general um that's what I'm focused on
right now
guys your Udacity nano degree is too
costly bro yo I try to get it for free
but look grading is not cheap they
didn't have the whole team these people
are awesome they they have to pay them
they got to feed themselves you know so
not all of them use you know what are
like me crooning on Alex net
architecture attached let me get back to
you on that on the slack Channel okay
I assume app increases the
dimensionality okay so can we have rap
again okay how would you wrap again does
the art improve the speed of training uh
yes yes it does because it's less data
it's less data so then because there's
less data the time for model is less
okay where can I learn data science my
channel watch all my videos I put my
life into this channel and I'm just
getting started guys we are going to be
the biggest machine learning community
in the world and I'm talking to you we
are the most important people in the
world the biggest companies are going to
come out of this community we are here
to help each other we are here we're
here to help each other learn and grow
and get better okay we are the future
okay and I do not accept any of you
giving up okay you ask questions in the
comments you ask questions in the slack
channel you ask questions here but you
do not you do not give up not one here
okay and you you believe in yourself I
believe in myself I have failed so many
times guys I have failed so many times
at so many things I have been rejected
from jobs I have been fired before I
have been so many things and yet I just
kept going and that's what I want you
guys to do okay we are here to to
progress our species and to and to just
make the most awesome stuff possible
okay we need to publish papers we are
going to publish papers we're going to
make startups we're going to do so much
amazing so many amazing things you're
just getting started you're just getting
started we're in sixty six thousand
strong community of machine learning
engineers and and the machine learning
subreddit is 80,000 we're gonna hit a
hundred thousand by April first mark my
words we're gonna hit a hundred thousand
which makes us the biggest machine
learning community the world okay so I'm
pumped for this and I hope you guys are
as well because
are just getting started and do not tell
yourself you can't do this don't ever
tell yourself that you can't do this I
refuse to accept that the people who
actually make valuable contributions
believe in themselves so step one step
zero remember all the steps that I just
forgot step one do this that step is
believe you can do it that step zero
okay so remember steps Europe how do I
join your side channel I'm going to play
it in a second how do I find such
excellent sources for understanding
topics on the net I what it looks a
better answer than I just google through
I just go through all the results in
Google like to like page five
I basically curate data for you guys so
I'm looking through all the to find
the best sources and then I talk about
it except we were going to make a
difference I love you guys too okay so
that's it for the slides let me do one
more question
oh yeah I said I would do a rap right so
someone's right up someone threw out a
um were a topic but okay guys so the
robot code guess I talked about the
robot last live session and I even
bought I should have brought it with I
bought a robot kid off of Amazon and I'm
making this new video and I was going to
and I had I had the script for it but
the problem was that um and this was my
feedback from Udacity and they were
right in this video that I'm gonna
release on Friday I'm going to say I was
saying a how to build an Arduino how to
talk about motors how to talk about
convolutional nets I try to put too much
into one video so I'm taking that robot
and I'm not gonna do it in this weekly
video I'm just going to talk about
convolutional neural networks but I'm
going to make a robot its own video in
the future okay it's gonna be I already
bought the robot kit it was 90 bucks on
Amazon I'm committed to doing it it's
going to be its own video okay so to
wrap on tensor flow okay okay
tensorflow yo here we go yo I don't even
see let your different be here we go on
this our ending wrap here we go yo what
is this beat sucks for this yo yo here
we go
I love tensorflow that's how I roll I do
it every day because I'm in a community
so of 3600 song machine learning and
is I'd you it meant I sit back and I
drink a beer and coffee and all of that
stuff like I'm on me I keep going man
I'm like zombie I wake up every day I do
machine learning and then I go to sleep
because I'm yearning to do
something else
not really I just do what I want man cuz
I'm really pretty and really it doesn't
even matter what I say because I know
all of you are gonna go out and do
something ok but that was it rap I'm
gonna end it now going back down town
and I'll take a bow ok that was it so
alright guys thanks for showing up in
this live session um and I love you guys
and I'm gonna post all the links and
within the hour like always I love you
guys and definitely watch the tensorflow
live desk summit as well later on in the
day or right now I know I'm I'm going to
so for now I've got to go edit this
video and uh think about this other
video that I want to make on the weekend
so thanks for watching Oh in the mic
drop
where's the mic what do you have a mic
on whatever see you but</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>