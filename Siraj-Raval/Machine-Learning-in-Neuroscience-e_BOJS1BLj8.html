<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning in Neuroscience | Coder Coacher - Coaching Coders</title><meta content="Machine Learning in Neuroscience - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning in Neuroscience</b></h2><h5 class="post__date">2018-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e_BOJS1BLj8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and neuroscience
and machine learning two different
fields that are actually very much
connected today I'm gonna talk about how
neuroscience and machine learning are
very connected I'm gonna talk about how
recent advances in neuroscience will
move machine learning forward and before
I stalk about that I want to show you a
demo and this demo is an interactive
network visualization tool for exploring
functional brain connectivity it's
called spectra vis and after this video
you're probably going to want to look at
more neural imaging techniques and this
is a great way to get started I found
this repository on github and basically
what I've allowed you to do is simulate
a brain in silico but it's a very you
know it's it's it's not the full brain
obviously but it's a it's a good model
of how neural imaging tools can be
simple and it was created using d3.js
very cool visualization library you can
hit start and you can see the network
just you know move it's very it's very
cool but I just wanted to show you that
really quickly and you can you know
switch the timing the frequency of
connections all these different things
you get you get all of these different
mappings of how the neurons are moving
how they're interacting in the network
it's pretty cool so check out that after
this the link to that is gonna be in the
video description but let's get started
with this I've divided this talk into
three different parts the history of
neuroscience in AI contemporary
neuroscience in AI and the future of
neuroscience in AI okay so let start off
with the history and obviously I've got
this super generic image that is shown
everywhere but it's the idea of this
image is saying hey we've got this
neural network and what it was inspired
by is the human brain okay so at the
dawn of the computer age when computer
science was was invented when it was
starting up all of AI research was
neuroscience everything was based off of
neuroscience it was the same thing every
single discovery came from neuroscience
when it came to AI and the early
pioneers of this field straddled both
neuroscience and AI there was they were
there were connected to both
collaborations between all of these
different fields was highly productive
so let's let's look at some examples
here how can I talk about computer
science without talking
toreen right Turing was the man Turing
test okay he invented the Turing test
invented a bunch of stuff but he's in a
lot of ways the father of computer
science but the idea for behind the
Turing test was let's see if human can
tell if he's talking to either a human
or a machine and if he can't tell then
the computer has passed the Turing test
right Turing was very very very much
into the brain he said we are not
interested in the fact that the brain
has the consistency of cold porridge
that was his quote but what he meant was
he was very interested in how the brain
work he was very certain that it was a
very complex organ and his view is
shared by many in that we cannot
represent the brain using anything but
math because it's such a complex system
right think about the brain has a graph
right thing about think of it as a graph
and if you think about as a graph you
have these neurons nodes connected to
each other it's this combinatorial
explosion right and in terms of the
search space when you take a signal and
you propagate it forward it's gonna be
split up into hundreds if not millions
of different signals for different
neurons and this is a combinatorial
explosion right there's no way to
describe this via colors or you know
regions other than by sheer beautiful
math right and if you think about math
right there's this idea of this this
mythical book the book of math that when
you die God is able to show you this
book and it has all the theorems for
everything listed out right and so when
you die you're able to see oh this was a
theorem this for the theorem and over
time we humans have been able to
discover some of those theorems but
there are a lot more we haven't
discovered now imagine a book of AI
write in a book of AI neural networks
and deep learning would be one of those
theorems but there are way more that we
still have to understand and to to find
write to discover neuroscience is a
great way to do that right so Turing he
had this computer program that he
invented at the University of Manchester
that accepted a single input right a
single numbers input and output a single
number and he wagered and this is
another famous thing by Turing the
Turing's wager that it would take
someone a thousand years to figure out
how that blackbox worked how that number
converted from one from one into
integer to another and then he said he
extrapolated from that idea and he said
well if you think about the brain as
that black box it's gonna take a really
really long time to figure out how that
works
and he's so he was very skeptical that
we would be able to figure out how the
brain worked even though most his work
involved trying to recreate the brain in
silico but if turning could have just
seen the modern supercomputers that we
have the advances in neuro imaging
techniques that we have he would have
been blown away right so yeah it's it's
definitely possible to recreate the
brain in silico but let's keep going
here so McCulloch and Pitt's to other
neuroscientists right they invented the
model of the single neuron now this is
where it all started they wanted to
create some sort of mathematical model
that represented how a neuron works and
this was a very basic model the model
said this let's take some inputs write
some some set of numbers X like this
represented here let's apply some sort
of function to that an output is one
single integer right that is something
like what a neuron does it's accepting
these inputs these signals right it's
doing something to those signals and
then it's outputting a signal and that
signal propagates to other neurons and
that creates this network this neural
network of neurons right and then they
kept going right so then hopfield took
took this idea and he said let me take
the McCulloch pitts neuron let me
combine them together so the outputs of
one neuron are fed as the inputs of the
next neuron and let's just keep doing
that over and over again and so this was
called the hopfield network right so
this was the first recurrent artificial
network he connected McCulloch Pitt's
neurons together then came Hinton right
so Hinton said oK we've got these layers
of neurons what is the best way to
optimize these neurons so that it can
complete its objective right and so he
invented with his colleagues the back
propagation technique which said we've
got some output prediction let's compute
the error based on their what their
label is that's the error value let's
compute a gradient and the gradient says
how much to update the weights of the
network over time let's update those
weights incrementally backwards in the
in the in the reverse direction as we
forward propagated the input data and
this is called back propagation
and Hinton was also a huge proponent of
a neuroscience level of thinking when
applied to neural networks in machine
learning right during the AI winter when
everyone said he was crazy he stuck to
the idea of neural networks being a way
forward in AI and it turned out in you
know I've seen in the deep learning
revolution of late that he was right he
was also a big proponent of neuroscience
inspired ideas like dropout turning
neurons on and off randomly to increase
the generalization ability of a network
of nonlinearities to allow a network to
learn both linear and nonlinear
functions the idea of hierarchical and
layered networks etc etc and so all of
this leads up until now the problem with
now is if you were to if you were to
learn machine learning right now if you
were to learn about all the different
types of models support vector machines
random forests decision trees it's not
very clear that all of this these things
are inspired by neuroscience and in a
lot of ways they're not so there's this
disconnect between neuroscience and AI
that didn't exist before and what's
what's happened is because the
discoveries in neuroscience have just
grown and grown and the discoveries in
AI have grown and grown over time these
two fields have become so complex they
it's hard for any one person to become
an expert in both fields let alone one
so it's hard to make any discoveries
that that intertwine these two but the
human brain is the only proof we have
that this type of intelligence works so
it's it's it's yes it's hard but it's
essential it's essential to do so
Maur this other this this the scientists
had this idea of describing a biological
system in three different parts so one
part was the implementation the hardware
of how you know this this software is
running this wet where you can call it
then there's the algorithmic part what
representations can implement such
computations and on top of that is the
computational part why do things work
the way they do equations on top of
those initial algorithms it's hard for
us to work on the implementational part
because creating hardware requires a lot
of capital right not everybody's got
that money so we need to focus on the
algorithmic and
computational parts and anyone with a
laptop and GPUs in the cloud so this is
being democratized can do that so now
let's talk about contemporary
neuroscience okay and and how it's
played into AI so there are four key
areas in AI that have been affected by
contemporary neuroscience the first is
attention okay so the brain does not
learn by implementing a single global
optimization principle within a uniform
and undifferentiated neural network what
I mean by that is it's not like we have
some giant LS TM network running
everywhere right it's not just some
giant recurrent network we have
different modules that are good for
doing different things so one part of
the brain could be doing something like
a hopfield network and another one could
be an LS TM network and another one
could be you know a self-organizing map
and so there are different modules that
are good for different things and so up
until now for convolutional networks in
particular which are networks that are
good for working with images the they
work directly on entire images or video
frames with equal priority given to all
the image pixels but we learned that the
primate visual system works differently
rather than processing all the inputs in
parallel visual attention shifts
strategically that's the key point here
strategically among locations and
objects centering processing resources
and representational coordinates on a
series of regions in turn and so recent
AI discoveries have implemented this
idea of an attention mechanism of
focusing on the parts that are most
specific and usually the way they do
this is by having some weights for this
attention mechanism that are updated via
gradient descent over time and so there
are differentiable weights and so the
network learns where best to to focus
its attention on in whatever the input
data is as it updates over time another
key area is episodic memory so the
difference between semantic and episodic
memory is this semantic memory is very
concrete it's like you know this is an
apple I know that this is a you know
this is a bed so you know whatever
episodic memory is more continuous it's
about an event that has happened in the
past and we have that you know we've
we've learned this from neuroscience
that we have both types of memory it's
one of the canonical themes in
neuroscience
that we have that our intelligent
behavior relies on multiple memory
systems it's not just one type of memory
and so deep mind recently you know
created this deep queue learning
algorithm right so this is the reason
that Google bought them what they did
was they were inspired by the idea of
episodic memory and they created this
algorithm called deep two that used
experience replay directly inspired by
the idea of episodic memory and here is
the pseudocode for that right so we
initialize some replay memory D and this
can be considered a data store like
think of it as an array okay and then we
initialize some action value function
that we update over time and then during
the training process the agent selects
an action from some probability
distribution it executes that action it
gets a reward and then it stores that
transition inside of the episodic memory
and then it performs grading descent to
update its weight and it repeats and
every time it read the URI does this for
a new episode its sampling not just from
this random distribution but from the
episodic memory that it's stored over
time and so the so this replay memory
episodic memory same thing in this case
it's kind of like this external memory
store right and so it was it was
inspired by it and it was critical to
his success over time this thing was
able to succeed in like 16 or more of
different Atari games the same algorithm
which is incredible if you think about
it the same algorithm 16 different novel
environments all it was fed was the
pixels of the game that is very
incredible the third is working memory
right it operates over a few seconds
temporary storage manipulates
information and focuses attention while
you're working on something you're using
some memory on that right this is very
similar to episodic memory we know from
neuroscience that we can maintain and
manipulate information with an active
store known as working memory so one way
that we've implemented that in AI is
through the idea of long short-term
memory LS TM sells right these are cells
that have certain dates an implicate a
forget gate and what these gates
actually are are perceptrons
there are many neural networks that have
weight values
as we differentiate the L SCM Network
these weights are also updated over time
and what happens is the gradient is
trapped in these weights so when one
work went on wherever when a recurrent
network is reading in some sequence
whether that be text or numbers over
time the gradient slowly vanishes as it
goes more and more back in the network
and the problem with this is it's not
updating the weights at the at the input
of the network as much as its updating
the weights at the end of the network
and this is a problem if there's got to
be an equal update so L SCM networks
help prevent that from happening by
storing by locking in that memory over
time and this was directly inspired by
neuroscience the idea of working memory
but the problem with L SEM cells are
they take the function of sequence
control by controlling what to do next
and memory storage and stores them
together when we know that the brain
working memory in the brain separates
these two concepts so updated an updated
version is called the differentiable
neural computer ok so the idea is that
there is an external memory store which
is essentially a matrix and we have some
input data and then it outputs a
prediction and in the middle it's
updating not just the weights of the
network but the external memory store so
normally the weights of the network or
the memory but if we if we take that
memory and we create a you know we we
create an external memory store we can
separate the storage of memory from the
processing of the network itself and
this let it do things that an LS TM
could not do a wide range of complex
memory and reasoning tasks like finding
the shortest path through a graph like
structure like a subway map for example
the fourth and last thing from
contemporary neuroscience is continual
learning right we are constantly
learning things right we're learning how
to record do two videos we're learning
how to read a book we're learning how to
run we're learning how to do karate
we're able to learn how to do all these
things we're able to learn how to do all
these different things with one brain
but there's this problem in neural
networks called catastrophic forgetting
where if we take a neural network and we
say
learn how to do X right whatever that
objective is and then we take that same
neural network and we say okay now learn
how to do Y what whatever it's learning
for that second training iteration is
going to overwrite the weights it
learned for the for the first one and
this is a problem right we don't want
that to happen so there is this synaptic
consolidation where connections between
neurons are less likely to be overridden
if they have been important in
previously learned tasks so one way to
prevent that is this idea of elastic
weight consolidation right we can use
first of all by the way we can use
advances in neural imaging to further
study how these these connections are
happening in the brain but the idea
behind elastic weight consolidation is
that it's it's slowing down learning in
a subset of network weights identified
as important to the previous task
thereby anchoring these parameters to
previously found solutions so it allows
the network to learn multiple different
tasks and I'm not saying it does it
perfectly but it's one step inspired by
neuroscience that lets a neural network
learn different tasks over time so now
let's get to the future right five areas
of AI that neuroscience will improve and
I've got this image here symbolic
concepts computation except there's a
lot of ways to divide all this stuff up
you know there's a lot of ways but ok so
the first thing is understanding
physical reality ok human infants we
have learned or able to interact with
the physical world in a way they have
these core concepts built in like space
and number like how the concept of space
and object miss all these things now if
you think about robots right if you
think about robots in simulations they
work great right if you if you have a
robot pick up an object in a simulation
it works great just try to take those
learnings and apply them in a real robot
in the real world
exact same setting I promise you it
won't work ok almost all the time it
never works because when it comes to the
physical world our current models are
just not good for some reason so we've
got to figure out how why why that is
happening right why does it work so well
in simulation if we if we have all
those variables together you know in you
know how that the arm is moving the
physics the inverse kinematics the
gravity yet it just doesn't work in real
life why we got to figure that out and
that's a way that neuroscience can help
there's this idea of the interaction
network it's very recent it's a very
weak sin model that says I want to learn
how this ball is bouncing right and so
what it does it separates relational
reasoning with object reasoning and
that's that's as much as I'm gonna get
into on that because there's a whole
video I can make on how that works the
larger point I'm trying to make here is
that any task that seems very simple if
you really think about that problem for
a while you realize that there are so
many different subproblems there right
so if you think about it problem a lot
there are subproblems right it's not
just reasoning it's relational reasoning
its object reasoning and as we study
this more and more or realize what those
subproblems are which will help us in
the larger scheme of things know what
the overarching solution to those
problems are okay so check out this
interaction network paper as why I've
got a link to it in the description
efficient learning right building
metadata meta knowledge right we are
able to look at something and after only
a few examples know exactly what to
classify it as right you show me a
picture of something I've never seen
before and very very likely to show me
another picture of something similar and
I will know exactly however with deep
learning we've got to show this deep
neural network hundreds of thousands of
sometimes millions of images for a table
to classify and this is not how the
human brain works arguably and so the
idea of meta learning is very important
and it will be more important in the
future of AI which is also very closely
related to transfer learning right so
normally we would say okay here's a
neural network to learn this now here's
a neural network to learn this and
here's a neural network to learn this
but what if we could have the same
neural network apply what it's learned
from some previous tasks to this task
that's the idea of transfer learning
which were really really good at so
there was a recent report from this
paper from two years ago that said that
neural codes thought to be important in
the representation of map-like spaces
might be critical for abstract reasoning
in more general domains
allocentric means map like spaces in
this context but the idea is that
allocentric coding systems encode the
location of one object or its parts with
respect to other objects egocentric is a
relation of the self to other things but
what this finding was was that the
location information that we encode
about objects and how they relate is
important for abstract reasoning in
general domains and this is a very new
finding and we can apply that to AI in a
lot of ways right so lastly I'm going to
talk about imagination well actually two
more things I want to talk about
imagination and planning right so deep
cue was awesome right but the problem
with the cue was it was reactive it was
seeing how the environment reacted to
what its actions were and then it
reacted to that it wasn't proactive it
wasn't sitting there planning out all
these different trajectories but what
we've learned is that that is that we do
this right humans plan out things
whether it's consciously or
subconsciously we plan out how things
are gonna happen in our head before we
execute that action and so we have this
imagination right this is essentially
imagination happening there's this great
architecture by deep mind very very
recent it's called I 2a but I could also
make an entire video on how that works
I've got the link to the paper here if
you if you'd like to see it but the idea
is that it's using this external module
for imagination and in a single rollout
it's making these predictions so for
neuroscience for AI the idea of
imagination is gonna be very very
important in the future and we can learn
a lot about how this works by looking at
how humans plan out and simulate actions
in their head before they actually
execute those actions now last thing I
want to talk about our virtual brain
analytics so deep learning has been
thought of as being a black box and that
makes sense deep neural networks are
very complex with sometimes millions of
parameters right all these different
numbers in the matrices of our weights
how are we supposed to visualize that
right and there have been you know
people who said okay we've cracked open
the black box but let's be real in a lot
of ways we haven't there's so many
different types of neural architectures
that it's hard to say we have just
totally
how neural networks work it's not true
we still have a long ways to go but what
we can do is we can apply findings in
neuroscience to that to to help with
that problem
the idea of two-photon imaging for
example allows us to non-invasively map
out how neurons are interacting in our
in our in our brain and this is very
very recent if we can take that and then
we can apply that in silico that idea of
neuro imaging we can better understand
how our neural networks are working on
our computers visualizing brain States
through dimensionality reduction is
commonplace in neuroscience and it can
be applied more in AI research so this
is gonna be very important as the
complexity of neural networks increases
over time and as as soon as we create
this it's gonna be not just good for AI
research for AI ethics for for AI
productivity why is this AI doing this
what why did I come up with a solution
that I did it's gonna be important for
both business for ethics for research
for everything so more explainable
solutions and we can look at
neuroscience at neuro imaging as a way
to help with that so that's the end of
this video I wanted to go over some
things that you know could get you
thinking about how the brain inspires AI
and how it will inspire AI in the future
check out this spectra vis repository
link to it in the description and hope
you like this video please subscribe for
more programming videos and for now I've
got to study my brain so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>