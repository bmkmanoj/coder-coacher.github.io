<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Use Tensorflow for Seq2seq Models (LIVE) | Coder Coacher - Coaching Coders</title><meta content="How to Use Tensorflow for Seq2seq Models (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Use Tensorflow for Seq2seq Models (LIVE)</b></h2><h5 class="post__date">2017-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ElmBrKyMXxs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">when I go to one start okay
five-four-three-two-one
hello world it's Suraj welcome to this
live session today we're going to build
a sequence the sequence model in
tensorflow
and so let me mute this okay
so we're going to build a sequence of
sequence model in tensorflow oh my god
there's so many there we go
just muting everything we are going to
build let me say that again we're going
to build a sequence to sequence model
intention flow are we going to do it for
a chatbot no are we going to do it for a
translation system no are we going to do
it for anything useful no the point of
this is to learn the architecture so
that's what we're focused on we're going
to focus on learning the architecture
it's not about the application this time
and I'm rolling up my sleeves because
it's just cool it doesn't actually have
anything to do with anything but so
that's what I'm doing right now and okay
so that's what we're going to do so
we're going to have some toy data and
you can see right here this is the
output this is what the output should
look like so we're going to give it some
sequence of inputs we're going to give
it some sequence of inputs like this let
me make that bigger and it's going to be
like that that could be a sequence for
eight five could be and then the end is
just padding we just add zeroes at the
end randomly okay and then we try to
predict that same sequence that's that's
the same sequence we want to predict so
we give it an input sequence we encode
it and then we decode it and then we
compare that decoded output to that
initial sequence so it's actually going
to be different and I'm going to show
you why
right the this is a this is an example
of how memory works in a sequence a
sequence model okay
so and as you can see it starts off bad
but eventually at the very very end the
predicted output is the same as the
initial input okay so that's what we're
going to do we're going to try to
predict that same input input sequence
and in the process learn how memory
works in a sequence a sequence model
okay so that's what we're going to do
and the first question is well what type
of this is not an autoencoder
but what it is is a bi-directional
encoder decoder architecture so there's
the simple encoder decoder architecture
which we can look at and it looks like
this encoder decoder architecture and
you know a lot of initial papers use
this they use an initial encoder decoder
architecture where the are you kidding
me Google this is your stuff Google so
it just looked like this right so you
would have values go one way in the
encoder and then they would decode the
other way but what we're going to do is
we're going to do an improvement on that
we're going to add we're going to make
the encoder bi-directional now recall
that we talked about this in the
language translate translator video and
we talked about how having a
bi-directional encoder gives you the
full context of that input sequence you
get you get you could talk about both
the future and the past so that's that's
why we're using a bi-directional encoder
and yeah that's what we're doing and
yeah so let's let's start building this
thing shall we so let's just drive right
into it but first let me answer two
minutes worth of questions and we're
going to answer questions every at
15-minute intervals so any questions you
guys have yes I have a new hairstyle
machine learning deep learning questions
I finally got time to cut my hair I know
time is very hard to come by these days
but that's the way I like it
what is the best way to identify if a
feature in a model is statistically
significant or not for inputs ABC and
output e what is the best way to tell
which inputs most contribute to e that
is a great question so before deep
learning we had to manually pick what
those best features were because we
didn't have a way for our model to learn
what is the ideal features to use so
that's a whole field of engineering that
existed before deep learning which was
called feature engineering and the ideal
way to pick that was think about what
features you personally would use when
trying to predict
output so if it was a classifier and
you're trying to and you're trying to
classify a dog you would pick well what
would I need to know to classify a dog
well I would look at its what type of
dog it is I would look at its the color
of its fur and you know the size of its
ears and things like that so that's a
good rule of thumb to go by what are the
features you personally would look for
when trying to classify or do another
task okay uh is this also in line for
Udacity and a degree DL yes this is a
part of the Udacity nanodegree and these
videos are public for you guys okay how
about predicting the alphabet I have a
little snippet that generates the data
set for your previous sequences sequence
code you can absolutely predict the
alphabet that is a 25 character long
sequence okay and this this exact code
can be used to predict the alphabet okay
you can apply this exact code well you
would just switch out the data but fix
this can be applied to more questions
and then we're going to get started with
the code does this model use gr use no
it does not but we will use gr use gr
use are being used more and more and I'm
not going to say they are objectively
better than L STM's but they are turning
out to have better outcomes than L STM's
in a lot of cases yes one more question
would you work on neural link so I'm
very excited about the idea we need that
to happen for us to keep up with AI for
us to merge with it that's the goal for
us to become the gods and not have it
become some you know runaway God we want
to merge with it right and become
amazing so yes I would I work with them
no because I'm focused on making content
for you guys so that's that's what I'm
trying to do and no one else can do what
I'm doing so that's why I'm doing well
in the way that I deal with my
personality and stuff so that's what
that's what my job is that's what I'm
focused on but I will collaborate with
neuro-link I will collaborate with open
AI I will collaborate with Google that's
it for question so let's get started
with the code okay so the first thing
we're going to do is we are going to
import our dependencies so dependencies
time let's see and let me know if this
is not big enough right let me know if
the text is not big enough so so these
are our dependencies so the first thing
we're going to do is we're going to
import numpy to do our
tricks math and by the way the code is
in the description check it out follow
along as I'm writing this okay and I'm
going to explain it there's a lot of
comments there's a lot of documentation
for this code I'd really try to document
it as best that I could
they're both okay so in markdown and in
the comments so we're going to up import
numpy and then of course tensorflow for
ml and then we're going to have our
helpers so our helpers just it's it's
one class and it only has two functions
those functions are formatting the data
and generating the random sequence of
data generating sequence sequential
generating random sequence data and
we'll talk about what that data looks
like okay so those are our dependencies
so now what we're going to do is oh this
is one more thing we want to run this
reset default graph function which is
going to clear the default graph stack
and it's going to reset the global
default graph it's just that one of
those initial steps that we just have to
do and we normally wouldn't have to do
this but we're going to do this in
tensorflow because like if we had
multiple graphs for some reason this
would be a good thing if we wanted to
clear our cache but we're just doing it
because it's you know it's a good
introductory step and of course we want
to activate our session okay we're going
to do this at the very beginning umm and
the version of tension flow that I'm
using is 1.0 okay hold on no the F dot
version okay so that's a version of
tensorflow
let's not spam guys okay so here we go
with this so let's start off by defining
the vocabulary size so recall that for
encoder decoder architecture we have to
have a fixed size input vector now
sequences are variable lengths like how
are you versus how are you doing today
one is only four characters long but one
is five characters so how do we solve
this well we add padding to the end what
we do is we add zeros to the end so you
know one sequence could be this you know
one sequence could be like this the next
sequence could be this
but they have to be the same length so
what do we do we add zeros just like
this so they are the same length so
that's what we're doing right now
okay 350 people live that's the way I
like it
okay so let's define our pad at 0 and
our end of sentence as 1 so e OS stands
for end of sentence and it's a token
that specifies for our model when our
sentence ends okay and now we're going
to define our vocabulary size so you
know that is the we are predicting what
is what is the max max length for that
input sequence so we're going to say
it's going to be 10 so in this case it's
going to be 10 because we're generating
toy data but in other cases this could
be very very long okay so then we're
going to define our input embedding size
which is going to be 20 which is the
length of the characters and so vocab
size is the words and then character
length is and then the and then input
embedding size is the we take that input
sequence and we convert it to a vector
and that that vector is what we then
feed into the model it's not that we
just feed this raw these raw words
directly and we have to convert them to
vectors an embedding vector it means the
same thing and we're going to talk about
that okay okay so that's it for that so
the next step is for us to define our
hidden units so let's let's define these
hidden units and then talk about what
they are so encoder hidden units and
decoder hidden units we have 20 of these
and for our decoder we're going to say
we want 20 times 2 so why are we saying
20 times 2 let's let's talk about why we
are saying 20 times 2 in the original
paper they had a thousand units so in
the original paper bytes let's cover for
sequence the sequence model where they
introduced this model they use a
thousand for them they use a thousand
for both the encoder and then a thousand
hidden units for the decoder and
generally that's what we want we want
them to be the same number of hidden
units but in this case I'm saying the
decoder is going to be double the size
of the encoder why well we want it to be
a little different we want that output
value to be a little different
it's just going to be the same as the
input right and we want it to change a
little bit so adding hidden units is
going to change ever so slightly our
output so that we can then minimize it
right so that's what we're having it be
multiplying by two so yes great so now
we're going to define our placeholder so
placeholders time placeholders are
chatted out what are they they are
gateways they are gateways for data into
our computation graph these are the
gateways and they are they are
primitives and tensorflow
and they are we always have to use them
right and what we're going to do is
we're going to have three placeholders
we're going to have placeholders for our
inputs and our decoder and the targets
okay so then we're going to define a
couple of parameters the size of these
and then we're going to name them as
well because these are names for our
computation graph so we're going to
actually talk about a tensor board next
live session oops I just spilled
something so this is a good way for us
to differentiate between different
primitives in tensorflow that's why we
add names to it and so then we're going
to have inputs length the length of the
inputs so actually that's the the next
placeholder the length of the input so
we have our inputs the length of them
and then the the decoder targets so then
we have our shape the shape is going to
be none and then we have our data type
okay so we want them to all be the same
data type because that that's what we're
operating in and if we didn't do that
then we'd probably have some kind of
errors that start popping up okay so in
32 D type is in 32 and then the name is
going to be encoder inputs encoder
inputs length okay and then we're going
to have
okay so input our length and then we
actually have one more no yeah that's
fine
decoder target so decoder targets are
going to be gift up placeholder shape
and then none none because they are
empty right now we we're going to define
them later in a second okay datatype I
do things a PI torch is going to be hot
so data type is going to be TF in 32 and
got to remember to answer questions
after this and then finally our decoder
targets okay so those are that so we're
going to get a error right here
so the I knew it so it's for this
encoder inputs let's see here invalid
syntax let's debug this so we have
encoder inputs and then we have it's
going to be a tensorflow placeholder
let's see the whole thing here what's
going on over here want to maximize that
and so we have encoder inputs and then
we have it's a TF placeholder with a
shape value none none data type and then
we have input so it's actually not on
that it's on encoder input sleng and the
syntax yes that's the one
great and now for the length it's going
to be TF that placeholder shape
none data type input a length so why do
I have W here in my code I shouldn't
have that and then someone saying
there's a comma missing from shape nan
equals none comma like that yeah okay
encoder input slang thought shape so in
my actual code I put a W there for some
reason where I put a W like right
where'd I put it at the end that the
comma yes hold on so I put a W here at
the end in my the initial code and let
me add these okay
see why did I put a W that o Siraj here
we go okay so there's no W great okay so
now to our embedding layers okay oh let
me answer two questions okay so we are
at the 15-minute mark let me answer two
questions any questions guys yes t f dot
n 32 okay so now we're going to define
our embeddings oh and i'm here i don't
know if I mentioned this at the
beginning but I'm here at the upload VR
studios so check them out subscribe to
them with azad Balaban Ian totally
forgot about saying that so yeah just
remember that remember his name okay
links down below links down below yes so
and this wouldn't be possible without
him so big round of applause so now what
we're going to do is we're going to
define our embeddings so we have our
variable oh and then one question so
what's best suited for ML Mac or Windows
or Linux Linux and Mac any kind of UNIX
based system why because most of these
libraries are they when I look at sack
overflow errors when I look at github
issues it seems that people on Windows
tend to have more issues because the
people who are writing these libraries
are using them on UNIX based systems so
it just so happens to be that way yes
so okay so now for our embedding so we
want to convert our that our sequences
to embeddings right so we're going to
use the tension flow primitive variable
to do this okay we have we're going to
we're going to initialize an embedding
matrix randomly so we're going to use
TFS built-in uniform random function to
to build this and it's going to be the
vocab size so we're going to this is
going to be a matrix of values and it's
going to then we're going to fill it
we're going to fill it okay so now we
have our boat capsized and then we have
our input embedding size
and then we have what is that what is
the what is the interval that we want to
generate from from negative one to one
is it's a distribution from negative one
to one of values and then we have our
data type of course our data type is
going to be is the audio really out of
sync dot float and looking into it we
have Assad looking into that okay and
then we have encoder inputs embedded so
now we're going to take that embedding
matrix and we're going to add in the
inputs okay so what do we just do so
what we just did is we in randomly
initialize an embedding matrix that can
fit the input sequence and then once we
did that we then put our encoder inputs
into that embedding matrix and that
embedding matrix is what we then feed to
our encoder so let's now build that
encoder right we have our embedding
matrix and now we can feed that directly
to our encoder so for our encoder so
define encoder so for our encoder
tensorflow has so many updates like
they're all over the place and version
changes and API changes like all
software and so the LS TM cell is always
changing where it is before it used to
be I think it was in the lsdm cell used
to be in where was it in the previous
version it was in just a different place
and now it's in under ops RNN cell and
in later versions it'll probably change
so it's always a good idea to keep up
with these API changes and the best way
to do that is the tensorflow of
documentation okay so we're going to
import our lsdm cell and our LCM state
tuple I'll talk about the second one and
why we're importing that in a second
okay so that's we're going to define our
encoder so that our encoder cell and
it's just one cell is going to be an L
STM cell right that we just imported and
we're going to define the number of
hidden
units as that number we defined
previously and that's how many which was
a 20/20 neurons 20 of these voice of God
is awesome I'm glad you guys like the
voice of God so that's 20 of these
encoder cells okay and remember each of
these is an LS TM itself each neuron is
an LST M okay and so that's our encoder
cell and C deprecation warning but it's
fine it's just a warning but it's will
soon be deprecated I knew it would
happen so so now so this is actually
like a spatial a very hellish spatial
line to write out which is probably
going to you know in Python especially
with all of the syntax so I'm just going
to paste this part in let's talk about
what this is this is the dynamic RN n so
when using a standard RN n to make
predictions we're only taking the past
into account right so for certain tasks
this makes sense whenever we're
predicting the next word in the sequence
but for some tasks it would be useful to
take both the past and the future into
account okay so this is the
bi-directional part we're taking both
the past and the future into account so
let me talk about what we're doing here
okay so in order to demo this I'm going
to do some V R to demo this okay we're
going to talk about how a bi-directional
layer works over time okay so let's get
some V R going and I'm just going to
draw this out in VR for you guys
cool thanks probably two of these okay
so let's see if we could get our VR mode
on yep we're in our VR mode okay so so
um so we have our encoder decoder right
so we have our encoder encoder this is
so fun and then we have our decoder and
this is a high-level abstraction of what
it is right we all get this idea that we
are taking our encoder and so we're
giving it a sequence right so we're
giving you a sequence like you know how
are you how are
you and we're taking the sequence we're
actually converting it to our embedding
matrix RS which is a vector and then
we're feeding the vector into right so
it's it's here's the steps right so you
can see you guys can see all this right
this is dope yep
so we have our input sequence we
vectorize it and we feed it to our
encoder and then we want that final
hidden state right we want this final
hidden state so the hidden state we
don't feed the output of the encoder to
the decoder remember we feed the hidden
state which we'll call H so let's just
say hidden right so this is we embed
this into a vector and then the encoder
further embeds it across as many as many
neurons as we define and so because this
is bi-directional it's looping over up
over the sequence from both left and
right so since it's bi-directional its
looping over the sequence both in both
left and right directions and then what
we're doing and so now it's time for 3d
so I'm going to go over here what we're
doing is through time and this is this
represents through time we're feeding in
the H the hidden state and we're also
feeding in the pre the previous input so
this is back propagation through time so
there's two values so VP through time my
handwriting needs to get better in VR
but just like bear with me for a second
so so I'm trying to ml like through time
so write input sequence vectorized
encoder get the hidden state and take
the hidden state and the previous input
and feed it back into the encoder okay
so we feed it back into the encoder and
then up that's one full time step so in
one full time step once we feed that in
then we feed that into the decoder so we
roll it feet decoder roll feet decoder
okay so over and over and over again for
every word that we have in our input
sequence and then we feed it to our
decoder our decoder will output a a
vector and then from that vector we
convert that
into our in this case it's going to be
the initial value how are you but in
other cases in chatbots it would be you
know I'm doing fine or in you know
translations to be French okay so that's
the high level what we're doing here
hope you guys like that okay so back to
this people love the V are people of the
VR okay it's super cool isn't okay so we
have to keep going guys so it's super
cool I know so now here we go with this
it's a vibe it survived yeah uh great so
now let's do what I was just talking
about so let's now programmatically do
what I was just talking about okay so
we're going to concatenate our tensors
along one dimension so we're going to
take so this is the actual step or we're
combining our this is the bi-directional
step so this is a bi-directional step
let me write a bi-directional step okay
so we'll talk about what that means
bi-directional step so what we have here
what we have here is we have our outputs
for our encoder so we have our encoder
outputs and the encoder output we're
going to use tension flows concatenate
function to do this we're going to have
both our forward outputs and our
backward outputs so our forward outputs
are what we we spit out from this
dynamic cell right we got our forward
outputs our backward outputs our forward
final States and our backward final
state so they're States and outputs for
both the encoder for sorry for both the
forward and the backward parts of the
bi-directional RNN okay forward and
backward fw + BW means forward and
backwards so we have States for both how
are you and then you are how okay and
what this does is it allows us to take
into account the full context so both
the past and the future
this is definitively better I am telling
you right now that it is better to do
this every time it is more
computationally expensive to have a
bi-directional a
but just think about it it's you want
the future and you want the past when
you're trying to make a prediction it
it's a word a story I sequence is all
about what the story is about what's
happening all over it whether it be
music whether it be art words numbers if
there's a pattern if there is a pattern
both the past and the future matters so
having bi-directional layers although
more computationally expensive gives us
better predictions okay so so now we're
going to take our encoder outputs and
we're going to take our in color out
forward outputs in our encoder backwards
outputs and we're going to concatenate
them along a along two axes axis axes is
the plural and so that's going to give
us our encoder outputs and then what
we're going to do is we're going to take
the final state the final state C so
that's that's for our outputs and then
we're going to have our concatenate
function to take our
okay so we're going to take our encoder
forward final state dot C and then we're
going to take the encoder backwards
final state dot C okay and so what is
this what are these words what is C and
one let me let me write the cell and
then we're going to I'm moving a bit too
fast okay so hold on so this specific
part I'm going to paste because there's
a lot to explain here so okay so we have
a final state C and we have a final
state H and I can slow down as well so
what we have here is we have so H and C
are commonly used to denote output value
and cell state so it's both the output
value and the cell state and we want to
concatenate both of those for both our
forward and our backwards our forward
and backward outputs okay so that's what
we're so we're concatenated and then we
get a final state using the LS and so
now this is why we imported lsdm state
tuple at the beginning this is why we
imported it because we're going to use
that final state C and the final state H
so that's the set C when I say C and H
I'm talking about the internal state of
the cell and the output value we're
combining all of those to get that final
state for the encoder and that this
value right here is what we then feed
into our decoder it is a combination of
both our forward our backwards our cell
state and our output we combine it all
together and that's what we feed into
our decoder so now let's let's define
our decoder so our decoder I think
there's an encoder mistake where you
misspelled Oh eat oh are at the end I'm
trying to find it here someone said
there's a encoder - misspelling a
mistake I don't see it in this
uh someone encoder someone said there's
an encoder mistake I don't see it I'm
not either or whatever so where were we
so now we're going to define our decoder
so for our decoder for our decoder we
want to do it's going to be very similar
but different and I'll show you what the
difference is so let's define it so this
is going to be similar we define it the
same way it's still an LS TM sell out
remember the number of hidden units is
different it's double that of the okay
and then we're going to define our batch
size and then our we're going to unstack
so let me talk about what I'm doing
right here so what I'm doing right here
is what I'm doing here is I'm going to
define our encoder and our TF to unstack
TF dot shape encoder inputs okay
so let's define our decoder defining our
decoder defining our decoder so we have
our decoder cell and we're going to
we've define our decoder cell and so
what this this step does is what we
really care about is the batch size this
is going to give us the batch size we
have all of those inputs and we want to
feed it into our decoder in Bash's
batches because that's how we train
right we feed our data into batches data
into our decoder in batches like subsets
there are subsets okay okay so right so
then we're going to have the length of
the decoder which is going to be
the encoder inputs length plus three so
why do we add plus three because we have
two additional steps one for the leading
end of sentence token for the decoder
inputs we want it to be a little bigger
because we have an end of sentence token
at the end that's just going to help us
help our model know that this is the end
of a sequence this is in natural
language processing we have these end of
sentence tokens all the time I'm
actually I don't think we have models
that can know when the sentence is over
yet actually I haven't seen a model that
doesn't use these up either padding or
some kind of end of sentence token to
specify you know that the sentence is
over yet we need to get there we need to
get there where we just feed it in and
we don't have to do any of these
pre-processing steps but we'll get there
okay so now let me answer some questions
since we're 15 minutes in does not
dividing into smaller batches is is
dividing into smaller batches more
computationally expensive no no it's not
because it's still the same amount of
input data but what what having it in
batches does is it it makes our
prediction better because we have more
iterations that are happening so there
are more weight updates happening it
actually honestly it is a little more
computationally expensive but it's so
it's so small the difference is so small
so don't don't don't worry about that I
appreciate it Paul for waking up at 4
a.m. and Mel born for this ok this is a
good question in what ways are gr use
outperforming LST ms I recently read a
paper that says adding a bias of one to
the LSC MS forget gate closes the gap
between the LSC m and the GRU
so GRU cells are very similar to LS CMS
and we're definitely going to talk about
that in this coming weekly video in
three days but to sum it up so gr use
have less gates than lcms there's just a
reset gate and there's a update gate so
whereas L STM's have a forget gate they
have
regain so it's two gates so it's less
computation happening so it's less
computationally expensive and not only
is it less computationally expensive it
tends to have better results
specifically for dynamic memory networks
which is a really really cool model type
I'm going to start talking about as we
get further into this course we have
four more videos in this course and we
are now we are now guys if you don't
understand all this don't worry we are
at the bleeding edge of deep learning
this course is called intro to deep
learning but we went from doing linear
regression at the start to we are about
to go we are about to get into
generative adversarial networks and
we're about to get into the bleeding
edge of all of AI which is not even deep
learning in the last episode which is
I'm most excited for which is called
probabilistic programming which is some
future stuff and you know how much I
like the future so that's going to be
awesome one more question
hey Suraj what model would you use to
analyze repetition in speech repetition
in speech great question repetition in
speed what you mean might analyze
repetition in speech so I just got to
clarify for you analyzing repetition in
speech so maybe the frequency of
repetition so how would I do this so
those this is what good way of thinking
about it so you would you could consider
it a classification problem so an easy
way to do that would be to an easy way
computationally to do that would be to
pre label what each sentence is so you
know you would say like how often does
this label you know X up here versus y&amp;amp;z
and ABC D D so be a multi-class
classification problem and you would use
a deep net and you would use a you
wouldn't even have to use an encoder
decoder because you would just define
how many times this a label shows up in
the data so it would be a multi-class
classification deep net using a
supervised learning with labels and
inputs one more question
evolutionary strategies that is coming
ganz are coming and
our large batches always worse than
smaller batches in terms of prediction
accuracy our large batch is always worse
than smaller batches in terms of
prediction accuracy that as as far as
I've seen yes but it's not like it's not
like there's like some in some
exponential in terms of like you know
you can never approve you know you can
just get infinitely smaller in terms of
batches and by infinitely smaller I mean
you can have an infinitely large number
of batch sub batch steps but there's
there's a there's a there's a bell curve
right so there's a point where it's at
the optimal level of batch step and then
it goes down again remember batch steps
are one of many hyper parameters and all
of machine learning all of deep learning
I'll sorry all of deep learning is about
tuning those hyper parameters and to me
what I would like to see what I would
like to see you guys work on what I
would like to see more people work on is
ways of learning these hyper parameters
because designing these hyper parameters
is so annoying sometimes right because
they are so arbitrary there's no way for
us to really know we have to learn them
manually and it takes up a lot of our
time from looking at what is really
necessary and that is a high level
architecture so let's find out easier
ways of learning these hyper parameters
okay so that's it for questions now
we're going to we've define our decoder
and the lengths and now we're going to
define our weights and biases so hold on
so our weights and biases are going to
be our so our output projection so let's
define our define our weights and biases
so remember this is all for the decoder
we're working on the decoder value so
for our decoder values we have both
weights and biases and these weights are
what we're going to use we're manually
defining these so remember we didn't
define these for the encoder because
because tensorflow had those built in
for us in that dynamic RNN function but
we're going to be a little more detailed
with our decoder why because we are
going to implement attention ourselves
okay so I want to show you guys how
tension works so I've kind of skipped on
a tension before I've just said you know
hand wave this is a tension boom but now
we're going to implement a little bit of
what's what is called soft attention and
we'll talk about that so we're going to
define these weights and biases for our
decoder manually okay so where were we
where were we
so where were we right so these weights
are going to be initialized randomly
using again ten flows random uniform
function we use that a lot and then we
want to define the size of that in terms
of tensors so it's going to be the size
it's going to be a three dimensional
tensor so we have the number of hidden
units the vocab size and then the
distribution from negative one to one
given our data type and it's all in 32
bits and right so then our bias our bias
is going to be TF double tip that zeros
and then the vocab size followed by the
type okay
so then that's the title and let's talk
about this okay
and I'll answer questions in five
minutes so think about your questions in
five minutes this is to me the next
question mark I'll answer them TF top
float32 okay okay weights and biases
these are weights and biases guys and so
it's going to be up sized to the number
of hidden units so right these weights
and the reason we said we put the the
hidden units as a as an input to this is
because the weights have to connect all
of those hidden units to the output what
the help is going to be so we take all
those values and then we multiply it by
the weights which is initialized as a
random matrix and is going to learn what
the optimal weight value should be to
get that value right so right so okay
ok so now we're going to get into the
attention part okay so let's talk about
attention so to get attention let's see
let's see guys if you're lost don't
worry about it this stuff is remember
the bleeding edge and I'm going to get
better I'm just going to get better at
explaining it I'll get better examples
you're going to get better just by
looking at this you're getting better
kudos to you thank you for being here by
the way I need to say thank you to you
guys you guys are awesome for being here
for wanting to learn this awesome stuff
this is the most important stuff in the
entire world
more important than anything else you
can be doing in the world is working on
AI if you are a smart person in the
world right now more important than
politics more important than climate
change more important than anything else
in the world you should be working on AI
why because we are the closest to
solving it across all these problem
spaces and if we can solve this we can
solve everything else so what we're
going to do is we're going to talk about
what the next steps here and to do that
let me just paste this in because we
have a lot to go still and we don't have
time to really type out everything so we
are now is the padding step remember
eyes I define the pad in a OS functions
before now is now we're going to
actually pad those inputs so we have
we've embedded them those values we've
embedded those values into our
projection matrix right we took our
inputs and we embedded them and now what
we're going to do is we are going to now
we're going to add those padding's so we
so we're going to add the padding's to
it and that's what so the the embedding
lookup function of tensor flow retrieves
the rows of the of the parameters tensor
which is in this case the embedding
matrix and the behavior is similar to
using indexing with arrays and numpy so
this is essentially just adding the
padding and the end of sentence token to
our
two are embedding lookup okay so that's
what the end of sentence and padding
steps do and now get ready for attention
guys because I have never like fully
programmatically defined attention and
so now we're going to define attention
so bear with me this is going to be
awesome
bear with me okay so now we're going to
now we're going to implement attention
okay and there's their their two
functions here that we have to implement
so for attention now remember for the
encoder we defined that that function
what was it the function that we defined
was let's look at it again just so we
remember what it was it was called
bi-directional dynamic RNA that
functionality the looping functionality
that I did in view are a second ago
where we're taking the previous hidden
States and the input and feeding both of
those back in that's all done by this
one line all of that is done by this one
line but for the decoder we are going to
do that manually because we want to see
how it works so this is what we're going
to do manually we're going to loop this
is this function does that loop manually
for us so what we are doing in this is
your first thing what is the end of
sentence step to get that initial input
what is the initial cell state and then
what is the loop state and all it does
is it's going to initialize these values
and then return them because the loop
isn't actually happening here the loop
happens in the next function all we're
doing in this function is defining what
the end of sentence a is what the cell
state is what the initial output is
which is going to be none to start off
with and then return those values to to
then loop in a second so now now we're
going to loop okay so now we're going to
loop
let's get to the looping part let's
paste this in as well got a lot to go a
lot to go okay so for our so here is our
attention mechanism so this is
considered soft attention okay it's
considered soft as opposed to hard
attention because it's a very trivial
form of attention I'm going to explain
exactly what that is okay so let's talk
about what we're what we're what we're
doing here so we have loop FN initial
and so okay so let's talk about what
we're doing here what this does is is
going to get the next the next input in
the next state so remember the loop
that's what this is doing it's
transitioning for the loop and so let me
talk about what this does but I said I
would answer questions at the 45 minute
mark so let me answer some questions
right now so Paul Grady's got a question
oh yeah and he's asking are we going to
be exposed into tensorflow
in the Udacity course alright the intro
to answer flow or should they start
focusing and learning it on their own
way yes we will be learning tensor flow
in the Udacity course and we will be
learning tensor flow just from here on
out until you know anything better comes
along which PI torch is starting to get
pretty hot so also check out PI torch
because I definitely am because their
documentation is pretty dope when we
write codes to create a predictive model
what is the output is it an object a dll
an exe thanks in advance
the output is a set it while the output
could be anything but it is uh it could
be numbers it could be words it could be
depending on what you're trying to
predict you're trying to predict the
next word are you trying to pick the
next number are trying to predict a
label is this a cat or a dog but it's
going to be a variable encode a
programmatic variable a space in memory
that we define with a word and
programmatically like label so it's a
set of integer numbers but it's not an
exe it's not some kind of it's not it's
not it's not a bye
executable file that you just run like a
program it's just a set of it's like
memory so if you were to even print you
know a variable it'll be a variable okay
when we feed words from our two more
questions when we feed words from our
text example to the model are we how are
we handling pronouns for example if we
have a text on shirt Albert Einstein
many of the times will refer to him as
he his him etc okay so great question so
before deep learning so this again
before deep learning we had to define
these things with you and this was a
whole subfield of natural language
processing which was called which is
called a part of speech tagging so we
would manually tag these parts of speech
and how do we use that while we you
there are a bunch of banks like word
banks that just they're like
dictionaries where they have like just
give it a word and it'll tell you the
part of speech these are pre-recorded
and so that's that's one way to do it
but with deep learning we have to think
about this at a higher level where it's
just data in data out we only don't even
I know you know as humans we want to
worry about these things but we won't
have to we don't have to because it'll
learn what are pronouns more or less
it's not going to define them as
pronouns but internally and it's hidden
States the more data we feed it it'll
learn what a pronoun is and by that I
mean when to use it when not to use it
one more question
the bioinformatics question what about
deep learning and bioinformatics what
about it so I guess you're asking about
use cases so bioinformatics are like
like eye scans and stuff right like bio
thumb prints and stuff yes that
absolutely we could use machine learning
for that to learn what to learn what a
certain biological signature looks like
like what is this type of person what is
their biological signature look like by
giving it data of a certain type
remember it's a label it's a class it
could be a classification problem that's
kind of generally what I see
bioinformatics for deep learning in
bioinformatics also anomaly detection
like maybe somebody is very different so
somebody who has some off genetics some
one of their genes is like different
than the rest and it can be used for
anomaly detection which would be
unsupervised clustering and for that I
would use state of the art generative
adversarial networks that's it for
questions let's get back into this okay
where were we so back to this to the
loop function so here's for the
attention mechanism so the attention
mechanism and the only time going to
answer questions after this is at the
very end so that's in ten minutes
the loop function we're going to get the
next input so what are we doing here
we're going to get the dot product
between the previous outputs and the
weights and then add the bias so matrix
multiply the previous output and the
weights and the bias and remember to
think about that be our thing that I
just did the VR demo that's what's
happening here we're looping it over
time and that's going to give us our
output logits and then we're going to
use the Arg max function and this line
is attention this line is attention what
do I mean the Arg max function will
return the index with the largest value
across the axis of a tensor that largest
value is the one that we are picking has
our prediction so that picking that
choosing step is attention what do you
pay attention to remember attention is a
very broad term it's a very broad term
and we could use it to pick what is the
best value arbitrary
early we could generate a probability
distribution and say you know over this
certain threshold those are the values
we want attention mechanisms are words
that we use for how we pick what the
best value is from a set by whatever
definition of best that we define is and
that depends on your use case so this is
the attention mechanism remember it's
really not that complicated it's just
one line the attention mechanism is just
one line of code ok so then once we have
our prediction we're going to embed the
prediction for the next input using this
embedding lookup layer so we have that
input and we want to feed it back into
our network and that's going to be our
next input and then we're going to say
this this line defines what the what the
ending sequence is going to be the or
the ending scalar this this tells us
that we are done looping and so then
what we do is we have to
and so this reduce all is the logical
end of elements across dimensions of a
tensor so we are saying this is going to
output a boolean scalar are we done or
not and then a conditional that says is
it if it's done or not then continue and
get that input value and it's just like
a remember from data structures and
algorithms when we would for binary
trees and for for any kind of or for any
kind of tree like structure we would
then set the previous state to the
current state and in the previous leaf
to this leaf that's kind of what we're
doing here it's the same thing it's a
it's a data structure it is a data
structure a cell is a data structure and
the time element is why we're switching
the previous to the current this is that
you should remember this from data
structures and algorithms by the way if
you haven't taken data structures and
algorithms definitely do that and see my
how to succeed in any programming
interview video ok so we got to start
wrapping up ok so I'm getting notice we
have to start wrapping up so what I'm
going to do is I'm going to start
reading this can we we can end in 8
minutes 5 minutes ok we have 5 minutes
guys this is just a one-time thing next
time I'm going to be
I won't go over this much over the time
I'm allotted so don't worry about it
guys so let's let so we had that and so
I'm going to be reading off of my code
because we are running out of time so it
don't worry we're almost done we only
have a few more lines of code and so let
me explain it so that's our looping
mechanism and so what we're saying is
we're doing two looping mechanisms one
for just the first state because we have
no data we want to fill the data and
then we do our main looping mechanism
which is what we just did know a cell is
not a tree data structure it's just a
ton analogy I was using that tree has
nothing to do with this it's just an
analogy don't worry about it so uh so
this is our loop right this is the
looping state this is we are doing this
manually and if you don't want to do
this tensorflow has a line of code that
does all of this for us okay but we want
to look at how this works and so we are
looping we are taking that input and the
previous time step and we are feeding it
back into the network and that's going
to give us our decoder output and the
decoder final state do we care about the
final state no we care about the output
value the only final state we care about
is for our encoders the only final state
that is true Jordan I why do I have to
listen to someone it is my channel is
true I'm getting my own studio don't
worry about it it's going to happen soon
we are building up we are building guys
we are building an ml Empire and we are
going to get there don't worry about it
so that's our decoder output and we want
what we want to do is that decoder
output we want to format it into a valid
prediction when we want to format it
into a valid prediction and to do that
we're going to flatten the matrix and
then we're going to get that prediction
value okay so we're going to flatten the
matrix and get that prediction value
because we want our tensors to be the
same size and shape and the decoder
prediction is going to be our final
prediction value okay and what we're
going to do and I'll talk about what
we're going to do is what we're going to
do is
we are going to minimize a loss using
cross entropy so the decoder prediction
and the actual value is what we're going
to minimize and let's talk about that
and the training we're going to function
we're going to be using is atom to do
this so now is for our helper functions
we define the the helper function the
this is the data that it's going to
generate okay it's going to be this
sequence of numbers and we're going to
continuously generate that data using
this next feed function for all those
batches we're going to add the end of
sentence and Pat it and we're going to
continue to do that and feed it into our
placeholders and this is the training
step it we're just a bunch of print
statements and we're saying run the
session compare the loss and then print
out the values and minimize a loss every
time so in one minute let me explain
exactly what's happening here we have
our input and our predicted output okay
so I remember these are our inputs are
all of those generated values four eight
five four eight six and we're patting
them with zeros and our predicted value
remember because we our predicted value
is going to be that decoded output and
we and what we're minimizing is the
difference between the predicted output
and the initial input and that's the
loss that we are minimizing over time so
eventually it's going to look like this
where it's going to be if the predicted
output is going to look exactly like the
input and I have this plot right here
that shows just the loss minimizing okay
so we are so that's that's it for the
code next time I'm going to have more
time and I'm going to take more I'm
going to have more time okay so and the
code is all there and document it and
I'm going to add even more examples to
the code for you I have two other pipe
ipython notebooks I'm going to add to
this code so you guys get extra help two
questions and we're out of here so yes
there are girls here we need more women
and machine learning
women thank you for being here we need
more women in machine learning spread
the word and be nice guys okay we this
is a this is a gender equal everybody
quill opportunity place do you reckon oh
my god okay so do you how do you choose
learning rate for faster convergence
remember a Viru learning rate is one of
those hyper parameters that we want to
that we kind of guess and check right
like all hyper parameters but if you
look at papers papers are a good source
look at their results and copy them and
also github code and ipython notebooks
because you can see the output one more
question and we're out of here okay
is it possible to train a standard RN n
on a non reverse translation problem
where sentences are reversed yeah
because it would still be a sequence of
words right I don't see why I wouldn't
it would it would still be a sequence of
words
you might have to flip the direction so
maybe you would just have a backwards a
backwards layer instead of both a
forward and a backward layer but yeah
you could absolutely do that it's just a
sequence okay so that's it for the
questions okay thanks guys for showing
up I appreciate it
we need to learn how to learn our hyper
parameters okay
love you guys codes going to happen in
at noon a special thanks to upload VR
for the space to azad Balaban ian for
hosting this and please subscribe do you
have it tell your friends subscribe
subscribe subscribe I'm trying to hit
100k by April 15th that's the goal okay
and then 500k by the end of the year I'm
just saying it right now so then I have
to do it
500k by the end of the year for now I've
got to make this Empire even bigger so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>