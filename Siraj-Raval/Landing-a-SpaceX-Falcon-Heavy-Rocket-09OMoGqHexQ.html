<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Landing a SpaceX Falcon Heavy Rocket | Coder Coacher - Coaching Coders</title><meta content="Landing a SpaceX Falcon Heavy Rocket - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Landing a SpaceX Falcon Heavy Rocket</b></h2><h5 class="post__date">2018-02-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/09OMoGqHexQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and my demo today
is landing this SpaceX rocket in a
simulation using machine learning this
thing has learned how to properly land
using all of those gravitational
constants balancing its weights velocity
physics all these things but the key
here is machine learning it's not rocket
science
it's machine learning which is even
better so that's what we're gonna do
today I'm gonna code this out it's gonna
be about 150 lines of Python I'm going
to be using the tensor flow libraries
the tensor flow library and the open AI
gym library but before I get there we've
got to learn a bit about reinforcement
learning as a bit of background before
we get to the technique that it uses
which is called stay with me proximal
policy optimization I know that's a
mouthful and we're gonna get there but
first we've got to learn some background
on reinforcement learning okay so open
AI it was started by Elon Musk Sam
Altman a bunch of really prominent
people in the tech space but the goal
was to build safe AI in a way that was
distributed that was open so everybody
had access to it in order to ensure a
more beneficial future for Humanity and
one of the major contributions that open
AI has made to the world is releasing
the gym environment so Jim is this
really cool library on github that
allows anybody to train their AI
algorithms in a bunch of different
environments it's very cool universe was
kind of their next environment which
they hired me to make a video on back in
the day and then he LAN retweeted it it
was awesome but anyway that's a tangent
but Jim Jim is kind of what has stayed
and in the mainstream universe didn't
really take off for whatever reason but
but Jim is still there it's it's it's
still very active people are still using
it in a bunch of examples you can find
it on github and they've got a bunch of
different environments from Atari games
to 3d simulations using Moo Joko a toy
Tech's classic control a bunch of
different examples right so it's a
collection of environments it's all in
Python and it's really easy to use the
whole point of it is so that you don't
have to focus on building the
environment you can just focus on the
algorithm and they have this leaderboard
as well on the website but that they
took that down for whatever reason but
you can still find a leaderboard that
can
the strength of different algorithms on
on github and it's in this gym Wikipedia
page it was updated 25 days ago but I
would still make a PR if you have some
kind of update and they're gonna they're
likely gonna accept that that request
okay so let's start with reinforcement
learning so this is right so there are
different ways that we can class the
learning experience right supervised on
supervisors reinforcement online offline
you know there's there's a million
different ways that we can class
learning but anytime that we have some
kind of time element right so when we
add the element of time which is where
an environment comes into play that's
when we can use reinforcement learning
it's a computational approach where an
agent interacts with an environment by
taking actions where it tries to
maximize a reward so this is a very
simple way of visualizing this we have
an agent that is a bot an AI a person an
animal a human anything that takes an
action in the environment if it's
learning how to ride a bike that action
would be to push down on the pedal the
environment will then update by giving
it a reward right so if I push
successfully then my muscle won't have
any pain so my brain is getting that
reward back right and I've updated the
state in my environment my foot is now
lower and this this just repeats over
and over again but if I push in the
wrong direction or my you know my foot
goes just crazy in a different direction
and I'm gonna get a negative for reward
and I want to avoid that negative reward
so a much more formal mathematical way
of describing this agent environment
loop is to describe it as what's called
a Markov decision process this is based
off of Andrey Markov and early
mathematicians initial principle the
Markov property but essentially it's a
way of estimating probabilities and how
they change over time in an environment
so we have a state right a state is how
you an agent is currently in a game like
what's its position
what's its move what's its
the way it's it's positioned in relation
to other objects in the environment and
then we have an action this could be you
know punching a wall or hitting a
ping-pong ball or jumping up and down
it's something that the agent does we
have a transition probability that's
that relates the state to the action we
have a reward which is what the agent
receives if the action was correct
so by correct I'm saying how does it
affect the objective what is the
objective of the game to win to lose how
does it effective if it's a if it's a
winning move if it's a winning action
then the reward will be something like a
plus one if it's not it's a minus one
and there are different quantities that
are involved here like policies
utilities Q values and that's there
there's a lot that we can go into
there's so many different ways and
pathways we can go into when we're
talking about reinforcement learning but
what the one that I want to focus on is
the policy and the value which I'm going
to go into in a second but before we get
into that let's talk about offline and
online learning remember I said there's
so many different ways that we can class
the the different learning processes and
one way is offline versus online so
online just means learning in real time
as we're playing the game which is what
we're going to do and offline is before
or after or in between games so it's
like we've already completed a set of
actions in an environment we're done and
now we're learning from that whereas
online is during the actual process of
interacting that's what we're going to
do so to summarize a bit about RL we
have an agent it interacts with an
environment and it learns what's called
a policy so this is a very important
word we have a state and an action and a
policy describes how an agent decides
how to best interact with the
environment using its current state and
the list of potential actions it could
take right so it looks at what's called
the state transition matrix which
relates all the states to all the
actions and the policy says what's the
what's the optimal action I can take
right now that will maximize my goal
right how do I get to my goal in the
most efficient optimal way possible and
the policy will tell the agent exactly
how to do that but
addition to the policy there's also
another feature called the value what's
called the value function so the value
function will evaluate every single
possible move and it will create this
list of all the possible values for
every move and it will choose what that
best value is the policy uses the value
function to then evaluate every move so
the value function is saying this move
is this is this good
this move is this bad this move is this
is even better and in the and then the
policy will use that value function to
then pick the best move so the value
function is a part of the policy the
policy is the higher order function on
top of the value function and this value
function can be learned and approximated
by any learning and approximation
approach like in our case a neural
network right function approximation so
we're gonna learn a value function and
it's gonna help us learn a policy and we
can broadly categorize reinforcement
learning into three different groups for
our sake right now for proximal policy
optimization there is a critic only
method where we first learn a value
function and then use that to define a
policy right we we learn a value
function using a neural network and then
use that to define the policy then
there's an actor only method where we
directly search the policy space we
don't have this the policy is what's
evaluating all the moves without
separating these modules of a value and
a policy function
you know independently and then there's
an actor critic method this is where we
have two different neural networks one
learns the value and one learns the
policy where right so the critic
observes the actor and evaluates its
policy determining when it needs to
change so the critic will be a neural
network the actor will be a neural
network the actor will then take us
action in the environment and then the
state will be returned to both the actor
and the critics the critic will observe
this the new state and then send its cue
value which is the quality of that
action to the actor so that it can
update its weights and the process
repeats and what's what's been found is
to have two is is when we have two
different neural networks an actor and a
critic this usually outperforms both
actor only and critic only methods
so we're getting closer to proximal
policy optimization let's keep going
here so there's this idea of the policy
gradient approach on Drake our Posse has
a great blog post on this and I'll link
to it in the description but the idea is
that let's say we're in a game right
let's say we're in pong and we're trying
to evaluate how good each move is right
so let's say one of the moves is moving
the paddle up and then the ball hits the
paddle and then it goes to the opponent
well one one thing you could say is well
this was good right so return a reward
of +1 but the problem is what if that
Smoove was good in the short-term but it
was bad in the long term what if it was
better to move the paddle down the ball
would go down the agent would hit the
opponent would hit it you'd hit it again
and then you were lot more likely to win
the game if you would have went down
then he went up if you just updated your
weights immediately you wouldn't have
known that you instead you should wait
until the game is finished before
evaluating how good that move was and
that's where the policy gradient
approach comes in it waits for the game
to finish for the episode to finish
before using that result as a gradient
to then update the weights so it's kind
of like supervised learning where the
label is the result right the log
probability of whether or not that
action was good or bad and then updating
the weights so that's the policy
gradient method so now now on top of
that there's something called the trust
region policy optimization method this
is the same idea as the policy gradient
method except it doesn't stray too far
from the old policy so one of the
downsides of the policy gradient method
is that sometimes updates are way too
big and this causes a lot of noise in
the loss function and it and it prevents
the the network from from converging on
the on the proper optimization scheme so
what the TR Pio method does is it says
let's not stray too far from the old
policy stay near to the old policy
making too large a change is a bad thing
so how do we do this well a naive
solution is to make a very small step
every time but then how do you take
small steps what TR Pio does is it
controls the rate of policy change
using a constraint then then comes ppl
right so PPO proximal policy implement
optimization is an implementation of TRP
oh that adds what's called a KL
divergence term to training the loss
function and so there's a there's a
there's a KL divergence term that trains
the loss function and there's also one
that updates the old policy so check
this out here's what the graph looks
like for what we're going to build in
what's called tensor board we have a
pulp we haven't fault we have a policy
function we have a old policy function
we have an we have a critic and the
actor or the actor is gonna be this
policy function okay this is gonna make
a lot more sense when I start coding it
but just recognize that we have what's
called an actor a critic and this is an
implementation of what's called proximal
policy optimization alright so let's get
to the code so the first thing we're
gonna do is we're gonna import
tensorflow all right these are our this
is our machine learning library we're
gonna import Jim and then we're gonna go
ahead and create this PPO class right
this is where our main object is going
to be that we're gonna use later on
now we're gonna initialize it in it via
our Python init function go python now
who's ready for this
the first thing we're gonna do is
initialize our tensorflow session as a
TF dot session right this allows us to
execute graphs or part of graphs this is
the tension flow graph right the main
part of our pipeline that we're gonna
place all of our objects into then we're
gonna create a placeholder this is just
a variable that we will later assign
data to at some point but right now it's
gonna be a float 32 object the size of
the tensor is going to be none give it
some hyper parameter that I then I'm
gonna define later and then it's gonna
be called the state ok now we're ready
to define our critic okay so this critic
is going to be a neural network so we're
going to use attention flows variable
scope
so the variable scope keyword is it
allows us to create new variables and to
share already created ones so this is
just a way of organizing our code a bit
better so the first layer of our network
we're gonna use the layers sub module
and it's gonna be a dense layer because
every single node and the first layer is
going to be connected to the next layer
we're gonna use that that's placeholder
as a data entry it's a gateway for data
into the network and then we're gonna
say the non-linearity is gonna be called
riilu which is a very popular nonlinear
function that makes sure our network can
learn both linear and nonlinear
functions remember neural networks are
universal function approximate errs now
let's create our value right so the
value is going to be this dense layer
and it's going to be the output of the
first layer is going to be fed into this
value variable now we're going to create
a discounted reward this is a reward in
the future that's not worth quite as
much as the reward right now and we'll
say okay this two is going to be a
placeholder tf2 float32
the size will keep it small and we're
gonna call it discounted reward to keep
things keep things organized and I'll
call it discounted reward now all right
so now the advantage this is the next
variable we're gonna use it's called the
advantage so we'll say the TIA the
advantage is going to be the difference
between the discounted reward and the
value and so basically this estimates
rather than just discounted returns it
allows the agent to determine not just
how good its actions were but how much
better they turned out to be than
expected and there should be a minus not
an equals there we go now for our loss
function so our loss function is going
to minimize that advantage over time
right we want to make sure that
advantage gets smaller and smaller so
our expected
action is going to be closer and closer
to what the real the optimal action
should be and it's a mean squared error
that's why we're using these functions
right here and lastly we're gonna make
sure to use gradient descent because one
because gradient descent is the most
popular optimization strategy out there
for neural networks and one
implementation of that is called Adam
and I've got a great video on Adam if
you just want to check out what's it
called which activation function should
you use I talked about the difference
between Adam and Nesterov momentum and
so many different amazing activation
functions using that loss function as
its parameter okay so that's our critic
we've got our critic everybody's a
critic these days right so now we're
gonna go into our actor the actor
himself Nicolas Cage because he's on the
Internet so the actor is gonna be our
policy network by the way okay so the
actor is gonna be our policy network and
our critic is actually our value network
I forgot to mention that part so the
critic is the value network and the
actor is going to be the policy network
so we separate these two concepts into
different different modules so for our
actor we're gonna say okay it's gonna
return the normal distribution of
actions and then the trainable
parameters I'm gonna go ahead and have a
simple helper function for this to build
a very similar neural network I'll call
it pi and then I'll say yes it is
trainable it was very similar it's a one
layer neural network which I can define
later and so now remember we want to
have two different policies we have an
old policy and then we have a newer
policy so I'll say let's have the same
exact outputs and we'll use the same
exact helper function except we'll call
this one the old policy and then this
one's not going to be trainable
now for our for us to sample our actions
now the next thing we want to do is we
want to sample our actions from both the
old and the new policy networks so we
can compare them so again we're going to
use that variable scope function to then
sample actions and set we're going to
say ok so the sample operation is going
to use this TF dot squeeze function and
then we're going to sample from it and
the axis is going to be 0 because we
don't want multiple dimensions and this
is basically just gonna choose an action
from from our distribution right so it's
gonna output a distribution of possible
actions we could take that's what the
neural net learns for the policy Network
and then we're gonna just choose an
action from that and then we're gonna
say ok so that's that was a sample
action and now we want to have a another
variable scope we'll call this one the
update the old policy this one's going
to update the old policy so the Wolves
call it update OPI and we'll say ok
update old policy old P dot assign P for
P of P and zip and then we have those
the new policies parameters now ok so
that's it for our for us sampling from
the actions from both now we can create
two placeholders so these are going to
be placeholders for the action and the
advantage and we're going to use these
to help us compute our loss functions in
a second so we'll say ok so the action
stuff that TFA is going to be a
placeholder not a hot ler a holder this
is not Bitcoin placeholder 2 you have to
float 32 it's gonna be very similar size
and it's gonna be called the action
and I'll just copy and paste it again
because it's a very similar line and
this time it's gonna be called the
advantage now said TF advantage and
there's gonna be one okay so now we have
these placeholders and now we can
finally say let's compute let's compute
our loss functions
so again variable does scope notice how
I'm I'm putting all of my modules inside
of this variable scope and another
reason this is useful is because in the
tensor flow graph we are able to name
these things so we can see how all of
these tensors are flowing through the
nodes in the computation graph that we
create so inside of our loss function
we're gonna have something called a
surrogate okay so you might be thinking
what the hell is a surrogate so this is
actually really interesting so let me
show you this so a surrogate is so when
it comes to sometimes in machine
learning the error surface doesn't is it
very smooth right so if you think of
gradient descent as dropping a ball
dropping a ball into a bowl and then
having that ball find the minimum so
that's easy if the error surface is
curved but sometimes it's non continuous
so this is some discrete math for you
but sometimes the surface doesn't just
easily go from one Ridge to another it's
not smooth it looks more like this right
so the ball won't just flow down to the
minimum it's gonna hit different you
know steps it's gonna go all over the
place so we want to make sure that error
surface is continuous that is very flat
that it's smooth that we can easily find
the gradient and to do that we'll create
two loss functions we have a loss
function and then we have a loss
function for the loss function and we
call this a surrogate loss function and
that the surrogate loss function will
first smooth out that air surface and
then we can find the minimum which i
think is very cool all right we're gonna
use these two terms to compute the KL
penalty so the KL by the way stands for
KU black Leibler two guys in the in like
back in add a couple
ago who discovered a really cool loss
function it's used a lot in generative
adversarial networks and in proximal
policy optimization but basically let's
let's go let's check this out so the
both of these two terms the the ratio
and the surrogate will help us compute a
Cahill panel penalty and what's gonna
happen is we're going to use the old
policy in the new policy to come to
compute the KL divergence between both
we'll take that KL divergence term get
the meed and then compute a loss using
the surrogate so then we're gonna find
the difference between the surrogate and
the lambda that we just computed times
that KL divergence term and that's gonna
give us a loss function we will minimize
that loss function using gradient
descent so these are the two separate
distinct respective loss functions that
we're minimizing one for the policy and
one for the value network which we'll
also calling the actor and the critic
network we can then write all of that to
disk and then run the session so I
actually want to show you some of the
helper methods as well that's the base
code for creating the policy and the
value networks but let's look at some of
the helper methods as well so for the
build
Anette function that I talked about this
is it it's just a single layer neural
network we compute a mean and a Sigma
and then we use that to compute a normal
distribution and return that as well as
the other parameters the global
variables but in the training loop
here's here's a real the real magic
where we implement what we just created
we'll say let's start our training loop
create an initial environment we'll have
a buffer for both the states the actions
and the rewards and in a single episode
we'll use the PPO class we just created
to choose the best action depending on
the state and that's going to return
that action now we'll take that action
and then execute that in the environment
using this step function really easy
part of using open a eyes gym and that's
gonna return the state the reward and
then whether or not we're done and some
log criteria which we're not actually
using what add all three of those to our
buffers and then we'll compute the total
reward over time then we update our PPO
object we'll say well let's get that
value that get V
using the state and then get the
discounted reward then finally we're
gonna we're gonna pop all of those off
the stack and then update our policy
using those that that new state that new
action and that new reward and that's it
then we can render the environment and
then display the frames as a gift for
whatever we want to and return all those
rewards at the very very end this is the
training loop we run it for a set number
of epochs and then eventually what's
gonna happen is that rocket is gonna use
both the policy and the value networks
the actor and the critic to learn what's
the best way to land on that landing pad
right it's gonna say if it doesn't land
it's a minus one right if it does land
it's a plus one and it can use all of
these different methods turn left turn
right turn up go you know move faster
move slower on its booster and then
there's a bunch of other gravitational
constants that are in the environment
set up for it that we you can see in the
documentation I'm gonna give you
basically it's gonna use all of those to
learn well what's the best way to try
all this out the policy is computing
these stochastically that is it's using
a distribution to sample from all the
possible actions and the value function
is going to say well here are here's the
here's how I'm gonna rate all these
possible actions the policy will say hey
value function I see how you're rating
all of these possible actions and I'm
going to learn from you and the value
function is learning as well so there
are two different sets of gradients that
are being computed that are being
computed over time and eventually
through that plus one minus one feedback
loop that we're getting from open a eyes
gym environment these two networks going
to work together to help this thing land
as efficiently as possible
right so we can say Python PPO dot pi
and then we can see it run just like
that so this thing is it's this is a
trained version of the algorithm if you
want to train it yourself I've got the
source code in the description check it
out it's the github link and I hope you
liked this video please subscribe for
more programming videos and for now I've
got to get a seat on the Falcon heavy
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>