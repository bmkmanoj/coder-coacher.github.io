<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generative Models - The Math of Intellligence #8 | Coder Coacher - Coaching Coders</title><meta content="Generative Models - The Math of Intellligence #8 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generative Models - The Math of Intellligence #8</b></h2><h5 class="post__date">2017-08-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HyuBTMaKFmU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world its garage and let's train
an AI to create a set of topics for a
news article that we give it we can
frame the machine learning process a lot
of different ways learning from data
that has labeled learning from data that
doesn't have labels learning how to act
on continuous data but a more
application focused way of framing the
learning process is it either
discriminative or generative
discriminative models tell us what some
data is they discriminate differentiate
classify call it what you will they ask
the question is this a picture of
Bitcoin or Bitcoin cache seriously what
is going on there what language is this
text who is singing this song what show
is this what type of object is this
generative models are even more exciting
they generate new data new images new
videos new music new text which reminds
me of a video by another youtuber named
Kerry for now just know that there are
two neural nets a generator and a
discriminator that are each Co evolving
to outsmart the others definitely
subscribe to him put more formally most
of the advances in machine learning have
been in discriminative models where we
try to estimate a function called the
posterior probability that is the
probability of Y given X where X is an
input sample and Y is an output so we
can imagine X to be an image and Y to be
the kind of object that is in the image
like a Tesla it tells us how much the
model believes that there is a Tesla
given an input image compared to all
possibilities it knows about generative
models instead estimate a function
called the joint probability the
probability of Y and X that is the
probability that X is an image and there
is a Tesla in it at the same time the
reason we estimate the joint probability
for generative models is because using
it we can generate images of Tesla's by
sampling car types Y and new images X
from the probability of Y and X there
are so many awesome generative models
out there that we could go over
auto-encoders try to reconstruct the
input data then we can use
learned dense representation to generate
new types of similar data generative
adversarial networks consists of two
neural networks one trying to fool the
other one by presenting fake but
realistic looking data as a
discriminator improves so does the
generator until the generated data is
indistinguishable from the real thing
but let's start with a relatively basic
one called latent dirichlet allocation
or Lda darish lei is a type of
distribution like the gaussian is
specified by a vector parameter
containing variables corresponding to
each topic which we write as such these
are darish leis distributions for three
different topics the bottom triangle has
a side for each topic and the closer a
point on the triangle is to assign the
higher the probability of a topic the
curve is the probability density
function over the mixture of topics so
the edges of the triangle all have a
zero probability just like the
probability Facebook has a super
intelligent AI sensationalists AF the
word latent is there because a variable
we have to infer rather than directly
observe is called a latent variable
since we're directly observing the words
and not the topics we refer to the
topics as a latent variables as well as
the distributions themselves we are
allocating latent darish lei
distributions over text data LD a is a
way to automatically discover topics
from a set of documents and was created
by Andrew Peng who popularized it in his
Coursera course on machine learning one
of the things I learned both when L am
running the Google team and now building
a new team at Baidu topic modeling is an
efficient way to analyze large volumes
of text data
there are many use cases for it search
engine rankings analyzing how research
topics evolve in a conference finding
out the interests of different users
using their tweaks suppose we have a
collection of sentences we could use LD
a to automatically discover topics that
these sentences contain if we want to
topics then the first topic would be a
list of words each with percent values
that indicate how relevant they are
given a topic in that category the same
goes for the second topic then it
assigns percent values to each sentence
to define how relevant each topic is to
that sentence okay
I think I'm not sure what it was I
conditions us rizal on a couple
different dataset do you apply some
disabilities that's great yeah I love
Israel well we should have submit yes
I'm cool with the Replicators wolf
people we got to reflect I'm going to
write the paper we can think of this
whole thing as a three step process we
tell the algorithm how many topics we
think there are the algorithm will
assign every word to a temporary topic
then the algorithm checks and updates
the topic assignments looping through
each word in every document now let's
look at it programmatically we can
import our documents using the pandas
library to load our data points into
memory as a data frame object we're
going to perform some cleaning steps
here straight out of Compton I mean the
natural language processing playbook
since our words matter to recess
tokenizing stopping and stemming
tokenizing segments a document into
individual tokens that we define the
size of since we're applying Lda at the
word level we'll tokenize our documents
into words
once we've converted our documents into
a set of tokens we'll want to remove
stop words these are the meaningless
words that won't contribute to our
topics like for and or and art history
majors the very definition of a stop
word is flexible we can define that list
ourselves or we can use a predefined
list it also depends on the topic or
modeling if we're modeling a collection
of music reviews then terms like the who
will be hard to model since the will
usually be removed even though it's a
part of the band name next we'll perform
stemming this combines similar tokens in
our case words into a single token like
a a and a could be reduced to a now
we'll want to create a document term
matrix because we need to understand
frequency with which each term occurs
within each document the rows are docs
in the collection and the columns are
the terms will traverse through our
documents assigning unique integer ids
to each unique token while also counting
words
all of this will be stored in a
dictionary now we're ready to generate
an Lda model using this document term
matrix we'll iterate through every
document we have in our data set and
randomly assign each word in the current
document to one of our topics we'll go
through each word in the document and
for each topic compute two probabilities
then reassign a new topic by using these
probabilities the probability that we
place a word in the correct topic will
increase every time since we will place
more and more words into the correct
topic we can examine the results by
printing out the number of topics we
chose as well as a number of words per
topic these topics seem very news
related and the word seems to be very
relevant to the topic so I think we did
a pretty good job
to summarize generative model learn a
joint probability distribution between
input data and labels while
discriminative models learn the
conditional probability distribution
between them topic modeling is all about
finding the hidden semantic structure in
a collection of documents and latent
dirichlet allocation is a generative
topic modeling technique that can be
implemented pretty easily
Hammad shake is the wizard of the week
he created a linear regression model
using a Bayesian approach to find the
relationship between movie sales and
movie ratings this is very likely the
best may Jupiter notebook on the topic
on all of github so take a look at it
for sure very well done and the run rope
is no Ladell who use baby an
optimization to find the learning rate
for a neural network that correlates bic
one search frequency and its product
also definitely check that one out
great work now this week's challenge is
to perform lva
on a text data set of your choice bonus
points for real world youth cases post
your github link in the comments and
winners will be announced next week
please subscribe for more programming
videos and for now I've got to find a
better topic so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>