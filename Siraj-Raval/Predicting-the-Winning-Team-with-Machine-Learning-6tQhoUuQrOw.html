<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Predicting the Winning Team with Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Predicting the Winning Team with Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Predicting the Winning Team with Machine Learning</b></h2><h5 class="post__date">2017-08-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6tQhoUuQrOw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and our task
today is going to be to try to predict
if a team is gonna win a game or not now
this is for football or as Americans
call it soccer which is one of the most
which is the most popular game globally
when it comes to sports and of all the
domestic teams out there the English
Premier League is the most popular of
all of them so we're gonna predict the
outcome for an English Premier League
team using a data set of past games and
this data set I'll show it to you right
now has a bunch of different statistics
this is what the data set looks like
right here you've got a home team and
you've got an away team right here so it
could be Arsenal it could be Chelsea
Brighton Manchester City so you've got a
home team and you've got an away team
and then you've got a bunch of
Statistics so these are all acronyms but
I have definitions for all these
acronyms that we can look at right over
here right so we have acronyms for the
full time home team goals the home team
the away team the shots the target the
corners the amount of yellow cars the
amount of red cards so there's a lot of
different statistics here right there's
so many things that go into what makes a
team win or lose right and so we're
going to take all of these features and
then we're gonna use them to try to
predict the target or the label and the
label in our case is going to be the FT
R which is the full time result so the
FT R is right here right H a HD right so
it could be either the home team H the
away team a or a draw D so it's a
multi-class classification problem this
is not a binary classification problem
it's not just what the home team wins or
loses its multi-class because there are
three possible labels home team away
team or draw so that's what we're going
to try to predict given all of those
features in the data set before I show
you the steps
let me just demo this really quickly so
I can just say X test and then just take
the first row from this and the labels
are gone this is just for the all the
features given no label and we can see
that it says home write so it's able to
predict given all those other features
whether or not a team is going to win
lose or tie the game
okay so back up to this we're gonna try
to predict the winning football team and
our steps are gonna be the firt it's a
four-step process so our steps are gonna
be to first clean our data set make sure
that we only use features that we need
what do I mean by that when it comes to
predicting who's gonna win a team
there's an entire industry around this
right there are pregame analyses by
commentators or postgame analyses by
commentators entire channels like ESPN
are dedicated to trying to predict who's
gonna win a match and in fact even
during the game there are commentators
trying to predict who's gonna win like
during halftime who's gonna win the full
game so this is a this is something
that's been going on for forever rice
it's gladdie or gladiator Roman days or
whatever it's been going on right people
trying to predict who's gonna win a
match but we're gonna do something that
people don't do often and that is using
statistical analysis or otherwise known
as machine learning mathematical
optimization to try to predict who's
going to win if you think about it this
is like one of the most perfect machine
learning problems out there trying to
predict who's gonna win think of all the
features out there and those features
don't necessarily have to do with the
game they could be the sentiment of the
audience the sentiment of the crowd of
news articles how are people talking
about a team what hashtags related to
the team are trending on Twitter are
they home are they away what's the
weather like that day what are the
forecast predictions so there's so many
different data points that could go into
potentially from across the web telling
us whether or not a team is gonna win or
lose but since I've never talked about
this topic before I'm just gonna start
off from a very basic level and based on
your feedback and how you feel about
this topic I can talk about it more and
do more advanced things later okay so
we're gonna clean our data set then
we're gonna split it into a training and
a testing set and what I mean by that is
we're gonna use scikit-learn to do that
I have still I have yet to find a better
library for splitting training and
testing data then scikit-learn it is
still like the best out there even if
I'm using tensorflow
or PI torch to build my model I'll still
use scikit-learn to split my training
and testing data it's just a one-liner
super simple and then once we split it
we're gonna train it on three different
classifiers so remember this is a
classification problem a multi-class
classification problem and so we're
gonna use either logistic regression
support vector machine or and I've
talked about both of those in my math of
intelligence series links to those in
the description but I'll also talk about
them a little bit in this video just as
a refresher and the third one is a model
that I haven't talked about before and
that's called X G boost well you could
think of it as a technique model same
thing so we're gonna use those three as
our classifiers we're gonna train all
three of them on the data set and then
we're gonna pick the classifier that has
the best result and that is gonna be the
classifier that we use to predict the
the winning team and we're also going to
optimize its parameter or its hyper
parameters using grid search right so
we're using an ensemble of machine
learning methods which psyche alert
makes very easy to do once we pick the
right one then we'll optimize that model
and then that will take that optimize
model and use that to predict the
winning team and so the history of this
is like I said it's been going on for a
long time and sports betting has just
been increasing in popularity for many
years right if you look at the past five
years it's growing at double-digit rates
and there's a lot of reasons for this
number one is just the accessibility of
the internet rights more people have
internet access and embedding on the
Internet is easier than in-person
another reason is just that machine
learning is becoming democratized and so
everybody's being able to build these
predictive models to try to predict
these scores so this is this is
definitely a field that's increasing in
popularity and and this is not something
that's happening in the fringe of
society this is a very mainstream task
Kaggle the data science community hosts
this yearly competition called march
madness or machine learning mania
whatever you want to call it to try to
predict the scores for the NCAA that is
basketball and you have an entire
community around this and people are
trying out different models and
discussing them so definitely check that
link out as well so this is something
that's happening and I also found you
know several papers talking about this
so it's not just something that's people
who want to make money do this is
something that legitimate researchers at
academic institutions look into and try
to try to predict right so from this
paper I I'm quoting verbatim it is
possible to predict the winner of
English County twenty20 cricket games
and almost two-thirds of instances right
and then for this other paper right here
something that becomes clear from the
results is that Twitter contains enough
information to be useful for predicting
outcomes for the Premier Li that's for
Twitter right right here so they use
Twitter sentiment to try to predict just
Twitter alone to try to predict who's
gonna win so there's a lot of different
angles we can look at here right we
could use sentiment analysis we could
use the past score history we could use
a whole bunch of different things we're
gonna use a score history but you could
try to simulate the game and a
simulation and then you know try to see
from that but you know that there's a
lot of different possibilities here and
check this out in 2014 Bing which is
owned by Microsoft correctly predicted
the outcomes of for all the 15 games in
the knockout round for the 2014 World
Cup every single game 15 of them own
hundred percent accuracy so you can be
sure that bings model is really good
however they are not going to share it
with us because it's it's kind of like
you know financial analysts at JP Morgan
or chase if they know how to predict
these stock prices they're not gonna
tell us why would they share their
profits with us so what we've got to do
is we've got to figure it out for
ourselves to try to reverse-engineer the
techniques so that we can benefit from
it okay so that was a little primer on
the background so back to the data set
so this data set that I got is from
football data UK you can find it right
here if you go to slash data PHP and
then what I did was I selected the
England football results and luckily for
us they've got data sets for every
season back like two decades so it's
perfect and if you want one you could
just click on Premier League and boom it
downloads just like that and I showed
you the data set so one thing right off
the bat that we can notice is that if we
were to just graph and I've already done
this beforehand for us and it's in
markdown right here we'll see that the
home team has the majority stake of this
graph so that means right off the bat
without doing any machine learning we
already know that if you are a home team
you have an advantage to win
probabilistically speaking if you're the
home team you're more likely to win than
if you're not just from bet from that
alone and we can reason about this a
couple ways we could say well if you're
the home team then you know football is
a team sport and a cheering crowd helps
you and
to travel through your less fatigued you
know you're familiar with the pitch and
the weather conditions all these things
you had a hot dog from the stand and it
tastes really good just kidding
baseball food or any kind of sports or
like stadium food is never good you know
what I'm saying I've got two great
repositories for us I'm about to start
the code here but I've got one for
another EPL prediction great ipython
notebook or jupiter notebook and I've
got one for that that kaggle competition
that I just talked about for NCAA
prediction definitely check them both
out and this guy adesh Panda has really
great tutorials and software on his
github so just check out all of his uh
repositories because he has some really
great example code so what we're gonna
do is I'm just going to code out a good
part of this just from the start and
then we're going to just go over the
rest okay so don't save all right move
move move move move okay so first things
first
so our dependencies in this case are
going to be to import pandas for data
pre-processing we want to import pandas
because that's like the most popular
data processing library and we also
talked about XG boost right that is one
of the other machine learning models
that we want to use we're just going to
form a prediction model based on an
ensemble of decision trees which I've
talked about as well decision trees and
so another thing we're gonna do is we're
going to import logistic regression
right that's model two of three there
are three different models that we're
going to train our data set on one of
them is XG boost the other is logistic
regression which is used whenever the
response variable is categorical right
either yes or no or you know some kind
of non continuous discrete value you
know black white red green you know
things like that
so which is perfect for us huh you know
win lose or draw so we have logistic
regression and then we have one more
which is going to be the support vector
machine right support vector machines
I'll talk about that as well and then
finally we're going to want to import
this display this display library
because we are going to display our
results okay so that's it for our
dependency
and now we can read our data sets so now
we're gonna go ahead and look at pandas
and pandas gonna is gonna let us read
from our CSV file that we downloaded
that I've called final data set CSV and
then once we have that we're going to
preview that data so I'm gonna say okay
just go ahead and display the data that
I've just pulled into memory as a
panda's data frame object I'll look at
its head that is just the first few
columns of that data set and once I have
that Hocking I can go ahead and print it
and now we can see this this data set
what it looks like and so notice there's
a whole bunch of acronyms here lots of
data sets have acronyms like this and
that can be confusing but like I said
I've got this legend of what each
acronym means the home team goal
difference the difference in points a
difference in last year's prediction for
the past three games the wins for the
past three games for the home team the
number of wins for the past three games
for the away team so you know I've kind
of aggravated this data and I've just
made it into something a little more
consumable and so still remember that we
still have one single target that we're
trying to predict and that is FTR right
the full time result for the full time
game who is the team that won the home
team the away team or was it a draw and
so that's our target that we're trying
to predict so before we get into
building this model let's first explore
this data set so if we were to explore
this data set we could say okay so first
of all let's just kind of think about
what is the win rate for the home team
so what is the win rate for the home
team so how often does the home team win
aside from anything else this is kind of
what we just talked about right how do
we do this programmatically what we say
okay get the total number of matches and
that's gonna be that first index in the
data frame object and then calculate the
number of features from it so we want
the number of features and we'll
subtract one because one of them is
gonna be the label that's not going to
be our feature right the FTR so we'll
subtract one from that and then we're
gonna calculate the matches one by the
home team which is going to be the
length of the data all right
Toph tftr okay and for that for the home
team so that's number of matches there
that were won by the home team and
finally we'll calculate the win rate the
win rate for the the home team as well
and then once we have that finally we
can print out the results and it's gonna
tell us exactly how many times the home
team has won as a percentage of all the
wins so I can go ahead and print that
I've got this print statement right here
and then we can go ahead and see the
result
okay so already this is a this is the
graph that I showed at the beginning 46
percent about 46 percent of winds are
from the team that is home just right
off the bat just something for us to
know right where we're exploring the
data we're trying to think about what
are the features that matter the most
right feature selection that's the
process that we're going to now we're
going through so remember when it comes
to deep learning we don't have to really
think about what are the ideal features
deep learning learns those features for
us however that's like a next step we're
just gonna try to build some more basic
models first and then you know whether
or not you know based on feedback of how
you guys like this topic I might do a
deep learning video on sports analytics
later but right now we're just gonna
build these three simple models and
thinking about feature selection is a
really important skill to have as a data
scientist
so if you write which deep learning you
don't have to do that but again you've
got to have a lot of GPUs and crucially
you have to have a lot of data right you
have to have a lot of data to be able to
do that now in this case we don't have
that much data we have in this data set
it download it in like you know two
seconds of course it was only 500 it's
only about 500 data points right we want
a huge amount of data at least a hundred
thousand now if we had at least a
hundred thousand data points then this
would be something to use deep learning
for right if we're trying to aggregate a
bunch of different results sent to MIT
from Twitter past team scores different
you know talking points from other
people then we would use something like
deep learning but in this case we want
to try to visualize the distribution of
this data so what we'll do is we'll say
okay so from pandas there's this great
tool that lets us come
what's called the scatter matrix and the
scatter matrix basically shows how much
one variable affects the other so we're
gonna build a scatter matrix for a set
of our features to try to predict to try
to see just visually what is the
correlation between these different
features and see just for ourselves this
this this will help us pick the relevant
features that we want to use right so we
have the home team goal difference we
have the away team goal difference we
have the home team points the away team
points the difference in points and the
difference in last year's prediction
okay and so once we visualize this some
of them have a positive correlation the
line is going up some of them have a
negative correlation so that means like
in terms of so that means if the goals
increase for the home team then maybe
the points decrease for the for the away
team right and so we can look at the
positive versus negative correlations
that's an indicator of how features are
related together right this doesn't have
some direct relation to what we're about
to do but just good practice to think
about ways of visualizing our data
seeing the relationship between between
different features and then trying to
predict what those best features are for
our model ok so then once we've explored
our data we're gonna prepare it so
remember we have one single target
variable one single objective or label
as we like to call it and that is the
FTR the full-time results so what we
want to do is say given all of those
other features try to predict the FDR
okay and make us some money yeah no I'm
just kidding I mean yes actually you
probably want to make some money we're
trying to predict the full time result
right and so we're gonna split it into
the FTR and then everything else then
we'll standardize it which means it's
all gonna be on the same scale we that
means we want all of our data to be an
integer format and we want it all to be
on the same scale so it's not like we
have like one feature is in the hundreds
of thousands and then the other feature
is in the you know between 1 and 10 if
we're if they're gonna be small values
we want them all to be small values and
what this does is it improves our
prediction capability of our model so
once we've standardized our data then
we're gonna add these three features
which is the the last three wins for
both sides and we looked at that before
right
hm 1 2 3 and then a
and an am one two and three so if we
look back at the data some of the data
was categorical like if we look at this
data set you know we have the referee we
have HTR we don't want any of that right
we want all of our data to be a number
we want it to be some continuous
variable no discrete numbers so we're
gonna pre-process those features by
saying create a new data frame find
those feature columns that are
categorical by saying if it's if the
data type is equal to equal equal to
object instead of an integer and then
convert it into an integer right
so that way we remove all the
categorical features we only have one
categorical variable and that is our
labeled the FDR we don't want our
features to be categorical those are
gonna be continuous variables and so
once we have that we've pre-processed
our data we've explored it we've added
the the features that we thought were
most relevant and we could see them all
here right no more categorical features
they're all numbers and so once we have
that now we can train and we can split
our model into a training and a testing
data set it with a very easy one-liner
which scikit-learn right this is gonna
split our with the train train test
split function it's gonna split that CSV
it's gonna split that data frame object
into a training and a testing set and it
already knows what the label is gonna be
and it's going to put them all in a one
dimensional array all of those labels
the FTR scores for each of the
associated inputs and we have 12
features right we have 12 features for a
single input and so for the next step
now we're gonna actually build this
model so I'm gonna come back to these
helper functions that are gonna help us
train the model but let's right now just
build this model right so I'll go down
here so let's just write this out right
now okay so I'm gonna say okay so we
know that the first model that we want
to try out or at least one of the models
that we want to try out is logistic
regression I'll give it some random
state as a seed that you know this could
be any number of things right well I'm
just gonna say you know 40 I could say
42 it doesn't matter but just some seed
number and we could try out different
seeds to see how the results vary but
I'm just gonna you know put some magic
numbers down right now to to get some
result out and so the next classifier
we're gonna build is a support vector
machine so the order of classifier
as I initialized them doesn't really
matter so so that's irrelevant but the
fact that I am initializing them is
important because it means that these
are the three important ones that we are
using and so my third classifier is
going to be XG boost now then I'm gonna
talk about what all of these are in a
second but let me just write them out
here we have an X G boost classifier and
then we have a C that's gonna be my 82
let me print that boom boom for a B and
C okay right and so if we train this
we'll see that clearly the XG boost
library did the best so we already know
that XG boost is the best model for this
data and notice that the XG boost model
had an accuracy score and an f1 score of
about 74 percent that is a 74% accuracy
on the testing data set which is a
really good it's it's really good it's
better than just guessing right it's way
better than just trying to guess what
team is gonna win that's about a 75%
accuracy it's pretty good so let's let's
go back and see what these models are by
the way so these models so logistic
regression now remember I have a video
on logistic regression and I have a
video on support vector machines just
search both of those on YouTube and then
the word Suraj it'll be the first link
that shows up but for logistic
regression
it's used to predict the probability of
an occurrence of an event by fitting
data to logistic curve so a logistic
curve consists of this equation right
here the probability right so if you
have two classes if it's a binary
classification problem whether or not
someone is dead or alive the x-axis
would be the concentration of the toxin
whether you know you're trying to
predict if someone's gonna live or die
based on this toxin and the y-axis is
gonna be the probability of each of
those classes and we use a logistic
regression curve denoted by this
equation where you just plug in the x
value and it will output a probability
to show that now in the multi-class case
as is our problem as is our problem
we're going to use multinomial logistic
regression which the library does for us
but that's what logistic regression does
it's use extensively
across a wide range of fields and it's a
very very popular model that's gonna be
our first and these are all
classification models by the way
remember once we frame our problem then
we can pick what model we want to use we
know that this is a classification
problem therefore we want to use a model
that is well suited for classification
and then the next question is based on
our data which of the models is best to
use and we don't always know that right
off the bat even very experienced data
scientists don't always know that so
they have to try out several models to
see which one works best and then for
support vector machines so what a
support vector machine does is it will
find so let's say we have two classes
and we plot them in two-dimensional
space just like in this image right here
what it will do is it will try to find
the points that are closest to each
other to find the smallest margin
between both classes and once it found
finds these points these support vectors
it will build a hyperplane right in the
middle so that they sort of the distance
between that line and both of those
points is the smallest and the reason it
does that is so that once we give it a
new data set a new data point whatever
side of the line it falls on that's the
class it's going to be that's how we
classify it so in the simple case for
two classes it's just a line and then it
just Falls and the data will fall on one
of two sides but in a more complex case
you'll have all three classes and then
it will draw a line that's kind of
curved between them so it'll just like
segment the graph into the three
different segments but the idea is still
the same finding those closest points
and finding the line that minimizes the
margin okay so that's where support
vector machines and the last one is XG
boosts so we talked about random forests
it's kind of it's it's very similar to a
random forest the XJ boost algorithm is
one of the most popular algorithms on
Kaggle when it comes to winners there
are a lot of XG boosts is happening but
basically the classification and
regression tree the decision tree that's
used for both classification and
regression is a good model it's not a
great model but it's a good model it's a
very simple model right you give it a
bunch of features and it's going to
slowly build there's a variety of ways
of doing this but it's gonna build a
tree or each a branch or level in the
tree equates to one question right so
you're trying to predict whether or not
it's gonna rain it'll be like is the is
there is a sky cloudy yes no yes okay
did it rain yesterday yes no yes okay
then there's a 75% chance it's gonna
rain so that's a decision tree right so
what XG boosts does it's a gradient
boosting technique what it will do is it
will create a bunch of weak learners
though those are decision trees that are
okay like their their predictive
capability isn't that good and it will
combine the results of all of them so
it's an ensemble method it will take all
these trees and then find a result by
using the prediction capability of all
of those trees so that's 4xg boost and
we have this tree right here where we
have input age gender occupation a bunch
of different features and we're trying
to answer the question
does this person like computer games
different trees will specialize in
answering different parts of the
question like does this person use the
computer daily what's their age what's
their gender and then we'll combine the
results from all of them so the function
of this kid is the result of these two
trees
predictions combined so that's what each
of those are and then if we go up here
back to these helper methods that I was
going to talk about by the way the f1
score is just a measure of a models
accuracy it's a very standard score that
just measures how accurate a model is so
back to these three helper methods what
we did here was we gave the train
predict function our classifier as well
as the training and testing data sets so
if we go back up here we'll see that in
the train predict method we indicated
the classifier that we're gonna use
then we trained it given the training
and testing data we predicted the
results and then we predicted the labels
and so this train predict method use the
train classifier map method to start a
clock fit the model and then print the
results and then for the labels it
started a clock made a prediction and
then stopped the clock so that was it
they just predicted the labels and then
it fit the classifier just that and so
once we did that we realized that x-g
boost gave us the best result right so
exci boost is the model that we want to
use a 74 percent accuracy but that's not
enough right now that we know that
x-g boost is the best model now we can
say okay let's optimize this model and
what there
different ways that we could optimize
this model right but in our case we're
gonna optimize it by optimizing the
hyper parameter so this is hyper
parameter optimization there are a bunch
of different hyper parameters that go
into XG boost so we were kind of
shielded from that because we use the
scikit-learn library but we can use
scikit-learn ironically enough to
optimize hyper parameters that we don't
even normally see so if we come down
here we can import grid search which is
a which is basically brute forcing
that's that's what grid search is we're
brute forcing all the possible
combinations of all the hyper parameters
will create an initial set of hyper
parameters here will initialize the XG
boost classifier and we'll make an f-1
scoring function and then perform grid
search on that classifier with the
scoring function given the initial
parameters that we just defined up here
and then it's going to find the ideal
parameters for that model and notice
that the f1 score and the accuracy score
increased right so after we optimize the
hyper parameters the f1 scoring the
accuracy score increased which means
that our model is now way more optimized
anyway disclaimer you know you could
make money using this you could lose
money using this who knows right this is
this is it this is an educated guess
this is a statistical guess based on
past data sets but we can definitely
improve this model right we could bring
in more data more relevant features we
could bring in sentiment analysis we
could add other features but there's
also one more thing that I want to say
so it's able to predict whether or not
the home team will win right but based
on a data point from this CSV file
however we don't always know what all of
these things are how are we supposed to
predict whether or not there's gonna be
you know X number of fouls how are we
gonna how are we supposed to know
whether or not there's gonna be X number
of off sides we don't necessarily know
these things beforehand these features
beforehand so the trick is to pick
features that are completely predictable
what do I mean by that that means that
what are features that are gonna help
predict who's gonna win but those
features themselves are predictable like
how many players are gonna be on the
team well you know for a fact they're
gonna be five players on the team who
are the players gonna be what is the
lineup
when is the game happening where is the
court right so things that you know for
sure and so if all of your features are
known they're all predictable then your
result will not require you to guess
what those features are which is the
case in this very basic example another
another way to improve the model is to
just use way way way more quality data
we could also just predict each of these
features themselves like we could try to
predict how many goals are gonna occur
in a game we could try to predict all
these things and then just have
probabilistic values for all of these
features and then try to predict the
home team right so there's a lot of
machine learning that could be happening
but ideally we know for sure what all
these features are gonna be and we can
use them to then predict the winning
team so if you're interested in this
topic definitely check out all the links
in the description and let me know what
you guys thought of this topic in the
comments please subscribe for more
programming videos and for now I've got
to play some soccer I mean football so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>