<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Convert Text to Images - Intro to Deep Learning #16 | Coder Coacher - Coaching Coders</title><meta content="How to Convert Text to Images - Intro to Deep Learning #16 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Convert Text to Images - Intro to Deep Learning #16</b></h2><h5 class="post__date">2017-04-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gmvRStL_Dag" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">poor world in suraj and we're going to
dive into some reality bending research
today
we'll write a program that generates an
image from just a text description
there's a human condition called
synesthesia where a simulation of one
cognitive pathway leads to experiences
in a second cognitive pathway so a
person with synesthesia could be
listening to Beethoven and it would
trigger the taste of a sweaty armpit in
their mouth or see the color red when
looking at the number nine the word
literally means joined perception many
great artists have this condition and no
doubt it adds to their artistic ability
there's something really intriguing
about this idea of being able to express
a thought an idea a concept in two
entirely different mediums can we
replicate this ability programmatically
hmm yes
generative adversarial networks have
demonstrated that it is possible to
create fake but photorealistic looking
images after training on an image data
more recently a paper called stat Gann
demonstrated that given any text
sentence their model could then create a
photorealistic image out of it think
about the implications of that this
ability to synthesize one data type to
another will eventually allow us to
create all sorts of entertainment with
just detailed descriptions designers
engineers and scientists could all hash
out theoretical concepts with real-time
visual feedback on their thought
processes but at the same time bad
actors could use this technology to
manipulate people into believing
something is real when in fact it's
actually fake or only to get smarter to
discern the difference between the two
and indeed several big names are all now
starting to work on ways of merging our
brains with machines which would help us
do just that think direct thoughts to
media synthesis you and I are living in
the most exciting time in human history
right now so how does that can work it
consists of two games the first one is
called stage one it takes a sentences
input and outputs an image with
primitive shapes and some basic colors
it's a low resolution
image like what a Windows Phone would
capture then the second Gann stage 2
takes that low res image and original
sentence as input and generates a much
higher resolution version of that image
by completing all the details
a popular task in machine learning is to
learn what's called a density model of a
distribution of some data set so for the
density model P of X it would accept
some input and say yes that's X or no
that's not X X being the data we want to
model gans have been used to learn
density models we want to be able to do
two things with this model the first is
a sample from it that is imagine
variations of the training data that
we've never shown it before and the
other is to condition on external data
so that we can specify particular
attributes as we sample we humans have
no problem doing this imagine a pink
elephant with wings see how fast you did
that they used a set of images of birds
and flowers with associated text labels
as their training data so the task was
to build a conditional density model
instead of answering the question is
this an X the question is instead is
this an X given a y this means we can
specify particular attributes we want in
the generated image we can ask the
generator to generate a bird with a
yellow tail and that's the weather the
bird has the yellow tail so we'll be
able to directly control the output of G
the first step is to encode the text
description into an embedding using a
pre trained encoder like word to death
now in our programmatic implementation
we'll see that the only dependencies
we'll need are tensorflow numpy and our
helper class if we move down to the main
function we can see a high-level
overview of what's happening we first
load up our skip top model and store it
in the model variable skip thought also
called sent to Veck is the encoder part
of an encoder decoder model that was pre
trained on a large text corpus to
reconstruct a piece of text from its
encoded vector so we'll use it to
convert our input text into an embedding
which is initialize our tensorflow
session and then initialize our models
inside of it we'll need to give our
models the text embeddings the session
and our predefined a batch size as its
parameters we want to extract latent
variables from our text embeddings and
we could just do this directly but
instead we'll randomly sample latent
variables from a Gaussian distribution
where the mean and the diagonal
covariance matrix are functions of the
text embedding why because the latent
space condition on text is usually high
dimensional like more than a hundred
dimensions and we have a limited data
set latent variables generated from it
directly would make it hard for our
generator to learn if we had much more
data than it would make sense but we
don't so this step helps us learn from a
smaller data set minimax
so for the generator ad convolutional
Network the text embedding is fed into a
fully connected layer to generate the
mean and the values for the diagonal in
the covariance matrix so that we can use
them for the independent Gaussian
distribution
we'll use these values to compute a
conditioning vector that we can then
concatenate with a noise vector to
generate an image through a series of up
sampling blocks the discriminator a
convolutional Network takes the text
embedding as input and compresses it to
less dimensions ultimately forming a
three dimensional tensor we also feed in
the image through a series of down
sampling blocks until it's a 2d tensor
we then concatenate both tensors
together and we use the resulting tensor
as input to a convolutional layer to
jointly learn features across the image
and the text
he has to judge two kinds of inputs real
images with matching text and generated
images with arbitrary text it's got two
separate two sources of error
unrealistic images for any text and
realistic images of the wrong class that
mismatch the conditioning information
this could complicate the learning
dynamic so we separate the error sources
in addition to the real and fake inputs
to D during training we add a third type
of input real images with mismatched
text which do you have to learn to score
as fake the discriminators goal is to
maximize the probability that the image
condition on the text is from the true
data distribution and minimize the
probability that the image is not from
the true data distribution the generator
job is to create samples that fool D it
minimizes the probability that the image
is not from the true data distribution
and we use the popular regularization
term the KL divergence to help avoid
overfitting
so eventually G from the stage 1 Gann
will get really good at generating low
res images they aren't very clear and
could contain all sorts of shape
distortions some of the details in the
text input could be omitted in this
first stage so we introduce stage 2 Gann
that generates photorealistic high-res
images it conditions on these low res
images and the text embedding again to
correct any defects from stage 1 and
extract any previously missed
information in the text to generate more
photorealistic detail for G similar to
the previous stage we'll use the text
embedding
to generate a lower dimensional Gaussian
conditioning vector which we then
convert to a three dimensional tensor we
also input the sample generated by the
first scan through a series of down
sampling blocks until it's a 2d tensor
then we concatenate the image and text
tensors into one tensor we then feed
that resulting tensor into several
residual blocks to jointly encode the
image and text features lastly it's bed
through a series of up sampling blocks
that will ultimately generate an image
for D it's a similar structure to stage
ones D but with more down sampling
blocks since the image size is a larger
in this stage once we've got some image
text pairs we can train our model in a
sessions run function for the number of
batches we defined we will separately
train our stage two again in its own
session using the generated images as
input once trained we can use the stage
two ganz generator to generate some
images given a text input if we look in
our saved folder after training we can
view the low res images that were
generated we can also view the high res
generated images in their respective
folder as long as you have some images
and some related text you can convert
pretty much any text into a novel image
using this model let's do a recap dans
can be used to learn a conditional
density model one great application of
this is converting text to images and
stackin is an architecture that can do
just that first converting text to a low
res image then to a high res image last
week's coding challenge winner is Mike
McDermott I had some issues in my video
generation code which he made a great
pull request for making the dependencies
much easier to install so you can get
started generating videos of your own
great job Mike it was much needed wizard
of the week this week's coding challenge
is to use again to convert text to
images using an entirely different data
sets
details are in the readme github links
go into comments and winners will be
announced a week from today in the last
episode of this course please subscribe
for more programming videos and for now
I've got to create the finality so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>