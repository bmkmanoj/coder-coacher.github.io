<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make a Language Translator - Intro to Deep Learning #11 | Coder Coacher - Coaching Coders</title><meta content="How to Make a Language Translator - Intro to Deep Learning #11 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make a Language Translator - Intro to Deep Learning #11</b></h2><h5 class="post__date">2017-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nRBnh4qbPHI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and let's make
our own language translator using
tensorflow
today there are about 6,800 different
languages spoken across the world and in
an increasingly globalized world nearly
every culture has interactions with
every other culture in some way that
means there are an incalculable number
of translation requirements every second
of every day across the world
translating is no easy act a language
isn't just a collection of words and a
rules of grammar and syntax it's also a
vast interconnected system of cultural
references and connotations and this
reflects a centuries-old problem of two
cultures learn to communicate are
blocked by a language barrier our
translation systems are fast improving
though so whether it be an idea or story
or a quest each new advance means one
less message will be lost in translation
during the Second World War the British
government was hard at work trying to
decrypt the Morse coded radio
communications that Nazi Germany used to
send messages securely known as enigma
they decided to hire a man named Alan
Turing to help in their effort and when
the American government learned of their
translation effort they were inspired to
drive themselves post-war specifically
because they needed a way to keep up
with Russian scientific publications the
first public demo of a machine
translation system translated 250 words
between Russian and English in 1954 it
was a dictionary based so it would
attempt to match the source language to
the target language word for work the
results were poor since it didn't
capture syntactic structure the second
generation of systems used in terlingua
that means they changed a source
language to a special intermediary
language with specific rules encoded
into it then from that generated the
target language this proved to be more
efficient but this approach was soon
overshadowed by the rise of statistical
translation in the early 90s primarily
from engineers at idea innovation at IBM
a popular approach was to break the
source text down into segments and
compare them to an aligned bilingual
corpus using statistical evidence and
probabilities to choose the most likely
translation nowadays the most used
statistical translation system in the
world if Google Translate and with good
reason
Google uses deep learning to translate
from a given language to another with
state-of-the-art results so how do they
do this
let's recreate the results in tensorflow
to find out the data set we'll be using
to train our language translation model
is a corpus of transcribed TED Talks
it's got both the English version and
the French version and our goal will be
to create a model that can translate
from one to the other after training
will be using Pinter flows built-in data
utils class to help us pre process our
data set and we'll start by defining our
boat capsized which is the number of
words we want to train on from our data
set we'll say it's a 40k for each which
is a small portion of the data then
we'll use the data utility class to read
the data from the data directory giving
it our desired boat capsized and it will
return the formatted and tokenized words
in both languages well then initialize
tensorflow placeholders for our encoder
and decoder inputs both will be integer
tensors that represent discrete values
they will be embedded into a dense
representation later we'll feed our
vocabulary words to the encoder and the
encoded representation that's learned to
the decoder now we can build our model
Google published a paper more recently
discussing a system they integrated into
their translation service called neural
machine translation it's an encoder
decoder model inspired by similar work
from other papers on topics like text
summarization so whereas before Google
Translate would translate from language
a to English to language B with this new
nmt architecture it can translate
directly from one language to the other
it doesn't memorize phrase to phrase
translations instead it encodes the
semantics of the sentence this encoding
is generalized so it can even translate
between a language pair like Japanese
and Korean that it hasn't explicitly
seen before so I guess we can use an LS
TM or current network to encode a
sentence in language a the RNN spits out
a hidden state S which represents the
vectorized contents of the sentence we
can then feed at
to the decoder which will generate
translated sentence in language B word
by word sounds easy enough
right wrong there's a drawback to this
architecture it has limited memory that
hidden state s of the LS p.m. is where
we're trying to cram the whole sentence
we want to translate s is usually only a
few hundred floating point numbers long
the more we try to force our sentence
into this fixed dimensionality vector
the more lossy our neural net is forced
to be we could increase the hint size of
the LF DM after all they are supposed to
remember
long term dependencies but what happens
is as we increase the hidden size H of
the LS TM the training time increases
exponentially so to solve this we're
going to bring attention into the mix if
I was translating a long sentence I
probably glanced back at the source
sentence a couple times to make sure I
was capturing all the details I interr
ibly pay attention to the relevant part
of the source sentence we can Lightner
let's do the same by letting them store
and refer to previous outputs of the LF
TM this increases the storage of our
model without changing the functionality
of the LSPs so the idea is once we have
LS TM outputs from the ink coder stored
we can query each output asking how
relevant they are to the current
computation happening in the decoder
each encoder output is a relevancy score
which we can convert to a probability
score by applying a softmax activation
to it then we extract a context vector
which is a weighted summation to the
encoder outputs depending on how
relevant they are memory paint enough
pay attention
yeah buddy L 90s plots to know spam is
mixed in the pop-out ammonium L ask to
be sent Debrecen style memory paint
enough pay attention we build our model
using tensorflow
built-in embedding attention sequence to
sequence function giving it our encoder
and decoder inputs as well as a few
hyper parameters we define like the
number of layers it builds a model that
is just like the one we discussed
tensorflow has several built-in models
like this that we can drop into our code
easily so normally this alone would be
fine and we could run this and the
result to be decent but they added
another improvement to their model that
requires more code 100 GPUs and a week
of training
that's what it took we won't implement
it all programmatically but let's dive
into it conceptually if the outputs
don't have sufficient context and they
won't be able to give a good answer we
need to include info about future words
so that the encoder output is determined
by words on the left and right we humans
would definitely use this kind of full
context to determine the meaning of a
word we see in a sentence the way they
did this is to use a bi-directional
encoder so it's two RNs one that goes
forward over the sentence and the other
goes backwards so for each word it
concatenates the vector outputs which
produces a vector with context from both
sides and they added a lot of layers to
their model the encoder has one
bi-directional RNN layer and seven
unidirectional RNN layers the decoder
has eight unidirectional RNN layers and
more layers the longer the training time
so that's why we use a single by
directional layer if all the layers were
bi-directional the whole layer would
have to finish before later dependencies
could start computing but by using
unidirectional layers the computations
can be more parallel we'll initialize
our tensor flow session then our model
inside of it let's see some results
after training first I'll give it this
phrase looks good and now another phrase
dope well it's not perfect and we still
have a ways to go we're definitely
getting closer to having a universal
translation model breaking it down
encoder/decoder architectures offer
state of the art performance and machine
translation by storing the previous
outputs of LS PM cells we can judge the
relevancy of each to decide which to use
via an attention mechanism and by using
a bi-directional RNN the context of both
past and future words is used to create
an accurate encoder output vector the
coding challenge winner from last week
is Ryan Lee this was very impressive he
created a recipe summarizer by scraping
125,000 recipes from the web and
documented it all beautifully with
installation steps so you can reproduce
the results yourself wizard of the week
and the runner-up is Sarah calling her
post converts scientific papers to text
and prioritizes them by topic
this week's coding challenge is to
create a simple translation system using
an encoder decoder model all the details
are in the readme poster help link in
the comments and I'll announce the
winner
next week please subscribe for more
programming videos check out this
related video and for now I've got to
get a better GPU so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>