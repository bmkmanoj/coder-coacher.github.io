<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning Frameworks Compared | Coder Coacher - Coaching Coders</title><meta content="Deep Learning Frameworks Compared - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning Frameworks Compared</b></h2><h5 class="post__date">2016-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MDP9FfsNx60" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj in this video
we're going to compare the most popular
deep learning frameworks out there right
now to see what works best the deep
learning space is exploding with
frameworks right now it's like every
single week some major tech company
decides to open-source their own deep
learning library and that's not
including the dozens of deep learning
frameworks being released every single
week on github by Cowboy developers how
many layers you got let's start off with
psyche out
psyche it was made to provide an
easy-to-use interface for developers to
use off-the-shelf general-purpose
machine learning algorithms for both
supervised and unsupervised learning
cycle provides functions that let you
apply classic machine learning
algorithms like support vector machines
logistic regressions and k nearest
neighbor very easily but the one type of
machine learning algorithm it doesn't
let you implement is a neural network it
doesn't provide GPU support either which
is what helps neural network scale since
like two months ago pretty much every
single general-purpose algorithm that
psyche learned implemented has since
been implemented in tensorflow sidekick
you just got learned there's also cafe
which was basically the first mainstream
production grade deep learning library
started in 2013 but cafe isn't very
flexible think of a neural network as a
computational graph in cafe each note is
considered a layer so if you want new
layer types you should define the full
forward backward and gradient updates
these layers are building blocks that
are unnecessarily big there's an endless
list of them that you can pick from in
tensor flow each node is considered a
tensor operation like matrix ad or
matrix multiply or convolution and a
layer can be defined as a composition of
those operations so tensor floats
building blocks are smaller which allows
for more modularity Caffe also requires
a lot of unnecessary verbosity if you
want to support both the CPU and the GPU
you need to implement extra functions
for each and you have to define your
model using a plain text editor that is
just ghetto model should be defined
programmatically because it's better for
modularity between different components
also cafe's main architect now works on
the tensor flow team we're all out of
cafe but speaking of modularity let's
talk about Charis Charis has been the
go-to source to get started with deep
learning for a while because it provides
a very high level API to build
deep learning models Tara sits on top of
the other deep learning libraries like
fian oh and tensorflow it uses an
object-oriented design so everything is
considered an object be that layers
models optimizers and all the parameters
of a model can be accessed as object
properties like modeled layers 3 dot
output will give you the output tensor
for the third layer in the model and
model dot layers 3 dot weights is a list
of symbolic weight tensors this is a
cleaner interface as opposed to the
functional approach of making layers
functions that create weights when being
called great documentation it's all
Gucci yes
I'm bringing that back but because it's
so general-purpose it lacks on the side
of performance curraghs has been known
to have performance issues when used
with a tensor flow back-end
since it's not really optimized for it
but it does work pretty well with the
Theano back-end the two frameworks that
are neck-and-neck right now in the race
to be the best library for both research
and Industry are tensor flow and theano
Theano currently outperforms tensor flow
on a single GPU but tensor flow
outperforms Theano for parallel
execution across multiple GPUs Tiano has
got more documentation because it's been
around for a while and it's got native
windows support which tensorflow doesn't
yet damnit windows in terms of syntax
let's just take a look at some code to
see some differences we're going to
compare two scripts in tensorflow
and Theano they both do the same thing
initialize some phony data and then
learn the line of best fit for that data
is do it can predict future data points
let's look at the first step in both
tensor flow and Theano
we're generating the data pretty much
the same way using numpy arrays so
there's not really a difference there
let's look at the model initialization
parts this is the basic y equals MX plus
B slope formula in tension flow it
doesn't require any special treatment of
the X and y variables they're just
they're natively but in Theano we have
to specifically say that the variables
are symbolic inputs to the function the
tension flow syntax are defining the B
and W variables is cleaner then we
implement our gradient descent function
which is what helps us learn we're
trying to minimize the mean squared
error over time which is what makes our
model more accurate as we train the
syntax for defining what we're
minimizing is pretty similar then when
we look at the actual optimizer which
helps us do that we'll notice a
difference in syntax again tensorflow
just gives you access to a bunch of
optimizers right out of the box things
like radiant descent or atom theano
makes you do this from scratch then we
have our training function which is
again more
see the trend here Diaw know so far is
making us implement more code than
tensorflow so it seems to give us more
fine-grained control but at the cost of
readability finally we'll get to the
actual training part itself they look
pretty identical but tension flows
methodology of encapsulating the
computational graph feels conceptually
cleaner than Theon O's tensor flow is
just growing so fast that it seems
inevitable that whatever feature it
lacks right now because of how new it is
it will gain very rapidly I mean just
look at the amount of activity happening
in the tension flow repo versus the
Theano repo on github right now and
while Kara serves as an easy used
wrapper around different libraries it's
not optimized for tensorflow
a better alternative if you want to
learn and get started easily with deep
learning is TF learn which is basically
Karos but optimized for tensor flow so
to sum things up the best library to use
for research is tensor flow the
world-class researchers at both open AI
and deep mind are now using it for
production the best library to use is
still tensor flow since it scales better
across multiple GPUs than its closest
competitor Theano lastly for learning
the best library to use is TF learn
which is a high-level wrapper around
tensor flow that lets you get started
really easily also shout out to Rahul do
for being able to generate an upbeat
midnight file badass of the week please
subscribe for more programming videos
for now I've got to go worship
tensorflow some more so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>