<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build an AI Writer - Machine Learning for Hackers #8 | Coder Coacher - Coaching Coders</title><meta content="Build an AI Writer - Machine Learning for Hackers #8 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build an AI Writer - Machine Learning for Hackers #8</b></h2><h5 class="post__date">2016-06-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x24VEUEph0Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my dear Megan tank this is our love
story human or machine human why I found
some drops of Soylent on it
hello world welcome to Terry geology in
today's episode we're going to build an
AI writer that is an app that can write
a short story about an image just by
looking at it sorry Stephen King you're
out of the game isn't it weird how just
by stringing together an exact
combination of words we can produce
something of profound beauty when we
read these stories our brains are
somehow encoding them into thoughts when
we encode a sentence to a thought and
more semantically similar it is to an
existing thought the more will be able
to relate to it so how do we get an AI
to write a story if it doesn't have any
experience living life in the real world
well we're going to build an AI writer
in Python using a deep learning library
lasagne and we've got a lot of code to
go over so I'll explain as we go let's
get pythonic at the highest level we
could code this app in just three lines
of code it's a little ridiculous we
import the generate class and call it a
load all function which will initialize
all of our machine learning model then
call a story function with the generated
models in image location as the
parameter that's it it'll output a story
but let's dive a little deeper the load
off function is just oiler plate
initialization so let's take a closer
look at the story function where the
real magic happens we'll start off by
loading an image into memory this will
be the image that we want to tell a
story about we'll use a load image
function to load it and have the
parameter set to the location of the
image on our machine the load image
function uses the scientific computing
library numpy to get the byte
representation of the image and then
resize it so it's smaller while
preserving its aspect ratio once we've
loaded our image it's time to input the
image into a deep convolutional neural
network to retrieve its features in a
previous episode we talked about how
convolutional neural nets were great for
image recognition since they roughly
mimic the human visual cortex the CNN is
pre-trained we initialize it in the
build ComNet function which is called in
the boilerplate load all method once we
specified all the layers we load up our
pre trained synapse weights file called
vgg 19 this file was trained on a huge
data set of labelled images so it will
be able to recognize the objects in a
novel image once we input our image into
our CNN it'll return an array of
features for us these features are the
highest level features in our neural map
the layer right before the output layer
the most abstract representation of the
image it's content once we have our
features we want to encode the image
features into a multi
neural language model so what is this
well it's based off a paper called
unifying visual semantics embeddings
in our code we're using a pre train
model that will input a joint image
sentence embedding into a multi modal
vector space they use an LS TM to encode
the sentence and a CNN to encode the
image then a decoder neural language
model generates a novel description from
the image since our model is pre trained
we embed our image into this multi modal
space our features are updated to
include the weight of the joint space
then we compute the nearest neighbors to
do this first we retrieve the array of
scores that is a list of all novel
sentences generated from the novel image
which we then sort in order of closeness
then we'll want to print out the nearest
captions now that we have a set of
caption sentences we want to compute a
set of skipped thought vectors for each
sentence skip map vectors are a vector
representation of a sentence this is
another implementation of the encoder
decoder model the encoder and decoder
are both recurrent neural networks we
take an input sentence and encode it
into a skip flop vector by inputting it
into the encoding recurrent neural net
since we are modeling a sequence of
words we use gated recurrent units or gr
use at each neuron gr use consists of
two gates an update gate and a reset
gate the gating units modulate the flow
of data inside the unit and unlike L SCM
cells there are no separate memory cells
l SCM cells control the amount of memory
content that is seen or used by other
units in the network
gru cells don't they exposed its full
content without any control so Gerry use
have a less complex structure and are
thus more computationally efficient
we're starting to see these be used more
and more they're relatively new so when
we feed it sentences into the RNN it
will create an abstraction the vector
representation or tiptop vector
synthesis that share semantic and
syntactic properties will be mapped to
either the same or similar skip thought
vectors the function returns these
vectors as a numpy array which we can
then modify via the style shift function
we'll take our thought vectors and
modify them to match the style of
stories using a pre trained recurrent
neural network the RNN was trained on a
data set of romance novels where each
passage was mapped to a thought vector
so we're essentially computing a
function that looks like this for a
style shift f of X is a book passage
thought vector X is an image caption C
is a caption style vector and B is a
book style vector we remove the caption
saw from the caption and replace it with
the book style to create a book passage
vector once we have our book passage
style vector we can generate the story
by running the decoder function on it
the decoder is another recurrent neural
net
that given a vector representation of a
sentence can predict the previous and
the next sentence will run the decoder
on our passage vector and that will
generate our story based on the image
for us let's take a look at what it says
about this picture let's read the first
few sentences she was taking the man out
of her mouth and she gave him a gentle
shake of her head oh my god
I can't wait to see what happened in the
past 24 hours I had never met a woman
before this thing is a probe for a small
chunk of code there's a lot of machine
learning going on here we use a
convolutional neural net to compute
image features an LS TM recurrent neural
net to encode our image into joint space
and retrieve the sentence captions a gr
u recurrent neural net to calculate the
Skip thought vectors of those sentences
and after style shifting an RNN to
decode our passing vector to a story
that's for neural nets you can run this
on your local machine this is necessary
models are pre-trained for more info
check out the links below and I just
signed up for patreon so if you guys
find my videos useful I'd really
appreciate your support to help me to
continue doing this full-time please
subscribe for more ml videos and for now
I've got to go fix a
nullpointerexception so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>