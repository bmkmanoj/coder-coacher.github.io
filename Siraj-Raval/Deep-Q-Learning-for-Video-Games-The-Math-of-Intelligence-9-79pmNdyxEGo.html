<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Q Learning for Video Games - The Math of Intelligence #9 | Coder Coacher - Coaching Coders</title><meta content="Deep Q Learning for Video Games - The Math of Intelligence #9 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Q Learning for Video Games - The Math of Intelligence #9</b></h2><h5 class="post__date">2017-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/79pmNdyxEGo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and in this
episode we're going to build our own
game bot capable of beating any Atari
game that we give it in 2014 Google
acquired a small london-based start-up
called deep mind for 500 million dollars
that is a lot of Benjamin for what seems
like pretty simple software it was a bot
for Atari games but the reason they paid
so much for it was because it was one of
the first steps towards general
artificial emotion that means an AI that
can excel in not one but a variety of
tasks its capabilities are generalized
just like ours are their paper was later
featured on the cover of nature showing
that their algorithm could be applied to
50 different Atari games and achieve
superhuman performance in all of them
they called their BOTS the VP to Lerner
but before we talk about that let's talk
about the concept of reinforcement
learning supervised and unsupervised
learning techniques are well known in
the applied AI community you give some
model a data sets with labels and have
it learned the mapping between the two
or a data set without labels and try to
learn what the labels are by clustering
or detecting the anomaly in the data set
we can use these data sets to create
data classifiers or data generators but
consider this scenario you're playing
the game Super Mario Brothers awesome
game and rather than play it yourself
you'd like to train an AI to play it for
you how should we think about this
problem if we screen captured game
sessions from expert players we could
use the video frames from the game as
input to a model and the output could be
the directions that Mario could move
this would be a supervised
classification problem since we have
labeled the directions to move assuming
we have lots of data and access to some
stick GPUs it makes sense to try out a
neural network here given video frames
in a new game it would know how best to
navigate
to beat the level right yeah but then we
need hundreds of hours of gameplay
videos to train on and it doesn't seem
like an elegant solution to this
specific problem first of all we're
training a model not on a static data
set but a dynamic one the training data
is continuous new frames are constantly
emerging in this game world this
environment and we want to learn how to
act in this world humans learn best by
interacting with an environment not by
watching others interact in it
environments are stochastic any number
of events can occur
it seems best to learn by trying out
different possibilities so rather than
framing this problem as solvable by
pattern recognition
let's frame it as solvable through a
process of trial and error this is the
kind of problem that reinforcement
learning is made for we do have a few
labels a plus one every time Mario does
something positive they're just not
instantly available to us their time
delayed instead of calling them labels
let's call them rewards so how do we
formalize this process mathematically
well we start off with an environment
where an AI or agent can perform a
number of actions in since environments
are unpredictable we want to keep track
of its current state the agent acts in a
given state of the environment and based
on its actions it may or may not receive
a reward an increase in the score we can
then represent one full episode of this
process an episode would be a single
game from start to finish as a sequence
of state actions and rewards for the
agent the probability of each state
depends only on the immediately previous
state and the performed action but not
on the states or actions before that
this is called the Markov property in
probability theory named after the
Russian mathematician Andrey Markov and
since our agent is making decisions
based on this property this process is
considered a Markov decision process
how could you submit it without asking
me first
it doesn't matter we're on the front
page of nature below Elise
we want our agent to be smart AF to plan
not just for short term rewards but for
long term rewards as well for our Super
Mario example stepping on a Koopa what
increase our score in the short term but
that's it
however consuming a star would increase
our score in the short term but also
increase our score in the long term we
could represent a total future reward
for a single episode from a time point t
onward like this what we just sum them
all up but remember that our environment
is unpredictable so we can't be sure
that we'll get the same rewards in
another episode
the farther into the future we go the
farther the rewards could diverge so we
can add a discount factor between zero
and one to our equation what this means
is the more into the future the reward
is the less we take it into account so
we want a balanced value ideally we want
to choose an action that maximizes the
discounted future reward so how do we do
that well we can represent this
discounted future reward when we perform
an action in a state and continue
optimally from that point on as a
function this function represents the
best possible score at the end of a game
after performing a given action in a
given state it's a measure of the
quality of an action in that state so
we'll call this function Q for quidditch
no I wish for quality whenever Mario is
deciding between several possible
actions the solution is picking the
action that has the highest Q value
computing this Q function is where the
learning process comes in the maximum
future reward for this state and action
is the immediate reward plus the maximum
future reward for the next state this is
also called the bellman equation we can
think of the q function as a matrix
where the states are the roads and the
actions are the columns we'll start by
initializing the Q matrix randomly and
observing the initial state of the game
then we can approximate it through a
training process where we first pick an
action execute it observe whether or not
we received a reward because of it
and a new state were in this is called
q-learning it's used to find the optimal
policy for any Markov decision process
it's a very popular algorithm in
reinforcement learning since or any
finite Markov decision process it's been
proven to eventually find the optimal
policy if we apply queue learning to
Mario and the state would be presented
by the location of the enemies on the
screen and the obstacles as well as some
other factors but that's a game specific
state what can we represent as a state
that could be generalized across many
games well one thing that all games
share in common are pixels if we could
somehow convert pixels from the game
screen into actions we could feed it any
game and it would learn the optimal
policy for that game if we use a
convolutional neural network we could
use game screens as input and help put
the Q value for each possible action so
deepmind used the CNN with three
convolutional layers and two fully
connected layers normally you'd also
want to use pooling layers which makes
the network insensitive to the location
of an object in an image which is
perfect for classification tasks like
detecting if a picture has a certain
object in it but for our use case the
location of the player and the enemies
is crucial so we won't use pooling
layers will input for different game
screens as input so that we have a way
of representing speed and direction of
the game characters and the outputs are
the queue values for each possible
action since we're using a deep network
to help approximate the cue function we
can call this process deep q-learning
deep mind added a few other tricks to
this but that's really the basic idea
the best part is that this algorithm can
learn how to excel in any game
to summarize we can use reinforcement
learning if you optimize an agent for
sparse time-delayed labels called
rewards in an environment Markov
decision processes are a mathematical
framework for modeling decisions using
States actions and reward
and q-learning is a strategy that finds
the optimal action selection policy for
any MVP Noah Liddell is the wizard of
the week
the challenge with to build Lda and he
definitely delivered Noah used Lda to
mind topics from scientific papers found
on nature comm using it he was able to
find important keywords that summarize
all the jargon very neatly such an
awesome use case a plus and the
runner-up is a Jung won a jeongwon use
LD a to mind topics from news articles
and classified them with the CNN to
predict stock prices this week's coding
challenge is to implement a Q learning
algorithm from scratch for any game
you'd like post those github links in
the comments and I'll give the two best
emissions a shout-out next week I hope
you liked this video and if you did
please hit the subscribe button for now
I've got to find my cue matrix so thanks
for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>