<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Do Linear Regression using Gradient Descent | Coder Coacher - Coaching Coders</title><meta content="How to Do Linear Regression using Gradient Descent - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Do Linear Regression using Gradient Descent</b></h2><h5 class="post__date">2017-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XdM6ER7zTLk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj welcome to this
live stream if for those of you who are
in my previous livestream I'm doing one
again because why not thank you guys for
coming today what we're going to do is
we're going to find the relationship
between student test scores and the
amount of hours studied now to us
clearly there must be some kind of
relationship right the more you study
the better your test score will be but
let's prove that mathematically by using
a strategy called linear regression so
linear regression is a very simple
machine learning technique and the way
we're going to optimize it is by using
probably not probably the most popular
optimization method there is gradient
descent so in this live stream you're
going to learn about gradient descent
and linear regression so this is going
to be awesome so get ready for this I'm
glad to see that there are some cool
people in the room right now
what I'm going to do is I'm going to
first talk about conceptually how we're
going to do this and then we're going to
implement it mathematically and
programmatically it's really the same
thing in the end so let's pull up what
we need to pull up great got a good
number of people in here so let's go
ahead and do this so the first thing we
want to do so what I have up here is a
is a visualization it's an animation of
what this looks like
okay and it's going to start off as a
horizontal line we want to find the line
of best fit we want to find the line of
best fit so that using this line we can
then predict what a student's testscore
will be given the amount of our studied
or vice versa but how do we get that
line of best fit well we're going to use
gradient descent to do that and so this
is just a visualization of what the
gradient descent process will look like
to get there okay and gradient descent
is used everywhere
in machine learning and in deep learning
okay we think about everything in terms
of optimization where we have some loss
function that we want to minimize over
time and gradient descent is the
technique we use to do that
okay and so we're going to talk about
that so let me start out by answering I
mean I'll do a two minutes worth of QA
for questions and then we'll get right
into the code so hey guys how's it going
okay we've got some any questions
questions about deep learning AI machine
learning uh waiting for the rap we'll
see was wool yes questions questions
really are the the great thing to have
in the beginning do a ma with other
people okay so I'll give it I'll give it
ten more seconds ten nine eight seven
we're about to get into this we are
about to get into it once I start
there's no stopping I'm like an Oreo
that really has no relationship to what
I just said can we do QA after the end
of the session yes we can so we're going
to do QA every 15 minutes so right now
there are no questions because no one
knows what the hell is going on is there
any other method to get least square
error yes the method we're using here is
the sum of squared error there are
plenty of methods to do it but this is
the most used one and one more question
is is linear regression like this ml or
DL now that is a great question linear
regression is just ml it's machine
learning there is no neural network but
the reason I'm doing this is to
demonstrate gradient descent because
you're going to use this in almost every
neural net that you built you're going
to use this all over the place okay so
that's it for the questions save your
questions every 15 minutes I'm going to
be answering questions and this live
stream will be an hour long more or less
so let's get started with the code shall
we
and thanks to Udacity for hosting this
by the way here we go
so to do this let's look at our data
first what is the data that we have how
do we this is the data set that we have
by the way so the data set is a
collection of test scores and amount of
our study so the X values are on the
left hand side and those are the amount
of hours study so right here 53
sixty-one these are the amount of our
study and the y-values are the test
scores let's prove this relationship
that's our data set it's in a data CSV
file you can find it on my github it is
at the very top it is the most recently
updated code repository it might be in
the description as well I'm not sure if
it's not I would appreciate if it is so
let's get it go ahead and get started
with the code so the first thing I'm
going to do is I'm going to define the
let's see the main function I hope the
code is a it is visible for you guys you
guys are going to code yes what I want
you to do is I want you to pull up an
editor a text editor and I want you to
code with me so pull up sublime text
pull up atom and start coding with me
okay so let me get everything all set up
here are great so the first thing we're
going to do is want to say we're going
to define our main function and it's
just a standard thing we always do this
don't we sometimes you just have to do
things that you it's not really
necessary but we'll go ahead and write
that out so that's it for our main
function and now we're going to define
the real main function which is it our
run function this is the highest level
so let's define our steps here the first
thing we want to do and this is what we
do in machine learning is we pull our
data set we parse our data set into
memory so that we can then run our
algorithms on it so the data set is
going to be a collection of points on an
XY plane so we're going to say Gen from
text and the way we even have this
method available gen from text is by
pulling from numpy which is our very a
very much used machine learning library
almost almost every machine learning
repository is going to import numpy in
some way and if it doesn't directly it's
going to happen under the hood numpy is
the matrix multiplication library for
machine learning okay
I have paid version of sublime I'm
working on a different lap up in mine we
had some issues so I just I just
downloaded this version of slime don't
worry about it
so what we're going to do is we're going
to say Jen from Tech something it's a
little smaller because this line is
going to be kind of big we're going to
say data CSV so that is our CSV file
that we just used and now I'm going to
say delimiter so this is just basic data
parsing and this method is from numpy by
the way and that's star a little symbol
let says lets us pull it without having
to say NP or numpy every time so let's
get our delimiter value and that's going
to be a comma so the comma just means
let's split it by the comma in the comma
is what's in between these values if we
were to look at it in in terms of a raw
txt file there will be a common would
come between the our study and the test
scores so it splits them both into a a
set of points X Y value pairs so that's
our those are our points okay and so the
next part is the learning rate the
learning rate is going to be 0.0001 what
the hell is this so learning rate is a
hyper parameter this is a hyper
parameter and hyper parameters in
machine learning are what we use as
tuning knobs for our model so we have a
bunch of these and right now this is the
only one we have the learning rate
defines how fast our model learns so you
might be thinking well why don't we just
set the learning rate to a million well
the reason is like all things in machine
learning it's a balance it's kind of
like a bell curve so if the learning
rate is too low our model will never
converge and if it's too high then the
model will take it sorry if the learning
rate is too low our model will be too
slow to converge whereas if it's too
high our model will never converge so we
want that balance that optimal learning
rate and in this case it's going to be
0.001
in general in machine learning we don't
always know off the top of our head what
the best hyper parameters are going to
be so we have to guess and check now in
the bleeding edge of machine learning
and deep learning right now is too
and what those hyper parameters will be
okay so that's it for that part and
let's just keep going here because we
are on a roll so that's our part that's
it for our learning rate and then we
have our initial B value which is going
to be zero and then we have our initial
M value which is going to be zero what
is this well this is our y equals MX
plus B function from from algebra we
have the slope formula so that so the B
is our y-intercept and the M is our
slope it is the the ideal slope that we
want so we're going to start off with
zero because we're going to learn these
values over time right now it's just B
and M they're zero but we're going to
learn them over time so that's our
initial values for R so this is the
slope formula that's it to start off
with and now we have our number of
iterations so how many how many
iterations do we want to run our our
training step for so we're going to say
a thousand why a thousand well because
we have such a small data set if the
data set was bigger then we would have
to do ten thousand or a hundred thousand
start incorporating GPUs and stuff like
that but we'll start off with just a
thousand okay so there is that and now I
want to print out the starting grid well
we'll forget the print step we don't
really need that right now okay so now
we're going to say let's get those ideal
bnm values let's get the ideal bnm
values by using the gradient descent
runner step now this is where the logic
is going to happen which we're going to
define now right so this is the highest
value and then we're going to print out
what what B and M are so print print B
and then print M depending on what they
those are going to be the optimal values
so this step is going to output the
optimal values for each of them so what
we're going to feed it is we're going to
feed everything we've just defined where
we defined our points we define that
initial B value we define the initial M
value we define the learning rate we
define the number of iterations
we define a couple things and so we're
going to speed this all into the model
okay so that's it for the highest level
and okay so now what we're going to do
is we're going to get into the gradient
descent runners step so the gradient
descent runner step is going to be
defined now okay so let's look at what
this looks like
given our values that we define which
were the points and the starting B value
and the starting m value and the
learning rate and the number of
iterations okay so let's do this okay
who's with me we've got the B values the
B value to start off with is going to be
what we gave it which is zero and the M
value as you can guess is also going to
start off as zero
they'll both start off at zero we've got
to learn these things this is machine
learning it's not machine defined and
statically so for I in range number of
iterations which is a which are a
thousand we're going to say well let's
get those values okay using something
called the step gradient now that is
where the real good stuff happens that's
where it's at okay we're going to define
that but given what we start off with
the B which is zero in the M and the
array of points so we're going to take
those points the XY value pairs and feed
it in as an array we can convert that to
an array by using this function array
and then the learning rates which is a
static value it's a constant value okay
all right and I will answer questions in
three minutes so we have that for our
learning rate and then we're once we're
done with that so for all of those
iterations once we have that those
optimal values for B M we'll return them
and then we can print them in that that
highest level function okay so that's
for bnm that's our gradient descent
runner step now now it's time to define
this
step gradient function so now we're
going to define the step gradient
function step gradient is going to be we
given that current B and given the
current hem we're going to give it the
points and then the learning rate okay
so now we're going to define how great
descent works all of it is going to
happen in this step but before we do
that before we do that let's write
another function so put let's put this
on hold for a second we're going to get
to this but let's write another very
important function so we have two
functions left in this code really but
it's the real meat of it is going to be
explaining what they are it's the step
gradient and we have one more function
which is the compute error for given
points function given B M and the points
so this we're going to write this one
first let's talk about what this is okay
so I have a great visualization here and
we're going to talk about what this is
so when we start off with this slope
it's going to be zero right it's just a
horizontal line and then you see that
this is what's happening in the code you
see that it's going up like that I love
how the codes behind me I can just do
this you see it going up so how does it
get up well for us what we want to do is
we want to compute an error value and
what is that error value how do we it's
a way of for us to estimate how bad our
line is so we can update it every time
every step every in machine learning we
call it time step every time step we
want to improve our models prediction
and to do that we need an error value
and this sometimes they call it error
sometimes they call it loss error loss
it means the same thing we want to
minimize error we want to minimize loss
and the way to do that is to compute
what that error is and depending on the
use case it can be different things in
this case simple linear regression we
have a set of points we want to we want
to
define what that error is so here's what
it is we have we start off with a line
and we have a set of points and what we
want to do is we want to measure the
distance between each of those points at
a certain time set so just imagine the
line is static so for each time set the
line is static it doesn't move we want
to measure the distance from each point
to the to the line so we want it's
called the sum of squared errors
okay so sum of squared errors is the
name of this and so what this looks like
is if we were to look at an image it
looks like this given some points let's
measure the distance from each of those
points to the line that we have drawn
and then square them and then sum them
all together and then divide by the
number so it's divided by the total
number of points and that is our error
value and you can and we want to
minimize that error value so just think
about minimizing this error how are we
going to minimize it that's the next
step gradient descent but right now this
is how we calculate our error this is
how we calculate our error and so now
let's take some questions for five
minutes and then we're going to get back
into the error value
why am I so stupid so let's stop this
self-deprecation okay no one is stupid
it's just about who knows these facts
these are static facts who has spent the
time and energy to study this self okay
no self deprecation you guys are awesome
for even being here this stuff is the
most important stuff in the world okay
can we use machine learning to not for
nonlinear equations yes this is a linear
equation but yes we can use it for
nonlinear equations for very very high
dimensional data yes for 100 dimensional
data but we will get there when we get
there could you make the font smaller
yes I can do that okay two more
questions and then we're going to get
right back into computing this error
value programmatically and
mathematically could you why is Python
better except for the object oriented
programming Python is as my friend who
works on the tensorflow team at Google
says the lingua franca of machine
learning it is the language and why is
it well there's a couple reasons one it
just had that head start the people who
were the first to do machine learning
algorithms decided that they wanted to
use Python and we just kind of built
from there that's one two is the amount
of libraries that we now have four and
three is because it's just a dope
language Python is awesome if you've
coded in it for a while you realize wow
this this does a lot of things that
other languages should do in a lot less
code it's more efficient and if you have
come from a an iOS background with
objective-c if you've come from a C
background a C++ background we don't
have to deal with deadlocks we don't
have to deal with memory leaks all this
is done by our interpreter and we can
just focus on what matters which is the
algorithms and so that's why Python is
used because we can focus on the
algorithms one more question
can you make a video about different
layers in a neural network we'll see
we'll see I don't know exactly what you
mean by that that's why I said so okay
so now let's get back to computing this
error so how do we compute this so get
ready for some math I'm going to explain
what's happening here so this is the
error equation okay so this is how we
compute that sum of squared errors so
get ready for this okay a little
refresher on on this this is this is
algebra okay we have our Y value which
is a the Y value for a point okay and
then we have minus MX plus B well what
is MX plus B well it just turns out that
y equals MX plus B right so y equals MX
plus B is the same MX plus B is the same
thing as Y so we have two points we have
the Y point from our a that from the
data set and then we have the Y point on
our line and we want to minimize the
distance between both of those points
and so that's why we are subtracting
them so we subtract the point from our
data set from the point on our line and
then we square it well why do we square
it well for two reasons one is because
we want the value to be positive we
don't want to have to deal with negative
values why because we are going to
summit in a second and two is because we
don't actually care about the value
itself we just care about the magnitude
from so we're looking at it from a more
abstract perspective we just care about
the magnitude of these values we want to
minimize the magnitude so that's going
to give us the difference between our
y-intercepts and then little refresher
on this e that means Sigma that's a
that's a notation for set is called
Sigma notation and what it does is it
defines a set of values that we want to
iterate over so what we're saying here
is for when I equals 1 up to N where n
is the number of points so for all of
those points we want to measure the
squared error values for all of the
points against our line and then 1 over
N means we want to find the average of
those and that's going to give us one
value and that one value is the error
value the sum of the squared errors and
every time step we want to minimize this
value using gradient descent ok ok so
let's define this programmatically the
sum of the squared errors
so we'll start off with this error being
zero it's just there is no area we have
to calculate it so what we're going to
do is we're going to iteratively compute
this for every point that we have okay
so we're going to say for I in range
zero so we start off with zero until the
the ending point is the number of points
we have so that's the length of the
points given those points okay
so for all the points we're going to do
this we're going to compute the error so
get ready for this we're going to
programmatically in code we're going to
programmatically complete that math that
equation that we just saw so we'll start
off by saying well what are the X values
and we can find those X values by saying
I where I is zero so for the first
column and in our data set and then it's
our in our array right because they're
their points and then we're going to say
what are our Y values okay for one so X
Y values so right now we have one x
value and one Y value and the for loop
is because we're going to continuously
do this every time so let's compute that
total error and so this line is going to
represent that equation that we just saw
okay this is this line will represent
that equation programmatically so we're
going to say the total error value is
going to be Y remember it's Y minus we
can literally just word for a character
by character say what this is going to
look like M times X plus B okay and then
this means squared so we're going to
square that and then we're going to add
it every time and so that's the sum of
the squared errors and oh well this part
is just going to give us the sum of the
squared errors and then we have to we
want to average it with that that 1 over
N that we had here that's that next step
so once we have all of these will divide
it by the total number that we had
return total error value to the oh yes
thank you for saying that I did miss an
eye you got it
total error divided by float over length
times points okay it makes sense doesn't
it
Anthony I appreciate you being here
shout out to Anthony Cooper okay so
we've got total error divided by float
length minus one so that's it that's
what we had
that's our computing error for our given
points okay even gods make errors
everybody makes errors well let's see
let's go see okay so now and I'll answer
questions in six minutes so every 15
minutes so let's keep going here that's
it for our computer error function so
where were we we were in that last
function and the most important function
because it's talking about gradient
descent listen up because this is going
to be used almost every time when you're
using deep neural networks all the time
know this like the back of your hand
grading descent let's go let's go tails
I got some see I got some of my homies
in here cool so let's go with this we've
got gradient descent let's do this so
for B gradient starts off as zero and up
for M gradient it's also starts off at
zero
alright before we even code this let's
explain what this is doing what the word
gradient means let's look at this not
that let's look at this what is this
this is a very colorful rainbow period
let's get back to this now I'm just
kidding what this is is it a graph that
shows three up dimensions the three
dimensions are and it's the same graph
it's just look these two images are the
same graph it's just looking at it from
a different angle so let's just focus on
this one to not make it confusing we're
just looking at one graph right here
it's going to make the whole thing
smaller because that's how Mac works we
have our y-intercept we have our slope
and we have our error okay so what this
is it's it's a graph of all the possible
y-intercepts all the possible slopes and
all the possible error values so
remember that error value that we
calculated and so if we were to map all
of those triplets so they could we could
think of them as Triplets of X or of
slope y-intercept and error value pairs
they would make this graph what we want
to do is we want to find that point
where the error is smallest we want to
find the point where the error is
smallest and if we look at this graph we
can see what that point is visually it's
going to be at the bottom of the curve
now we call this in machine learning the
local minima now the reason we say local
as opposed to non local is because we
have a very simple graph here so
sometimes there are many minima and we
want to find which minima is so that's
that's second-order optimization right
now we're focus on first order
optimization so let's find that smallest
point because that smallest point at the
very bottom of this graph is going to
give us the ideal y-intercept and slope
so if we find the minimum error the
minimal error the smallest error
possible we'll also get the ideal
y-intercept and the ideal slope the
ideal bnm and what do we do with those
ideal bnm values well we plug them into
our y equals MX plus B equation and what
happens when we plug them into our y
equals MX plus B equation we get the
line of best fit now I want to say by
the way this is not the optimal way of
doing linear regression we could compute
these bnm values using simple algebra it
doesn't really require us to do gradient
descent we're only doing this strategy
because it's a way to learn gradient
descent there are easier ways to do this
but we want to do this the dope way okay
so that's what that is and so we're
going to compute this using gradient
descent okay so great is that so that
was the first part of the explanation
and so if the reason I showed this graph
is because
the way we get that smallest point is by
calculating what's called the gradient
the gradient is also considered the
slope okay not to be confused with the
with the M value we're talking about a
slope in the direction of getting us to
that gradient value so we have some y
value we have some B value and we want
to iteratively every iteration we have a
thousand iterations we want to
iteratively move our point where we are
in space in this three-dimensional space
down to that smallest point and the way
we do that is by calculating the
gradient and the gradient gives us a
direction it means slope it means
direction and we need to talk about this
for a second this is important listen up
so gradient values are used all over the
place of machine learning so I was
talking to Ian good fellow right he's
the guy who invented generative
adversarial networks and he was saying
that some function some problems we
can't do we can't solve because we don't
have the gradient so clearly gradients
are used across machine learning
sometimes in machine learning we call
functions differentiable that's another
word for can we compute the gradient
from it using what's called a partial
derivative so to compute this gradient
value a direction it's a tangent line
essentially a gradient descent is
essentially a tangent line it's going to
give us a direction direction means
positive or negative do we do we move up
do we do we move down and so it gives us
that line that the direction that we
want to move and this is another example
of grading descent it's essentially a
bowl like curve a bowl eye curve and
this is also an example of
bad internet connection or bad a server
so this would probably be better so so
yeah it's a bowl it's a bowl we could
think of this entire process as a bowl
and the ball are our X are the points
that we are that we have the whether
that be the slope or the y-intercept and
in even more complex problems the
features and the numbers and the words
for natural language processing it's all
abstracted to a bowl like problem where
we are trying to find the optimal point
for that bowl it's going to give us
those optimal values that we need to
make our model amazing okay amazing
slash optimal should we go up should we
go down that's what the gradient gives
us and to calculate that gradient and
let me say this list last thing and then
we're going to get right into the code
to calculate that gradient we're going
to compute what's called the partial
derivative in calculus with respect to
our values and our values are B and M so
this is the equation for the partial
derivative okay for that we're going to
we're going to take this these equations
these two equations and we're going to
programmatically encode them now this is
the last part but it's also the most
important part because that is gradient
descent so we start off with y minus MX
plus B which is why okay and we compute
the partial derivative so what that
means is we differentiate this is from
calculus so we take the value whatever
it is so little refresher so if we have
let me just write this out actually so
if we had an equation that was x squared
what is a derivative of this we take the
exponent this is a two and we move it to
the in front of the variable so it'd be
2x okay so that's a derivative but we
are calculating the partial derivative
with respect to B and M so what is that
what do I mean by partial by partial I
mean we're only using the values that we
were calculating a derivative using only
B and then we're calculating a
respective or a different partial
derivative using just
em okay so let's so this little symbol
over here this little squiggly just
means partial may find the squiggly okay
and then let me also answer some
questions because I think it's time to
answer some point this this little
squiggly means partial derivative okay
so it's going to make a lot more sense
when we encode code this
programmatically if you aren't as
familiar with the math term so questions
okay
why are there two equations there are
two equations because we are calculating
respective derivatives for both B and M
those are the two values that we are
trying to find and we're going to look
at this programmatically so two more
questions and then we're going to cook
code this last step nice so gray in
descent helps us to put rainbows at the
okay that's not up okay exactly
best use of time and money will there
always be one low point in the error
function we can work to using gradient
descent when we have a linear function
yes because it is our local minima but
sometimes we have very very complex data
we have data that has hundreds of
features we have data that is unlabeled
we have data that we don't sometimes we
run unsupervised algorithms that means
algorithms that delay the data is not
labeled and we try to find something
from it so sometimes it clusters in
certain points and we say oh this
cluster there there must be some
relationship here so in more complex
cases no in more complex cases there are
there are there could be local there
could be several local minima and we
want to find the ideal one but we'll get
to that right it's just another set of
we want to learn where to do gradient
descent and that's that's later steps
one more question to start off with ml
and AI do you recommend starting with
the ml n be followed by nope
are there why the gradient gives the
minimum that that is a mathematical
question yes so what is the gradient
give the minima because it doesn't give
us the minima to start off with the
gradient just tells us how to update our
value should we update them with A+
should we update them with a negative
should we multiply them by you know how
do we modify our values to make them
closer and closer to the optimal values
where the error is the smallest and once
we're finally there
that is our minima okay it doesn't
directly give us a minimum all it does
is it gives us a direction to minimize
our error or loss okay if we can compute
a gradient so that means if a function
is differentiable then we know that we
can optimize it so let's write this out
let's write out this step gradient
function okay so we are so we can't we
call this function in our gradient
descent runner recall that we week we
called it right here given the BM points
and learning rate so now we're going to
write this out
we have our okay so make this a little
smaller so we can really see what's
going on here
and
zero M equals M gradient equals zero n
equals float lengths points okay these
are our initial gradient values for B
for the be gray and the M gradient and
we're going to update them over time and
it's going to be the number of points
okay that's just the number of points
and now we're going to calculate that
gradient step so okay so for each of the
points we're getting gradient values for
each of the points and we're going to
put them all together and it's going to
give us one a optimal BM for each time
step or a more optimal BM at each time
step remember this alone isn't gonna do
it we have to iteratively do it and
that's what we did beforehand yes
oh you're right yes you are right yes Oh
guys you got me I already defined it
over here that's what's up I did define
it here yes great where was I move down
that there we go okay so grain descent
runner and then we have the yes the flow
the points and the end al U is the
number of points okay so now let's we're
going to iterate through all of those
points that we just define right the
points the length of the points and then
so for each of those x and y value pairs
so for points where I is zero where Y is
not know where I is zero for each of
those XY pairs I won
let's calculate the gradient so how do
we do that let me look back at this
equation and see what that was it is
okay so what we're doing here to
calculate this gradient or this partial
derivative okay
is what we are doing is we are going to
take Y minus MX plus B and then multiply
it by the x value okay and then at a
negative sign and then do that for every
point and then multiply it by 2 over the
number of points okay we can do that
let's do that
remember these are laws they are
constant the partial derivative is the
same every time it is a law it is a
fundamental law it is a beautiful law
because it's always the same it is
predictable and predictability is great
sometimes when you need it to be so
let's calculate that B value negative 2n
that's what we had negative 2 over N
times y remember we are just writing it
out just like we just saw it in the
equation it's the same thing
em current times X
Plus be current y minus MX plus B times
two over the n- and then we're
continuously adding that right so that's
it for our gradient for B that's going
to give us a direction or positive or
negative that we should update B it okay
and then we want to do the same thing
for M so in fact I'll just paste it
because it's very similar but it's a
little different you know I'll show why
and make sure make sure you guys can see
it all for our M gradient it's going to
be negative two n times X times y minus
M current times X plus B current yes
okay so we're doing this for every point
and we're going to we're going to
concatenate all of those values together
using this plus equals function to get
that B gradient in the M gradient okay
and so once we have those values we're
going to use them now it's time to use
those values to use those values we're
going to update the real B and M values
using them and how do we how do we use
them well we say for let's say for B for
example we take what we have currently
for B whatever it is at every step
because we're running this every time
step every of a thousand minus the
learning rate times the B gradient this
is why we define our learning rate
because we multiply it by the gradient
and it's going to it's going to convert
it into a value that we can then it can
either make it convergence faster slower
it could never converge but this
learning rate is going to give us a a
good convergence a rate of convergence
is going to give us a good rate of
convergence and then we have the new M
which is going to be M current same deal
- the learning rate and I'll answer
questions in four minutes so keep them
ready
time's the M radiant return and then
we'll return them just like that okay so
what do we just do here let me answer
some questions
okay I
it is the learning rate it is the same
thing I just defined it differently in
the step okay so yeah so that's for our
gradient okay can you explain again why
we are doing newbie and new hem so we
had an initial bnm write those words ero
value they were to 0 they had nothing we
wanted to update them using the gradient
so the gradient will tell us how should
we update them should it should be B 0
should be 1 should it be negative one
should B be 0.5 the gradient is a value
that we use to update the PNM value the
initial bnm value to get the new bnm
value to do that we're going to
calculate the partial derivative with
respect to B and respect to M which are
these two respective lines right here
given all of our points okay given all
these points we're calculating that will
sum them all up and then we'll multiply
it by the learning rate and subtract
that from what we have currently and
that's going to give us our new bnm
values
okay so can you explain why do you
multiply the learning rate times the
gradient again
what's that value so okay so the
learning rate if we so in a data set
this small with only a thousand data
points we don't really need a learning
rate more or less I mean we don't really
our model is going to converge this is
just good practice to always think about
a learning rate because we use learning
rates in deep learning right we use them
in deep neural networks
it's about bit a balance you'll find
that all this stuff is all about about a
balanced time versus computing if power
versus efficiency versus amount of code
it's always a balance what a learning
rate does is we are pre defining how
fast we want our model to train and if
we do it too fast then we'll never
converge that means we're never going to
find convergence means finding the
optimal values for our function whatever
it is if we do it too slow then it'll be
too slow to converge so it's about time
and balance we've set point zero zero
zero one for our learning right here
statically ourselves manually but you
know ideally we can learn what that
value is the part where we sum it all up
this is the last question someone asks
what is the part where we sum it all up
is it in the for loop yes it is in the
for loop we are summing it all up right
here plus equals we're summing it all up
okay so so yeah so yeah let's run this
code that's it for the code actually
let's run this thing we've got Python
100% O's NBN error okay of course if
name
hold on
oh yes two equal signs because that's
how we roll in Python
anytime I hear deep-learning is a subset
of machine learning you are correct it
is a subset and what we have here is a
space that there we go okay so let's get
to questions oh my god oh you know what
it is is because does this computer so
I'm using a new computer by the way so
it might not numpy might not be
installed we can install with it Oh Mike
okay so here's what it is here's what it
is so let me look back at this
what this does so I actually don't have
numpy installed on this computer I'm
using a different computer
this was the last-minute thing don't
worry about it the code works I just
come I I had a compiled earlier look at
the values here we've got the ideal
after a thousand iterations be em and
the error value this happens in
milliseconds because we have such a
small data set and also no no actually I
do have numpy nevermind
because I just compiled it what am I
thinking hold on so let's see what we've
got here
I'm going to do some debugging because
we've got some time so why not let's
let's debug Jen from text is not defined
because we've defined it
right
interesting
also this is the this is the code that
I'm that I'm reading it from
there it is okay so there it is
so I'm actually curious what what the
error was what was it it was Jen from
text but it wasn't defined because
because
oh I misspelled it that's why duh okay
anyway is because I misspelled it but
yes I misspelled the Jen from text so
I'm for a thousand iterations we get the
ideal B we get the ideal n values both
of them and we can plug them into our
equation and it's going to give us the
optimal line of best fit for our values
which we can then use to make a
prediction given a test score or given a
right that's what's up yes that's what
it was Jen from txt not Jen from te XT
that's that's exactly what always thank
you okay can't confirmed Illuminati
confirmed so questions ending questions
guys this is going to give us this is
that this is what this code is going to
do right here
exactly right win-win it's all about
winning okay so we've got any questions
how many people do we have in here we
have a good number 242 that's a good
number okay single closed set double how
to use the momentum technique in order
to find not only the local minima
momentum is something that we don't
really use in linear regression we use
that for nonlinear models with high
dimensional data we'll talk about that
later
can we use a dynamic learning rate
that's a great question yes actually if
the learning rate gaps ideally that's
honestly that's that is what we should
do we should have a dynamic learning
rate we should not not just have a
dynamic learning rate we should have
dynamic all of our hyper parameters be
dynamic and they're all learning in real
time and responding to feedback and
optimizing themselves this is actually
um what's like on the bleeding edge of
machine learning right now how do we
learn to learn so it's kind of like a
meta learning three more questions and
then we're out of here is it possible to
overshoot the minimum yes and so that's
why we have a learning rate so we don't
overshoot it for larger data sets yes we
can absolutely overshoot our minimum our
model could just be training forever and
never converge to more are you going to
implement LS TM from scratch yeah I
actually have a video where I do that it
is a generate Wikipedia articles check
that out and then one more question how
large can our data set be with this
technique what if we had billions of
data points with this technique this
okay so this technique will scale this
will scale and it's a linear no pun
intended relationship to the amount of
data that you have so the more data the
longer the training time but this will
work for for larger data sets that that
that are linear so that means that
there's you know an X Y value pair two
values that you want to find the optimal
form okay I'll take one more question
is the feedback related to back
propagation this is not back propagation
so back propagation is a form of
gradient descent this is not back
propagation this is gradient descent for
linear regression if we take gradient
descent and apply it to deep neural
networks that's when it becomes back
propagation so gray descent is the big
is the big boy and then back propagation
is an implementation of gradient descent
where we are descending our gradient by
propagating an error backwards across
all of our layers so we go forwards and
then we go backwards that's it for the
questions thanks guys for showing up and
for learning this stuff this is the good
stuff okay
it's only going to get better it's only
going to get cooler from here on out
okay so for everything that you learn
moving on from this is going to be
awesome
so thanks guys for showing up for now
I've got to go to send my own gradient
in life so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>