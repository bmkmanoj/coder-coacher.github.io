<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make an Asteroids Game Bot (LIVE) | Coder Coacher - Coaching Coders</title><meta content="How to Make an Asteroids Game Bot (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make an Asteroids Game Bot (LIVE)</b></h2><h5 class="post__date">2016-12-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h2qVYpK6TPE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">starting broadcast all right livestream
is starting soon we got people in the
house oh we're all did Siraj good see
everybody we are here or I am here in
Portland Oregon uh just for a week just
to check it out see what it's like here
it's actually pretty cool up here in the
Pacific Northwest
hi everybody I'm gonna listen names
everybody who's here alexandros Radu
Marko max David said to Rafael Faria
Sasha hi everybody
so Yash today we are going to make a bot
for the game of asteroids and we're
going to have it get better and better
at the game until it's able to beat it
and it's going to use a technique called
neuro evolution it's called neuro
evolution okay that's the name of this
technique and we're going to do this in
JavaScript okay and this is a part of a
collaboration with another youtuber
named Daniel Shipman that I am excited
to do okay so that's what we're gonna do
I'm gonna start off with a five-minute
Q&amp;amp;A like always and then we're gonna get
started with the code alright here we go
five and a Q&amp;amp;A let's do this I'm doing
pretty well I'm excited to be here in
Portland I'm gonna make an experimental
video here is Dan joining live know
which tool do we have to use it's going
to be just pure JavaScript neural
evolution like a genetic algorithm yes
I'm from Israel did you go to nips I
didn't go to nips I didn't go to nips
but next time I will for sure
are you building the game - not the game
just the AI code you should come on up
to Seattle I would love to I would love
to advice for upcoming computer
scientists um find a paper that you
really like just find a paper that you
really like and then try to replicate it
and then
if you need help use an advisor or
someone who has a lot of experience but
I've found that replicating papers has
gotten me really good really fast I had
a lot of at least 4 ml and I'm sure I
mean the same can be applied to computer
science theory what do you do Suraj I do
this YouTube channel full time any news
about rocket AI guys I'm sorry to break
the news for you but rocket AI is total
hoax it's not real everybody was in on
it they trolled everybody temporally
recurrent optimal learning troll okay
that's the acronym are you drinking
coffee yes what can a high schooler do
in machine learning right now um you
could get started with TF learn or
scikit-learn those are two great
libraries to get started with find a
data set on kaggle and then run a model
on it and see what you can find well you
ever cover will you cover other AI
concepts beyond neural networks
absolutely tensorflow versus chaos
tensorflow which AI tol will you be
using in this video I'm not I'm using
pure JavaScript
please bust a rap on screen right now
I'll say that till the end for the end
papers from where i triple e archives
arxiv org archive is a good place shall
I treat you for technical help yes you
can tweet me by the same time remember
we have a community guys it's not just
me we're all here to help each other
learn so there's a slack channel with a
link in all of my videos so I'll
remember to always ask questions there
as well two more questions and then
we're gonna get started thanks Alexander
uh is that a promise about busting the
rap yes it is did you please remind me
though difference between Siri and
Google Siri is worse in every way cuz
like Wow Apple you just totally miss the
boat on AI and just now you just like
publish your first paper Congrats
but Google now is this like way better
like they're the learning algorithms
they have the data that is trained on
it's just way more what do you think
about octave it's not that great how to
implement deep compression by song huh
deep compression well first of all
anything with compression and your own
networks has to do with an auto encoder
so you're probably gonna use an auto
encoder the word deep implies many
layers so like lots of layers and the
specific one that he made I haven't seen
but show me link and then I'm
a video on it okay so those are my
questions those are my questions now
we're gonna get started with the code
okay so I'm gonna go ahead and screen
share let's do this alright so I want to
start off by showing you guys a demo of
this code so you can see what it looks
like okay I'm gonna show you guys a demo
of this code so looks like this so what
happens is ass playing and many
different generations many different
space ships are created this is so this
is neuro evolution I'm gonna talk about
what's happening here but as you can see
it's just there are a lot of them that
are being spawned a lot of ships and
they're all trying different moves and
the one that has the best move is the
one that's going to survive and
reproduce and create a better one and so
over time it's gonna get better but you
can see here that the basic idea I want
to show you by looking by showing you
this game is that these things get
better over time and there are many of
them many of them that are spawned okay
so that's the basic idea on the link
will be in the code ok are we lagging
relyin cool okay so now let's go ahead
and get started with this we're gonna go
ahead and get started with the code
I'll make this bigger so everybody can
see what I'm typing so this is going to
be neural evolution that's the technique
we're using is the technique forth okay
so that's the techniques that we're
gonna be using and yeah so there's that
and so that's what we're gonna do and so
what happened what's happening here is
like while most learning methods focus
on modifying just the strength of the
neural connections this doesn't this
modifies not just the neural connections
it modifies the actual structure of the
network so adding neurons it's adding
connections it's it even modifies the
learning rates so a lot of Piper
amateurs are modified by this it's not
just the connections so that's what
makes it different from like from
regular backpropagation okay
so bigger font okay let me make it a
little bigger okay boom that's the
technique that we're gonna be learning
uh and this is a type of reinforcement
learning this is a type of reinforcement
learning that's we're gonna be learning
right now and because it's happening
through trial and error so what's gonna
happen is we're going to and the reason
we're doing this is because the optimal
actions at each point in time are not
always known like remember this is a
game this is a game things are random
things are random and neuro evolution
proves to be a good technique for this
because it you're able to optimize
behavior given only sparse feedback so
sparse feedback is available is
available alright but that's what's
happening here so the steps are we're
going to do this in increasing order so
we're gonna go ahead and create art
neuro evolution object but in that we're
gonna create increasingly larger objects
so the first one is the neuron and the
next one is the layer then it's going to
be the network then the genome and then
finally the generations so as you can
see each of these gets bigger over time
so so will create new revolution as our
big master object then we're gonna
create a neuron and so a neuron there
are many neurons inside of a layer of a
network and then there are many layers
inside of a network and then each
network is considered a genome a genome
is another word for individual or being
and in in what's called a generation
okay so there's a generation a
generation has multiple genomes each
genome has its own neural network each
neural network has its own set of layers
and each layer has its own set of
neurons okay so it's all abstractions
okay everything is alright everything is
is just like this alright so so that's
what's happening right now and sparks
feedback means that you don't have a lot
of feedback like you don't have a lot of
data the word sports is applied to data
what we don't have a lot of it so if
there's a lot of zeroes in our data that
that would be sparse okay so let's go
ahead and get
by doing this alright so let's go ahead
and create our first variable it's
called neuro evolution neuro evolution
that is our big master variable and it's
going to be a function and we're gonna
give it options so what are these
options options are another word for
hyper parameters and I'm gonna talk
about what the specific hyper parameters
are for this okay so we're gonna start
off by defined defining our self burial
by calling this okay that's the first
step now we want to create those options
so remember options are our hyper
parameters this is unsupervised learning
indeed exactly options are our hyper
parameters so we're gonna start off by
saying the first option we want is our
sigmoid function let me talk about what
the sigmoid function is but first let me
type it out so sigmoid is an activation
function okay um
sigmoid is an activation function and
it's a function that we run in every
neuron of our network okay so let me
write write this out I'm gonna create a
variable called ap that is negative a
over one and then I'm going to return
this equation what is this equation well
this is the actual sigmoid equation and
it is a static equation let me explain
the hyper parameters in a second but
this is a static equation and what it's
doing is it's converting every number
into a set up for into a probability so
this this sigmoid is run in this
function is run in every neuron of our
network okay and it's how we convert the
numbers into probabilities return ok
return what our hypo Krannert hybrid
parameters are but a tuning knobs of our
network they change over time okay they
are the tuning knobs of our network we
could change what they are okay and I'm
gonna show you a list of hyper
parameters in a second the next function
we want to create is called random
clamped okay random it's called random
clamped and what this function does is
it returns a random value as and we can
use that to generate weights as well so
that's a different point but we're gonna
use this to generate a set of weights
later okay
and so there's that and I want to return
math.random we're gonna use the random
function we're gonna multiply by 2 to
make sure it
the number is big enough for us and it's
subtract by what ok so there's those
there's that
now we're gonna create our list of hyper
parameters the first one is going to be
the population which is the size of our
population right how big do we want this
this population to be the next one is
called elitism what its elitism this is
the this is the number that we're going
to apply to each of so so so elitism is
whenever we are so let me give you a
quick explanation of genetic programming
for second etic algorithms so you have a
population so step one is a you have a
population and then what happens is they
perform crossover which is they they
they breed together and then the the end
result is then you mutate those children
and you keep going right there's three
right so so so you select the best ones
selection and then you perform crossover
you take those and you make them and I'm
gonna show you guys what mate means in
this context and then we're gonna mutate
them and so this is what elitism does
elitism is dealing with the mutation
elitism is worth taking how 20 percent
right it's point two so we're gonna take
20 percent of the best offspring okay um
so that's what's gonna be happening so
right now we're gonna say random
behavior is is going to be zero point
two okay so what is random behavior
random behavior is the new random
networks for the next generation so the
rate so how random do we want this to be
right the next one is the mutation rates
mutation rate is saying what the rate of
is that were updating the synapses how
how much do we want to mutate that okay
and then we have a mutation range okay
why the trade-off between 20 percent
good question 20 percent as opposed to
something larger like I don't know more
than 50 you want something that's less
than 50 because you only want the best
ones you only want the best ones and you
want to diminish the rate that that you
want to diminish the the rate that the
population is breeding so that by the
time you're done first of all it's
faster that the network trains faster
and you only have a few at the end okay
okay so there's that so the mutation
range is going to be 0.5 then we have
our an actual network so the network is
going to be 1 and so what is the network
well now I'm defining now I'm defining
the size of the network what does that
mean it means it like the structure of
the neural network
I want one layer ok I went three of them
so this is a three layer feed-forward
neural network okay so that's the
network and let me define that as an
array so that's an array and so now we
want another type of parameter called
historic which saves the latest
generation that's saved that's what it's
going to store what's the last
generation right now we'll initialize it
at zero is it there are no generations
right now the next one is going to be
low historic so we're only gonna save
this score right it's a boolean that
says like we're only gonna save the
score now we want to sort sort it right
so we want to sort it up till negative
one so what order do we want to sort
things in we're gonna sort it right now
in descending order because it's
negative one if we wanted it to be
positive we would do positive one okay
this is a bomb so now NB child 1 all
right so this is the number of children
that we want to breathe okay
so that's it for our hyper parameters
let me just move that up that did for
hyper parameters and now we can go ahead
and initialize this alright so let me go
ahead and move that here and say ok so
now we're going to initialize our our
set variables to have options available
ok so what is this now I'm going to UM
behavior oh good call typo on behavior
behavior all right great and variables
no variable it's not very almost scary
though so we want to create a sense what
do I mean by a set we want to set up
options we're gonna initialize that like
this um okay so so now that we've
initialized our variables we're going to
say we're gonna loop and what we're
gonna do in this loop is we're gonna say
for every option that we have for every
option that we have I
you to say if if there's nothing there
right now then I want you to fill it
into our existing list of options okay
just Shh add those in okay so I've got
up my comments here okay so then if the
option is there and it's not defined
undefined then we are going to add it to
our options array okay so we want every
option to be in my options array okay so
options up till I alright so now so
there's that and so now let's create our
neuron we're gonna create our neuron all
right so our neuron is going to be it's
going to be a function - so the neuron
has an internal value and as it has a
set of connections to every other neuron
in the next layer okay so we're gonna
define both of those - both of those two
variables okay so the first one is the
value it's gonna be zero like what is
the value of this and run well we're
gonna initialize it at zero and then the
weights or connections to every other
node which we're going to store in an
array which is empty right now the next
step is going to be to randomly
initialize our weight values we want we
want them to be distinct we want them to
be distinct okay alright so now we want
them to be distinct right so let's go
ahead and do this
so the first one we're gonna do is we're
gonna create a we're gonna create a
neuron it's going to be a prototype
object which is the higher-level
object and we're gonna we're gonna
create our population function okay
we're gonna populate it but and we're
gonna do this by randomly initializing
weight values okay so let me show you
what I mean by this
well initialize a function and the
parameter is gonna be envied okay
which is the number of weights that we
want why randomize the weights uh
because uh well I mean there are many
many methods of like like how to initial
initialize weights Rand
is the most popular but there could be a
better more more a more efficient way of
doing that but right now and that's an
area of research as well but generally
we we randomly initialize our weights
and then we make them better over time
but just like randomly sampling data we
can do that better as well but random is
just an easy way to to start off okay so
we'll initialize our weights array and
we're going to iterate through every
weight that we're given so bar I equals
0 I is less than the number of weights
and then I plus plus okay
I put so we're gonna say add every
weight to the list of weights that we
have and what are we using we're using
that random clamp function to add a
randomly initialized set of waves to our
weights array okay so that for our
neuron now let's do our lair remember
we're increasing the size of of how big
we are in of the objects that we are
creating every time so we did our
weights and now we're gonna do our lair
and a lair has an ID and a number of
neurons okay those are its two
attributes an ID and a number of neurons
so we'll start off by creating our lair
uh and I'll definitely do a recap at the
end of the code so we start off by
creating a layer and a rip a layer is
gonna be initialized by this index
variable that is the parameter the index
parameter and what does that mean well
that's gonna give us our ID that's how
we initial that's how we um identify
this layer right so it's gonna be index
or zero okay and so then we want to
initialize our neurons how many do we
have we create a layer none so it's
gonna be an empty list okay it's an it's
gonna be an empty list now we're going
to create our layer prototype for
populating the lake okay so we're gonna
populate the layer with thank you for
that options that was a typo now we're
gonna populate the layer with neurons
okay which we've just created we've just
created neurons
okay layer dot prototype to populate and
we want there to be
two parameters a number of neurons and a
number of inputs what are the input
going to be it's going to be the set of
actions that we take during the game
okay that pool we're gonna update or
wait with so we're gonna initialize an
array of neurons and we're going to say
okay so we're going to iterate through
every single one of our neurons that
we're gonna add them all to our layer
okay let me see this question you can a
layer be iterated or is it explicitly
specified we can uh we can iterate
through the layer generally in machine
learning we don't update the number of
neurons inside of a layer we don't
update the actual layer itself other
than the weights but in this case in
neuro evolution as opposed to just
regular back propagation neuro evolution
in neural evolution we modify all parts
of the network we can modify the number
of neurons in each layer we can modify
the number of layers all nothing is safe
from neuro Revolution okay so it's not
just back propagation we're not just
updating the weights we're updating the
entire neural net okay so we're gonna
iterate through the number of neurons
and we're going to say let's initialize
our first neuron it's gonna be a new
neuron object okay so we're gonna say we
want a new neuron and this is remember
this is a class that we've just created
and we want to use that populate
function that we already wrote given the
number of inputs okay so that's the
parameter for that populate function and
then we're done with that
once we populated a number of neurons we
can just push those neurons to this this
layers array okay so we can just by
using the push function okay so that's
our layer and now we're gonna go even
higher what was the next step the
network right we've created our neurons
who've created our layers and the next
step is the actual networking sets okay
in a network consists of layers so let's
initialize our network variable by
saying well what is a network tab let's
think about this a network has a set of
layers that's it's those that's the
activity hats will initialize a layers
parameter okay so the number of layers
and now that we've initialized that
let's go ahead and create a prototype
for
giving it the layer parameters okay so
we're gonna call this prototype we're
gonna call this prototype perceptron
generation okay because that's another
word for a neural net perceptron
although it's not as sexy like people
don't use it as much but whenever you
know usually whenever genetic algorithms
come into play someone like throws
around the word perceptron but I I think
neuro evolution in genetic programming
is going to make a huge comeback like
like it had its heyday in the 80s and
then people kind of gave up with it but
like I I think we're gonna see some
great results it's actually coming out
of a open the eyes universe like this
that sandbox environment there's a lot
of possibility here which makes me very
excited about it okay anyway so what
we're doing here is noted if Jim Jays is
a little confusing for beginners noted
okay
so we're going to create what am i doing
here so I'm creating three variables
right the index of pre for the number of
previous neurons and then the layer that
we're on which we're gonna initialize
because we just created that using the
index as a parameter okay so what we're
about to do is well first of all we want
to populate the layer because we are
creating a neural network this is going
to create the network itself okay by
creating a stack of layers okay okay so
that's what we're doing um uh I knew
that would be a Westworld reference at
some point somewhat one of my friends
was like dude you need to watch
Westworld
someone on your channel is gonna mention
it it's gonna be great for memes and
references so you need to start watching
it and lo and behold someone has
mentioned Westworld so I might need to
check up on that show anyway so so we've
created our three variables so let's
populate the layer with the inputs that
we've already created right up here we
have our input we have our um let's see
we have our previous number of neurons
okay so each of our layers is going to
be populated just like that and we're
going to add each layer to our existing
array of layers
okay so layer that's that and then we
went to the index we're gonna iterate
through the index and the index tells us
which layer were on okay so we're gonna
iterate that that index counter variable
so now we're gonna we're gonna iterate
through every single
a layer in our network and we're going
to we're going to add it to our we're
gonna iterate through sorry wait a
minute technical term for this is we're
gonna iterate through every single layer
in our list of layers and we're gonna
add that to our network
okay so we've stored a list of layers
and we want to add each of them to our
network so we'll initialize our first
layer and so this is for the number of
hidden x' and what do we mean by hiddens
hiddens is the the number of hidden
layers that we've that we've specified
in the parameter so we say so we for
each of these layers we want to populate
it with a number of a pins okay um oh
sorry never know let me redo that one so
what I just said undo that it's not
hidden x' number it's not number of
hidden layers it's number of hidden
neurons okay
to answer your question Bhaskar yes I
will do a much more simple neural
network tutorial it's coming up it's
gonna be it's coming up in January okay
so so we're gonna initialize by using a
number of hidden neurons and a number of
previous neurons and why do we want the
number of previous neurons or we want to
wait we want something to point to right
because our weights are going to be
pointing okay so now we want to say well
now that we've initialized that layer
well replace what we have in our
previous neurons with whatever we have
in our hidden stright because we've
we've propagated forward and we're
replacing the old variable with what
we've already done okay so now we're
going to push this layer on to the list
of layers and we're going to iterate
that index counter variable so let's
look over what we've just done what
we've just done is we've iterated
through the number of hidden Euron's
that we were given in the input
parameter we have initialized a layer we
have populated it with those neurons
we've set the previous neurons to
whatever we have wherever we had now and
then we've pushed that layer to the list
of layers okay and so now that we've
done that we'll create another layer and
why do we want to create another layer
well there's one there's one more thing
that we didn't think about we didn't
think about the output remember we we
said this function on ALP
a list of outputs right that we want so
we want to create that last layer for
just the output right so as so these are
our hidden layers right so remember we
created a first layer which is our input
layer who created a list of start for a
list of hidden layers and now we're
creating our output layer okay so that's
what we're doing and we'll create it by
initializing with the number of output
neurons and then the number of previous
neurons and lastly once we've created
that we can just go ahead and push it to
our list of layers that's it okay so our
input neuron was created our list of
hidden our input layer our list of
hidden layers and then our output layer
okay so now we've done that we've
created our Network and before we create
our generation before we do that we want
to create a set of helper functions okay
so the first one I want to do is is
called compute okay so this is an
important step so computation step okay
so let's do the computation step this is
an important step and it's a part of our
network so what do I mean by computation
well as data flows through our network
it's not static
we're actually applying we're applying
operations to it mathematical operations
and I'm going to talk about what these
operations are okay so so the first
thing we're going to do is we're gonna
check if the layer and the neurons are
not empty and if they're not we're gonna
assign it the input value so we're gonna
check we're gonna say okay so check each
of those input values okay let me check
each of those input values and I want to
see if if the initial layer and the it's
number of neurons so I want to see if
it's number of neurons is empty that's
what I'm looking for
what's that neuron got neurons and then
wherever we are right now and so if
there's something there then we want to
add it to our the current layer that
we're on
and we're going to say so whatever layer
that we're currently on add all of those
neurons to it and then and it's going to
be those input values right the input
values so okay so that was that
introductory step where we actually fill
that layer with with neurons okay
so now that we've done that now that
we've done that now it's time to now
it's time to actually do the computation
okay so what's happening here we're
gonna the data is gonna flow through the
layers I'm gonna apply an activation
function to it and then we're gonna
return the output okay so let's do that
activation function step we're gonna say
okay let's gonna start off um this step
is for the unclear okay so let me um let
me see okay so okay so the activation
function so let's talk about this so
what is happening here um I am what am i
doing I'm updating the weights
okay I'm updating the weights using a
sigmoid function exactly so someone said
sigmoid exactly so that's exactly what
I'm doing so what I'm gonna do is I'm
gonna iterate through each of my neurons
okay so so so let's see iterate through
neurons in layer okay so let me just say
I iterate through neurons in a layer
okay so from revoir I equals 1 ah and
for I is less than this that layers dot
length and let me see this when we
better check a pistoleer it's outside
loop since it'll never change ah let's
see just outside the loop since it'll
never change actually actually um that's
a good point that's a good point
and we could just do that if we knew the
size of right so we could actually write
that differently you're right that's a
good point we could we could write that
differently it still works but we could
have written that differently right it's
just an old check exactly okay so where
was I we want to iterate through every
neuron in the layer ah and
where was I still length and we've got I
plus plus okay so what we're doing
iterative every neuron in each of our
layers and we're going to do a double
iteration so we're going to say for bar
J for J and distant layers so for the
length of the neuron and then for each
of the layers okay so so for this top
layers where was I was at this top
layers
I got neurons then where was I okay so
so the first thing okay so let me talk
about what we're about to do well before
I do this I need to initialize this
variable called sum and let me talk
about what sum is going to do so we're
gonna initialize a variable called sum
and what sum is going to do is that is
what we're going to apply the the
activation we're gonna use to create
that activation function so the sum is
going to be if we take the previous
layers neurons if we take the previous
layers and the neurons wherever we are
with that um and then we take the value
we're gonna multiply that by this layers
weights so the where that we're to the
layer that we're currently on we're
gonna multiply it by the waste
so this thought layers I doc neurons
it's not as good as gradient descent um
I mean we haven't seen someone do it as
well as you know state-of-the-art
gradient descent with that crop but I
mean think about it I mean we we have
all been in an evolutionary environment
so I think there's still hope for
genetic programming in general okay so
we're creating this the this the sum
right and so this sum is what we're
going to apply to thee we're going to
apply it to the activation function so
I've prayed this loop and I'm going to
say for each of those layers neurons J
dot value I'm going to say self dot
options and then okay so here's the
activation function step
so I take that activation function and
then the parameter is going to be the
sum that I've just calculated so what's
happening here the activation function
is going to take the sum well which is
the combined weight of the neurons in
this layer and I'm going to I'm going to
run the activation function on that sum
and it's going to convert that number
into a probability and so the
probability is gonna help the data
decide where to flow okay some decide
where to flow and finally once I'm done
with all this I could just say well now
that I've iterated through that layer
make sure that the previous layer is now
the current layer where I what right
just was okay okay so we've done that
that activation step right ok so let's
go ahead and write our genome now so a
genome is just another word for a being
or a you know living a living being
inside of our generation
so each genome has its own on neural
network right so it's you know you or I
or genomes and we have our own neural
network so we'll call this genome ok
this function or this variable genome
and genome has a score and it has a
previous layer neurons dot value is it
is it let's see Shh let's see no okay
so for genome we're going to say scoring
the network those are two parameters and
okay so now it's time to UM
J does exist J is a part of this far j
up here on line 122 okay so for var
genome I want to say okay so it has a
score and it has its own network right
and the score is like where it's at in
the game and that's how it's gonna
that's how we are going to keep track
internally of how well it's doing and
decide whether or not we want it to
breed for the next generation okay so we
know those are our two attributes of
score and it's Network okay so that's a
genome and then now it's time for that
highest level object we're ready to code
our generation and what is our
generation a generation consists of
several genomes okay so a generation is
just a list we can think of it as a lid
of genomes and that's the only parameter
that we have uh the number of genomes
okay oh good call don't capitalize
genomes um genomes data okay great
this not genomes and it's an empty list
right so it's going to be an empty list
awesome
fitness functions for the win okay
alright guys so let's go ahead and
create our function for adding the
genome to our generation okay
so add the genome and add a genome to
our generation so we need a function for
this add a genome to current generation
hey let me get this for a second okay
okay so for a current generation bye
guys hi everybody
okay so let's go ahead and do this so
for our generation we want a prototype
that's going to be called add genome
that's what this is that's what this
does okay and we're gonna say okay so
given a genome which is our parameter we
want to add it to this generation that's
that's our task so we're going to say
well we're gonna say we're gonna iterate
through wherever we are and we're gonna
say I is less than the length of the
genome that were apps which says okay so
what we want to do right now is we want
to make sure that we are we have sorted
it right so that's the first thing we
want to do we want to sort it and we
don't want to add a genome if it's not
sortable so we're gonna create a check
for that we're gonna iterate through the
list of genomes and we're gonna check
the score and if the score is less than
zero so if it's a bad score then what we
want to do is we that's that's the first
check okay and there are two inner
checks so if there's if it's less than
zero and if this if the genome score is
greater than what we've already have in
our list of genomes or Apes remember we
have one in memory and then we have one
that we're looking at if it's greater
than that score then we just want to
break because it's not going to be a
part it's not gonna be in order
okay so we want to break
what's the other breaking what's the
other edge case there's one more edge
case that we want to code and that edge
case is let me back up a little bit and
let me say else if the genomes score is
less than where we currently what we
currently have
if the genome score is less than this
genomes score then we also want to break
so remember we only wanted to add it if
it's sorted and if it's and that this
dis prevents us from adding things out
of order and what do I mean by sorted we
are sorting by the score we have a list
of genomes and they are sorted by their
score so now we want to add our genomes
okay so this set genomes got splice okay
and splice is the verb that we're using
to add the genome tool to our or our
array of char list of genomes okay so
now let's write our breeding function
okay so we've we want we picked the
genomes that we want the next step is to
greet them so let's let's say okay it's
time to breathe time to breathe time to
breathe okay so so that's what we're
gonna do and this is our breeding
function this is our last major function
that we're gonna write okay so time to
breed let's go ahead and write this
right out this function okay stick with
me guys we got this
we're almost done okay so we're going to
create a breeding
function okay and what does a breeding
function going to take us as parameters
well it's gonna take two sets of weights
okay the set of weights from one parent
and the set of weights from the other
parent um okay so we've probably got
about 10 more minutes okay so everybody
relax sit back enjoy the ride okay so
we're going to pass and a number of
children right so these are our two
parents and then a number of children
okay that's what we're gonna do and so
we're gonna do breed these two so
remember these are two good networks
okay we've we've identified them as good
networks because the scores that they
provide are over a certain threshold
that
predefine okay so what we're gonna do is
we're gonna iterate through the number
of children okay however many children
we want are these parents to have okay
so we're gonna say NB 0 as long as it's
less than the number of children that
you've defined we want to iterate
through all of them
and we're going to of course iterate
through each of them alright so the
first thing you want to do is create our
data variable and what is our data
variable - well we take we use the built
in JSON function case on built in parse
function to get the list of weights from
the first parent and it's point and so
the weights are gonna be just a huge
mismatch of numbers right these are just
a huge list of numbers some matrix of
numbers but we want it to be machine
readable for readable right and so if we
wanted to be readable then we're going
to you know what so attached it seems
complicated and I'm gonna get better and
making it more accessible but you know
it all depends on the syntax and this
could definitely be less complicated and
it will get less complicated over time
as I do more live streams so don't give
up hope okay and this is definitely get
easier over time so json dot stringify
and what are we string string applying
um this is indeed neuro evolution okay
so we're gonna so we're gonna stringify
the first weight so that it's readable
and we're gonna store it in a data
variable okay so now it's time for us to
say let's do a little bit of mutation so
we're gonna take the first parent and
we're going to iterate through every
single list of weight in our network
okay um alright so let's go ahead and
say um g not network weights by the way
you guys should next time for the next
live stream I didn't tell you this time
but you should totally be coding along
with me okay so maybe not this time but
next time I want you all to be coding
along with me in future at live streams
okay but for right now just just just
check this out so we're going to iterate
through all of the weights that we have
okay we're gonna iterate through
everything one of those weights and we
want to now
perform some mutation right so in order
to mutate we want to say okay well let's
arbitrarily define some thresholds so
we'll rent we'll randomly generates a
number using the mascot random function
okay we'll randomly generate that and
we'll say well if it's less than 0.5
then we want to update our or weights
from our that we stored in our data
variable right and the data variable is
just a temporary variable it's gonna
store our weights and ruk right so if
it's if it's right to work we're
arbitrarily and randomly going to update
those weights so now we've created each
you done Network and we're gonna we're
gonna add all this way from g2 okay up
up up above so there's that and so now
we're going to add some mutation to each
way so now here's the usual here's the
fun part we're gonna perform some
mutation and so we mutated so what's
happened we mutated the weights in the
temporary data variable using one of the
parents the weight of one of the parents
g2 and now we want to update the weights
given this other so there here's our
other arbitrarily okay so noted so I
need to heavily heavily comments the
code
okay so math at random just less than or
equal to so okay so what we have we're
weak so one of our hyper parameters was
called the mutation rate the mutation
rate defined like how fast do we want to
mutate so that's the kind of thing
arbitrarily think arbitrary number we're
looking at um but we basically want to
say like take those weights and update
them by adding whatever a random a
random variable times whatever our
mutation range is right so like that's
gonna be and we could tune that
differently and it's gonna give us
different results like for the mutation
rate and then we want to subtract the
range so we want it to be within a
certain range okay
but we've added some mutation and now
finally we can go ahead and say we've
added our mutation go ahead and push
what we've just created the list of
mutated weights to our data's variable
and at the very end we could go ahead
and return and so let me write a comment
as well return the list of breda genomes
these are that you know that have been
bred so these are the updated more
advanced stronger more robust neural
networks and that's just gonna keep
going into our each new generation and
they're gonna get better over time
okay so list of breda genomes and we
want to return that okay so that's the
that's the wait that's the code that
we're going to do let me go go all the
way up I'm gonna do a short walk through
what I've just done and so let me so
remember this is the link to the demo
code if you guys want to see it on the
web I showed it at the beginning and let
me show it again for a second it is the
game of asteroids it looks like this
right so many of them are generated many
little spaceships and they're all trying
out different things every time and you
can see them all dying and once they're
all dead new ones will spring up okay so
it's just like that and so let me
explain what's happening here okay so
we're using something called neuro
evolution neuro evolution is a technique
okay and this is so here's here's how we
starting off okay we want to create our
neuro evolution variable than our list
of neurons our layers our network our
genome and our generations it gets
bigger and bigger each remember that we
each object gets bigger and bigger so
it's a type of reinforcement learning
because we are learning in real time
from what the game is giving us okay so
we will initialize our neural evolution
variable well well all of our hyper
parameters that are going to they're
going to signify how we're going to
generate these new neural networks so
remember every spaceship is its own
neural network we'll define our set of
options and we'll start out by
initializing our neuro our neuron
variable each neuron has its value in a
set of weights okay and we want to
randomly initialize each of those
weights for each of those neurons okay
and once we've created a neuron will
create a layer and a layer has an ID and
a number of neurons which start off as
zero but then we'll populate the layer
by adding every neuron
by however many would define like number
of neurons then we'll create a network
and a network consists of a number of
layers which we just defined previously
which contains a number of neurons and
we're going to generate that layer by
first generating the let me mean let me
add a comment here this is a input layer
right that's our input layer then we
create our hidden layers and then our
output layer by using those three
parameters that were given input ends
and output okay and then we have a
computation step this is where we
actually apply the activation function
in each neuron of every layer in our
network we turn those numbers into
probabilities of the score of and the
weights of of the of each of the BOTS
and each of those is a genome right
that's how we define them as each as a
genome and a generation contains a
selection of genomes and we want to add
a genome to the current Network by
making sure that it's sorted in order
and you will use a splice function to
add it and lastly we'll breed our
network by taking the weights of one of
our parents ma mutating it and then
creating an entirely new set of weights
in the state of variable by mutating the
other one and then we'll return the list
of those weights and we can use those
waves to update the the next generation
of neurons okay
so now let me end the screen sharing
okay all right so we'll do a last minute
last Q&amp;amp;A and then we're we're done for
the day
all right so any any last last questions
go for it ask me anything um why does it
need to be sorted it's so it's better
for computational complexity if it's
sorted then we can it's going to be a
retrieval is gonna be faster retrieval
for the spores so that's just for
computational complexity does it make
sense to change in network input with
genetic algorithm yes absolutely um well
the network input is already gonna
change because games are dynamic right
so there's that what screen sharing apps
you use I use Google Hangouts I guess
you've also rap now you're absolutely
right let me decide what to rap about
someone said something about Karros yo
and I'm gonna do this without a beat
okay here we go
I was
Chara's ma'am my life is so lost I'm
sitting here in Portland man what is
this it's like I don't want to curse
before Lin's
not really and things are going wrong my
mind is so loose that I'm going all
around okay so that was that um Wow man
got that Portland Jam happening right
now it's on point it is 10:00 a.m. on a
Wednesday everybody it is 10:00 a.m. on
Wednesday for me so that's that's what
happened um so that was my impromptu rap
and how am I I'm doing good am I gonna
test it I'm gonna add the code to the
link description okay so remember to
check it out
that just happened Brian you're
absolutely right uh that just happened
it is a place I'm going to shoot an
experimental video here and cool one
more question and we're good to go
is there an easy way to save the data
that is created by the network in order
to implement say that in order to
implement it
yeah no the data is saved in the data is
variable and it's just a list of weights
and we can we could use those weights to
update other types of networks as well
and that's that would be a part of
transfer learning okay so we could learn
from one game and then ideally apply
those weights to any game and that's
what deepmind did for the Atari DQ
learner okay um do you have a job no
this is my job data set of all reddit
comments available anything in the
future I'm sure I'm gonna do something
with Reddit comments I'm not in India my
parents are from India I was born in
Houston Texas and now I live in San
Francisco California I'm new to this
channel what languages do you use mostly
Python in JavaScript and cool okay guys
uh Atari didn't use neuro evolution they
used something called a deep Q learner
okay uh right now I've gotta go um did I
dye my hair yes what's new for 2017 my
opus magnum my opus magnum My Beautiful
Dark Twisted Fantasy which is this new
machine learning course but I'm not
gonna say much about but just know that
something big is coming guys okay cool
so yeah that's it for this live stream
um
thank you guys so much for showing up
I'll add to coach in the comments I love
all of you thanks for showing up during
Christmas I know you know you have a lot
of things you can be doing so for now
I've got to go hike a waterfall so
thanks for watching come on love you
guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>