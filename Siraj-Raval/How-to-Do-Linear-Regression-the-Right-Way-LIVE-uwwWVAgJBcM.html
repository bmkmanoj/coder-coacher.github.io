<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Do Linear Regression the Right Way [LIVE] | Coder Coacher - Coaching Coders</title><meta content="How to Do Linear Regression the Right Way [LIVE] - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Do Linear Regression the Right Way [LIVE]</b></h2><h5 class="post__date">2017-01-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uwwWVAgJBcM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">see let's see everybody I'll minimize
myself I don't need to see myself fun
you see you guys I need to see you guys
a world it's Raj I am high for this live
session yo class is in session everybody
we are about to do some math this live
session so I'm super excited but first
of all let me taste a roll call
all right so let's see Colin Brennan nil
David Akash Sebastian Raj Spencer the
rash Nikko Clement hi guys Michel
Benjamin alright so that was row called
uh welcome to this live session uh for
the for the deep learning intern
deepening course okay this is going to
be so awesome because I have been
waiting to do some math and guess what
guys guess what
I bought this a tad to write some math
on okay and I've never used this before
light strings on super excited for this
I'm going to show you guys the math
behind linear regression by the end of
this video you guys are going to are
going to know like the back of your hand
how to do linear regression that
includes gradient descent and guess what
we use gradient descent all over the
place of machine learning don't worry if
you don't know what that is I'm going to
show it to you okay so we're going to
deep dive into this so um we're going to
start off with a five-minute Q&amp;amp;A like
always and I think we've got some
Udacity peeps in the house as well a
Dhruv Nico and a Matt who's a the other
instructor for the course so you guys
city guys if you're if you're I think
you're here a shout out like say some
things and people know who you are and a
so I'm gonna do my five minute Q&amp;amp;A like
always and I'm going to answer all the
questions related to me
and you know by everything but if you
have anything like you'd ask these
specific questions they will answer
those ok so let's start off with a
five-minute Q&amp;amp;A and then we get right
into the code in math ok do I have to
know about partial derivatives we are
going to do a partial derivative um but
I'll show you how that works um I had to
cut off PewDiePie to catch this whoa I'm
honored I'm on a baby girl let me see
that regression all right so that's not
a question guys let's get some let's get
some real questions in there some some
quality some quality questions all right
would you want to check out my buy of AI
assistant demo sure yes opposed to
github link in the comments of one of my
videos I read all my comments I answer
all my comments see I'm not I'm not I'm
not fake you know I'm saying I answer
all my comments I'm here for you guys
have I enrolled in is calculus required
for linear regression yes a little bit
of calculus but I'm going to go through
that don't don't be afraid by the word
tacos this is this is actually very
intuitive but um can you mention some
details about the upcoming as well uh
looking to predict your genre from all
right
what basic max will give you don't do it
you need to know basic algebra okay and
then we're going to learn the calculus
necessary to do this in this video
okay our ganz the future yes I mean the
idea between generate generative models
which in general are really exciting
because there's you can generate things
that don't exist and that has a lot of
potential for art and culture it ganz
can change culture right we can generate
music we can generate arbitrary
paintings in ways that humans couldn't
best book to understand math behind ml
machine writing a probabilistic approach
that's a pretty good one just math today
or coding to both mostly coding linear
regression versus other classifiers like
SG DC linear regression is definitely
easier what is no free lunch it's a
theorem the no free lunch theorem very
high level it's like well you you can't
you can't make assumption you can't make
assumptions whenever you are doing
anything about related to proving
something just um
when will you do NLP yo I'm going to do
so much NLP
in this course I can't wait for NLP it's
coming up will you cover games I kind of
want to just do cans right now you know
what I mean like I'm super excited for
games I will do again um I will give an
intuition why to do gradient descent
over over yes I will explain that linear
algebra is a way to go yes what's the
different between scikit-learn and PFR
scikit-learn and Kiev are a great
question so tf1 is a high level rapper
on top of tensorflow it's very similar
looking into cyclic learn but side you
learn specifically is uh it does so TF
run only focuses on deep neural networks
psycho learn uses support vector
machines and all sorts of other machine
learning models whereas TF art is the
same kind of it has the same brevity but
it focuses on only on deep neural
networks you prefer what that no no
rectus no uh when will you start when
will you start working on anaconda I
mean mostly almost like they start using
docker to containerize things
all right rap for 50k subs okay let me
rock 50 steps and this time I'm going to
play an intramental I'm just going to
wrap without a mr. battle I mean don't
be discouraged rap hip-hop instrumental
on YouTube whatever tries playing
someone say a keyword and we really
started triumph it buckles from always
just let's go play all right let me just
unplug my mind so you all you guys can
see this as you music where's the music
at 50k subs I got 50 cakes out man my
mind is so fresh I'm looking at this
coffee man looks like the best I got
kaki all my highlighting so eyes to the
side I got a USB for my oh my I'm gonna
be waiting back to stay like this all my
online I see it man it's all fine as I
write these equations online I see it
coming back right moving your aggression
right now
mom hi okay okay okay so that was it for
the rap okay now we're gonna get started
at the code okay so let's go ahead and
do this I'm going to start screen
sharing and then we're going to get
started um alright here we go there we
go Google Hangouts all right and what
does hangouts want to do hangouts wants
to screen share hangouts ones to screen
share your entire screen share all right
so I'll minimize this and minimize and
then I'll move this out of the way so I
can see what you guys are doing okay and
we're going to code this baby okay I'm
in the corner here let me make sure that
what you guys are saying is what I want
you to see um yes what you guys are
saying is exactly what I want you to see
perfect alright okay so here we go
here's what we're gonna do guys let me
make this big test this assess which is
big enough right okay so in this lesson
we're going to do linear regression okay
and what is linear regression right okay
so linear regression
in this case um let me make sure
everything's working um everybody's here
live tries working live video is not
working ok so here's here's how it goes
so we are going to do this ok uh so this
is gonna be called video regression this
is linear regression and let me let me
just let me just show you guys the best
way to explain it is to show you two
visually so I'll show a cute visual what
exactly we're going to be to it and to
showing visually I will give you a link
to this and I will just show it right
there this is what's happened hi so
we're gonna we have a set of points and
this point are these points are the test
scores or students and the amount of our
study okay so this is what it looks like
so this right on the right described
here these set of points are the set are
the the x-values are the amount of hours
they studied and the y-values are the
test scores they got okay and
intuitively to us there must be some
kind of correlation between these two
values but we want to prove this
programmatically we want to prove I mean
sorry mathematically we want to prove
that there is a relationship and how do
we prove that there is a relationship we
draw a line of best fit so how do we
know what that line of best fit is that
linear regression is what we don't know
we don't know we want to we have to find
that and the way we're going to find the
line of best fit is using gradient
descent and that process that training
process looks like this we're going to
draw a random line compute the error for
that line and I'll talk about how we're
going to compute that error and that
error value is going to say how how well
fit is this line to the data and then
based on that error is that it's going
to act as a compass it's going to tell
us
well how best should you redraw the line
to be closer to the line of best fit and
we'll keep doing that so it'll be like
draw line computer draw a line computer
draw a line computer until eventually
the line that we drop is the optimal
line that we should draw okay so that's
at a very high level but now I'm going
to go into the code and we're going to
talk about this in detail all right so
let's go ahead and start it um so to
start off to start off I'm going to
write my main function okay so let me
move all this stuff out of the way okay
right into the code all right okay right
into the code for this and guys also if
if
people have questions and I'm not able
to answer them because I'm busy doing
suffering please help me answer
questions I very much appreciate it
I have very much appreciate it okay so
let me just show it by writing the main
function well why Mike what is the main
function - well text that's where the
meat of the code goes right okay so in
the main function will write a run
function which is where we're going to
store all of our and logic okay so let's
write up the run function so the run
functions are transfer to show what
we're doing at high level and a high
level so step one is collect our data
right always a machine-learning we want
to collect our data so we'll get our
data points and we're going to oh look
how are we going to collect our data
right well to collect our data we have
to import the one library that were
using I know that we're using a single
library and that library is numb hi
alright and we're going to use this
little symbol that means you want to
continually say numpy whenever we call
it Beth or its functions okay so what is
the function we're going to use from num
pot so the function Vinny's from numpy
sorry right name thank you mean good
call so the front two degrees from dumb
pi is Jen from text and what this is
going to do is it's going to get a data
point from our audited data file and let
me show you guys the beta file as well
but basically we're going to separate
separate it by the comments okay
and we're gonna get those points so what
what does this what does this beta look
like well let me pull up terminal and
show you guys exactly what this data
looks like it looks like beta dot exp
okay
so let me zoom in on this one do way
more joint so these are just the our
study on the left side and then the test
forms for a bunch of students for an
intro to computer science class
okay the our study uh and the test
scores they got okay so that's what
we're going to pull that's our data set
that's what we're going to pull into our
points variable so he's a bunch of so
points is going to contain a bunch of XY
value pairs where X is the amount of our
study and Y is the test for okay and
it's separated by the comma okay so
that's
that's step one we've done that and
different taxes to essentially running
two main leads the first look converts
each line of the bonds with sequence of
strings and the second one is converting
each frame into the appropriate data
type okay so that's step one
run step two if you can find our hyper
parameters okay in machine learning we
have what are called hyper parameters
these are tuning knobs for our model
they are basically the parameters that
define you know how our model is is
analyzing certain data how how fast it's
fitting to the data what what what
operation is performing idea there's a
whole bunch of hyper parameters um thank
you for the feedback there's a whole
bunch of type of parameters and what
we're going to use is the learning rate
now the learning rate is used a lot in
machine learning and you know and it
basically defines how fast a fast should
are a model converge and converge it's a
word for convergence convergence means
when you get the optimal results the
optimal model like the line of best fit
in our case that's convergence so how
factually we converge well you might be
thinking well should the learning rate
just be like a million if you went to
convert super fast well no like all
hyper parameters it's a balance okay so
if the learning rate is too small we're
going to get slow convergence but if
it's too big then our error function
might not decrease okay so it might not
converge so that so that's our first
type of parameter our next type of
parameter is going to be the initial
value for B and the initial value for M
and what is B and M well what we're
going to do is we're going to calculate
the slope right so this looks like y
equals MX Berkeley so this is why I said
we only need to know basic algebra this
is the formula this is neat slope
formula for bubbling okay all lines
follow this formula where Y so M is the
slope B is the y-intercept x and y are
the points okay so that's the line okay
so this is our initial B value our
initial slope our an initial y-intercept
we're going to start off as zero okay so
and then the last type of parameter is
going to eat is going to be the number
of iterations how much how much do we
want to train this model well we have a
very very small data set there's only a
hundred points okay um and for that
we're not going to we're not going to
iterate a million times or 100,000 times
we're just gonna iterate a thousand
times
okay so that's our high parameters and
now step three is going to be to UM a
thick our
trainer model is praying our model train
our model okay so the first step is
going to be to show the starting brave
descent
okay at B equals what is the starting
rating descent it's going to be zero
right and then M is going to be the
starting point it's 1 4 that will say 1
and this is just for us to see the
difference here okay um okay dot for max
initial B Mitchell M I so um okay so
what's happened here complete error or
line given points so let me just write
this out and like entropy hmm and then a
point okay so what is what's happening
here let's go over what I just wrote you
so in this line we're going to show
these starting up D value to starting MW
so what is our starting line step what
is a starting slow and what is our
starting error and I'm going to show you
how we're going to calculate that there
and to get that error given our be given
it given our B and M values we have to
sponsor your called compute error for
line at given points it's going to take
the VM and the points and it's going to
compute the error for that and it's
going to output that so that's going to
be our starting point okay and then now
we're going to actually perform our
radiant descent and it's going to give
us the optimal slope B and the optimal
slope I'm sorry it's gonna go see
optimal slope and the optimal
y-intercept
so for great descent we're going to work
we're going to call this method the
gradient descent monitoring so given
point give it an initial V value and
initial and so initial end value in our
learning rate so this is where we're
going to use all the type of parameters
right because this is where we're
training a model so number of iterations
those are all the things we need for
this okay and we're going to define
these functions in a second okay we're
gonna we're going to go deep dive in and
define these functions
okay so then after we've paid our model
well now we just train down right so let
me just copy and paste this so now this
is not our starting point this is now
our ending radiant any point so face our
ending point where he is this B is 2 m
is 2 and then error is 3 and these
numbers just to find a what we're going
to see at the end for the number of
iterations for B and then for M and then
for computing the air for line at given
points given B that the final D value
the final head value of n plots
okay so okay so that is high-level
what's happening here so all I did was I
just printed out the initial B and M
values nothing and then the error uh and
then I computed the gradient descent and
then I turn out the final values so I'm
about I'm about to do this now okay so
we haven't actually done it now we're
going to do it so the first thing I want
to do talk about is how we're going to
compute that here okay so let's let's
let's um right at that first function
what was that first function call it was
called um compute error for line at
given points okay okay so and the data
set I'm going to provide that as well um
but but let's let's go ahead and write
up this method okay so this is the first
step we're gonna write up this method if
you care for line at given place okay
I'm so excited to show you guys because
I've actually use my math tag for a
second okay so so let me write one write
this up okay okay here we go okay so um
okay
I mean write this out okay so we got a
line here oh man what a great what a
great line that is okay so this is our
plot okay and so we've got a bunch of
data points here we've got a bunch of
data points right it's all over the
place and what we're going to do is we
are going to draw a random line through
that data okay we don't know we don't
know the line invested so we're going to
draw a random line through the data and
then we're going to compute the error of
that line and so that error is going to
tell us how good our line is okay and so
how do we know how good our line is well
what we're going to do is we're going to
for every single Y value on that line we
are going to calculate the distance from
each point from our data to the line
okay so all of these distances all of
these distances distance one distance
two this is three distance 4 this is 5
this is 6 oh and then you know we
probably have more data points down here
these distances the distance for this
line and so we're going to take all
those distances and we're going to sum
them and so let me show you the equation
for that okay now so rather than
actually writing out this equation like
really you know sloppily I'm going to
show it to you using this ok so ok so
this is the equation so let me explain
what this is what this is so we got all
those distances right we got all those
distances we're going to sum those
distances together and that and then get
the average of that but guess what we're
not just going to sum those values alone
we're going to square those doggies and
why are we squaring those values because
we're squaring those values because we
want we want to first of all to be
positive uh and it doesn't really matter
what the actual value is it's more about
the magnitude of those dogs right so and
we want to minimize that magnitude over
time so this is the equation for that ok
so let me explain what does what the
hell this is okay so we're computing the
error we're computing the error of our
line given M and B so given m and B
we're going to compute the error overlap
M is our slope and B is our y-intercept
so this e looking thing is called this
is called Sigma notation it's a little
worth giving
a little refresher here this evening
we're gonna see a lot of machine
learning it's called Sigma notation and
basically the talk it's a way of
describing calculating the sum of a set
of of a set of values alright so as the
sum of a set of values so which is what
we're doing we're talking the sum of a
set of points so for and so if the
starting point is where I equals 1 and
the ending points for an N means for
every point okay so for every point we
want to calculate the difference in Y
values right so it's y minus MX plus B
and why do we say MX plus B because in
this one equation and y equals MX plus B
right so it's y minus MX plus B which is
set which essentially boils down to just
Y so it's Y minus y squared and we'll do
em and then we're doing that for every
single point and so we're going to add
all of those points together ok so and
then get the average and so that's quite
1 over N because we're going to get the
average of that and that's value that
value is the error okay so at a high
level that is what that is
so now let's programmatically write this
out ok so we're going to start off by
initializing the air conditioner I skate
at 0 ok so our total error at the start
is just going to be 0 this there's not
anything that that's um uh we don't have
an air yet ok so then for every point
for every point so providing range of
starting at 0 and then going for the
length of the points right with all of
our data points so for every data point
that we have we're going to say let's
get the x value all right so let's take
that x value so x equals points I 0 and
then we're gonna get that Y value right
so then get the white I drank so I'm
just basically programmatically showing
what I prom just talked about
mathematical right so we've got the x
value we got the y y value and we want
to compute that distance right we're
going to do this on every single time
we're okay so for then
the difference Square it and then and
add it add it to the to the total okay
so so here's that you're the actual
employee right so we're going to we're
going to be plus people because it's a
summation and we're going to program we
show what I just talked about right here
right y minus MX plus B squared okay and
we're going to the sum of that
so Y minus M times X plus B where okay
and we're going to that for every point
so this whole iteration loop right here
is that equation okay
minus the average part so that that's
gonna give us the total value the last
part is you average it so we'll say
total error divided by floats the length
of the points there won't be a float
value and that is the equation that is
the equation right there okay so and
then 50 average 15 average so back so
this is a 10 line function just describe
what I talked about right here in this
math equation okay we got we saw all the
distances between all those points as I
showed right here with something all up
we squared them and then we we got the
average and that is our error okay and
so that it's dependent and we're
calculating that because we we want a a
way for us a measure of us something to
minimize over time right we've something
to minimize every time we redraw our
line we want to minimize this error
because this error basically is a signal
it's a compass for us it's telling us
this is how bad your line is you need to
get better you need to make me smaller
I'm really big right now make me smaller
and that's what rate descent does that's
what gradient descent does and I'm going
to talk to you I'm going to explain how
rated descent works in a second but
that's that's that first function right
so what okay so what was the second
function we wrote it was called gradient
descent runner so this is our actual our
grading descent function okay so let's
now
write this out okay this is our second
of three methods um before we're we're
done okay
so gradient descent run so given a set
of points given a starting value for P
given a starting value for M even our
learning rate and given our number of
iteration we're going to use all of
these things to calculate gradient
descent we're going to use every single
train okay okay so let's get that
starting I'll be in empower
okay so the starting value for P we're
going to say to be and the starting
value for tram we're going to sighs -
yeah okay simple enough and now we're
going to perform gradient descent what
is radius a I cannot wait to explain
trades that guys I found the perfect
analogy great time really time okay so
um so uh okay so before me before I
explain that let's just let's just
perform considerations because the
actual math is playing the third and
last function that I'm about to write so
for every single iteration that we
define we're going to perform what's
called the gradient descent so we're
going to update the and hem with the new
more accurate PNM by performing of the
gradients by performing this gradient
step okay um so II am is we're going to
return via am by performing just raises
gradient step we're going to explain
this is the it's where the math is
happening given our current V our
current have given are the array of
points that we have
and then finally given the learning rule
we're going to tackle eight that final
value of the in it and this one
once this gradient descent is done we're
going to return that optimal B&amp;amp;M right
and so that's what we talked about this
bottom part right it returned that
optimal vnf value have to performing
writing descent and we then printed it
up because that optimal B and M value
gave us a line of best fit we plug them
into the y equals MX plus B's in quite a
formula it gave us the line investing so
now we're going to write out the
gradient step okay and this is gradient
mother-effing descent okay
so this is how it's gonna go get okay
here's how it's gonna go down step
grading so I'm just gonna say it's time
for the magic the magic the greatest
greatest okay so that's how excited I'm
I just wrote the greatest twice okay so
okay so given our current vnm values um
points and the learning rate and this
actually isn't going to help with them
so hopefully that so given our learning
rate okay let's perform braids this time
so okay what is gradient descent okay so
let me show you guys it's so um okay how
much why um described it so we have let
me just show you this image this is
going to help a lot okay so this is a
graph okay so let's just look at the
graph on the UM I mean it's the same
graph it's looking at it from two
different angles it's the same graph
okay so let's look at four let's look at
the one on the Left just just a thick
one it's the same graph though we have a
bunch of y-values I'm sorry a bunch of B
values and a bunch of em vouch and then
we have that error right that error that
I just talked about right so given so
the 2d graph would be given or any every
single y-intercept we could have given
every single M value we could have what
is the error okay so for every Y
intersect and slope pair what is the
error and so what we'll find this is a
three dimensional graph this is a three
dimensional graph because the error
value yeah it's it's kind of like it
starts off high and then as you approach
what's called the local minima in our
case the local minima which is these
smoked at that point at the very bottom
that is our fact that is where we're
trying to get you okay so given a set of
y intercepts and given us given a set of
slopes possible winding sets and
possible slopes we want to compute the
error for those two things and if we
were to graph that could wrap the
relationship between these three things
it would look like this now it tends to
always look very similar to this
in more complex cases we have like many
minima we have many little values but
what we're trying to do is get that
point where the error is smallest and so
how do we get that point where the error
of the smallest well we're going to
perform what's called gradient descent
to get that smallest point that Valley
that smallest point and a great analogy
for this a great analogy for this is a
bulb so let me just search Bowl okay
okay it's like it's kind of like a bowl
it's like we drop a ball into a bowl and
we want to find that point where there
were the ball stops that end point the
lowest point that that vnm value is our
optimal line of best fit doubt okay and
the way we're going to get that is
gradient descent and we're going to
descend right we're descending down the
bowl using the gradient and gradient is
another word for slope we're going to
descend down that bowl until we get
through iteration
that's that lowest point and gradient
descent is use everywhere in the machine
learning okay it is like the
optimization method for deep neural
networks it's not that apparent right
now but know this know and understand
gradient descent like the back of your
hand because it is going to be very
useful in the future okay so I don't
know why I'm doing that equation that
was that was in this or those that was
the equation for the sum of squared
errors that we just talked about some
squared distances okay so so so how are
we going to calculate like writing
descent well now let's let's actually do
it okay so for our step gradient
function we'll start off with an initial
gradient value for a B so B but B is
going to be zero and so the m's gradient
is going to be zero as well okay these
are the starting point for our gradients
and Brady means slope and so the rate
the gradient is going to act like a
compass it's going to always point
downhill so this is what I mean by once
we calculate that error it's going to
act as a compass for us it's going to
tell us up where we should be going what
direction we should be going how we
should best redraw our line so for okay
well someone has why is the lowest point
the best the lowest point is the best
because
was it is where our error is the
smallest and when our error is the
smallest that's that's when we have the
line of best fit when the error is the
smallest that B and M value those cute
when we plug into our slope equation is
going to give us the line of best fit so
that's why we're calculating the error
okay so so for I even range 0 to the
length of points okay so we're going to
get so what we're going to do is we're
going to iterate through every single
point on our scatter plot okay so every
single data point that we have we're
going to collect it okay so we're going
to say okay whiz so 4 will turn up at
the first point right we'll start with
first point which gives us an x value
and a wideout x value and a lie back so
let me also write out a little comment
for this starting point points for r3
okay now we're going to get the
direction with respect to B and M now
this is uh the last part but it's a very
very important part and this is where
calculus comes into play
okay so I'm going to talk about how
we're doing things okay so let me talk
about what we're what we're about to do
so what we're going to do is so tipping
so for every single point for every
single point that we have we're going to
calculate what's called the partial
derivative okay it's called the partial
derivative with respect to B and with
respect to M okay and what that's gonna
do is it's going to give us a direction
go for both the V value and the end up
right so remember remember in this graph
we want a direction right we want it we
want to be going down the gradient and
so right and so and so right so on this
left hand side you see this gradient
search it's going to ask you see the M
values and the V values are increasing
in the direction that they should be
because gradient descent is essentially
a search pulse it's a search box we're
trying to find that minimum error value
okay um and what we're going to do to
get that is we're going to we're going
to compute the partial
derivative with respect to be in it and
let me show you the equation for the
partial derivative okay
the partial derivative is going to be
right here all right so this is what the
partial derivative does the partial
derivative is and we call this a little
little um so we call it partial we call
it partial because and it's not calling
us the whole story right we say it's
partially because we're calculating will
calculate for for both B and M there are
two different things
and so it's going to give us the tangent
line so it's going to give us this line
as you see right here right see this
line is that line is our direction or
we're going to use it to update our dnm
values okay okay so that's what that is
and let me also show you the equation
for the partial derivative because we're
about to write it out so here's what the
equation for the partial derivative with
respect to M and E looks like okay
they're two different um equations right
so let's talk about the one on top so
this little curvy thing that you see up
here that's that just signifies that
this is a partial derivative next that's
the math signifier this is a partial
derivative now we talked about Sigma
notation right because it's a summation
of values right and that's what we're
doing for so named the partial
derivatives for all of our points okay
for all of them to compute that gradient
value okay and the partial derivative
with respect to M and E is going to look
like this so let's let's write this out
okay um so the B gradients so it's going
to give us two values so the B gradient
is going to be plus these bolts and then
what was it let me look at the equation
again 2n okay to 2 over N so negative 2
over 10 all right
and thanks goodbye and then we're going
to and then what was it there was y- and
he start equations these are laws there
are beautiful laws that always stay the
same and the
they they it goes they give us humans is
a way of understanding the direction
that we want to move in okay so we print
it okay so all right so then we'll do
the same thing it was the second
equation it looked pretty much the same
- is it didn't it doesn't have this X
right the second one doesn't have this X
right so we'll say but it does have this
2n does have his 2n and then it does
have um oh she does have this X it does
have um y minus M current times X plus
um okay okay okay so now we computed our
partial derivatives right so let me one
more time show you guys it's giving us
directions for to go for both D and M
either and remember the partial dick
it's not telling us the whole story they
telling us what direction should we go
for D and what direction should we go
for M and it's going to talk the
direction remember for a bowl to get to
that bottom point where that error is
the smallest
right here okay so right here where I'm
where my where my mouse is that point
that point is what we want to get to and
that's what the partial derivative is
going to help us so what we when we've
computed the partial derivatives
the sum of them with respect to V NF now
we're going to now we're going to update
our PNM values right so we're going to
use that to update our PNM values and
guess what this our last step this is
our last step using this partial
derivative using our partial derivatives
right plural there's two of them
so and that's going to give us a new
value for PNM or updated PNM value so we
have our current value for B whatever it
is that we we fed into this that
gradient function that keeps updating
every time and this is where our
learning wave comes into play
okay this is why our learning rate is so
important important because it defines
the rate at which were we're updating
our B and M value rights more than 0.001
right and then also our and current yes
learning rate times the M gradient okay
and then event and then we'll return
those values and we're doing this every
time right this is this movie a new
antics on our final p.m. it's arts asset
function word for word we're doing this
every iteration right we're doing this
for for the number of iterations we had
a thousand but it's going to return a
new B and M value every time and guess
what guys that's that's it for a code
that was it so so let's go over what we
what we've done um okay but let me
actually let me let me check for errors
right let me let me check for errors and
then I'm gonna answer more question
because I wouldn't really want to make
sure you guys understand how this works
okay so let me tell me let me um demo
this so I condemn it up high
bubble name n is not define okay
oh right guess what I didn't define n
and is the number of points blank of
points all right so let's go learning
rate is not defined where where is money
right not define learning rate is not
defined oh wait a second oh yeah right
learning rate right okay what else what
else is that umm I can overflow for
double scalars 114 total error
why - Omaha uh-huh
typo learning late learning right old
Erik okay so what's going on here okay
let's say this so yeah I mean it print
out the final that okay so it got our
final value um right here um and if we
wanted to let's see hold on a second if
we wanted to we get our back up here
disengage so right so let me clean let
me blow this blow this up like way way
below and let me just separate it so
this is what our happens going to look
like
right so boom just like that that's how
fast the train in milliseconds why
because our dataset was so small it okay
if that was so small
alright so that would happen and after a
thousand iterations we got the optimal B
and M values so right I didn't start off
with DN M is 0
and we've happily the error for a random
line that we drew and it was huge but
eventually after running gradient
descent we got the optimal be the
optimal M and the optimal and the lowest
error point which is that that smallest
point in the bowl and to do that we use
gradient descent with respect to B and M
okay so let me go over one last time
every single thing that we've just done
just to really go over it and then we'll
do my last five into it okay so we
started out by collecting our data set
right our data set was a collection of
two test scores in the amount of power
study right the X Y value pairs the test
scores and the amount of our state a two
variable data set then we define our
type of parameters for our linear
regression our learning rate which talks
about how fast we should we should learn
our initial vnm values for the slope
equation y equals MX plus B the number
of iterations a thousand because our
dataset is pretty small and then we ran
Brady descent so what did gradient
descent look like well for every
iteration
4080 radians we computed the gradient
with respect to both D and M and we did
that constantly until we got get optimal
B&amp;amp;M value that gives us that line of
best fit now how do we compute the
gradient to do that we said okay well
we'll have a starting point of 0 for
both of those gradients
remember radiant is just another word
for slope and then uh we said okay so
for every single point in our
scatterplot for every single point our
scatterplot for our data I will compute
the partial derivative with respect to
both V and M and that got those two
values are going to give us a direction
I sent them direction where we want to
go how do we get to that lowest point in
that bowl right that three dimensional
graph that lowest point and we use the
learning rate to determine how fast we
want to update our DMM values we got the
difference between the current value and
will be have before and we return that
so for every point we did that for a
thousand iterations okay and that's what
it gave us the output and it looks like
visually visually it looks like this
right
the dudeism it's kind of like we look
for chin right it starts off fast and it
gets slower slower as it approaches
convergence the word we use when we have
an optimal line of best fit
convergence see let me do it one more
time uh-huh just like that okay so that
was that was that and now I'm going to
screen share and do a last five minute
Q&amp;amp;A all right stop screen sharing hi
everybody
okay let me let me bring you guys back
on screen to my last five minutes you a
ask me anything and yeah how's it going
everybody any questions I'm open to
questions where did I use numpy it's at
the very top all right what's the
practical use of linear regression
great question anytime we want to find
the relationship between two um two
different variables and then you know in
more complex cases there could be more
but we want to prove mathematically
right math is all about proving things
you know in a way that is unfalsifiable
that no one can say hey that's not true
well I can prove it mathematically so
it's a way to show the relationship
between two value pairs right so maybe
prices and the time of year right so
what is the real estate market outlook
life or you know anytime that there you
intuitively you think there was a
relationship you can prove it with
linear regression and but really I did
this to show gradient descent that
optimization processes that it's very
popular in deep learning we're going to
use that in our deep neural networks in
in the in the rest of the course okay um
why are you biased towards Google I mean
II it's not really a yeah I mean
tensorflow is awesome it's not like it's
not like Google's like Suraj I want you
to talk about it I just tend to flow is
the best deep learning library out there
right now that's one and of course it
wouldn't be because Google knows what
they're doing they they handle billions
and billions of queries every day they
have to be able to do machine learning
at scale the problems they solve
problems that no one else is even
thought of soul and all of those
solutions are found in tensorflow for
machine learning please make an AI dr.
um you can create a classifier to
classify between um different types of
disorders that you see in an x-ray
that's going to augment doctors at first
but eventually replace them uh how about
fitting a quadratic curve instead of a
linear line uh we can do that as well
I'm going to provide the dataset and the
code uh I can talk slower sure how to
find the optimum learning rate uh that's
a great question um there's several
methods of doing that but that's great
intuition sometimes we can we can use
machine learning to find the optimal
hyperplane it's kind of like machine
learning for machine learning but we'll
talk about that later this is the first
life session of the Udacity course uh
teaches calculus I'll do more of that in
the future I'm going to be I'm going to
keep doing calculus okay um two more
questions and we're good to go
two more how would you recommend me to
start machine learning watch this series
and watch my learn Python for data
science series watch my intro at
attention flow series watch um my
machine learning for hacker series
watch my videos ah why is your viewed
SAT too expensive I didn't decide the
price guys I try to get a low is
whatever you got paint you got paid
graters for that okay and grading is not
cheap human graders look all the videos
are gonna be released here on my channel
so alright so I'm here for you guys okay
I'm trying to draw my friends throw my
stuff
Siraj robble okay this is the egg okay
so that's it for the questions and all
right so for now I've got a shoot a
fighting scene for in my next video
what yeah so thanks for watching
mom love you guys I'll post a link in
the comments right when I'm done alright
not behind the video description I post
the game link and then the data set
everything is going to go in the
description within the hour
alright bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>