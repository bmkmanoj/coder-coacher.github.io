<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make Data Amazing - Intro to Deep Learning #5 | Coder Coacher - Coaching Coders</title><meta content="How to Make Data Amazing - Intro to Deep Learning #5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make Data Amazing - Intro to Deep Learning #5</b></h2><h5 class="post__date">2017-02-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/koiTTim4M-s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">data is sacred every hour of every day a
new center is connected to the web every
trip every memory every creation every
discovery joins the ever-growing web
we're building together amidst the world
of ever-growing skepticism and altered
data is true it's transparent provable
most of it is unstructured streams of
raw numbers being a mass that busying
rates but by applying intelligence to it
we can find the patterns and connections
that matter we can find a meaning hidden
in the numbers The Economist says more
for you is less for me but the lover
knows more for you is more for me too
and when we visualize the right data it
gives us that most precious feeling at
the intersection of art and science
wonder hello world it's Suraj and today
we're going to learn how to pre-process
a data set yes
preparing data is one of the most
important yet most overlooked parts of
the machine running pipelines a lot of
introductory tutorials just have you
import a pre-processed version of a data
set like handwritten characters or movie
rating in just a single line of code
real world is not that easy once you've
decided what problem you're trying to
solve or you have a question that you
want the answer to it's time to find the
right data set I want to know what
happens the plans they send you I toured
them on Microsoft Azure the predictions
your deep net makes are only as good as
the data you give it garbage in garbage
out so you want to make sure your data
is relevant to your problem there tons
of resources defined publicly available
datasets and I'll link to some in the
description the de facto standard format
for data is CSV most software packages
out there deal with data in that format
and you can convert your data into CSV
format just as easily there's so much we
could potentially do to our data but
there are three key pre-processing steps
for every data set we'll cover
cleaning transformation and reduction
we're going to look at three different
data sets and go through these steps for
each one to get them ready to be fed as
well not so let's start with our first
the first data set will use is
music-based
the data was collected from a game
called CagA tune in the game two players
listen to a song and tackle with genres
and instruments that they think are
relevant when the song is over the
player who had the most correct tag
get the point so I would win every time
our data set has 25 thousand songs with
the correctly labeled packs we want to
train a model on this data so that given
a new song it will correctly classify if
Chandra will import tandas to help us
parse this data set then the reads PSD
function will let us store the data into
two-dimensional pandas data structure
known as a data frame data frames are
easily modifiable and we'll call our
variable new data let's explore this
data first shall we will display the
first five rows using the head function
with five as a parameter so basically
each row is numbers has an ID and then
either a 1 or 0 next to a tag to
indicate whether or not the given mp3
has bettah seems simple enough we can
use the info function to get some more
data only 38 Meg's so for our cleaning
steps is there anything we need to do
not really each label has a simple
binary tag is consistent and luckily our
data does not have empty value
but my soul does we can move right on to
the transformation step what are some
modifications we can make to this data
that will make it easier for our model
to understand well notice how a lot of
the tag are pretty similar sounding like
you know singing female vocals we can
generalize these features into one
feature called female let's create a
two-dimensional list of synonyms that we
find in our data then we can merge them
and drop all the other columns except
for the first one for each synonym
within our matrix let's get the max
values from each of the features and add
them all to our first synonym in our
data frame object which will effectively
merge the values into one column then
we'll drop the rest of the features from
the data frame now we've got more
generalized features next for the
reduction step what can we remove from
this data that's not necessary
everything seems pretty solid so let's
go ahead and put it into training
validation and testing sets that we can
feed into our model notice how in this
example I'm not thinking about which
features to use and which not to do
before deep learning we have to pick the
right features to use to feed our model
but deep neural net learn high-level
features from whatever features we give
it it decides for itself what is
relevant to the problem from what data
architecture engineering is the new
feature engineering the second dataset
we'll use is a collection of network
connections either labeled normal or ad
the abnormal connections are intruders
trying to break in we want to be able to
classify a connection given the set of
other features when we look at this data
it seems pretty dense no missing values
nothing really jumps out as an outlier
so let's get the cleaning step and move
right on to transforming it our
numerical features are all operating on
different scales so we should normalize
them to ensure each feature is treated
equally by our model after storing our
data into a panda's data frame so I
can't learn has a handy sub module
called standard scalar which will import
then initialize after that we're ready
to move on to our reduction step we got
a lot of features and there are probably
a lot that are highly correlated we
could use a technique called
dimensionality reduction to reduce the
number of features we have this will
also let us visualize our data in 2d or
3d space this doesn't mean that our
model will be more accurate necessarily
to that our data is easier to read one
method of doing this is called PCA which
stands for Porsche Club of America way
strong definition principal component
analysis a davis got so many features of
squash them into three like little
creature first so normalized then our
correlation matrix size colliding
sectors and values out of the i've sort
them how many dimensions do I want I'll
select that many items up front make a
projection matrix Rama and you could
discern my data three dimensional
problem two like a judges so let me
summarize this process again let's say
we had four features and we wanted to
reduce them to just to using PTA there
are five steps to this the first is to
normalize of data once we have it for in
a variable then we want to compute a
covariance matrix to construct this we
compute the covariance between each
feature with every other feature so we
subtract the mean from the feature
matrix calculate the transpose and
multiply it by the feature matrix minus
the Meeks then we take that whole value
and divide it by the number of features
minus one this gives us our covariance
matrix
next we'll perform eigen decomposition
on it to get the eigen vectors and eigen
value eigen isn't such a fun word to say
I can vectors are the principal
component of a data set they give us the
directions along
our transformation acts the eigenvalues
give us the magnitude of each will short
both in descending order then create a
matrix out of them or use this matrix to
transform our original feature matrix
via the dot product we can then plot our
data in 2d space and use these principal
components to replace our many features
let's look at one more data set this
time for airline prices for flights
between New York and Paris we want to
predict the ticket price from just the
departure date we've got departure and
arrival date airports and flight prices
up to 120 days before departure notice
how we've got quite a few missing values
in our data so for our cleaning step we
could remove these values build them
with zeros fill them with the average
price across all the days or try to
predict them using a learning algorithm
let's go ahead and calculate the average
price for each row across all days using
the mean function and then we'll iterate
through the data and if it's no we'll
replace it with the mean price then we
can smooth our data that means finding
outliers in it that we can remove define
these we could run clustering or
regression algorithms on certain values
to find the outliers and then remove
them or just remove them by eye since
our data set is small a pseudo ladder no
need to reduce our data this seems like
a good set let's break it down there are
three steps to pre-processing a data set
cleaning transformation and reduction
the according learns the relevant
features from our data so architectural
engineering is the new feature
engineering and principal component
analysis is a popular dimensionality
reduction technique that can be
implemented with scikit-learn
the winner of the coding challenge from
the last video is Charles David blot
Charles David used just numpy to build a
three layer neural nets capable of
predicting an earthquake and he used a
random search strategy to find the
optimal hyperplane burrs for his model
wizard of the week and the runner-up is
city jack growth he used ten circles for
his prediction using just three inputs
the coding challenge for this video is
to use a dating data set to predict if
someone gets a match based on their
personality traits
details are in the readme poster github
link in the comments and all out the
winner next video please subscribe if
you want to see more videos like this
check out this related video and for now
I got a predictive roses really smell
like poo poo poo so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>