<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Pong Neural Network (LIVE) | Coder Coacher - Coaching Coders</title><meta content="Pong Neural Network (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Pong Neural Network (LIVE)</b></h2><h5 class="post__date">2016-11-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hqf__FlRlzg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello hello world it's Suraj welcome to
this third live session that I've ever
done I'm excited to be here
I'm going to start off with a Q&amp;amp;A
section and this time it's going to be a
little more structured so I'm going to
start off with a five-minute Q&amp;amp;A section
let me mute myself here we go I just
muted myself okay so I just made it
myself and we're going to start so v
minik you away and then we're just going
to go to concepts coding and then
summary and then a less Q&amp;amp;A at the end
okay so let's go starting now
this is not beer pong I know it should
be that would be cool maybe in another
video thanks for liking my hair I work
hard at it I dye it actually this is
this is dyed it's dyed silver um hi
everybody hi Bartos hi ivy rube
tensorflow we're going to be using
tensorflow
in this video absolutely I have tried
implementing a generative adversarial
Network those are amazing and they're
going to do good things for generative
models in general ah do I do any
freelance work yeah I mean sometimes
clients approached me to make videos for
them I'm trying to just do this
full-time like I don't want to like you
can't control me you know what I'm
saying like I'm just gonna I just want
to do the YouTube channel full-time the
self-driving car team is the only one
that I'm like okay I'll do something for
them hi
what is up Dion and mish I'm going to
show us how to build recurrent neural
network in tensorflow next time what are
the dependencies go to the top of the
chat screen I talked about them there
thanks Nicole oh hi from Argentina I
want to visit Argentina and learn salsa
no I'm sorry oh my god I meant a tango
that's what I meant
alright alright so here we go
more minutes yes I'm Indian American yes
I'll never get it you're right I'll
never get your nickname will you be
using q-learning good call Lord that's
exactly what I'll be using what is pong
neural network
okay so pong neural network is we're
going to build calm from scratch the
game you know the two paddles and the
ball in the middle and we're going to
train a neural network to get better at
it over time hi Antonio Brazil sounds
cool
uh then Vignesh I'm good how are you a
good PDF tutorial on CN NS for beginners
wait till later I'll send you a link
I'll send you a link I'm the best
massively online course for total
beginners in machine learning my channel
just implement all the code in each of
the videos and if you do that you're
going to get good at no time start from
the very beginning of my videos um let's
see game on game on exactly hi cush I am
using pylon Python 3 and can you do this
using Java yes you could do this in Java
have I tried implementing 2d models
restricted Boltzmann machines I've done
videos on were tricking both machines
Microsoft AI or Google ai Google ai of
course I mean Microsoft did hire 1500
person AI team recently but like I mean
you know we'll see how that goes
ok so that was it 5 minutes is over so
now we're just going to get started ok
so now the next part is me explaining
the concept ok so what we're going to do
what we're going to do is build a deep Q
Network to read in pixel data from the
game upon you guys know the game of pong
right to paddles and there's a ball in
the middle right and it's just you're
just trying to get the ball to go past
the paddle that's it and so we're going
to have to we're going to have one
Iwilei on ok this is an evil eye that's
the guy we want to beat it's just going
to be really smart and it's going to
know everything right from the start and
then there's going to be us and we're
going to be Super Dome at first ok we're
be super dumb but through trial and
error but through trial and error we're
going to get better over time so it's
going to it's going to notice where the
ball is going and if it's losing it's
going to say okay I need to get better
so it's going to be using something
called deep Q learning okay deep Q
learning and guess who else did this
deep mind deep mind did this all it does
is it reads in the pixel data it doesn't
know anything about the game it reads in
pixel data and the score that's it those
are the two inputs to the DQ Network it
reads in pixel data and the score and
based on those two things
it'll get better over time reinforcement
learning trial and error better over
time okay so and we call this the agent
environment loop the agent environment
loop looking at the environment the
agent sees if there's a reward or not
and gets better and better over time
okay that's what we're going to do and
Charles I had coffee but not enough but
anyway well we'll see okay here we go
okay so that's the explanation of the
concept that's a two minute explanation
and now we're going to get started with
the code okay we've got a lot of code to
go over because we are building this
pong game from scratch we are building
it from scratch and then we're building
the reinforcement learning algorithm
using tensor flow so there's a lot of
code to go over that's about 300 lines I
want to say so let's just get started
okay here we go never enough coffee
exactly so all right here we go I'm
going to screen share and we're just
going to get started
because we have a lot of code to cover
let's do this
all right screen sharing time screen
share holy all done
oh my god
moving this put this right here so I can
see it
all right screen section exactly exactly
I'm going to make this really big that's
text really big okay I know you guys
like that big text boom boom boom boom
boom getting bigger and bigger buh buh
buh
no this isn't a blind text I'm writing
this in sublime text is this big enough
how's that is this big enough I'll wait
for a reply and then I'll keep going is
this big Oh bigger
okay how about boom boom boom boom okay
here we go all right here we go here we
go here we go that's as big as we're
getting okay so the first thing we want
to do is build upon we're going to build
a palm class okay we're going to build
palm from scratch all right here we go
ah no text is big like my mom okay here
we go so we're going to import pie game
first this is going to help us make a
graphical user interface games in Python
pie game is a great library for making
any kind of 2d game in Python okay
whether it's pong whether it's snake
whether it's you know a side scroller
adventure game like even Super Mario you
could build it in PI game I love it
check it out read the docs alright so
this is how we're going to build pong
the next dependency is going to be
random all right
pi game and random and I'm going to
upload a copy I'm going to save a copy
of this video and save it to my channel
so check it out later if you if you have
to go okay and so the random is going to
help us define which direction our ball
is going to go in okay all right so
let's start off by defining some of our
variables define variables for a game
okay the first one is going to be the
framerate okay so let's just say 60
that's going to be the the framerate for
how
fast our game is going to move next we
want to have the size of our window our
game window okay so that's going to be
the window width and which is going to
be 400 pixels and then window height
which is going to be 400 as well so it's
going to be a square window same width
same height all right the next step is
to define the size of our paddle okay so
that the paddle is you know the stick
that the ball hits okay so paddle with
and they're going to be the same for
both us and the evil AI they're going to
be the same size so we're just going to
have to define it once well say 10
pixels and then we'll define the height
at 60 because the height will be it'll
be longer than wild right okay so that's
the size of our paddle and now we want
the size of our ball size of our ball
okay so the size of our ball will be
let's say ball with and then all height
it's going to be 10 all right so that's
the size of our ball and now we want the
speed of our paddle and ball okay so how
fast do we want our paddle involved to
move now the size of our ball because
our ball is actually not a circle it's a
rectangle it's a very small rectangle
all right yeah please ball jokes go in
the comments keep them up I love it I
love it when you have big balls here we
go so the speed of our paddle in our box
we want to paddle speed going to be
let's take two and our ball X speed
which is the X direction of our bottle
under the our ball Y speed which is
going to be 2 and so X&amp;amp;Y speed we have
two different speeds for our ball what
which direction X&amp;amp;Y right there's an XY
coordinate plane
there's two directions in pygame right
in our window we're having a ball with
these ball jokes aren't we I love it
that was clever to top 1% okay so now we
want to define our color spectrum so
we're going to have our RGB colors or a
paddle and ball paddle and ball we're
going to have let me just go up
what is we're going to we're going to
use white which is going to be what is
the color for white cue 55 255 and 255
that's going to be the color of our
paddle in our ball and now we're going
to define our black which is the
background for our game zero zero zero
that's it okay those are our colors now
let's initialize our screen initialize
we're going to randomize or let me we
could randomize our speed I mean it
doesn't really that I mean we don't
wanna get that complex right now okay so
let's just initialize our screen so
we're going to say screen is going to be
so we're going to use a piping module to
initialize our screen if we're going to
use the set mode method of display to
set the size and we're going to use our
variables that we've created our width
and our height to define this screen
okay that's our window width and our
window height okay great now that we've
initialized our screen let's draw our
ball okay so now we can write our first
function here called draw ball we're
going to draw our ball it's going to be
a small rectangle okay and if the two
parameters are going to be the x
position in the y position that we want
to develop that we want to draw our ball
in okay so our ball is we're going to
define our ball variable and we're going
to use the rect function of Pi game to
create that rectangle that ball
rectangle okay and so let me make sure I
didn't misspell this
and I'm going to say let's initialize
our rectangle with our screen as the
parameter and then sorry not our screen
we're going to use our exposition or Y
position that we input as parameters
where we want to start off with and then
the width or with up for Oh
Siraj for president I I would I would be
down actually I would be down that would
be awesome
I would terrify everything I would just
make everything run on a code it would
be amazing if I was president okay
so future plans future plans let's make
this YouTube thing work first ball width
and then ball height um you guys are so
awesome man I really I love you guys
seriously like I had my ups and downs in
life like I've been pretty filling
pretty down lately but like just the
fact that like you know I've got you
guys it makes me feel better so so thank
you okay so here we go make America
learning again I know I've got to focus
I know I just the confidence or so
amazing okay anyway so that's our ball
and now we're going to draw our ball
so pygame draw okay so we're going to
draw our ball using the draw function
we've already defined the rectangle and
we're going to use our screen or color
and then our ball that we just define to
define our ball okay that's it that's
how we draw our ball okay now we're
going to thank you now we're going to
draw our first paddle on our first
paddle is us okay that's us slash are at
us and we're going to learn over time
okay so our first paddle we're going to
draw a paddle one and we're going to use
the first Y position as our parameter
okay so we're going to do the same thing
we're going to create our paddle using
the PI games rect myth method and then
we're going to use the we're going to
use our paddle buffer as the first
parameter and then our Y position and
then our width and height okay that's so
that's going to be how big it's going to
be and where it
want you start and the reason I use the
buffer is to show that it's not good so
that it doesn't hit the edge of the
screen all right ba-ba-ba-ba-bom okay so
that's our ball and now we're going to
draw it just like before except I game
draw um
not rekt and it's going to be white as
well screen white paddle one all right
oh okay so that's our first paddle all
right now we're going to do the same
thing for our next paddle and draw a
paddle to that's going to be our Iwilei
odd that's the guy we want to beat he's
gonna be really good we want to beat
this guy
alright he's on he's on some okay
see I never know if I can or cannot
curse or if I should so I just do it
anyway because like whatever you know
I'm saying I have nothing to lose okay
so paddle to Y position alright so uh
okay
it's got a focus here I'm going to
answer you guys but I got to focus
so now paddle two is going to be we're
going to do the same thing we're going
to draw it using the rect function and
then we're going to say window width -
paddle buffer so it doesn't hit the edge
of the screen and then we're going to
say uh - paddle width right - paddle
width and then paddle to Y position and
then paddle it amenda this the size of
it paddle with the paddle height okay I
got to keep going here alright and then
we're going to drive by game dot draw
direct and then the screen of our color
and then our paddle okay I should use it
Linux machine i I agree I grew I agree I
will eventually I
I will okay so now here comes the hard
function okay okay here comes the hard
function we're going to update this ball
okay this is the function where we
update the position of the ball so we're
going to say update ball and we're going
to take in parameters the parameters are
going to be at the position of our both
of our paddles our ball and the
direction that the ball is going
those are going to be our perimeter so
we're going to take those women say
paddle one pipe is Y position paddle two
Y position and then fall x position
we're going to say the ball wide
position those are that that's the
position of our our ball position the
window width is spelled wrong right I'm
going to line 40 let's see where is it
window doo doo doo there you go thank
you appreciate it
that's pilgrim ball Y position ball egg
position and line for tu caps are thank
you thank you appreciate appreciate it
boom boom all right so that's the
position of our ball and then we want a
direction of our ball so the balls
Direction is going to be the ball x
direction and then the ball Y direction
okay so those are our parameters and now
we can just go ahead and get started
9:32 instead of rect typo fix 36 capital
also capital typo fixed there at that
okay so now okay we have got to get get
through with it's okay we're going to
update the x and y position update x and
y position so the balls so we want
update the x and y position of our ball
okay so that's that's how we're updating
the speed of our ball and where it is so
ball x position it's going to we're
going to take the original position and
we're going to add the direction that
it's going in and we're going to
multiply it by the speed of the ball
okay and so that's going to update the
position of our ball okay depending on
how fast it's going and where it's going
all right
so ball Y position
Plus ball i direction x ball y speed
okay alright there we go with that and
we're going to say the score it's going
to be zero it's going to start off with
note score I mean the score is going to
be zero and we're going to update the
score and we're going to return the
score as well depending on I'll try not
to explain every step balls
exactly so now we're going to check for
a collision okay so this is going to
going to check for a collision and if
the ball hits the left side then our
learning agent will switch the direction
or the ball is going to the ball is
going to switch the direction okay so
it's going to be a bunch of if
statements here so we're going to say if
ball x position is less than the paddle
buffer plus paddle width and the ball Y
position plus the ball height is it's
greater than or equal to the paddles
position and the balls Y position - the
ball height it's less than or equal to
the paddle one position on Y position
plus the paddle height all right so this
says okay so I'm going to write down
what this means this means we're going
to check for a collision and if the ball
it's the left side if the ball goes
it'll hit the left side then we then
switch direction so we're going to say a
ball X direction it's one okay else else
if if it doesn't so
then the balls position it's less than
or equal to zero then we're going to
have a negative score so we're going to
say we're going to change the direction
of the ball and then we're going to say
we're going to make the score equal to
negative one okay and we're going to
return and we want to return whatever
we've just calculated so we're going to
return the score we're going to return
the paddles position for both of them
we're going to return the balls position
we're going to fold the x and y position
and we're going to return the direction
of the ball we want to return that's the
x and y direction of a ball oh all right
I'm a little bit off this is crazy
okay there's a lot of variables going on
right now okay um and then all right
can't see code I know right it's okay
just we're not going to be able to see
all the code because we have a we have
to switch between like how big the text
is and like if there's a trade-off
between how big the text is and how
visible it is thank you it's the score
is negative one thank you
all right now we want to check if it
hits the other side so if ball x
position is greater than or equal to the
window with window with - paddle with
and it's paddle buffer and the ball Y
position plus the ball height greater
than or equal to paddle to Y position
and ball Y position - all ba
ah ball - ball height is greater than
equal to paddle to Y position plus
paddle height okay thank God that we are
done with those two statements okay
right okay so we did that and now we're
going to switch the direction of the
ball
so if it so this basically means if it
hits the other side then we want to
switch directions again right so now
we're accounting for both sides that it
hits so ball x-direction equals negative
one all right and then else if it
doesn't hit it so else if the ball x
position it's greater than equal to the
window width - ball with then we want
the positive score so the Balder
x-direction equals negative one and it
scores one and now we're going to return
that same right that we returned up
here the same thing and the score is
here and now I'm going to look at the
comments for second boom hi hi what am I
saying
there's too much like this salty ball
exposition all right here we go
so now
cool so now okay so let me just explain
what I just wrote and let me let me make
it smaller for a second hold on we're
checking for a collision here okay so
we're saying if the ball hits the left
side then switch directions if the ball
hits the right side then switch
directions again so it's basically the
logic is you know if it hits one of the
sides then we want to switch directions
for the ball all right that's it that's
all all right
break the line break the line break the
line break the line
lots of line breaks here okay there we
go all right and I'll keep the font size
and I'll break this line and I'll break
that line and I'll just fit it all in
here all right boom boom boom boom boom
boom boom all right okay so now um oh
and there's one more thing we need to
account for what if the ball hits the
top what if the ball hits the top we
don't know what to do then right we have
to account for that so if the balls Y
position is to up Center equal to zero
then we want to make that Y position
equal zero so then we want to change the
direction of it and we want to say else
if ball Y position less than or equal to
the window height minus the ball height
all right so if it hits the bottom then
we want to change position as well so
ball Y position equals window height
minus ball height okay and so then we
want to make the Y Direction ball
y-direction equals negative one and
eventually we return the score we return
this quarter this is to player pong
exactly but one of them is an AI an evil
AI that already really good and one of
them gets an AI that we want to make
better okay
oh I'm going to fix the colons yes thank
you
boom um anyway so now okay that's it for
that method thankfully we are done with
that method oh my god that was so
intense okay so now we need we have two
more uh start two more functions to do
okay so let's do it
yeah it's AI versus AI is it's a bad AI
and then a good AI that's us that's us
we're going to get better over time
it's already going to be good from the
start um alright so here we go
let's update paddle one going to update
the paddle position if we move up so if
the ball moves up so that's going to be
us so if the action is one so it did
anyone where we're moving like which
direction we're moving in we want to
update the position of our paddle right
so we can go up or down right so the
action is just an array of where it's
going right so up or down alright so
paddle 1 Y position equals so we're
going to update the position of our
paddle and we're going to subtract the
speed of it alright and we're going to
add this : so now if it moves up so now
it's if it moves down
if action is two equals one so if we try
to move down we want to update the
position to move the paddle down okay
paddle on Y position close paddle speed
okay don't let it move off the screen
okay so if the paddle so if it if it
tries to go above where we want it to go
then we're so less than zero then we're
going to update that let me see where we
are right now
okay cool little / low so paddle one by
position equals zero and then I'm going
to say paddle on Y positions greater
than window height - paddle height okay
so now we have taken into account every
possible scenario that this this our
paddle could move in okay all right um
thanks acrylic love back from San
Francisco so we want to do the same
thing for paddle to right so I'm just
going to copy this method or function
we're going to do the same thing but for
paddle to right and the differences
we're going to say ball Y position and
this is going to be pedal - battle - OH
- all right now we can define our pong
class the game of palm right we aren't
moving really fast here where we haven't
even gone to raif let's define our pawn
class def in it self 54 colon colon at
64 where are you talking about no I
didn't
okay so now we want to define our game
okay so the sir game class this is now
we we have our functions we want to
define our pong game okay so we're going
to start out by saying well by saying
random number for the initial direction
of the ball where do we want our ball to
go we'll define a random number at using
the random grant int function is a
pretty long program this is a pretty
long program I'm thinking like let's
just keep going we just we just need to
keep going here so random it's we're
going to start from zero to nine so it's
going to be a random number between zero
and nine okay and now we're going to
keep the score in order to keep the
score we're going to define a tally
variable self Akali equals zero now we
want to initialize the positions of our
paddle positions of power path alright
so now we're going to say self paddle
one position is window height divided by
two - paddle height divided by two and
self dot paddle is window heights so
it's going to start up in the middle
basically that's why we're dividing like
two we want it to start off in the
middle window height divided by two -
paddle height divided by two
all right so now we want to define the
ball direction ball direction definition
and we're going to say self dot ball
x-direction it's one self duck ball
y-direction is going to be 1 and then
the starting point for our ball the
starting point will be the self duck
ball x position is going to be the
window height divided by two and then so
it's going to start off in the middle as
well right so the ball with / - all
right now
we're going to say two more functions
okay we're going to define the get
present frame all right these are two
more functions and we're done call in 94
and 109 I'll remember that okay so so
now we want to get the present frame so
that means the frame that we are
currently at why are we defining this
function because we want to feed our
reinforcement learning algorithm the
pixels right so we need in order to get
the pixels we need to get the pixels
from the game all right
so four okay so four for each frame we
need to call the event queue all right
so pi game event pump like this is going
to be like if we need to repaint our
window or anything and we're going to
we're going to repaint if we need to run
anything so that's why we have pump so
now we want to make our background black
we want to make our background black so
we're going to run our fill function
okay so screen dot fill and we're going
to say black so that's what's going to
be our background all right I know it
might it might just explode when I run
it but that's okay draw now we're going
to draw our paddles so we're going to
say drop so we're going to call the
function that we just made right we're
going to draw our paddles and we're
going to use our Y position first and
then we're going to draw the next paddle
paddle too and we're going to say self
dot paddle to Y position now that we've
drawn our paddles let's draw our ball
and guess what we we made a draw a ball
function let's call it all right this is
everything we've just done and we're
going to say it's going to be at that x
position we're going to intial we're
going to initialize it at that x
position and we're going to use that Y
position as well okay
now we've grown our ball we've gone our
paddles to draw our ball lead on our
screen and now we want to take all of
those pixels from the entire game and
we're going to return that and how are
we going to do that well let's define a
let's define a variable called let's say
we want to get pit pixels so let's
define
our variable called image data okay
we're going to define a variable called
image data and image data is going to
use the use the array 3d function of Pi
game to get the pixels from the screen
so we're going to say surf array dot
array 3d all right and we're going to
say pi game display dot get surface boom
so that's going to get all of our pixels
now we want to update the window we're
going to say pi game display dot flip
alright Leonardo I will definitely take
that into account we're going to update
our window and now we want to return the
surface data return the screen data all
right so we're going to return image
data um exactly okay so that's that's
that's going to return the screen data
and we want one more okay and we're
going to want one more function to get
the next frame so it's going to be def
get next frame um and we want to use
self and we want to use action okay and
the reason that self in action are going
to be our two parameter action we're
going to use action because the action
is the like what to what direction we
want to move in alright and we're going
to define this later when we create our
reinforcement learning algorithm so for
this we want to do the same we're going
to start out the same way we're going to
say PI game event pump to call the event
queue and then we're going to fill the
background screen in just like we did
before because for the next frame right
thank you um and sort of your just wait
we're almost done with it well give in
four minutes we'll be done with this and
we'll get to the reinforcement stuff
that's going to be super interesting
okay just just hold tight so now we're
going to update our paddle and we're
going to say self dot paddle one wide
position and then we're going to update
our paddle using the paddle method we
just
and we're going to say we're going to
say the action and then the action is
going to be fed back to us from our
reinforcement learning algorithm that we
that we already do all right action is
going to be hi Victor thank you
the action and then the self dot paddle
1 y position all right and then we want
to say self paddle 1 Y position we
filled our screen we want to draw our
paddle 1 self dot paddle 1 Y position
and then we want to update the position
of it self dot paddle to Y position
we're going to update the position of
the evil AI right by 2 equals update
paddle to boom and then self paddle your
Y position softball y position all right
so ok so that's going to update both of
our paddles over time and then we want
to draw the ball so draw a ball self dot
ball x position
self duck ball Y position now we want to
get the surface data again and so we're
going to be the same thing that we did
before we're going to get the surface
data I can just go ahead and copy and
paste that and then a picked a window so
PI game flip just like we did before and
then return it and that's it ok so that
we're going to say PI game oh and we
want to record our cally right self
cally is going to be whatever we are
plus the score that we've that we've had
ok and now I know peppy PEP 8 is going
to kill me later so we want to return
the score and the image data those are
our two things okay
all right these are the two
inputs that we are going to feed into
our reinforcement learning algorithm the
pixels on the screen and the score okay
so that's that
class alright and so now now we have
this pong class let's do our
reinforcement learning algorithm in
tensorflow
ok we're going to do our reinforcement
learning algorithm in tensorflow
all right so let's just start out by the
importing tensorflow
ok and I'm Adi hi Fernando okay we're
going to do this in tensor flow and
we're going to import so enter flow is
to do machine learning cb2 is OpenCV and
that's going to help us a format our
pixel data so it's it's good it so it's
so that it's better for reading into our
tensor flow graph we're going to import
the pong class that we've already
created and things are up and we're
going to import numpy as NP which is
going to be help us with math we're
going to import random we're just going
to hell just we're going to initialize
some random things later and then we're
going to import the collections library
and from the collections library we're
going to import DQ ok DQ is going to be
if it's a data structure the EQ is a
queue data structure and that's what
we're going to store our experience
replay ok we're going to we're going to
we're going to store some memory in this
Q function and I'm going to explain why
we're going to use that so let's go
ahead and start off by defining our
hyper parameters see I'm just like much
more interested when we get right into
the machine learning stuff because you
know I'm saying like that's that's where
it's at ok so these are our hyper
parameters we're going to start off with
our actions variable in our actions
variable R up or stay ok let me see
alright so now we want to define our
learning rate learning rate learning
rate is going to be gamma we're going to
set it as 0.99 now we want to define our
Epsilon
so I'll talk about what our initial um
is um we're going to say initial
epsilon is going to be 1.0 and then our
final epsilon at where we want to be
after training is going to be 0.05 so
what is this this is a when we update
our gradient or training over time we
want to make sure our epsilon reaches
0.05 okay an else and we're going to
we're going to apply this later how many
frames do we want do we want to anneal
our Epsilon okay so how many frames
we're going to define those frames we're
going to say explore frames it's going
to be 50,000 frames and then our
observed frames are going to be sorry
these are 50,000 this is 500,000 okay um
so now thanks Daniel
what kind of neural net is this great
question let's just I'm going to define
that let me let me say that in a second
let me just define two more hyper
parameters our replay memory which I'm
going to talk about and what kind of
neural network this is so replay memory
is going to be 50,000 and then our batch
size our batch size is going to be 100
all right so batch that so how many
times we want to Train all right and now
we want to create our TF graph okay what
kind of neural network is this this is a
convolutional neural network a
convolutional neural network is a type
of neural network that reads an image
data and we're going to make a five
layer convolutional neural network thank
you for that
in tensor flow and we're going to feed
the pixel data and the score into this
convolutional neural network and that's
what's going to help us do deep
reinforcement learning okay and and deep
mind did this they did this for that all
those Atari games and no matter what
hitori game they gave it it was able to
just look at those pixels and become
amazing over time all right exactly Lin
Wang exactly so now let's create our
tensor flow graph let me just go down a
little bit and so we're going to take
okay so create our graph create graph
and let's get started so now we wanted
to create our first convolutional layer
our first combinational layer is going
to convolution
lair and our bias vector that's what
we're going to first create so I'm going
to define that first I'm going to say
first convolutional layer using Petra
flow variables and we're going to define
the size of it okay and we're going to
say these sizes can be changed to be
whatever numbers but I'm going to have
these be these numbers because so okay
we're defining our first convolutional
layer okay and we're using the TF zeroes
a function to define those layers
what TF zeroes does is it creates a rate
of an empty tensor full of zeros it's an
empty tensor which is good for us
because our convolutional network is
going to start off empty we're going to
fill it with data over time and we want
to define the size of it okay we want to
define the size of it so we're going to
say B convolutional 1tf dot and so our
bias vector will help us how much we
want like so where we want our data to
flow like in what direction and so our
bias will help us in defining like what
part of the network we want our data to
flow in okay so we're going to say PF
zeroes and we'll define that as 32
because it's 32 bits
so it's 8x8 with 32 bits and so that's
our first layer so now our second layer
um and wow there's 231 people watching
this that's awesome
Wow I'm so happy like this is awesome
let's get more interest in machine
learning guys this is you guys are gonna
love it when I got okay you guys you
guys get me pumped okay I love it you
guys get me pumped all right so now for
a second convolutional layer we want to
okay how long is this going to take this
is going to take about I want to be done
within 15 minutes I want to be done in
15 minutes okay I want to have the
entire thing done
you see how long it takes so let's
define that layer and we'll define our
next convolutional layer TF dot variable
T F dot 0 s and for 432 64b
convolutional - equals P f dot variable
before and then our third layer just got
to keep defining those layers bum bum
bum three equals t FF variable D F dot
zeros okay we have a lot of explaining
to do and I'm going to explain these
magic numbers in a second all right so
let me just write this out and we
are going to explain this stuff all
right all right all right
that's how we do it so T I've got zeros
and we want this to be 64 bits as well
okay that's a third fourth and so now
our fourth okay so now our fourth is
going to be WC 4 okay nope of you FC 4
is equal to TF duck variable and TF duck
variable I'm going to take 0 and it's
going to the side 784 and we're going to
use our actions as a parameter here I'm
going to play this in a second and then
our bias
let me just code out all these layers
equal to get them variable gift that
zeros again and then 784 I will upload
the source code and then last layer
okay lots are going to be W you see five
equals T f dot variable P f dot zeros
784 actions B FC 5t f dot variable so
you know it's just a very repetitive
thing when you're coding your layers
because it's just you know layer after
layer you could define each of them and
then we want our input okay so what
happened here let's see like all this
stuff you guys have said I okay so we
created five layers and each of them we
define a size an input size and and we
define an input size right oh thank you
currency there yeah I would I would be
in a TED Conference yeah
so we defined a size for each of these
layers line 42 is the wrong variable
what are you talking about oh it needs a
No oh not really our under 50 plus
people now wow you're right you wanted
63 oh my god this is crazy oh my god
this is crazy
okay here we go so we have five layers
to our convolutional neural network okay
and so now we got to keep going because
we are running out of time out of time
out of time okay analogies are great
here so yeah that's a good point that's
a good point
um so now we want to create our input
where's our data going to go we created
we created this five layered neural
network and where do we want our data to
go well we have to create an input right
a placeholder where the data feeds into
that Network right okay so let's let's
um create an input for our pixel data
we're going to feed our pixel data into
this input we're going to say TF naught
plus holder we're going to say float and
none 84 84 84 all right so that so this
this is where our this is where our data
is going to flow into okay so now we
want to now we want to do our actual
activation function which is going to be
the rectified linear unit every time
make pixels great again that was great
every time we feed data into this at
each layer we need to perform some kind
of computation when we need to do
something to that data
okay everybody keeps talking about line
42 so let me just quickly look at what
is happening here um ah B FC 4 all right
thank you
AFC 4 thank you thank you so much so at
every layer we are going to perform the
rectified linear unit activation
function okay it's going to take that
data and it's going to turn it into a
probability and so really is in use a
lot and oh you're right I only have 28
percent battery I need to make sure oh
my god I've really on a time line ok so
here we go so let's compute our riilu
activation function on 2d convolutions
given for the inputs and filter tensors
alright so we're going to do that we're
going to compute it every time using the
potential flows built-in riilu function
ok take our convolutional QD
convolutional neural network and um oh
man
ok all right
to the convolutional neural network and
then so we're almost done all right we
have we want we have six more lines of
code okay and then we remember we're
going to just we're going to get through
this ok so um no we have about 12 12
more lines and then we're good so we're
going to take our convolutional neural
network and let me just make sure where
I am okay so I need to calm down
I need to calm down everything is good
it's all good meditation mindfulness I
know my battery's going to die I'm going
to die everybody's going to die but
hopefully if we can put our minds into
computers and machines and merge with
them and make this thing you'll already
happen we won't die ok I just said that
so now you want to input that so we're
going to perform our convolutional
is there really more than 300 people in
here oh my god
it's cooking this is crazy hi everybody
welcome to becoming awesome with Suraj
so we're going to take strides so our
stride for how like the channels that we
want our data to flow true
what is it padding is valid so that's
just the so that's a variable for okay I
just I just have to cook it I like
literally just have to go so TF cup n
entry Luke t FN n actually I can just
literally just copy and paste this you
know I'm saying I just changed the
layers here I say complex or to
accomplish three and then convolutional
three and then I need to say I'm going
to take I would say tennis yards yeah
singularity tennis yours okay so so
what's happening here we're taking all
that all the data that's computed in
each layer and we're sending it to the
next layer okay um now we're going to
take that last convolutional layer
convolutional three and we're going to
say T F dot reshape it and we're going
to take the previous convolution layer
and we're going to say negative one
three one three six and then we're going
to say FC for I know I know I have to
fix that type of TF n and riilu and
we're going to do perform matrix
multiplication on that function to make
sure that that is working and we're
going to use our reshape okay and now
our last layer is going to take
everything we got from before and we're
going to perform matrix multiplication
on it and if we do that then we have our
output penser right FC all right FC five
is going to be our output tensor that's
going to be the result and we're going
to feed that back into the network okay
and so we want to return that we want to
return the input tensor and the output
tensor
okay so let's skip right to our main
method now okay
it'll go boom def mate in our main
method because we are
out of time in our main method we want
to start off by creating a session app
enter flow section in art enter flow
session we use TF interactive session
that's the method we use to initialize a
section and then we want to define our
input layer and our output our output
layer or I can't even think right now
our output layer okay and we're going to
what we just defined a method to do that
right that create graph method or what
is that what's called yet crea craft
create graph is going to create our
input and output layer for us and we're
going to feed those into our training
graph method which is going to take both
of those and it's going to run a bunch
of going to run our reinforcement
learning algorithm on and then if main
aim equals main all main okay okay so
literally 300 300 oh my god literally no
you guys are not just just as smart as
me you guys are like smarter than me
I've got people who are subscribed to me
or just smarter than me okay in a lot of
ways we all shine in different ways I'm
good at being in front of a camera you
guys are good at you know other things I
know there's a lot of hairs and we are
running out of time because I only
allocated an hour for this and this is
my third time live-streaming and we have
uh we're gonna have some errors here and
I do not have time to fix the errors
here but let me explain what I've done
okay let me just go over what I've done
I have okay what have I done here I've
defined pong from scratch I've defined a
bunch of variables for defining for for
what the game is going to look like
we're going to draw the ball we're going
to draw both of our paddles we're going
to update the position of our ball and
then I have 20% battery we want to
update our paddles like weather weather
going and then we're going to define a
class for our pong game we're going to
say okay get the present frame and then
get the next frame we're going to take
that pixel data and then we're going to
feed it into our convolutional neural
network
deep Q Network which is a convolutional
neural network it's a five layer
convolutional neural network and we
built these with tensorflow
okay we're going to input that using
this placeholder variable that's where
the data flows into the network okay and
once we do that we're going to apply the
riilu activation function and each layer
of that network and we're going to by
the end of it it's going to using our
agent environment loop in our training
graph method we're going to be able to
train our network to be better and
better over time okay so I don't have
time to fix all the errors right now so
what I'm going to do is I'm just going
to run up what I already have it so you
guys can see what this looks like okay
can you guys see this yeah you guys see
this so I'm not doing anything right I'm
not doing anything about 17% battery and
the thing is like my MacBook sometimes
dies out like 15% sometimes it dies it
like 17 so might just die right now so I
want to make sure I'm able to upload
this code or this video to my youtube
channel and it works so so see there's
the evil AI and then there's the RAI and
it's going to get better and better over
time right right now it's really bad but
if you give it like you know a couple
hours it's going to get better and
better over time
typical MacBooks right so that's how
it's going to look it's going to get
better and better over time and now I'm
going to stop screen sharing and I'm
going to answer two questions and then
I'm going to upload the code I'm going
to have it well-documented I'm going to
add the stuff that I wasn't able to get
to and I'm going to do other awesome
stuff for you guys so two questions
let's go um 330 people watching hi
Daniel hi we got a Daniel shipment we've
got to do something together okay um max
you know I I might visit FSF state send
me an email why the riilu activation
function because it's good for DQ
learning specifically um more live maybe
next week maybe next week no Daniel
don't worry about it hi Shan ROC I love
you too um well let's be
what is reinforcement learning we're
using the agent environment loop to get
better and better over time it's like
trial and error like if you give a dog a
treat and if it does something that's
that's that's reinforcement learning
you're reinforcing it using a reward
function okay and ah cool who painted my
hair I do it at a salon what's next
we're going to do more neural network
stuff more machine learning stuff and
we're going to do it uh that in a way
that's more accessible to more people I
might give a TED talk you've got to come
to me like I don't have time to like go
out and like try to find a TED talk
books for machine learning I would
recommend um deep learning yearning deep
learning yearning by Andrew um how long
have you been learning ml it's been how
long has it been now maybe like three
years blockchain stuff I used to do that
stuff you want to see blocked in stuff
go to the beginning of my channel I used
to talk about that a lot I wrote a book
on that called decentralized
applications one of the best-selling
software engineering books in the world
by the way um uh can it be done with
chaos yes absolutely using a Theano
back-end a book for python learn Python
the hard way why not use a charger I
don't have time and for now I've got to
go get my charger so thanks for watching
and I love you guys
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>