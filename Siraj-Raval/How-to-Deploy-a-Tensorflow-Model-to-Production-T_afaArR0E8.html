<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Deploy a Tensorflow Model to Production | Coder Coacher - Coaching Coders</title><meta content="How to Deploy a Tensorflow Model to Production - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Deploy a Tensorflow Model to Production</b></h2><h5 class="post__date">2017-05-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/T_afaArR0E8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and today we're
going to learn how to deploy a tensor
for a model in presumption now I've
never really talked about this before
incredibly I talked about how to write
models how to train models how to test
models but I've never talked about how
to actually deploy them in production so
that you can use them at scale and a
bunch of people can use them like
hundreds of thousands or even millions
of people now this is super important
because this is the way to have your app
be used by many people right whether it
be in a mobile client that you can call
the server or if you just want to make a
web app both cases this applies to both
cases okay so what I'm going to do is
I'm going to talk about the architecture
around this technology it's called
tensorflow serving is a separate library
from tensorflow
but you use it with tensorflow I'm going
to talk about how it works and then what
we're going to do is we're going to
deploy a simple web app that lets a user
upload photos and it's going to return a
classification of what's in that photo
via a JSON response okay so let's start
off with an explanation and then we're
going to go over the code and then we're
going to go through the step-by-step
process of what it means to upload a
train model to a server and then call it
from a client okay so that's what we're
going to do today the clone 8 Central
flow model in production ok so this is
what the basic architecture looks like I
mean I have several images of what the
architecture looks like but this is one
of them okay let me make sure that it's
big enough to see just like that cool
so here's what it looks like so first of
all let me talk a little bit about
tensorflow serving so tensorflow is the
open source machine learning library
that we all know and love and
tension-filled serving was built to
handle it on a server what do I mean by
handle I mean handling all the
infrastructure that goes into building a
model using it versioning different
versioning the model maintaining the
lifecycle dealing with which model you
want to use taking the output from one
model and making it the input for
another model it's actually pretty
complex I mean Google uses this in
production right so it's got to be
pretty complex Google has got like
billions of requests probably every
single day right so we know that this is
a
battle-tested approach now I know like
pi torch is super hot right now I'm like
in PI towards everybody's liking PI
torch but PI torch was meant for
research right 10 turbo was meant for
production and it's it's been battle
tested by Google so if Google can use it
we can use it okay so in terms of the
architecture this is kind of what it
looks like
there's a lot of different components
that we're going to go over but at a
high level
once you so this is all like just think
of this this these two green boxes as
the server this is what's happening on
the server server side and then this
blue box is the client and the client
could be you know a mobile app or
somebody's desktop via terminal a
variety of things but so what's
happening on the server well we've got
these models right they could be so
model v1 and v2 they could be the same
model but different versions of it like
newer version and an older version or
they could be two entirely separate
models it doesn't matter it considers
them both ways right and it you can you
can tell it like I wanted to have
multiple versions of one model or you
could say I want multiple models right
it depends on what you want to do but we
have a file system that's on the server
that stores these models and what
happens is the library will load up a
model okay and then it's going to see
which version of the model that we want
to use all these things we're going to
specify right well we're going to
specify programmatically and then once
it's got that latest version let's just
say we want to use the latest version of
that model then it's going to serve that
model and what do I mean by serve that
that model then becomes the front-facing
interface to which any client can
request via what's called a signature
and we'll talk about all this but it's
going to request it via a signature ID
and it's going to return the results of
that model in this case is going to be
classification so and so that's what
that's what the client does and you see
this term right here G RPC G RPC request
and G RPC responds anybody wondering
what the hell is this
so RPC is let me let me pull this up RPC
vs. HTTP this is a really good link for
this but basically you could have rests
right so rest is like the way that we
work with HTTP API so eight so okay so
HTTP is the protocol of the internet
right
how data flows across the web the most
popular way there's also like BitTorrent
and like other protocols but the most
popular way is HTTP right all websites
run on it and RPC is a protocol that can
that is a protocol that can run on top
of HTTP it's really not complicated at
all I know it's like all these acronyms
but it's actually not complicated like
so a rest is like the default way right
MVC web apps node apps you know flask
they all use rest and what is rest well
it's got these set of verbs and so the
only difference with well the main
difference with RPC and rest or that the
verbs defer so for rest you've got like
get post put patch delete whereas with
RPC you only have get and post and if
you want to delete or do something else
you to specify that inside of the
request so if you want to delete you
would post a delete and so one of the
parameters of the post would be like a
delete request that's it and then
there's like a bunch of like you know
semantics like beauty and design ability
concerns it doesn't really matter the
point is that you can run this over HTTP
via an API just like anything else
that's what RPC is and G RPC is a type
of RPC that's you know tweaked in a
certain way it's basically negligible we
don't have to we actually have to worry
about it's happening under the hood but
that's the high-level architecture of a
client a request
it's either requests a either gets or
posts some data and then the server is
going to return it and the server
contains the model so okay so that's the
super high level and then we've got this
I've got this Nvidia graphic but I
wanted to show what inference is because
tensor flows tensorflow serving is meant
for inference for imprints and inference
means managing models giving versioned
access via reference counted look-up
tables ie then HTTP interface via RPC so
we have an untrained network right and
then we train it we have some training
data that is a training stage and then
once it's done training we have a
trained model then we can test it on our
own data to make sure that its class
find correctly and when we're done with
that then we can perform imprints and
inference is we're applying the
capability of this model to some new
data sets that the consumer would
probably be using right so that's what
this is for it's a running inference on
our models that is for production use
okay so that's what it's for and it can
serve multiple models simultaneously
right so we'll talk about how it does
this but if this is great for a be
testing right let's say you've got one
trained model let's say one is like a
convolutional neural net with like
sixteen layers it's performing really
well but then you've got this other kind
of like experimental model that's like a
convolutional and recurrent that mixed
together and you want to test it out
test out which one works best you could
put them on this you could place them on
the server and then have your hat
tensorflow serving serve them via a
client and then they could try out
different versions of it or you could
even have different versions of the same
model and then test out different
versions of it so okay so the one bad
thing about this is that it's written in
C++ tangible serving is written in C++
so tensorflow
is written in C++ as well because when
you are making something
industrial-grade
like for hundreds of thousand millions
of requests C++ is still like the most
trusted way to do it but Petra clothes
got Python bindings and so does
Pentacles serving but not like enough so
we still have to deal with some C++ but
that's okay we will get through it okay
so now to get a little more specific
right so get a little more specific with
what this looks like programmatically we
have this is this is kind of like what
the architecture let me let me move this
over here so there are four major
components let me make this a little
smaller there are four major components
here okay so we've got servable x'
loaders sources and managers and this is
all happening within tensorflow serving
so remember this is separate from
tensorflow for the most part if you just
want to train a model and put it on a
server you don't actually have to deal
with any of this all you could all used
to worry about is creating a servable
and it's going to do all the rest for
you but if you want to
you know have more fine-grained control
it's good to know what these variables
are okay so if you don't care about this
just skip forward but if you care about
this if you want to deal with different
versions of your model if you want to
make sure that the lifecycle of your
model is handled well then you this is
an important part so we got let's start
with sources right so sources basically
contain servable so a source is
basically the Gateway think of it like a
gateway you know like in the tension
flow graph we have placeholders so it's
kind of a rough analogy as a gateway for
data into the server into the server
right the tensorflow serving server and
a source taken serving bowls that's more
or less it so the server bowls are the
real fun part right so if you see here
we have a source and it's it's pointing
upward to a loader and so it's what a
source contains is it contains server
bowls and a servable is the central
abstraction of penter flow right they
are the objects that clients use to
perform computation so I serve all can
contain one model it can contain
multiple models you can have one model
per server Bowl or you can have one
servable with like 20 different models
or you could have one servable with
different versions of the same model
right so there's a lot of ways that you
can architect a server ball but that's
like the central the central class these
are four classes serving bowls loaders
sources and managers but server bowls
are like the central class around which
everything works around so an analogy to
tensorflow for a servable would be like
the computation graph like TF graph
everything is there to support the TF
graph and so servable is kind of like
that for the server so we've got a
source and we feed it a servable
so we'll have some model and then we
like and then we will look at this
programmatically but basically we put a
model in a server bowl put it into a
source the source feeds it to a loader
and so this is the loader right here and
what the loader does is it just it's
basically a placeholder not not a
placeholder in terms of gateway but it's
a not a not a placeholder a better word
would be kind of like temporary storage
temporary storage for the manager to
then come and manage
that models lifecycle so a manager is
really cool so a manager manager handles
a full lifecycle observables that means
it decides when to load a model when to
serve it for a client when to unload it
and it listens to the sources and tracks
all the versions so a source I forgot to
say this part so a source actually
tracks different versions of the model
right so when you when you look when you
put a servable inside of a source then
it's got versions built in and what the
source does is it points to different
versions and then it tells the loader
when the loader loads it hey these are
the different versions here's where they
are in memory and then the manager can
come in and say right here this dynamic
manager it could say okay here's the
version I want and so that's what the
word aspired version means right once we
have our loader we feed the expired
version whatever we want and we specify
that in the request to the manager and
the manager is going to be like okay I
want this version okay and so and so
once the manager knows what version of
the model to syrup it will serve it the
observable handle and that's that's how
it goes
right and so this so you have you can
have streams of serving bowls and what
do I mean by streams it's basically like
get you know when you like commit
constantly you have like new versions
it's just like that for a model so
basically this is your way of being able
to deploy a model to production and then
continue working on it locally and if
you have some new version of it you can
then just upload that new version to the
to the server and then it's going to
save the states of the old version of
the model and also serve the new version
and sometimes you might want to revert
back to the old version and you can do
so via its signature and we'll talk
about that but it's basically a two-step
process we're about to go into the code
but it's basically a two-step process a
source creates so we put a servable in a
source the source creates a loader and
the loaders are sent as expired versions
to the manager which then loads them and
serve them to client requests okay so
that I mean it's a lot to take in all at
once right so what we should do is we
should look at what it means to create a
model right so or create a server bowl
right so let's let's just like go over
this really quickly right so
what this is I mean okay
okay so check this out what this code is
if it's like 160 lines we're not going
to go over all of it we're just going to
go over like some of the basic bits here
so we can understand how these four
classes will interact with our model
this is really simple code all right all
it does is it trains an MN is T model to
recognize you know it's to recognize
handwritten character digits right if
some characters you know images one
through nine and they've got labels with
associated labels one through nine and
we want to learn the mapping between the
labels and the images right and so once
that once our model is trained on this
data then if we feed it some random
image like a you know an image of number
three it can say oh that's a three
because it earned the mapping so let's
go over what this looks like we've got
our imports right this is tensorflow and
tensorflow serving we've got some
training Flags and then in our main
function you know we we basically
specify how much we want to train and
you know different training options but
you know very similar process to all
sorts of machine learning we read the we
first read the data thankfully
tensorflow hasn't read data sets
function built-in we establish our
session and then we format our data
that's so that it can be fit into our
placeholders then we build our model
right very simple model it looks like
it's a single layer Network right X Y W
B you know one set of weights and biases
and then we squash it with some with a
soft max and we're using a cross entropy
as our loss and gradient descent to
minimize the cross entropy and what
that's going to do is it's going to
build a perceptron that we can then
train so once we train this model we're
going to use a mini batch gradient
descent so every so we're going to train
it on batches of 50 training examples at
a time
and it's going to when it's done
training will pronounced done training
so that has nothing to do with
tensorflow serving that's all just like
tensorflow right this is just how you
would build and train a model now here's
the potential flow serving part right so
once we built this model locally we've
trained it now it's time for us to save
it right because we want to take this
model that we trained locally and we
want to actually we could train it
locally but it could also be on the
server
for all intensive purposes let's just
say that this is going to be on the
server okay so what we'll do is we'll
say we'll say okay so let's let's define
where we want to save it so we'll create
an export path like we want to say that
right here and once we do that then we
can say okay we've got you know we've
got to saved path for this model and
this can be on the server or on the
client right we because you might want
to Train on the server as well but the
point is that we train the model and now
we want to save it so then we'll create
a servable from our model so like we've
got this model and we want to create a
servable and that's what we do with this
method right here this saved model
builder and observable is a class right
it's a class and it's how we feed this
model to our tensor code serving main
server class function our main potential
serving class right so then it can do
all that manager loading sources all
that stuff but we save our model just
like this using the saved model builder
it's going to save a snapshot of it so
we could later it could later be loaded
for inference and it can we can save as
many versions as we want we said that we
wanted to do you know we define the
model version but we can create an
entire stream of these models like we
want you know 30 different versions but
to keep it simple we're just going to
save one version of the model it's good
it's going to be one servable and once
we do that then what it's was going to
do is the main serving file it is going
to create a source out of it and the
source is going to house state so there
could be multiple server rules multiple
versions and then it's going to create a
loader and it's going to use this
function with TF save model loader
download to load that source and then a
manager class is going to decide how to
handle its lifecycle when to serve it
when not to okay so all of this form is
going to be on in the main class but
what we've done done here is we've
created a servable so that we can then
load this up via the serving file the
main tensorflow serving file and so what
we're doing right now is so here's
here's another very important part
we are creating a signature map and so
the signature map is how we these are
parameters that we can embed into our
survival class so that our pentacle
servings are tangible serving main class
knows which model to load up so we're
going to basically give identities IDs
IDs are signatures to all of our
components we'll give it to our inputs
we'll give it to our outputs we'll give
it to our class itself and some build
info and so all of these this is
basically metadata right we're giving a
bunch of metadata to the server in the
form of a server both so it knows what
model it is and it also knows you know
what the IDs are for all this components
so that later on whether on the server
or on the client we can reference those
parts maybe we want to pull apart like
we want to see the output maybe we want
to pull the whole model maybe we want to
take the output from one model and feed
it to it as the input to a different
model
that's what signatures are for so we
define it as metadata and so that's why
we use server bowls instead of just like
straight up models by abstracting that a
little bit more we can add metadata to
it so that you know our manage both our
manager on the server can know what to
interact with as well as our client our
client to know which model and which
input which output to to manipulate okay
so and then we add them all into that
builder you have the add meta graphic
variables function and then save it
right and once we saved it then we can
call it from our main server file okay
and so I just wanted to go over that
here's like quickly show how those four
main classes would interact with a
simple trained model okay so so let's do
this let's build this okay and so let me
let me start out by saying like this is
this is a it's definitely doable it's
definitely doable
but it's not like some walk in the park
to run this I had to like try this out a
couple of times before dealing before
having it worked successfully but
there's a lot of like possible errors
that can happen due to of course
dependencies but if you and you know for
whatever reason it's that way but it's
going to take
work to get this to work let me just say
this all I'm trying to say but if you
put in the time then you can easily do
it okay so let's do it right now
and so some of these commands are going
to take a while so I'm going to like
edit forward through those commands
because they can take like 20 minutes or
30 minutes to load up all those
dependencies right so before we so
before we do anything let's set up our
development environment right so what we
want to do is we want to download this
tensorflow serving repo right and what
we're going to do is we're going to
create create a docker image for all of
those dependencies because it's got a
lot of dependencies so we don't have to
use docker but we will use docker
because it's easier so docker for those
of you who don't know is basically like
a super lightweight virtual machine so
what is the virtual machine have you
ever used parallels on like Linux or Mac
space and to run Windows so sometimes
you want to run run a game right so you
would download a virtual machine which
is basically an entire operating system
just to run that game because that game
has dependencies that our operating
system specific but sometimes we don't
want to have to download an entire
operating system just because we want to
run one app right so what would be
better if we had a lightweight virtual
machine that was super super small but
it contained all the dependencies we
need for the app that we're trying to
build and that's what docker is docker
basically contains all the dependencies
that we need so like let's say one app
needs Python 2 but another app needs
Python 3 and we and we we only have one
version of Python on our system we could
use docker to contain cells contain one
of those apps download Python to all the
other dependencies and then those
versions wouldn't affect any of the
versions on the rest of our system it's
all self-contained so it's super cool
actually been using docker a lot more
often and I will eventually uh but yeah
that's a little refresher or tutorial on
what docker is and here's some install
instructions it's in the it's in the
ipython notebook okay
so let's go ahead and download this baby
so the first step is to clone the repo
Oh hold on guru okay
sound effects let's first clone the repo
and we'll put that will use this
recursive flag because it's got a few
dependency repos that we will need and
so it's going to basically clone all of
those and it could take a while
okay and so once we have that we can
then CD into that serving directory okay
and so here we are we have tensorflow
serving so we've got that file locally
and so this file has a lot of
dependencies that we're going to need
right and so what we can do is we can
say so this this file has a doctor plot
or this repo has a has a docker file and
what the docker file does is it says hey
these are dependent these are the
dependencies that you're going to need
so let's just see what that docker file
looks like see so here's what the docker
file looks like it's saying you got to
get all these dependencies right you got
to then you know if you don't have tip
install that you got to get the G RPC
client for running those HTTP requests
you got to set up basil so we don't want
to have to do this ourselves right so
that's what this docker file is going to
do it's going to do all this create a
docker image out of it and then we can
run our docker image and then
self-contained
in that all of our dependencies are
going to be there and we can run our
code is normal right
by the way basil is Google's tool for
downloading dependencies very useful
very useful tool but yeah that's what
the doctor file looks like internally
and so once we have that we can build it
so let's build this it's going to create
a docker image with all the required
dependencies for us so let's go ahead
and do that boom
okay great successfully built and so now
we've done that now they can go ahead
and run the container locally so what
this is going to do is it's going to run
hard dock it's going to run this docker
image that we created using the docker
daemon so for the daemon however you say
it
daemon daemon I never know but you had a
dr. Damon I'm going to say Damon it's
not evil it's great so let's go ahead
and run it into dr. Damon and we're
going to specify the bill that we just
defined right we called it
tension-filled container and so now we
can run it okay error response that's
because I have docker already running so
let me quit docker and then rerun it
docker okay no time for that and then
see what's going on here
Dockers running okay
okay so now okay we are in the docker
container right we can see what's in
here
I'll make sure this is bigger boom okay
make that a lot bigger
okay so we've created our docker
container here it is and now that we're
in this container we've got all of our
we've initialized it using our docker
image our docker file and now we can
clone our repo into it now that we're
that now that were in here so we'll do
the same thing now that we're inside of
our image and this time we're cloning it
not to that docker file or cloning it
for the related files right the actual
Tenterfield surveying right we're in the
image and now we can download this files
so it's going to take a while
okay so then once it's downloaded via
get now we can CD into it let's see what
we got here we've got a CD into it and
then configure it ok and what that's
going to do is going to use Google's
basil build tool it's going to use
Google's basil build tool from inside
our container and then it's going to
download all those third-party
dependencies so we'll CD into serving
and then we're going to run the
configure let's see CD serving then
tensorflow tensorflow and then configure
it boom ok it's going to ask for specify
the location a bunch of things
do I want nkl support no no default
default default
I mean auto-ship then let me redo this
configure ok mkl support default default
default yes I want Google cloud platform
yes Hadoop don't care about the
experimental compiler verbs don't care
OpenCL don't care
CUDA normally yes but for all the times
of purposes for this tutorial I don't
care MPI no okay and then it's going to
do it ok so then while that runs the
dependencies that we got that we're
going to need are tend to flow serving
the library and the pre-trained
inception model right the inception
model is going to let me show you what
the inception model is by the way
basically it's a huge convolutional
neural network that won the 2014 image
net competition with state of the art
results they train it on hundreds of
thousands of images and this is like a
compressed view of it right so the
actual view is up here but then the
compressed view is up here got a bunch
of layers like like a hundred layers but
basically we don't have to train our
whole model right we could if we wanted
to but we're going to use transfer
learning we're going to use this pre
trained model for our own use case ok so
that means we don't have to deal with
the training part at least for this
example ok so then once we have that or
once we have our basil dependencies
built we can then test it out by sorry
once we have our
repo configured then we can build our
dependencies using basil right so with a
basil build and that's going to take 20
to 50 minutes it's going to take a while
to build those dependencies okay and
then we can run our model server right
so it's going to take a while to do a
basil build but so it's done configuring
and now we can do basil build and it's
going to take a while okay and so once
it's built we have our model server
built now we're going to move on to the
second dependency in the second
dependency is downloading inception
right so we're going to curl that into
this repo and it's going to pull it it's
going to download it just like that and
while it's downloading we can see well
what's the next step the next step is to
run it and the server locally is to just
run it right we want to run so so look
at this command here so while that
downloads we'll look at the command so
what we're going to do is once it's
downloaded we going to untie it we're
going to you know uncompress it and then
we're going to save it as a check point
and then once it's saved as a check
point we can then refer to it in our
tension flow model server like that's
the main file a potential flow serving
right that's that's the model that we
want to use so in the beginning I talked
about this custom MN is T model but for
the sake of this tutorial to keep it
short and simple wherever you this
pre-trained model and it's going to feed
it and we're going to feed it in via
this parameter model name and then the
path of where it's located and then it's
going to run the server with that model
that's going to do everything necessary
is going to check the version it's going
to feed it into a loader the manager is
going to maintain it to life cycle and
then we can we can call it from our
client so right now the server is local
and then we can call it with our client
ok so right so this is going to take a
while let me fast forward so then once
we have our server running then we can
call it via our client so this is what
our client looks like let me show you
the client right
so the client looks like this very small
very small but all it does is it's
sending Ag RPC request and going to
receive a response right so it's going
to send a request and so this is why we
had signatures right this is why we need
signatures as our parameter and so once
we have these signatures we can make the
request as long as it's making a simple
request that's that's the client file
that's there's two files that are that
we that we've talked about right this
client file and then we have that custom
file that on the server that's going to
train that model for us and then create
a servable with it so that our main
conductor serving file which is Q C++
file can deal with can handle its
lifecycle and we want to touch unless we
want to be very specific but now for the
provision of this tutorial okay so okay
where were we so we once we've built our
server it's running locally then we can
use that client to then take this image
which is a what is this image it's a
panda it's a panda image cute little
Panda Kung Fu Panda and then we're going
to we're going to pull it on the web and
then we're going to send it to the
client via this via this parameter right
here image and then using the inception
client if it works we're going to see a
panda classification output in terminal
from the server so this server is local
so if we run this right assuming we have
it locally or we run this bloke I have
the file locally so if I run this we get
a response we get a response back from
the server and so the server is going to
send a response these string values are
the likelihood for or the or the
classification values and then we have
the likelihood for each of the
likelihood for each of these
classification labels down here as well
as floats but yeah see Chinese panda
panda a black white panda you know
cardigan pretty bad but yeah the best
results are up there at the top and so
that so we get those results back as as
as as JSON and so then that's that's it
right so now we have the server running
locally we have our clients and now we
want to put it on a server right we
don't want to have it all local want to
put this thing on the server
so to do that we're going to push it to
Google cloud and so Google has this
great tutorial on how to do that and
it's going to be using kubernetes
and what is kubernetes kubernetes is
automatic container management so this
is what Google uses internally and it's
basically production grade container
management so you can have multiple
containers multiple models and basically
there's a lot of infrastructure that
goes around dealing with these huge
models and when to use them and stuff
and basically it's like a little it's a
tool that helps tensorflow serving serve
models and so it's running on the cloud
and we can that's that's how we work
that's what we're going to use so we can
go right to this part two because we did
all the other parts okay so basically
this is how it works right so you log in
to Google Cloud via terminal so assuming
you've created account you log in via
terminal and then you create a container
cluster okay and so this cluster is
going to just contain all of your your
server code right so that the whole
tensor flows serving repo as well as if
you have some custom model or Inception
that that goes in the container
everything but the client right tangible
serving and the custom model both are
both are contained in a single container
and then we say okay since Google Cloud
is so tightly integrated with kubernetes
we can do this all in a series of
commands so we can so we can figure
Google Cloud to say here's a container
that we want now that we've created it
here's the one we want and this is the
that see this Inception serving cluster
is what we're going to put in that
container and once we have that we can
upload the docker image once we tag it
with some value to that container in
Google Cloud and once we've done that
now we're going to deploy three replicas
three replicas of the inception
inference server using kubernetes right
so the so the replicas are exposed
externally by a kubernetes service and
what kubernetes does is it says well I
know you have several versions of this
container let me pick which one I want
to use and we can then create a
kubernetes configuration file and then
deploy our container to it just using
these two commands
and finally when that's done we can
query the model right just like that and
it's going to do the same thing return a
JSON file that we can then you know read
and whatever else okay so that's the
high level of how that works the code is
in the is in the github repo it's in the
description definitely check it out and
I commented extensively what else so
okay so to end the session let me answer
some questions from from the channel and
then we're out of here ok so what kind
of questions do we have here
eyes are two questions randomly question
why
so here's a question from my
differentiable neural computer video how
does it understand the order of problems
like how can a hundred cents across s
text first then a graph problem why
doesn't it just go to the graph neural
network first so it doesn't know the
order we tell it the order so we need to
define what we've been trained it on one
data set and then we could train it on a
different data set the differentiable
neural computer is definitely like the
dopest model I've ever seen before
because it's so generalized and it can
do so many different things so once we
train it on one data set we didn't have
to synchronously train it on the next
data set and then just keep going like
that but it doesn't know to say let me
first look at this graph and then let me
let me first look at this you know
subway graph and then let me look at the
text you got to train it in order and so
one more question can you please explain
how to save the models in tensor flow
once a training is done so they can be
utilized later whenever I want or is
there any other video in which you
explain that so like we talked about
right here is how we save those models
okay and so we save it just like this we
define an export path and then we build
it using this so basically so a server
Bowl contains a tech point file the PD
file all the related metadata things
that come with the model and the reason
we even use this right why don't what
why do we even use this is because how
else are you supposed to run a model in
the cloud right that's why you need a
tangible specific library because these
computation graphs need some kind of
environment to run in and that's why
it's very useful to have x float serving
as a file if I were to if I were to
build a model in production right if I
wanted to build an app at scale the
hundreds of thousands of people would
use I would use tensorflow serving ID
there is no better tool out there for
production in terms of reliability and
in terms of scalability so those are the
two criteria you should look for when
dealing with production grade apps okay
so yeah that's it for this tutorial if
you like this video please subscribe for
more like it
and for now I'm going to take kubernetes
three times fast so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>