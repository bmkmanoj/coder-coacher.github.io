<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Do Mathematics Easily - Intro to Deep Learning #4 | Coder Coacher - Coaching Coders</title><meta content="How to Do Mathematics Easily - Intro to Deep Learning #4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Do Mathematics Easily - Intro to Deep Learning #4</b></h2><h5 class="post__date">2017-02-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N4gDikiec8E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Perry okay Siraj consumer Martin on deep
learning totally hello world its garage
and let's learn about the math needed to
do deep learning matches in everything
not just every field of engineering and
science it's between every note in a
piece of music and hidden in the
textures of a painting the funding is no
different Matt helps us define rule for
our neural network so that we can learn
from our data if you wanted to you can
use the morning without ever knowing
anything about neck there are a bunch of
readily available api's for tasks like
computer vision and language translation
but if you want to use a library like
tensorflow to make a custom model to
solve a problem knowing what math terms
me when you see them pop up it's helpful
if you want to advance the field through
research don't even trip you definitely
need to know the map
pink wanting mainly pulls from three
branches of next linear algebra
statistics and calculus if you don't
know any of these topics I'd recommend a
cheat sheet of the important contest and
I've linked to one for each in the
description so let's go over the
four-step process of building a deep
learning pipeline and talk about how
math is used at each step
once we've got a data set that we want
to use we want to process it we can
clean the data of empty values remove
features that are not necessary but
these decks don't require math a step
that does though is called normalization
this is an optional step that can help
our model reach convergence which is
that point when our prediction gives us
the lowest error possible factor because
all the values operate on the same scale
this idea comes from statistics and you
have a seventeen point four percent
chance of making this plate
there's several strategies to normalize
data although a popular one is called
min max scaling if we have some given
data we can use the following equation
to normalize it we take each value in
the list and subtract the minimum value
from it then divide that result by the
maximum value minus the min die we then
have a new list of data within the range
of 0 to 1 and we do this for every
feature we have so they're all in the
same scale after normalizing our data we
have to ensure that it's in a format
that our neural network will accept this
is where linear algebra comes in there
are four terms in linear algebra that
show up consistently scalars vectors
matrices and tensors a scalar is just a
single number a vector is a
one-dimensional array of numbers a
matrix is a two-dimensional array of
numbers and a tensor is an N dimensional
array of numbers so a matrix scalar
vector and Specter wait not Specter can
all be represented as a tensor want to
convert our data whatever formats NB
that images words videos into tensors
where n is the number of features our
data has and define the dimensionality
of our tensor let's use a three-layer
feed-forward neural network capable of
predicting a binary output given an
input as our base example to illustrate
some more math concepts going forward
when do we use math and keep learning
when we go alive during processing when
a model parameters fly searching and
random waste the initializing cancers
quo inputs you out and measure the error
to measure the DAO it gives us what's
real and what's effective that
propagates to get common corrected we'll
import our only dependency numpy and
initialize our input data and help with
data as matrices once our data is in the
right format we want to build our deep
neural network deep nets have what are
called hyper parameter these are the
high level tuning knobs of the network
that we define and they help decide
things like how fast our model runs how
many neurons per layer how many hidden
layers basically the more complex your
neural network gets the more hyper
parameters you'll have you can tune
these manually using knowledge you have
about the problem you're solving to get
probable values and observe the results
based on the result you can tweak them
accordingly and repeat that process
iterative
but another strategy you could use is
random search you can identify ranges
for each then you can create a search
algorithm that picks values from those
ranges at random from a uniform
distribution of possibilities which
means all possible values have the same
probability of being chosen this process
repeats until it finds the optimal high
performers
yay for statistics we only have number
of epochs as our hyper parameters since
we have a very simple neural network we
use probability to decide our weight
values - one common method is randomly
initializing samples of each weight from
a normal distribution with a low
deviation meaning values are pretty
close together we'll use it to create a
weight matrix with a dimension of three
by four since that's the size of our
input so every note in the input layer
is connected to every node in the next
layer the weight values will be in the
range of negative 1 to 1 since we have
three layers will initialize two weight
matrices the next set of weights has a
dimension 4 by 1 which is the sides of
our output data propagates forward in a
neural network each layer applies its
own respective operation to it
transforming it in some way until it
eventually outputs a prediction this is
all linear algebra it's all tensor math
we'll initialize a for loop to train our
network 60,000 iterations then we'll
want to initialize our layers the first
layer our input gets our input data the
next layer computes the dot product of
the first layer and the first weight
matrix when we multiply two matrices
together like in the case of applying
weight values to input data we call that
the dot product then it applies a
non-linearity to the result which we
decided is going to be a sigmoid it
takes a real-valued number and squashes
it into a range between 0 and 1 so
that's the operation that occurs in
layer 1 and the same occurs in the next
layer we'll take that value from layer 1
and propagate it forward to layer 2
computing the dot product of it and the
next weight matrix then squashing it
into output probabilities with our
non-linearity since we only have three
layers this output value is our
prediction the way we improve this
prediction the way our network learns is
by optimizing our network over time so
how do we optimize it enter calculus the
first prediction our model mates will be
inaccurate to improve it we first need
to quantify exactly how wrong our
prediction is will do this by measure
the error or cost the error specifies
how far off the predicted output is from
the expected output once we have the
error value we want to minimize it
because the smaller the error the better
our prediction training a neural network
means minimizing the error over time we
don't want to change our input data but
we can change our ways to help us
minimize this error if we just brute
force all the possible weights to see
what gave us the most accurate
prediction it would take a very long
time to compute instead we want some
sense of direction for how we can update
our weights such that in the next round
of training our output is more accurate
to get this direction we'll want to
calculate the gradient of our error with
respect to our weight values we can
calculate this by using what's called
the derivative in touch with when we set
derivative true for our non Lin
it will calculate the derivative of a
sigmoid that means the slope of a
sigmoid at a given point which is the
prediction values we did it from l2 we
want to minimize our error as much as
possible and we can intuitively think of
this process as dropping a ball into a
bowl where the smallest error value is
at the bottom of the bowl once we drop
the ball in we'll calculate the gradient
at each of those positions and if the
gradient is negative we'll move the ball
to the right if it's positive we'll move
the ball to the left and we're using the
gradient to update our weights
accordingly each time we'll keep
repeating this process until eventually
the gradient is 0 which will give us the
smallest error value this process is
called gradient descent because we are
descending our gradient to approach 0
and using it to update our weight values
irritably I understand everything now
still understand everything so to do
this programmatically we'll multiply the
derivative we calculated for our
prediction by the error this gives us
our error weighted derivative which
we'll call l2 Delta this is a matrix of
values one for each predicted output and
gives us eight direction
later use this direction to update this
layers associated weight values this
process of calculating the error at a
given layer and using it to help
calculate the error weighted gradient so
that we can update our weights in the
right direction we'll be done
recursively for every layer starting
from the last back to the first we are
propagating our error backwards after we
computed our prediction by propagating
forward this is called back propagation
so we'll multiply the L to Delta values
by the transpose of its associated
weight matrix to get the previous layers
error then use that error to do the same
operation as before to get Direction
values to update the Associated layers
ways to error is minimized
lastly we'll update the weight matrices
for each associated layer by multiplying
them by their respective Delta when we
run our code we can see that the error
values decreased over time and our
prediction eventually became very
accurate sort of break it down deep
learning borrows from three branches of
math linear algebra statistics and
calculus a neural net performs a series
of operations on an input tensor to
compute a prediction and we can optimize
our prediction by using gradient descent
to back propagate our errors recursively
updating our weight values for every
layer during training the coding
challenge winner from the last video is
Jovian Lynne Jovian try out a bunch of
different models to predict sentiment
from a data set of video game reviews
wizard of the week and the runner-up is
Vishal bocce he tested out several
different recurrent and eloquently
recorded his experiments and is really
the coding challenge for this video is
to train a deep neural net to predict
the magnitude of an earthquake and use
the strategy to learn the optimal
hyperplane need to learn the readme
poster can help link in the comments and
I'll announce the winner next video can
subscribe if you want to see more videos
like this check out this related video
and for now I got to get my mat turns up
to a million so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>