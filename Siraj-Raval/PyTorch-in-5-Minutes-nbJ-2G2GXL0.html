<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PyTorch in 5 Minutes | Coder Coacher - Coaching Coders</title><meta content="PyTorch in 5 Minutes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PyTorch in 5 Minutes</b></h2><h5 class="post__date">2017-04-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nbJ-2G2GXL0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and let's learn
about a popular new deep learning
framework called high porch the name is
inspired by the popular torch deep
burning framework which was written in
the Lua programming language learning
Lua is a big barrier to entry if you're
just starting to learn deep learning and
it doesn't offer the modularity
necessary to interface with other
libraries like a more accessible
language would so a couple of AI
researchers who were inspired by torches
programming style decided to implement
it in Python calling it high torch they
also added a few other really cool
features to the mix and we'll talk about
the two main ones
the first key feature of Pi porch is
imperative programming an imperative
program performs computation as you
typed it most Python code is imperative
in this numpy example we write four
lines of code to ultimately compute the
value for D when the program executes C
equals V times a it runs the actual
computation than in there just like you
told it to in contrast in a symbolic
program there is a clear separation
between defining the computation graph
and compiling it if we were to rewrite
the same code symbolically then when C
equals E times a is executed no
computation occurs at that line instead
these operations generate a computation
or symbolic graph and then we can
convert the graph into a function that
can be called be at the compile step so
computation happens as the last step in
the code ode Styles have their
trade-offs symbolic programs are more
efficient since you can safely reuse the
memory of your values for in-place
computation
tensorflow is made to use symbolic
program imperative programs are more
flexible since Python is most suited for
them so you can use native Python
features like printing out values in the
middle of computation and injecting
loops into the computation flow itself
the second key feature of Pi fork is
dynamic computation graphing as opposed
to static computation graphing in other
words PI Park is defined by run so at
runtime the system generates the graph
structure tensorflow is define and run
where we define conditions and
iterations in the graph structure it's
like writing the whole program before
running it so the degree of freedom is
limited so in
we defined the computation graph once
then we can execute that same graph many
times the great thing about this is that
we can optimize the graph at the start
let's say in our model we want to use
some kind of strategy for distributing
the graph across multiple machines this
kind of computationally expensive
optimization can be reduced by reusing
the same graph static graphs work well
for neural networks that are fixed size
like feed-forward networks or
convolutional networks but for a lot of
use cases it would be useful if the
graph structure could change depending
on the input data like when using
recurrent neural networks in this
snippet we're using penter flow to
unroll a recurrent Network unit over
Ward vectors to do this we'll need to
use a special tensorflow function called
while loop we have to use special nodes
to represent primitives like loops and
conditionals because any control flow
statements will run only once when the
graph is built but a cleaner way to do
this is to use dynamic graphs instead
where the computation graph is built and
rebuilt as necessary at runtime the code
is more straightforward since we can use
standard four and if statements any time
the amount of work that needs to be done
is variable dynamic graphs are useful
using dynamic graphs makes debugging
really easy since a specific line in our
written code is what fails as opposed to
something deep under section dot run
let's build a simple two layer neural
network in pi ports to get a feel for
this impact we start by importing our
framework as well as the auto grab
package which will let our network
automatically implement back-propagation
then we'll define our batch size input
dimension hidden dimension and output
dimension well then use those values to
help define tensors to hold inputs and
outputs wrapping them in variables well
set requires gradients to false since we
don't need to compute gradients with
respect to these variables during back
propagation the next set of variables
will define our our weights we'll
initialize them as variables as well
storing random tensors with the float
data type and since we do want to
compute gradients with respect to these
variables we'll set the flag to true
we'll define a learning rate then we can
begin our training loop for 500
iterations during the forward pass we
can compute the predicted label using
operations on our
variables mm stands for matrix multiply
and clamp clamps all the elements in the
input range into a range between min and
Max once we've matrix multiplied for
both sets of weights to compute our
prediction we can calculate the
difference between them and square the
sum of all the squared errors a popular
loss function before we perform back
propagation we need to manually zero the
gradients for both sets of weights since
the great buffers have to be manually
reset before fresh grades are calculated
then we can run back propagation by
simply calling the backward function on
our loss it will compute the gradient of
our loss with respect to all variables
we set requires gradient to true for and
then we can update our ways using
gradient descent and our outputs look
great
pretty dope to sum up high park offers
two really useful features dynamic
computation graphs an imperative
programming dynamic computation graphs
are built and rebuilt as necessary at
runtime and imperative programs perform
computation as you run them there is no
distinction between defining the
computation graph and compiling right
now tensorflow has the best
documentation on the web for a machine
learning library so it's still the best
way for beginners to start learning and
it's best suited for production use
since it was built with distributed
computing in mind but for researchers it
seems like pi torch has a clear
advantage here a lot of cool new ideas
will benefit and rely on the use of
dynamic grasp please subscribe for more
programming videos and for now I've got
to torch my hair so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>