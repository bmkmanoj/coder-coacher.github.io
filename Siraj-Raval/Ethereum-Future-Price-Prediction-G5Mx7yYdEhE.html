<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ethereum Future Price Prediction | Coder Coacher - Coaching Coders</title><meta content="Ethereum Future Price Prediction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ethereum Future Price Prediction</b></h2><h5 class="post__date">2018-01-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G5Mx7yYdEhE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and do you want
to predict cryptocurrency prices of
course you do
who wouldn't everybody's talking about
cryptocurrency these days and if you
understand machine learning why not
apply that skill to try to predict some
cryptocurrency prices right in this
video I'm gonna show you how to predict
the price of Bitcoin
but this can really apply to aetherium
this can apply to any altcoin it can
apply to any cryptocurrency and we're
gonna use a very simple model built with
Karos a deep learning library that is
very popular and I've talked about it
before we're gonna use it to predict
future prices now you might be asking
wait does this work
are you sure this is gonna work and let
me just tell you this JP Morgan Morgan
Stanley all of these big banks all of
these big hedge funds new Mirai
all of these hedge funds I promise you
they are using some kind of algorithm to
predict future prices will they share it
with you definitely not because that is
their profit right that is their secret
of course we're not gonna share it so
the answer is yes it's possible there's
millions of variables out there and you
can get a greater than 50% return if you
do this the right way
is this the absolute optimal right way I
don't know
but it the graph looks really nice and
I've made sure to really avoid
overfitting so using a dropout and some
other techniques that I'll talk about so
let's just get into this okay all right
so what I'm gonna do is I'm going to
show you this code now this code all of
it is available in the description of
this video so definitely check it out
and follow along because I'm gonna be
analyzing this code I'm gonna be talking
about different techniques okay so let's
let's do this so the first step is for
us to import our dependencies which I've
got right here
the first dependencies are of course all
those Karos sub modules remember chaos
is the easiest way to get started with
deep learning if you haven't searched
chaos explained
Siraj on youtube great video for you but
anyway in this case we're gonna use
what's called a recurrent Network
specifically okay this is notice a long
one a bi-directional LST M recurrent
Network I'll talk about exactly what
that is but I've talked about it before
in my intro to deep learning course
which you should watch as well if you
haven't but it's it's basically a more
advanced version of a recurrent Network
that takes into account future values in
a sequence instead of just the past but
Carris is awesome it's a great library
it's got loads of documentation great
examples and yeah you can basically
learn all about deep learning just from
reading the chaos documentation which
I've got right here so it's a great
library okay so enough of that so what
I'm doing here for these four first four
lines is I'm importing the relevant
parts of chaos that we're going to need
for this model the first part is dense
dense means a fully connected layer that
means all of the nodes in one layer are
connected to all the nodes or neurons in
the next layer which usually happens at
the end of a deep network usually the
last two or three layers are fully
connected whether it's for convolutional
networks or l STM networks I've got the
activation sub module which is going to
be used for our activation function
right this is our non-linearity so our
network can learn both linear and
nonlinear functions which of course the
stock graph is definitely nonlinear and
then drop out drop out is a technique
that I'm going to talk about but in this
case I'm using it to prevent overfitting
then I've got the LSE M class which is
which stands for long short-term memory
I'll talk about all this bidirectional
is a type of network sequential is a
type of graph like in order you know
ABCDE instead of like you know multiple
branches like a like a directed acyclic
graph or something it's more of a single
it's more of like a like a linked a
singly linked list in terms of a graph
in terms of bi-directional it's a doubly
linked list but well we'll get into that
ok so the next part R is scikit-learn
psyche learns a great library but at
this point it's really all of its
functionality has been eclipsed by great
libraries like tensor flow PI torch but
what it's really got going for it is its
metrics modules like this you know it's
Logging's like it's SK learn metrics
it's got so many great sub modules that
I use still but we're gonna be use it to
import the mean squared error which is
going to be our loss function this is
what we're gonna minimize over time to
make our network better and better at
predictions lastly we've got logging for
you know logging with the training
process numpy and math for standard
matrix math operations map plot live for
plotting like our graph at the end and
lastly data processing is going
be pandas pandas is the data processing
tool that data scientists use in fact I
have seen data scientists put pandas
expert in there on their resume like
it's that important the library so if
you haven't seen pandas yet definitely
check it out so this is a huge like
infographic but let me first read this
this first step here so for data
processing what we're gonna do is we're
gonna input this data as a CSV file and
I'll show you this this data in a second
but we're going to time-series transform
this data so it's it's gonna be from
number of days by the number of features
that was the original array right and
we're gonna convert that to the number
of days - the window size times the
number of days per sample times the
number of features so we're turning it
from a 2d array to a 3d array and this
is just a pre-processing step so we can
then feed it into our network right and
normalization is what were what we're
about to do so the reason we want to
normalize data is so that our model will
be able to converge faster right
normalization means I mean it means a
lot of things but check them now let me
go to this infographic here but we have
five five rules of data normalization
the first is eliminating repeating
groups anytime you have some values that
are being repeated eliminate them
eliminate redundant data eliminate
columns not dependent on the key so if
attributes do not contribute to a
description of the key or move them to a
separate table so it's kind of
compartmentalizing all of these like
different features isolating independent
multiple relationships and isolating
semantically related multiple
relationships so you can really read
this small text later on but those are
like five key rules for data
normalization but in general remember
that data normalization is a step that
we always perform in data pre-processing
it's just a very important thing to do
and so in this case what we're gonna do
is we're going to divide each value in
the window by the first value of the
window and then subtract one so ie if we
had an array like this 4 3 2
once normalized it would become 0
negative 0.25 negative 0.5 and notice
that these values are much smaller and
the interval between these values is
much smaller so it's kind of dense
it's kind of condensing this graph so
it's smaller and more readable and
easier to converge on for our model the
unnormalized basis we're going to keep
them that means those original numbers
just and so we can compare the models
predictions with the true prices we're
gonna split our data 90% training 10%
testing ok so this data what it's going
to look like is like this it's gonna
have so many different columns for all
of these features so you might be
wondering how is predicting
cryptocurrency any different from
predicting stock prices or any of the
other you know financial predictions and
the answer is cryptocurrency has its own
set of variables that the stock market
doesn't have you've got block size block
high annual hash grow Metcalfe's law
market capitalization hash rate you know
minor revenue value all of these
variables that are unique to block
chains right very very cool stuff so in
terms of where you can get this data set
if you just Google Bitcoin blockchain
data set you've got several I got mine
from toggle which is the first link but
you've got several here in fact you
could just pull it from the blockchain
API directly if you search Stack
Overflow you've got some people who are
who have there we go get Bitcoin
historical data this guys like you can
get the whole trade history and CSV
format from here API to Bitcoin charts
calm quando is another one but basically
I use Kaggle just because I really like
haggle and yeah but there's a lot you
got a bunch of CSVs for you know all
sorts of things the exchanges you can
play around with what data you're going
to use right what is it what is the
exchange volume between different crypto
currencies but there's there's so much
data out there that it is just a crime
to not be training some machine learning
models on this data so back to this so
we've got we've got lots and lots and
lots of data here we can look at some of
it but that's that's that's the gist of
it so let's keep going here
so then comes our loading data step
right so what we're gonna do is we're
gonna read the data file using pandas
built in read CSV function and we're
gonna store this that pandas dataframe
object in this raw data variable then
we're gonna change all the zeros to the
number before the zero occurs so in this
way we're removing all the unnecessary 0
values we want very
data so that's what this nested loop
does we convert that file to a lists for
further and easier pre-processing and
then we take that data variable and we
convert it to a 3d array remember I said
we're converting in from a 2d array to a
3d array right and then we fill that 3d
array with the normalized values right
this is that normalization step we keep
the UH normalized prices as well so we
can compare the two later on we split
the data into training remember
90-percent training 10% testing we
shuffle the data and then we create
those training and testing variables we
get the day before Y tests price so we
can use that for prediction later on and
then we get the window size and sequence
length we return all of those variables
right we computed all of these values
and now we can return them right to now
step two step two for us is going to be
to build our model now recall I said
we're going to build a recurrent network
to be more specific
we're building a three layer recurrent
network with 20% dropout at each layer
to reduce overfitting in the training
data specifically the type of recurrent
now we're going to build is called a
bi-directional LS TM network the model
is going to have about 500,000 trainable
parameters don't be alarmed this is very
normal for deep networks right each of
those values inside of the matrices of
the weights those are parameters that
are being trained they are tunable
values that are getting updated through
some optimization scheme in this case
the optimization scheme will use is Adam
right so Adam is the optimization scheme
the loss function is going to be mean
squared error the linear activation
function in this model is going to
determine the output of each neuron in
the model and then it's going to use
Karros is sequential bi-directional LS
TM layers so now I'm gonna go into what
I just said so recurrent Nets are
different from feed-forward networks
because in feed-forward networks every
at every time step we're only feeding in
the new input data right you know we
have you know let's say like 10 data
points like every time step we feed in a
new one ducted uh but with recurrent
networks there's loops right instead of
just feeding in the input data right the
next data in the set of data
we're also feeding in the hidden state
that was
learned from the previous time step
hence the loop right hence the looping
part we are feeding in the pin state
from the previous time step and the the
new input data so we're concatenating
both of those values and we're feeding
them in and the reason we do this is
because when it comes to sequences of
data memory matters right if we have a
sequence like 1 2 2 4 5 6 and we want to
predict the next number in that sequence
we have to know that before the next
number which is going to be 8 spoiler
alert we have to know that the first 7
numbers were 1 through 7 and that's why
we feed in the hidden state that is
learned over time but what happens is
when we have really long sequences
whenever we're optimizing this network
the gradient vanishes slowly slowly
slowly slowly as we go from the last
layers back to the first layers and the
gradient is what we use to update all of
these weights so in order to prevent
this vanishing gradient so all the
layers all the weights in each layer are
updated properly someone invented this
technique called a long short-term
memory cell so the L SCM cell it has a
lot of features to it but I'm not going
to talk about all of them right now but
basically what it does is it's got an
input gate of forget gate and an output
gate and these act as valves they act as
valves like plumbing valves that can
store and lock in memory over time and
what this does is it prevents the
vanishing gradient problem which for us
means we can predict longer-term
sequences perfect for predicting
cryptocurrency prices over a long period
of time and now lastly it's not just an
LS TM recurrent Network
it's a bi-directional LS TM recurrent
network so if you've seen some of my
older videos the only other time I've
talked about a bi-directional LS TM is
when I was talking about how to create a
language translator I think it was deep
learning number 16 or 15 because Google
needed that to to create a really good
you know state-of-the-art language
translation service but bi-directional
artists rnns are based on the idea that
the output at time T may not only depend
on the previous element in the sequence
but also future elements it's not just
about the past it's about the future
right so for example to predict a
missing word in a sequence you want to
look at both the left and the right
context right like I went to the
Jim to get every day what was that pause
swole right do I want to get I went to
the gym to get swole when everyday so
I'm looking at everyday what do I want
to do every day I want to go to the gym
every day what is it that I want to do
every day at the gym I want to get swole
right that was a top that was a example
I just came up with anyway let's just
keep go let's just keep going here so
bidirectional RN ends are good in this
case because the price of Bitcoin
ideally we have we have this you know
list of all the prices over time right
and we can predict future prices that
are in the past you know what I'm saying
like as our network is training in one
direction right we also have and
recurrent network training in the other
direction and then we can then
concatenate their learned hidden states
together to form a more accurate
representation of the of the sequence
right so it's essentially two RNN
stacked on top of each other the output
is then computed based on both the
hidden both hidden states so the hidden
states of both are intense and for the
atom optimizer what I have here is a
graph of the loss functions decreasing
over time and notice how atom is the the
fastest one or the best one of all of
them on the MN is C data set basically I
have a great video on the differences
between all of these types of activation
functions to search which activation
function should I use on YouTube and
it'll be the first link look I'm telling
you I have made so much content on deep
learning anything that you would ever
want to know
I have content on it so now we're gonna
initialize this model okay so when we
comes to initializing the model like I
said carrots makes it super simple a
bi-directional LS TM recurrent network
is a relatively complex model to make
but with carrots we can do this in just
about 10 lines
well initialize it as a sequential model
well had our first recurrent layer with
drop out let me talk about drop out as
well after this so ad then our second
recurrent layer right you just you're
literally naming them like human
readable bi-directional LS TM with those
two parameters a third recurrent layer
an output layer remember I said that the
dense fully connected layer comes at the
end and then
our activation function and then our
loss function at the very end which is
going to be Adam so why do we use drop
out so drop out was a technique that was
invented by Hinton which was one of the
godfathers of neural networks but
basically the idea is that some
overfitting is a real problem right when
we have very homogenous data it
overfitting is a very big problem right
because when you learn something think
about grooves in your brain right if
you're learning the same thing over and
over and over again and all your inputs
are the same you're gonna have these
very specific grooves in your brain so
if you have some kind of new data your
brain is not gonna know how to predict
it right because it's been trained on
such homogeneous data
so what dropout is is essentially it's
this technique which randomly turns
neurons on and off and what this does is
it forces the data to create new
pathways right create new pathways
between weights in this case right
between essentially matrix
multiplications weights in each layer
and because it's creating all of these
new pathways randomly right these nodes
are turned on and off these neurons are
turned on and off what happens is the
weights become more robust to more
heterogeneous data which means you could
give it you know new data that's not
like the training set and it would be
better able to generalize so it
increases the ability to generalize
that's why we've added dropout here and
we can tune dropout on and off like we
could say you know 20% 30% 40% and it's
one of those things where you just got
to kind of you know trial and error it
out and see what works best now now
we're gonna train the model right so
we're gonna train it with the batch size
of 1024 for 108 pox and we're gonna
minimize the loss of its training data
using the mean squared error and so now
we can look at this so for fitting the
model we say okay time to start
recording and just models outfit that's
it and we give it all of those variables
that we computed at the very beginning
the training data and we return the
model right so this is our function for
fitting the model right here and then we
test the model right we do the same
thing we're testing the model on the
testing data the models given X values
of testing data and will predict
normalized prices which is the Y you
know underscore predict and
for testing it's you know it's
relatively simple we just say test the
model on the X test create an empty 2d
array fill the 2d array with all the
predicted values and then plot those
predicted values and see see what turns
out at the end right so we're still
writing these functions we're going to
we're going to actually compute them at
this at the end right step 5 is to
evaluate the change in price so we can
plot the models predicted change in
price each day against a real change in
each each price daily right so we can
basically model the prediction versus
the actual price which is the real graph
that we want to see in the end to see
how good our model is performing then we
process the percent change in price at
the very end which is the Delta right
the change between what what the price
that it was before and the price that it
is now and then we compare the
prediction to the real data so when all
is said and done we're gonna have some
true positives false positives true
negatives and false negatives right our
model is gonna predict the real values
some times and it's gonna predict the
fake values sometimes right and so we
want to know how how often it's
predicted predicting real values and how
often it's predicting fake or off values
and we want to minimize those fake or
off values as much as we can and so to
do that we're going to compete we're
going to minimize our loss function so
remember recall I said that the loss
function that we're gonna use here is
the mean squared error right this is a
very popular loss function and this is
what it looks like it's the mean of the
squared error so what I mean is we take
the value returned by the model F and we
subtract the actual value for the data
point right that is the error right the
prediction minus the real value that's
our error we take that error value how
far off our models prediction is and we
square it and then we add all of those
values up together for every single data
point we have and then we divide by the
number of data points there are and that
single value is our mean squared error
and that is what we want to minimize at
every single time step while we're
training okay using the atom
optimization scheme which is a variant
of gradient descent so right this is
just another graph of true positives
versus false positives and negatives and
negatives but anyway when we take these
values and we add them and
them on top of each other we get these
other three metrics of how good our
model is performing precision recall and
f1 score so position to precision means
how often is our model getting true
positive compared to how often it
returns a positive recall is how often
does the model get a true positive
compared to how often it should have
gotten a positive and f1 score means a
weighted average of both of those two
values right and so we just compute
those right here C mean squared error
right here precision recall f1 we
literally just took those equations and
wrote them out programmatically using
just native Python so now we can put it
all together right we've we've compute
we've created all these functions and
now we can put it all together so our
first step is to load up all this data
which we're doing right here and then we
initialize the model right
so I've already trained it beforehand so
we can just look at the results just
right here in this jupiter notebook
right but remember it's a three layer
bi-directional LS TM network then we
train the model right so this the model
is training by just running that fit
model function that we created earlier
and then we can test the model and so
here is the graph here is the good stuff
that we have we have been wanting to see
all right so this is the the Bitcoin
price over time for a period of over 250
days notice how the real price is
different from the predicted price but
just by very little which is kind of
exciting right
so our model it could be a bit over fit
let's not let's not kid ourselves
but as we add more features as we add
more data to this model it will become
more robust we can implement features
like we can implement techniques like
drop out we can implement techniques
like you know all sorts of normalization
techniques out there and I can go into
that in a future video but there are all
these ways that we can make our model
more robust I have other videos on
predicting stock prices which some of
the learnings can be applied to here as
well search predicting stock prices
Suraj on YouTube in like three videos
will show up and then we have one more
really important graph and that is
predicting the percent change so what is
a predicted present percent change from
the previous date to the current day
versus a real percent change and notice
how there's a bit more variance here
between the real values and the
predicted values so we can we condemned
do better here and then we compare you
know the predictions and the true data
and then we get our statistics at the
very very end position recall f1 and
mean squared error please subscribe for
more programming videos and for now I've
got to predict the future so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>