<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Genetic Algorithms - Learn Python for Data Science #6 | Coder Coacher - Coaching Coders</title><meta content="Genetic Algorithms - Learn Python for Data Science #6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Genetic Algorithms - Learn Python for Data Science #6</b></h2><h5 class="post__date">2016-11-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dSofAXnnFrY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj in this video
we're going to use genetic programming
to identify if some energy is gamma
radiation or not I'm getting angry gamma
rays
now I wish ADA science is a way of
thinking about discovery data scientists
need to decide the right question to ask
like who's the best candidate to vote
for in the u.s. election then decide
what dataset to use like tweet history
of candidates and pass endorsements of
each candidate and lastly decide what
machine learning model to use on the
data to discover the right answer
live those on with the right data
computing power and machine learning
model you can discover a solution to any
problem but knowing which model to use
can be challenging for new data
scientists there are so many of them
that's where genetic programming can
help genetic algorithms are inspired by
the Darwinian process of natural
selection and they're used to generate
solutions to optimization and search
problems they have three properties
selection crossover and mutation you
have a population of possible solutions
to a given problem and a fitness
function every iteration we evaluate how
fit each solution is with our fitness
function then we select the fittest ones
and perform crossover to create a new
population we take those children and
mutate them with some random
modification and repeat the process
until we get the fittest or best
solution so taking this problem for
instance let's say you want to take a
road trip across a bunch of cities
what's the shortest possible path you
could take to hit up each city once and
then return back to your home city this
is popularly called the Traveling
Salesman problem in computer science and
we can use a genetic algorithm to help
us solve it let's look at some
high-level Python code we have the
number of generations set to 5000 and
the population size set to 100 so we
start by initializing our population
using our size parameter each individual
in our population represents a different
solution path then for each generation
we compute the fitness of each solution
and store it in our population fitness
array now we'll perform selection like
only
the top 10% of the population which are
our shortest road trips and produce
offspring from them by performing
crossover then mutate those offspring
randomly and repeat the process as you
can see in the animation eventually we
will get an optimal solution using this
process unlike Apple Maps alright so how
does this all fit into data science well
it turns out that choosing the right
machine learning model and all the best
hyper parameters for that model is
itself an optimization problem we're
going to use a Python library called
teapot built on top of psyche learn that
uses genetic programming to optimize our
machine learning pipeline so after
formatting our data properly we need to
know what features to input to our model
and how we should construct those
features once we have those features
we'll input them into our model to train
on and we'll want to tune our hyper
parameters or tuning knobs to get the
optimal results instead of doing this
all ourselves through trial and error
teapot automates these steps for us with
genetic programming and it'll output the
optimal code for us when it's done so we
can use it later so we're going to
create a classifier for gamma radiation
using teapot after installing our
dependencies and then analyze the
results teapot is built on the popular
psyche I learned machine learning
library so we'll want to make sure that
we have that installed first then we'll
install pandas to help us analyze our
data and numpy to perform math
calculations our first step is to load
our data set we'll use pandas read csv
method and set the parameter to the name
of our saved csv file this is data
collected from a scientific instrument
called a Cherenkov telescope that
measures radiation in the atmosphere and
these are a bunch of features of
whatever type of radiation it picks up
thanks Putin since the class object is
already organized we'll shuffle our data
to get a better result
the Iowa function of the telescope
variable is pandas way of getting the
positions in the index and will generate
a sequence of random indices the size of
our data using the permutation function
of numpy random sub module since all the
instances are now randomly rearranged
we'll just reset all the indices so they
are ordered even though the data is now
shuffled using the reset index method of
pandas with the dropper ammeter set to
true we'll now let our tell a bearable
know what our two classes are on
adding both of them to an integer with
the map method so G or gamma is at the
zero and H or Hadrian is set to 1 let's
store those class labels which we're
going to predict in a separate variable
called Pella class and use the values
attribute to retrieve them before we
train our model we need to split our
data into training and validation sets
we'll use the Train test split method of
scikit-learn that we imported to create
the indices for both the parameters will
be the size of our data set we want both
sets to be arrays so we'll set the
stratify parameter to our array type
then we'll define what percent of our
data we want to be training and testing
with these last two parameters we have a
7525 split now in our data and we're
ready to train our model will initialize
the teapot variable using the teapot
class with the number of generations set
to 5 on a standard laptop with 4 gigs of
RAM it takes 5 minutes per generation to
run so this will take about 25 minutes
this is so teapots genetic algorithm
knows how many iterations to run for and
we'll set verbosity to 2 which just
means show a progress bar in terminals
during the optimization process then we
can call our fit method on our training
data to let it perform optimisation
using genetic programming the first
parameter is the training feature set
which we'll retrieve from our tell a
very well along the first access for
every training index the second variable
is our training class set which we'll
retrieve from our tell eval like so we
can compute the testing error for
validation using teapots for method with
validation feature set as the first
parameter and the validation class set
as the second we'll export the computed
python code to the pipeline pi class
using this method and name it in the
parameter as a string let's demo this
thing after training we'll see that
after five generations Chi pot chose the
gradient boosting classifier as the most
accurate machine learning model to use
it also chose the optimal hyperplane
rate and number of estimators for us
yeah boy so to break it down with the
right amount of theta computing power
and machine learning model you can
discover a solution to any problem
genetic algorithms replicate evolution
via selection crossover and mutation to
find an optimal solution to a problem
and teapot is a Python library that uses
genetic programming to help you find the
best model and hi
parameters for your use case the winner
of the coding challenge from the last
video is Peter Mitrano he added some
great deep dream samples to his
repository and even deep dream my own
video badass of the week and the
runner-up is Kyle Jordan good job
stitching all the deep dreams frames
together with one line of code the
challenge for this video is to use
teapot and a climate change data set
that I'll provide to predict the answer
to a question you decide this will be
great practice in learning to think like
a data scientist post your github link
in the comments and I'll announce the
winner next time for now I've got to
stay fit to reproduce so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>