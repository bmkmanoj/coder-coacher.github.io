<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks - The Math of Intelligence #4 | Coder Coacher - Coaching Coders</title><meta content="Neural Networks - The Math of Intelligence #4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks - The Math of Intelligence #4</b></h2><h5 class="post__date">2017-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ov_RkIJptwE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and by the end of
this video you will understand the basic
math behind neural networks since we'll
build four types of them you know that
feeling you get when you're in love and
you see that special someone that
magical ethereal sensation that just
can't be described by words well it
actually can be described by math the
features of a face word associations the
tone of their voice these are all
related variables and we can represent
this relationship using a function we
have different models for approximating
different functions but humans can
approximate almost any function so
wouldn't it make sense to model our own
capability neural networks inspired by
the brain are universal function
approximate errs that means they can
learn any function although silicon is a
very different medium than the chemical
soup in our head they're still able to
replicate much of what we do like
nothing else we've ever created
this was proven in 1989 we show that
arbitrary decision regions can be
arbitrarily well approximated by
continuous feed-forward neural networks
with only a single internal hidden layer
and any continuous sigmoid 'el
non-linearity wait what are you trying
to throw hands here I'm okay if you want
to look let's just build a simple AF
neural network alright so given some
input data X and some Associated output
labels Y there exists a function that
represents the mapping between them our
goal is to learn that function so we can
then input some random x value and it
will predict its associated Y value this
input data is represented as a matrix
where each row is a different data point
and each column is one of its features
just arbitrary ones and zeros in our
case so how do we learn this mapping
imagine there existed a matrix such that
every time we multiplied our input data
by that matrix the result would give us
the correct output every time that'd be
pretty awesome right that's what we're
trying to do find that matrix that
coefficient of the function work
to learn we'll call it our weight value
so we'll initialize it as a matrix of
random values eventually we want this
matrix to have the ideal values values
such that when we compute the dot
product between our input data and this
matrix it will give us the correct
output but wait we've got to add
something else here it's not enough to
just say the product of these two
matrices is our output no we'll need to
pass that output into a sigmoid function
to normalize or adjust the result to
probability values between 0 and 1 so
multiplying our input by this matrix and
passing the result to this activation
function gives us an output value for
guesses one for each data point but the
guesses are wrong we know what our
actual output should be so let's compute
the difference between our prediction
and the actual Y values we're pretty far
off we know that gradient descent works
well for linear regression so let's try
it here too
we'll compute the partial derivative of
the error with respect to our weight and
that gives us a gradient value gradient
descent is all about finding the minimum
of an error function so let's adjust the
values in our matrix by adding our
gradient value to it now our predicted
output is slightly better and we just do
this over and over again our error will
decrease every iteration or time step
and eventually our network will be able
to predict the right label a neural
network is really just one big composite
function that means it's a function that
consists of other functions each layer
is just a function that takes as input
the result of the previous function
output that's it
the data set we used had a linear
relationship there was a direct
one-to-one relationship between the
input and output values but what if we
use a data set that instead had a
one-to-one relationship between a
combination of inputs a non-linear
relationship our simple AF neural
network wouldn't be able to learn it in
order to first combine the input data
into a representation that can then have
a one-to-one relationship with the
output we need to add another layer our
first layer will combine the inputs and
our second layer will then map them to
the output using the output of the first
layer as its input so we'll multiply the
input by the first weight matrix
activate it with our sigmoid and pass
the result to the next layer as its
input it just does the exact same
process input times weight activate it
rhymes when we compute our prediction
value since we have multiple ways values
to update will recursively compute our
error gradient for each layer we have in
the opposite direction then we can
update each weight value individually
that's why when gradient descent it's
applied to neural networks specifically
it's called back propagation because
after we forward propagate our input
data we backward propagate our error
gradient to update our weight values
what we just built was called a
feed-forward Network they're great for
modeling relationships between any set
of input variables and one or more
output variables but what it's time
mattered in the input sequence by that I
mean what if the ordering mattered like
if we're trying to predict the next
stock price or musical note in a
sequence well we need to modify our
feed-forward network to become a
recurrent Network which will allow it to
learn a sequential mapping we could
apply a linear transformation to the
matrix totally I'm all about the
projection
that fasting will definitely help us
with our unconditional Network you're
going to work ethic thank you my single
dad he helped me this life isn't fair
and did you have to work hard for your
dreams sounds like my kind of guy yeah
you have to get along so this time we
got sequential input data we still
initialize our weights randomly like
before we still multiply our input by
our weight matrix and apply an
activation function to the result for
every layer the difference in forward
propagation this time though is that for
each element in the sequence we don't
just use it alone as the input we use
the hidden states from the previous time
step as well so a hidden state at a
given time step is a function of the
input at that same time step modified by
a weight matrix added to the hidden
state of the previous time step
multiplied by its own hidden state -
hidden state matrix back propagation in
this case works the same way as in a
feed-forward Network we calculate the
gradients of our error with respect to
the weights and use them to update the
weights but because recurrent Nets are a
temporal model we call it back
propagation through time
chocolates best propagate through time
but can a neural network still learn a
function if we have unlabeled data you
betcha
and one neural net that can do this it's
called a self-organizing map it works
pretty differently so let's look at it
we still start off by initializing the
weights of the network randomly think of
it as a two dimensional array of neurons
each neuron has a specific position
topologically and contains a vector of
weights of the same dimensions as the
input vectors the lines connecting nodes
just represent adjacency they don't
signify a connection has normally
indicated when talking about the preview
networks then we'll pick a data point
from our training set to calculate the
Euclidean distance between that vector
and each weight the one that's closest
to it is the most similar it's the best
matching unit we iterate through all the
other nodes to determine whether or not
they are within the radius of it then
adjust the weights of its neighbors this
whole process is what we repeat
iteratively and is a part of the
training procedure this map of nodes
self-organizes into clusters for each
learned label nearby locations in the
map represent inputs with similar
properties we can even visualize it and
this acts as a great tool to observe
these clusters and when we give these
networks more aka
deeper layers lots of data and lots of
compute we call that field deep learning
it's the hottest subset of machine
learning although they are not a
cure-all solution they perform really
well if you have those two things
to summarize neural networks are just a
series of matrix operations no matter
the type they're just one big composite
function because they use nonlinearities
in this series of operations like the
sigmoid they can approximate any
function both linear and nonlinear and
if we don't have labels for our data we
can use a type of neural network called
a self-organizing map to learn the label
clusters The Wizard of the week is Hamad
Shaykh I was so so impressed by his
notebook it Illustrated how using l2
regularization reduces overfitting for
high degree polynomials in the context
of linear regression specifically the
relationship between movie sales and
movie ratings you make me smile and the
runner-up is on jari he used a
regularized linear regression to predict
the impact of climate change on
temperatures in LA what a dope use case
this week's coding challenge is to build
your own self-organizing map see the
description for all the details and post
your github link in the comment section
please subscribe for more programming
videos and for now I've got to practice
my Dutch so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>