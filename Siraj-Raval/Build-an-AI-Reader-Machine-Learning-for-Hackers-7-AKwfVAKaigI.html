<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build an AI Reader - Machine Learning for Hackers #7 | Coder Coacher - Coaching Coders</title><meta content="Build an AI Reader - Machine Learning for Hackers #7 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build an AI Reader - Machine Learning for Hackers #7</b></h2><h5 class="post__date">2016-06-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AKwfVAKaigI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what light in yonder window breaks and
Julia is the son who's the father no no
no she's a bright ball of gas though
someone's son is Julia is the star in
the sky hello world welcome to Serie
geology in this episode we're going to
build an AI reader that is an AI that
can analyze human language this type of
task is considered natural language
processing or NLP NOP is the art of
solving engineering problems that need
to analyze or generate natural language
text we see it all around us Google
needs to do it to understand exactly
what your search query means so they can
give you back relevant results Twitter
uses it to extract the top trending
topics Microsoft uses for in-car speech
recognition NOP is basically extremely
dope because it deals with language
Kurzweil once said that language is the
key to AI a computer able to communicate
indistinguishably from a human would be
true AI there are 6500 known languages
in the world and each of them have their
own rules for syntax and grammar some
rules are easy like I before E except
after C and some one based on intuition
since there is no consistent use case so
how do we write code to analyze language
before the 80s NLP was mostly just a
bunch of hand coded rules like if you
see the word dance translate it to
kuduro or if a word ends an ING label it
as present tense well this worked it was
really tedious and there are a million
corner cases it's just not a sustainable
path to language understanding a way
forward was and is machine learning so
instead of hand coding the rules and a
aya learns the rules by analyzing a
large corpus or piece of text this is
proven to be very useful and applying
deep learning to NLP is currently the
bleeding edge so when thinking about
what tool to demo in this video I was
really torn between an NLP API called
API to AI and Google's newly released
English parser parse emic parse fix file
API that AI takes in an input query and
returns on an analysis of the text Parsi
is Google's newly released English
parser both have similar functionality
but I'm gonna go with Parsi because a
it's currently the most accurate parser
in the world be if you built it into
your app that's one less networking call
you have to make which means you can
parse tax offline and see building this
parsing logic from the source allows you
to have more granular control over the
details of how you want text to be
analyzed Parsi was built using syntax
net an NLP neural net framework for
Google's tensorflow machine learning
library so we could use syntax net to
build our own parser or we could use a
pre trained parser Parsi
let's do that once you parsed your text
there are a whole host of things you can
do with it let's try it out with our own
example we're going to build a simple
Python app that uses parsing mc-- parse
phase to analyze a command by a user and
then repeat it back to them both where
it is different we'll begin by importing
our dependencies then we'll set up our
program to receive and store the user
input the input text is the corpus will
be analyzing we can get an array of all
the part of speech tags in the input tab
using the tagger function so what is
part of speech tagging it's assigning
grammatical labels to each word in a
given corpus you know all those words we
learned back in elementary school so
take the phrase I saw her face and now
I'm a believer if we tagged each word in
that phrase individually without looking
at the sentence as a whole we might tag
saw as a certain bird which means this
would be a quote from Leatherface but if
we look at this word in the context of
the sentence we realize that it's a
different verb Google trained Parsi by
interpreting sentences from left to
right for each word and the sentence and
the words around it extracted a set of
features like the prefix and suffix put
them into data blocks concatenated them
all together and fed them to a
feed-forward neural net with lots of
hiddenly
which would then predict the probability
distribution over set a possible POS
tags and going in order from left to
right was useful since they could use a
previous words tag as a feature in the
next word so what is a parse tree
well part of speech tagging isn't enough
there's another part the meaning behind
some piece of text isn't just the type
of word that's being used but also how
that word relates to the rest of the
sentence take the example phrase he fed
her cat foo there are three
possibilities of what this phrase could
mean number one he fed a woman's cat
some food that's the obvious one to us
intuitive humans but there's also number
two he found a woman some food that was
intended for a cat or number three he
somehow encouraged some cat food to eat
something the meaning of the sentence
depends on the context of each work the
team used something called the head
modifier construction to sort word
dependencies this generated directed
arcs between words like that and Cat Cat
being a direct object the word debt the
sentence starts out unprocessed with an
initial stack of words the unprocessed
segment is called the buffer at the
parser it encounters words as it moves
from left to right it pushes words onto
the stack then it can do one of two
things
it can either pop two words off the
stack attach the second to the first
which will create a dependency arc
pointing to the left and push the first
word back on the stack or create an arc
pointing to the right and push the
second word back on the stack this will
keep repeating until the entire
sentence is processed the system decides
which way to point the are depending on
the context ie previous POS tagging once
that's done it uses the sequence of
decision to learn a classifier that will
predict appendices in the novel corpus
it applies to softmax function to each
of the decisions which normalizes or
adjust them to a common scale and does
is globally by summing up all the
softmax scores in log space so the
neural net is able to get probabilities
for each possible decision and a
heuristic called beam search helps
decide on the best one when predicting
them once we have our parse tree and
parts of speech variables let's store
the root word and the dependent object
into their own variable we'll call a
synonym API to retrieve a synonym for
the dependent object then construct a
novel sentence that repeats the command
that users entered back to them in
different wording looks like it works
pretty well the scope of what you can do
with this is so that you can use this
text analysis to create a text
summarizer work recognize the intent of
a query or understand if a review is
positive or negative or my personal
favorite create a political debate back
checker links with more info below
please subscribe for more ml videos for
now I've got to go fix a buffer overflow
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>