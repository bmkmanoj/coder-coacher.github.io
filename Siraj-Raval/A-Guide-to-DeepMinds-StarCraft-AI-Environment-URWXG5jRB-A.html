<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Guide to DeepMind's StarCraft AI Environment | Coder Coacher - Coaching Coders</title><meta content="A Guide to DeepMind's StarCraft AI Environment - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Guide to DeepMind's StarCraft AI Environment</b></h2><h5 class="post__date">2017-09-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/URWXG5jRB-A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world its Suraj and today we're
gonna talk about deep mines Starcraft 2
AI environment so recently there
released this Starcraft 2 environment
that lets you train reinforcement
learning models and that's what you're
seeing right now it's a demo of it
happening but basically you can use
Starcraft 2 the game as a testbed to
train and run your AI models on and
these can be reinforcement learning
models that can be deep learning models
that can be really anything they can
just be scripts that are that aren't
even machine learning just like hard
coded BOTS
whatever but the point is that it's it's
meant to be a testbed for people to
train and test their AI models on so
it's a really exciting time right now
for deep reinforcement learning as a
field because open AI beat dota 2
recently in the world champion they beat
the world champion at dota 2 and then
after that deep mine released his
Starcraft 2 AI environment so there's a
lot of exciting things happening right
now in deep reinforcement learning
there's a lot of low-hanging fruit in
this section of machine machine learning
as opposed to supervised learning where
a lot of it has been solved more or less
right gradient based optimization you
compute the gradient and then you update
your weights and you know your labels
it's very you know it's been tried and
done before but for deep reinforcement
learning there's a lot of unanswered
questions so it's a really exciting time
right now what we're gonna do is we're
gonna we're going to run a pre trained
model and what I'm gonna do is I'm going
to on my machine set up and install all
the required dependencies and the script
everything you need basically to go from
zero to having Starcraft 2 running on
your computer with deep mines
environment installed and model a pre
trained model running okay so what the
what the model is is it's called a deep
Q learner I'll talk about what that is
but it's a deep Q learner and it's going
to be running on the collect mineral
shards MIDI game of Starcraft 2 which
means it's just a bot that's collecting
little trinkets called mineral shards
and it'll do this autonomously without
you needing to do anything and so
from there you could you know modify it
or run your own algorithms but once you
have something set up it'll be a lot
easier to to get into the the bottom
things and what am I doing with my hands
okay so let's get to this for a second
okay so first things first what is a
history here so deep mines first attempt
at running game simulations came for
Atari games right they that's what
Google bought them they they created an
algorithm call the deep queue learner
which is it which is the algorithm that
we're running and they use that
algorithm to be any Atari game and the
way they did this is they combine two
different ideas in machine learning they
combined the idea of deep learning which
is all about learning features you don't
have to engineer what those features are
like I'm looking for a dog that has long
ears and it has brown fur no it will
learn what the necessary features are to
map what it sees to some label and so
they use a convolutional neural network
for this to to create features learned
dense representations from game screens
right so all it got were the pixels of
the game the pixels and then it learned
dense representations from those pixels
and then it come and then it converted
what it saw into an output and that
output was an up down left or right
value right anything that you can use on
a joystick for an Atari game and the way
it did this is it didn't just take in
the input from the what I saw in the
game it also used what's called cue
learning it's cue learning is a type of
reinforcement learning where we
initialize what's known as a cue matrix
and the Q matrix has a collection of
possible actions that an agent can take
in a game and all these actions are
weighted like this is an OK action this
is a better action this action could be
the best action and what it does is it
picks an action from the Q matrix using
some strategy that you decide it could
be random it could be based on some pre
weighted value like an Epsilon whatever
but you pick an action from the Q matrix
you perform it in the game you observe
what's happening and then you see if you
got a reward or not right a plus 1 a
minus 1 and then based on that reward
you'll update the Q matrix so that the
actions are all way
differently right and so the idea is
that eventually the Q matrix will have
the best actions for you to perform at
whatever time step you're in right the Q
matrix acts as a weight similar to how
in a neural network the weights improve
over time in reinforcement learning in Q
learning the Q matrix improves over time
so they combined both of those ideas
together right so deep Q learning and
the way it does this is and I've got a
little bit of pseudocode here for how it
works and this is the full pseudocode
for how their algorithm works now keep
in mind that their algorithm wasn't just
a simple you know take a convolutional
Network and then run to learning on top
of it it's also got two different
features from neuroscience that the
first one is called replay memory and
replay memory is essentially a buffer in
memory that's a temporary buffer that
stores states actions and rewards
basically your experience of what what
what has happened in the game and this
basically this improved their AI when
they use this concept of replay memory
that acted as a temporary buffer that
they could pull actions from as well as
the Q matrix so if we look at this
pseudocode we'll see that first it
initializes some replay memory matrix
and initializes a q matrix randomly and
it observes the initial state of the
game where are we in the game and that
would be the the collection of pixels
that it first sees and then it runs the
training loop so in the training loop it
will select an action from the q matrix
either randomly by using some
probability value epsilon or bike
selecting the optimal Q value from the
queue the the optimal action from the Q
matrix that it sees and then it will
execute that action and in open a eyes
universe environment they call this the
step right the environment step function
so it'll execute the action and then
it'll observe the reward that it
receives and it will store that the this
the state the reward the old state and
the action all of those that that those
four values into replay memory and then
the next step is for it to compute a
loss function and we can see the loss
function
here it's also called the bellman
equation okay this is also called the
bellman equation not to get you too
confused but that's just what it's
called as well but basically it will
randomly sample from the replay memory
and then it will use a sample that it
that it retrieved to compute a loss
function and then it will minimize the
square of that loss function at every
iteration and as a loss function is
minimized the Q matrix is values are
improved at every time step which means
that eventually every time the agent
pulls an action from the Q matrix it's
gonna be more and more optimal such that
it's going to minimize a loss which will
maximize the reward it receives I notice
that's quite a mouthful to you know say
in 30 seconds B but that's that's how
the deep Q learner works at a high level
the next step for them was to try it out
on go that the ancient Chinese game of
Go so a lot of AI experts experts said
that it would take 10 years 20 years 30
years for an AI to be able to beat the
game of go because there were so many
possibilities right there's so many
possibilities and the search space is
far too vast for an AI to just
brute-force through all the options
there are so many different combinations
of game states that it's just too hard
for an AI to compute with our with the
limits of computing power that we have
now that was their thinking but they
were wrong because they're always wrong
when it comes to deep learning and all
these new technologies but basically
alphago was their attempt their
successful attempt at beating the game
of Go and they used they used two
different neural networks here they
actually used three they use three
different neural networks they use three
different neural networks here one was
for the policy network and one was for
the value network and both of these
computers two different values one is
the policy and then the next is the
value and it used both the policy and
the value to help it help guide what is
essentially a gigantic tree search and
the tree search is called a Monte Carlo
tree search and here is a brief
description of how it works but
basically the Monte Carlo tree search or
MC TST
simulates an a search tree and the AI
selects an action at each time step
based on the action value and prior
probability which is the output of the
policy network and some exploration
parameter so it uses the values of the
policy network and the value network as
guides to help it search the tree of
possible moves that it can play at every
time step and they trained alphago on
tens of thousands of hours of expert
gameplay and then they gave it
they gave alphago to the world champion
Lisa Dole and it beat Lisa Dole
so beating the game of Go was a much
harder challenge than beating the 20
different atari games given just the
pixel values of the game screen right
but now more recently they decided let's
let's up the ante even more right and
they decided let's not just do this
alone let's let's open source this so
that everybody gets to use this
technology right so Starcraft Starcraft
is arguably one of if not the greatest
PC game of all times
PC fanboys come at me but anyway
Starcraft is one of the best PC games of
all time and it's got hundreds of
thousands of players across the world
it's got people who's a day job is to
just play Starcraft all day
competitively right in South Korea
specifically much close to South Korea
if you're out there but anyway so so
Starcraft 2 huge awesome game if you've
never played it before then this is a
great opportunity to download it it's
free I'll show you a little bit about
that in a second and if you have played
it before this is a great way to help
improve your own strategy right because
when you're building an AI for Starcraft
2 you're thinking about all the things
that requires to be a good Starcraft
player right when you're thinking about
when you should spend your wealth how
you should build your army what you
should invest your resources and time
and energy and all of these things
you're gonna try to want to replicate in
an AI that you built and so if you think
about an AI for StarCraft there it has
to be able to do a bunch of things that
are quite difficult first of all it's
got to have an effective use of memory
right
it's got to be able to remember not just
the things that have that have happened
in the short-term it's got to be able to
remember the things that happened back
in the long-term in the past and not
just in the past it's got to be able to
plan over a long period of time
sometimes you want to make decisions
that help maximize your current value
right you you want to kill an enemy
because the enemy is next to your you
know vulnerable troops but but other
times you want to make an action that is
not as intuitive in the short term but
in the long term it is right like
sometimes you want to spend a lot of
money on some resource and so you're
gonna have a little money right now but
in the long term that resource that you
purchased is gonna help you a lot more
right so it's not as it's not as you
know obviously intuitive as something
like an Atari game where it's just like
all you have to do is defeat you know
get from point A to point B or remove
some block or kill all the aliens on the
screen it's not that simple right
and even as something as even some tasks
as simple as expand your base to some
location is actually pretty complicated
you hope you have to coordinate mouse
clicks your camera available resources
and what this does is it makes actions
and planning hierarchical and this is
generally very hard for reinforcement
learning algorithms to grasp the concept
of hierarchy is quite hard for
reinforcement learning algorithms to
grasp right because you're performing an
action and you're receiving a reward
right so there's the agent environment
loop it's not like a deep learning where
we have all of these layers and there's
all this structure that's built over
time so deep queue learning was one good
example of having a hierarchical
structure a hierarchical model in a
reinforcement learning environment and I
think it was one of the first but we're
gonna see a lot more and a lot of the
key discoveries that are gonna come out
of the field the entire field of machine
learning this year and next year are
going to come from deep reinforcement
learning when some really smart people
combine the ideas that come from deep
learning mainly hierarchical learning
and the
ideas of reinforcement learning from
learning from an environment in real
time andre karpati had a recent talk at
Y Combinator I think it was called why
conf where he said that AG I artificial
general intelligence is going to result
from having simulations right from from
creating an AI that can adapt in a
simulation similar to how we adapt in
the real world and this is a simulation
enter twilight zone music but anyway so
reinforcement learning deep
reinforcement learning super hot field
and this is your chance to get into it
right you don't have to work at deep
mind you don't have to work at open AI
you can just be some kid who has a time
and energy to work on this stuff and if
you have internet access and you have
the time to work on this you too can
make an amazing algorithm and you post
it on github you post it on hacker news
on the machine-learning subreddit you'll
get great feedback you can join some
online research groups on a slack
channel or on several of the forums
online and you can just do great work
and all of this can be added to your
portfolio your github your resume for
future prospects whether that be
studying at a university or working at
one of these fields but the point is in
order to get anywhere you've got you've
got to do something right and Starcraft
2 is a great test fit it's a great set
of tools I've tested it out myself and I
think it's it's it's it's a really great
place to get started with deep
reinforcement learning ok so ok now so
on to the code right so the basically so
it was a joint collaboration with
Blizzard so Blizzard already released an
API that lets a user create scripted
BOTS machine learning based BOTS that
are running from pickle files
you know pre training models replay
analysis and tool assisted human play
and so deepmind's environment its
repository is called PI SC 2 pi
Starcraft 2 so it's all in Python thank
the pods right it's it's in Python but
it has four components to it the first
is the API that it wraps from Blizzard
in Python the next is a data set of
anonymized game replays okay so it's got
a lot of these anonymized game replays
that you can download from right here
I'll go through that in a second
and it's got a a series of simple RL
minigames one of them is what we're
going to do for this demo and to test
out different environments right
to test out the different algorithms I
mean so this was blizzards initial API
and then deepmind wrapped it with their
own Python repository right so what
we're gonna do is we're going to just
set up everything right so I'm gonna go
through these installations that there
are seven steps here and I'm gonna go
through all of them to get started okay
so first of all before you do anything
you've got to download Starcraft to the
Blizzard client it's free you just sign
up on Blizzard and you you select the
starter edition right that's it just to
let the starter Edition and it's free
and you can just download that and then
you can play it just like that it'll
take you know depends on your bandwidth
but it took me about an hour to download
and set up and already I was running
through the tutorial in Starcraft 2
right just an hour for me
so definitely download that and once
you've downloaded Starcraft 2 then move
on to these seven steps so the first one
is to install pi sc2 right and so
luckily they have wrapped it into a nice
little Python library for us so I can go
ahead and install it using pip so I'll
say sudo pip 3 install PI SC 2 ok and
it's downloading and it's got it's
building off of all those dependencies
that right it's it's it's built on top
of the Blizzard API that I just talked
about and and some other things ok so
now that was step one we've installed PI
se2 and so the next step is to install
the sample code so the sample code you
can just clone it directly from my
github just like git clone and it's
gonna download it just like that right
and the sample code contains the
pre-trained model and it contains all
the Python files you need to run this
very simple reinforcement learning bot
and so once you've got that then that's
step two and then step three is to
download the mini games from the
Starcraft 2 Maps
right so we can
click on this link and just like that
here are our maps our minigame Maps okay
and so we can move all of these into our
folder for Starcraft 2 and it's got to
be in the maps folder right here we go
in maps so I'll just copy and paste it
just like that so now my Starcraft 2
application that I've downloaded with
the Blizzard client has these maps and
so once I have these maps now I can
install tensorflow and open AI baselines
oh ok so so right if you don't have
tensorflow you can install tensorflow
with pip 3 or pip install you can
install tensorflow I've already got
tensorflow
and then once you've got tensorflow so
tensorflow is to be able to train and
run these machine learning models and
then you have to install open AI
baselines so baselines is a collection
of high quality reinforcement learning
algorithms right the deep Q Network is
one of them that we're going to be using
and it's got policy gradients which is
another popular reinforcement learning
algorithm basically it's your way of
being able to implement these
reinforcement learning algorithms
without having to code them from up from
scratch and then you can modify existing
ones tweak them to see if you can get
better results so it's a good way to you
know test around with some tests some
different effects okay so then I can
download the baselines environment and
once I've downloaded baselines I've put
my folder I put my maps into my
Starcraft 2 folder then I can go ahead
and open the project with IntelliJ so
the reason I say use IntelliJ for this
and not just open it with sublime or you
know some regular text editor is because
this oh hold on permission error of
course we got to do sudo of course there
we go so the reason I say IntelliJ is
because there's some logs that that are
really nice to to view when it comes to
how your agent is running which is easy
to do with IntelliJ so if you've never
used a an IDE in the integrated
development environment like IntelliJ
this is a great this is a great reason
to do it
so go ahead and download IntelliJ you
can download from here there is a paid
version but there's also a free version
the community version get the community
version so you don't have to get the
paid version and then if you want to
train the model right from scratch then
you can run Python 3 train mineral
shards up pie but what we're gonna do is
we're gonna run a pre trained model so
that we don't have to Train it from
scratch we just want to we just want to
get something to work right so let's go
ahead and just open up that model
okay so IntelliJ is opening right now
and once it's open I can go ahead and
open my project so I'm going to import a
project and where is my project so my
project is in downloads PI SC - so I'll
go to downloads PI I see two examples
alright and then I'll open it from
existing sources finish
you can install plugins to support
Python which I'll do right now so
clearly I'm going to I'm doing all the
steps as if I've never done any of this
before and then I'm going to restart it
to initialize the Python plugin okay
all right so now I've got the the code
in important into IntelliJ and it's
detected a Python framework and I'm
gonna say ok to this and then
and there we have it and now the client
is running in the background it's it's
it's executed Starcraft 2 its detected
that my system has Starcraft 2 installed
and then it's run this script to run a
pre trained model inside of the
Starcraft 2 environment so it acts so
it's able to access the Starcraft 2 game
because the deep because deep mines PI
sc2 repository under the hood it's using
blizzards API it's but it's a local API
so it's not like it's connecting some
more remotely it's connecting to the
game that's right on your desktop or
your laptop in terms of the code the the
model and the environment are built from
us so in this in this code it's got the
deep convolutional network right here as
you can see the parameters are here the
number of hidden layers are here and
then it's got the and then it's got the
it's wrapped that open AI environment
that's step function right so it's it's
giving it it's given it these parameters
and it's combined both the convolutional
network and the q network together to
then train this AI and it saves it as a
pickle file it saves the pre-trained
model as a pickle file and then once
it's trained the pickle file we can we
can access that pickle file to run the
pre-trained model in the deepmind
environment in the Starcraft 2
environment and if we look at this code
it's actually you know it's quite a lot
of code and I can make a different video
to talk about how all the code works but
right now I just wanted to help you
install and configure this script so
that you can run it yourself don't be
afraid to run it it's actually pretty
easy and all in all including
downloading Starcraft 2 downloading the
code installing all your dependencies
it'll take probably an hour and a half
to go from zero to running your own RL
algorithms in this game ok so I hope
that I hope that helped please subscribe
for more programming videos and for now
I'm gonna play some Starcraft 2 so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>