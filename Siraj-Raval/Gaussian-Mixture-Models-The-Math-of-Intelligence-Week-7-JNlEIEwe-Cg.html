<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Gaussian Mixture Models - The Math of Intelligence (Week 7) | Coder Coacher - Coaching Coders</title><meta content="Gaussian Mixture Models - The Math of Intelligence (Week 7) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Gaussian Mixture Models - The Math of Intelligence (Week 7)</b></h2><h5 class="post__date">2017-08-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JNlEIEwe-Cg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and imagine that
you've built a game and you want to
predict whether or not your players are
going to continue playing next month or
not you want to classify your players as
going to turn or not going to turn and
you've got a single data point and that
is the amount of money that that user
has spent in the game this month either
withdrawn or deposited to buy some
in-game currency or digital gold or some
kind of items right and based on that
data point alone you want to predict
whether or not the user is going to
continue playing the game or not now
normally when predicting customer turn
as it's called you want to use several
data points you want to use a time that
the user has logged in you'd want to use
a bunch of different features but for
the simple one-dimensional example we're
just going to use that because what
matters is the model that we're going to
learn today and the model is a Gaussian
mixture model that's what the model is
called and it is a universally used
model for generative unsupervised
learning or clustering which is what
we're going to do it's also called e/m
clustering or expectation-maximization
clustering based on the optimization
strategy that we're going to use which
we'll talk about but the idea is very
simple for a given data set and in our
case is going to be the amount of money
withdrawn or deposited each point is
generated by linearly combining multiple
gaussians okay so that's what we're
going to do and before we start with
this amazing amazing example and code I
want to say one thing okay this is a
message from Suraj this is the message
the message is I had a this is a 30
second message I went and I met
subscribers for this channel I met the
community here in Amsterdam I met 15 or
so people I went to London this past
weekend and I met about 15 or so people
and I was just blown away by the quality
of people that have subscribed to this
channel the community that we have built
is incredible seriously there are people
working for the United Nations there are
people who have met or working to create
a machine learning community in Tunisia
they are there are people who have apps
with 500,000 plus users that are trying
to stop smoking there are people who are
working on
global warming there there legitimately
working I'm using machine learning to
prevent global warming and it is so
inspiring and it is amazing it inspires
me and it increases my conviction to
help you guys get better because it's
not about me you guys are the real
inspiration you are my hero
okay you are my hero for watching this
for giving your attention to machine
learning we we are the most important
community in the world because we have
an interest in the most important
technology in the world we are the most
important community in the world and we
are the ones who are going to solve the
problems in this world using machine
learning in the way that only we can
remember we are smart and we are cool I
have met you guys I'm going to continue
meeting you guys in person I'm going to
India next month I am very excited for
that and I'm just going to meet all you
guys everywhere I'm going to do a world
tour at some point but anyway it's a
very exciting time to be in this field
and we are the we are the real
changemakers okay we are the ones we're
going to solve these problems
so it's amazing that we exist and we are
growing at a rate of 500 to 800
subscribers a day reply to people in the
comments asked for people asked people
for questions in the slack channel we
are growing startups have been built in
the slack channel by the way it is
incredible what is happening it is truly
incredible anyway I was just excited
about my past weekend in London meeting
you guys and I'm going to continue to do
so back to this so we're going to build
a Gaussian mixture model and first of
all what is a Gaussian now I know it's
kind of talked about this before but
look let's go into it a little bit more
a Gaussian is a type of distribution
it's a very popular and mathematically
convenient type of distribution but a
distribution is a listing of outcomes of
an experiment and the probability
associated with each outcome so let's
say we've got some kind of game right
and you can get a score of 1 a 2 3 4 5 6
you could get a set of scores and you
could and then you want to calculate the
frequency or the amount of time that a
person has made this score right so the
tally and the frequency columns are the
same thing and so what we could do is
you can notice that there is a
here that the number nine right so
notice how this kind of follows a bell
curve the frequency goes up and then it
goes down now it's not exact because 7
is greater than 6 but more or less it
looks like it follows a kind of bell
curve the frequencies go up as the
scores go up and then it has a peak
value and then it goes down again and we
can represent this using a bell curve
otherwise known as a Gaussian
distribution and a Gaussian distribution
is a type of distribution where half the
data falls on the left of it and the
other half falls on the right of it so
it's a it's an even distribution and you
can notice just by the thought of it
intuitively that is very mathematically
convenient right and so what do we need
to define a Gaussian or normal
distribution what we need two variables
we need a mean which is the which is the
average of all the data points and
that's going to define the center of the
curve and then we need the standard
deviation which describes how spread out
the data is as well and once we have
that we can write out this curve and
this is used throughout many fields and
you can think about many examples of
where you you know anytime you have some
kind of data that's increasing has a
peak and decreases very likely a
Gaussian distribution would be a great
distribution to model this data whether
it's a car or a roller coaster that's
increasing in velocity reaches a peak
and then decreases or a sound wave
acoustic wave forms there's a lot of
applications of this now so that's a
Gaussian distribution and I'll show you
the formula in a second but this is
multis are multiple Gaussian
distributions right where we have
multiple means and multiple standard
deviations and if we have those we can
write out well we can plot out multiple
gaussians which is what we're going to
do in a second but the formula for a
Gaussian distribution using the mean and
the standard deviation once we calculate
those two things we can write out this
this this formula this this graph and
this is this is a formula for it and
it's called the probability density
function that's what it's called so we
can say for a given point X for a given
point X we can compute the associated Y
which is
which are all of those values remember
in a one-dimensional case we know all
the X values we want to compute the Y
values and the Y values are the
probabilities for those X values so then
given some new x value we can output the
probability of that x value being a part
of the curve or being a part of the data
set or being a part of the class in this
case so we can say 1 over the standard
deviation times the square root of 2
times pi times the natural number e to
the negative x value which is the data
point minus the mean squared over 2
times the standard deviation squared so
this is a function of a continuous
random variable whose integral across a
interval gives the probability that the
value of the variable lies within the
same interval so we have some interval
so that's a Gaussian distribution and
that's the formula for 1 so what is a
Gaussian mixture model well we sometimes
our data has multiple distributions or
it has multiple peaks
it doesn't always just had one peak and
we can notice that we can we can look at
our data set and we can say well it
looks like there are multiple Peaks
happening here there there are two peak
points and the data seems to be going up
and down and up and down twice or maybe
three times or four times but if we
think that there are multiple
distributions Gaussian distributions
that can represent this data then we can
build what's called a Gaussian mixture
model and what this is it is a
probability distribution it's more a
probability you can think of it as just
a probability distribution that consists
of multiple probability distributions
and that are multiple gaussians so for D
dimensions where D is the number are the
number of features in our data set the
gaussian distribution of a vector x
where x equals the number of data points
that we have is defined by the following
equation and right so you can see how
you can substitute D for the number of
features we have this is the for the
mean and so Sigma represents the
covariance matrix of the Gaussian and so
you notice how we're not using the
standard deviation
instead we're using the covariance
matrix when it comes to the Gaussian
mixture model and this is what it would
look like right so in the case of two
dimensions it would look like this it
would give us a a curve for all of our
possible data points and so why do we
use the covariance well the covariance
is a measure of how changes in one
variable are associated with changes in
a second variable right they Co vary
it's not just about independence of
variation two variables and how they
change depend on each other they Co vary
and the covariance will be technical
it's called the variance covariance
matrix is a measure of how these
variables relate to each other and in
that way it's very similar to the
standard deviation except when we have
more dimensions the covariance matrix as
opposed to the standard deviation when
we plug it in here gives us a better
more accurate result and so the way we
compute the covariance matrix I talked
about this before when we talked about
eigen vectors and eigen value pairs but
if number the parameters that it uses
are the number of scores in each of
those data sets the variance the
covariance and the where C is the size
of the the number of columns in the data
right it's a column by column matrix I
will compute this in a second but the
probability given in a mixture of K
Gaussian where K is a number of
distributions that we that we believe
that there are in our case we believe
that there are going to be two okay
let's just say we think in our data set
there going to be 2 so 4 K gaussians 2
in our case we're going to multiply W
which is the prior probability of the
J's Gaussian times the value that we
just looked at up here right here where
this is the Gaussian distribution of the
vector of our of our data points and so
once we have this value then we can say
we can multiply it by W where for each
for each of our gaussians and that is
going to give us our probability value X
for a given X data point and so this is
what it looks like
right so if we were to plot multiple
Gaussian distributions it would be
multiple bell curves but what we really
want is a single continuous curve that
consists of multiple bell curves and so
once we have that huge continuous curve
then given our data points it can tell
us the probability that it's going to
belong to a specific class okay and so
what class can how do you tell what
class something is given this curve well
let me go to the bottom here once we
have that best fit mixture notice it's
this this image right here we can tell
what class it's in by where it falls on
the curve for the one dimensional case
so this will be class 1 this would be
class 2 this would be class 3 like that
now in the two dimensional case or three
dimensional case it's easier because
there are circles right like k-means
clustering right there circles and
that's what they that's what they come
out to if we were to measure probability
values just like this we're not just one
feature like we're doing for for us but
multiple features like two features so
if you have probability values for what
the y-axis which is a second feature set
like maybe the the number of other users
that this person has friended and then
this is the probability of the you know
the amount that the user has either
deposited or withdrawn then we can draw
these cluster circles but for the one
dimensional case it looks like that a
bell curve and so the problem that we're
trying to solve here with the GMM is
given a set of data points X let's draw
from an unknown distribution which we're
going to assume intuitively is a
Gaussian mixture model that is that the
data set consists of multiple gaussians
estimate the parameters of theta which
consists of the mean and other values
that we'll talk about of the GMM model
that fits that data and the solution the
way we find these these parameters of
our model which is what machine learning
is all about mathematical optimization
is by maximizing the likelihood P of X
the probability of x given our
parameters and x is the data point that
we want to predict the probability the
class the we want to maximize the
likelihood
that X belongs to a particular class we
want to find the way we do that is to
compute the maximum likelihood estimate
which is which is this equation right
here we want to find the maximum
probability value for a given class so
what that is we want to find the class
that this data point X is the most
likely to be a part of and so notice how
so I also added this image to show how
you know given in the two-dimensional
case these distribution bell curves come
out to circles if you if you have
distributions for not just one axis or
one dimension but two dimensions so it's
it's very similar to k-means that we've
talked about the k-means algorithm it
uses the same optimization strategy
which is the expectation maximization
algorithm it's similar to k-means and
that k-means k-means finds k to minimize
X minus the mean squared but the
Gaussian mixture model finds case
minimize X minus the mean squared over
the standard deviation over and the
reason that we add the standard
deviation into the mix is because in the
denominator the standard deviation
squared takes variation into
consideration which it when it
calculates its measurement what does
this mean this means that the k-means
algorithm gives you a hard assignment it
either says this is going to be this
data point is a part of this class or
it's a part of this class now this is
great in a lot of cases we just want
that hard assignment but in a lot of
cases it's better to have a soft
assignment instead we want the maximum
probability okay this is going to be 70%
likely that it's part of this class
class a but we also want the probability
that it's going to be a part of other
classes right it's not just a single
output it's not a it's not a discrete
output instead it is a continuous output
it is a list of probability values right
it could be a part of multiple
distributions it could be in the middle
it could be 60% likely this class of 40%
likely this class not just Class A and
so that's why we incorporate that's why
we incorporate the standard deviation
so so how is this thing optimized well
it's optimized using the
expectation-maximization algorithm so
the basic ideas of the en algorithm or
to introduce a hidden variable such that
it's knowledge would simplify the
maximization of the likelihood so we
pick some random data points right we
pick some random data points we draw a
distribution around that data points we
then estimate up the print we didn't
update our parameters using that
generated distribution and if we repeat
the process and every time we draw a new
data point it's going to be closer and
closer to the data point that best fits
the data set that we have such that if
we were to draw a distribution around
that data point it would fit that it
would fit that data set the best and so
there are two steps there's the e step
the expectation step and the expectation
step is to estimate the distribution of
the hidden variable given the data and
the current value of the parameters and
then the M step of M step the
maximization step is to maximize the
Joint Distribution of the data and the
hidden variable so that's the high level
and we're going to talk about the
implement date implementation details in
the code but you might be thinking well
wait a second wait a second what about
gradient descent you might be thinking
about that because we've talked about
gradient descent and the entire industry
loves gradient descent and here's the
thing you can obtain the MLE using
gradient descent but what is the deal
with gradient descent gradient descent
is using the the derivative you are
computing the derivative of a node right
a node in some kind of graphical model
you're computing the derivative and what
the derivative tells you is the
direction it tells you the direction
that your data wants to move in what
direction should you move the parameters
data of your model such that the
function your model is optimized to fit
your data but what if you can't compute
a gradient what if you can't compute a
derivative you can't compute a
derivative of a random variable right
this this Gaussian mixture model has a
random variable it is a stochastic model
that is
it is non-deterministic you can't
compute the derivative of a random
variable that's why we're not using
gradient descent in this case because
you can't compute the derivative of a
random variable okay so there are
actually ways to compute there are
actually ways to compute the derivative
of a stochastic model like a variational
auto encoder but it's a trick and if you
want to know more about that I've got a
great video on that search variational
auto-encoders Siraj on youtube that'll
be the first link so back to this so
when should you use this thing right
when is this actually useful besides cut
some return anomaly detection think
about any case where you are trying to
cluster data where you are trying to
classify data that does not have labels
so one use case is trying to track an
object in a video frame right if you
know the moving objects distribution in
the first frame we can localize the
object in the next frame by tracking its
distribution that is you know kind of
the probability distribution of all the
possible ways that the subject can move
and based on that you can create a
bounding box around the subject such
that in the next frame it's very likely
that that subjects movement will fit
into the bounding box I have some
related repositories here for using
tensor flow and a Gaussian mixture model
to classify song lyrics by genre
very cool use case using tensor flow
definitely check it out is called word
to Gaussian mixture very cool check that
out it's got some great instructions and
I've got one more which is a great
general-purpose tutorial for Gaussian
mixture models it's a Jupiter notebook
definitely check that out as well and
I've got some great links for you in the
description all right so check that out
as well got the great graphs and
everything so let's let's look at this
code okay so in this code we're going to
import our dependency and then test out
a distribution graph let's just see if
we can draw out a distribution
orthogonal or unrelated to our data set
just to see if we can do that so we're
going to import for dependencies here
the first one is not plot live which is
our handy dandy
plotting tool and the next one is numpy
which we always use from a
tricks math then we've got SCI pi which
is going to help us normalize our data
and compute the probability density
function which we talked about earlier
right we should I showed you the
equation for that and then we have one
more Seabourn which is also going to
help with plot specifically colors okay
so once we have our four dependencies we
can go ahead and start this so the first
step is for us to plot out a Gaussian
distribution let us see if we can do
that first so we've got let's just say
we're going to use numpy x' linspace
function to return an evenly spaced set
of numbers over a specified interval
which is a negative 10 to 10 we have a
thousand of these data points that we
want to generate and then look at we
have our real data set in a second but
this is just for the sake of an example
and so then we want to create a normal
continuous random variable and that's
what the stats norm PDF the probability
distribution function function is going
to help us do and so we say LOC equals 0
and this specifies the mean so a mean of
0 and scale is 1.5 and that's the
standard deviation so we're going to
draw a Gaussian distribution using those
two parameters we're going to plug those
in into the probability density function
and it's going to output the Y values
for all of our X values right we gave it
our X values it's going to plot out all
of our Y values and to plot it out
always to do is say use map plot live to
plot out all of our x values and all of
our computed Y values and then there it
is so we can write out a Gaussian
distribution easily we can do that ok so
let's go to let's go into our date our
data set we're also going to import
pandas and we're going to read our data
set here it's a CSV file and we can read
it into a panda's data frame object
which is very easy to manipulate in
memory once we have it there and we want
to show the first 5 examples and here we
are we have the first 5 examples okay so
for each of these our users each of
these are different players and the
amounts that they that player has spent
in a given month this month in Bitcoin
okay in bitcoins or it's normalized any
currency it doesn't
what currency is in but it's in Bitcoin
okay and so it's either a positive
number it's a negative number that means
that the user withdrew that amount but
the idea is that that's it that's we've
got one feature so it's a
one-dimensional problem and we're going
to write out a probability distribution
for that single feature and if we do
that if we do that then we're going to
be able to predict the class of a given
user based on how much this person this
user has either spent or withdrawn
whether or not they're going to turn or
not okay so that's our data set and now
we're going to show the distribution of
the data as a histogram so let's let's
show it as a histogram as well so this
is where Seabourn is going to come in
we're going to say Seabourn do create a
distribution plot given our data that we
that we've loaded into memory with
pandas we want 20 bins of this data and
let's plot it out ah let's see
this clot
okay so that's the distribution that's
the histogram of our data right so for
all of those number of players this is
what it looks like so we could look at
this and we could say oh okay so it
looks like one Gaussian might fit this
but two looks better let's let's see
let's try out one see okay so that's I
mean clearly two two distributions would
be better than one right you can see two
peaks here so two distributions would
fit this data better than one so that's
why we're going to use a Gaussian
mixture model instead of a single
Gaussian right so we want one that a
continuous bell curve a continuous curve
that consists of two bell curves so to
define this model to two normal
distributions are going to have five
parameters four of them are the the
first four are the mean and the standard
deviation for each of those
distributions one and two and the fifth
one is the probability of choosing one
of them and so the way we're going to
write out this Gaussian mixture model
where theta equals those four values and
the probability of choosing one of them
W is just like this and this is the
probability density function for a
Gaussian mixture model consisting of two
gaussians now we look at the probability
density function for a single Gaussian
and it looks like this right here and
this is what it looks like for two
distributions right right we get it so
far so now we're going to fit this model
that's our model right all machine
learning models are functions there are
functions or we initialize those
parameters randomly or using them some
kind of you know smart sampling method
or something but we initialize those
parameters very stupidly and then we
learn what the best or most optimal ones
are and in neural networks and a bunch
of convex optimization problems we
generally use gradient descent because
we can compute the derivative of
deterministic nodes but this is a
stochastic model so we're going to use
the popular expectation maximization
algorithm which is a two-step process
we first perform e which is to update
our variables and then M which is to
update the hypothesis what
me well this is an iterative method for
finding the MLE that estimate the
parameters in statistical models so we
start with some initial values perform
the expectation step and then the
maximization step check of its converge
or not by some threshold that we define
and if it has been to continue
iteratively and if it has stopped so the
expectation step given the current
parameters of the model estimate a
probability distribution maximization
step given the current data estimate the
parameters to update the model and
repeat so more formally using the
current estimates for the parameters
create a function for the expectation of
the log likelihood and then for
maximization compute the parameters
maximizing the expected log likelihood
found on the East step so the M
parameter estimates are used to
determine the distributions of the
latent variables in the next step so e/m
is trying to maximize the following
function which is what we define the
probability density function up here
where X is a directly observed variable
that is our data point theta are all
those parameters as five parameters of
our model and Z is not directly observed
that is a latent variable that is the
random value that we plot right to
continue and get more and more smart it
gets more and more optimal where we plot
it as our as our model learns okay and
we see is a joint distribution on X so
it's a distribution between the latent
variable that we've plotted and the
gauss gaussian that we've already
plotted so there are four steps if we
think about it there are four steps we
first initialize the parameters of our
model theta and then we compute the best
values for Z given theta and then we use
the computed values of Z to compute a
better estimate for theta and then
repeat so in another way another way to
say this and I'll show you a visual way
in a second but we want to initialize
the parameters of the model either
randomly or usually randomly and then we
find the posterior probabilities of the
latent variable given a given the
current parameter values and the N step
is to re estimate the
parameter values given the current
posterior probabilities and we repeat
that process again so check this out
visually this is what it looks like so
we have two gaussians so in this case we
have probabilities for two features so
these are circles instead of those
curves right instead of those bell
curves and so we have two gaussians and
they're just randomly we generated them
and we have our data points so what we
do is we say for each of those data
points which Gaussian generated it so we
give them all probabilities for each
Gaussian so that's the Estep for each
point here estimate the probability that
each gaussian generated in our case we
have two gaussians right so we're going
to generate the we're going to estimate
the probability that each data point is
from the first a a and then the second B
okay and so when we have that then we
compute M the M step is to modify the
parameters according to the hidden
variable to maximize the likelihood of
the data and the hidden variable and
that's it and so every time we do that
these Gaussian curves these Gaussian
distributions these clusters are going
to be more and more optimal to fit the
data where they need to to classify to
to distinguish between classes so back
to our data so let's define what a
Gaussian looks like right so we so will
give a galaxy in its own class and so
Gaussian is initialized by the mean and
the standard deviation so we calculate
that first from our data points and we
can do that very easily right for the
mean it's just add all the numbers up
and then divide by the number of them
and for that the standard deviation it's
the formula for that is look up there
that's the formula for it okay so back
to this so once we have those we can
compute the probability density function
that we looked at up there the
probability distribution s'ti function
is this right so we can compute this
programmatically which I'm going to do
right now so programmatically that looks
like this right
just like that same thing and so we can
say let's find the Gaussian of best fit
for our data so we have our data in a
data frame and let's find the Gaussian
of best fit and so this B are our ideal
means and our ideal standard deviations
for our data but that's not enough we
want to write it's not enough to just
have one that's too easy
there's no expectation maximization
algorithm happening there we don't
really need it but if we were to plot
out this Gaussian very easy it's a
single Gaussian but it's not it's not
well fit to the data we want two of them
right so we're going to keep going here
and this is where the expectation
maximization algorithm comes into play
so first we initially we randomly
assigned K Cups or centers in our case K
are the number of gaussians right now
k-means K where they are the number of
gaussians - and then we iterally refine
these clusters or you know bell curves
based on the two steps for the
expectation step we assign each data
point x to both clusters the probability
with the following probability and the
maximization step is to estimate to
create an estimation of the model
parameters given those probabilities and
then repeat them such as the
probabilities are our are maximized for
each class so then we'll create a class
for our Gaussian mixture model we've
already created a class for our Gaussian
but here it for our Gaussian mixture
model so for a mixture model where there
are two gaussians we don't just need one
mean and one standard deviation we need
four in fact we need five parameters
right we the mean and a standard
deviation for one Gaussian we need a
mean and a standard deviation for the
second Gaussian and we need our data set
as well as well as the mix which is the
initial W value remember the W value and
so we'll initialize both of them will
have initial so that's why we created at
first Gaussian class so you can easily
initialize two gaussians just like that
using both means and Saturday
Asians as well as the W value right
which is going to be updated over time
the weight value and the weights define
how mixed these distributions are like
right the how mixed both of them are so
now for the estep and the m-step so
basically we make initial guesses for
both the assignments a point to the
distribution and their parameters and
then proceed iterally so once we
initialize the assignment and parameters
we can alternate between the expectation
and the maximization steps until our
estimates converge that means that they
do not change much between iterations
for a mixture of gaussians and this is
similar to convergence in the k-means
right which I talked about I think a few
a few lessons ago k-means clustering
Siraj search that on youtube so for the
east step we we first initialize the
log-likelihood at 0 which we're trying
to maximize and then we say for each of
our data points compute the unnormalized
weight values using both of our data
points and these are the probability
values and then we compute the
denominator by adding both of them
together we normalize them by dividing
both of those weights by the denominator
and add them both into the
log-likelihood
so we compute the log of the sum of both
of those values and that gives us the
log likelihood and we return the weight
to both at our are our parameters of our
model right and so once we have that
we'll we'll we'll use those as
parameters for the maximization step
right this is we want to optimize those
weight values so we'll compute so we'll
compute denominators of our weights
right so zip tells us the absolute
values of our weight so it's all
positive which just makes things a
little more pretty mathematically for
for both of them and then we'll take the
sum of all of those values both on the
left and the right side and we'll
compute new means and new standard
deviations for our new distributions
right we're updating them every time
we're maximizing the likelihood that
they belong to the correct class every
time and then a new weight value so that
is our update step our maximization step
is our
update step okay and so the probability
density function is going to be that
function that I talked about up there
right when it comes to two gaussians not
a single Gaussian and so so for the
fitting process now that we know these
we can say well we let's write it for
five iterations on our data set using
the class that we just built let's train
this thing so what does training look
like train training looks like we'll
take our Gaussian mixture model where we
have fitted fitted it to our data set
well well that we've given our data set
to and we'll say okay let's try anything
so we'll iterate if the log-likelihood
if the current log likelihood is better
than the best cycle log likelihood then
set the best log likelihood to the
models log likelihood and then set the
best way value to the current weight
value okay and so we do that for as many
iterations as we can as we as we defined
five of them and so once we've done that
notice how our means and our standard
deviations and our weight values the
fifth parameter are all being updated
every time and that is the expectation
maximization process and so once we've
done that then we can look at the
mixture what is what does it look like
and so this is this is the end result
right this is the end result so we have
one distribution to rule them all Lord
of the Rings that we've got a single
distribution and once we have this
distribution we can then predict given
some new data points right some new
spending amounts what is the likelihood
and what is the likelihood that it's
going to be a part of a specific class
in our case it's going to be one of two
classes is there going to spend is
person going to spend or not and so
that's what it looks like and if we had
more if we had more features we could
turn this into a clustering problem with
circles which would make it a lot easier
to look at
okay so what have we learned here let's
summarize a little bit about what we've
learned Gaussian mixture models take our
old friend the Gaussian and had other
gaussians right plural sometimes we get
out up to as many as we'd like and it
allows us to model more complex data
where there are multiple peaks and
valleys and we fit it using the
expectation maximization algorithm the
e/m algorithm very popular x2 very
popular optimization strategy and it's a
series of steps to find good parameter
estimates where there are latent
variables so we initialize the parameter
estimates randomly given the current
parameter estimates find the minimum log
likelihood for Z which is the data plus
the latent variable the joint
probability distribution and given the
current data find the better parameter
estimates and repeat that process over
and over again and the distributions
that are going to be really bad at first
and slowly they're going to converge and
they're going to fix our data set
perfectly okay and so this can be used
beyond Gaussian mixture models instead
think about whoa in this case you have
to guess the number of gaussians right
you have to guess well I think there's
going to be two by looking at your data
but what if you didn't have to guess
then well that in in that case we could
use kernel density estimation but that's
for another time before we get there if
we had to have built the intuition
around Gaussian mixture models a great
model if you know the shape of your data
more or less intuitively see you next
week
please subscribe for more programming
videos and for now I've got to fit my
curves so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>