<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Win Slot Machines - Intro to Deep Learning #13 | Coder Coacher - Coaching Coders</title><meta content="How to Win Slot Machines - Intro to Deep Learning #13 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Win Slot Machines - Intro to Deep Learning #13</b></h2><h5 class="post__date">2017-04-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AIeWLTUYLZQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world is Suraj and today we're
going to implement a popular
reinforcement learning technique called
policy gradients to help us choose which
slot machine to play that will give us
the highest winning we talked a good bit
about supervised learning it's where we
build an algorithm based on both input
and output data which is great for
classification tasks unsupervised
learning is when we don't have a history
of outputs we have to build an algorithm
based on input data alone reinforcement
learning is similar in that it is also
building an algorithm based on input
data alone but we frame the problem in a
different way the algorithm presents a
state that depends on the input data and
it's either rewarded or punished via an
action that it takes and this continues
over time it learns from the reward or
punishment and continually updates
itself over the long term to maximize
the reward reinforcement learning
doesn't get as much love these days from
the AI community as the other to do
because some of the most interesting
problems right now like those in speech
recognition NLP and computer vision are
areas where it's hard to define the
notion of a long term reward
don't-ee a reinforced currently the
problems that are best solved by
reinforcement learning are either toy
problems like getting an npc to get
through a level without dying or really
complex problems like self-driving cars
or folding laundry or understanding my
ex-girlfriend things that you could do
in a simulation human level tasks so a
lot of the work in reinforcement
learning is theoretical instead of
application based which is just as
important don't get me wrong however
there are some real-world use cases
today
Finnair the flight booking company for
example uses RL to decide what action to
take for what customers to increase
their lifetime value and deepmind reduce
Google's cooling center bill by 40% are
using RL to decide the most efficient
energy routing strategy for the lowest
cost so let's take a look at our problem
which comes from probability theory the
problem is that a gambler which we'll
call our agent has to decide which slot
machines to play how
time to play each machine and in which
order to play them every time that a
machine is played it outputs a reward
value which is randomly generated from a
probability distribution specific to
that machine the goal of the gambler is
to maximize the sum total of rewards
earned through a sequence of lever poles
he italy plays one lever per round and
observe the Associated reward to do this
this is called the multi-armed ended
problem
the Bandit being the name of an
old-style slot machine with an arm or
several on the side that you pulled out
the agent has to make a choice between
using machines that are known to produce
good results exploitation and trying out
new machines that have unknown results
but could give a results than the others
exploration exploitation is optimizing
decisions based on existing knowledge
and exploration is it something to
acquire new knowledge it's a trade-off
that all reinforcement learning agents
may when optimizing for a reward value
and it's particularly relevant in this
problem so let's start initializing some
values after importing tensorflow
and numpy the only two libraries we'll
need to use we can define our end it
will be using a forearm standard that is
one slot machine with four levers and we
can refer to each arm as abandoned so
we'll define our bandits as a list and
each of these values will help decide if
a reward is given when pulled the lower
the vended number the more likely we'll
get a positive reward the higher the
bandit number the more likely we'll get
a negative reward we want our agent to
choose the bandit that will give that
reward and we'll initialize a variable
for storing the total number we'll next
define a pulled bandit function which
given a bandit value will first generate
a random number from a normal
distribution with a mean of zero then
compare the parameter value to the
generated number and any one the result
it will either return a positive or
negative reward in practice this model
is used anytime you have a project with
a fixed budget it can be used to help
best allocate resources to maximize
success since it's specifically designed
to deal with the uncertainty about the
difficulty and payoff of each
possibility Porsha had this thing in
college our agent needs to learn which
kind of reward it gets for each
or action so that it can choose the
optimal ones it's learning a policy so
we're going to use a popular method
called policy gradient to solve this
we'll use a simple neural net that
learned bit policy for picking the best
actions and adjusting its weights
through gradient descent using real-time
feedback from the environment since
we're only using a single bandit our
agent ignores the state of the
environment just like the US government
there's only ever a single unchanging
state if we were introducing multiple
bandits then our agent would need to
take state into account when deciding an
action we would learn a value function
instead but let's keep it simple with a
single state William for what the policy
so actions or quality our policy
gradient network consists of a set of
weights and each weight corresponds to
each of the possible arms to pull and
represents how beneficial our agent
thinks it is to pull each arm we'll
initialize the weights to one which
means our agent will be optimistic about
each arms potential reward when we
update our network we'll use what's
called an epsilon greedy policy this is
a way of selecting random actions with
uniform distribution from a set of
available actions using this policy
either we can select random actions with
epsilon probability or we can select an
action with 1 minus epsilon probability
that gives maximum reward in a given
state we'll define epsilon as 0.1 it's
the chance of taking a random action
basically noting the time our agent will
choose the action that corresponds to
the largest expected value but sometimes
with a probability it will choose
randomly so this way you can try out
different arms to continue learning
about them our agent is the neural
network it's feed-forward and only has
one set of weights we'll initialize them
as a tensor where each of a set of 1 for
the number of bandits then we use the
Arg max function to choose the weight
with the highest value and store that as
our chosen action we now need to
establish what this training process
looks like since we want to feed the
reward and chose an action into the
network to compute the loss and then use
that to update the network will
initialize tensorflow placeholders for
both the reward and the action values
next we'll define the responsible way
it corresponds to the units in the
output layer which corresponding to the
chosen action when updating the policy
we want to update the likelihood of the
actions we actually took as opposed to
all possible actions so this will be a
slice of our waste and we can define the
size of it as well so this is what our
policy loss equation looks like this is
what we want to minimize this character
is the policy which we take the log of
and a is the advantage this is a
critical part of RL it's a measure of
how much better an action was than some
baseline there are different ways of
deciding what that baseline is and it
can get pretty interesting but right now
we'll just set it to zero so we can just
think of it as just a reward we received
for each action this loss function lets
us increase the wait for actions that
give a positive reward value and
decrease them for actions that give a
negative reward value when we define our
loss function programmatically we can
see that it corresponds to the equation
where reward holder is the advantage we
can then optimize it with gradient
descent and a given learning rate when
we minimize our loss it will return a
gradient update so for our training step
we'll initialize a TF graph then for a
given number of episodes we'll either
try a random action or choose one from
our network exploitation versus
exploration will receive a reward from
our action of picking one of their
bandits then we'll update the network
using our gradient weight values
relevant to the action and all weight
values we can use a feed dict to feed in
both the action and the reward we want
to see which reward and which bandit we
are on during each iteration so we'll
print them out when we compile this we
can see that bandit force values
increase way faster than the other ones
it decides one is best and then goes
all-in on it we can extend this codes
later on so that both state and action
of Beck reward which would be considered
the contextual bandit problems so let's
go over what we've learned reinforcement
learning is usually applied to toy
problems or really complex problems
policy gradient methods are a type of RL
technique that optimizes a policy with
respect to the expected long-term return
using gradient descent and we can apply
this strategy to the popular multi-armed
bandit problem
which asks how to best allocate
resources to maximize success the coding
challenge winner for this video is Mike
McDermott might improve the bleeding
edge memory network model from my last
video to create a Q&amp;amp;A chat bot by adding
a bi-directional LTM and time
distributed end player to it this is
seriously amazing stuff he could publish
as a results to a journal wizard of the
week and the runner-up is Michelle Bochy
who also had publishable results and you
can run his code right from the command
line you guys blew my mind and I vow to
you the coding challenge for this week
is to use policy gradients to solve the
pretextual bandit problem so States is
taken into account details are in the
readme github links go in the comments
and winners are going to be announced
next week please subscribe and for now
I've got to maximize my arm size so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>