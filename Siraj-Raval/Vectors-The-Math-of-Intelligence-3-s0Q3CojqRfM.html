<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Vectors - The Math of Intelligence #3 | Coder Coacher - Coaching Coders</title><meta content="Vectors - The Math of Intelligence #3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Vectors - The Math of Intelligence #3</b></h2><h5 class="post__date">2017-06-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s0Q3CojqRfM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and what's the
deal with vectors you're going to see
this word a lot in machine learning and
it's one of the most crucial concepts to
understand a huge part of machine
learning is finding a way to properly
represent some data sets
programmatically let's say you're a
manager at Tesla and you're given a data
set of some measurements for each car
that was produced in the past week each
car on the list has three measurements
or features its length width and height
so a given car can then be represented
as a point in three-dimensional space
where the values in each dimension
correlates to one of the features we are
measuring this same logic applies to
data points that have 300 features we
can represent them in 300 dimensional
space while this is intuitively hard for
us to understand as three dimensional
beings machines can do this very well
this data point X is considered a vector
a vector is a one-dimensional array
think of it as a list of values or a row
in a table a vector of n elements is an
n-dimensional vector with one dimension
for each element so for a four
dimensional data point we can use a one
by four array to hold its four feature
values and because it represents a set
of features we call it a feature vector
more general than a vector is a matrix a
matrix is a rectangular array of numbers
and a vector is a row or column of a
matrix so each row in a matrix could
represent a different data point with
each column being its respective
features less general than a vector is a
scalar which is a single number the most
general term for all of these concepts
is a tensor a tensor is a
multi-dimensional array so a first order
tensor is a vector a second order tensor
is a matrix and tensors of order three
and higher are called higher order
tensors so if a one D tensor looks like
a lineup who are you I think they get it
but I think they get it you could
represent a social graph that contains
friends of friends of friends as a
higher order tensor this is why Google
built a library called tensor flow it
allows you to create a computational
graph where tensors created from
datasets can flow through a series of
mathematical operations that optimize
for an objective and why they built an
entirely new type of chip called ATP you
or tensor processing unit as
computational power and the amount of
data we have increases we are becoming
more capable of processing
multi-dimensional data vectors are
typically represented in a multitude of
ways and they're used in many different
fields of science especially physics
since vectors act as a bookkeeping tool
to keep track of two pieces of
information typically a magnitude and a
direction for physical quantity
for example in Einstein's general theory
of relativity the curvature of
space-time which gives rise to gravity
is described by what's called a Riemann
curvature tensor which is a tensor of
order four so badass so we can represent
not only the fabric of reality this way
but the gradient of our optimization
problem as well during first order
optimization the weights of our model
are updated incrementally after each
pass over the training data set given an
error function like the sum of squared
errors we can compute the magnitude and
direction of the weight update by taking
a step in the opposite direction of the
error gradient this all comes from
linear algebra algebra roughly means
relationships and it explores the
relationships between unknown numbers
linear algebra roughly means line like
relationships it's the way of organizing
information about vector spaces that
makes manipulating groups of numbers
simultaneously easy it defines these
structures like vectors and matrices to
hold these numbers and introduces new
rules on how to add multiply subtract
and divide them so given two arrays the
algebraic way to multiply them would be
to do it like this
and the linear algebraic way would look
like this
we compute the dot product instead of
multiplying each number like this the
linear algebraic approach is three times
faster in this case any type of data can
be represented as a vector images videos
stock indices text audio signals dougie
dancing no matter the type of data it
can be broken down into eight sets of
numbers the model is not really
accepting the data it keeps throwing
errors let me see I looks like you got
to vectorize it what day is the model
you wrote expect the tensor of a certain
size as its input so we basically got to
reshape the input data so it's in the
right vector space and then once it is
we can compute things like the cosine
distance between data points and
the vector norm if their apartment
library through though you gotta love
numpy vectorization is essentially just
a matrix operation and I can do it in a
single line awesome well you vector yup
I've got a bad propagated out for today
cool work you can debate all right yeah
yeah yeah a researcher named McCullough
used the machine learning model called a
neural network to create vectors for
words word to Beck given some input
corpus of text like thousands of news
articles it would try to predict the
next word in a sentence given the words
around it so a given word is encoded
into a vector the model then uses that
vector to try and predict the next word
if it's prediction doesn't match the
actual next word the components of this
vector are adjusted each words context
in the corpus acts as a teacher
sending error signals back to adjust the
vector the vectors of words that are
judged similarly by their context are
iteratively nudged closer together by
adjusting the numbers in the vector and
so after training the model learns
thousands of vectors for words give it a
new word and it will find its associated
word vector also called word embedding
vectors don't just represent data they
help represent our models to many types
of machine learning models represent
their learnings as vectors all types of
neural networks do this given some data
it will learn dense representations of
that data these representations are
essentially categories akin to if you
have a data set of different colored eye
pictures it will learn a general
representation for all eye colors so
given a new unlabeled I picture it would
be able to recognize it as an eye
vectors good once data is vectorized we
can do so many things with it
a trained word Tyvek model turns words
into vectors then we can perform
mathematical operations on these vectors
we can see how closely related words are
by computing the distance between their
vectors the word Sweden
example is closely related to other
wealthy northern European countries
because the distance between them is
small when plotted on a graph war
vectors that are similar tend to cluster
together like types of animals
associations can be built like Rome is
to Italy as Beijing is to China and
operations like performing hotels plus
motel gives us Holiday Inn incredibly
vectorizing words is able to capture
their semantic meanings numerically the
way we're able to compute the distance
between two vectors is by using the
notion of a vector norm a norm is any
function G that map's vectors to real
numbers that satisfies the following
conditions the lengths are always
positive the length of zero implies zero
scalar multiplication extends lengths in
a predictable way and distances add
reasonably so in a basic vector space
the norm of a vector would be its
absolute value and the distance between
two numbers like this usually the length
of a vector is calculated using the
Euclidean norm which is defined like so
but this isn't the only way to define
length there are others you'll see the
terms l1 norm and l2 norm
used a lot in machine learning the l2
norm is the Euclidean norm the l1 norm
is also called the Manhattan distance we
can use either to normalize a vector to
get its unit vector and use that to
compute the distance computing the
distance between vectors is useful for
showing users recommendations both of
these terms are also used in the process
of regularization we train models to fit
a set of training data but sometimes the
model gets so fit to the training data
that it doesn't have good prediction
performance it can't generalize well to
new data points to prevent this
overfitting
we have to regularize our model the
common method to finding the best model
is by defining a loss function that
describes how well the model fits the
data to sum things up feature vectors
are used to represent numeric or
symbolic characteristics of data called
features in a mathematical way they can
be represented in multi-dimensional
vector spaces where we can perform
operations on them like computing their
distance and adding them and we can do
this by computing the vector norm which
describes the size of a vector also
useful for preventing overfitting the
wizard of the Week award goes to Vishnu
Kumar he implemented both gradient
descent and Newton's method to create a
model able to predict the amount of
calories burned for cycling a certain
distance the plots are great and the
code is architected very legibly check
it out amazing work Vishnu and the
runner up for the last-minute entry is
Hamad Shaikh love to have details your
notebook was this week's challenge is to
implement both l1 and l2 regularization
on a linear regression model check the
github readme in the description for
details and winners will be announced in
a week please subscribe for more
programming videos and for now I've got
to not be normalized so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>