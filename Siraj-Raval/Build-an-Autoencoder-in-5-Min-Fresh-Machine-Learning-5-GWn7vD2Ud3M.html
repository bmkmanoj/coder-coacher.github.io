<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build an Autoencoder in 5 Min - Fresh Machine Learning #5 | Coder Coacher - Coaching Coders</title><meta content="Build an Autoencoder in 5 Min - Fresh Machine Learning #5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build an Autoencoder in 5 Min - Fresh Machine Learning #5</b></h2><h5 class="post__date">2016-07-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GWn7vD2Ud3M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">is generating novel data pulao world its
Suraj and today we're going to learn
about a special type of neural network
called an autoencoder then we're going
to implement our own auto encoder using
tensorflow
to generate handwritten digits an auto
encoder is a simple type of neural
network with only three layers it's got
an input layer a hidden layer and an
output layer just like your mom just
kidding
what makes an auto encoder special is
that the output neurons are directly
connected to the input neurons and the
goal is to get the output values to
match the input values so if I added an
image of a dog to the input layer the
output layer would then output that same
image when we input that image it's a
vector in n-dimensional space which is
sent to the hidden layer after some
activation function is applied to it
this reduces it to an M dimensional
space so there are less dimensions this
process is called dimensionality
reduction and it happens in every neural
network so we can think of the first
layer of the network as the encoder
since it's compressing data and the last
part as a decoder since it tries to
reconstruct the original input from the
smaller representations in the hidden
layer so when do we use auto-encoders
all day all day well one use case is
data compression kind of like creating a
zip file for some data set that you can
later unzip another is image search
where you search Google using an image
as your search query instead of text
it's very likely that Google is using
Auto encoders to help fuel this use case
whenever one of google's BOTS is
crawling the web and finds an image it
will compress it using an auto encode
that would mean Google would have all of
its index images compressed in the form
of an easily searchable array whenever
you search for a new image it will
compress it and find all the points
nearest to it in the compression space
since there is less noise then rank them
according to their similarity there's
also the one class classification use
case that's when we only have a data set
with a single class which is called the
target class when we feed it to an
autoencoder it'll learn to detect
objects of that class when it receives
an object that doesn't fit that class
category it'll detect it as an anomaly
whether it's a deadly virus or fraud
attempt system downtime or someone who
actually doesn't play Pokemon go that's
right auto encoders are for you and if
you want to train a deep network you
could use what's called a stacked auto
encoder that is a set of auto encoder
where the outputs of each layer are
wired to the inputs of the successive
layer once you train it on some data set
you can use those weights to initialize
your deep net instead of randomly
initialized weights one of the more
recent applications of autoencoders is
generating novel yet similar outputs to
our inputs like faces that look really
similar but are different than the input
faces for this we use a newer type of
auto encoder called a variational auto
encoder which learns a distribution
around data so it can generate similar
but different outputs there are lots of
techniques that are used to prevent
auto-encoders from successfully
reconstructing the input image like
denoising where the input is partially
corrupted on purpose the idea is that if
it can reconstruct an image despite it
being corrupted it'll be a more robust
decoder so let's build our own simple
auto encoder to learn to detect
handwriting digits using tensorflow
shall we
first we'll import tensorflow the
awesome machine learning library then
numpy the pythonic scientific computing
library
we'll also import our input data which
is the collection of handwritten
character images then we'll define our
auto encoder hyper parameters we know
that each character image is 28 by 28
pixels so we'll set the width to 28 then
we'll initialize a variable that
represents the number of input nodes we
also want to have 500 nodes in the
hidden layer I generally like to have
2/3 a number of nodes in my hidden layer
as my input layer as a starting point we
want to purposely corrupt our input data
later so that our decoder gets even more
robust in its reconstruction over time
so let's add our corruption level to 0.3
which isn't nearly as high as a US
government's once we have these
variables we'll use them to help us
build nodes we'll start by creating a
node for the input data then we'll
create a node for the corruption mask
which will help us reconstruct the
original image well then want to create
nodes for our hidden variables after we
initialize our weights
well initialize our hidden layer as well
as the prime values for each tie the
weights between the encoder/decoder and
result in an output value after that we
define our model via a function that
takes in our variable parameters well
make sure to get a corrupted version of
our input data then create our neural
net and compute the sigmoid function to
create our hidden state then we'll
create our reconstructed input and
return that so our model will accept an
input and return a reconstructed version
of it despite the self-imposed data
corruption so we can utilize this
function by building a model graph with
it which we'll call Z so now that we
have our MA
all we need to create a cost function
which is what we want to minimize over
time the more we minimize it the more
accurate our output results will be then
we want to create a training algorithm
which we'll call a train op and use the
classic gradient descent algorithm to
help train our model which takes the
cost function as a parameter so it can
continuously minimize when we train it
before we start training our model we
need to load our data so let's read from
our local directory will set one-hot to
true which is a bit widest operation
that will make computation faster then
initialize the variables for our images
and labels for both our training and
testing data finally we'll begin our
training process by initializing a
tensor float session well initialize all
of our variables first then begin our
for loop which will iterate 100 times
for every image and label we will
retrieve an input then create a mask
using the binomial distribution of our
input data and use that as a parameter
to run our tensor flow session using the
training algorithm we defined earlier
we'll calculate a mask for the outer
loop as well and print out the results
as we go along let's take a look at the
results we can see our score gets better
and better over time with training and
eventually our neural net is able to
reconstruct and classify handwritten
characters for more information check
out the links down below and please
subscribe because I'm just getting
started
for now I've got to go boost some
gradients so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>