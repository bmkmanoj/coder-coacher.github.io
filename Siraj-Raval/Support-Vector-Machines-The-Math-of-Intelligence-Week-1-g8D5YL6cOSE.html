<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Support Vector Machines - The Math of Intelligence (Week 1) | Coder Coacher - Coaching Coders</title><meta content="Support Vector Machines - The Math of Intelligence (Week 1) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Support Vector Machines - The Math of Intelligence (Week 1)</b></h2><h5 class="post__date">2017-06-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g8D5YL6cOSE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and today we're
going to build a support vector machine
to classify two classes of data I've
already recorded this video before but
I'm really becoming quality and the
video quality was not good and we have a
standard to keep on this channel so
listen up here we go we're going to
build a support vector machine to
classify two classes of data right and
the way we're going to optimize the
support vector machine this type of
machine learning model is to use
gradient descent okay so that's what
we're going to do and this is what it
looks like it's it looks like this so
say was about two classes we have one
class that's going to be denoted by red
dots and the other class is going to be
denoted by blue dots so if we were plot
both classes on a 2d graph an XY graph
then we could draw a line a decision
boundary that best separates both of
these classes and that line is called a
hyperplane and our support vector
machine helps us create it okay that's
what our support vector machine helps us
do and we're going to talk about all the
details of how this thing works but this
is at a high level what it looks like
and we're going to build it with just
numpy and that plot libe okay to graph
it so no tensorflow or any of that
alright so I hope you're as excited as I
am for this because we are going to go
into the theory as well as the code as
well okay here we go so to start off
with what are some use cases for an SVM
so classification is one which we're
going to do and that is in fact the main
case that SEMS are used for that is the
most popular case but they can also be
used for other types of machine learning
problems regression that is if we have
some set of data points and we're trying
to predict the next point in that set of
data so because stock prediction would
be a good example and also time Oracle
also called time series prediction
there's also a liar detection so say you
have a security system and you're
tracking all of your users and all of
your users have a set of metrics a set
of features that identifies them like
the time that they've logged in and
what they're doing there could be an
anomaly and you nominally could be your
bad guy the guy who's trying to break
into your security system and SVM's
helps you detect who that person is
there's also clustering but we're not
going to talk about which which is also
which is a form of unsupervised learning
learning as is outlier detection but we
are going to do supervised
classification that is when our data has
labels right we are trying to learn the
mapping between the labels and the data
and if we learn the mapping that is a
function the function represents the
mapping the relationship between these
variables if we learn this function then
our machine learning model had done its
job then we can use this function to
plug in some new input data and it's
going to output the prediction right and
with this is all of machine learning
really so let's look at it so in this
example we're going to use toy data
right because it's more it's about the
it's about the math and the algorithms
but I have these two other examples for
you just in case you want to do
something a little more useful so the
first one is for handwritten digit
classification right you have our we
have a set of digits and they all had
their labels and we want to learn the
mapping between the labels and the
digits and so that's what this
repository will help us do and the great
thing about this repository is its using
scikit-learn which is a very popular
machine learning library in one line of
code you could implement an SVM but
we're going to implement it from scratch
because we want to learn how this thing
works but once you've done that then you
can go on to using something easier
something simpler like this okay and
it's a real-world use case I've got one
more example here that I that I have for
you guys and that's for pulse
classification so the idea is that for a
human given some metrics like their age
and their above the other one their
pulse rate obviously we can predict what
their emotions will be so it's emotion
classification and we're using an SVM in
this repository as well as well as I
can't learn to implement that SCM so
check out those two repositories once
you really understand the math behind
support vector machines from this video
and from
associated code okay so yeah so those
are two other examples so how does this
thing compare to the other machine
learning algorithms and there are so
many of them there are random forest
there are neural networks well as a rule
of thumb as DMS are great if you have
small data sets so I'm saying like a
thousand rows or less of data right a
thousand data points or less if we have
that then SVM's could are great for
classification and they are very popular
however other algorithms random forests
deep neural networks etc require more
data but almost always come up with a
very robust model and the decision of
which classifier to use depends on both
your problem and your data and as you
build this mathematical intuition all of
these choices will become very clear to
you so we're starting off with the
support vector machine okay and so I
also have this quote by the famous a
computer science professor Donald Knuth
who now I know the chaos Island Donald
Knuth who said that premature
optimization is the root of all evil or
at least most of it in programming and
what does he mean by that that means if
you can make a performance gained by
using something way more complicated
like a deep neural network but it's only
going to be like it's only going to be
by like 0.1%
then it's unnecessary your time as a
programmer is very valuable and you only
want to do the minimum amount of work
that you have to to get the results that
you want so if you're trying to use a
deep neural network for a problem that
only requires something very simple like
a support vector machine you should use
a support vector machine not the deep
neural network just because it's hot and
it's outperforming everything else
almost all the time because it requires
more computing data and way more it
requires more computing power and way
more data write to things which if you
don't have don't use it use a support
vector machine right these are all
methods of intelligence this is all
about the mass of intelligence there are
many ways to approach it
so what is
support vector machine so this thing can
be used for both classification is that
this is that this is that this and
regression got these points what's the
next point in a series so given two or
more labelled classes of data remember
we are using supervised learning it can
create a discriminative classifiers that
is a classifier that can discriminate
between different classes is that this
is that this is that this right and the
opposite to discriminative by the way is
generative where we generate new data we
take some training data very 8 and vary
it in some way using our model and that
output is very similar to the training
data but it's novel data but that's for
later on anyway so the way we build this
hyperplane and we'll talk about that
term but the way we build this
hyperplane or line this decision
boundary between the classes is by
maximizing the margin that is the space
between that line and both of those
classes what do I mean by that when I
say both of those classes what I
actually mean are the points in each
check out this image are the points in
each of those classes that are closest
to the decision boundary and these
points are called
support vectors okay we call them
support vectors because they are vectors
they are data point vectors that support
the creation of this hyperplane that our
support vector machine will create right
so we are maximizing the margin and why
do we do that because we want to draw a
line that is in the absolute perfectly
the perfect middle spot between both of
these sets of data such so that when we
plot a new data point if it is of a
certain class it will have the maximum
likelihood of falling on that side of
the decision boundary where it should
and the only way to do that to maximize
the space with which a new data point
can fall into its correct class category
is to maximize the space between data
points and put a line right in the
middle of that space you see what I'm
saying I can't get feedback but I'm just
going to assume that that that was
intuitive right so right so small margin
we're maximizing the margin and we're
trying to draw a
decision boundary a line of best not a
line of best fit but a line of best
classification between both of those and
we call it line a hyperplane okay so
what is a hyperplane well a hyperplane
is a decision surface so given n
dimensions right so let's say our data
is n dimensional where n is the number
of features that you have length width
height tongue color tongue color or did
that come from
tongue color and skin color and whatever
other colors and so a hyperplane is n
minus 1 dimensions so if you have a
two-dimensional graph where just like
this on the Left right here where I'm
pointing my mouse where this R symbol
with the two exponent denotes a
two-dimensional graph a hyperplane would
then be 2 minus 1 right n minus 1 to one
dimension so would be a line right but
if you we are in three dimensional space
or our to three then a hyperplane is
going to be two-dimensional because it's
3 minus 1 which is 2 right so we have a
plane and so you we can extrapolate this
to many dimensions so if we had a 400
dimensional space which we often do in
machine learning our data doesn't just
have 2 or 3 features it has many many
features right it's not so neatly
packaged for us to visualize and that's
where techniques like dimensionality
reduction and all these come into play
which we'll talk about but right now if
we have a 400 dimensional space a graph
of points then a hyper plane would be
399 dimensions which we can't really
visualize I mean think about it humans
we are not that good at visualizing or
in fact it's impossible for us to
visualize anything in more than three
dimensions but for machines it's very
easy it's very intuitive and that's all
that matters as long as our machine is
able to draw this decision boundary of n
minus 1 dimensions then given some new
data point if we put it into that model
that functions that functions if we put
it into that function if we plug it in
and it's going to output the correct
class of whatever it is ok so and this
actually comes from geometry so I guess
there is a little
of geometry in machine learning right so
nonlinear versus linear well we're only
going to talk about linear
classification because nonlinear
classification is more complicated and
we'll get to that but the idea here is
that let's say you have some you know
some sets of dates some some two data
sets right two classes of data the best
let's say the best the line that best
separates these two classes of data
isn't linear let's say it's got curves
like in this in this example right here
how are we supposed to build a
hyperplane like that
it would well it would be more
complicated and there is actually a
trick to do this called the kernel trick
and we'll talk about that later but
there's a way to take that map that
input space into a feature space such
that the hyperplane that you draw is
linear even though it wouldn't be
otherwise and so that's called the
kernel trick and we'll talk about that
later right so we're only talking about
linear classification for support vector
machines supervised linear
classification right as opposed to
unsupervised anyway there's so many
different ways that we can frame this
problem right there's so many different
ways we can frame the learning process
and more will be discovered it's a very
exciting time to be in this field okay
so let's get let's go ahead and get to
building shall we
but first of all I also want to say one
more thing so no matter what model
you're using a random forest a support
vector machine a deep neural network in
the end we are approximating we are
guessing iterally close we are getting
educated guess and rising of a different
word for approximation but we are
approximating a function right we are
trying to find what if the what is that
optimal function and that function
represents the relationship between all
the variables in our data right that
function is that is that is that
relationship it's that mapping and if we
can find that function then we have
learned we have learned from our data
and so every machine learning model
under the hood is just a function that
we are trying to approximate and its
coefficients are its weights and they
are being updated over time through some
optimization technique
be decorating dissent usually or
Newton's method which we'll learn about
or you know whatever it is okay so yeah
so whatever it is we're just trying to
approximate a function right this is a
way of thinking approximating a function
whatever we're using decision forests
whatever we're using it's all about
approximating a function decision trees
all right so let's go ahead and get and
get to building right so first we're
going to import numpy and so numpy is
going to help us perform that operations
matrix math and then we're going to plot
our data using map plot live okay so the
first step is for us to define our data
so our data is going to be of this form
X Y bias so the first so there are five
data points here right there are five
data points and we've got the x
coordinate the y coordinate and we just
input our bias into our data to make
things easier later on but we can for
all intensive purposes ignore this bias
term but we are basically just at we
have these set of XY coordinate pairs
that we can plot on a graph and each of
these data points has an Associated
label an output label that output label
is either a negative 1 or a 1 ok so for
the first two they're going to be
negative one and for the last three
they're going to be one so these last
three so what we can do is we can plot
these example on a 2d graph okay so we
can say let's plot the let's plot the
first two with the negative marker and
let's plot the last three with the
positive marker okay and so when we plot
it it looks like this and what we're
also going to do is we're going to print
is we're going to plot a possible
hyperplane that is a hyperplane that is
just a line and we don't know it's just
our naive guess we don't know if it's
the optimal hyperplane in fact it's not
but it just so happens to perfectly
separate our training data classes just
so for us to just see what it looks like
right is it just for that example okay
so that's that so now what we can do is
get into the math so I hope you're ready
for this all right so let's get into our
calculus all right
so right machine learning machine
learning is all about optimizing for an
objective function
and the way we optimized for an
objective function is by minimizing a
hope you said loss or error function
because that is the correct answer we
are minimizing a loss or error function
so let's go ahead and first define our
loss function our loss function in this
case is going to be called the hinge
loss so the hinge loss is a very popular
type of loss function for support vector
machines okay and the class of algorithm
that support vector machines fall under
our maximum margin classification
algorithms right we are trying to
maximize a margin that is the distance
between classes such that we can draw
the best decision boundary between those
classes that best separates both of
those classes okay so this is what it
looks like this is what the hinge loss
looks like the hinge loss looks like
this with the word see that's how we
denote the hinge loss given these three
terms the three terms are going to be X
Y and f of X where X is the sample data
Y is the true label and f of X is the
predicted label right so it's going to
be 1 minus y times f of X and this
little plus sign down here just means
that if this result if the result of
this these sets of operations is
negative then we're going to just set it
to 0 because we always want the result
to be positive okay so what this means
is if we can break this down into this
equation right here where my or on where
my mouse is now over or we can say if y
times f of X is greater than or equal to
1 which would make this come out to be 1
minus a number that's greater than 1
which would be 0 or a negative number
then set the result to 0 because we want
it to be positive and if it's not then
it's going to be some non-negative
number greater than 0 okay so that's our
loss that's how we define our loss and
remember these this Y and this f of X
both of these values these scalar values
these single values are going to be a
single number right and that's well
that's why we can multiply them and so
our objective function then is going to
consist of the loss function which
notice how it looks a little different
but it's really the same thing this 1
minus y times X
xw it's the same as its lost up here
it's just a different way of denoting it
and we can say the Sigma term means that
we are going to take a sum of terms
where the number of terms is n and n is
the number of data points that we have
so for all five data points
we'll find a loss of each of those data
points using this this loss function the
hinge loss and we'll sum them all up
together and that that total sum will
represent our total loss for our data
right it's a single number it's going to
be a single number and then once we have
that we're going to define our objective
function so our objective function in
this case it's going to be denoted by
this min lambda W okay with the square
sign and so what is this all right so
our objective function is going to be
denoted by the loss Plus this
regularizer term which is denoted right
here with this min and the lambda so a
regular riser is is a tuning knob and
what the regularizer does is it tells us
how best to fit our data so if the
regular riser term is too high then our
model will be over fit to the training
data and it's not going to generalize
well to new data points it's going to be
over fit but if the regularizer term is
too low then our model is going to be
under fit so that mean is going to be
too generalized and it will have a large
training error so we need the perfect
regularizer term to for our model to be
as generalizable as possible and fit to
our training data it's that balance term
right and it says it's also it also
comes out to be a single scalar so given
our weights we square that and then we
use this Lam lenda term here we multiply
about this by this lambda term
okay so right so that's our objective
function our objective function consists
of our regularizer and our loss function
we add them both together so what we
want to do is we want to optimize for
this objective and by optimizing for
this objective we're going to find the
optimal regularizer term and we're going
to minimize the loss so we're going to
do two things by optimizing for this
objective and so the way we're going to
optimize is we're going to perform
gradient descent right and so the way
we're going to perform
gradient descent is by taking the
partial derivative of both of these two
terms of both of these terms we're going
to take a partial derivative of the
regularizer and we're going to take the
partial derivative of the of the loss
term okay so this is what it looks like
right so remember from the power rule
for partial derivatives all we have to
do is move the power to the coefficient
and then subtract 1 from the coefficient
and then for the other term we do the we
do the same set and so for the last term
we do the same thing and it comes out to
this and so it's going to be 0 or it's
going to be negative Y times X so
there's there's a case for both them ok
so then what we would what we then have
is a missed classification condition any
classification condition so we can so we
can so basically we can say if it's
misclassified so if we misclassify our
data depending on these partial
derivatives then we can update our
weights a certain way and what I mean
what do I mean by a certain way I mean
we can update our weights by using both
the regularizer term and the loss
function term okay because this isn't 0
it's going to be negative Y times X but
else if we if we have correctly
classified then this value is going to
be 0 it's going to be a 0 so we don't
need to update our loss we only update
our regular only update our weights
using our regularizer term and so this
term right here is a learning rate by
the way so it's weights plus learning
rate times the regularizer term the
learning rate by the way is how we is
another tuning knob for how we help
that's we learn so if the learning rate
is too high as our model is learning it
could just it could just miss it could
just overshoot that minima entirely it
just keep going but if it's too low it
could take way too long to convert or in
fact it could just never converge so we
want to have that optimal learning rate
okay so those are our terms and so now
now let me plug into some Tower here
okay so so now let's get into the code
for this right we've talked about the
math let's get into the code so for the
code part we can say all right well we
want to initialize a support vector
machine we're going to perform
stochastic gradient descent by the way
but we're going to initialize a support
vector machine with a set of waist
vectors and these weight vectors are the
coefficient of the model that we're
trying to approximate there are three
values that we initialize with the set
of zeros then we have a learning rate
which is one a number of epochs which is
a number which is iterations to train
for over the data set over the entire
data set and then a list of errors that
we're going to store all of our errors
in okay so basically we could say and
now here's the machine learning part
here's all that math that we just did we
can fit into these ten lines of code
right we are literally just taking those
equations and converting them into code
right now so we'll say for the number of
epochs for so for a hundred thousand
times
we'll set that error to initially zero
let's iterate through every single data
point that we have so for all five data
points so we have five data points we're
going to iterate through all of them
100,000 times okay and so we then we
have our first case which is the miss
classification case so in the mid
classification case let's see what that
was y times X w is less than 1 this is
exactly what it comes out to
programmatically y times the dot product
of X and W is when it's less than 1 so
if it's less than 1 then we have
misclassified our data so we can update
our weights using that full equation
that we saw right up here right up here
or we're using both our regularizer and
our loss function to update our weights
right because this is nonzero it's going
to be negative Y times X using our
partial derivatives and our partial
derivatives are the gradient right they
are the they are the gradient value that
so that we can update our weights in a
direction such that such that the such
that the error is minimized that's what
they're going to help us do ok and so
we'll update our weights doing that and
then we'll say well we got an
so let's make our error count 1 else if
we've correctly classified then we
update our weights using just the
regularizer term which is 1 over the
number of epochs by the way our regular
eyes the term I think I might have
forgotten to say that our regular riser
term is 1 over the number of epochs so
it's inversely correlated so that the
regularizer parameter will decrease at a
number of epochs increase and so the
update rule for correct classification
looks like this or we're only using the
regularizer term all right so we've got
that and so once we have that we can
plot it out and we'll also add all those
errors to this list of errors so we just
have that you know we want to see how
the error frequency decreases over time
during training that's the actual
machine learning and so when we plot
that we'll see that the error decreases
over time the error value decreases over
time and that's what we want right so
we've trained our model right we trained
it on those five toy data points and so
now what we can do is we can now plot
this model we can plot this model and we
can we can add testing data as well
right but we have we had a missed
classification case and we had a correct
classification case and depending on
whether it was missed classified or
classified correctly we updated or
weights using a different strategy okay
and the pole goal is to make those
weights the optimal values that they
should be such that our error value is
minimized and we have the equations for
that right here and the reason we took
the partial derivative derivative it's
so that we can best up their weights to
optimize for our objective function
which consists of these two terms
minimizing the loss and optimizing the
regularizer term such that our data is
best fit to both our training data and
any novel data points we give it that is
generalizability right so hopefully that
all makes sense
that's essentially what we just did if
you if you understood like 80% of that
pat yourself on the back because you are
a boss all right we are all bosses all
right so machine learning bosses okay so
- this okay so now we're going to plot
this data all right so let's apply it so
if we have less than two sample exam
samples then we'll plot it again so this
is the same thing that we did before so
we'll do it again and then we're going
to add our test samples so our test
samples are going to be just two points
and they're also going to be two toy
data points which is assume that we know
what class this data points you know
consists of until add our two test
samples plot our hyperplane that's
trained on the training data and
hopefully it classifies both the
training data and the testing data
accurately such that they've all lie on
the perfect side of the decision
boundary so that they are correctly
classified and so when we plot it we'll
see that that is the case all the
positives labeled data points are on one
side of the line and all the negatives
labeled data points are on the other
side of the line we have correctly
classified both our training and our
testing data by optimizing for our
objectives and by doing so we are
minimizing our hinge loss so that we've
maximized the margin or the space
between the two data classes so we can
draw the optimal hyperplane and we use
the regularizer term that we also found
the optimal value for by including it in
the objective function so that our model
with best fit to both the training and
the testing data all right to that in a
nutshell is how support vector machines
work so to to go over this again let me
just say that right so the gradient is a
vector whose components consist of the
derivative so in all of calculus we have
derivatives and so the reason we take
the derivative is because calculus is a
stuff is the math of the rate of change
we want to know how something changes
and the way we we study how it changes
you will use it in physics a lot to
write for moving bodies the derivative
is how we is how we is how we understand
we
directions something is moving in right
it we derive the direction from it and
there's several ways of representing the
derivative in calculus we use it
obviously the derivative which is the
derivative operator but then we also use
the gradient operator and so the
gradient and so you hear these terms a
lot you hear gradients and you hear
derivative a lot but they're really the
same thing in that the gradient is a
vector whose components consist of the
partial derivatives of whatever
coefficients of whatever function that
we're trying to approximate okay and so
that's the gradient and the derivative
and sit in the next week's lesson we're
going to talk about the Jacobian which
is a matrix of first order partial
derivatives and also the Hessian which
is a matrix of second order partial
derivatives and these are all words that
represent how we organize and represent
change in data in in functions right and
with the reason we want to represent
change is so that we can minimize for a
loss we can up my further objective we
can iteratively get closer and closer to
approximating two educated lis guessing
what the optimal function is so that we
learn the mapping between data points
okay
so hopefully that all makes sense all
right so that's it for this lesson
please subscribe for more programming
videos and for now I've got to go invent
a new type of SVM so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>