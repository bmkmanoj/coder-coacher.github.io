<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generating Pokemon with a Generative Adversarial Network | Coder Coacher - Coaching Coders</title><meta content="Generating Pokemon with a Generative Adversarial Network - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generating Pokemon with a Generative Adversarial Network</b></h2><h5 class="post__date">2017-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yz6dNf7X7SA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and you got to
catch them all when I say catch them all
I'm talking about pokemons I've got to
catch them all but I've also got to
train them all
I got a train them all using a
generative adversarial Network what
we're gonna do in this video is we're
gonna generate all new Pokemon you heard
me correctly more than the original 150
shout out to the original 150 and the
five billion other ones that came after
that
but we're gonna generate all new
pokemons using what's called a
generative adversarial Network now I've
done this in one or two videos before
but this is probably gonna be the most
clear explanation I've ever done of it
it's a very popular machine learning
technique popularized just and created
just two years ago and this is gonna be
awesome so this is a demo of what I've
already generated with my Gann GaN for
short so we'll call them ganz from now
on and as you can see it looks pretty
funky we give it some training images of
real pokemons and then after training
using this generative adversarial
approach it's gonna be able to generate
new Pokemon that have never existed
before so let's dive right out right on
into it at the end I'm gonna go into the
code but first let's talk about what
these things are so there's a lot of
different ways that we can classify the
learning process for computers right the
machine learning process we could talk
about supervised unsupervised
reinforcement learning but one of the
most popular ways is discriminative and
generative right so for generative
models we are learning the probability
the joint probability of x and y for
discriminative models we're learning the
probability of Y given X so you might be
thinking what does that mean right so
here's what it means here's a really
simple example that I found off of Stack
Overflow so let's say this is our
training data right it's it's just a for
X Y value pairs right X Y XY XY X Y and
they're all integers and so the the
joint probability that is the
probability of x and y is going to be
these four values okay and then the
probability the the the probability of Y
given X is going to be these four values
so take a second to kind of just stare
at this and eventually you're gonna get
it but let me help explain
let's say let's say y equals zero and x
equals one so this top left corner here
what that means is what is the
probability that Y is going to be zero
and X is going to be one out of all the
possible cases well if we look up here
we'll see okay here's one case here's
another case these two are not so out of
the four possible cases
two of them are cases where X is one and
Y is zero so that's two out of four or
one half and so that's how it works
for the joint probability for the for
the probability of Y given X it's if if
X is gonna be one how many times is y
gonna be zero so here's x equals one so
y 0 here's x equals 1 y 0 and those the
only two x so out of all those times
it's going to be 0 so that's a 100%
probability right so what's the
probability that you're gonna get 0
given every time that x equals 1 right
so that that's the difference between
both of those it still doesn't explain
the difference but that's the
mathematical explanation so the idea
behind generative models is that we are
trying to create these systems that
generate new data given some training
data set right so the pros of this are
that we have knowledge about the data
distribution what that means is any kind
of training data that you have any kind
of training data we can represent all of
it as some distribution right some curve
either a Gaussian or a multinomial all
sorts of distribution and what a
distribution is it is it is a range of
possibilities right it is a range of
possibilities of what some data could be
and if we learn what that distribution
is then we can say let's take some point
on that distribution and generate some
data from it what that means is that
point doesn't necessarily have to come
from our training data set but it's a
part of the range of possibilities and
thus it can be a novel data point
meaning a new type of data that looks or
feels very similar to the original data
basically we can generate new data if we
learn what the distribution of the
existing data is the cons of this is
that this is very expensive to learn
right degenerative models in general are
harder to Train than discriminative
models and we need
lots of data discriminative models on
the other hand they don't try to
generate anything they're really easy to
use that's the pro the con is that all
they do is they discriminate right they
classify they differentiate given this
stock given this cat is this a dog is
this a cat it tries to put labels on
things the probability of why a label
given some input X right so that's why
it's the probability of Y given X
whereas for the joint probability its
probability of x and y so given some new
X we can generate a Y or given some new
Y we can generate an x at the same time
right so that's a difference so a lot of
times we talk about discriminative
models and there are a lot of popular
ones the logistic regression support
vector machines neural networks can be
discriminated that can also be
generative as we're gonna see here but
for generative models we have naive
Bayes mixture of gaussians hidden Markov
models and if you're curious about how
any of these models work I would point
you to my math of intelligence playlist
I have videos detailed videos on every
single one of these that you're saying
here they're all in my massive
intelligence playlist so take some
coffee and just go through that whole
playlist if you want to learn how all
these models work but my idea right now
is to just show you that there are two
types of machine learning models that we
can we can think about right so again
discriminative models probability of Y
given X and generative models are the
probability of x and y and we can use
Bayes theorem to derive that as well
discriminative models learned the
boundaries between different classes if
it learns the boundary then given some
new data point we can say oh it's on the
left side it must be a dog or it's on
the right side it must be a cat whereas
generative models learn the distribution
of the classes and then once we have
that distribution we can say boom
generate this generate this generate
this okay so that's kind of how it works
so one class of generative models is the
generative adversarial Network okay so
the generative adversarial network at a
high level looks just like this
ignore the detective part for now but
what happens is we have two neural
networks by the way I met
ian's Goodfellow the creator i
interviewed him if you're curious
just search Suraj Ian good fellow on
YouTube great interview anyway we have
two neural networks here we have a
generator Network and we have a
discriminator Network and our job is
going to be this right we input some
random noise into the generator network
that is some random group of numbers a
vector and we're gonna feed it to this
neural network this generator Network
its job is to take that input and do
what all neural networks do apply a
series of matrix multiplications to that
input until we have an output and that
output we're going to consider a fake
image now the discriminator is going to
say it's only going to look at the real
training data set that we have that is
the real that are the real images and
once we have these real images we'll
feed those into the discriminator
network and it's going to output a
probability label whether it's real or
it's fake because the discriminator
takes two inputs it's going to take the
real image and it's gonna take the
generated fake image and it's job is to
say is the fake image real or fake
right and at first it's gonna say
obviously this is fake you just took
some random noise vector and generated
an image from it it had nothing to do
with the training data of course it's
fake and then what's gonna happen is
when it makes that prediction we're
going to compute an error value like
always and then what do we do we
optimize moving forward and when we
optimize we are optimizing to minimize a
loss function right and when we minimize
a loss function we're using a strategy
like gradient descent right with
gradient descent we can compute a gray
in value to update the weights of both
networks both the discriminator network
and the generator network what that
means is they both get better over time
and what that meet what that really
means is that because they get both get
better over time the generator is
getting better at generating very
realistic-looking data until the
discriminator cannot decide what's real
and what's fake and that's when the
generator has done its job well right so
one way to think about this is that the
discriminator is a detective is trying
to figure out what's real and what's
fake and the generator is a forgery it's
trying to say hey this is a real copy of
the Mona Lisa but it's not it's a fake
copy of the Mona Lisa and it's
using random noise random numbers
generated as its input and as its
weights are updated it's getting better
and better at this overtime we can think
of the discriminator as a binary
classifier real or fake 1 or 0 and and
the way we can do this is because we
ourselves we as programmers know what's
real and what's fake
so we can assign those labels and
because we can assign labels
it's a supervised problem and because
it's a supervised problem what is a
technique that we use here we go go for
it's a back propagation right that's how
we update our gradients we use back
propagation and notice because we have
two networks we have two networks we are
going to be back propagating gradients
for two different networks so that means
we have two different optimization
problems running simultaneously so the
models play two distinct literally
adversarial roles because they are
adversaries one is trying to fool the
other constantly and the other is trying
to not be fooled right and because we
are using this optimization technique
they're both getting better and better
over time
so you might be thinking ok this sounds
really complicated I don't know what the
this looks like but let me tell you this
there are really only four components I
don't know I put five but there really
only four components to think about here
you have are the genuine data set
whatever that is images videos whatever
that data set is and then you have I
which is the random noise vector which
is trying to which is the kind of
starting point that the generator uses
to generate anything you have G the
generator and your D the discriminator
right you know how to build a neural
network I made a million videos on that
so you just build two neural networks
and then you have some training data set
you have a loss function and you go
that's a generator adversarial Network
at the end no I'm just kidding there's
more to it than that
there's more to it and I'm gonna show
you but first let's talk about the use
cases of this thing right
so obviously generating Pokemon is a
great use case by the way please
let's just chill with the MN is T I know
everybody uses MN is T as the baseline
but can we just start using Pokemon as
the new MN is T
I'm saying it right now let me drop this
right now let's start using pokemons
please as the baseline the original 150
as the new baseline for generative
models instead of MN is T because as
awesome as young Laocoon is the guy who
you know the godfather of convolutional
nets and his you know data set MN ideas
it's just time to move on just to keep
things interesting I've made a million
videos about MN is T I'm not gonna rants
about it too much anyway back to this we
can you do so many things with ganz for
example we can turn black-and-white
images to color we can we can turn in
maps into digital maps we can set we can
turn day to night you can make a drawing
of a purse and then turn it into a real
purse right to get really trippy you can
feed it a plain text plain English input
like the flower is white and pink in
color with petals that have veins and
then it's going to generate images from
that text extrapolate for a second here
what that means is as these things get
better we can feed them not just text of
you know pretty little flower images we
can say design me a rocket design me
forget a rocket design me the particle
accelerator from CERN you know that 50
billion dollar machine under Geneva
design that for me here all the specs
here's all the data go what that means
is the cost of the barrier to entry to a
million things becomes possible design
you know anything a living room design
clearly there's a lot of possibilities
for design and engineering right 3d
modeling but it's not just that it's
also for science so these researchers at
in silico Medicine said let's let's
generate a new drug using a using dance
so the goal was to train the generator
to sample drug candidates for a given
disease as precisely as possible to
existing drugs from a drug database
right and so what happens is after
training it was possible for them to
generate a drug for a previously
incurable disease using the generator
and using the discriminator to determine
whether the sample drug actually cures
the given disease so there are a lot of
use cases for this
and it's not just that there are a lot
of use cases there are a lot of ganz
fantastic ganz and where to find them
that's a great blog post by the way
there are so many different types of
ganz out there seriously but one huge
improvement to the initial gand paper in
2014 is the deep convolutional Gann or
dcen and so the main difference here the
main main difference is that instead of
using feed-forward networks as both the
generator and the discriminator they use
a convolutional network as the generator
and AD convolutional network as the
discriminator so that's just a
convolutional network flipped and what
what happened is they made a few
important discoveries one important
discovery was batch normalization is a
must for both networks fully kinda
hidden connected layers not a good idea
avoid pooling simply stride your
convolutions so again pooling recall for
my video about capsule networks hinton
the Godfather of neural networks
themselves is also skeptical of pooling
in fact he replaced it with that capsule
strategy so here's a great research idea
for you apply capsule networks to
generative adversarial networks boom
take it run with it be the first to do
that ok
capsule generative adversarial networks
paper of the Year award right there
paper of the Year award
right so riilu activations are your
friend almost always why because it
prevents the vanishing gradient problem
exactly if you didn't say that's ok
Vanilla Gans can work on simple data
sense but DC gains are far better and if
you have some new state-of-the-art
algorithm compare it with the base line
of using ADC again another one and
conditional Gans now these are really
interesting because notice remember that
image I showed you where it said here's
a flower with petals it's white it's
pretty generated they use the
conditional Gann to do that what that
means is what we're feeding into the
generator is not just some noise we're
also feeding in these strings so we're
conditioning the data on these extra
strings like male black hair blonde
makeup whatever
and we're also conditioning the
discriminator on this as well and what
happens is we are feeding both as
vectors right so we're converting those
strings into numbers
vectorizing them we're converting and
the noise to vectors
we're contaminating both vectors and
feeding that in as input into the
generator as well as a discriminator and
when we back propagate when we update
our weights the networks are going to be
more suited to generate data or
discriminate data that is conditioned on
those strings because there's a there's
an association between those strings and
the training images because we have to
pre label them right we have to pre
label the images with these strings and
because it's learning to generate and
discriminate condition on these strings
we can input these strings as as this
Sol input after training and then it's
gonna be able to generate images like
white pretty flower or whatever which is
very cool lastly there's one more type
of Gann of the many many ganz out out
there that I want to talk about that is
a very important again it's called the
Wasserstein or Wasserstein however you
want to say Gann or W again and the the
different that the real discovery of W
gains was that if you've ever tried to
train again before and you should after
this video if you haven't do it do it if
you haven't it'll be great you'll learn
a lot trust me the group the great thing
about W gains is that it improved the
the loss function so if you look at a
general Gann like a generic vanilla Gann
this is what the loss function looks
like or even a DC Gann how do you know
what to stop training right how do you
know when it's when it should be done
usually this is this is a really ugly
loss function there's there's no idea
there's no way to tell when to stop
training you just have to guess and
check so this looks much prettier right
now we know when to stop training when
that loss is maybe at the 500,000
iteration mark now that loss is pretty
low and everything how training
afterward is gonna lead to to
diminishing results so that's what W
gains do they replace the loss function
which is normally called the Jenson
Shannon divergence with instead the
Wasserstein distance they try to
minimize the Wasserstein distance so I
can go into that but that's a lot for
one video but that's at a high level of
what it is right so that's that's more
of the state-of-the-art when it comes to
Gans
okay so that's that now let's talk about
the code okay let's let's go into the
code now for this pokemon game let's
take a look at it so this code is about
285 lines so it's a lot but it's not too
much right we we can fit it all into a
single class file and I've it's built
with tensorflow
okay it's built with tensorflow so the
training data is this right these are
all Pokemon images right here right
we've got several Pokemon it's not just
the original hundred-fifty we got a lot
of them Dugtrio whatever I only know the
original 150 because when I was a kid
that's that's all we which one was this
piglet Tigger
I don't know anyway no I know it's not
Tigger it's I forgot the name anyway
let's look at this python file okay get
ready for this so we have a processing
data function that we can just skip
basically what that does is it just
formats all those images into the same
size because we want all those images to
be the same size when we feed it into
our networks right because that's that's
called that's the that's the
vectorization process so we have two
functions each of these functions
represents one of the networks we have a
generator function and then we have a
discriminator function so don't get
afraid by looking at this this is just
standard tensorflow code if we wanted to
we could you've we could have used Karos
and this all could have been abstracted
to ten or twelve lines but we want to be
very specific about what the details are
of what this looks like so remember with
convolutions convolutional networks this
this gain by the way is is aw game so
what that means is it's a DC gain that
means add a convolutional and a d'
convolutional network for the generator
and the discriminator respectively
and for the loss function it's
minimizing the Wasserstein distance and
therefore that makes it aw gain right
and so for the generator we have our
convolutional network that we can see
here now for convolutional networks we
have blocks
I call them blocks right so convolution
bias activation and then we repeat
that's one block convolution bias
activation repeat now a lot of times we
have pooling and that's considered in a
block and we repeat that but not in this
case because for DC Ganz they found
pooling was not a good thing so we have
a convolutional layer we have a bias and
then we have an activation function
which is really right and then we just
repeat that over and over again
convolution activation bias repeat
convolution no convolution bias
activation repeat and we do that for
five or no six layers so it's a six
layer convolutional network and that is
our generator and at the very very end
we have this 10h function that's going
to squash the output and then we return
the the output of that okay so that's
for the generator now for the
discriminator we just flip it right it's
flipped the other way because we are
trying to discriminate given an image as
input what is the output probability
what that image is real or fake so we
say convolution activation bias repeat
and tensorflow has these functions for
each of these operations that are a part
of convolutional networks Thank You
tensorflow
so then we do it again convolution
activation bias repeat convolution
activation bias repeat and we do that
for four layers and at the very end at
the very end at the very end we use a
sigmoid to squash the outputs and then
return the probability values for real
or fake okay so those are our functions
for the both the generator and the
discriminator now in our training loop
it looks like this we say okay well we
have placeholders and remember in
tension flow we are building a
computation graph that is a graph of
work where data flows through where the
tensors flow through right so a
computation graph you can think of a
neural network as a computation graph
you can think of a neural network as a
function right it's a glorified function
and so it's a set of operations right so
we'll create placeholders and the
placeholders are gateways they're the
gateways through which we input data
into the computation graph so what is
the type of data that we're going to
input what we're going to input some
image from our training data set we're
gonna input some random vector for the
generator right so we need to create
placeholders for both the real image and
the random input then we'll have a
boolean value just to say it should we
train it or not that's just for us now
we can say take our generator and feed
it as its parameters the random input
right so we're feeding it a random input
with us with a random dimension of that
input and then we're gonna say yes let's
train it and it's gonna output a fake
image right it has nothing to do with
the training data set right it's only
gonna learn to morph morph that input
into it's only gonna learn to morph that
initial random input into something that
looks like the training data because of
the optimisation scheme which is back
propagation correct so we're gonna we're
gonna output a fake image and then for
the discriminator we give it the real
image right and it's gonna output the
real result we're also going to give the
discriminator the fake image and it's
gonna output the fake results so the
probabilities of both and we're gonna
use those the difference between the
fake result and the real result as our
loss for the discriminator to update
that and for a generator it's going to
be the fake result okay so remember
there are two distinct different
optimization schemes happening at the
same time as these networks are
adversaries of each other and we're
going to use the and we're going to use
the rmsprop optimizer for both of them
to optimize both so rmsprop is a type of
gradient descent okay there's I also
have a video on all the different types
of gradient descent out there it's
called which activation function should
you use
Siraj search that on YouTube okay so we
have two distinct optimization schemes
gradient descent for both of them aka
back propagation and then we have a
bunch of you know check point and
restore variables all kind of
boilerplate and then let's get to the
good stuff our training loop right so in
our training loop we're gonna say for a
number of epochs and for our batch size
that we predefined for a given number of
iterations let's update the
discriminator and then we'll update the
generator and to update them what we
mean is it what we mean is we have a
tensorflow session and inside the
session the graph is initialized will
feed it both the trainer and the loss
function has will feed it will feed it
the training noise and the training
image right for the for the
discriminator and for the generator
we're gonna feed it the training the
training noise for the random input
until we feet when we feed both of those
in and we are and we're also saying well
we know we want to optimize both using
this it's going to optimize both given
both of those distinct inputs at the
very end we can checkpoint and save our
model every 500 epochs and that's it and
then we I also have this testing
function here that I've commented out
but if you want to test it after
training which you should then go ahead
and comment that out and then you can
start Jenner thats how you're going to
start generating the Pokemon after
you're done training so I also want to
say do not do not attempt to train this
thing on your puny CPU on your MacBook
or whatever you have I
I tried it's gonna take you know 24 to
48 hours if you try to train on a CPU
use Amazon ec2 use Floyd Hub use Google
cloud you can get free credits for this
for you know free trial but use a GPU
it's a hundred X plus times faster than
a CPU deep learning is meant to be run
on a GPU unless of course you have a
deep learning machine right but train
this thing on a GPU and you can do it in
three to five hours if you liked this
video please hit the subscribe button
and for now I've got to go find more
ganz to train so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>