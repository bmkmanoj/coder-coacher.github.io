<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Learn from Little Data - Intro to Deep Learning #17 | Coder Coacher - Coaching Coders</title><meta content="How to Learn from Little Data - Intro to Deep Learning #17 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Learn from Little Data - Intro to Deep Learning #17</b></h2><h5 class="post__date">2017-05-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tChcZpBbTTA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Hello, world.
It's Siraj.
And if you only have
a little bit of data,
can you still learn from it?
We're going to
build the model that
learns to classify
images using a very
small dataset for training.
Deep learning is still
very much a dark art.
It's an emerging practice in
the world of machine learning
that isn't well understood,
even by those pushing the state
of the art, which is exciting.
Because it means that there's
so much potential for discovery.
And it's not just one algorithm.
It's a collection of them.
Recurrent nets,
convolutional nets,
restrictive Boltzmann
machines, Barbara Streisand--
These are all networks
that can recognize patterns
in the world.
And they themselves
have shared patterns.
One shared pattern is that they
all use a hierarchy of layers,
the other is using
differentiable layers,
so that we can use
gradient-based optimization
to improve their prediction.
Design patterns are nothing
new in computer science.
There are many books
on design patterns
for topics like object-oriented
programming and user
interfaces.
But when it comes
to deep learning,
there's not really a definitive
design pattern guide.
We're all kind of figuring it
out collectively right now.
You might glaze over
a deep learning paper
and assume that there is some
solid mathematical foundation
behind what the
researchers attempted.
Because you see all sorts
of equations for things
like Hilbert spaces
and measure theory.
But the reality is that our
collective understanding
is still pretty minimal.
Theories are often formulated
because they are mathematically
convenient.
For example, the
Gaussian distribution
is ubiquitous, not
because it's some divine
construct that the universe
has bestowed on us,
but because it's
mathematically convenient.
So defining standard
design patterns
for pattern recognition networks
is a field ripe for discovery.
In the context of
machine learning,
we can call it meta-learning,
or learning to learn.
Can we design a system that
learns how best to learn?
That it learns how
to perform well
at an immediate task
in the short term.
And in the long term, it
learns a common structure
across many tasks.
We see meta level constructs
in nature all the time.
DNA is a great example.
It carries the
instructions, the blueprint
to create learning systems
that can expire our brains.
But it acts as long-term
memory by transcending death,
just like Oracle.
As long as there is a
mechanism for memory and one
to alter behavior based
on that memory, then
that mechanism can serve
as a meta level construct.
In the past few
months there have
been several papers
on meta learning
that have been published.
But I want to talk
about one that
uses meta learning
as a tool to solve
another task, one-shot learning,
the goal of learning from one
or only a few data points.
This is what we should be
aiming for, since GPU costs are
too damn high.
They used a modified
version of a model
called a Neural Turing
Machine, to learn
to classify character images
with just a few examples.
DeepMind first drizzed-opped
the idea of NTMs in 2014.
It contains two components.
The first is a neural network
that we call the controller,
and the other is a memory bank.
The controller takes
vectors as inputs,
and outputs vectors as well,
just like all neural nets.
But what makes it
special is that it also
interacts with a memory
matrix, using read and write
operations.
This is where the Turing Machine
analogy comes from, not just
because it sounds dope, but
because a Turing machine
manipulates symbols
on a strip of tape,
according to a table of rules.
It's like having a working
memory for a brain.
The network learns how best to
use its memory when learning
a solution to a given problem.
For the controller, they use
an LSTM recurrent network,
since its internal state is a
function of the current state
and the input to the system.
It can perform
context-dependent computation.
So, a signal at a
current time step
can influence the network's
behavior later on.
And we need all the components,
including the memory store,
to be differentiable so that
we can incrementally update
their values during training.
To achieve this, they added
an attention mechanism,
so that each read and
write operation interacts
to a tunable degree with
all the elements in memory,
rather than addressing a single
element like a normal Turing
machine would.
Each row in the memory matrix
represents a memory location.
Read and write heads
use a weighting vector
with a component
for each location.
So if there are 10
memory locations,
then the weighting vector with
just one value at index 3,
would focus the attention of the
memory operation on location 3.
But a weighting
vector, like this,
spreads its attention
to the memory
across multiple locations.
A read operation is just a
combination of the memory
matrix and weighting vector.
A write operation though, has
two parts, an erase operation
then an add operation.
The way there read and
write heads are produced
is by combining two memory
addressing mechanisms.
The first is content based.
We focus on locations
based on the similarity
between their current
values and the controller's
emitted values.
The second is location based.
It facilitates iterations
across locations of the memory
and random access jumps.
Controller and memory
bank, read and writes
are so, so dank.
So, so dank.
So the authors of our one-shot
learning paper knew that NTMs
were a subset of memory
augmented neural networks.
And they saw the potential
to improve on it,
so that they could learn
from just a little data.
They discovered that
using a pure content based
memory writer, instead
of content plus location,
let them do just this.
That's because there's a
trade-off when training MANNs.
The more complex the
memory mechanism,
the more training the
controller requires.
The dataset has 1,600
separate classes,
and only a few
examples per class,
perfect for one-shot learning.
They randomly
selected five classes,
and randomly assigned each
class a label between 1 and 5.
So the model gets shown
an instance of a class,
tries to classify
it, then it gets
informed of what the
correct label is.
We'll only need TensorFlow
and Numpy for our model.
We'll first define
our memory bank,
initializing each of the
variables that make it up.
Then we can define
our controller,
a feed-forward neural network.
We'll define each set of
weights and biases layer
by layer, until we've
reached the output layer.
We can define the
interaction that
happens between both components
under the step function, which
is called every time
step during training.
Just like with a regular
NTM, we read a vector
from memory that is a linear
combination of its rows,
scaled by a normalized
weight vector.
For the given input x, the
read vector will produce a key.
We compare each key
against each row in memory,
using the cosine
similarity as a measure.
This produces the
read weight vector,
which tells us how much
each row should contribute
to the linear combination.
The difference here is that
there is no extra parameter
to control the read weight
vectors' concentration.
To write to memory,
the controller
interpolates between writing
to the most recently read
memory rows and writing to
the least used memory rows.
Using the read weight vector
at a previous time step,
and the weight vector that
captures the least used memory
location, the controller
combines the two,
using a scalar parameter
and the sigmoid function
to create a write_weight_vector.
Each row in memory
is then updated
using the write_weight_vector
and the key issued
by the controller.
The model eventually
returns the probabilities
for each class as a vector.
After we've initialized
our TensorFlow session,
we'll use gradient
ascent via Adam
to optimize our network for
every image label pair we
feed in via a dictionary.
We'll print out our
results iteratively.
After training,
we can test it out
on some different
recognizable characters.
And notice how the accuracy
is surprisingly good.
Normally training time
would take a lot longer
for similar results.
These results are very
promising for one-shot learning.
And that's all it
takes to train, folks.
Let's get down to brass tacks.
A meta learning
system learns how
to perform well at
an immediate task,
and also learns a common
structure across many tasks.
Memory augmented neural networks
like a Neural Turing Machine,
use a controller and an
external memory store
to perform meta learning.
And meta learning can
be a way to achieve
one-shot learning,
which means learning
from one or a few examples.
This week's coding challenge
is to use a memory augmented
network to learn to classify
two classes of animals.
Details are in the Read Me.
Get Help links go into comments.
And winners will be
announced in one week.
And although this is the
last video for this course,
I'm still just getting started.
Please subscribe for
more programming videos,
and for now I've
got to go celebrate.
So, thanks for watching.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>