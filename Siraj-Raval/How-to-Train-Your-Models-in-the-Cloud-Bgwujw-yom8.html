<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Train Your Models in the Cloud | Coder Coacher - Coaching Coders</title><meta content="How to Train Your Models in the Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Train Your Models in the Cloud</b></h2><h5 class="post__date">2017-05-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Bgwujw-yom8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our world it's Suraj and how are you
supposed to train all these deep
learning algorithms if you don't have an
amazing GPU we're going to talk about
why GPUs are important for deep learning
then I'll show you how to train a big
style transfer model in the cloud super
easily the central processing unit or
CPU acts as the brain of a computer it's
a small square light component with a
bunch of short rounded italic connectors
on its underside
like vendors it's fundamental operation
is called an instruction every computer
has both a CPU and memory so when you
run a binary file like an exe which
contains instructions the OS copies the
program into memory then feed it to the
CPU one instruction at a time the basic
computation unit of a CPU is called a
core it can run a given task it does
things like maintain the program States
and the correct execution order it can
use one or more cores to perform a task
at a given time
CPUs generally don't have more than 12
cores and are meant for general
computing tasks there are some tasks
that can only be executed sequentially
like doing the dani death
some tasks though can be run in parallel
in fact they would benefit from
parallelization rendering graphics is a
great example each pixel can be
processed independently of the others
and so to help do this the graphics
processing unit or GPU was invented
modern GPUs can have up to a thousand
cores or more they're made for parallel
computation a single GPU is like lots of
little CPUs all running at once CPUs are
optimized for latency they're good at
fetching small amounts of memory quickly
while GPUs optimized for bandwidth
they're good at fetching large amounts
of memory all at once lighting and
shading effects require massive matrix
operations to be calculated all at once
and the GPU helps make this happen it
just so happens that another use case
that involves an ungodly amount of
matrix operations that can be
parallelized is a partner every computer
needs a GPU you have one - without it
you wouldn't see an image on your
display but the question for deep
learning is should you get a dedicated
GPU or to use the cloud building your
own rig if you do it right can be the
most cost efficient method the best GPU
overall in terms of computing power is
the Titan XP if you have any moderate
budget and the gtx 1060 is your best bet
and if you're on a tight budget then the
gtx 1050 TI should be your go-to it's
kind of like when the internet was first
coming of age everyone had an opinion on
whether or not to post their website on
their own hardware or use the cloud
eventually though the industry standard
became the cloud because maintaining
hardware was an unwanted hassle call
1-800 train that shit com wait so which
cloud provider should you use let's talk
about three AWS Google cloud and ploy on
Amazon's Cloud offering is currently the
industry standard and many big names
rely on it there are three types of
instances or virtual servers that AWS
offers the first is on-demand
this lets you rent an inn
with a specific capacity that you can
start and stop as you need to when you
restart it it picks up right where you
left off and only running instances are
built then there are reserved instances
for these you commit to paying for a
number of them for a certain period of
time beforehand the payoff for your
commitment is that these are half the
price of on-demand instances then there
are spot instances these are the
cheapest option
they are the spare computing capacity
that Amazon has at any given moment you
have to bid on them if your outfit your
spot instance is terminated
automatically and they can't be stopped
and restarted only terminated so you
can't just resume where you left off at
a later time AWS is dope sauce extreme
but there are a lot of steps to get
started with it and the learning curve
is quite steep so how does it compare to
Google cloud when it comes to pricing
Google cloud wins a two CPU eight gig
ram ten cents will cost sixty nine
dollars a month with AWS compared to
only fifty two dollars a month on Google
Cloud it offers a pay per minute model
instead of a paper our model which is
useful if you have short on-the-fly
tests to run that means that 2.01 hours
of compute on Google cloud is equivalent
to three hours on AWS while AWS offers a
generous one-year free trial in which
you can use 750 hours a month of a small
CPU instance Google cloud offers you
$300 worth of credit for 12 months and
an unlimited time for each year it's
still beat by AWS so in terms of number
of offerings if you need a cloud sequel
solution you can use my sequel with
Google Cloud but AWS offers a bunch of
different options Google cloud is more
flexible when it comes to configuring
your instances though it lets you
customize how many CPUs and how much RAM
to use and it has something called
preemptable instances which is similar
to Amazon spot instances except you
don't need to bid it can run for up to
24 hours but could be interrupted by
Google if they need the computing power
so it's choosing between the two while
AWS offers more cloud options Google
cloud is cheaper and easier to use
but there's another even newer option
ployed hub if you are a beginner or just
want to try out computing this is
the best option you get 103 hours of GPU
usage and you don't need a credit card
to sign up they offer per second billing
and under the hood they are using AWS
reserved instances but are providing
simplicity as a service it's like Heroku
for deep learning let's test it out by
deploying a style transfer model to it
for training ourselves a first second to
create an account on Floyd hub easy
enough then we want to install on the
Floyd command-line tool using the Python
package manager tip this tool will let
us interact with the Floyd up cloud
directly from terminal once that's
installed we can use this tool to help
us authenticate with the server via
Floyd login we can paste our off token
in as well then we can clone the style
transfer repository directly from github
to our local machine once it's
downloaded we can CD into the main
directory and initialize it as a foil
project we'll call it
style transfer now we're ready to run
the model we can run it by wrapping the
Python run command for the main file
inside of the Floyd run command this
repository has some flags that we can
use so we'll go ahead and specify the
style we want to use the directory will
save our checkpoint to output is a
special path meant to store checkpoints
on toy hub we can define what images we
want to test on how much influence we
want the content weight to have on this
file transfer and the number of
iterations to train for oh and of course
the data source we want to train on
that's all it takes to run and now it's
training in the cloud what this has done
is it's synced our local code to the
cloud spun up a GPU instance set up an
environment with tensorflow installed
and executed our command in that
environment once it's running we can
easily check the status of it in
terminal using the status command and
the ID of our run or we can view it in
the web dashboard under slash
experiments we can also view the logs
using the logs command which will show
all our print statements and we can
monitor training right here if we wanted
to if we make a change to this code
locally we can just rerun the project
with the run command boy hub will upload
a new version of the code and start
another run of the project so it's
gotten version control built in
just like it in fact it verges the
entire pipeline code data parameters and
environment for exact reproducibility
toy table uses a content addressing
scheme for both runs and data sets if
there is a particular data set we want
to train on again and again it's useful
to just upload it directly if we have it
in our current directory we can just
create the remote directory for our data
and name it using employee data in it
and RNA then we can upload it using data
upload we can use this data ID for any
model we've run later on it takes about
eight hours to train this model and when
it's done we can test it out by running
VG evaluate scripts giving it some fresh
images to style transfer by pointing it
to our images directory when it's done
we can observe the output by running the
elbow command using the run ID much
deepness all right let's cut the hay if
you want the absolute cheapest option
and are willing to deal with the hassle
building a custom deep learning machine
is the way to go and link to how to do
that in the description AWS and Google
Cloud are two great cloud platforms AWS
offers more cloud products and is the
industry standard a Google cloud is
cheaper and easier to use and point hub
is the easiest way to train your models
in the cloud and the best way for
beginners to get started please hit that
subscribe button for more programming
videos check out this related video and
for now I'm going to fly to Amsterdam so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>