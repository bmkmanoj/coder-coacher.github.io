<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make a Neural Network (LIVE) | Coder Coacher - Coaching Coders</title><meta content="How to Make a Neural Network (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make a Neural Network (LIVE)</b></h2><h5 class="post__date">2016-11-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vcZub77WvFA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's see who we've got in the room
today all right and go up here and then
see who we got
what is that sound it's heard some music
what is that what is that hi
hold on I'm hearing something what is
that sound
what does it sound it exactly hi
everybody I'm about to get in this room
in a second I'm going to talk to all of
you guys I'm pulling up this video so
here we go oh I'm hearing a an ad so
it's my noise cancellation hi everybody
hello world it's Suraj and today we're
going to create a a 2 layer neural
network let me just get this ad out of
the way all right there okay here we go
okay so we I'm in the room and I'm ready
to go
hi everybody okay that was crazy okay so
I am live and there are 83 people
watching right now and oh everybody hi
Andre hi David hypercar Yasu today is
Wednesday MN ist no it's not MN I see
okay so here was what we're going to do
we are going to a two layered neural
network from scratch that can predict
the XOR value okay so what does
next-door mean that means we're just
going to give it an array of numbers
okay so that means it's going to be ones
and zeros I'm not going to use C ah no
I'm not going to use tensorflow
I'm not going to use anything but numpy
and and the time library we are building
this thing from scratch just a two layer
neural network no sidekick nothing
because people have been asking
need to build a neural net from scratch
for a while so I'm gonna do it okay
and so a disclaimer okay so like I've
made a video on building a neural
network before I know you did that in
kindergarten that's awesome but Parker
Ali Abba us Mitchell I'm gonna keep
shouting out
I built a neural network before in a
video called building Aruna build a
neural net in four minutes
and it's my most viewed video but that
was like a pretty easy you know a neural
net this is gonna this is the first time
I'm going to say that this is going to
be a little bit intensive it's going to
be a little bit of math there's gonna be
quite quite a bit of math in here so
this is not going to be dead simple I'm
just going to say that for the first
time because you guys want to build a
neural net from scratch so leg let's
just go you know say so okay here we go
we're going to do this and so it's going
it's going to predict the XOR value okay
so it's gonna we're going to give it one
two zeros and then we're going to
generate some random data and we're
going to we're going to compare them so
it's like XOR means and computer science
it's like exclusive-or which means it
only returns true if they're both
different like or ones or if it's a one
in a zero oh it's a 1 if it is 0 into 1
oh it's a 1 if it's a 0 into 0 it's a 0
if it's 1 in a 1 its what it's a 0 okay
so it exclusive or all right so that's
what we're gonna do and I'm gonna start
off by doing a five-minute cool way like
always and then we're gonna get right
into the code okay so ask away five
minutes starts now where do I work I
don't work for anybody
although I do have a video coming out
for open AI which I'm super psyched
about can't say anything about it yet
anyway uh how do I learn math for
machine learning a linear algebra
specifically and statistics uh Udacity
has some great courses on both of those
things what do you think of Mike
reducing LSD I think it's a great idea
but you have to have a friend with you
what are your thoughts on the Udacity
nanodegree machine learning great course
what Linux distro do you recommend
Ubuntu how long have you been coding six
years but only seriously like three
years how can we help you other than
with money please promote my videos
seriously like get people to subscribe
to my videos like I feel like I put
all my time and energy to these videos
and I don't have nearly as many
subscribers as I want so anything you
guys can do to get people to subscribe
would be super helpful when would you
use an or net over a random forest up
man just go by this simple rule of thumb
if you a deep neural net will outperform
any other machine learning method pretty
much 90% of the time at least so if you
have a lot of data use a neural net if
you don't have a lot of data though if
you only have like 100 samples then I
would look into a random forest big ml
comm does that for you okay what's your
goal with YouTube I want to be a public
figure I want to be the face of machine
learning the face of artificial
intelligence the face of computer
science and I want to use YouTube as a
platform to get this stuff out there and
get everybody building AI and building
intelligent software all right how can
can you tell us how long will this
livestream at approximately be this is
going to be about 30 to 40 minutes what
is the coolest AI ml project you've come
up with I think building an AI Composer
it's my second machine learning for
hackers video build an AI Composer was
like my favorite project cuz you
generate music with machine learning how
cool is that why didn't you try for
masters and PhD well I mean my parents
definitely wanted me to have a master's
in PhD and they still kind of do and
they always will but I'm just going to
keep doing this because I have the time
to do this I can study on my own and I
have good advisors more or less so I'm
kind of doing a PhD but it's like not an
official it's like a it's like a new
thing and I just don't want to have to
deal with the bureaucracy of academia
not to say all research labs are
bureaucratic but most of them are um
please try to make these a little early
for us here in India I can do it 30
minutes earlier and much love to
everybody in India more d3 videos please
okay I can I can uh yeah all right good
to know
bose qc25 headphones
exactly it's uh what did you go to
school I went to Klein Oak High School
in Houston Texas that's where I grew up
and then I went to Columbia University
in New York City uh and that yeah for
undergrad
do you believe academia is the best path
for learning machine learning and AI uh
man that's that's a great question I
think any anything that could give you
the time and the time and and the time
to study machine learning on the
Internet is the way to go if so if that
means academia if that means you get a
scholarship and they're paying for your
housing and they give you a little
siphon so you can eat that's the best
way but if you can make money otherwise
and then you can just like live like you
know do some contract work then you can
you know then that's what I did I just
made some money doing contract work for
mobile development and I use that money
to just live and study machine learning
on the Internet
I can't view classes in real life I
can't pay attention to professors who
just go on and on and on and they just
uh you know I'm saying so I just wanted
to X by the 3x button on life and the
internet is the way people ask me this
question all the time like how did you
learn machine learning what is the way
to do this I just tell them every time
the Internet
the Internet makes everything obsolete
colleges were invented before the
Internet and that was the way to do it
and they were centralized but now with
the internet anybody can learn anything
online so I watch videos very fast and I
look at other sources like Reddit and
Twitter feeds I follow people like
Balaji Srinu Boston and the whole
injuries andreessen horowitz team so I
have a lot of data inputs I'm gonna have
one more question that I'm going to get
started uh what's the best entry point
for people that are relatively new to
machine learning and AI my videos start
from the very beginning and download the
code for each video and run it locally
and get it to work and study it I
promise you you will get to be a pro in
no time
ok so that's it for the questions let's
get started we're going to build this
two layer neural network and it's going
to predict the XOR output all right I'm
going to help so here I go I'm going to
share my screen and we're going to get
started
here we go screen share desktop start
alright I'm going to move this out of
the way and
- boom let's see you guys can see all
this alrighty
great cool cool cool cool
alright so let's get started um this is
a big enough thing yet ok so it's a two
layer your old network ok so here we go
we're even let's get started so the
first thing we do is import our
dependencies the first dependency we
want to we want to import is numpy which
is our scientific computing library
numpy is going to last you all of our
matrix multiplication which is very
important in neural networks matrix
multiplication is very important in
neural networks and I'm going to explain
why so that's our first library and our
next one is gonna be time and time is
going to help us a time time how long
our training is going to be ok so let's
so that's it that's all we're going to
implement the only libraries we're going
to import ok so let's just get started
let's go ahead and start with finding
our variables ok so the first one we
want to want to we want to define is our
number of hitting neurons and we're just
going to say 10 why because we're going
to have 10 values like ones and zeros an
array of ones and zeros that we're going
to input into our network and we want to
compare it to ten other ones and zeros
and then how put the export value that's
that's what the output is going to look
like just ones and zeros ten of them
okay and so now we want our to show the
number of inputs how many ten there's
ten uh neurons and a ten inputs and then
how many help us do we want to have well
we want to have ten glad to have you
here good vibes alright so here we go
so we want the number of outputs to be
ten okay and then one more thing we want
sample data
how much sample data do we want to have
well let's just have some really big
number and we're just going to pick ten
of them so what's a big number that we
can pick and we're just going to pick
ten of that let's do this is a neural
network for classification yes we're
gonna pick 300 all right 300 sample data
that we're going to generate okay so
that different variables okay so now
we're going to define our hyper
parameter what are our hyper parameters
well the first one is going to be the
learning rate and the learning rate
basically defines how fast we want our
network to move and these are tuning
knots we can make it go faster or we can
make it go slower depending on what we
want okay so that's our learning rate
and there's one more tuning knob there's
one more hyper parameter that we want in
this model and that's called momentum
okay
these are two different variables and
we're going to use them both to to lower
the loss function which is going to be
called cross entropy as we train on your
own net okay those are the only two
hyper parameters that we're going to use
all right
so those are our two hyper parameters
and
that's it so now we're going to generate
a random sample oh sorry now we're going
to see our now we're going to exceed our
random number generator what does this
mean well we're going to bet we're about
to generate some random data right and
we want to make sure that every time we
run this code it generates the same
random numbers every time why because we
want to test the code right and so
that's what succeeding the seeding make
sure that we generate the same random
numbers every time we run our code okay
so that's it so let's see this okay
so there we go well you know what I'd be
a little more descriptive
non-deterministic
see that's the technical term for this
in computer science right and I did a
video on I mentioned non determinism in
my P versus NP video okay so um so we
have that can someone say how many
people are in the room it's kind of it's
it's blocked okay so we've got the
sigmoid public so so now we've seeded
our numbers and now we're going to
define our activation function the
traditional active thank you good vibes
the traditional activation function that
we that we use is the sigmoid function
so let's write that out as a function
right and so sigmoid is a function
that's running every neuron in our
network so what is sigmoid look like so
let me just write out this formula and
my video should be watching order
absolutely uh
honestly if you were to if you were to
watch all of my videos from like a very
very start like the what is Bitcoin
video you would learn so MA I literally
put everything I've learned into these
videos like literally everything I've
ever I am into those videos okay so so
what is a sigmoid function the sigmoid
function turns numbers into
probabilities okay they turn numbers
into probabilities what does that mean
well in our neural network when we have
our input data it goes through the
network and each of the weights is a set
of probabilities like which way should I
go Oh point point seven percent this way
Oh point six percent this way Oh point
three percent this way output okay
and so probabilities and these
probabilities are updated when we train
our network so and so it gets better and
better over time so that's what the
sigmoid function does every time it hits
every time our input data kids um one of
those neurons are one of those layers
it's going to turn that number into a
probability that's all it does okay so
and so traditionally we would just use
one activation function we call the
sigmoid the activation function but this
time we're going to use two activation
functions why because it's a 4x or
specifically the the 10th the tangent
function the tangent function is is is
helpful because it it just makes the
loss even better than if we were to use
just so plain old sigmoid and I'm not
going to go do like super simple so I
could just use a sigmoid but I want to
be accurate right I'm going to make the
most accurate neural network that I can
from scratch okay so I'm going to use
two activation functions one is going to
be for our first layer and the other is
going to be for our second leg okay so
let me just write that down and so this
whatever it's returning is the actual
function itself it's the programmatic
function of the math function of the of
the dependent prime function okay so
those are our two activation functions
right okay so that's it those are our
two activation functions so now let's
write our training function okay so I'm
gonna I'm gonna I'm going to hit enter
I'm going to like move up so just
so take a screenshot okay three two one
here we go okay so I'm going to go down
to do that and now I'm going to write my
craning functions except train and so
the training function is going to write
a Gotham it doesn't make a different fit
to one or one point out you're
absolutely correct
so the training function is going to
take five parameters I'm going to write
them in them and explain what these
parameters are okay okay
be be be yell be cool DW
okay okay
so what are we doing here so what is the
X the X is the input data the V is our
transpose which is going to help us
perform matrix help multiplication the
the the sorry let me I miss something
let me start over the X is our input
data the T is our transpose which is
going to help us perform matrix
multiplication the V and W are our
layers to our network our two layers and
be V and V W are our biases our biases
are going to help us make a more
accurate prediction okay and they're
going to be one bias for each of the
layers in our network okay so that's
those are our inputs so let me just
write back a input data transpose a
layer 1 layer 2 and then biases ok so
let's just do this so the first one is
going to be a forward forward
propagation so we're going to do matrix
multiplication matrix multiply
Plus our biases okay so let's do this ok
so we're going to let me just write this
and I'm going to splain it right at the
right okay so NP to make your matrix
multiplication do the dot product and
then let me write one more buff up and
then do the same but this time use our
tangent function a ok so let me explain
this ok here we go I am going to
randomly shuffle the data and
so this is going to be a simple
perceptron yes top 1% exactly you can
call it a perceptron you can call it a
neural feed-forward neural network it's
a clever rebranding that they call it
all deep learning but you know this
stuff has been around since the 50s we
just now have amazing data and compute
and that's why it's awesome ok so here's
what we're doing this is forward
propagation ok
we're taking the dot product of the
input data X and we're putting it into
our first layer V the dot product is is
computing that matrix multiplication and
it's adding the bias in to get that a
that a delta value then we're going to
perform that first activation function
hi that's the first time I've been
called the Beyonce of neural networks
thank you that's pretty good actually
ah ok so then we're going we're going to
do the we're going to perform our
activation function on that date okay so
that's our first operation the next one
is going to be now we're going to have
to do our sigmoid function ok so so we
already applied our first activation
function now let's buy our next one
which is going to be the we're going to
take the value that we just had that we
just created see that Delta and then
we're going to add that the next layer
right the second layer W and we're going
to perform the dot product so we've
computed the first part and we're going
to take that result and we're going to
compute that next layer using the result
using the dot product okay so okay so so
now we're going to add our next bias
right by is for that we just need to do
this place right but we did it once now
we're going to do it this next time so
and we're going to calculate that
sigmoid using that value that we just
had and we're going to call it Y we're
going to call it Y ok so that's our
forward propagation so that goes forward
and so what we so when we have a
feed-forward neural network it's not a
recurrent but we do have something
called backwards propagation so so it
goes forward and backward it doesn't go
lude it just goes forward and backwards
so what does that mean we update it we
update our weights one way and then we
update them backwards and we just keep
doing that while we're training ok so
let's move backward propagations
backward so say so say backwards ok um
so let me just write this and then I'm
going to explain it it's going to be Q
lines okay let me get some of that
coffee and you know I'm saying that
stuff helps already go alright
MP dot products um keep W okay okay um
so yeah these are our two Delta's that
we're getting from for our backward
propagation and that's why we use our
transpose our transports are transposed
is basically our matrix of weights
flipped and we flip it because we're
going backwards right we we make the
matrix itself backwards of values and
then we and then we use that to
calculate our backward propagation
ultimately we want this evie value okay
and what are we going to use this to
stop this value for what we're going to
use it to predict our loss and we're
going to compare our predicted loss
function from an actual loss function
and we're going to minimize our loss
doing that okay so that's the goal we
want to minimize our loss that's how we
train we want to minimize our loss hi
Panagiotis from greece that's an awesome
name okay i love greens okay here we go
so let's predict our lives predict our
loss
okay so in so to predict our loss we're
going to calculate the app we're going
to take make let me let me just I'm just
right to sell and could someone shout
out how many people we have in the room
right now
evie we want to predict our lives okay
so what we're going to do is predict our
loss and these are two deltas and we're
using both of these that we're using the
Z value okay that we predicted from up
here this form thank you from this
forward step and then we're going to use
the the x value which is our input okay
um and so there's that and so now we're
going to calculate our loss okay thanks
guys so now we're going to calculate our
loss
so this is going to be quite a
long lines let me just write this out
and you know in tensorflow
in a bunch of libraries the loss
function is just a line of code you know
like you just say lost parameter but
we're doing this from scratch so we're
going to actually use the math here so
let's go ahead and say and and I have
something to say about this as well let
me about using math and machine learning
and if you should learn it or not and
what the poll okay so let me just do the
log oh one - why okay let me make sure
that this is correct loss is the
negative of the mean of the transpose we
take a log function of y and we add 1 -
2 transpose okay
so this is this is this is called
cross-entropy this is 8 this is 80
um Michaela I would I would do a meet-up
for sure yeah you know I should I should
I should do that
I actually I had a talk yesterday one of
my friends asked me to have a talk at SF
State and I did that um but there
weren't that many people I mean there
were like 15 but like dude there's like
a hundred sixty-eight people here you
know I'm saying live is awesome but yeah
there were enough people I would totally
do a meet-up yep um I'm looking to
organizing that or if you would look
into that that would be super helpful as
well okay so here we go so this is
cross-entropy okay so why cross-entropy
well there's a there's a bunch of
different loss functions there's the
mean squared error there's a bunch of
different loss functions but we're using
cross-entropy
because we're doing classification
generally if you're doing any kind of
classification task you want to use the
cross entropy function why it just tends
to give us a better result okay and then
one more thing about using math and
machine learning this is a long function
and it's kind of difficult to understand
do you need to know the math to use
machine learning to get good at machine
learning and my answer is no but it
helps no you don't but it helps because
if you ask some of the best JavaScript
programmers in the world right if you
ask some of the best front end designers
in the world they make the web sites
like Airbnb like stripe like you know
the best designers in the world how like
if you ask them to make a rectangle
they'll just do like you know um Rex you
know make right or some function but if
you
some like the details the mathematical
details the pixel values and you know
all the math that goes into making a
rectangle they won't know all of this is
a series of abstractions it's just a
series and series of abstractions
eventually we won't even need to know we
won't even need to know what
cross-entropy or any of these functions
are eventually we won't even need to
know programming we'll just tell it what
we wanted to do right so it's just it's
just a series of abstractions we're all
building on top of each other um and
right so I don't think it's necessary to
know the math to do this but it helps
okay that's that's it exactly don't
reinvent the wheel that's it
um okay so here we go so that's a cross
entropy uh right so we calculate all
that let's assume our last line of this
is going to return we have to return our
loss function okay so we're going to
return a loss function room and we're
going to return our Delta values and our
our errors right they are errors in our
deltas okay so - okay so that's it
that's it okay so that's it for a
training step I'm going to show
screenshot down I'm going to go down a
little bit more three two one okay all
right so here we go
now let's write our prediction so we
have one more functions right and then
we can get go ahead and get started with
our actual the meat of our code so we
have one more function to write and it's
going to be the prediction function this
is going to predict our value okay so
let me write down the variables be V and
then BW BW all right all right okay so
uh I coulis smiley Wow all right we got
some non ml people in here okay here we
go uh everybody say hi to Cooley smiley
she was a youtuber that I met in LA okay
so here we go so we're going to do a
prediction step okay so what does that
mean that means we are going to perform
matrix multiplication to predict our
value or our end result and to do that
we're going to use these these variables
that we've already calculate MP dot oh
my god
I need to just say this Alma Diaz said
when I'm on a date I don't need a neural
network to predict my loss that is gone
that is gold that is gold okay so um
let's anyway it's just gonna be two
lines it's gonna be two lines alright so
let me write that out and then I'm going
to explain it like what the heck is
happening there um bump and then we're
going to turn it and we're going to use
our sigmoid function to UM shove it up
step the greater than 0.5 as type int
okay so here we go sigmoid what's
happening here so a and B are both of
the the final values that we're
calculating using those variables that
we already use in our now okay and we're
using the and we're going to return
whatever we returned is going to be our
prediction and what is our prediction
going to be it's going to be a number
it's going to be a 1 or 0 okay and we're
saying if only if the value is greater
than 0.5 do we return a sorry if our
value is greater than 0.5 it's going to
be a 1 else is going to be a 0 and point
5 is just to predict it's a probability
all right tell us all right good point
good point okay so here we go um oh
thank you s hut all right so that's it
those are our functions let's go ahead
and get started we're going to set up
our initial parameters so we're going to
create our lair so let me create the
lakes now it's time to create our lives
okay so let me go ahead and do this
other bug okay so here we go so B equals
so our first layer is going to be random
normal scale equals 1 size equals number
in number hidden and then select our
first layer and now let's make our next
layer remember this is 8
you layer neural network okay so two
layers and we're going to create them
both the same way okay um and I'm going
to explain why we're using these
variables as our size right after I
finish typing this out okay there we go
two layers V and W and we're going to
the size is basically those that number
of the number of input layers which is
number of input values which is ten and
the number of hidden layers which is ten
as well
okay so good so ten in and then ten out
okay every time where's the stripe in
your hair it's there odd teeth I need to
make it more gray okay it's there for
sure though definitely check that out
okay here we go so here we go here we go
we're going to say all right Missy Giove
we're going to say we're going to
calculate we're going to create our
biases okay we've used our biases in the
parameters but we haven't actually
initialized our biases so let's use our
bicycles okay so we're going to say our
first bias is going to use our hidden
variable as a parameter and then we're
going to use our next bias which is
going to be our next value is going to
be the number of output layers okay
doesn't the up okay so those are our
biases okay and now we're going to
generate our data we've created our
layers we create our biases and now
we're going to create a variable called
parameters which we're going to input as
a parameter it's going to be an array
it's going to be an array of all of our
values that we've just created our two
layers inner biases
it's just something easier to input it
into our training function whatever we
call it um my hair is totally open
source if you guys want to have a silver
stripe in your hair go for it but just
like you know you can cry to me if you
want to I don't I don't care
everything about me is open source take
my ideas take my everything just it's
I'm open source you know I'm saying so
parameters and oh man it's really hard
to not
in comments because you guys are so
awesome I don't want to not talk to you
right um so all right so we create our
parameters let's generate our data okay
x equals NP dot random dot binomial um
all right so we're going to generate our
data and so this is why this is where
that that that sample of 300 variables
this is where it's going to come into
play this is why we initialize it so
that we can generate data we're going to
generate 300 samples and what I'm going
to use a learning rate in a second and
and um so that's what we've generated
our data and so now we're going to
calculate our transpose right guess what
guess what guys guess what it is time it
is time to train this ish okay it is
what I call training times it is
training time this is going to be
awesome
this is our last step it's a for loop
we're going to train okay
um all right and I'm going to get better
at the video quality and I'm gonna get
better at everything so just everyone
relax so here we go so training time
we're going to train this for 100 feet
box okay so for ePub for every Deepak in
the range of a hundred we're going to do
this for 100 a pox okay everybody say hi
to each other everybody's cool okay so
we're going to start off with and by
initializing our our MP or error array
what you're going to up we're going to
add data to in a second and then we're
going to save our update variable is
going to be the length of our parameters
that we just initialize up there and
we're going to also initialize our time
because we're going to time how long we
want to run this neural net okay we're
going to time it and so that for that
second variable that second dependency
time comes into play just for this okay
so now let's do this okay like for each
data point we want to for each data
point we're going to update our wait
of our network okay
we're definitely still fighting we're
definitely a bike
so the known network is to find the XOR
value of ones and zeros okay so for each
data point we want to calculate our law
our weights our weight okay so we're
going to say for every um value in the
reign of our input data okay so weeks
index and how do we say how big our
input data is we use the shape function
of X okay so we're going to say we're
going to take our log function we're
going to calculate our loss and our
gradient by doing what by using the
training function that we just
calculated right now we just uh sorry
initialize and declare and then create
and that's where our parameters comes in
play just for simplicity take we're just
using crams instead of saying x b b b pw
just say frames great thing about python
okay so that that's us calculating our
loss and gradings now we want to update
our locks well how do we do that well
we're going to have two more little
loops here okay so we're going to say
for J in range of the length of the
parameters
um prams J minus equals update okay okay
and so I'm going to do one more one more
and then that's going to be hit that's
that's literally I'm going to explain
what I'm doing and why I'm why I'm doing
it okay so um so okay so this is where
our learning rate comes into a pike
comes in to fight this war our momentum
comes into play both of those hyper
parameters that we initialize this is
where we actually use them so learning
rate times gradient um plus momentum
times the update value that we just
deleted those two for loops are going to
help us calculate our loss that's that's
going to help us calculate our loss okay
and then we want to append our error
with the loss that is it that's it what
am I doing here
stop okay cool say okay so that's it and
now we're going to print that that value
it's 35 minutes in okay how many people
we gotta hear someone shout it shout it
out now let's print out our results okay
we're going to print them out EPOC and
so I'm going to print it out using % be
using and so sorry % e and then okay
cool um logs is going to be % point 8f
we're going to print out our log so
gonna clean up our time like how long
has this thing been going on and then
we're going to say % um and then we're
going to take feet POC we're gonna print
out our epoch we're going to print out
our loss which is going to be the mean
value of our error and then art at the
time there's gonna be time dot clock and
percent minus the X starting point so
someone check this and make sure that I
have don't have any errors in my syntax
on this line epoch loss time point right
point four okay F s I think I'm good I
think that's good oh and so then uh
typos at line 83 line 83 what is my
typos my typos is lens is length
thank you okay so okay that's it for our
training function let's go ahead and try
to predict something okay so as I try to
predict something how do we do that well
we say okay let's say X is going to be a
random value find a binomial value one
point between 1.5 and how many of them
we want
okay so we're going to try to predict
something we're going to try to predict
the XOR prediction okay go okay well we
gotta print those that x value like what
are we going to predict and then finally
our prediction which we create we use
our predicts method for let's go ahead
and do this demo this baby okay
and our parameters boom that's it let's
see what is going on here let me let me
run this um all right bye can anyone see
this I can see this okay so let's go
ahead and run this okay so I've got
number of samples is not defined number
of samples is not defined
okay so on line 67 number of samples is
not defined oh did I not define that
what I did oh you know what it's a it is
a Oh
oh yes it is training it is okay okay
okay okay okay okay yes yes okay so what
just happened what just happened is I
only had a single error and I fixed it I
knew it
so this trained for a hundred steps and
uh you guys are so nice you guys are
awesome uh so I know I know I got to
calm down I'm just like really excited
that this worked so what happened that
first line right here let me let me make
this bigger uh make it a lot bigger I'm
making a lot bigger that first line is
our input data right and this next line
is our X or values if those are our X or
values okay and we basically said here's
our input data here's some randomly
generated ones and zeros and then
compute the XOR and then get better and
better over time
and as you can see our loss function is
getting better over time sees your loss
function point one for you know and then
one three and then way up here it's like
36 and so it gets better it's timing it
it's time in the epoch when the code
works it's alive it's smart it works
better
extra value okay so I'm going to end
this with a five-minute Q&amp;amp;A and I'm
going to post the code on github and
guys please share my videos like I'm
trying so hard to hit 50k subscribers by
January 1st so that's the best thing you
can do for me is hit subscribe Jake your
friends and hit subscribe I'm gonna make
so many videos I'm like just getting
started ok so 5 minute to a day and then
we're out of here thank you voice in the
box thank you sue darshan um what am I
trying to predict Trump I'm trying to
break the X or value alright can I use a
neural network to predict Black Friday
sales yes absolutely
um just look at a history of past sales
and try to create a future sale you want
to put that into a spreadsheet either
manually or I'm sure you can find it
somewhere
thank you Joe - I have not blocked you
cross-entropy in NLP exactly what's my
favorite color
sapphire blue what's your name on github
ll source L help
I actually wrapped about my name on
github if you looked I have not blocked
you I have wrapped about my name on
github just look at my channel trailer
it's a few do you watch the flash I
don't but I should what would be your
favorite company to work for you know
man I I don't know if I could work for a
company anymore maybe I mean the best
one will be open they are open they are
because it's awesome now that would be
the one um could you use an ER 11 to
classify a multi-dimensional time series
uh yeah no for sure absolutely a time
series is sequential data right so you'd
want to use a recurrent neural network
our CNN is good for prediction yeah I
mean for image actually CNN's have been
used for text prediction as well like
what's gonna be the next David they've
been using NLP will you buy the MacBook
Pro 2016 with in touch or yes I have it
I have it I have it right here but guess
what it's got USB seaports in my display
doesn't use USB see uh there's a little
touch bar so I had to use my old MacBook
so that sucks for Apple right ice it's a
deep mine but like I mean the to I mean
they're both good they're both good but
I like opening eyes openness and
democratizing thing value how
holding my I'm 25 years old are you on
are you an AI or Sarah geology I am
myself I am Suraj I'm officer Raj - Raj
Raj Raj two more questions and we're out
of here
how did you get into CS account shrift
in London I met a guy named Alex McCall
he was the dude who was like hello may
I'm coding and I was like yo this text
editor you've got here it's so pretty
and colorful I want to do stuff like
that
so yeah I'm really envious of you in San
Francisco you can make a lot of money
and do great things and from then on I
was like I gotta be a computer scientist
I was 8 I was 19 years old I was an
economics major at the time because all
I cared about was money and then I was
like wait a second there's more to life
than just money
alright so one more question um when are
you oh here's a good one can we use
neural networks for AGI that's a great
question I don't who knows I mean yes I
mean just the idea of a neural network
our brain is we know it's a neural
network but like gravy does our brain
use backpropagation deserve deserve rain
use gradient descent is our brain using
recurrent networks is it using
convolutional it's like all these
different type of networks we don't know
and but we're getting really close and
the state of the art is being broken at
an increasingly fast rate um every day
deep learning right now is where physics
was in the 1900s the Einsteins and
Newton's and Marie Curie's and famous a
scientist of computer sciences are alive
right now over there even they're in
middle school or elementary school or
they're in the field so it's a very very
exciting time for you watching and
awesome so that's it for this tutorial
thanks everybody for watching I'm going
to have my video come out very soon on
Friday and then a very very very very
special video that's going to come out
on Monday and it's going to be awesome
it's going to be awesome okay it's going
to be the video I've been most excited
to make ever alright so thanks guys for
calling out the number of people here
thanks everybody for watching this video
thanks Brian Christina
Aditya Andre
Kayle a cigar Shawn Ian elfish Dani top
1% multi ship juvie Oz you guys are
awesome thank you for watching please
share my videos I love you guys and for
now I've got to write so many video
scripts
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>