<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Solving the Basic Game of Pong | Coder Coacher - Coaching Coders</title><meta content="Solving the Basic Game of Pong - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Solving the Basic Game of Pong</b></h2><h5 class="post__date">2017-12-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pN7ETkOizGM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and pong
everybody's played it at least once and
let's see if we can create an AI to beat
the game by using a reinforcement
learning technique called
policy gradients when deepmind beat all
those Atari games using the same
algorithm a few years ago or when they
created the AI that beat the best go
player in the world
last year the AI community was blown
away but if you think about it the
algorithms they used weren't novel at
all in fact they've been around for
decades the deep Q learner that beat the
Atari games just use a standard Q
learning algorithm with function
approximation and you could find an
example of that from the standard RL
book by Sutton in 1998 as well as a
convolutional net which has been around
since the 1990s
alphago used a strategy called a Monte
Carlo tree search as well as something
called a policy gradient approach all of
these are standard well-known components
my point is it's not the algorithms
that's been the driver of recent
progress in AI but the amount of data
and computing power we've been able to
feed these algorithms recently so we've
talked about a lot of algorithms I've
listed here but one we haven't talked
about is called policy gradients and
we'll use it to help beat the game of
pong you might be asking shouldn't we
use a deep Q Lerner like deep minded
well it turns out that Q learning isn't
that great in fact most people prefer to
use policy gradients as they've been
shown to work better than Q learning
when tuned well the reason for this is
because it's an end-to-end
approach meaning it's one system that
can learn the entire game there's an
explicit policy and a principled
approach that directly optimizes the
expected reward
Pung is a great example of a simple
reinforcement learning task and we can
represent it as a Markov decision
process think of this as a graph where
each node is a particular game state and
each edge is a possible transition
each edge also gives a reward and the
goal is to compute the optimal way of
acting in any state to maximize rewards
you know how the game works two players
each gets a paddle and you have to
bounce the ball past the other player at
a more technical level we're going to
receive an image frame which is a 210 by
160 by 3 byte array basically pixel
values and we want to decide if we move
our paddle up or down based on that
after every single decision we make the
game will execute our action and give us
a reward we get a +1 if the ball goes
past the opponent and a minus 1 reward
if we miss the ball or zero otherwise
the goal is to move this paddle so that
we get lots forward something to keep in
mind is that we should probably not make
any assumptions about the way that the
game of pong works why because we
actually don't care about pong we care
about complex high dimensional problems
on getting a robot to save a person in a
natural disaster but if we can get an AI
to learn pong we can someday get them to
do more useful tasks using their ability
to generalize to any task our first step
is going to be to create a neural
network that represents our AI it's job
is to learn a policy so the network will
take the state of the game and decide
what we should do move up or down based
on that game state we'll call it the
policy network we can just use a 2 layer
neural network that takes the raw image
pixels of the game and produces a single
number indicating the probability of
going up every iteration we can sample
from this distribution to get the actual
move since we only have two layers the
neurons in the hidden layer will be able
to detect various game states like if
the ball is on top or our paddle is in
the middle and the neurons in the next
layer can then decide if in each case if
we should be going up or down think
about how difficult this problem is for
a second we're getting about a hundred
thousand eight hundred numbers and
forward our policy network which can
easily involve on order of a million
parameters for both layers if we decided
to go
up for a given time step the game could
return a zero reward and give another
hundred thousand eight hundred numbers
for the next frame it's possible we
could repeat this process a hundred time
steps before getting any non-zero reward
if we finally get a plus-one how can we
tell what made that happen was it some
move we played just recently or 40
frames ago maybe it had something to do
with frame twelve and then frame 38
which of the million knobs of our neural
network should we change and how in
order to do better in the future
this is a common problem in
reinforcement learning called credit
assignment how do we solve this well
let's think of the easy case first
aka supervised learning assume we had
labels for every single move as in we
could feed an image to the network and
get some probabilities for two classes
up and down we could compare that
prediction to a label which would tell
us the optimal thing to do at that time
step like go up or down once we find the
error the difference between the label
and the prediction we could use back
propagation to update our neural network
the most common optimization scheme
using a gradient vector to update our
weights after every single parameter
update for every training time step our
network would be slightly more likely to
predict a move that more closely relates
to the ideal move ie the label but let's
get real for a second we aren't going to
have labels for every single move this
is where the policy gradient solution
comes into play if our policy network is
fed an image of a game state and help
puts a probability of going up as say
30% and down as say 70% we can sample an
action from this distribution and
execute it in the game like down for
example but our problem is that we don't
know if down is good yet but that's okay
we can just wait a bit and see if it is
we'll just wait until the end of the
game then take the reward regards either
a plus 1 if we want or a minus 1 if we
lost and use that number as the gradient
for the action we've taken so if going
down ended up losing the game eventually
we can find a gradient that discourages
the network to take
the down action for the input in the
future and that's how policy gradients
work we have a policy that samples
actions and then actions that happen to
eventually lead to game wins get
encouraged in the future actions that
lead us to losing the game get
discouraged for training we can
initialize the policy network with two
sets of weights and play a hundred games
of pong assume each game is made up of
200 frames and we've made 20,000
decisions for going up or down and for
each one we know the parameter gradients
which tells us how we should change the
parameter if we wanted to encourage that
decision in that state in the future the
only missing step is to label every
decision we've made as good or bad if we
won 12 games and lost 88 we can take 200
times 12 decisions we made in the
winning game and do a possible update
we'll take the other 200 times
88 decisions we made in the losing game
and do a negative update that's it the
network will now become slightly more
likely to repeat actions that worked and
less likely to repeat actions that
didn't work not that crazy right three
things to remember from this video none
of the algorithms that have achieved
state-of-the-art results out of deepmind
our novel it's the data and the
computing resources we have now that
gives them breakthrough results the
policy gradients method involves running
a policy for a while seeing what actions
lead to high rewards than increasing
their probability through back
propagating gradients and the credit
assignment problem is a common one in RL
where we're not sure how to decide which
action and a I took contributed to the
reward he received last week's coding
challenge winner is Alex my OSA Dolph he
implemented a simple q-learning bot
using open a eyes gym environment to
solve the frozen lake environment really
efficient code he did it in just under
40 lines of Python and the runner-up is
Aditya Viera Parmar who solved the same
environment using a greedy action
selection algorithm this week's
challenge is to use a policy gradients
method to solve a game
other than pong poster github links in
the comments section and I'll announce
the winner next week please subscribe
for more programming videos and for now
I've got to go to sign some credit so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>