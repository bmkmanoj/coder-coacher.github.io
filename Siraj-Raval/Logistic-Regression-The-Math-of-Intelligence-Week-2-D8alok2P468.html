<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Logistic Regression - The Math of Intelligence (Week 2) | Coder Coacher - Coaching Coders</title><meta content="Logistic Regression - The Math of Intelligence (Week 2) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Logistic Regression - The Math of Intelligence (Week 2)</b></h2><h5 class="post__date">2017-06-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/D8alok2P468" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and today we're
going to build a machine learning model
called logistic regression to predict if
someone has diabetes or not given three
other features their weight their height
and their blood pressure using those
three features are going to predict if
they have diabetes or not okay so in
this video we're going to go over two
really two key concepts we want to go
over logistic regression which is a
machine learning model and we want to go
over the optimization technique Newton's
method so we've already talked about
Newton's method but we want to really
reinforce that idea in our head of how
it works and the medium that we're going
to use to help reinforce that idea is
this machine learning model called
logistic regression so before we start I
just want to say one thing - to really
understand how logistic regression works
we're going to have to go over some
terms from all four or actually five
pillars of machine learning all all five
disciplines that I talked about at the
beginning of this course calculus linear
algebra probability theory and
statistics so actually four pillars not
five we have to talk about little bits
and pieces from each of them so the
reason I say that is don't worry too
much if you don't understand every
single detail of why something is the
way it is we're going to talk about all
of these concepts in the in the
direction or in the in the context of
learning about logistic regression keep
in mind I have entire videos coming out
on all of these subjects so for example
we'll talk about matrix transposes but
and I'll say one reason why we would use
a matrix transpose but I'm not going to
say all the reasons but I do have a
video on matrices in general and linear
algebra coming out in a few weeks right
so just don't worry too much if you
don't understand every single detail
there's so many mathematical dependency
chains that we can that we can follow
conceptually right when we think about
these things but we want to have a very
specific chain of a very specific chain
or very specific train of thought
because there's so many that we can go
over right we only want to learn how
logistic regression works and to
reinforce our knowledge of Newton's
and if we do that and if we get how it
works and we're going to and I'm going
to define all the terms that we're going
to use right so you're going to get the
definitions you're going to understand
how the equations work and work but if
you don't understand every single detail
don't worry I have entire videos coming
out on it and I have it all planned out
so don't worry okay so I just wanted to
say that it was because there's a lot
that we're going to coat that we're
going to go over in this video okay so
okay now that I've said that let's go
ahead and start so that's so our task is
to compute is to predict if someone has
diabetes or not given their weight
height and their blood pressure and this
is toy data so we're actually going to
generate this feature these features
okay these three features okay so let's
start off with defining what logistic
regression is so logistic regression
compared to linear regression is a so
let's compare to linear regression right
so the key difference between logistic
regression and linear regression is that
linear regression predicts a continuous
outcome what is that
so versus logistic regression which
computes a discrete outcome so
continuous is means that we can have
values that range from from negative
infinity to infinity or just an infinite
possible set of values right so if you
have some series some number series the
next value could be any any you know it
could be any one of a set of numbers out
of infinity but for a discrete outcome
it's either one thing or it's another
right yep you kind of you box it into
some some some container so a discrete
outcome would be is it blue or is it red
whereas a continuous outcome would be
what is the value what is the next value
in a series of house prices right you
can have two values would be one or two
it could be some number between there
like 1.5 it could be a number between
that like 1.25 and then infinitely small
in that in that directions you see what
I'm saying it could be an infinite
number of possibilities and so linear
regression is great for that but what if
we want to predict an outcome like say
if someone has diabetes or not well that
is a discrete outcome it is a binary
outcome either they do yes or they don't
know
and so logistic regression is basically
our way of modeling a discrete outcome
right and so the outcome is which is the
dependent variable is going to be
continuous in the case of linear
regression and discrete in the case of
logistic regression and so why do we
call it logistic regression well it all
comes from that key function called a
logistic function that underlies how
that underlies a model it is really the
core functionality of the model and we
also call the logistic function the
sigmoid function so if you've done deep
learning before or if you've done neural
networks before it's okay if you haven't
it's used a lot okay so this is what the
function the sigmoid or logistic
function looks like it's 1 over 1 plus e
to the negative x ok and so why do we
use e so so there's a lot of reasons
that I mean there's entire textbooks on
why e the natural number or Euler's
number is used but one great reason is
because when you take the derivative of
e to the X you're going to get either
the X again and it's the only fun only
known function that does is where if you
derive it it's going to give you the
exact same result and of course this
goes for all all versions of versions of
e to the X where you can have any number
of coefficients so 5 e to the X or 3 e
to the X the same thing applies so the
point of that is that it makes it
computationally convenient or
mathematically convenient right so you
can see from a high level how having a
function that if you derive gives you
the same value is unique and in that
uniqueness can help with other
computations ok so so right so that's
that's why we use that's one of the
reasons why we use e anyway
e is a part of the sigmoid or logistic
function it is an s-shaped curve so
given any number of values it's going to
output a value between 0 and 1 and that
is a probability value and that's what
we want right we want the probability
that this person has diabetes or they
don't have diabetes and then based on
some threshold like let's say it's above
percent we could just say oh well if
it's above 70% yes they have it or below
no they don't right so we can define our
threshold or we could just output that
probability value but we want a discrete
outcome right a boolean yes or no so
that's at the heart of the algorithm and
so logistic regression uses an equation
as its representation okay it is an
equation value and what I'm going to
show you what the equation looks like
but the central premise of logistic
regression and when you want to use it
is when you have an assumption that your
data is linearly separable and that
means that given n dimensions where n is
a number of features in your data you
could separate it linearly so you could
draw a line that acts as a decision
boundary between all of your data points
so in a two-dimensional space this would
be in n minus 1 so one dimensional line
in a 3-dimensional space it would be a
three minus one or two dimensional plane
and this goes on for infinity right for
where n is a number of dimensions and so
that would be a hyperplane so basically
you're under the assumption that your
data is in fact linearly separable and
if it is then you can use logistic
regression as a way to model what the
class is given the feature set okay so
so let's let's talk about this so what
is the function for logistic regression
look like so let's say we have the
following function right there denoted
by these beta beta coefficients here
right so it starts with beta 0 and then
beta 1 beta 2 and then you have these
variables x1 x2 now all of these are our
unique variables right so just think of
it like y equals that well skip the Y
but MX plus B right and you can you can
have an infinitely long equation of this
form where you can have B plus B 3 x3
plus B 4 x4 so it's of this form and so
where X where B is beta which these are
the coefficient values which are the
weights right that we want to learn we
want to learn what the optimal weights
are that's going to output what we
wanted to output and so if we had this
function and we plugged in some point so
let's just say it's this function alone
if we were to plug in some point a B or
XY whatever you want to call it into
that at a coordinate pair if we were to
plug it into this it could either give
us a positive result it could give us a
negative result or it could give us a
zero which is a point that lies right on
the decision boundary so assume that if
it's assumed that if it's positive it's
going to be of one class if it's
negative it's going to be of the other
class and if it's zero it's going to be
right in the middle so it's of neither
class and so what we want is to set this
equal to some us to some probability
value or some probabilistic value right
so so we could do one of several things
well one is let me skip this for a
second let me let me go down here so one
thing we could do is we could set this
equation not to just Y like we do with
linear regression but set it equal to
the probability of x given the function
right so given this given this function
we plug in the coefficients it's going
to output a probability value so we
could use the probability or we could
use the odds and the difference between
the probability and the odds is that the
odds is this is this expression right
here which is the probability of x over
1 minus the probability of x so note
that probability of X and the odds of X
essentially denote the same thing which
is the problem the ratio of the
probability of the event happening
versus it not happening so why do we use
the odds so here's why so given you know
so given four possible values that we
could set our equation to probability
the log of the probability the odds and
the log of the odds if we set the
equation equal to as you see right here
the log or logits or law gifts of the of
the odds then it's going to let us have
the greatest range of values such that
it's going to map to between 0 &amp;amp; 1 let
me explain that so that means that we
can have any value for our coefficients
so the right-hand side of this equation
right here which is this could this
equation right here can be between
negative infinity and a
and infinity and the left-hand side will
be equal to some probability value
between 0 and 1 and so what this means
is we can have any number of values that
we can use for our coefficients so it
allows the widest range of coefficients
to be used for our function such that
the probability output is going to be
between 0 and 1 and when it comes to
reasons of why do we use the log
probability versus the other three they
like the probability the why do we use
the log odds for such as the logs
versus just the odds alone and versus
the probability versus the log
probability well to sum it up it makes
it easier it can be extremely
complicated if we were to just use the
probability alone because we would have
to find a set of constraints under
regression line for the regression
coefficients because basically I mean
the log put getting or computing the log
of the probability makes it easier for
us because the value is going to be
between negative infinity and infinity
for what's whatever is on the right-hand
side and there's a lot more Theory here
right so there's a lot of probabilistic
theory here but let's just say like this
is what I said at the beginning let's
just for all intents and purposes of all
the things that we could set our
equation to we're going to set it to the
log odds we're going to set it to the
log odds and that's it okay so Givens
whatever regression equation we have on
the right hand side we're going to set
it to the log odds and that's going to
give us the optimal relationship between
any number of values that we could use
for our coefficients and a probability
value right so the general model for
logistic regression looks like this
where we have some log odds probably
logged odds on the left and then some
cokes and then some function on the
right right so so in this case you could
say the probability of having a disease
is P so we've seen a log probability of
having that disease over 1 minus the
sorry the log of the probability of the
disease over 1 minus the probability of
the disease is going to be equal to our
function when these coefficients are the
effects
the genetic factors and so each of these
coefficients are going to be basically
tune these are weight values that will
be learned as we learn as we train our
model and then these X values are going
to be the variables for those genetic
factors so though each of those will
represent one of the features that that
results in the probability that an
individual has a particular disease okay
so right so so this whole process that
we're doing here that this whole process
of finding the ideal values for the
coefficients to to to find the ideal
probability values there are the ideal
probability values it's called maximum
likelihood estimation or MLE
so maximum likelihood estimation is a
general approach to estimating
parameters in statistical models by
maximizing the likelihood function so in
deep learning we call this back
propagation so there's a difference here
between ml e and optimization okay so
Newton's method is an optimization
algorithm okay so you can use this
algorithm to find the maximum or minimum
of many different functions including
the likelihood function you can obtain
ml e you can obtain maximum likelihood
estimates using different methods and
using an optimization algorithm is one
of them ok so so let's talk about the
word maximum likelihood estimation we
are maximizing the likelihood that our
model classifies some novel data point
as a correct class that's what we call
it maximum likelihood estimation we are
trying to maximize the likelihood that
our model estimates or predicts the
correct class for a novel data point and
so optimization is one technique we
could use to perform an le but there are
other techniques but we don't have to
worry about that ok so like I said we
are focused right now we are focused on
logistic regression and Newton's method
and we're trying to inform we're trying
to perform ml e and we're using Newton
method as our optimization technique to
perform ml e
ok right so so why use Newton's method
right so we talked about
logistic regression the what what the
general equation per looks like we
talked about that we talked about how
the process of us training our logistic
regression model
it's called maximum likelihood
estimation and in machine learning or in
deep learning it's called back
propagation right so that's why you
never hear the term MLE in deep learning
because we just call it back propagation
for supervised learning where there is a
label but in general machine learning
the word the phrase maximum likelihood
estimation is used quite a bit so anyway
why Newton's method well it usually
converges faster than gradient descent
when maximizing logistic regression log
likelihood so it's faster when it comes
to logistic regression and but the one
thing to note and this is not a reason
but one thing to note is that each
iteration is more expensive
computationally than gradient descent
because we are calculating the inverse
of the Hessian and the Hessian and we'll
talk more about this in a second is a
matrix of second order partial
derivatives and those are derivatives
with respect to each of our coefficients
or each of our weights in our function
but it's not just the derivative of them
that would be the Jacobian it's the
derivative of the derivative okay and so
let me say another thing
so the Hessian is going to be used quite
a bit in this video and we're also going
to use some matrix terms that were not
without that we've never used before but
we'll learn about them as we go and yeah
anyway so yeah we're going to learn some
matrix operations as well here anyway so
as long as the data points are not very
large Newton's method is preferred it's
a preferred methods okay so if we have
few data points and we're using logistic
regression Newton's method great for
optimization to perform maximum
likelihood estimation okay so one more
thing before we get to the code and that
thing is what are some other good
examples of logistic regression and
Newton's method remember although the
utilities that we're trying to perform
is the technique if someone has diabetes
or not given some features the data
in fact generated it is in fact I data
and the way to improve your
understanding of this is to remove this
anyway
is to look at other examples so I've got
one example here that uses a
scikit-learn a little bit for data
pre-processing but more or less it's
using it's very mathematically legible
so check out this one and it does
something similar using Newton's method
analogous or Russian or spam
classification so that the idea is that
you can detect if an email is spam or
it's not spam and then the other one is
click-through rate classification you
can classify based on some consumers
click-through rates if they're going to
or based on their other patterns if
they're going to click through so you
can compute the click-through rate
anyway these are two great examples
definitely check them out to improve
your understanding of both Newton's
method and a logistic regression anyway
so yeah so that's it for our examples
we've defined logistic regression we've
defined Newton's method both at a high
level and now let's get to the code
alright so for the code part we've got
our dependencies here and our
dependencies are importing numpy for
matrix math pandas for data manipulation
so a lot of data a lot of data
scientists use pandas a lot in fact
there's some when asked what what tool
do you use they'll just say pandas they
won't even say python because it's that
ubiquitous but basically pandas is a
really popular library for manipulating
data and what it does is it takes some
data set whether it's an Excel
spreadsheet or something else and it
will put it into an object a native
object in the in the MV dependency that
is called a data frame and once your
data is in a data frame you can perform
a whole bunch of really easy getters and
setters on that data so you could
specify the column and then the row and
you could delete that data that data
points or you know whatever but it's
basically very convenient for data
manipulation and so then we have import
warnings for error logging and we do
have one single very very very thin
wrapper on top of numpy but we're only
going to use it for one line and I'll
talk about the details here but
basically
is a it fits our data into a matrix
anyway don't even worry about it for all
intensive purposes this is not using any
you know heavy libraries it's all very
thin libraries all the logic is going to
be there anyway okay so the first thing
we're going to do is we're going to
we're going to define the sigmoid
function right so the function that
talked about before that s-shaped curve
and what the sigmoid function does is it
outputs a probability between 0 and 1
okay that's it which is 1 over 1 plus e
to the negative x so now I have this
right here to this but let's get this
for now I'm going to come back to this
so we've defined our sigmoid function
which is that key function that is the
core of logistic regression we define
that function on its own we're going to
use it later but now let's define our
hyper parameters right these are the
these are in machine learning we call
them hyper parameters and these are the
parameters of the model that we're
building so the first thing we're going
to do is we're going to set the seed and
the seed is used for anytime you're
using any kind of randomly generated
data if you have a seed then whenever
you rerun that program it's going to
output the same random numbers so why is
this useful for reproducibility which is
good for debugging right you always want
the same values to be generated you
could test your code it so it's
basically for reproducibility for
debugging and then we're going to define
a convergence tolerance as this very
very small number right here basically
this is the minimum threshold between
the predicted output and the actual
output is going to tell our model when
to stop learning right so when the
difference between our predicted output
and our actual output reaches this
threshold then we're going to say okay
that's good enough as opposed to just
saying well whenever we finish the
number of iterations we're also adding
this threshold which we're going to call
the convergence tolerance and we'll use
it and you'll see why later but
basically it's a threshold for when we
can stop learning I had this other term
here called the l2 regularization term
is going to help us regularize so now
I'm going to go back up here and talk
about what regularization is so
regularization is a very important
technique in machine learning to prevent
overfitting we use it all over the place
in deep learning in general machine
learning we use it everywhere basically
it's a technique to prevent overfitting
and what do I mean by overfitting that
means when your model is too trained on
the training data and it's not going to
generalize well or new data so if you
give it some new data it's not going to
be able to make proper predictions
because the training data that you
tested it that you train the data that
you trained it on is too similar it's
too homogeneous so if you give it some
very different data set it's not going
to be is not going to predict that well
so what regularization does is it's a
technique to help prevent this and we
have several techniques to prevent
overfitting but regularization is a very
popular and important one and so
mathematically speaking it adds a
regularization term so there are two
types of regularization right here that
we're going to talk about one is called
the l1 regularization and the other is
called the l2 regularization so the
difference is l2 is the sum of the
square of the weights while l1 is just
the sum of the weights right and so we
have the actual output minus the
predicted output and then squared plus
our regularization term which is the sum
of the weights versus the sum of the
square of the weights times this lambda
term which is the regularization term
okay and that's going to give us and
what that's so adding this this addition
is basically what this helps our model
not get over fit to the data and so when
should you use l1 versus l2 well here's
a list of when you should use them and
when you shouldn't there are there's
actually a bigger list out there but at
a high level we are the reason we use it
is use case specific so if you have if
you have a small data set and you don't
care about computational efficiency
because it's a very small data set then
you would likely use l1 we
if you don't you could use help too and
then also relates to the sparsity of
your data which means how many zeros
does the data have how sparse is it if
it's if there's a lot of zeros and it's
very sparse okay so and there's a lot
more reasons but basically in this video
we're going to be using the l2
regularization term and so anyway yeah
so that so there's that anyway so we're
going to use l2 regularization and then
we're going to have 20 iterations for
training okay so now so these variables
are used to help create our data okay
and so these terms come from linear
algebra so the first term defines the
covariance between X and Z covariance is
a measure of how two variables move
together so it measures whether the two
move in the same direction which would
be a positive covariance or in the
opposite direction which would be a
negative covariance so we have two of
those and so we want to set it to 0.95
so we have a positive covariance we want
these very blue wants two of our
variables to be related very closely and
that would be our height and our weight
and blood pressure can be kind of off
we're actually creating this data right
now so we can define what the what these
terms are whereas in normal data we
wouldn't define it they would just exist
right and then we would have to discover
these relations but we're defining these
relations right now
right now so then we have a number of
officer observations which is a thousand
and a thousand is the number of data
points we have data points observations
same thing and then we have a sigma turn
which is the variance of the noise which
is how spread out is this data okay
so again we're going to get back to
these terms we have a lot of probability
to go over in the future but at the high
level that's what those terms meet the
spread of these the spread of numbers
the relationship between two variables
and the number of data points that we
have all right so now we're going to
define our model settings so we have a
set of true beta coefficients so let's
just assume that we know what the ideal
coefficient should be for our model okay
so we have those true beta coefficients
and then we're going to have our
did beta coefficients and we're going to
ask we're going to we're going to use
the true beta coefficients to help us
calculate our predicted beta
coefficients and then we have a set of
variances for each of these inputs and
that is how spread out each of the
inputs are and remember we are manually
defining these as well as our model will
manually define what our model looks
like the shape of our model given our
three features right X Z and B which are
the height weight and blood pressure
okay so we defined our model hyper
parameters and now we can generate and
organize our data right so to generate
this data now this is assuming we don't
have real data so we're going to
generate it and so I'm going to
introduce a few statistical terms here
specifically the distribution right so
what is the distribution at a high level
of distribution is a function that
provides us all the probabilities or
provides us the probabilities of all
possible outcomes of ASA tastic process
that is a process that cannot be
predicted a stochastic process is one
that cannot be predicted and a
deterministic process is one that can be
predicted okay so it looks like a bell
curve usually there's all these
different types of distributions out
there you've got Bernoulli binomial
normal you have a whole bunch of
different distributions that represent
all the probabilities of a possible
outcome for a specific outcome and so
that's it at a high level and we have I
have an entire video on distributions
coming out later but for now know that
that's what a distribution is and we're
going to use the distribution to
generate our data we're going to use
what's called a multivariate normal
distribution to generate values for X
and Z so we're going to use the same
distribution to generate values for x
and z because remember they are closely
correlated height and weight and they're
and we also defined our covariance right
has very high 0.95 so x and z are very
are very closely related
and then b is going to be our variable
for our blood pressure and we'll and
we'll generate that using a normal
distribution so a multivariate
distribution normal districts a
multivariate normal because we had
multiple variables
X and Z and just a plain old normal
because we only have one variable to
generate it from okay and we're going to
compute what's called the transpose of
the result and the results of these are
going to be in a matrix and a transpose
is when we take the rows and the columns
of a matrix and we flip that right so
that's that's just what that's a
definition of a transpose okay and
that's going to make it more neatly
formatted for us for future operations
that we're going to perform on it okay
so we have our variables so let's create
a pandas dataframe remember I said how
in pandas a data frame is a very neat
object a neatly packaged object and lets
us manipulate our data very easily so
we'll put all those variable all three
of those variables in our panda's data
frame object and then we're going to
compute the log odds for our three
independent variables using the sigmoid
function so we're going to take the
sigmoid of let's take these three
functions x or the dot product since
it's technically a matrix instead of
just saying time so we're going to say
that dot product is because we're
multiplying two matrices together so the
matrix of our features times or by
computing the dot product with that and
these these ideal coefficients that we
defined before plus Sigma which is the
which is the what was it it was the
variance of noise how spread out our
data is times a normal distribution
between zero and one for each of our
data points N and so that's going to
give us the log odds which we can denote
up here okay and once we have that then
we could say okay so those that's those
are the log odds and we want to compute
the probability sample from a binomial
distribution okay so a binomial random
variable is the number of successes X
has in end repeated trials of a binomial
experiment a probability distribution of
a binomial random variable is called a
binomial distribution right so we have
some Y output value and it could be
between 0 and
one and so we'll just compute all those
probabilities like using this so
randomly generated probability values
okay so then we can create a data frame
that's going to encompass our input data
our model formula and our outputs right
just like this so these are our expected
output so we generated these outputs
randomly but they are expected outputs
so compare the expected outputs to our
predicted outputs and want to minimize
the difference between them
okay and that is our learning process
all right and so if we were to print
this out we would get all these randomly
generated values and notice how we don't
just we don't just have we don't just
have values for our three variables or
sorry our three features we also have
values for e to the X and then V squared
plus Z okay and so which is all a part
of our model our formula our function
that we're trying to learn here okay and
so we have that now we've defined our
data and we define what a logistic
regression is how we optimize it
Newton's method we've defined our data
and now we can have this helper function
which is going which is just going to
catch matrix errors that's not even
wouldn't even really need it anyway so
now it's time for us to for us to learn
our model and the way we learn our model
is by performing Newton's method for
optimization and so the way for us to
perform Newton's method is like so okay
for a logistic regression here's how
utans method works so recall that
Newton's method for maximizing or
minimizing a given function f of its
coefficient so function given the beta
value iteratively computes the following
estimate guys so the ideal coefficient
is going to be whatever the current
coefficient is minus the Hessian a the
Hessian of our function or the inverse
of our Hessian times the gradient of our
function and so that's how we let the
high level of Newton's method for
logistic regression okay so the Hessian
how do we compute the Hessian of the
log-likelihood for logistic regression
the way we do that is like so we'd say
the Hessian of our function is equal to
the negative transpose of n times P plus
1 which is what X represents n times P
plus 1 times n times n and n times n
diagonal matrix of weights where each of
these weights is P times 1 minus P times
X again we already defined X so we'll
multiply again ok so I know that was a
lot to take in but that's the formula
that's the formula for computing the
Hessian that's the formula for our
Hessian and in the function then the
formula for our gradient is much simpler
it's our gradient is the transpose of x
times the column vector minus n vector
of probabilities by the way n is the
number of data table samples we have and
the gradient is a vector of whose
components are the partial derivative
with respect to each coefficient we have
in our function whereas the Hessian is a
vector where it's computing not the
partial derivative but the but the
partial derivatives of the partial
derivative so it's a derivative of the
derivative it's a second order
derivative so if you were to say X cubed
is your function the derivative would be
power rule 3x squared and then the
derivative of the derivative would be 6x
okay so that's how that works and so by
the way and so this is what that W looks
like it's a diagonal which is if you
were to have a matrix of values you
would just take the diagonal of that of
P times 1 minus P where P are the
predicted probabilities computed at the
current value of the coefficient that
you have ok so and so you can connect
this to something called iteratively
re-weighted least squares but we're just
not going to do that this does for a
separate time as it's for more
understanding but in fact I think it
would be more confusing and we haven't
talked about that right now anyway right
so to
or Newton's method which is a
second-order optimization technique on
logistic regression we're going to
compute both the hessian and the
gradient and we'll use both of those
values to help find the optimal
coefficients for our function and
logistic regression is basically on one
side it's a Jena it's a generic
regression function you know when you
have some set of coefficients for any
number of parameter values for any
number of features or dimensions and on
the left hand side you're not trying to
equate it to a single scalar that maps
directly to that that output but instead
a probability value that ad log odds the
log of the probability over 1 minus the
probability and this is going to come
out to an s-shaped curve so you can then
predict the outcome of some events or
the class of what something is going to
be which is going to be a discrete a
single value yes no or even multivariate
or even a multivariate output like red
blue or green ok so right now let's talk
about the implementation here so we've
talked about the equations so let's look
at the code part now okay so the first
thing we're going to do is we're going
to compute the probability value and to
do that we're going to compute the dot
product of our coefficients are ideal
coefficients and our coefficients that
are that are to be learned the
coefficients that we're learning and
then we'll squash those values with a
sigmoid function squashing means we're
converting them into values between 0
and 1 and then we're only going to do
that for two dimensions so that's why we
have that nvm nvm min equals two
parameter and we're going to take the
transpose at that and so for for
matrices computing the transpose means
taking the rows and the columns and
flipping them so the columns become the
rows and vice versa and that's just one
of many matrix operations that we do to
keep it simple right now it just makes
things easier it makes it more organized
and formatted for for the next
operations that we're going to do
we'll talk way more about matrices later
on anyway so that's how we compute our
probability which is going to be an
array of values and then we're going to
use that probability to help compute our
weights which are as we set up here
right up here the diagonal of the
probability times 1 minus P ok and so
we'll have that here and that's our
weights those are our weights and we'll
use our ways to derive our Hessian so
remember this was a formula for the
Hessian the negatives negative X X's
transpose times W times X again right so
in the case of logistic regression
that's how we compute the Hessian you
can see variable by variable that exact
equation is then mimicked here
programmatically to compute that Hessian
and the same case will be for our
gradients right so our gradient is going
to be x axis transpose x we're going to
take the transpose of x and compute the
dot product of it and then Y minus P
right where Y our our predicted outputs
and then P our probability values and
that's going to give us our gradient
value and this remember this is all for
a single step so this is for a single
collection of data points right we have
a way to go through all those rows and
columns right but this is for a single
step and we have multiple steps during
training okay so that's our gradients
our Hessian our weights values and our
probability values okay and we're going
to use all of those we're going to use
them all to help us well we use the P we
use P to help us compute W which we use
to compute our Hessian and our gradient
as well but basically we're going to use
both our Hessian and our gradient to
compute what the optimal weights should
be right so to compete to do that we're
going to take the least square solution
but actually so okay so okay so we do
this twice we do this once the Newton
step and this is basically a copy of
what we just did before except for one
thing and that's this line right here
notice how right here I have Lin al
genes versus Lin alpha llst
sq which is Lisa
squares so the reason that it's here
twice is because right now we're
computing the amount or the scalar that
we're going to use to update all all of
our coefficients but we're doing it
using the full Hessian right and so the
reason that we did it two ways is if we
use the full Hessian it can be
computationally expensive because
there's a lot of values but we can get
around that by using a by not computing
the full Hessian but using a solution
called least squares and that's called
the so if we don't have to compute the
full Hessian then that's computationally
less expensive right and so there are
methods for doing this and these are
called quasi Newtonian methods or we
don't compute the full Hessian but we
don't actually have to talk about that
we'll just assume that we have to
compute the full Hessian right now okay
so that's what this line does and it
uses the regularization term we defined
before to make sure that this data is
not over fit okay so that's what those
into a mutant step it's going to return
this scalar value this beta value that
we can use to update the coefficients of
our function that we're trying to learn
and yeah so then we have a convergent
step so remember how we define a
threshold that threshold is when we want
to stop learning right so given our old
beta values our new beta values and then
our tolerance yet and a number of
iterations will check the change in the
coefficients by taking the difference
and making in performing absolute value
because we want it to be a positive
value and if the change is greater than
our threshold and we still have less
iteration and we have a certain number
of iteration still to go then we'll
return true else false okay so that's
our way of stopping training ok so then
to the real meat of our code right so
we'll go ahead and define our initial
set of coefficients and these are our
weight values right that we want to
learn they're all going to be zeros
right for the number of columns that we
have which are
the number of features or they'll start
off as zero and we'll learn them over
time given some number of iterations and
a boolean value that tells us if we've
reached convergence or not so this is
our way of saying should we keep
training or not and that that's what
this is going to do so we'll say while
not coefficients have converged so if we
haven't reached convergence then go
ahead and begin training using Newton's
method began optimizing so we'll say
well set the old coefficients to our
current values perform a single step of
Newton's optimization on our data and
once we have that we'll then take our
coefficients it's going to output the
updated of beta values okay and so then
once we have our updated beta values
then we can increment the number of
iterations that we're going through and
then check if we've reached convergence
yet where our beta values are the
coefficients for the model that we're
learning and we can say hey if these
values are at that certain threshold
we're good we're done here
okay so then when we print out the
results it's going to tell us number of
iterations and then the beta values over
time and ideally they become closer and
closer to our predefined beta values for
our data set I know that's a lot to take
in but if you've got the basic idea of a
logistic regression being different from
a linear regression in that it's trying
to predict the probability of a certain
outcome not just some direct linear
mapping between x and y and that we use
Newton's method as a way to optimize
logistic regression and the way we
compute Newton's method it's in compute
the second derivative with respect to
each weight in our in our function the
coefficients the weight values if you've
got that part
that's all that's really necessary for
this video with a lot of terms that we
thought to go over a lot of
probabilistic terms like variances
variance E's and and noise values
there's so much more so don't worry
about it it's a lot we're going to go
over that and the rest of the series so
that's it please subscribe for more
programming videos and for now I've got
a Ryan Bay
alpha and beta so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>