<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probability Theory - The Math of Intelligence #6 | Coder Coacher - Coaching Coders</title><meta content="Probability Theory - The Math of Intelligence #6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probability Theory - The Math of Intelligence #6</b></h2><h5 class="post__date">2017-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PrkiRVcrxOs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and let's laser
focus on the role of probability theory
in machine learning by building a spam
classifier from scratch life is full of
uncertainty we try things which we think
will probably succeed but we're not
certain will it rain today is it okay
for me to dance in public should I
invest more time in this relationship
probability theory gives us a framework
to model these decisions and by doing so
we can make them more efficiently there
exist branches of math that help us make
decisions when we have perfect
information but probability trains us to
make decisions where there are indeed
observable patterns but also a degree of
uncertainty
aka real life it's a measure of how
likely something is to happen and the
practice of analyzing events governed by
probability is called statistics a
simple example is flipping a coin there
are only two possible outcomes
heads or tails we can model the
probability of heads happening since we
know two things the number of ways that
can happen and the total number of
outcomes we've got a 50% chance in this
case just like how often Bluetooth
decides to work this is a random
variable it denotes something about
which we are uncertain an unpredictable
event it's not a variable in the way
that algebra denotes them instead it has
a whole set of values also called the
sample space and the probability of any
one value in this set it denoted this
way they can be either discrete so they
only take certain values or continuous
taking any value within a range we have
two possible events and B say we're
tossing a coin and throwing a six-sided
die we can measure their probabilities
three different ways given that a coin
lands on heads what's the probability
that the die lands on four this is the
conditional probability we can also
model the probability that both events
occur like what's the probability that
the coin lands on heads and the die
lands on four that's the joint
probability and if we want the
probability for specific outcomes like
just the coin or just the die
we call that the marginal probability we
make lots of assumptions like this in
machine learning so
the wrong Numenta so there's this really
popular formula called Bayes theorem
that's built on top of the axioms of
conditional probability it's called a
theorem because we can prove its truth
using logic it states that for two
events a and B if we know the
conditional probability of B given a and
the probability of a we can compute the
conditional probability of a given B in
other words the posterior probability of
a given B can be calculated by
multiplying the likelihood by the prior
probability terms and dividing their
product by the evidence term the prior
probability of an events often called
the prior is the probability calculated
using info that is already known the
prior probability of rain on a given day
could be calculated as 0.6 if you know
that 60% of the days on that same date
have been rainy for the past 100 years
we started with a prior and now we have
new information that we can use to more
accurately reott the same probability is
the Bayesian statistician linda lee once
put it grab your glass when you see
waitron quote today's posterior is
tomorrow's prior we can use this theorem
to update probability in light of new
knowledge
so how is this used in machine learning
there's a family of linear classifiers
that are based off of Bayes theorem
called naive Bayes classifiers they tend
to perform really well especially for
small sample sizes that's why they
outperform more powerful alternatives
naive Bayes classifiers are using a
bunch of different fields from
diagnosing diseases to sentiment
analysis to classifying emails as spam
which is what we'll do they make two big
assumptions about the data the first is
that the samples are independent and
identically distributed
they act as random variables that are
independent from each other and are
drawn from a similar probability
distribution the second assumption is
conditional independence of features
that means that the likelihood of the
samples can be directly estimated from
the training data instead of evaluating
all possibilities of X so given an N
dimensional feature vector we can
calculate the class conditional
probability that means how likely is it
to observe this particular pattern X
given that it belongs to class Y in
practice that assumption is violated a
good amount of time regardless they
still perform pretty well to make a
prediction using naive Bayes we'll
calculate probabilities of the instance
belonging to each class and to let the
class value with the highest one this
kind of categorical data is a great use
case for naive Bayes we'll start by
loading up our data file it's in CSV
format so we can open the file using the
popular pandas data processing module
and store each line in a data frame
object using its read function each
email message is labeled either spam or
ham we can split the data into a
training set to test our model and a
testing set to evaluate its prediction
capability for our spam classification
problem in the context of Bayes theorem
we
set a - the probability that the email
is spam and B as the content of the
email so if the probability that an
email is spam is greater than the
probability that it's not will classify
it as spam else we won't
since Bayes theorem results in the
divisor of probability of B in both
cases we can remove it from the equation
for our comparison calculating the
probability of a and the probability of
not a is simple they're just percentages
from our training set which are spam
versus not spam the harder part is
calculating the probability of B given a
and the probability of B given not a to
do this we'll use the bag of words model
that means we create a piece of text as
a bag of unique words with no attention
paid to their ordering for each word we
calculate the percentage of times it
shows up in spam and not spam emails and
to calculate another conditional
probability for an entire email we just
take the product of the former
conditional probability for every word
in the email this is done during
classification not training time with
these functions we can construct our
classifier function which gets called
for every email and uses our previously
defined functions to classify them
that's it now we can classify new emails
as spam or not Sam really easily hello
ready for coastal access smoothing
same thing I can't believe so what
happened to the word in the email we're
classifying isn't in our training set we
have to handle this edge case somehow
and a solution is to use something
called Laplace smoothing which we can
insert into our code as an alpha
variable this just means we add one to
every count so it's never zero because
if we didn't it would set the
probability for some word sake up to
zero then the probability of the whole
email becomes zero regardless of how
many other spammy phrases there are we
do haven't gotten good results why we
are loving telecasting you mean like
embed random variables
sir variational aquaculture that's great
paper all we do we might even be able to
make our model is unpredictable with you
my parents pretty high
are there improvements we can make to
our model sure we could have used a more
efficient technique instead of
bag-of-words
and used engrams instead of counting
individual words but hey that's more
than enough for this video
to summarize probability theory helps us
formally model the uncertainty of life
which is awesome Bayes theorem describes
the probability of an event based on
prior knowledge of conditions that might
be related to the event and naive Bayes
classifiers apply Bayesian theorem with
independence assumptions between
features the wizard of the week in
Hammad sheikh hamad notebook
demonstrates how to use principal
component analysis to visualize a high
dimensional data set and detect if a
person has diabetes or not I'm very
impressed with the quality of his
documentation definitely definitely
check it out and the runner-up is
Christian Beekman who used three
different auto-encoders to visualize
plant data very cool
this week's challenge is to write your
own naive Bayes classifier on a text
data set with better results than my
demo he tells them to read me github
links in the comments and winners
announced next week please subscribe for
more programming videos and for now I've
got to accept uncertainty so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>