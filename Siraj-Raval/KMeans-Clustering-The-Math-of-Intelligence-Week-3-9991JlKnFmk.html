<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>K-Means Clustering - The Math of Intelligence (Week 3) | Coder Coacher - Coaching Coders</title><meta content="K-Means Clustering - The Math of Intelligence (Week 3) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>K-Means Clustering - The Math of Intelligence (Week 3)</b></h2><h5 class="post__date">2017-07-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9991JlKnFmk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and we're going
to detect who the intruder is in our
security system so say we've got a
website and we're monitoring all of this
network data and there's one guy one bad
dude one bad hombre who's trying to
break into our security system we want
to find who this guy is and the way
we're going to do this is by
implementing a machine learning model
called k-means clustering and i've never
implemented this before so I'm really
excited and it falls right in line with
the rest of the stuff I've been talking
about in this series so let's get
started first I've got this little image
to show you what the algorithm looks
like it's a five-step process and this
gift kind of shows what that looks like
we start off with data that has no
labels it's just a cluster of unlabeled
data we don't really know anything about
the data other than the features we
don't know what classes the data belongs
to that's what we're trying to learn and
what the algorithm will do is it will
iteratively take this data that's
unlabeled data and it will create
clusters from this data and we don't
know what where these clusters are going
to be the algorithm is going to help
learn where those clusters should be
okay so and I'll talk about what these
steps are but let me define what this
term is k-means clustering this is one
of the most popular techniques in
machine learning you see it all the time
in capital contests in the machine
learning subreddit everywhere it's a
very popular algorithm and it's very
easy more or less I mean more than other
things that I've been talking about so
that's a good thing but let's talk about
what we've learned so far what we've
learned is that machine learning is all
about optimizing for an objective right
we are trying to optimize for an
objective that's that's the goal of
machine learning and we've learned about
first order and second order
optimization what's first order gradient
descent and it's variance right or we
are trying to if we were to graph the
error of a function versus its weight
values we want to find the minimum of
the function so that we can find the
ideal weight value so that our error is
minimized and to do that with gradient
descent we compute the first derivatives
right the partial derivatives with
respect to our weights using our air
but for second-order optimization we do
the same thing except we compute the
second derivative that is the derivative
of the derivative and there's pros and
cons to both and we talked about when
you would use one over the other in the
previous videos but what happens here's
this here's a big question what happens
if you don't have the label how are you
supposed to compute the error right it's
usually the predicted label - or sorry
the actual label - the predicted label
that's the error use it to compute the
partial derivatives with respect to each
weight value but if you don't have the
label how are you supposed to compute
the error and that's where unsupervised
learning comes into play
specifically k-means clustering so I've
got this diagram here to show the
differences here so there are two
outcomes that we could possibly want
right either a discrete outcome that is
some contained outcome like red or blue
or blue or black or white or up or down
or not guilty or guilty right there are
these containerized outcomes but our
binary outcomes not just binary because
it could be more than 0 &amp;amp; 1 it could be
multi class but outcomes that are
containerized into specific labels
whereas continuous outcomes are like
time series data where it could be a
value between 2 &amp;amp; 3 or between 2 &amp;amp; 2.5
or between 2 and point and 2.25 and I
could just go infinity in that direction
of that of that numerical interval right
so with supervised learning we learned
how to do predict a continuous outcome
using linear regression that was our
first video and then the next thing we
learned was how to predict a discrete
outcome and we use logistic regression
for that and that's when we have labels
but if we don't have labels then we use
clustering to predict a discrete outcome
that's what we're going to do we're
going to predict a discrete outcome and
by defining these classes for people
these discrete classes and then we're
going to find the anomaly that is the
intruder and so then for a continuous
outcome you'll want to perform
dimensionality reduction and that's next
week so we're not we're not
but we'll cut we talked about this one
and this one and now we're going to talk
about clustering so let's keep going
here so unsupervised versus supervised
learning what are the pros and cons well
for supervised learning it's more
accurate I mean you've got the labels of
course it's more accurate right it's
like it's like having training wheels on
your bike but you have to have a human
who labelled this data or it's just
labeled itself somehow but unsupervised
learning is more convenient because you
because most data is unlabeled right you
don't just have this neatly labeled data
like oh this is this or this no data is
messy the world is messy life is messy
so that's what we want ideally to run
our algorithms unsupervised but the
problem is that these algorithms are
usually less accurate right but it
requires minimum human effort because no
one's got to label these these data
points by hand ok so let's talk about
how this algorithm works mathematically
and then we'll get right into the code
ok and so what I'm going to do is I'm
going to glaze over most of the code and
I'll write a few of the most important
bits ok so how does this work so we've
got a set of points right so a set of
points X and so what I'm going to do is
I'm going to take this and I'm going to
make two copies of it so we can see this
gif
while I talk about the algorithm so let
me do this hold on just like that and
put that here and then put this here ok
so so the way this algorithm works is
we've got a set of points right a set of
points X and then and then we define a
value K and K is the number of clusters
that we want right K means algorithm
that's where K comes from so we've got a
set of points X all of our data points
and we've got K that's our input and so
we're going to say - let's just say -
for now and we'll talk about how to
realize what the best K value is let's
say - right now and so then once we have
that we're going to place a set of
centroids at random locations how many
centroids K centroids so if we chose to
for K then these centroids are just data
points
that are just randomly plotted on the
graph okay these are they're called
centroids because eventually they're
going to be the center of each cluster
that we learn to be centers so these
centroid points are there k of them and
we just plot them randomly okay so we
defined K we have our set of data points
and then our set of centroids K of them
and we just plot them randomly ok now
what now here's the steps we do and we
repeat them until convergence which is
what we've predefined beforehand with
threshold value so what we do is we say
for each point in the data set so let's
say we have 40 data points so for each
point let's say for one of them we're
going to find the nearest centroid how
do we do that well we compute the
distance between that data points and
and each of the centroid points so
there's two in our case there's going to
compute the distance the Euclidean
distance between that points and both of
the centroids and is going to find the
one that's closest to it and that's
where this Arg min function comes in
what is the minimum value in this set of
values so we're going to find the
shortest distance and that's going to be
our cluster so we're going to assign
that data point to the to the cluster J
where J is for the centroid that is
closest to that data point okay so then
what happens is we've got a set of
clusters now and we do this for every
single data point so every single data
point will belong to a cluster and that
cluster will be defined as a centroid
point that is closest to that data point
okay so that's the initial cluster
that's going to be defined then for each
of those clusters J we're going to take
all of those data points in that cluster
we're going to add them all up
and then divide by the number of them
and what is this called it's called the
mean right or the average so now you're
getting to see where this name comes
from right k-means right it all makes
sense um I got a little okay so k-means
is the name of the algorithm and so we
find the mean point or the mean value
for all the points in the data set and
that mean is going to be become our next
centroid point okay so we've defined
centroids we've grouped our data point
into each of these centroids and then we
will then find the mean of all those
values and that will
those values will be our new centroids
in our case there will be two rights OB
there'll be two new centroids that we
then plot and then we just keep
repeating the process so we go back to
for each point X and for those new
centroids find the distance for all the
closest data points and it's going to be
a new cluster and so that's what you're
seeing here it's going to be a new
cluster and we just keep repeating that
process until when until none of the
cluster assignments change and then
we're good so right so that's kind of
how that works and so uh right so we can
also terminate the program when it
reaches an iteration budget so we'll say
after you know X number of iterations
just stop running right so then one a
one great question that I hope you're
asking is how do we know what value for
K we should use well it's very simple if
we know what classes we want to classify
or how many classes we'll just say that
that's the value for K so if we know
that we want to classify people as
either from a certain region or from a
certain place then we'll just say like
of these three places you know Spain
Mexico and I don't know Argentina random
the countries was I guess those are on
my mind then we know that K should be
three because we have three countries
that we're targeting but if we don't
know how many classes we want these are
just unknown unknowns then we and we'll
have to decide what the best K value is
and that can be a guess and check method
but there's actually a smarter way to do
that and it's called the elbow method so
the elbow method is a very popular
method what you have to do is if to rub
your elbow no I'm just kidding so what
happens is you've got this graph here
okay and it looks like an elbow right
it's got this elbow point right here so
what you do and I've got this part in
JavaScript to show you what this
algorithm looks like I know JavaScript
right so here's what it looks like we
perform k-means once so we so we define
a set of K values between 1 and 10 okay
so that's a good starting point K could
be either 1 cluster or it could be 10
clusters
let's try k-means for all ten of those
and so what we do is we perform teh mean
for all those K values and then so
that's what this is so for as many k
values as we have between 0 and 10 let's
perform k-means to find all those
clusters and then for each of the
clusters let's find the mean value and
what is the mean value it is the error
value so what is the distance between
the centroid and all of its points and
that's going to be the mean or the error
and what we want to do is we want to
compute the sum of the squared error
values and so that's what this line is
right here the sum of the squared errors
and so we say Matthau which means square
the data point for each data point minus
the mean so for every data point in our
data set we're going to take a data
point minus the mean value and square it
and so what that's going to do is if we
were to graph that for all of those
number of cluster values for all those K
values will see that that sum of the
squared errors for each of those
iterations of k-means makes this elbow
like graph and what we want to do is we
want to pick the K value that is right
at the elbow right at that tipping point
right here so it would be 6 in the case
of this graph and that is our optimal K
value okay because after that there's
very diminishing returns as you can see
we want to find the minimal error value
and we found that for this K value of 6
the error is it's not as smallest but at
the point where it's everything after
that is just diminishing returns and we
could say 10 or 12 or 14 but then for
computational efficiency sake we could
just say 6 so we don't have to run that
many iterations so we'll just say 6 okay
so that's the elbow method all right so
three more points and then were going to
get started with the code so how is the
distance computed between the centroids
in the data points the Euclidean
distance right we've talked about the
Euclidean distance for linear regression
right Euclidean distance it's a very
simple distance formula you just take
each of the data points and then you say
X 1 minus X 2 plus squared plus y 1
minus y 2 squared and if you have V
values V 1 minus V 2 squared and the
square root of all of us and that's the
Euclidean distance and what that does is
it
take two data points and it gives us a
scalar value and that is the distance
between all those data points so you can
apply the Euclidean distance to a
multi-dimensional data set where you
have you know any number of dimensions
you just subtract the values for each of
those dimensions for each of those data
points the square and then you find a
square root anyway I'll show you the
equation when we get to it but yeah the
Euclidean distance and to answer any
questions about why the Euclidean
distance as opposed to other distance
metrics the answer is because K means
technically minimizes the within cluster
variance and I know we haven't talked
about this term yet and we will but I'm
just trying to be very detailed here and
if you look at the definition of
variance which I'll define more in
detail later it is a it is identical to
the sum of the squared Euclidean
distances from the center so in
Euclidean space and because it is
identical to the sum of Euclidean
distances we use the Euclidean distance
as our distance metric as opposed to
something else with the Manhattan
distance okay so last for you or two
more actually when should you use this
if your data is numeric and that means
if your features are numeric right you
have numbers for your features if you
have a categorical feature like of the
shoe color or true or false a boolean
value you can't really map that in
Euclidean space right these are these
are categorical features so but if your
data is numeric k-means is your is your
is your algorithm it's also the simplest
algorithm you'll see it's actually a
very simple algorithm we talked about
the pseudocode it's a relatively simple
algorithm and the advantage it has over
other techniques is that it is fast
that's the real key value it's simple in
its fast you know quick and dirty
clustering algorithm it's great for that
okay
and it really shines when you have
multivariate data so that is more than
one dimension okay so lastly two other
examples I've got here one for fraud
detection and then one for MST without
labels I know what that is possible MISD
without labels yes it's possible
anything is possible
so anything is really possible here so
for credit
fraud detection and for finding these
labels for these for these Mis t images
work let's just say we don't know the
label usually we know the labels well
let's just say we don't know the labels
if we don't know the labels and it's
going to cluster the images into what
their what their respective clusters
should be and those are clusters for one
then twos and threes well in terms of
the image the images okay so check those
other two examples out and now let's get
into the code okay so I move in here I'm
moving okay so the first thing we want
to do is we're going to import numpy for
matrix mass map plot live to map plot
live to plot out our graph and then the
animation module of map plot live
because we're going to graph we're going
to animate some graphs in a second okay
so then let's take a look at our data
set okay so where is our data set here
our data set is is it what our data set
looks like okay so we've got two
dimensions we've got two features in our
data set and the feature on the left is
how many packets are sent per second and
the feature on the right is what's the
size of a packet okay so what we're
trying to do is we're trying to detect
the anomaly and that is a DDoS or if you
know what DDoS thing is it's basically
flooding a server with packet requests
until the server goes down so try and
see how many packets are sent per second
from this user and then what is the size
of that packet okay and that that's that
those are our data points and we have a
few of these data points okay so that's
it just to feature through our data and
we'll talk about like I said
dimensionality reduction later on if we
have a million features how do we reduce
it to two or three so that we can
visualize it so that's our data set that
we want to load okay so then here's what
we got here this is the Euclidean
distance now this is the formula that I
was talking about so given two data
points P and Q take each of the values
so if it has you know Q it could be X it
could be Y Z to each of the values
subtract them to get the difference
square it and then add them all together
and then square root that whole equation
and so that's what the Sigma notation
denotes it's the sum of a set of values
we're starting at I equals 1 for n
values find the difference difference
between all the data all the
feature values in a data point squared
and then find the square root of all of
that for the sum of the sum of the
squared errors okay technically the sum
of the squared errors yeah and that's
the Euclidean distance so that's the
Euclidean distance and so now let's look
at the actual algorithm itself so what
I've done here is I've defined its
hyper its hyper parameters and then I'm
going to code out the algorithm itself
okay so for k-means we've got a k value
that is a number of clusters and since
we we're just going to say to okay we
have two clusters that we want and we
have an epsilon value and the epsilon
value is it's a zero it's a threshold
it's the minimum error to be used in the
stop condition okay when we want to stop
training okay when our error is zero and
then we have our distance or what what
type of distance we want to compute the
Euclidean distance okay so let's keep
going let me make it a little bigger so
what we're going to first do is store
the path centroids and this history of
centroids lists now this is not really a
part of the algorithm this is just for
us so we can then graph how the
centroids move over time later on so we
can visualize it okay so then we're
going to say okay the distance metric is
going to be Euclidean so we define it
right here as disk method that variable
and then we set the data set so we load
up that data set from txt we've got
those two dimensions right the amount of
packets that are sent and then the size
of each packet okay so we've got our
data set we've defined a distance metric
and now we're going to say okay let's
get the number of instances and the
number of features from our data set by
taking this shape attribute of our data
set so our rows and our columns or rows
are going to be the number of data
points we have
and there are columns are going to be
the number of features we have so we
want to store those two values then
we're going to define K centroids right
so like I said we randomly plot these
centroids on the graph and we're they're
going to be how many centroids they're
going to be K centroid special K through
serial okay so
okay so we're going to say our data set
is going to be we're gonna say okay so
from our data set how many clusters do
we want to find right chosen randomly so
we're going to say size K so we're going
to find a random number between zero and
a number of instances that we have that
that is the number of data points minus
one of size K okay and so we're going to
store that the the centroid values in
this prototypes variable right so we've
got our centroids and then we want to
set these to our list of passed
centroids as well so I'm going to take
those centroids that we just defined
randomly and set them to our history
centroids list so then we can just keep
a copy of it over time and we'll keep
adding our centroids that are calculated
to this history centroids list so we can
graph it later you know for our own
visualization okay and then we have our
prototypes old a list which is going to
be initialized as a bunch of zeros it's
empty it's an empty vector or tensor and
so that's going to keep track of
centroids every iteration right so we've
got our prototypes which is our current
and our prototypes old which is our the
set of centroids that we had before and
those two are what we're going to use
for the actual algorithm the histories
or the history centroids it's just for
us to see how it changes over time okay
and then we have one more list and that
is the belongs to list to store the
clusters over time the clusters
themselves all the data points contain
in a cluster okay and then we have our
distance method which is going to take
the current prototypes and then the
prototypes old and the distance between
them and it's going to store that in the
norm variable okay and then the number
of iterations which will start off as
zero okay so now let's go ahead and
write out this algorithm shall we so
we're going to say okay so while the
norm the distance is greater than
epsilon where epsilon is zero in our
case we're going to say well the number
of iterations we're going to add one
because this is our first iteration we
have begun training or model so we'll
say ok well the iteration is going to be
one and so let's compute what the norm
is so given our prototypes where we are
currently
the distance between those prototypes
and the prototypes that we had before
and that's going to be our norm then
we're going to say okay so for each
instance in the data set so let's go
through every single instance so we'll
say okay well what is the index of an
infant and what is the actual value of
that instance those are the two
variables that we're going to use to
enumerate or iterate over the data sets
we'll say let's define a distance vector
of size K so now it's time for us to
define what this distance vector looks
like so we'll say okay so the distance
vector is going to first of all be
initialized as a set of zero values of
size K okay so it's going to be empty at
first and we're going to add value to it
over time okay so let's say okay so for
each centroid value we have our random
centroid values right we already
computed the values randomly at the
beginning right so we're going to say
okay so for each for each centroid value
let me just make this a comment for each
centroid value so for the for the
prototype and there's a stored in our
prototype so we'll say for each index
and then the actual value for the
prototype let's go ahead and do this for
all those prototypes so for each
instance in our data set and then for
each centroid so we have two nested
for-loops right and then for each
centroid and numerate prototypes we're
going to say we want to compute the
distance between what we have to do
which compute the distance between each
data point and its closest and each
centroid so for every data point we're
going to compute the distance between
its and every other centrally and we
want to find the minimum distance
centroid that closest centroid to it so
we can put it into that specific cluster
so they compute the distance between X
and what X is a data point and centroid
so we'll say distance vector is going to
be index prototype so for the centroid
value so here's we're going to do we're
going to compute the distance using
distance method between the prototype
which is the centroid and
since value so for all those X values
for all those data points we're going to
compute the distance and store it in a
dis inspector list and then it's going
to be indexed by the indexed prototype
okay and then we're going to do that for
all of them so that's why we have a
nested for loop so that's going to store
all of those distances in that distance
vector method and then what we want to
do is we want to find the smallest
distance and assign that distance to a
cluster so we're going to say okay what
is the what what cluster does each this
does each a data point belong to and
we'll say well we'll define it and it
belongs to array and define it by its
index instance and so now we'll do their
argument step or we'll find a minimum
value from between all those distance
vectors that we calculated okay between
all of those distance values and we use
the Euclidean distance that we defined
previously in its own function and so
it's going to say okay so that that
we're going to store them all in there
and then we're going to say okay so then
we have a list of temporary prototypes
oh those are temporary centroids because
we're going to update our centroids in a
second and we want to save where we are
right now so we could store it in our
history list so say we'll create a
little tensor right here and it's going
to be of size K by the number of
features right because that's the number
of clusters we have and we're going to
say for each cluster hold on for each
cluster and our K of them let's say okay
so for for each cluster and there are K
of them let's go and say let's get all
the points assigned to a cluster look at
all those points assigned to a cluster
right so we're going to say okay I'm
just going to paste it in for it because
it's a little faster we only have a
little bit left okay so for each cluster
and there are K of them we're going to
get all the points assigned to a cluster
okay and that's what this what this line
does right here we're going to get all
those points assigned to a cluster and
store them in instances close okay so
those instances closed
all the data points within a specific
cluster and we're doing this for each of
the cluster server right and for each
cluster K of them we're going to find a
mean of those data points and so now you
get the k-means part for K clusters
compute the mean of the data points in
that cluster using NPN mean so that is
the average where we add them all up and
then divide by the number of them for
all of those instances and store that
mean in prototype and then we're going
to that going to our new centroid so at
our new centroid to our temporary
prototypes list and we did that so we
can then take that temporary prototype
and it sign it to our main prototype
list ok so then with the temporary
prototype variable acts as a buffer as
we're computing new centroids to store
them and then we can add it to our main
prototypes variable and then we have our
history centroids list which will also
append to just for us to calculate this
for us to visualize how this enjoyed
move over time later on and you'll see
exactly what I mean by that at the end
of this we return our calculated
centroids and that is at the end of our
algorithm when we have reached
convergence and there are history of
centroids all those centroids that we've
calculated over time and then belongs to
which is the all the clusters or that's
where we cluster all those data points
and then the index is going to be the
cluster that it belongs to that K
cluster and so that's how we define all
those data points in a single cluster ok
so that's it for the algorithm and then
we're going to graph what this looks
like
so in are plotting algorithm we're going
to say ok where we're going to have two
colors red and green for our centroids
we'll define them here well split our
graph up by its AXI axis and it's actual
plot and then for each point in our data
set and we've got several points in our
data set we're going to say ok we want
to graph all these points by their
cluster color which we've which we're
going to define so all the all the data
points in that one cluster are going to
be one color and all the data points in
another cluster are going to be another
color okay so then we'll get all those
points by looking in D belongs to list
ok by its index right so for the index
for each point in our data set get the
instances those are all the data points
that
to a specific cluster and then assign
each data point in that cluster a color
and plot its own respective color okay
and so that's what we do here and then
we're going to log the history of
centroids remember how I said how we
want to see what the history of
centroids that are calculated are so we
can see how it changes that's what we're
doing here so in the history points list
we'll say for each centroid ever
calculated print them all out and then
plot them in their own graph okay so
that's what's happening in the method
and then we're going to actually execute
it right we've written methods for our
k-means algorithm we've written a method
to plot out our results and now we can
execute these methods so we're going to
load our data set train the model on
that data where k equals two so we want
two clusters and then it's going to give
us back our computed centroids the
history of all the centroids that were
computed over time and in our list of
data points defined by the cluster that
each belongs to and so we could take
those three values and then use them as
parameters for our plotting function and
that's going to plot our result and so
when we run that we're going to get
these two clusters right so it's if we
didn't run our K means algorithm and we
just plotted our data it would look just
like this except there would be no color
because we all know what the clusters
are but what happened was the algorithm
learned that that that the right classes
were red and green for B for these for
these respective clusters and the blue
points are the centrist and center
points and so those are the centroids
okay and so over time what happens is
look so here's here's what I mean by
over time over time what happens is it
learns the ideal center points for the
center for the centroids okay it starts
it learns these ideal center points over
time the first if they're randomly blue
and then it gets better and better
iteratively it finds better and better
center points find the most optimal
center point for the cluster that we're
going to learn and then it plots it okay
so that's it please subscribe for more
programming videos and for now I've got
to find my sense
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>