<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Predict Stock Prices Easily - Intro to Deep Learning #7 | Coder Coacher - Coaching Coders</title><meta content="How to Predict Stock Prices Easily - Intro to Deep Learning #7 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Predict Stock Prices Easily - Intro to Deep Learning #7</b></h2><h5 class="post__date">2017-02-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ftMq5ps503w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">can we actually predict stock prices
with machine learning investors make
educated guesses by analyzing data
they'll read the news study the company
history industry trends there are lots
of data points that go into making a
prediction the prevailing theories is
that stock prices are totally random and
unpredictable a blindfolded monkey
throwing darts at a newspapers financial
pages could select a portfolio that
would do just as well as one carefully
selected by experts but that raises the
question
sliding top firms like Morgan Stanley
and Citigroup hire quantitative analysts
to build predictive models we have this
idea of a trading floor being filled
with adrenaline infuse men with loose
ties running around yelling something
into a phone but these days they're more
likely to see rows of machine learning
experts quietly sitting in front of
computer screens in fact about 70% of
all orders on Wall Street are now placed
by software we're now living in the age
of the algorithm hello world it's Suraj
and today we're going to build a deep
learning model to predict stock prices
records of prices for traded commodities
go back thousands of years
merchants along popular silk routes
would keep records of traded goods to
try and predict price trends so that
they could benefit from them and finance
the fields of quantitive analyst is
about 25 years old and even now it's
still not fully accepted understood or
widely used just like Google Plus it's
the study of how certain variables
correlate with stock price behavior one
of the first attempts at this was made
in the 70s by two British statisticians
inbox and Jenkins using mainframe
computers the only historical data they
had access to were prices and volume
they call their model ARIMA and at the
time it was slow and expensive to run
but by the 80s things started to get
interesting spreadsheets were invented
so that firms could model company's
financial performance and automated data
correction became a reality and with
improvements in computing power models
could analyze data much faster it was a
renaissance on Wall Street people were
excited about the possibility they
started showing up at seminars and
discussing their techniques you should
see what's going on of the bigger firms
I mean I know all the information but
all this quickly died down once people
realize that what works is actually a
very valuable secret
since then the most successful quants
have gone underground in the past few
years we've seen lots of academic papers
published using neural nets to predict
stock prices with varying degrees of
success but until recently the ability
to build these models has been
restricted to academics who spend their
days writing very complex code now with
libraries like tensor flow anyone can
build powerful predictive models trained
on max of datasets so let's build our
own model using care off with a tensor
flow backend for our training data we'll
be using the daily closing price of the
SMP 500 from January 2000 to August 2016
this is a series of data points indexed
in time order or a time series our goal
will be to predict the closing price for
any given date after training we can
load our data using a custom load data
function it essentially just reads our
CSV file into an array of values and
normalizes them rather than being those
values directly into our model
normalizing them improves convergence
we'll use this equation to normalize
each value to reflect percentage changes
from the starting point so we'll divide
each price by the initial price and
subtract one when our model later makes
prediction will be normalize the data
using this formula to get a real-world
number out of it to build our model
we'll first initialize it as sequential
since it will be a linear stack of
layers then we'll add our first layer
which is an LS TM layer so what is it
let's back up for a bit recognizes beep
single erics with me you don't have to
think what you did I already know I
found out from today it's easy to recall
the words forward but could we sing them
backwards
No the reason for this is because we
learned these words in a sequence it's
conditional memory we can access the
word if we access the word before it
memory matters when we have sequences
our thoughts have persistence but
feed-forward neural Nets don't they
accept a fixed size vector as input like
an image so we couldn't use it to say
predict the next frame in a movie
because that would require a sequence of
image factors as inputs not just one
since the probability of a certain event
happening would depend on what happened
every frame before it we need a way to
allow information to persist and that's
why we use a recurrent neural net
recurrent Nets can accept sequences of
vectors as inputs so recall that for
feed-forward neural nets the hidden
layers weights are based only on the
input data but in a recurrent net the
hidden layer is a combination of the
input data at the current time step and
the hidden layer at a previous time step
the hidden layer is constantly changing
as it gets more inputs and the only way
to reach these hidden states is with the
correct sequence of inputs this is how
memory is incorporated in and we can
model this process mathematically so
this hidden state at a given time step
is a function of the input at that same
time step modified by a weight matrix
like the ones used in feed-forward nets
added to the hidden state of the
previous time step multiplied by its own
hidden state two hidden state matrix
otherwise known as a transition matrix
and because this feedback loop is
occurring at every time step in the
series each hidden state has traces of
not only the previous hidden state but
also of all those that preceded it
that's why we call it recurrent in a way
we can think of it as copies of the same
network each passing a message to the
next so that's the great thing about
recurrent Nets they're able to connect
previous data with the present task but
we still have a problem take a look at
this paragraph it starts off with I hope
senpai will notice me and end with she
is my friend he is my senpai let's say
we wanted to train a model to predict
this last word given all the other work
we need the context from the very
beginning of the sequence to know that
this word is probably senpai not
something like buddy or mates in a
regular recurrent Nets memories become
more subtle as they
to the past since the error signal from
later time steps doesn't make it far
enough back in time to influence a
network at earlier time steps during
back propagation Joshua Ben geo called
it the vanishing gradient problem in one
of his most frequently cited papers
piled learning long-term dependencies
with gradient descent is difficult love
the bluntness a popular solution to this
is a modification to recurrent Nets
called long short-term memory
normally neurons are units that apply an
activation function like a sigmoid to a
linear combination of their inputs in an
L a TM recurrent net we instead replace
these neurons with what are called
memory cells each cell has an input gate
an output gate and an internal state
that feeds into itself across time steps
with a constant weight of one this
eliminates the vanishing gradient
problem since any gradient that flows
into this self recurrent unit during
backprop is preserved indefinitely since
errors multiplies I want and still have
the same value each gate is an
activation function like sigmoid during
the forward path the input gate learns
when to let activation pass into the
cell and the output gate learns when to
let activation pass out of it during the
backward pass the output gain learns
when to let error flow into the cell and
the implicate learns when to let it flow
out of the cell through the rest of the
network
so despite everything else in a
recurrent net staying the same doing
this more powerful update equation for
our hidden state results in our network
being able to remember long term
dependencies so for our LS TM layer
we'll set our input dimension to 1 and
say we want 50 units in this layer
setting return sequences to true means
this layers output is always fed into
the next layer all its activations can
be seen as the sequence of predictions
this first layer has made from the input
sequence we'll add 20% dropout to this
layer then initialize our second layer
as another LS p.m. with 100 units and
set return the sequence to false on it
since its output is only fed to the next
layer at the end of the sequence it
doesn't have put a prediction for the
sequence instead a prediction vector for
the whole input sequence we'll use the
linear dense layer to aggregate the data
from this prediction vector into one
single value then we can compile our
model using a popular loss function
called mean squared error and use
gradient descent as our optimizer
labeled rmsprop
we'll train our model with the fit
function then we can test it to see what
it predicts for the next 50 steps at
several points in our graph and
visualize it using that plot line it
seems that for a lot of the price
movement especially the big ones there
is quite the correlation between our
models prediction and the actual data so
time to make some money and play some
Thea but will our model be able to
correctly predict the closing price 100%
of the time he'll to the no it's an
analytical tool to help us make educated
guesses about the direction of the
market that is slightly better than
random so to break it down recurrent
Nets can model sequential data since at
each time step the hidden state is
affected by the input and the previous
hidden state a solution to the vanishing
gradient problem for recurrent Nets is
to use long short term memory cells to
remember long term dependencies and we
can use lsdm networks to make
predictions for time series data easily
using chaos and tensor flow the winner
of the coding challenge in the last
video is Vishal Bachchu
Vishal used transfer learning to create
a classifier for cats and dogs he chose
a layer from a pre trained tensorflow
model and build his own custom
convolutional net on top of it to make
training much faster wizard of the week
and the runner-up is GNC I loved how he
added a command-line interface for users
to input their images the coding
challenge for this video is to use three
different inputs instead of just one to
train your lsdm network to predict the
price of Google stock details are in the
readme poster gambling in the comments
and I'll announce the winner in a week
please subscribe for more videos like
this and for now I gotta count my stack
of layers so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>