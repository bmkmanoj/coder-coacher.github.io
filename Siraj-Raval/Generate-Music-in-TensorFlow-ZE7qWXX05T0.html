<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generate Music in TensorFlow | Coder Coacher - Coaching Coders</title><meta content="Generate Music in TensorFlow - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generate Music in TensorFlow</b></h2><h5 class="post__date">2016-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZE7qWXX05T0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Hey are you okay I want to make music
but I don't know how hello world it's
Suraj in this episode we're going to
talk about the bleeding edge and
machine-generated music then write our
own music generation script intent or
flow and just under 90 lines of Python
I'm a big fan of Hans Zimmer don't have
to McConaughey no but I don't have the
time to put in 10,000 hours to be like
Hans but just imagine a program where I
could say I want a song that conveys the
feelings of hope and wonder while
sailing the seven seas but also the gas
just feeling of eating too many beans
the program would convert that speech to
text and for each word it would find the
Associated vector it would then compare
those vectors to vectors created by a
model train on a labeled music data set
for a range of human emotions the output
of the machine would then be a set of
chords that I thought best represented
those emotions it would lower the
barrier to entry for me to compose music
machine learning is a layer of
intelligence that is meant to extend our
own I'm so really bad at folding shirts
a laundry folding robot would be so dope
deep mine release the paper a few days
ago called wavenet which is now the
state of the art in both music
generation and text-to-speech
traditionally audio generation models
are concatenative that means in order to
generate speech from some text sample it
utilizes a huge database of speech
fragments by picking a few and combining
them to create a full sentence same for
music this works but it's hard to
incorporate things like emotion and
natural sounds when the output is
created by putting a bunch of static
fragments together ideally we can have
audio generation that is parametric
where all the info requires to generate
audio is stored in the parameters of the
model that we give it that's what
wavenet is capable of instead of
generating an audio signal bypassing its
output through signal processing
algorithms it directly models the raw
waveform of the audio signal researchers
don't really do that but wavenet did the
model they used was a convolutional
neural network where each layer had
dilation factors that lets its
interconnectedness grow exponentially
the deeper the data flowed through the
model and each generated sample at each
step was fed back into the network to
generate the next step let's take a look
at the computation graph for the model
the input data a single note starts as a
raw audio wave we first format the way
so that it's better suited for
processing then we Inc
to produce a tenser with a number of
samples and a number of channels we feed
that into the first layer of the
convolutional Network which reduces the
number of channels for easier processing
than through a stack of layers with
dilated convolution we combine the
outputs of all the layers and increase
the dimensionality to the original
number of channels feed it all total
loss function which measures how well
our training is going and finally the
output is fed back into the network to
generate the wave at the next time step
this process keeps repeating to generate
more and more audio this neural net was
huge it took 90 minutes on their GPU
cluster to generate a single second of
audio instead we're going to build a
more simple music generation script in
tensorflow we want to first import numpy
as a scientific computing library in
python then panda is our data analytics
library then of course tensorflow our
machine learning library TQ DM will help
us show a progress bar during training
and finally mid-on manipulation which is
our helper library let's start off with
the first step our hyper parameters or
tuning ops for our model we're going to
use a type of neural network called a
restricted Boltzmann machine as our
generative model an RBM is a two layer
neural net the first layer is visible
and the next layer is hidden nodes
within layers are connected to note in
the next layer but no two nodes of the
same layer are linked that's the
restriction part each node decides
whether to transmit any data it receives
to the next layer or not by making a
random decision we want to first create
a range of notes that our model
generates will write variables for our
lowest note and our highest note as well
as a note range which is the difference
between the two then we'll write
variables for the number of times the
size of the visible layers the number of
hidden layers number of training epochs
batch size and finally our learning rate
which is a tensor flow constant since it
always stays the same
next we can write out variables we'll
start out by creating a placeholder
which will serve as our gateway for
inputting data into our model then a
variable that will store the weight
matrix or the connections between our
two layers we also need two variables
for biases one for the hidden layer and
one for the visible layer we don't want
to create a sample from our input in X
these aren't Gibbs sample helper method
Gibbs sampling is an algorithm that
creates a sample from a multivariate
probability distribution it constructs a
statistical model where each state
depends on the previous state then uses
randomness to obtain a sample across the
distribution it creates will get a
sample of the hidden nodes as well
starting from our original input and a
sample of the hidden node starting from
our generated sample then we'll update
our weight values based on the
differences between the samples that we
drew and the original values we will run
these three
big steps for our weights hidden biases
and visible biases when we later run our
session and we can store all of these in
our update variable all right it's time
to start a session so we can run our
graph well initialize our variables
first we're going to reshape each song
so they are vectorized in a good
training format then we're going to
train our RBM on the examples one at a
time once the model is fully trained we
will make some music we run our Gibbs
chain where the visible nodes are
initialized a zero to generate a sample
and reshape the vector to be the proper
format for playback finally we'll print
out the generated chords dope the state
of the art and music generation uses ACN
n to generate raw waveforms
parametrically instead of concatenate
that algorithm requires lots of compute
right now so we can use an RBM to
generate audio samples based on training
data easily and Gibbs sampling helps us
create audio samples during training by
randomly obtaining them from a generated
probability distribution based on the
input data and so the winner of the last
coding challenge is Rohan Verma he
created an image classifier for
astronomers to classify galaxies as
either elliptical or spiral badass of
the week and the runner-up is Chi Cheng
Liang for creating a bird classifier for
biologists I bow to both of you
the challenge for this video is to use
the RBM script to generate an audio
sample with a happy upbeat sound post
your code in the comments I'll judge
them and announce the winner in my video
one week from now for now I've got to
get more silver hair dye so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>