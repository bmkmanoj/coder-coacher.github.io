<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Which Activation Function Should I Use? | Coder Coacher - Coaching Coders</title><meta content="Which Activation Function Should I Use? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Which Activation Function Should I Use?</b></h2><h5 class="post__date">2017-05-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-7scQpJT7uo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and activation
functions which one should you use for
your neural network there's so many out
there
artificial neural nets are roughly based
on our brains neural net not in the way
that they're made up of a biochemical
soup but in the way that multiple nodes
or neurons are interconnected and
signals can pass through these notes
it's this hierarchical structure that
gives us such amazing results our
universe itself can be considered to
have a hierarchical structure elementary
particles form atoms which form
molecules which form cells than
organisms planets solar systems and
entire galaxies to make sense of this
hierarchical complexity evolution has
settled on a specific kind of structure
for our brains that can represent all
these layers of abstraction in an
ordered way and so when we model this
rule of hierarchical layers on computers
we get very similar results even though
the substrate is different it's silicon
so it's incredibly exciting to study the
mathematical relations of deep neural
networks and ways of improving them
because by doing so we're not just
coming closer to better results for
specific tasks we're coming closer to
discovering fundamental laws embedded in
our universe the basic idea behind how a
neural network learns is that we first
have some input data that we've got a
vectorize then we feed it into the
network which means we basically perform
a series of matrix operations on this
input data layer by layer in a simple
case for each layer we just multiply the
input by the weights add a bias apply an
activation function to the result and
pass the output on to the next layer to
do the same thing and we keep repeating
that process until we reach the last
layer the final output value is our
prediction we find the difference
between it and the expected output which
is a label then use that error value to
compute the partial derivative with
respect to the weights in each layer
going backwards recursively we then
update the weights with these values and
repeat the process until the error is as
small as possible and that's leap
tarnished doob learning who the so you
might be asking why do we apply these
activation functions to our data
couldn't we just multi
the input by the weight values add a
bias and propagate that result forward
well they do something really important
they introduce nonlinear properties to
our network so we can call them
nonlinearities why is this a good thing
let's take a step back and linear
function is a polynomial of just one
degree like y equals 2x or y equals x so
if we were to map these functions on a
graph they would always form a straight
line if we added more dimensions they
would form a plane or a hyper plane but
their shape would always be perfectly
straight with no curves of any kind
that's what we call them linear but
every other equation is nonlinear
polynomials of higher degrees like y
equals 2x squared trig functions like
sine or cosine nonlinear functions when
plotted produce a line that always has
some kind of curvature linear equations
are easy to solve but they are limited
in their complexity we want to be able
to represent any kind of function with
our neural network and neural networks
are considered Universal function
approximator x' that means that they can
compute any function at all almost any
process you can imagine can be thought
of as a function computation trying to
name the song that you're hearing
translating Spanish to English punching
an evil clone of yourself
so we need a way to compute not just
linear functions but non linear ones as
well if we didn't use the nonlinear
activation function then no matter how
many layers our neural network has it
would still behave just like a single
layer Network because summing these
layers would give us just another linear
function this is not strong enough to
model many kinds of data though but by
using a nonlinear activation the mapping
of the input to the output is nonlinear
and we want it to be differentiable that
means we're able to calculate the
derivative of it an example of a
differentiable function is x squared
since we can differentiate it to 2x
which is its derivative we need it to be
this way so we can perform the
backpropagation
optimization strategy where we find a
nonlinear error gradient to learn
complex behavior the whole idea behind
activation functions is to roughly model
the way neurons communicate in the brain
with each other each one is activated
through its action potential if it
reaches a certain threshold we know to
activate a neuron or not the activation
function simulates the spike train of
the brain's action potential input times
wait a device activate input times wait
add v activate so we can think of a lot
of possible activation functions but how
do we know which one to use this choice
is dependent on a couple factors not
including if it just sounds cool let's
talk about the three most popular ones
sigmoid tan h and relevant sigmoid has
the mathematical form of f of x equals 1
over 1 plus e to the negative x it takes
some number and squashes it into a range
between 0 &amp;amp; 1 it was one of the first to
be used because it could be interpreted
as the firing rate of a neuron where 0
means no firing and one means a fully
saturated firing it's pretty easy to
understand but it has two problems that
have made it fall out of popularity
recently the first is that it causes our
gradients to vanish when a neuron
activation saturates close to either 0
or 1
the gradient at these regions is very
close to zero during back propagation
this local gradient will be multiplied
by the gradient of this gate output for
the whole objective so if the local
gradient is really small it will make
the gradient slowly vanish and close to
no signal will flow through the neuron
to its weight and recursively to its
data the second problem is that its
output isn't zero centered it starts
from zero and ends at 1 that means the
value after the function will be
positive and that makes the gradient of
the weights become either all positive
or all negative
this makes the gradient updates go too
far in different directions which makes
optimization harder I can control the
gradient so how do we improve on it well
there's another activation function
called the hyperbolic tangent function
or 10h it squashes the real number into
a range between negative 1 &amp;amp; 1 instead
of 0 &amp;amp; 1 so its output is 0 centered
which makes optimization easier so in
practice it's always preferred to the
sigmoid but just like the sigmoid it
also suffers from the vanishing gradient
problem
enter relu or the rectified linear unit
this activation function has become
really popular in the last few years
it's just max 0x which means that the
value is zero when X is less than zero
and linear with a slope of 1 when X is
greater than zero it was noted that it
had a 6x improvement in convergence over
can H in the landmark imagenet
classification paper by Khrushchev ski a
lot of times in computer science we find
that the simplest most elegant solution
is the best and this applies to relu as
well
it doesn't involve expensive operations
like 10 H or sigmoid so it learns faster
and it avoids the vanishing gradient
problem almost all deep networks use
relevant days but it's only used for the
hidden layers the output layer should
use a soft max function for
classification since it gives
probabilities for different classes and
a linear function for regression since
the signal goes through unchanged one
problem that reloj has sometimes though
is that some units can be fragile during
training and die meaning a big gradient
flowing through a revenue on could cause
a weight update that makes it never
activate on any data point again
so then gradients flowing through it
will always be zero from that point on
so a variant was introduced called a
leaky relu to fix this problem instead
of the function being zero when X is
less than zero instead has a small
negative slope there's also another
popular variant called Max out which is
a generalized form of both relu and
leaky relu but it doubles the number of
parameters for each neuron so there's a
trade-off the original question was what
type of activation function should you
use in a neural network and the answer
is real Lulu relu relu but if a lot of
your neurons dive and try a variant like
a leaky relu or max l sigmoid just
shouldn't be used anymore nor should 10
H and although relish should be applied
to the hidden layers the output layer
should use a soft max for classification
or a linear function for regression
there are other activation functions out
there and there's still a lot of room
for improvement in this area they are a
crucial part of neural networks so any
new discovery here will have huge
impacts in the field moving forward if
you liked this video hit the subscribe
button
for more like it check out this related
video and for now I've got to take a
leak irulu so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>