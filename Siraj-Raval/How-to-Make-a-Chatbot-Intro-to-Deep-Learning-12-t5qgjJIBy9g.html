<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make a Chatbot - Intro to Deep Learning #12 | Coder Coacher - Coaching Coders</title><meta content="How to Make a Chatbot - Intro to Deep Learning #12 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make a Chatbot - Intro to Deep Learning #12</b></h2><h5 class="post__date">2017-03-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/t5qgjJIBy9g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and let's build a
chatbot that can answer questions about
any text you give it it an article or
even a book using care offs
just imagine the boost in productivity
all of us will have once we have access
to expert systems for any given topic
instead of sifting through all the
jargon in a scientific paper you just
give it the paper then ask it the
relevant questions entire textbooks
libraries videos images whatever you
just feed it some data and it would
become an expert at it all seven billion
people on earth would have the
capability of learning anything much
faster the web democratized information
and this next evolution will democratize
something just as important guidance the
ideal chat pod can talk intelligently
about any domain that's the Holy Grail
but domain-specific chat BOTS are
definitely possible the technical term
for this is a question answering system
surprisingly we've been able to do this
since way back in the 70s lunar was one
of the first it was as you might have
guessed rule-based so it allowed
geologists to ask questions about moon
rocks from the Apollo missions a later
improvement to rule based Q&amp;amp;A systems
allowed programmers to encode patterns
into their BOTS called artificial
intelligence markup language or AI ml
that meant less code for the same
results but yeah don't use AI ml it's so
old it makes numa numa look new now with
deep learning we can do this without
hard coded responses and have much
better results the generic case is that
you give it some facts as input and then
ask it a question it will give you the
right answer after logically reasoning
about it the input could also be that
everybody is happy and then the question
could be what's the sentiment the answer
would be positive other possible
questions are what's the entity what are
the part of speech tags what's the
translation to French we need a common
model for all of these questions this is
what the AI community is trying to
figure out how to do Facebook research
made some great progress with this just
two years ago when they released a paper
introducing this really cool idea called
a memory Network LS TM networks proved
to be a useful tool in tasks like text
summarization but
their memory encoded by hidden states
and weights is too small for very very
long sequences of data be that a book or
a movie a way around this for language
translation for example was to store
multiple LSP m states and use an
attention mechanism to choose between
them but they develop another strategy
that outperform LF TMS or QA systems the
idea was to allow a neural network to
use an external data structure as memory
storage it learns where to retrieve the
required memory from the memory bank in
a supervised way when it came to
entering questions from COI data that
was generated that info was pretty easy
to come by but in real-world data it is
not that easy most recently there was a
four-month long cattle contest that a
startup called
meta mind placed in the top 5% for to do
this they built a new state-of-the-art
model called a dynamic memory network
that built on Facebook's initial idea
that's the one we'll focus on so let's
build it programmatically using Karos
this data set is pretty well organized
it was created by Facebook AI research
for the specific goal of improving
textual reasoning its grouped into 20
different tasks each task tests a
different aspect of reasoning so overall
it provides a good overview of all the
different capabilities of your learning
model there are a thousand questions for
training a thousand for testing per task
each question is paired with a statement
or series of statements as well as an
answer the goal is to have one model
that can succeed in all tasks easily
will use pre trained glove vectors to
help create a sequence of word vectors
from our input sentences and these
vectors will act as inputs to the model
the dmn architecture defines two types
of memory semantic and episodic these
input vectors are considered the
semantic memory whereas episodic memory
might contain other knowledge as well
and we'll talk about that in a second we
can fetch our babel dataset from the web
and split them into training and testing
data the love will help convert our
words to vectors so they're ready to be
fed into our model the first module the
input module is a GRU or gated recurrent
unit that runs on a sequence of word
vectors a GRU cell is kind of like an L
STM cell but it's more computationally
efficient since it only has two gates
and it doesn't use a memory unit the two
gates control when its content is
updated and when it's erased off a
recess of a resistance of a recess and
the hidden state of the input module
represents the input process so far in a
vector it outputs hidden states after
every sentence and these outputs are
called sacks and the paper because they
represent the essence of what is fed
given a word vector and the previous
time step detector will compute the
current time step vector the up linking
is a single layer neural network we sum
up the matrix multiplications and add a
bias term and then the signal it
squashes it to a list of values between
0 and 1 the output vector we do this
twice with different sets of weights
then we use a reset gate that will learn
to ignore the past time steps when
necessary for example if the next
sentence has nothing to do with those
that came before it the update gate is
similar in that it can learn to ignore
the current time step entirely maybe the
current sentence has nothing to do with
the answer whereas previous one bit then
there's the question module it processes
the question word by word and outputs a
vector by using the same gru as the
input module and the same wait we can
encode both of them by creating
embedding layers for both then we'll
create an episodic memory representation
for both the motivation for this in the
paper came from the hippocampus function
in our brain it's able to retrieve
temporal states that are triggered by
some response like a sight or a sound
both the fact and question vectors that
are extracted from the input enter the
episodic memory module it's composed of
two nested gr use the energy ru
generates what are called episodes it
dozens by passing over the facts from
the input module but when updating its
inner state it takes into account the
output of an attention function on the
current fact the attention function
gives a score between 0 and 1 to each
fact and so the gru ignores facts with
low scores after each full pass on all
the facts the in
gru outputs an episode which is then fed
to the outer GRU the reason we need
multiple episodes is so our model can
learn what part of a sentence it should
pay attention to
after realizing after one pass that
something else is important with
multiple passes we can gather
increasingly relevant information we can
initialize our model and set its loss
function have categorical cross entropy
with the stochastic gradient descent
implementation rmsprop
then train it on the given data using
the fit function we can test this code
in the browser without waiting for it to
Train
because luckily for us this researcher
uploaded a web app with a fully trained
model of this code we can generate a
story which is a collection of sentences
each describing an event in sequential
order then we'll ask it a question
pretty high accuracy response let's
generate another story and ask it
another question hero status let's go
over the three key facts we've learned
GRE use control the flow of data like
LST M cells but are more computationally
efficient using just two gates update
and reset dynamic memory networks offer
state-of-the-art performance in question
entering systems and they do this by
using both semantic and episodic memory
inspired by the hippocampus drum roll
please no never mind
nemanja Tomac is the coding Challenge
winner from last week he implemented his
own neural machine translator by
training it on movie subtitles in both
English and German you can see all the
results in his iPhone notebook amazing
work wizard of the week and the
runner-up is Vishal bot - despite the
massive amount of training time nmt
requires michelle was able to achieve
some great results I vow to both of you
this week's challenge is to make your
own Q&amp;amp;A chat bot all the details are in
the readme github links go in the
comments and announce the winner a week
from today please subscribe for more
programming videos check out this
related video and for now I've got to
ask the right questions so thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>