<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Word2Vec (tutorial) | Coder Coacher - Coaching Coders</title><meta content="Word2Vec (tutorial) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Word2Vec (tutorial)</b></h2><h5 class="post__date">2017-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pY9EwZ02sXU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">tada tada Dada Dada Dada Dada Dada Dada
I've been listening to a lot of
classical hey my life I must be live
right now because I hit the live button
I can't see myself so I always have to
wait like 15 seconds or so there I am
okay right okay
oh well it's Suraj how's it going ah so
today we are here live and we are going
to make some word vectors out of a
series of books called Game of Thrones
if you know Game of Thrones give it a
shout-out in the in the comments let's
see who knows what this is it doesn't
matter if you don't the point is we are
learning about the concept of word
vectors and we want to take some books
and make them into vectors and once we
have these vectors we're gonna do a
bunch of really cool stuff with it
alright so who's in the house let me
name some names we got Jake Akash party
Taher Teddy recce party ricardo angel
got a lot of people in the house alright
so okay uh let's go ahead and do a 5 min
a Q&amp;amp;A and then we're going to get into
the coat it's going to be an AI Python
notebook and I'm going to give you guys
uh the data set as well so uh
five-minute Q&amp;amp;A let's get started
glove or word Tyvek we're using word to
Veck not glove
although glove is similar also I haven't
used glove before but I've heard good
things another question
best character in Game of Thrones uh to
be honest I don't even read Game of
Thrones I just picked it because I
thought it was I thought the I think the
idea is cool I don't have time to read
anything or watch TV shows and anymore
I'm just focused on content any math's
we are going to do yes we're going to do
some ash we're going to use the cosine
similarity as a measure of distance
between word vectors it's a corpus yes
it's five different books but we're
going to treat it as one big corpus
please say my name PU Sh
okay um okay doing the D pointing
foundation and feeling utterly lost keep
pushing through or go back to school
okay guys listen okay so clearly this is
there's a lot of stuff and deep learning
and there's a lot of math and from what
I've read I have taken your feedback
into consideration we need to really
dive into the math and we're going to
we're gonna go so deep into the math in
the next video okay so the next weekly
video get ready we are going to really
dive into the math please say my name
Colin what's a word vector I'll explain
that in a second clean the camera
there's moisture okay okay um video
isn't clear bro okay I can't help that
right now is opening i worth to explore
yes one minute rap before this I love
your rap I'll do that um yeah I'll do
that um let me answer some more
questions how can I classify images from
a live feed and put the output in a file
from a live feed you want to oh man in a
live feed I can we know how to do it
recorded but in a live feed you'd want a
live classifier to look at this to look
at your screen or maybe segment the
screen so you want to use probably
JavaScript for that JavaScript would be
easy Khan's net is Andre Carpathia's
library would be great for them the cam
looks dirty hey man I was just at the
beach recording some cool stuff uh
predict who dies using word vectors that
that's gonna be possible
are you going to use tensorflow for this
no we're going to use
uh word Tyvek and we're going to use a
bunch of other smaller libraries alright
okay so one more question and then we're
going to get started okay here we go if
we increase vocab size by adding words
will that help yes yes it helps uh it
helps if it will it helps if if the
words are relevant to the to the problem
we're trying to solve if they're
relevant to the story which is Game of
Thrones in this case okay so that's it
for the questions let's get started with
this I'm going to start screen sharing
it's going to be an ipython notebook and
then we're going to oh that's right a
rack let's let's do a little rap for a
second um let me do a rap on vectors um
vectors right okay so start the row with
the rap okay oh here we go uh I rap
about vectors I do it man cuz I'm Victor
Victor vector what's the vector Victor
I'm on a plane going crashing down like
my mind is a scalar scalar clouds going
down scalar clouds going up I see one
two three in the sky and it's my enemy
no it's not it's just mine and I'm gonna
fly back to this screen so here we go
that was it for the rap and let's get
started because we are getting serious
guys all about words that get sits in
English favorite movie the matrix the
matrix how about that I'm just going to
add to list the people who said the
matrix let's get started this is an NLP
session we're going to do some natural
language processing ready okay let's
let's start screen sharing let's start
screen sharing
okay okay let me move you guys over here
okay all right so let's uh get started
with this let me make sure that this is
showing hold on okay so uh we want to
let me just first stop say we want to
the the goal is to create word vectors
from bit from uh Game of Thrones
uh data set no look guys this is these
are just four five books let me show you
guys now I'll just show you guys ah and
play with them and uh and analyze them
to see semantic similarity okay let's
let's look at what uh we've got here so
hold on I can even show you guys the
data set for a second so these are just
the five books for Game of Thrones
okay there's literally just the text
files for the books they are these huge
text files for the books that's that's
all they are okay and I just applied
them I downloaded them from Pirate Bay
no regrets uh you know what I'm saying
so that's what that's what this is and
that's it
we've just got five books in the series
we're going to take all these books and
we're going to create word vectors from
then we're going to treat it as one big
corpus okay so that's what we going to
do so let's just go dive right into this
baby alright so the first thing we want
to do is we want to import our
dependencies now we've actually got a
lot of dependencies for this so I'm
going to explain every single one so the
first one we want to do is import future
and why do we want to import future can
anyone tell me why in the comments as I
type this the reason we want to import
future is because it's the missing link
between Python 2 and Python 3 it allows
us to use the syntax from both it's kind
of like a bridge between the two
languages and we're going to import
three functions that we're going to use
for this okay so that's the first step
uh once we have that we want to uh
we're going to we're going to encode our
word so we're going to make forward
encoding how are we going to encode our
words while we're going to use codecs
the next one is we're going to perform
some read regex and regex
is basically whenever we want to I want
to like search some file really fast
that's that that's all about regex it's
it's a it so it's a way of like quickly
and efficiently searching through a
large text or number database for what
you need the next one is for logging so
actually we don't need it we need we
don't need a log oh now I actually
haven't talked about concurrency too
forward so this is going to be
interesting we're going to import this
multi processing library to perform
concurrency and if you don't know
concurrency is a way of running multiple
threads and having each thread run a
different process so it's it's it's it's
multi-threading multi processing it's a
way of moving it's a way of having your
program run faster okay
and I haven't talked about this before
but we're going to do this a lot in the
future especially when we get to
distributed machine learning which is
later on in this course okay so so
that's it for multi processing the next
one is dealing with the operating system
system like we want to like reading a
file like reading a file and for that we
will want to use the OS module and we
want to do some pretty printing make it
human readable and how do we do that we
import pretty crappy prints for sure
okay ah the next one is more regular
expression so blob was uh I'll show you
guys a difference here but this is for
this is for a more granular regular
expressions so this is so this is like
step one by the way is like step zero
import of dependencies and I've got two
more and then we're gonna get started
with the actual logic of the code okay
so what else we got we got the natural
language toolkit because we're gonna be
using NLP Kay okay
natural language - okay let me show you
guys in LC paper ik second if you don't
know now you know let me show you guys
LPK because NLP cane I made videos about
this before is okay NL ck is
awesome it is so easy to use let me zoom
in on this thing okay
literally it can tokenize sentences in
single sent single lines of code so
if you have a sentence like at 8 o'clock
on Thursday morning Arthur didn't feel
very good
you feed at 2:00 NLT Kay and boom it
will give you the tokens for each word
why is this useful well you can have
part of speech tagging POS tagging that
which means like oh is this a noun is
this a verb is this a CD how does it
know these things because it has a pre
train well it's actually two things it's
for something for some bit it's got a
pre train model which a trained uh and
another part of in ltk is it's got its
using the uh which I talked about in the
last video which is the uh uh like
having that that database of
pre-recorded sentiment the lexicon
that's the word I'm looking for
alright so then we're going to talk
about to back now words avec is the
that is what we are going to you that is
the real meat of this code and I'm going
to really deep dive into what word
Quebec is at a high level right now work
avec is what Google created to basically
they trained up the cranium shaker a
neural network on a huge data set of
word vectors and it created vectors and
we can use these vectors in other way so
it's like a generalized collection of
word vectors okay so we're I'm going to
talk about all this in a second let me
just keep having al feyza the tendencies
the neck length dimensionality reduction
when we have our word vectors they're
going to be multi-dimensional they're
going to be 300 plus dimension word
vectors okay because they're so
generalized and we want to plot them on
a 2d graph so we can see them and how
are we going to do that well we're going
to perform a technique called
dimensionality reduction I have a video
on this called visualize it is set
easily check it out check it out okay
now so then we're going to part our math
library which is numpy and then we're
going to import our plot thing library
which is going to be matplotlib and
finally we're going to AH what's the
next one parts well parts of days that
we got one more after this so many
libraries but no it's it's important
because right now we're talking i we
want to talk about the concepts here and
we're going to dive into these specific
processes later on okay
uh so import pandas SPD and lastly
here's the last one
visualization seaboard Seabourn is going
to help us visualize our data set okay
as SN s okay that's it for our
dependencies boom alright
oh okay so now that we've done that uh
we are going to we're not using pre
trained word vectors we are we are using
word vectors that we trained in real
time let's see no model named pipe ply
lot o PI plot i plot no model named
Hernandez Oh pandas see this is why I
love Python ipython notebooks okay so
okay so that was that
now our next step is to process our data
okay so step one is to process our data
what does this look like well before we
do anything before we do anything we
want to clean our data so how do we
clean our data well n lck has a really
handy function for this well the first
one is called Punk'd and the next one is
called a stop word so what is this -
what this does is it downloads punked
downloads uh it's a tokenizer it's a pre
trained tokenizer it's a pre trained
tokenizer and well it's kind of let us
do is tokenize our text remember I
talked about tokenization last time
tokenization is where we take a sentence
or a word and we take start we take a a
piece of text and we split it into
tokens and those tokens to be sentences
or words or even characters it's
whatever we specify in this case we're
gonna do sentences okay so that's what
pump does and stop words are words like
and are sorry click and and the and a
you know of words that don't really
matter they don't really have a lot of
semantic meaning and we want to remove
these words and why do we want to do
that so that our vectors that we create
are are more accurate okay so so that so
we done that and it's gonna download ok
it's it says okay you've already got
them if we don't have them it's going to
download it
the next step is to object the book name
things matching the text file and now
like I said we had them right here so
let me make this bigger
here's our books right here today and in
text file so we can just say okay so get
the book file names book file names and
overuse glob Robby's going to let us get
those books that just have that ending
dot txt right and it's going to print
those out for us make sure that we
actually uh that we that we actually
printed them all right the hands there
and it starts here okay boom boom okay
no no no there we go okay so that's for
our text file and let me print out the
books let's print them out let's print
them out make sure that we got them
filenames sorted glob glob let's see
what we got here dot txt um let's see
hmm I so we should have them here and if
we don't then the problem is that we e
hold on sorted glob glob let's say that
the hold on a second so um let's see
let's see let's see uh interesting so
what the what this is is a fun
say listen ah okay so okay so it seems
like they might be in the route yeah
they might be in the route one second
and what I can say is one second then
closing folder
hold on second this is actually not oh
you're right where a lot of people think
dot slash okay hold on a second oh my
god some of people saying some oh
okay so okay okay okay okay
in Val it's a text so let's get that
current directory swear god this is so
annoying
right now hold on okay Lex okay so um
hold on a sec okay so oh my god okay
names got some tax book file names we
talked about I just named them right
here
let's see what file names Oh a second
invalid syntax so we really don't have
time for errors like this Oh God okay
guys so I've actually had a comments
that with a lot of up votes and the
comment was we should just not spend
time actually writing out the code and
just show the code since I'm already
reading off of it anyway so I already
have the code anyway so why don't I just
show you guys the code and I'm going to
explain it as we go okay because
seriously I don't have time to look at
why this is not working right now print
book filenames that's exactly what it
was okay so we don't we don't have time
for this okay so let's just talk about
this okay let me move this over here
okay and I've got my notes here as well
okay hold on a second so okay so where
were we let me let me see if this
actually works in wall CKY let me just
let me just let me just let me just
restart the tol ipython notebook and run
this from scratch okay let's run it from
scratch if I thought notebook and we
close out everything and okay so here we
go hold on that was I miss boom okay so
let's see let's how to give it to us
okay and try out this one okay so let's
let's just run what we have here I love
inline populating log log log boom okay
okay
file names okay so clearly there is
there clearly we're having a problem
right now and the point is let's just
okay so this is this is precompiled
anyway and i talked about what this is
doing and we're going to really deep
dive into the important concept here
okay so we're gonna keep going onwards
we don't have time to stop with this
right now so okay so we've got our book
file names right and we next up is to
combine the book at the one string and
why do we want to do this because we
want to have wine a corpus for all of
those books and that's what this does we
initialize a raw corpus we say you let
me make this bigger because we are we
really want to we start with you because
it's unicode right it's a unicode string
and we want to convert it into a format
that we that we can read easily and what
is that format
utf-8 right here okay so UTF a so this
is where the codex library comes into
play we are using a codex library to
read in the book filename and convert it
into utf-8 format now that remember that
corpus raw function we just initialized
up here well now we want to add all the
books that we see to that corpus and the
way we're going to do that the way we're
going to do that is we're going to add
we're going to add it all to this corpus
draw and at the end of it it's going to
have all of those books in one in one
variable in memory corpus raw okay which
is going to be a very very very big
Barrett variable okay that's what we're
going to do and so that's the first step
and once we have that then we're going
to split the corpus into sentences now
remember when I said we downloaded that
a pumped-up model right up here let me
show you guys NL ck download punked well
now we're gonna actually load that into
memory that it's a trained model and
it's it's loaded in it's in a byte
stream and that's what pickle is it's a
it's that file format that we can load
as a byte stream now that's what that
does and it's gonna load it up into this
tokenizer variable this tokenizer is
pre-training it turns up words into
tokens and the type of tokens we want
our sentences in our case right so we'll
use a tokenizer up or use a tokenizer to
tokenize that corpus but which is every
single word we have right and let me
reopen this so every single word we have
and this could be anything guys this
could be any piece of text you want any
book anything you download any big piece
of text this same the same principles
apply okay um and we're going to put
those all into this raw sentences
variable once we have that raw sentences
variable we're going to convert it into
a word list so what do I mean by a word
list well um I also a message make sure
so for our word list we want a list of
words but right but so but but what what
exactly do I mean by that let me um so
for our word list let me see the
comments what do you guys up to uh okay
so this is okay so let me let me comment
this as well so once you convert into a
list of words remove unnecessary
characters we want to remove unnecessary
characters that's why we have that a to
z a to z I'm going to split the split
into words no hyphens you know and and
it's going to be a list of words so
that's an array so sorry it's a list of
words okay
once we have that list of words for each
sentence we're going to take those words
and tokenize it so it's going to be a
sentence where each word a sentence
where each word is tokenized that's what
this does so for every sentence that we
have we're gonna initialize an EMPI
sentence list we're going to add it
we're going to add sentences to it and
uh and then once we have that all right
so I've got
comments that I should slow down a
little bit I can do that I can do that
okay so let me slow down a little bit
there's so much I'm trying to cover so
let me I'll slow down okay so we have a
list of words each word is tokenized
okay each list of words is considered a
sentence and we can see that when we
print it out
so we have our draw sentence which is he
was an old man past 50 he's seen as the
Lord Lynx come and go this is a sentence
taking directly from Game of Thrones and
it said that it did set the 5th index so
if because we combined all of our books
in order this is going to be the 5th
sentence and the 1st book because it's
one big corpus
ok and once we have that uh once we have
that we're going to convert it into a
word list which is tokens and CDs you
characters they basically help us
convert them into their unicode
representations right so whenever we're
dealing with words any kind of text we
have to make sure it's in the right
format and Unicode is that format we
want for vectors okay and utf-8 8 is
that format for reading from files so
once we have our big book corpus let's
print print out how many tokens we have
okay um and each sentence is and what
we're going to do is we're going to
consider each uh uh each each sentence a
token and we can print out as 1 million
eight hundred you know ten thousand
tokens okay so okay so know we're going
to get into vectors I'm gonna explain
back to in a second in detail we haven't
gone to that part okay so that's what we
that's what we've just done
now we're gonna train word to Veck okay
these are our hyper parameters let's
let's talk about vectors for a second
like word okay so I'm trying to find a
great image for this in a second so word
embeddings are here so tensorflow
probably has a great image for this so
okay so here's a great one here's a
great one copy image address let's blow
this image up let's get it really big
okay so here are vectors for example to
look at the one on the left okay look at
the one on the left King man woman queen
these are word vectors so when we have a
set of
okay we have a set of words like so
let's say um you know uh masculine uh
John uh you know Bob Sergei you know
like words that are like like men right
so so so words that all have the same
semantic similarity we can all we can
generalize all these words into a vector
representation we also call them word
embeddings so there's a lot of terms
here but we want to make sure that it's
it's all really the same thing it's we
don't want to be confused word
embeddings or vectors same thing so if
we have a set of words we can generalize
something to man okay so that what man
is okay so man woman king queen these
are generalized vector representations
that we have created after training on a
very large corpus of text okay so when
we have a new word like we say like uh
you know what's what something manly I
mean this is this is right now this is a
gender landline I mean this example but
let's let's pick an example like what do
men have that women don't um this is
like uh you know um right so uh a penis
right that's the only thing I can think
of that's like literally true and
doesn't cross any kind of you know other
things so penis right so penis would
then if we were to feed it to a trained
word to Baek ma de
it would see that penis is closest to
man okay why does it notice because it's
it's trained on a corpus of text tune up
to eventually find the similarity that
that generalized representation across
all of these words and how does it do
this well it converts words into vectors
and then it creates vectors of vectors
and these vectors are when we when we
plot them out we can see how similar
different things are yeah beard was a
great one as well okay so balls okay cuz
is it that right and so then once we
once we have these vectors we can see
what's similar to them right we can do
all sorts of things mainly there are
three things we want to do okay let me
write this down
maratha sound so write this down this is
very important so one three
have vectors three main tasks there are
three main tasks tasks the bet are some
women have beards so three main tasks
that vectors help with here are the
three main tasks uh the one of them is
distance failed on distance uh
similarity similarity and ranking okay
so those are generally what they help us
with and um and what do I mean by that
similarity like if so vectors are good
for things like if we want to if we want
to think about if we think about any
kind of uh what's the word I'm looking
for like if you want to see how what's
how similar two words are which we're
going to do later on if you want to rank
to something right so if we wanted to
browse all the scientific papers in the
world will create vectors out of all of
them and then we want to rank them in
terms of some metric that we decide like
what's the one that has the most
information on say a climate change we
could then rank them semantically so it
would say here's a number one paper that
has the most a verb verb usage or word
usage about the about the topic climate
change and it uses vectors for that okay
these are really useful okay so let's go
ahead and talk about these hyper
parameters so known features is a
dimensionality of these word vectors
okay um so we're so number of features
is the dimensionality and we say 300
because hmm we say 300 because this we
could say 400 we could say 500 the more
dimensions that good stuff the more
dimensions the more complex so more
computationally complex XS are expensive
to train uh but uh more but also more
accurate okay so the more dimensions the
vector has the more general lot more
dimensions means more generalized more
generalized okay so we're going to say
300 for now okay
minimum word count threshold is what is
that the the smallest set of words that
we want to recognize okay when we
convert to a vector the number of
threads to run in parallel so if we are
so what is the actual structure of a
vector that's it's a great question
oh the structure of a vector is so the
definition of a vector and we're going
to talk about it in the next video but
the depth we're gonna really deep dive
into later but the definition of a
vector is us up it's it's a set of
numbers okay
and uh in in this context in machine
learning and fizz's it's got a different
context we talked about the it talks
about a direction in our case we're just
talking about a set of numbers so we can
think of it as a list a bit of a huge
list of numbers okay that's what that's
kinda what it's represented as um okay
so an intensified we think we use the
word tensor because a tensor is an N
dimensional array of numbers so a vector
is a type of tensor so Becker is a type
of tensor this doesn't really belong
here I'm just saying this right now
vector is a type of tensor okay so so
then the number of threads running
parallel this is where that multi
processing library that we imported
comes into play okay we want to say how
many workers do we have so the more we
have the faster more open more more
workers faster we print contact window
length is the the size of the of what
we're looking at it's add at a time like
this the size of like if we think of it
as like looking at blocks of seven words
at a time that's the contact window down
down temple setting for frequent words
is a um hold on scalar vector matrix
tensor exactly we're going to be talking
about those in the next video but down
temple settings for frequent words is uh
once we have once we once we are trained
word Tyvek model is noticing a lot of
frequent words we don't want to have to
look at them constantly so any number
between zero and one be negative five is
good for this the best generally that's
that's what we found is is it is a good
it's generally a good use case for this
but basically how often do we want to
look at the same work the more frequent
aware it is the less we want to use it
to create vectors because it's already a
part of our training model so then the
seed is for the random number generator
right so that's random number generator
and why do we use a random number
generator we use it to pick what part of
the text we're going to look at the turn
into vectors okay we're and to see make
sure that it's deterministic this is
good for debugging reminisced ik good
for debugging okay so this is our actual
model right here a word to beck model we
imported from the gen sim library and
let me show you guys jensen for a second
Jensen is super useful uh it's for topic
modeling basically you give it any kind
of corpus like this it'll create a model
it will train it you can save it you can
load it later on and then given some
words like woman and King something that
was in the something that was in the
actual corpus it'll give you words like
it'll give you things like how similar
they are uh uh
what doesn't match uh what's the what
give you the straight-up vector so you
could use it later on Jen Sims a great
library okay so that's going to actually
train our model and this is going to
take two since our model is relatively
small in the context of deep learning
this is only going to take thirty
seconds or so d'etre okay um all right
so and it's actually not the same
definition as in physics and there's a
lot of debate about this axis I've been
looking at a lot of Stack Overflow
answers and a lot of poor answers and
it's crazy how much people are debating
over these words in machine learning but
the point is it's just numbers it's
number representations that we that we
create and then we can feed into our
model okay so then we're going to build
our vocabulary and uh so then we're
going to build our vocabulary for using
those sentences okay this is how we
actually uh this is how we load the the
corpus into memory
we haven't actually trained it we built
our model right this is step three build
our model
but which I should have wrote in a bring
up here so step three is Bill Maher Bill
Maher okay so once we built our model we
have loaded our corporates that we clean
in to memory and we printed out the size
of it now we can start training and it's
going to train on all those sentences we
gave it it's going to take 30 or 30 or
40 seconds and when it's done training
we're going to save the file for use
later on okay and easily do that you can
get that os module great for that we can
we can save it and we can save it and
then we can load it later on in fact we
can load it right now but we'll load it
for memory right now okay um and once we
have that we're going to compress those
work okay so that's going to be three
hundred dimensional word vectors once
it's trained so in this Thrones to bec a
model right here that we've trained it's
going to have it's going to contain all
of those word vectors that we just
trained is going to everything is in
memory right here but these are three
hundred dimensional word vectors okay we
cannot map a three hundred dimensional
word vector on a plot for us puny humans
to see how are we going to do that well
we're going to use a method called t-sne
which stands for T's stick it was it T
stochastic distributed Haber and Betty
okay and I have a great video on that um
please let me let me just um it's a it's
an awesome technique this is not useful
but basically I'm gravy don't let me
just have the video name for that um PCA
is another my video it's called what the
called how to visualize a dataset easily
I really dive into this this this uh
method right here how to visualize data
set easily in a nutshell t-sne
cakes or three hundred dimensional
vector and squashes it into just two
dimensions why so that we can then plot
it and view it how does it do this it's
actually a long explanation and the
video is great for that five and
explanation
definitely check it out okay um but uh
right so once we hit so that's what
t-sne does it's going to create those
vectors and so it's going to squash it
okay and we're going to take all those
vectors and put them in one gigantic
matrix right we've initialized t-sne
here but we haven't trained t-sne right
so t-sne is a model it's a machine
learning model and we have to train it
okay so we'll train it on that word
vector matrix and this can take a minute
or two like it says and uh uh so it's
going to create this word vector it's a
2d matrix right so this is one gigantic
matrix and it's got the the plots on the
point with it okay so then we're going
to plot what we've got okay
so what I mean by plot well we want to
plot it in 2d spray space so for every
word we have in that vocab we want to we
want to have we want to have three uh
columns the word the X the x coordinate
and the y coordinate now how does it get
these coordinates well that's what t-sne
does not only is it does it uh not only
is it uh so it's squashing these vectors
into two-dimensional vectors but it's
also giving us the x and y coordinates
of those vectors in in two-dimensional
space okay so these are all words from
that corpus right these are all
gameofthrones e words right uh so that's
what that does and ah once we've got
that then we're going to plot them on a
graph so this is where Matt plot light
comes into play right we're going to
plot these points we're going to plot
them on a graph and it's a lot these are
our word vectors there's there's a lot
of them here right and we brought it
down to scale so we could see a lot of
them but all of our word vectors or word
embeddings whatever you want to call
them are here in 2d space now what are
we going to do with them well we can see
what vectors are close to each other
let's start with that
let's see what what vectors are close to
each other and what that tells us about
the data okay so uh the first thing we
want to do is zoom in on this right and
that's what this function does it
creates a bounding box of x and y
coordinates in that graph that we have
and it shows just that bounding box
that's what this function does okay and
so then we'll use that we'll use that to
we'll say okay so in the bounds of this
and that the XY bounds of of these
coordinates at we give it let's see what
we what it gives us this is what it
gives us when we look at this corner
well what are all these um these are
names for risk on Greger
trigon sand or these are all names and
they look like male names as well right
they look pretty much male okay so
interesting just by training vectors on
art our model it created up just by
training and creating vectors and plying
them and a two-dimensional graph using
t-sne uh I remove stop words and special
characters stop words will included in
that list uh at least in NL TK um but it
it shows that these words are all close
to each other because it knows that the
distance between worlds it's small so it
grabs them very close to each other it's
pretty cool right so if you look at a
different region so if you look at a
different region we'll see that hey this
is food right this is a totally
different region pepper pickled you know
a cod olives turnips these are all
similar words and I want to really
stress the importance the brevity of
vectors okay it's not the brevity the
wrong word a wrong word vector the the
this the enormous awesomeness of vectors
okay word vectors are for word clusters
are related there's so much we can do
with this um in every field in legal in
law we can we can train an AI judge
using this thing using semantic
similarity to see the difference is
between a different case data or we
could doctors we could we can see what's
similar we could use this to try new
drugs like what is a
what is it submit what is the cosine
similarity or what is some some
similarity metrics that we defined
between a corpus of scientific papers on
some problem that we're trying to solve
so we can
so this most similar function is already
created for us but basically we'll say
well we'll give it stark and then it'll
show this the similarity by a number so
it ranked them all of these names are
similar to stark right um and how are we
doing this well there's a lot of methods
for measuring semantic similarity
there's a lot of methods for measuring
the similarity between vectors and the
one we're using here is the cosine
similarity so let me let me bring that
up the cosine similarity um
the cosine celerity impossible distance
metric we can use because turning these
words into vectors turning our videos
into vectors turning our images into
vectors gives us a way to mathematically
reason about these things we can we can
reason about them just like we reason
about numbers in a in a in a in a in a
in a mathematical way right so uh so
this is the this is the formula for the
cosine similarity right so given two
vectors we can we can use the dot
product and the magnitude of those
vectors to calculate them okay so that's
one method there's a lot there's like
the ha seein I think is collected up
Hokkien similarity um like that
sorry the Gaussian no of course the
Gaussian similarity but it's like the
anyway
there's a lot and I know I'll link it
more in that in a different on so um
anyway so then I will use that to say
okay so given these three words stark
Winterfell and Riverrun
it'll say so we'll start is related to
Winterfell as X is related to Riverrun
and what is X and that's what this does
it says okay so it's going to measure
the similarity between two the first two
parameters we give it and then it's
going to say for that similarity be that
like 0.5 or 0.6 and point 7 what is the
what is what is something that similar
to that last parameter Riverrun and
it'll find that from our list of
existing vectors and in that case in our
case it'll be truly Tyrion Dany things
like that okay um okay so so there's
that that's the end of this already and
I add because I didn't type out the code
it went a little faster uh but uh yeah
so let's wait where's my where's my
screen here we stop screen sharing and
go back to this oh man
okay hi guys letsa you are ending five
minute Q&amp;amp;A and then we're going to end
this live stream okay I have an awesome
video coming up for you guys have been
working really hard to thank you song
drum I really appreciate it uh I'm
really excited about this next video I'm
using a MacBook
uh.thanks vocalize ah what else
thanks party can you please elaborate on
how is the projection of each word of
each work to the coordinate is I work to
the coordinate what do you mean what a
real-life applications of this great
questions thanks T rocks okay real-life
applications of word vectors take any
take any a piece of text take a book
okay after this live stream download a
book okay download an e-book and convert
it to a text format and then use the
code that I give you and you can easily
feed it towards event and create vectors
what do you do with these vectors you
well then you can besides a similarity
in the distance um what's up what's a
good application for back like uh that
little corpus of what your friends are
saying you can see what what's what you
could rank personalities like you know
what you know chats like this guy's
chance forces this guy's chat or uh this
guy's um you know what he said for in a
speech versus what he said if you want
to compare you know Hitler to Trump I
just went political not trying to go
political anyway but I just did if you
want to compare anything weird vectors
are good for that um ranking uh words
are everywhere guys any kind of
similarity or ranking that's what it's
for and there's a lot of there's a lot
of possibilities okay so how are the
words assembled in suit vectors is it
just context are there any ontological
network being built if so how um right
so so when Google release word to that
okay so train your network on these
vectors and these are labeled this is it
was a labeled corpus of words and I'm
actually going to do this unsupervised
as well but basically um we we convert
words to vectors and single single words
to vectors and so that gives us a number
point eight or point nine and once we
have those vectors then we could create
even more generalized vectors by looking
at that similarity and that similarly
could be like
point-9 another 9.9 on an eight these
are very similar okay so and that
creates even more generalized actors
okay so anyway guys I've got to go I've
got to edit introduced I'm shooting to
do some Udacity talking to do uh hands I
love you guys stop we're gonna and next
livestream is going to be much more you
know dope and that's what this one
wasn't there Aldo uh I'm Joe
you guys are dope we're Aldo okay so for
now I gotta go take a chill pill so
thanks for watching love you guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>