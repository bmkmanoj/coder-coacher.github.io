<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dimensionality Reduction - The Math of Intelligence #5 | Coder Coacher - Coaching Coders</title><meta content="Dimensionality Reduction - The Math of Intelligence #5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dimensionality Reduction - The Math of Intelligence #5</b></h2><h5 class="post__date">2017-07-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jPmV3j1dAv4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and let's
visualize a data set of eating habits to
see what we can learn from it shall we
there's some really hard scientific
questions out there are we alone in the
universe what is consciousness what is
dark matter really questions like these
have multi-million dollar payouts and
have been troubling scientists for
hundreds of years but guess what we very
likely already have the answer to them
the problem is that they aren't in plain
sight they're hidden in data the former
leader of the US government efforts to
sequence the human genome said that it
took several years of effort a huge team
of researchers and $50,000,000 to find
the gene responsible for cystic fibrosis
and a not but that same project could
now be accomplished in a few days by a
good grad student you watching this
right now yes you can make a Nobel
worthy breakthrough with just your
laptop the data you need is freely
available you just have to discover the
hidden relationships in it the field of
dimensionality reduction is all about
discovering nonlinear non-local
relationships in data that are not
obvious in the original feature space if
we reduce the number of dimensions in
some data we can visualize it because a
projection in 2 or 3d space can be
plotted really easily unless you use PHP
training a model on a data set with many
dimensions usually requires vast time
and space complexity it also often leads
to overfitting not all the features we
have available to us are relevant to our
problem if we reduce the dimensions we
can reduce the noise the unnecessary
parts of the data and find those that
are surprisingly very closely related
and once in a smaller subspace we can
more easily apply simple learning
algorithms to it we can divide
dimensionality reduction into two
different topics feature selection and
feature extraction selection is all
about finding the most relevant features
to a problem it can be based on our
intuition as in which features do we
think would be the most relevant or we
could let a model one of the best
features by it
extraction means finding new features
after transforming the data from a high
dimensional space to a lower dimensional
space will perform the latter using a
technique called principle component
analysis our data set is going to be a
record of the 17 types of food that the
average person consumes per week for
every country in the UK so we've got 17
features / dimensions let's see what
kind of insights we can get from this
data pca transforms variables into a new
set of variables which are a linear
combination of the original variables
these new variables are known as
principal components pca is an
orthogonal linear transformation that
transforms data to a new coordinate
system such that the greatest variance
by some projection of the data lies on
the first principal component the second
greatest variance on the second
component and so on the variance is the
measure of how spread out the data is if
I were to measure the variance of the
height of a team of basketball players
it would be pretty low but if I added a
group of primary school children to the
mix the variance would be pretty hot our
first step is to standardize the data
PCA is a variance maximizing exercise it
projects the original data onto a
direction which maximizes variance if we
were to graph a small data set that
shows the amount of variance for the
different principal components it will
seem like only one component explains
all the variance in the data like Putin
at the g20 summit but if we standardize
the data first then we'll see that the
other components do indeed contribute to
the variance as well standardizing means
putting the data on the same unit scale
for us that would be grams for
everything not a combination of
kilograms and grams that means the data
should have a mean of 0 and a variance
of 1 mean is just the average value of
all X's in the set X which we can bind
by dividing the sum of all the data
points by the number of them the way we
calculate variance is by computing the
standard deviation squared
deviation is the square root of the
average distance of data points to the
mean it's used to tell how measurements
for a group are spread out from the meet
once our data is standardized we're
going to perform I dinna decomposition
so if your mama is so fat she's not
embeddable in 3-space eigenpairs will
help fix that i ghen is a german word
that roughly translates to
characteristic and in linear algebra an
eigenvector is a vector that doesn't
change its direction under the
Associated linear transformation do you
come to my solo fine and you compose my
sole value pair decompose my soul
another way of putting it is that if we
have a nonzero vector V then it's an
eigen vector of a square matrix a if a B
is a scalar multiple of B so the lambda
scalar is an eigen value or
characteristic value associated with the
eigen vector V eigen values are the
coefficients attached to I inverse that
give the AXYZ magnitude if we had a
shear mapping and displaced every point
in a fixed direction notice how the red
arrow changes direction but the blue
arrow doesn't the blue arrow is the
eigenvector of the mapping because it
doesn't change direction and its length
is unchanged its eigenvalue is 1 both
terms are important in many fields
especially physics since they can help
measure the stability of rotating bodies
and oscillations of vibrating systems
many problems can be modeled with linear
transformations and eigenvectors give
very simple solutions
our wage dimension the definitely are
you speaking above other people what
better reason that so many we can
commence a symphony in it for the press
conferences and debates a bottle on a 2d
proud wish so we could even paint
ourselves I'm down states in city data
but they're by memory if we had a system
of linear differential equations for
example to measure how the crows of
population of two species x and y affect
one another like if one is a predator of
another solving this system directly is
complicated but if we could introduce
two new variables V and W which depend
linearly on X we can decouple the system
so instead we are dealing with two
independent functions the eigenvectors
and eigenvalues for this matrix of
coefficients do just this they D couple
the ways in which a linear
transformation acts into a number of
independent actions along separate
directions that can be dealt with
independently so we'll need to construct
a covariance matrix then we'll perform
eigen decomposition on that matrix a
matrix is just a table of values a
covariance matrix is symmetric so the
table has the same heading across the
top as it does along the sides
it describes the variance of the data
and the covariance among the variables
covariance is the measure of how two
variables change with respect to each
other
it's positive when variables show
similar behavior and negative otherwise
pca tries to draw straight lines through
data like linear regression
each straight line is a principal
component a relationship between an
independent and dependent variable the
number of principal components equals
the number of dimensions in the data and
PCA job is to prioritize them if two
variables change together it's very
likely because one is acting on the
other or they're both subject to the
same hidden force performing eigen
decomposition on our covariance matrix
helps us find the hidden forces
at work in our data since we can't
eyeball inter variable relationships in
high dimensional space when calculating
the covariance matrix the mean vector
that's used to help do so is one where
each value represents a sample mean of a
feature column in the data set once we
have our eigen pairs we'll want to
select the principal components we need
to decide which ones can be dropped and
that's where our eigen values come in
will rank the eigen values from highest
to lowest the lowest one they're the
least info about the distribution of the
data so we can drop a number of them
like they're cold
next we'll construct a projection matrix
this is just a matrix of our
concatenated top K eigen vectors we can
choose how many dimensions we want for
our subspace by choosing that amount of
eigen vectors to construct our D by K
dimensional eigen vector matrix W lastly
we'll use this projection matrix to
transform our samples onto the subspace
via a simple dot product operation if we
project our data onto one dimensional
space then we can already see something
interesting
notice how Northern Ireland is a major
outlier it makes sense according to the
data the Northern Irish consume way more
potatoes and alcohol and way to a few
healthy options the same thing happens
if we graph both components we can see
relations between data points that we
wouldn't otherwise to summarise
principal component analysis is a
technique that transforms a dataset onto
a lower dimensional subspace so we can
visualize and find hidden relationships
in it the principal components are I can
vectors coupled with eigen values they
describe the direction in the original
feature space with the greatest variance
in the data and the variance is a
measure of how spread out some data is
the winner of last week's coding
challenge is ohm Jarrah he implemented a
self-organizing feature map for colors
and for handwritten digits really
efficient code and well-documented
great job on wizard of the week and the
runner-up is Hamad Shaikh who developed
a super detailed
notebook on self-organizing maps for
class size effects on students
this week's coding challenge is to
perform PCA from scratch on a data set
of your choice poster of github link in
the comments and I'll give the winners a
shout-out next week please subscribe for
more programming videos and for now I've
got a release of music video so thanks
for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>