<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Guide to Running Tensorflow Models on Android | Coder Coacher - Coaching Coders</title><meta content="A Guide to Running Tensorflow Models on Android - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Guide to Running Tensorflow Models on Android</b></h2><h5 class="post__date">2017-06-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kFWKdLOxykE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">a world it's Suraj and today we're going
to learn how to use a pre trained tensor
flow model on an Android device so this
applies to any device that runs on
Android we would train the model on our
desktop or on our laptop or on the
server and then we would use that pre
trained model on our mobile device so
there's no training that would happen on
the device the training would happen on
our bigger machine either a server or
our laptop but then the inference the
actual making of you know making
predictions that would happen on our
Android device so that's all we're going
to do today and we're doing it for a
hand-written character image classifier
so the user would here's what it looks
like so you draw an image and then you
hit detect and it's going to output the
result and the result is the probability
of what that class is it's a it's a
multi-class classification problem so
it's a very very simple app and the
point of this is to learn how to use
tensorflow with Android very simple use
case right and there are a couple other
examples I'm going to give but in this
example it's just as simple as it gets
so it's a great way to get started you
can use this repository that I'm going
to link to that I'm going to be looking
at here as a base for any other type of
classification project that you want to
make cool so that's what we're doing
we're going to write a handwritten
character image classifier the user
draws it with their hand and then hits
detect and it's going to classify it and
doing this on a server or on your laptop
is good because training is a very
computationally expensive process and
it's a very energy intensive process
right
we can't just plug our phone into our
into the wall all the time but we can do
that for a laptop or desktop so that's
why we do it on those machines but in
Printz we can easily do that on our
phone so what I have here is this little
architecture image of what this looks
like from a from a high level we've got
two we've got two repositories here we
have the Android SDK and then we have
the Android NDK right so what's the
difference here
well the SDK is a standard development
kit and the NDK is the native
development kits what
mean by that well the SDK is written in
Java and the NDK is written in C++ we
use the SDK as an interface to code
everything we want in Java at a high
level so that's all of the things that
are native to Android activities and
fragments and image listeners so I'm
going to actually go into quite a bit of
detail on how Android works I'm not sure
how much you know I've never really
talked about this stuff so rather than
being a flyby I'm just going to go as
detailed as possible and hopefully you
know you don't already know all this
stuff ok so I'm just going to assume you
don't know much about Android so right
so Android has a very specific way that
things work is a very specific way that
objects interact with each other right
you have these activities and then you
have these fragments and image listeners
and all these and then unclick and
uncreate all of these event listeners
for how you would interact with an
Android app it's specific to a
touchscreen like interface and so that's
what the SDK is good for you can do all
that stuff in Java but what the NDK is
good for is for looking at or is for
working with C++ files and what is
tentacle written in the core of tension
flow it's written in C++ so that's why
we use it so we can look so we don't
have to deal with C++ that's why we had
the SDK but under the hood the SDK talks
to the NDK which is written in C++ so
what happens is you see these two arrows
so this arrow arrow one takes the image
the bitmap image at the user draws and
send it to this potential flow j'ni dot
CC class which is like the main tend to
close a wrapper for Android written in
C++
it takes that image its and it it it
turns it into a tensor so it resizes it
into a tensor gives it to the model of
the pre-trained model which is a
protobuf file which we'll talk about
more but is a pre trained convolutional
network and it's going to output the
prediction which is a tensor back to
that C+ C++ file and then that C++ file
is going to return a list of probability
values in the form of an array straight
to our sdk that is our java class and
we'll talk about all of these classes in
order from the highest level to the
lowest level
but that's that's essentially what it's
doing here okay so that's the example
that I'm going to use but there are
several examples of how Tangipahoa can
work can work with Android so this one
is probably the most well-documented one
that I've seen but it's called Android
1000 machine learning example basically
it is a general purpose classifier so if
you have a device you can plug your
device into your computer with Android
studio and then in real time is just
going to classify everything that you
look at right so like this wallet
you know 29% wallet that's a pretty bad
classification 29% wallet but hey 20%
ballpoint pen and the reason it's so
well documented is because it comes with
an Associated blog post which they've
got all of these pointers to work with
Basel and you know how to set up your
workspace so that it's pointing to the
right version of the SDK and the NDK so
yeah all that stuff but the reason I'm
not doing that is because it is more
complicated than what we're going to do
but that's a good next step to go to
after you do this okay so that's the
image classifier example there's another
classifier which is a little more
detailed so it does the same exact thing
as the first but once it's classify the
image it then queries both Wikipedia and
Wolfram Alpha for some more data about
whatever object it is so classify an
image like let's say at the lemon and
then use the lemon as a parameter when
it makes an HTTP request to the server
and then returns a short paragraph
description of what a lemon is so you
can see how this can be pretty useful
for now we're going to need the apps
that are actually useful to day-to-day
life right so there's that and there's
one more that I want to talk about
they're all really classifiers because
you know you have the camera on the
device so why not just classify
everything there's so many use cases for
classification that haven't even been
attempted before it's such a bit such a
broad application right so this is guest
sketch and there's actually a web app
version of this too but you would draw
something and then it would classify
whatever it is
so obviously you need a model that's
trained on a lot of different images not
just M&amp;amp;I as tea but like a lot of
different images so that's when you do
something like this
inception would be a great model to use
for this pre train inception I love
having named at inception even though it
has nothing to do with the movie
but it works like a dream so okay that
was anyway where were we okay so we're
talking about installation so that's
what we're first going to talk about so
I'm again remember I'm assuming you
don't you've never used Android before
so if you have used Android before then
just skip ahead like I'm going to
estimate four minutes but okay so the
first step is to download Android studio
so Android studio has everything you
need so you know there are several
things that you could use with Android
you can use eclipse the IDE you can use
Android studio or you could just use
this with terminal and a text editor but
I'm saying that you should use Android
studio because whereas before it was
super buggy like when I was using it a
lot a year and a half ago now I just
downloaded it and I've been looking
through it a lot it's actually really
really good now like that it takes care
of a lot of dependency issues for you
inside of the IDE which is amazing
so you can download it from here it's
got its it's available for all the
operating systems and so these three
steps are the next three steps and
actually if you're using Android studio
you can do it all inside of Android
studio you don't actually have to do
this from command line this is if you're
using you know Eclipse or terminal and a
raw text editor like sublime or
something but you would download the SDK
and then you download the NDK okay as
well as the build tools and once you
have those you would point those to the
right you would point your workspace to
those to the location of both so your
app knows where to where to find these
repositories okay so that's what we've
got there and so in my Gradle file I've
got this you know the card SDK version
that I so that I know where that's going
to be okay and so let's see what else we
got here two properties local properties
Gradle is the build system for Android
by the way you see use maven but now
uses Gradle so yeah that's those are the
dependencies that we would use and then
once you've downloaded these
dependencies then you want to train them
up right so remember we train the model
on a desktop or on a server and we do
that in Python
just like you would even if you weren't
using Android you would first train the
model there so let's go over that what
the Python file looks like at a high
level and then we'll keep going with the
Android app and the code
The Associated code so for the model in
Python let's see what we got here this
is what the model looks like so this is
a tear-off version there are two
versions we've got the Kaos version and
a tensorflow version I'm going to go
through the Kaos version really quickly
okay so we're going to import arrows and
Python okay so once we have those we're
going to build a convolutional Network
write a convolutional net is used to
detect images write convolutional nets
are used for any kind of image
classification or even generation we can
use convolutional nets for image
generation we just reverse the way
things work like lose use gradient
ascent or we would slice off the last
tab and then add some kind of add a
stochastic node in there so it generates
some novel data we would add a random
variable in C variational autoencoders
for how that works but you can generate
new data types and also obviously
generative adversarial networks which I
have made videos on both of those topics
see my Udacity deep learning in a degree
course all on YouTube okay so a couple
of methods here the first one is to load
the data and this is just you know
standard reshaping we have to remember
we have to vectorize our input data what
do I mean by vectorize I mean we need to
take our image raw image and then
convert it into a tensor format so that
we can feed it into our model and so in
this case it is a 4 dimensional tensor
right so the length width the size and
then the the depth which is which is
going to be 1 right because there's only
a single layer image so we reshape it
using numpy native reshaping functions
if they float 32 so these are float 32
these are 32 bits pixel values for an
image and add 255 possible and then we
it's got out of 255 possible RGB values
and so once we then once we have that
vector rise we'll split it into a
training and a testing set and that's it
for load data and then for our model
great thing about curraghs is it's only
a few lines and we can build a
convolutional network just like that
it's going to be a sequential model with
convolutional blocks so we do a perform
convolution which is essentially like
taking a flashlight over a filter of
images and looking for what sticks out
so in this case it would be the MST the
the actual digits right because they're
black on white background so it would
detect all that and then we perform
pooling so pooling is an operation where
we look at an image like an array so
let's say it you know we've got like
four different we've got a square where
you know each of the values in the array
represent then we for max pooling we
would take the maximum value from that
square so if let's say we got like four
nine twelve fifteen we would take that
maximum value which is going to be a
fifteen so the next value and that's you
know opposed to opposed to average
pooling or one of those this is just a
more often used pooling strategy okay so
that I actually already explained it so
I don't put a go to that
okay so we do pulling and then we do it
again we again for call convolution what
this is doing is every time it's
creating more images and smaller
versions of that image right and we keep
doing that until we're at the very end
and now we've got this very high
dimensional four by four by 256
dimension tensor but we need an output
probability right we need we need a
prediction value or a set of prediction
values for the probable classes
multi-class classification so what will
we do well we have to flatten that
tensor into a one dimensional array
which is the list of probabilities so
we'll we'll hit we'll flatten it and
then we'll add two fully connected
layers - which are the dense layers and
the last activation will be a soft max
function which is going to output a
single value for each of the class
outputs okay so that's it for the model
and then yeah
okay thanks Internet okay so that's it
for the building the model part and then
for training with terrorize remember
it's just two lines we compiled the
model and by compile we define two main
things we define the loss function that
we're going to minimize because it's a
multi-class classification problem we're
using categorical cross entropy and
because it's an and what are we going to
use to actually minimize our loss or you
we're going to use an optimizer a form
of gradient descent which I have a video
on called evolution of gradient descent
and we're going to use a de Delta which
is an adaptive learning rate
grading descent strategy okay and so
once we have that then we're going to
actually train the model which is which
we do with one line a fit function so
intensive flow this would be the akin to
defining a session defining the graph
and then iterating through you know
defining the for loop where we iterate
through all the batches here we just
defined a number of batches as a
parameter the number of epochs who do it
all in one line and then the training
data as the input okay and so that's the
first key part is training and so the
other key part is exporting the model so
once we trained it we have to export it
so we have to export it into a protobuf
file right a PD file the PD file is the
saved weight file that our model lives
on that that is our train our trained
model right so once we try it on a model
and we save it as a profile so we can
then use it later and what is it part of
us file well it is a serialized version
of our model right with all the learned
weights and so we do that in this method
right and this is the way to do it for
Android okay so well actually this can
apply to any type of use case but it is
a general-purpose saving function right
so definitely save this function for
later use because you're going to use
this a lot whether you want to use your
model on a server with tensorflow
serving whether you want to use our
Android iOS
I know iOS has the you know the new what
was it called an elk or m/l which is
really cool and I'll talk about that as
well but Android is more used so I'm
going to talk about Android first so
right so
right the graph okay we write it and
when we the right graph function is
going to help us define what we're going
to call it and then we do the actual
saving and then we freeze it and what do
I mean by free well freezing means we're
going to save our weights wherever they
are so our weights are at a certain you
know they're all a certain they all have
certain numerical values right so we're
going to freeze them at those values
that means it's essentially like saying
declare these weights as constant right
so you can't change this they're
immutable variables and then
so once the weights are saved we're
going to open it again and then we save
it using its optimized for inference
function which saves it as float32
values which are the type of values we
need for Android devices so that's why
we save it open it and then resave it
and then we'll convert it into float 32
and then reshape it so we do it twice
okay and we on the second time we save
it we we have it serialized to string
values okay so that's it for that
and then our main function that's what
we call all these functions that we've
just defined we make sure that there is
no outs folder and if there is if
there's not then we create one we load
up our data save it in these variables
build a model train it on the data that
we just load it up and then export that
train model and then export that train
model using the pen turbos native Sailor
function and the model as parameters to
the out directory and then we can use
that same model in Android in our
Android app okay so that's it for that
part and so we've got that part and now
we're going to look at this part okay so
let's assume we've trained it we pray
our model it's saved now what do we do
well we've downloaded our SDK and our
NDK we need one more thing we need
something tensor for specific right the
SDK and the NDK are just a part of
Android they have nothing to do with
tensor flow remember up here it's this
this part right here with a tender flow
underscore j'ni dot C see that is what
Google may Google made this themselves
so that we can use tension flow
functions on Android on device so that's
what we need to download in the way that
we download this it's to download an aar
file that is an Android or cause our
archive file so what is it this is a way
of archiving the dependencies all those
that we need into a convenience a little
wrapper like one file that we have to
look at but under the hood inside of
this file or all of the C++ files but we
just don't have to look at them if it
wasn't wrapped like this then we would
have to look at all the C++ files and we
would use a tool like basil to build all
of them from source but the great thing
about aar is is that it is compatible
with the Gradle build system that it's
built that is made for Android so we can
just download it and then import it into
Android studio and it's going to know
all those files and it's going to put
them in the right directories for us so
there's a lot of great magic happening
inside of Android studio which is
actually very useful right it saves time
and energy and I've linked to a tutorial
on how to import an aar file which is
very similar to a jar file so it made it
we would use a lot more we would use
jars a lot a lot more but in Gradle we
use a our files a lot more so yeah check
out this tutorial on how to do that it's
very simple ok so once we've done that
we've got our 10 support specific
dependencies we've got our SDK our NDK
we've downloaded this repository once
we've done that we could just literally
compile it and run it on the simulator
or the device if you have one one of the
reasons I chose this specific repository
to demo is because it doesn't require a
device right not everyone has a device
you can do this in the simulator because
all of these classifier examples that I
talked about like this one and and and
this one and this one required you to
have a camera and you can't use a camera
on a simulator right you need the actual
device it would be cool if you could you
know use a webcam and but you can't so
this is the most accessible repository
that I could find okay so let's go over
the steps in this tutorial so what are
we going to do here we've talked about
the Python file and how that
works now let's go into the Android
specific code so we're going to go into
Android studio and go from a high level
to a low level so we'll go we'll go down
to paths
the first task we'll go down is the
actual visualization path so we'll start
at the main activity and then we'll
iteratively go down the chain of
dependencies draw model draw render and
then draw view which is the lowest level
of what's happening and that's the draw
specific code and then we'll go over the
machine learning specific code so that
is a tentacle classification class and
the classifier class which it calls as
the lowest level the classification
class okay so once we've done that then
we'll be able to compile and run our
model so let's let's start let's start
off here right so here's the model link
to it in is in the description check it
out if you haven't yet okay so let's see
here we have this main activity so and
by the way I have commented every single
line in this code so there is no excuse
to not learn how this code works okay
even if you don't listen to this video
check out the link every single line is
commented very well okay even the
dependencies so what we're going to do
is in the main activity we're going to
we're going to define the main activity
so that that sounds counter to it we're
going to define the only view that the
user deals with right which is this one
in the XML file right this is it right
here so the user will in this box right
here will draw with their finger the
number and then he'll hit they'll hit
the detect button and then in the text
you see the classes here we've got we've
got three classes we've got to draw view
that's where the user draws the number
and then we've got this button this
Clear button which will which will clear
the canvas if we want we've got this
detect button which is going to perform
inference so it's going to take that
image as input send it to our tensor
flow model through the enter flow
as opposed to the Android NDK which can
access the tender flow C+ C++ file on
device it's going to return the output
and it's going to return that output
it's going to output it inside of this
textview okay so that's that's like what
the visualization looks like it's a one
view app very simple and so and here are
our train models by the way right this
we've got two train models and not just
the Train models we have a set of labels
write zeros or nine so that we know what
are the possible class labels because
this is a multi-class classification
problem and so we've got two train
models one for tear-offs and one for
tension flow and we'll be using the
chaos one just because why not but the
pentacle one would probably will be more
accurate but it takes longer to Train
okay so let's go into this main activity
and look at what the what is happening
here right so we're going to import a
bunch of drawing classes by the way the
activity is how the user interacts with
the android architecture with the
Android app almost all activities
interact with the user so the activity
class take care of creating a window for
you which you can place your UI which in
which you can place your UI with the set
content view when we initialize it in
the oncreate function right right here
okay so you know if the more complex
your app is the more activities that you
would have we've got one activity and
that activity is drawing the number and
hitting the text or hitting clear and
repeating that that's the only activity
and in this activity all those actions
are encapsulated so we've got that and
then we've got a bunch of other
dependencies each of them is going to be
used for classification as well as
drawing elements right so it split
basically the dependencies either draw
what we're looking for or let me make
this bigger
can I make it bigger yes yes I can you
can do anything that you dream unless
it's crazy well even then you could with
the right amount of computation and
training okay okay so let's see okay so
now let's look at the main activity so
for the main activity we've got our UI
elements right those buttons that I just
showed you as well as the views right
we've got a set of views we've got the
UI elements and we've got our
coordinates so let's talk about each of
these in order okay so the pixel width
is the width of our image 28 by 28
pixels the button alright we've got two
buttons the class button is the detect
button the Clear button is the Clear
button the text view is what output
shows the output and the M classifiers
is an ArrayList which is an array with
some you know it's Mitchell II built on
top of the array class it's a list of so
you can perform you know getters and
setters on an array and so it's a list
of classifiers that we're going to use
so the reason it's a list is because
we're going to use two classifiers in
this in this repository not just one
we're going to use two one for tens of
low and one for careless so we can
compare the results right so that's why
it's an ArrayList and then we've got our
views right so we've got our draw view
which is the big view and we've got our
draw model which is the smaller version
where the user actually draws and then
we've got our set up points and these
are coordinates that the user is is
tapping that the user taps okay
and then we've got the private set of
coordinates which is like a copy of
those but internally that we can then
modify inside of this class but they're
not exposed publicly so other classes
can't interact with them and this is
just good programming practice to to
have you're about to have your variables
that are not going to interact with
other classes set them to private
because you know otherwise things can
happen that you don't expect where
they're calling a class but it's not and
then it's going to modify some variables
but because you haven't set it to
private it's going to yeah because you
haven't set some variables to private
it's going to modify the variable you
don't want that to happen so yet they've
set them to private okay so we've got
that and now
in our on create method which is like
the basic building block for an activity
class it's going to run only once for
the entire lifecycle of the activity
right we've got a bunch of these
activity specific lifecycle methods but
the first one is on create and it just
runs once okay so we've got that and so
now let's go ahead and create an
instance for this for this main activity
and once we have that we're going to say
okay so let's get that drawing view so
we define this in XML s we're going to
call it by its ID that we define an XML
store in drawing do the same for the
model and then we're going to set the
model so we're going to take that view
and set its model to that draw model so
we're basically encapsulating it inside
of that view and we're going to create a
non-touch listener to activate whenever
the user taps it and then we're also
going to do the same for the Clear
button as well as the detect button okay
and then we're going to do it for the
view so this is this is just us wiring
our XML to our class our programmatic
class so we can then manipulate these
values and then we've got a couple of
functions here we've got our onresume
function which is you know assuming that
the user has guns gone at it gone out of
the app it's going to call this and call
these methods which basically it's a way
of saving our state and making sure that
our app is in crash and these are just
basically like they're like exceptions
they're like Android wide exception
rules so it's like saving where we are
and so we've got that and now let's load
our model so this is going to create a
model object in memory using the saved
tens upload protobuf model file which
contains all the learn weights so I'm
going to take these weights and it's
what you save them in to this M
classifiers ArrayList array so we can
then use them so it takes those saved
weights in local storage on Android
device and saved them into memory so
it's an in-memory ArrayList that we can
then perform inference with and so we
have a try-catch function for that
because if they're not there that we
have to say hey there's an error
initializing these classifiers ok so
then we've got this onclick method so
this is whatever the user clicks on
anything so we have a bunch of if-then
statement to define where if the user
clicks what happens so if the user
clicks the Clear button then we have to
clear the model reset it
and invalidate any new entries the user
does and then we set the text to empty
like nothing is there anymore now if the
user clicks the classify button then
we're going to take the array of pixels
right so we get the pixel data of
everything the user is drawn up to that
point and we sort in a pixel array and
then we say okay we'll initialize the
string is empty because we're later
going to fill it with the output
prediction and we're going to say okay
so for each of the classifiers for both
tangible and kiosks recognized and so
I'll show you what the recognized
function does but recognize or classify
what those set of pixels are you know a
0 a 1 a 2 or 3 what what is this what
did the user draw and then take that
label and if it's empty then I'll put in
a question mark it's not empty Dan
output the label and set that into the
textview okay so that's the highest
level of what is happening and so now
we've got this on touch method which
when the user moves their finger draw a
line accordingly in that direction okay
so so we take the action and we store it
as an integer and the actions all have
predefined in in selected memory or not
in memory but that are part of Android
so check this out so if the action that
the user has so basically if the user
has touched the screen which we defined
with motion events action down which is
a zero see these are all predefined in
Android SDK so basically if the user has
touched the screen again drawing the
line but if a user has moved the user
has started moving then start then start
actually drawing the lines in a
different direction so touching it just
initializes the line drawing and moving
it actually be dense drawing the line
more than just the dot and then if it's
finger is lifted action up then stop
drawing okay so then these are the
implementations of these functions that
we just find the price touchdown as well
as the press touch move so for the press
touchdown we get the coordinates of
wherever the user had touched calculate
those positions in memory and then start
drawing the line using what we were
before and where we are now and then in
this main drawing function this is when
the users
removing their finger take both of those
sets of coordinates or who were before
and where we are now and then draw a
line between those two points and keep
doing that every time okay so yeah and
then press such up we just take draw
models and line function which is going
to help us stop drawing the line okay so
that's the main activity and so notice
how we had this draw model function that
is continuously used all over the place
so let's look at what's happening there
right what is happening in this draw
model function well in the draw model
function we are
okay so we are drawing the model that's
what we're doing the draw model function
so it's a collection of getter and
setter functions that we can use later
on to draw a character model so we'll
initialize it with a line elements which
is a it's just to value the X&amp;amp;Y
coordinates and then we'll have an
internal representation as well to
manipulate those values so we'll
manipulate them and it will return those
manipulated values back to the public X
Y values so then we can then draw them
on XML in the XML file so for a single
lines we have a private line class a
line consists of elements okay so we
have a model a model consists of line
and a line consists of elements okay so
what is that so let me talk about each
of these an element is the line from
point A to point B right so there are a
bunch of elements in a single line a
line is everything that you draw from
when you start to when you finish so
align could be an entire number or it
could be like you know you draw the
first oh that's a line and then you draw
the rest of the you know if you're
trying to draw a six-course you're on
the O and then you draw the you know the
curve both of those are two lines and
inside each of those lines are set of
elements right for point A to point B so
that's and then all of that is a model
okay the draw model
okay the model line element that's the
hierarchy of what it looks like object
hierarchy okay so we've got that and so
now let's talk about these other
functions so this is our current line
our width and height of that line height
of that line and then with a list of
lines right so inside of a model there
are lines and it's on line there are
elements so we're defining arrays for
each of these objects and so we've got a
set of getter and setter functions for
each of them okay so start line and line
add line elements at the sides of it get
the line and then clear okay so that's
it for our draw model and then we've got
draw render so we're going down even
lower level so we're going down the
rabbit hole of lines okay
so for our draw renderer class we're
going to say let's draw all these lines
to a canvas so we've got our render
model and so this is just straight up
drawing function so this is the straight
up drawing function so all of those
functions that we define in this main
activity as well as the draw model they
are high-level functions they're not
actually drawing them to XML this is how
we're programmatically deciding what
drawing looks like this is how we decide
what drawing looks like but then in this
draw render function we take all that
programmatic logic and we apply it to
XML so we're going to directly
manipulate XML elements in this class so
we're essentially drawing lines on
canvas so we'll first set this set
anti-alias functions are true because we
want to minimize distortion artifacts
this is just good practice to do and
then we get the size little line to draw
and then given that side we create a for
loop so get the whole line from the
model object set it color to black and
then get the first of many lines that
make up the overall line so we get the
element size and we say that's a single
element and if the element size is less
than 1 that means that the user hasn't
drawn anything so just skip it but if it
is if the user has then store that in an
element object if the coordinates of the
of that element object the x and y
coordinates and so for each coordinate
in the line so for all of the elements
in the line we have a set of elements in
an array right for a single line then
each of those elements and the
coordinates and then draw them on the
canvas using where we were before where
we are now and then this paint object
which are going to create make those
make that drawing black and then we and
then we make the Nuala then we make the
new coordinates the old coordinates in
this last next and last Y function and
so that's draw renderer and now go to
the lowest level for drawing which is
draw view
so for draw of you I mean this is almost
like unnecessary to go over because it's
so low level because just one of those
classes that you should just import and
like not even worry about because
there's not really any machine learning
specific knowledge here but I'll go over
anyway just set a high level we want to
first reset the view so it's empty -
empty the drawing so we'll hit the
reset function which is going to take
that bitmap image of whatever the user
dropped through and then empty empty out
all the pixel values so they're not
black so they're white again and then
this is the setup function it's a
private function where we define the
view size the size of the model bitmap
and then scale it so it's properly
scaled for whatever android device that
you're on again this would be automatic
like iOS would do this automatically but
and when the user begins drawing
initialize the model renderer class and
draw it on the canvas so this is where
we this is where we initialize that draw
renderer class that we just talked about
this is where we take it and we we apply
it and so then we draw it whatever
whatever it drew whatever expense to
draw we we then draw it and then we cap
so the rest of these are basically
calculating the position of the finger
on the screen so very very detailed XY
coordinates of where your finger is on
the screen and then we draw the canvas
using the bitmap which is an image that
we just drew we release it which is just
memory memory management so we release
it from memory
using this recycling function because
Android uses garbage collection which is
a type of memory management technique so
Apple uses retain cycles and Android
uses garbage collection two different
methods garbage collection is happening
on refreshing my mobile knowledge here
garbage collection is happening at
runtime whereas automatic reference
counting on iOS is happening at compile
time
that's difference so which one better
there are pros and cons to both that's a
dozen scope of this is about machine
learning okay but if you guys want more
let me know of this stuff because I've
done the little development quite a bit
before okay so so then we get the pickle
data and then we pretend to close input
so this is how we define the pic you
know every single pixel we have a for
loop or a we're going to every single
pixel and we're storing in that pixel
array so for all those other functions
we can use that pixel array for both
draw model draw renderer and even this
dump this function drop drop
okay so that's for all of the view logic
and now let's talk about the machine
learning specific logic so it's are at
the highest level which is our
tensorflow classification class then our
classifier and enter classification
class okay so for tentacle
classification we'll say how does this
work okay so bunch of Android native
native Android a bunch of standard
Android development chips as you know
SDK functionality the thing that is
being imported here and then we've got
this interface so remember that C++ file
that I talked about right here this is
what we imported okay so this is coming
directly from Google this is what they
wrote as the bridge between tensorflow
C++ and Android we import it right here
made by Google okay and so what we're
going to do is we're going to implement
this class so this class that we're
going to create is an implementation of
classifier wait what is classifiers
classifier is the public interface or
how our app interacts with the
classifier okay so that so publicly we
give it a string so whatever we name it
as well as the recognizer function
that'd be one public function that we
are making visible to all the rest of
the app the recognizer function we're
just going to take the pixel array as
input of the image and in return the
classification like the output this is a
zero twenty six percent chance zero you
know for words one hundred minus twenty
six seventy four percent chance nonzero
or 74 percent percent chance one or
whatever it is so yeah so it's an
implementation of that so that's the
interface for this class okay so what
we're going to do is we're going to say
okay so this is all this is the
threshold that we defined so it only
returns if it's at least this confidence
there's got to be at least ten percent
confidence of what it's looking at and
if it's not don't return anything it's
got to be at least ten percent confident
of whatever output probability that it's
predicting and so we have a set of
variables here that we're going to talk
about when we implement them so let's
just go through each of these functions
the first function is a read labels
function so given a save drawn model
let's read all the classification labels
that are stored and write them to our
in-memory labels list so remember we
have this classification where is it
labels file right here you know so this
is what we read it into memory so into
memory we say so for all those labels
initialize them as an array lists read
them Adam Toren our array and return
those labels that's what that does
so this is the this is the second most
important function this this recognizes
the first most important but it's the
second most important and so what this
does is given a model okay so given a
model and its label file and its
metadata let's go out a classifier
object with all the necessary made it
metadata including the output prediction
right so these are all the attributes of
the model that we can basically get and
set later on so we've got a name that we
define we've got the input a name for
the input so okay so we've got a name
for the model we have a name for input
data whoever name for an output
prediction we have our labels that we
just read right from from storage into
memory and we have the models path that
we and that's where that's where all the
raw assets asset files are as was where
the model is and so that's what it's
tend to flow in a inner inference
interface comes into play right that
comes from Google look at this all this
stuff this is how we take data and feed
it to the model this is the gateway this
class it's tender flow inference
interface class is the gateway between
androids Java functionality and tension
close C++ functionality made by Google
and so this is where we define what that
is and we'll set it to the TF helper
attribute and so how big is the input
well we'll define that will pre allocate
a buffer for the for all those outputs
that come out right there's not going to
be just one output this is a multi-class
classification problem wave a set of
outputs so store them all in an
ArrayList
okay so then so that's the second most
important class and then remember this
public class or just exposing the name
the string that we define and this is
the first most important not class
function this is the first most
important function class function do i
touch English I'm just switching between
languages and my mind is too like a
little bit over
okay so then for our recognized class
which is the most boring class here's
how it works using the interface we give
it the input name the raw pixels from
the drawing and the input size so we
feed that into our tensorflow
interface right that class and then we
check the probability values so we say
okay if we want to keep those
probabilities if we want to keep those
probability values in memory which is
why the boolean then feed it that those
probabilities as an array of
probabilities which we do which are so
we're going to set it to true and then
we get two possible outputs so when we
run it we we set these output names as
the parameter so it's going to fill
those output values so then we get the
possible outputs using the names which
are here and this is where we pre
allocate the buffer so that so then we
get those possible outputs via the fetch
method so we first run using those
output names and we fetch it okay so we
take the actual output and the output
names which is the list that we fill
with the output whatever the out
prediction is we fill it in to that
output names list and then we find a
best classification from those outputs
so we say let's initialize our
classification object and we say find
the best classification for each of the
output predictions if it's above the
threshold that we define for accuracy
write it out to the view in that
textview so say okay for the length of
each output print out the output print
out the labels just for us for debugging
and if the output is greater than the
threshold at the prediction value of the
output is greater than the threshold
okay and it's greater than the
yeah the output and it's great on the
output prediction that we define inside
of that classification object then
update the object with the new output
and its label and then we can write that
to memory well first we'll return it and
then we'll write it to memory when we
call it okay so yeah so we talked about
our classifier in our application class
okay so one more thing right so the
classification class itself we've got
two variables comp is the output
prediction and label is the input label
and so we define the threshold here you
know 1.0 which is 10% the minimum
threshold and then the label which is
null and the only public facing function
is this update function oh and the get
label and the get calm so we've two data
functions and then an update function
which is how we update it which we call
right here right and that's it so yeah
if you have an android device you can
easily deploy to that right just compile
it and then pick one also I want to talk
about one more thing when would you use
a model on your device versus on the
server you would use it on the server if
you want to constantly deploy a new
version of a model so you'd use
tentacled serving right the con to that
right you could the content that is and
you would need your mobile device to
constantly make HTTP requests so it
always have to be connected to the
Internet what you could do is you could
have it both ways so the best of both
worlds would be to have an inference
model on the device which is static
right you can just update it on to
deploy a new version of the app so user
would have to update yep but you could
have a version on the app and you can
have a version on the server that is
constantly updated but you can set some
frequency to check and then update the
model locally using the server so you
should have the best of both worlds okay
so let me answer two questions to end
this and then we're done we're done so
the two questions are can you do a video
that's like Siri like how to code the
Siri product so I could say what Apple
is they don't really open source
anything right so we don't actually know
what they're our machine learning like
how it works I'm going to bet though
that it's okay like it's not that goes
comparatively
because in this day and age if you want
respect if you want credibility the name
of the game is to publish and publish
often and Apple doesn't do that but how
to Siri work I'm going to guess that's
going to use a set of predefined
responses on a server making HTTP
requests
it's barely using any machine learning
maybe it's using machine learning just
to well it is using machine learning to
detect your speech but I don't think is
using machine learning to generate
responses if it is I'd be very surprised
yeah so that's the first question and
then one more question is how to train
feedback and get their respective rating
to predict the rating of new feedback so
that's a supervised classification
problem right you've got some text and
you got a label and then you want to
learn the mapping between the two and so
how do you learn the mapping between two
like a label and an input supervised
classification right this is general
supervised classification the way you do
this for text is using an ell FTM
network which is a type of recurring
that made for word or for text data cool
please subscribe for more programming
videos for now I've got to create a new
series so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>