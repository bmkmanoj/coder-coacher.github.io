<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Random Forests - The Math of Intelligence (Week 6) | Coder Coacher - Coaching Coders</title><meta content="Random Forests - The Math of Intelligence (Week 6) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Random Forests - The Math of Intelligence (Week 6)</b></h2><h5 class="post__date">2017-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QHOazyP-YlM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and how risky is
your credit that is the question that we
are answering today by looking at this
credit risk data set it is a German
credit risk data set we want to know
based on your employment history based
on your family history your in you know
your income are you at risk for not
paying back your loan usually we have
people who do this and it takes a long
time and you have to there's a bunch of
biases built into the system you know
you meet the person whether you're an
insurance agent or you work for some
bank and you're trying to assess whether
or not this person deserves a loan as
humans we have a lot of biases and these
biases don't necessarily add real value
to whether or not this person deserves a
loan or not right so the way to fix that
is to let machines do the job because
machines can find relations and data
that we can't so the data set we're
going to look at is a bunch of financial
attributes of somebody the status of
their existing checking account this is
a German data set by the way and I found
it on the UCI a website also a that is
just a great website to find data sets
on so definitely check that website out
if you haven't already we're going to
look at their credit history the
duration of payments they've made the
purchases they made the cars furniture
all of these things are features right
they're all features and we can use that
to assess whether or not this person has
is that risk for is that risk for not
paying back their loans this is used in
a bunch of different fields insurance of
Finance whether or not to rent a house
to somebody right the landlord assesses
whether or not you can pay pay it back
savings account that whether or not
they're employed all these features and
the label is this is a history of credit
risk so there's also label based on all
these features this person has already
been assessed as at risk or not at risk
right so that's that's the mapping that
we're going to learn right it's a binary
mapping and the way we're going to do
that is by building a random forest
that's really the goal here to learn
about random for it and yeah so
basically what it would look like it's
something like this
like this picture that we're looking at
right now eventually once we built this
random forest which consists of several
what's what are called decision trees it
will look like this we'll feed it a new
data point and then it will iteratively
ask a series of questions like is their
checking account balance above 200 or
below 200 and then based on that it will
ask another set of questions like if you
say no if the answer is no based on that
data point it'll say well what's the
length of their current employment and
then are they credible or not creditable
right so it'll just keep going down to
this this chain of decisions a decision
tree okay so that's what we're going to
build we're going to build a random
forest so what is a random forest well a
random forest is a collection of
decision trees so let's talk about what
a decision tree is so a decision tree is
actually a relatively simple concept and
so I just didn't even talk about that in
this series so far I'm just going
straight into random forest because
decision trees are easy stuff we want to
get right into the random forest part
right but let's go over decision trees
really quickly and so a decision tree is
basically a set of decisions on whether
or not to classify something the
technical name for this is the
classification and regression tree or
court and it was invented by a dude
named leo breiman a couple decades ago
like two decades ago and it can be used
for both as the name suggests
classification and regression both of
those things but we're going to use it
for classification because that is that
is what we're trying to do or we're
trying to classify whether or not
someone is creditable or not creditable
okay so how does this thing work well
you have a set of features right let's
say this features are the temperature
the wind speed and the air pressure and
based on these three features we want to
classify whether or not it's rainy well
what the decision tree that we build is
going to do is it's going to create
iteratively
iterative ly I should say recursively
actually it's going to recurse the tree
itself is built recursively you'll see
what I mean want to only look at the
code
but the experience elf is built
recursively and for all of these
features they will ask a series of
questions until it classifies it as
raining or not raining so the real
question is how do we build this thing
what is how do we build the optimal tree
where it is asking the right threshold
values like how does it know that the
temperature should be greater than or
less than 70 degrees Fahrenheit and then
based on that answer how does it know
whether the wind speed should be greater
than 4.5 specifically where are these
magic numbers coming from well they're
coming from the genie index that's what
it's called it's called a genie index
genie in a bottle genie in a bottle not
genie in a bottle GE is the Italian
genie so it was some dude named genie I
actually don't know we're not going to
get into that anyway let's talk about
the genie index so the genie index is
the loss function here but the
difference here between what we've done
before with gradient descent with
Newton's method is that there is no
convex optimization happening here we're
not trying to find the minimum of some
convex function there is no error that
we're trying to minimize the genie index
is a cost function that works
differently and here's how it works
basically for every single point in our
data set right we've got a bunch of
different features we want to find that
ideal threshold value and so I'm going
to explain this once and then explain it
again when we get into the code but we
want to find that ideal feature all
right what that ideal value for a
feature okay so what do I mean by that
so here's how it works
check this out we've got a data set with
a bunch of features right ten features
right so one let's say let's say one of
them is the income so the income could
be anywhere and I'm going to use USD for
this example it could be anywhere from
$10,000 a year to a million dollars a
year so what the genie index is going to
do is it's going to go through so what
we're going to do is we're going to
iterate through every single data point
for that feature so we're going to say
we're going to iterate through every
single data point and we're going to
compute this genie index which is one
minus the sum of it is no accep so the
the genie index is where is the formula
ohright it is one it is one - the
average times the average where the
average is proportion right one - the
average of all of the class values times
that average and that gives us the Gini
index and so what we want to do is it
comes out to some single value some
scalar value and so basically we use we
start from we start from data point zero
and we go up to data point n where n is
the number of data points and we compute
the index for each of these data points
for specific feature right so let's say
the first data point is 10,000 we'll
compute a Gini index for that data point
for that value for that amount of income
and so what happens is basically it goes
on a scale so a genius core of zero is
the worst case a genies core zero means
that based on that index for all the
other data points they're going to be
they're going to be evenly split between
less than that value and greater than
that value but that's not what we want
ideally we want a Gini score of 1 that
is the ideal Gini score and what that
means is that for that given a value
that given value for that specific
feature all the classes from 1 all the
classes from all the data points from
one class will be on one side of that
threshold value and all the data point
from the other class will be on the
other side of that threshold value that
will give us a 1 value that is the a one
Gini value right and so what we do is we
just compute the Gini index for every
single data point for every single
feature and we just do that for every
feature right away so for let's say for
you know income we'll compute the Gini
index so we'll say okay so let's start
with 10000 we'll compare every other
data point to 10000 and see whether it
lies on the left or right whether it's
greater or less than and that we get we
then we compute the Gini index from that
okay which is this formula right here
and we do that for every single data
point and so what's going to happen is
we're going to have a collection of GD
indices we'll have a set of Gini indices
and then we'll pick the one that that is
the highest and the highest one is the
one such that the data points are most
of our most
not evenly split that means the most
data points from Class A are going to be
on the left or right and the most data
point from the other class will be on
the opposite side you see what I'm
saying and by side I mean greater than
or less than so so the worst genie
the worst case is when the data points
are evenly split we don't want that we
want them to be all on one side and all
on the other side that means that when
we get a new data point it'll plop down
right into it it's a little bucket with
all the rest of its related data points
right so that's the genie index and
there are different measures of loss
when it comes to arithmetic based
machine learning models instead of
matrix matrix operation based machine
learning models like we see with neural
networks okay so that's a Gini score
Gini index whatever you want to call it
so how do we build this decision tree
well it is there are two parts we first
we've got to construct the tree so we
that's a recursive process that you'll
see when to construct the tree and once
we construct the tree then we prune the
tree so that means we identify and
remove the irrelevant branches that
might lead to outliers to increase
classification accuracy so wait a second
you might be asking why are we building
a random forest in the first place why
can't we just build a decision tree
alone well what happens is if you just
build a decision tree that's not fun no
there's a better answer if you just
build a decision tree then your decision
tree could be over fit that is a big
problem when it comes to the decision
trees right the decision tree gets over
fit to the data right it's like it's
like a the boy someone might memorize an
eye chart it's not like they can see it
properly right with one eye closed
they just memorize with the position of
where everything is right in that same
way we don't want to over fit for data
to our data so the way to prevent that
is to create a bunch of decision trees
on random subsamples of the data so
we'll define subset some set of sub
samples and we'll say for each of these
sub samples will create a decision tree
then once we have a bunch of decision
trees that we've trained right then by
train I mean we've computed the genie in
debt
for all of the features and then we've
recursively built the tree it's a binary
tree by the way I didn't mention that
decision trees are binary trees right
they're either a left node or right node
or no node right it's the leaf it's the
last node and so if you haven't reviewed
binary trees I mean we're essentially
building a binary tree right now but if
you want to learn more about data
structures and algorithms or if you're
curious
if you should know data structures and
algorithms for machine learning the
answer is yes for two reasons one just
to the logic sake you need to know how
data is stored because machine learning
isn't just about matrix operations it's
also about storing data right
serializing and storing data in the most
efficient way possible and retrieving it
and if you want to build algorithms you
got to have your basic data structure
and algorithm knowledge intact okay I
just wanted to say that back to this the
way random forests work are each of the
decision trees that are generated then
we'll just once we have a new data point
we'll run it through all of those
decision trees they'll all make a
prediction and then we'll make a
majority vote so it will calculate a
majority vote so each of the votes for
each of the trees whatever is the
majority vote which is the class that is
the class that we're predicting and what
this does is it gives us higher accuracy
than just using a decision tree alone
let me also say that random forests are
one of the most used machine learning
techniques out there they can be because
they can be used for both classification
and regression and therein lies almost
90 what 90 plus percent of problems
right and it also works well for very
small data sets which we tend to have a
lot of and so that's it's so random
forests are just used a lot they're you
so much that how can i how can i how can
I say this they're you so much that the
guy was in Josh Gordon the Google dude
his name on Twitter is random for it so
they are very useful hi Josh if you're
watching this ok back to this the okay
so we're training the we're training it
on subsets of there right one subset per
tree and that is our random forest it's
a forest because it consists of trees as
you
bubbly desk and so if you create a giant
random forest you get Lord of the Rings
Rivendell style no you don't you the
bigger the better generally you'll see
at the end the more trees we add the
better our accuracy score gets right so
yeah and each of our nodes are going to
represent a set of feature splits right
so what's the color green red green ok
what's the size small big big ok that
fruit is a watermelon right so we just
recursively do that are there other good
examples of this you might be asking in
the answer is yes of course there are
other good example stock price
prediction and classification I've got
two great examples here definitely check
them out the documentation is pretty
sparse but the code itself it's not
using any library so definitely check it
out alright so now let's go into the
order of functions that we're going to
follow so we're not going to have time
to write every single function we're not
using any libraries but we will write
the two most important functions split
and get split and that's going to really
take take on the majority of our logic
but that's we're going to do and that
going to be 40 lines of code but for the
rest of it this is the order of
functions that I'm going to follow the
chain of functions so let's just get
right down into what this chain of
functions will look like so I'm going to
first of all look at our dependencies
here so I'm going to import seed from
random and see it is going to generate
pseudo random numbers this is useful for
debugging you want to do it anytime you
have some random numbers and you want to
debug your code in production or
otherwise it's always great to have some
seed so that the random numbers that are
generated start from the same point
every time and so that's just great for
reproducibility of results I'm also
going to import R and range so it's
going to return a randomly selected
element from a range of numbers CSV
because our data set is by the way let's
see our data set our data set is a CSV
file is our data set is a CSV file so
let's open our data set and see what it
looks like
it is the numeric data right here right
so all of it is numeric at the end the
result is either 2 or 1 right it's a
binary level either 2 or 1 and the rest
are like 15 features here we're going to
use every single one of them
no feature selection we're going to use
every single one of these features okay
and it's using arithmetic so that we're
only importing math we're not even
importing numpy we're only importing the
math library okay so let's let's look at
this thing so we've got some really
basic data loader functions here load
CSV initialize the data set as a list
open it as a readable file initialize
the CSV reader and then for every row in
the data set
appended to this data set matrix it's a
2d matrix return it and so we have an
in-memory version of our data we know
that part that's that that's general to
all machine learning really whenever
you're reading a CSV file what else we
have here we have two more helper
methods functions one to convert a
column to an int and want to convert one
to convert a string to an int and one to
convert an into a string and that's if
we have string values so in this case we
don't we have numerical values so we
don't need this okay so let's go into
this order that I was talking about the
order of algorithm so the first so the
first thing we want to look at is this
main code here right so we started off
with the seed so that we always start
with the with the same random numbers we
loaded up our data set right that's CSV
file and then we converted our strings
to integers we don't actually need to do
that but then we said okay so how many
folds do we want to have and bold means
subsamples of data so we want 5 sub
samples what is the max depth and the
depth means how many nodes what is the
depth of the tree right how many levels
of that tree do we want to create so
we're going to say max 10 levels and
these are our hyper parameters we can
tune them we can make them more or less
and we'll have different results they're
kind of like note the number of neurons
in a neural network right and so we say
what's the minimum size what's the
minimum size for each of those nodes
how many features do we have we'll count
all of those as well and then what we're
going to do is we're going to create
three different random forests all right
we're going to create one random forest
with just one tree so it's actually a
decision tree and then one with five
trees and then one with ten trees and
then we'll assess and then we'll assess
how good each of these random ports are
by measuring the number of trees the
accuracy score for each of them okay
that's what we're going to do so notice
that here
is the big boy right here this evaluate
algorithm is that main function that
we're going to use to train our model so
we're going to give evaluate algorithm
our data set we're going to give it the
the random forest model that we built
the number of folds or the sub samples
of the data how big we want to tree to
be the mid size the sample size the
number of trees and the number of
features that we've counted so let's
look at what this evaluates algorithm
function looks like because that's
really the big boy right we want to see
what is going on in this main this big
function right here so what I'll do is I
will go right here okay so what it's
doing is it's going to say this let's
look at okay ready let's look at this
what it's going to do is it's going to
say okay so for a given data set and for
a given algorithm which is the random
forest algorithm that we're going to
feed it let's say the folds are the sub
samples that are used to train and
validate our models so we'll split our
data into a training and a validation
set I'll by the number of folds so what
do I mean by that as well okay let's
look at that cross-validation split
method where is that it's right here so
so we basically want to split the data
into K fold the original sample is
randomly partitioned into k equal sized
sub samples and then of those K sub
samples a single sub sample is retained
as the validation data and the rests are
going to be used for training data it's
splitting the data to that K minus 1 sub
samples are used for training and then
there's one sub sample that's left for
validation that's it okay so back to
this we talked about that function so
once we split our data into those folds
then we're going to say okay let's score
each of them right because we're
evaluating our random forest algorithms
all say for each of the sub samples that
we have of our data let's create a copy
of the data and then remove the given
sub sample what initialize a test set
because this algorithm really does two
things it it trains our model on the
training data and then it tests it out
on the testing data that is it makes
predictions on the testing data ignoring
the labels so we'll say okay
for each so we'll add each row in a
given subsample to the test set so that
we have test samples as well and then
we'll get the predicted label right so
for so we'll use a random forest
algorithm this is a random forest
algorithm that that's the next thing
we're going to look at but it's going to
get all the predicted labels from them
right from our training and our testing
set and then it will get the actual
labels right so and once it has the
predicted labels and it has the actual
labels then it can compare the two via
this accuracy metric and the accuracy
metric is a scalar value and it is how
we assess the validity of every random
force that we've built and so the
accuracy metric to go into this is
really simple for each label if the
actual label matches a predicted label
add one to the correct iterator and then
we just calculate the percentage of
predictions that were correct which is
the number of them divided by the number
of correct divided by the actual one
times 100
really simple like I said it's all
arithmetic it's all plus minus plus
minus multiply and divide that's all
this is there's there's no linear
algebra here it's all algebra so but
despite how simple this model is it is
quite powerful which is why it's awesome
okay so that's our evaluate algorithm
function so let's keep going down right
we're going down the chain go into the
moon moonride moonride moonride moving
my crazy in as well so back to this if
you got that reference to pool if you
didn't get that references from
SpongeBob back to this you're so cool so
back to this so we're evaluating our
algorithm here let's evaluate this thing
so if we're evaluating our algorithm so
what does this algorithm function even
do what what is this right let's let's
see what this algorithm function is what
this algorithm function is is it is our
random forest that is what it is we say
for the range of trees that we have
let's compute a subsample of those trees
and then for that sub sample we'll build
a specific decision tree for that
specific sub sample and then once we
built that tree we'll add it to our
subtrees and then we're going to make
predictions based on all of those trees
and we'll return the list of predictions
right seems simple enough right
so what is this bagging predict now
notice that we just keep going down the
chain right we just keep going down the
chain so this is the list of trees
responsible for making a prediction for
with each decision tree so it combines
the predictions from each decision tree
and we select the most common prediction
the one that comes up the most that that
label that the some of that label is
greater than the rest of the some of the
others right but and there are only two
labels because this is binary
classification you can also do
multi-class classification but we're not
going to talk about that right now okay
so then we talked about bagging predict
so what's next
subsample right what is this the sub
simple function so how are we
subsampling how are we choosing how to
split this data right that's the
question how are we choosing how to do
that well the answer is we are we are
creating a random sub sample and this is
where our randomness comes in right we
are creating a random sub sample in this
random range for the number of samples
in the data and we'll add that list of
sub-samples to this sample array and
then return that so the sample array or
list contains all of those samples from
our from our data set right we split
them and so that's four sub sampling and
so now where were we
so we talked about subsampling the
building prepar let's let's see how the
tree itself is built so if we give it
some sub sample the depth and size of
the trees it's too high per parameters
as well as a number of features we
expect this function to build this tree
so let's look at how this how this
function works how is it building the
tree itself well inside of this function
we said we notice that it's it's first
using this method get split which we're
going to code and which is where the
meat of the code goes but we're going to
build a tree and that involves creating
the root node and that's going to get
that split and that this kind of this is
going to output the root node right that
first node and then we'll call the split
function that's going to call itself
recursively to build out build out the
whole tree so once we've got that root
node then we're going to call split
recursively
and so it's just going to continuously
build that tree recursively by calling
itself in case you haven't heard of
recurred
it's when a function calls itself it's
like Inception
except it's recursion yeah wow I never
actually made that reference until I
just said that right inception is
recursion a dream inside a dream whoa
okay back to this um right right so
where were we so we were at split and
get split so let's let's write out get
split right so get split is the first
one that we want to write out this is
going to select the best split point in
a data set right that that key question
how do we know when to split our data in
this decision tree of the many decisions
that we make in our random forest the
answer is we'll have to compute it this
way this is an exhaustive and greedy
algorithm that means it is just going to
go through every single iteration that
it can write there are no heuristics
here there's no educated guesses there
are no educated guesses it's going to go
through every single data point to
compute that split so let's look at what
this looks like well we've got to give
it first of all our data sets of course
and we want to give it the number of
features right because for each of those
features we're going to compute that
split so given the data set we've got to
check every value on each attribute as a
candidate split so we're going to what
we're going to do is we want to say
let's get all of those class values
right and that the set that set of class
values is going to be a list that set of
class saw is going to be a list and it's
going to be a list for every single data
point and so we'll say all of the rows
from our data set are our data points
right we have that we know that we want
to calculate the index the value the
score and the groups and so we'll
initialize all of these has really big
numbers and they will be updated with
time but we'll initialize them as very
big numbers as well okay so check this
out so the Gini index essentially gives
us two things it gives us an index and
it gives us a value the index of the
feature of the feature of the so it
gives us it gives us the index
some feature whose value is the optimal
value to split the data on for that
feature you see what I'm saying it gives
us the index of say for income let's say
that 30,000 is the best feature it is
going to be that decision node from then
we can put everything based on 30,000
that is where the the classes are most
split it's going to give us the index of
30,000 in the in the dataset as well as
the value whatever it is 30,000 right so
that's the pair that the Gini index
gives us the index and the value and it
will also give us a score and the groups
and the groups are the sub samples and
the score is the is it's how good it is
right it's a measure of how good it is
so we all want to initialize our
features here as a list and then we're
going to say okay so while the number
features is less than the number of
features whereas there's going to be 0
and we're going to increase it every
time as we iterate through each of the
features well let's this decide some
random range right some some random
index in our data set to then to append
to our features list so say if the index
is not in the features which it won't be
then a pent at first but eventually it
will well a pendant will append the
index wherever we are to the list of
features that we initialize is empty and
then once we've done that we'll say for
every index in the data set
for each of those indices let's let's go
through every single row in the data set
so we're computing groups here right so
we're computing groups to split our data
into so we're saying the same what we
want to decide the the test values that
we want to split as well as the the Gini
index and so the Gini index is what
we're computing right here this is the
point that we're computing our Gini
index for the current group of data that
we're in right so for 4 we picked a
feature and we're going to get through
everywhere
for that feature we're going to compute
the Gini index for all those values and
we're going to pick the Gini index that
is D that is the largest right and that
is what we're going for and that once we
picked the Gini index at its largest
that will give us the index and value to
then build out that that note of the
decision tree
okay so then we computed that and now
we're going to say okay if the Gini
index is less than the optimal score
then we want to say we're going to
update these values to the new values to
the score the value of the index and the
group's so we're going to use a
dictionary to store the node in the
decision tree by its name so we'll say
return or were we
so we'll say return the index as well as
the value the value and the group's be
groups right so with the index the value
in the group's right because we've
computed all those right so this that
function gives us gives us the root node
right and so once we have the root node
then we can actually perform the
splitting write it down the best
splitting point and so now we want to
recursively compute the splitting itself
so that's where our split function comes
in given that root node how do we build
a tree such that it is split along the
ideal lines okay so let's let's let's
write out this split function okay so
it's going to so basically so this is
the binary tree part if you've if you've
created a binary tree before it is
exactly the same so given some root node
right we're going to say some root node
will compute a left and right leaf for
that node and then we'll delete the
original node so today okay so now we
can delete that and then
once we've done that we can check if
either the left or right group of note
is empty so we're checking if either the
left note or the right note is empty and
if so then we create a terminal node
using the records that we do have here
right so in a terminal node by the way
let's look at what a terminal note is we
select a class value for a group of rows
and then return the most common output
value in that list of rows what is the
most common output value in that list of
rows and that is the most common class
so that's what we're doing we're select
the most common class okay so that's the
first part and so we want to check if
we've reached our maximum depth right so
that that depth is our hyper parameter
is a threshold for how large we want our
trait to be so we'll check if we've if
we've reached that point so we'll say if
the depth is greater than or equal to
the max depth then so if we reached our
max depth then we create a terminal node
that's what that's what that's saying
okay so then we've got two more parts
here all right so the next part is to
say okay so first the two groups of data
that are split by the node we retrieved
them when we store them in the left and
right variables here and then we delete
that node then we check if either the
left or the right group of rows is empty
and if they are we create a terminal
node using the records that we already
have right here and so the terminal node
by the way is where we just select the
class value that's the most-used right
that is the that is the output class the
output class the terminal node is that
what is that what is the what is the
prediction itself right that's the end
point and so then we check it so then we
check it either the left or right group
of rows is empty and if so we create
that Terminal Road so then we check if
we've reached our maximum depths and if
so we create a terminal node and so
that's what this part is and then and so
lastly if the group of rows is too small
will create a terminal node else we'll
add the left node in a depth first
fashion until the bottom of the tree is
reached on this branch will do this so
we'll do the same for the right child as
well and so the right side is then
processed in the same way and then as we
ride back up to construct a tree all the
way back up to the root okay so get
splits notice how gets split is being
called here over and over again two more
functions and then and then we're good
with this the two more functions I had
where the Gini index and so the Gini
index is like I said it is it was that
formula right up here okay this is the
Gini index or Gini score whatever you
want to call it so the Gini index splits
the data set involving one input feature
and one value for that feature write
what the Gini index gives us is remember
that pair that that the value and the
index of some feature some features for
some data points right and that's that's
the line that we then split well that's
the boundary from which we can split
data based on that feature in the future
and so the way we compute that is it
starts off at zero it's some scalar
value and we're computing it for all of
the data points so for each class value
that we have for all of our classes and
we only have two fraudulent or not
fraudulent and we only have two credit
worthy or not credit worthy for each of
those classes we'll select a random
subset of that class we'll compute the
average value for that feature and then
we'll compute P times 1 minus P where P
is the average and that is our Gini
scalar okay and we'll add them all up
together because we have all of those
because it's the sum of all of those
values and that's where the Sigma
notation comes in and we'll return that
as a Gini score okay we compute that for
all of the subsets of our data and so
the last function to show you is the
predict function and the predict
function is right here right so whenever
we're actually making predictions this
is how it works it navigates down the
tree this is it's asking is this person
employed or not with this person go to
school what is this person's social
security number what is this person's
you know just a bunch of random
questions based on the features each of
the features that we have so predict is
recursive so whereas the node is always
changing for a given row the node could
be the left node or the right node so
whether or not the value for some data
point is the
than work greater than some nodes
threshold value that we've computed
using the Gini index it will then update
the node and then use that as a new
parameter to then run predict again and
eventually once it's reach the terminal
node the last node the label it will
return the label and that and we and
then because and that's for one decision
tree and because we have a random forest
it's computing that for every single
decision tree we sum up the values and
we use the one that is the majority vote
and that is our prediction so then if we
test our code will notice that the we've
got our accuracy scores here and so the
accuracy is getting is improving every
time so we've tried it for three
different random forests we we tried we
tried it for one with one decision tree
we tried it for one with five decision
trees and we try to it for one with ten
decision trees and every time the
accuracy accuracy score improved and so
what this means is if we give it a one
hundred one hundred tree random forest
or a thousand tree forest it's going to
do really really well okay so and then
we'll be able to predict whether or not
someone's someone is worthy of getting
their credit assessed or not and if you
made it to the end of this I'm very
happy so thank you and that's all please
subscribe for more programming videos
and for now I've got to do something
random so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>