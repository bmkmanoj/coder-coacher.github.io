<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Dream in TensorFlow - Learn Python for Data Science #5 | Coder Coacher - Coaching Coders</title><meta content="Deep Dream in TensorFlow - Learn Python for Data Science #5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Dream in TensorFlow - Learn Python for Data Science #5</b></h2><h5 class="post__date">2016-11-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MrBzgvUNr4w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj in this video
we're going to create a neural network
that makes images become really trippy
using Python and tensorflow we humans
have been using psychedelic drugs since
prehistoric times that wasn't a normal
mushroom they helped manifest parts of
our mind to conscious experience that
wouldn't normally otherwise one of the
most common experiences from
psychedelics are trippy visuals people
who've taken psychedelics repeatedly
described experiences of seeing both
open eye and closed eye visuals but how
what are these drugs doing in our brain
that makes us see things that aren't
really there the traditional way for us
to find out is to test live human
subjects tripping balls under an fMRI
machine but recently we've gotten
artificial neural networks to do the
same thing google train a neural network
on a labelled data set of images
everything from squirrels to temples and
as a train on these images it built
internal representations at each layer
eventually the first few layers learned
low-level features like lines and edges
and they got progressively more abstract
so the last few layers on
representations or faces and big shapes
so when we visualize one of the
higher-level representations at Bill we
can see that it contains a mixture of
features like the eyes of a dog and the
head of a bird and when they gave it a
novel image and asked not to classify it
but to maximize a similarity between the
image and a representation at a
particular layer the result was a very
trippy image it all sounds very similar
to the drug experience which is insane
our brains are carbon-based and they use
chemical signals as messengers a neural
network doesn't even exist in physical
space at all it's an abstract concept
being represented on binary silicon
transistors there's no reason to expect
that these two systems would develop the
same mechanism for processing visual
information even with existing
similarities natural selection is very
different from using gradient descent to
alter weights of connection between
nodes could it be that somehow encoded
into the fundamental rules of the
universe there is this ideal way of
doing object recognition what is he
doing he's beginning to be you're damn
right Morpheus we're getting closer to
understanding it every day and we can
learn a lot about the brain including
human development
treatment for cognitive disabilities and
drug effects from studying artificial
neural networks after we install our
dependencies we're going to replicate
Google's deep reme code in tensorflow
and then test it out on a novel image
numpy will be used to perform math
calculations the partial sub module of
funk tools lets us create new versions
of functions with one or more arguments
filled in this is good for reusability
which means less code to write pillow is
an imaging library an image will help us
modify our images tensorflow is our
machine learning library URL live will
let us download data from the web os
will let us use operating system
dependent functionality and zip tools
let us run free no it'll just let us
unzip files our imports are already at
the top of our script so we're going to
download Google's pre-trained neural
network create a tensorflow session
Pikul air in the pre train network to
enhance our input image apply our
gradient ascent to that layer repeatedly
and then output our newly deep dreamed
image let's start by downloading
Google's pre-trained neural network
called inception in our main method
we'll store a link to it in the URL
variable create a data directory where
we will extract it to then use the OS
module to retrieve the model name and
create a local zip file path if there is
nothing at that path we can download it
using the URL live module with the URL
variable as a parameter and store it in
the model URL variable we'll open our
zip file with the WB flag so we can
write to it in binary then write the
downloaded data to it then we'll extract
it using the zip file module now we can
create our tensorflow session we'll load
our inception graph file into our model
FN variable then initialize the graph
using the graph function of tensorflow
now we can initialize a session using
that graph we'll open our existing saved
inception graph using the fast G file
function and point it to the saved graph
once we opened it we can read that
graphed and parse it accordingly using
the parts from string method of tensor
floats graph definition module we need
to define our input so we'll create an
input tensor using the placeholder
method called input with the size of 32
bits then we'll define the image net
mean value of pixels in an image as 117
by removing this value from our image it
will help us with feature learning so we
will subtract it from our input penser
and store the value in our pre-processed
variable then
the graph def variable we initialize
with the input as newly processed tensor
so now we've got our tensor flow model
we've downloaded from the web and we've
loaded it into our session as a graph
with a bunch of layers
it's a convolutional neural network the
type of neural net that helps recognize
images let's load all those layers into
an array and store them in our layers
object so for every tensorflow operation
in our graph if it's a convolutional
layer load it into our array so we've
got layers laid layers like yeah we're
balling hard right now I know okay so
each of our convolutional layers
I'll put tens of hundreds of feature
channels to pass data in the graph and
we can collect them all and store them
in the feature nums burial let's print
them out and visualize what we've got in
terminal first we can see our number of
layers and the total number of feature
channels right here let's now pick a
layer from our model that we're going to
enhance we'll pick a lower level layer
and pick one of the feature channels to
visualize it's time to load our input
image using the pillow image sub modules
open method and store it in our image
variable will format it accordingly
using numpy and perform deep dream on it
with our render deep dream function with
a focus on the layer we selected earlier
in our deep dream function we can see a
couple of our predefined hyper
parameters we'll start by defining our
optimization objective which is to
reduce the mean of our input layer the
gradient function lets us compute the
symbolic gradient of our optimized
tensor with respect to our input tensor
now we can split our image into a number
of octaves and say for each octave let's
resize it using numpy and add it to our
array of image octaves then we can
generate details octave by octave by
iterating through each random shifts are
applied to the image to blur tile
boundaries over multiple iterations
using the Cal kraid tiled function we're
essentially applying gradient ascent
here to maximize our loss function which
merges are saved representation in this
layer with our input image more and more
every iteration so to break it down
neural networks are a great test bed for
learning about how the brain functions
and responds to certain stimuli they
store increasingly abstract
representations of what they learn in
their layers and we can create trippy
images by applying a gradient ascent
process to them based on any chosen
layer of a pre trained convolutional
neural network the winner of the coding
challenge from the last video is a V
root chakra for
he created a pretty cool ipython
notebook to demo his code and trained
his neural net on both price and
sentiment data using curraghs badass of
the week and the runner-up is Victor C
Rana very well documented code that asks
for user input on stock prediction the
coding challenge for this video is to
modify this code so it works not just on
images but video details are in the
readme poster get humbling in the
comments and I'll announce the winner in
the next video also I created a slack
channel for all of us programmingwizards
to learn from each other link to sign up
below for now I've got to stay sober so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>