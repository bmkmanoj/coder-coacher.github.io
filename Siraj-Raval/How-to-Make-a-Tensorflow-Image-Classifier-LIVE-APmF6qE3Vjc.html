<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make a Tensorflow Image Classifier (LIVE) | Coder Coacher - Coaching Coders</title><meta content="How to Make a Tensorflow Image Classifier (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>How to Make a Tensorflow Image Classifier (LIVE)</b></h2><h5 class="post__date">2017-02-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/APmF6qE3Vjc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">broadcast is starting
all right what we got here what do we
have here that's what I'm trying to ask
what do we have here we have people in
the house everybody yeah
rocky Oh convolutional network yeah okay
hi everybody welcome to the live stream
today we are going to build a
convolutional neural network and we are
going to build it with just tensorflow
we are not using curraghs we are not
using anything in fact I'm going to take
these off if we're not using anything
I'm not even using my headphone I'm not
even using my hands actually we are just
not skating I'm using my hands but what
we're gonna do hi everybody we're going
to build a convolutional neural network
from just tensorflow and me okay so what
that's all going to do and let me see
who else is in here we got no crafter
and Simon we got Daniel and Anthony we
got look and Samba Dan our RN do hard
mode is on hard mode is on and well we
have so many joke people here not
audible
am I not audible you can't hear me in
that the case cuz I'm gonna just turn
this off
imma just you can hear me right Bonjour
oh we got an international crowd yeah we
got a Shawn we got Paulo we got the
whole world in here guys we have 200 we
have 200 countries represented in our
community that's amazing isn't it so
what we're gonna do is we're gonna build
this and let's start with a five-minute
Q&amp;amp;A so just hit me questions just going
to go into it's I'm excited and I hope
you guys are okay give me with your best
questions one is the Hangout for patrons
it's gonna be in two weeks I'll schedule
it soon thanks for the reminder I know
you from Dan wok one yes I went to high
school with him he's a great guy I'm
doing great thank you you're awesome too
hi from the Netherlands and France and
Italy and India oh my god guys there we
have we have just started this community
and we already have people doing amazing
stuff we had someone who was who gave a
talk on on our somewhere anything in
South Africa or something hit someone
who got their data set featured on the
first first page of Kaggle so we have a
lot of great stuff happening and we're
just getting started so hello from
Greece get wide tensorflow because
tensorflow
is actually faster than numpy you heard
tensorflow is actually faster than numpy
why because it runs on a different
interpreter than just Python the
computation graph has its own
interpreter and it's not an it can run
in parallel so it's parallel execution
we'll talk about that during the talk
from Antarctica only Eskimos here we
have Antarctica in the house where it's
only two the open AI and GTA 5 I'll make
a video on that within three within a
month please explain your hidden layer
this tutorial because in pong lab you
didn't explain I will explain the hidden
layers I'm going to explain what's
happening in each of the filter maps
okay three more minutes any application
of ml in robotics sure there's a lot
control theories specifically like ideal
ways of moving your physical arm to pick
up the object and you can run a loss
function optimization function whose
back propagation everything else
everything applies it's just
reinforcement learning MN ist exactly
when you use MN eyes T the code is in
the description Google search BOTS not
yet a best resource is currently
available for learning about generative
modeling up wow I would say the best
resources are for that Ian good fellows
book because I mean he's the guy who
made generative adversarial networks and
so if anybody knows about it he does and
that's kind of that's still a bleeding
edge any video coming up on Apache spark
in ml live Apaches like not that cool to
me but I mean if you send me a link
maybe up maybe I'll be convinced CNN for
speech I have a few videos on that
search engine for speech recognizer but
I'll make more in the future what is a
good reference to learn about ml my
channel watch all my videos there is
more content in my videos most people
don't recognize this then in the entire
web I have I have written essay after
essay week after week my scripts for my
videos are huge huge technical writing
scripts across almost every topic at
this point and we're just going to keep
going in there more topics and there's
endless topics two more questions and
we're going to get started
resources for NLP let let me talk slower
he talks it resources to see there's so
many questions coming in guys by the way
let me say that we have somebody from
Udacity in the house his name is Luke so
shout out to Luke and he's going to be
in here to help answer some questions
because we had a lot of people here so
any questions you have Lucas and be an
answer it so look if you're here shadows
people can see you all right so okay so
to
more questions Kevin Murphy okay any
image similarity detection in TF there
is that's not what we're doing we're not
doing it we're not doing a similarity
detection right now uh but it is
possible in TF essays there's Luke right
there okay so one more question then
we're going to get started how to
practice ml in smartphone how to
practice ml in smartphone practice ml
you don't want to practice coding on a
smartphone but you can use ml on a
smartphone by having a server with your
uh with your with your neural net and
then you just call it with an API don't
run your don't run your deep deep model
on your mobile phone because it's not
going to work yet we need more we need
we need better weren't more robust
models that run on less computation
before you okay so that's it for our
questions let's get started with this is
going to be awesome we're gonna run this
code the the link to it is in the readme
okay so let's start screen sharing and
let's let's get started alright here we
go I think higher screen all of it share
all of it okay
so here we go I say here we go a lot so
I'm getting better at certain thing I
mean I like saying here he goes but I'm
saying um bless and this is a this is a
function of having to do this every or
wanting to do this every week okay so
here's our code okay let me just play
that out and then let me get these
comments over here so I could see them
at the same time on my different screen
yeah let me make this a little bigger
because we are really going to dive into
this we're really going to go into this
like all the details okay so okay so
here we go with this let me also just
have my face in the corner because you
gotta have the face I mean right you
just not the screen reporting the movie
reporting there I am whoa too big too
much Raj was never to mess around there
is fruit today right now okay so there
we go right there and then the code was
here here we go we're gonna build and
we're gonna build a convolutional neural
now CNN this is fake news oh my god
please don't bring politics in here I am
politics
free I am politics free okay we are
talking about a I don't bring that in
here okay so we're going to bill is unit
and why are we building a CNN why don't
we just use open CV right why don't we
use open CV it's easier right it's
easier to understand we can just talk
about the similarity because we write
because CNN's outperform every other
type of algorithm for image
classification they are this state of
the art the data set we're going to use
for this is the MN is T data set okay
we're going to use the MN is T data set
and it looks like this and why are we
using this data set because it is John
laocoön was the guy who first used it
in his 1990 you remember the weekly
video he use it for his first uh
convolutional net in the 90s which
worked really good really well but now
it works even better because we have
more data a more computing power the
other thing is it's a great easy to to
start with multi-class classification
problem
it's a multi-class classification
problem right there are ten classes here
one for every digit one for every digit
right so one would be 1 2 would be 2 3 I
know
equal it is a boring data set but it's
not about the data set right now we're
but the interesting part is going to be
this model architecture and that's going
to be the interesting part ok so five
minutes to answer questions on up for
350 people waiting a whole week man that
is quite the pressure but I it's all
good because you know it's Gucci it is
Gucci okay so here we go so that was
that's the emaan highest e data set it's
going to be a multi-class classification
problem and this is better than a linear
model because the best we can get with
OpenCV the best we can get with OpenCV
is um it is of 91% so we reuse
multi-class classification and this is
what it's going to look like so let's
let's let's start out by just looking at
our architecture now this is a fully
trained architecture that we are looking
at right now there is man man you guys
have the funniest comments you guys are
the funniest comments I love you guys
okay so this is what it a fully trained
convolutional net looks like and what
we're going to do is we're going to look
at this in detail before we get to the
code let's just look at let me blow this
up a little bit okay so here we go we're
going to start off with
input image like seven okay we have an
input image like seven and we're going
to put it into our convolutional net and
it's going to output a class the class
is going to be one of ten different
classes okay so that's what we're going
to do so how does it how does it output
that class is the question what is the
magic that is happening in a
convolutional net to output a class well
it it has layers so this is a trained
network so we're going to talk about a
trained Network and then we're going to
talk about how it became that good so
when we put an input image like seven in
it's going to go to the first
convolutional layer like you see right
here this first convolutional layer and
in the first convolutional layer we have
sixteen uh filter matrices these are
sixteen five by five filters and filters
can be thought of as weights okay they
can be thought of as weights in a
network these are sixteen five by five
filters and what these what is going to
happen with these filters is we're going
to take this image and we're going to
multiply each of the uh we're going to
multiply each of these filters by one
pixel in the image so it's gonna go left
to right it's going to go from left to
right and what does this look like well
I have this very handy little uh I have
this very handy little animation here
which is also in the notes and I need to
remove the parentheses but this is what
it's going to look like this Andrei's
blog at somebody's blog where is it
where is it words there it is this is
what it's going to look like right it is
convolving the word convolutional the
word and I'm going to contrast it to
other architectures in a second okay
hold on but did the word convolutional
comes from convolving because we are
conveying from one part of the image
across all of it convolving means
sliding we take our filter and we
multiply it by it's essentially a dot
product we are doing matrix
multiplications that's all it is it's
matrix multiplication or taking our
filter we're taking our filter and we're
multiplying it by every part of the
input image and it's going to and we're
going to all of those
the results are going to are going to
result in an in a feature map image and
let's talk about what that feature map
image looks like okay so okay so we're
going to multiply each of these images
by one pixel in the input image and it's
going to output a sixteen channels and
this each of these channels is a is a
matrix and we can think of this whole
thing as a feature map that's what we
call it we call it a feature map or we
could call it an activation map both
words for it and as either a feature map
or an activation map there are 16
channels of this and so that's the first
image right we are multiplying one month
right from left to right left to right
left to right left right
let me show one more a little animation
of this which one what a little
animation of it ok so here's one more
animation in this animation right and
this was in the weekly video it's
sliding just like that and it outputs
the feature map so we can think of this
feature map as one big-ass image right
actually there's several there are
several images and it makes one big-ass
feature map oh so that's the where
difference there are several images ok
this is going to be uploaded to YouTube
right when I'm done but it's there to
watch live exactly so that's what's
going to happen so ok so that's what's
happening in the first layer it's going
to create a bunch of images guys be nice
to each other we're going to create a
bunch of images and then in the next
layer it's a little more complicated in
the next layer it's a little more
complicated because we are now we have a
16 by 36 a set of filters so there are
six we don't actually see all of them
here we only have this verse 16 set the
second second 16 set a deaf Patel will
probably play me in a movie one day for
sure I'm sure but there's somebody well
for sure I'm convinced there's no movie
about me one day so we're gonna have 16
by 36 a convolutional layer and each of
these is we're going to multiply by each
of these so for the first so here's
here's what's gonna happen
so for this first image right up here
with my or my mouse is pointed at let me
go
and make this little bigger this
actually didn't make it bigger it just
made everything but this thing so so
good job Mac good job my foot
hold on so we're going to multiply each
of these images by each of these pixels
in one image and we're going to sew all
of these we're going to sum them all up
so for so let's talk about this first
left corner pixel at the top left we're
going to multiply every single weight up
here all 16 of them by that one pixel
and we're going to sum them all together
and the sum of that makes up a single
pixel right here then we're going to
take the next 16 and multiply it by the
second pixel and then the next 16 that
we're gonna see so basically all of
these feature weights all of these
filter weights are multiplied in sum
together to make a single image right
here on this new feature map that's even
bigger and we're going to do that for
every single image here so what happens
is so what happens is these channels
these 36 channels they're it's a it's a
bigger feature map and they're more
dense and what do I mean by more dense
it's it's it's only focused on the
features that it thinks are relevant
because all of the features and things
are relevant are going to show up here
then okay so once we have that wide so
you have 16 fill yes so I started with
16 filters and then we we started with
16 filters and then we get 16 by 36
filters and these are 5 by 5 pixels and
it's our it's just a matrix of numbers
it's a matrix of numbers okay Rohan
Pinto okay so so it's a matrix of
numbers and then we take our fully
connected layer right here
our fully connected layer and we and so
what are fully connected layer does is
it takes that huge multi-dimensional
feature map and it squashes it into a
two dimensional feature map it squashes
it into a two dimensional theater
feature map just so we could then squash
it with a sigmoid in the output layer
and what happens when we squash it with
a sigmoid is it's going to give us one
of these ten different probabilities and
one of these ten different probabilities
we could then convert to a class which
is going to be seven okay that's the
very highest level that we're going to
talk about right here um okay so that's
the very highest level and we're going
to talk about in detail as we right
the code as we write the code we're
gonna talk about it in detail now one
interesting thing about a convolutional
net that I really like is we can
understand how they work but we don't
understand so well we can understand
perfectly how they work all the matrix
multiplications we can get from a high
level why these series of abstractions
can detect a feature or how they can
detect a feature but we don't know why
it works so well and that to me is very
exciting and we'll talk about about that
in a second so that's the highest level
so let's keep going now now we're going
to get to start building this thing okay
before we start let's let's just talk
about what this is just another example
of convolution that's happening in the
first filter this is one more example so
the red so so here's the thing about
that so this is why it's red in black so
the red filter weights means that the
filter has a positive reaction to the
black pixels well the blue filters means
the filter has a negative reaction to
the clip back people pixels and what and
why do i what why do I talk what do I
mean by positive and negative positive
mean that it has detected something that
it there is that there is something
there and blue means that there is
nothing there and so what happens is
it's only going to get what it considers
to be relevant and this is what it
considers to be relevant in the first
feature map just this line over here
this feature it considers to be the
relevant feature in the first feature
wrap okay that's the result of the first
convolution and it does this for several
it's gonna say we make several of these
images they're all gonna look other but
for different parts of the of the number
they're all going to look like the
number seven but it's gonna highlight
different parts of it okay some kit yes
I did explain in this video thanks for
following that out but okay so so that's
what's gonna happen so now let's start
running our code we'll start off by
importing our dependencies we got
attention flow and numpy scikit-learn
and we'll talk about each of these when
we get to it how does it determine what
is relevant that that is the question
that is the question we don't know why
it determines these certain features as
relevant but we do know is it can detect
what is rolling we know how it detects
roll
been seen by multiplying matrices
together and then outputting a a binary
yes-or-no what as in the feature is
there or the feature not but we don't
know why why it's why it's in turn like
Y determining that this is this specific
thing is relevant so that's really
interesting right so let's get started
with this basically magic no I mean
we're going to figure this out I mean
give it give it like a few months I'm
sure someone's going to output a paper
on this so let's start building this
down if we're going to start out by
defining our hyper parameters our hyper
parameters in this case are going to be
d convolutional layers why do we define
a 5x5 filter size for this for this
feature map now somebody asked me why I
didn't do a a can I explain a difference
between architecture so let's talk about
a fully connected layer for a second
let's talk about a fully connected layer
okay so if you look at this image this
image it's it's showing a thick box and
this box represents a 3d filter it's
representing a 3d filter a
three-dimensional filter and the third
dimension by the way is the depth which
is RGB values so it has three different
two-dimensional mate matrices one for
red one for green one for blue and the
reason we're not using a normal
feed-forward neural network with a fully
connected layer is because there would
be a combinatorial explosion if we
multiplied a three-dimensional input
image by a two-dimensional a set of
weights it would be a huge number okay
especially for big images with 10-foot
1080 pixels so that's why we use a
convolutional layer instead of a fully
connected layer that's why we use that
because it is a it is a smaller it gives
us a smaller result so two back to where
we were going so that's why we're that's
why we're creating our filter maps and
it's 3d because there are there are
three color channels red green and blue
red green and blue okay so that's so we
define our filter side but as five by
five so that's going to be a specific
size for all those little filters that
we saw up there it's going to be a five
by five part um
okay thanks ro so and then we're going
to find a number of puzzle is going to
be 16 and so that's going to be in our
first filter is going to be 16 of them
and then for that Infernus or first
layer it's gonna be 16 or next one
there's gonna be 36 and I'll explain why
we use a fully connected layer at the
end we do use a fully connected layer at
the end and we'll define it as a size to
be 128 here so those are our high
parameters for our layers then we load
our data sets our data set is going to
be the M NISC data set we said one ought
to true which means we're going to use
one hot encoding one hot encoding is an
encoding scheme that is very simple that
it just it converts them to binary ones
or zeros which is great for a simple
classification which is what we're about
to do so this is our data is going to be
for in the data initialization why
there's why these numbers well we could
we could try we could try so choosing
hyper parameters is its own field of
study should we do big hyper parameters
small hyper parameters usually we test
out different things through guess and
check there are actually hyper parameter
optimization methods search strategies
for our neural net to try out different
sets of try out different sets of thanks
Luke for answering that try out
different sets of of ways of hyper
parameters so that's for a load loading
data and so we're going to use our we're
going to load up our training test and
validation data right now okay and then
we're going to define some more hyper
parameters so we know that those M&amp;amp;I is
t images are 28 pixels by each dimension
so we'll define it by image size these
are these are we don't want to have
what's the computer science word for it
we don't want to have not magic numbers
but I think it's magic numbers right we
don't want to have magic numbers those
are just scattered up values in our code
that we don't want to think about uh
right so that's all we're going to
define these features define these
variables beforehand then we're going to
define our image size as flats who's
going to be a one dimensional array of
this leg
and then our tuple and so it's a tuple
weak because we're going to input a
tuple into the this is our input we have
our image size and then we're going to
define the number of channels there's
going to be one because we're using
grayscale luckily for us this uh so I
talked about 3d convolution when we have
an RGB uh when we're using RGB but in
this specific case this is gray scale
these are grayscale up mn ist numbers so
there were only going to find one
channel but if we're using color images
we define how many channels we're going
to use three channels yes I'm doing a
time series video on Friday it's gonna
be dope you guys gonna love it to be
really popular
then we're defining the number of
classes ten of them okay because there
are ten numbers now let's plot out these
images this is a simple numpad plotting
code to plot out the number of images
okay
this network can be used for C far with
some modifications that I'll talk about
at the end so when we plot out these
images this is what it looks like is
going to all it's not it's not it's not
predicting anything pair just is just
finding out the images and their labels
okay
images and their labels oh my god time
series is going to go is going to be so
amazing with lsdm networks I'm gonna
talk about on Friday so that's it so now
let's talk about tensorflow now I talked
about how tensorflow is scalable it's
scalable because it could be used on
CPUs and GPUs if you run it in the cloud
you can easily scale it across CPUs and
you can define how many GPUs you want to
run it on you cannot do that with just
num pot that's why it's better than
using just num pot any kind of any kind
of production grade machine learning
that you want to do do not do it with
just dump I use tensorflow because it is
the best machine learning library that
we have so far so the first thing we're
going to do before we start defining our
computation graph is we're going to
define helper functions for our weights
and our biases our weights are going to
be initialized randomly they're going to
be initialized randomly and we're just
defining variables for them we're not
doing anything else we're just defining
variables and then our biases why are we
using biases what is the point of
so explanations for use of biases are
actually really bad across the net we
can think of biases as we can think of
them very intuitively for linear Russian
models because we just think of them as
the why if we think of them has the
y-intercept and a y equals MX plus B
slope formula it's just a y-intercept
but for a multi-dimensional array or
start for multi-dimensional computation
such as the case in tensorflow
we use biases as a constant value to
across it's it's always going to be a
constant value and is carried across the
matrix Mac and what this does is it
improves convergence it makes sure that
our it make sure that our model is more
accurate by having a constant value it
kind of a kind of anchor point to where
we started off and between where we
started off and what we ended and we'll
talk about biases more okay they make it
yeah to highest level it makes it easier
for the math to optimize so okay so
those are our two helper functions we
haven't actually started building our
computation graph yet right okay we
start building our computation graph
didn't do the math it's going to be all
right you can sit back and laugh but
it's okay because we're going to go so
hard you're going to barf in this class
I learned all that by the way
so this is our convolutional layer in
our convolutional layer we're going to
define our previous layer then our input
channels our filter size and number of
filters and then use pooling to - let's
talk about pooling I did not talk about
pooling so before we look at this let's
let's go back to the image for a second
then let's talk about two more things
that we didn't talk about let's go all
the way back up to this initial image
that I showed you guys so in the first
convolutional layer in the second
convolutional layer it's not just
convolving it's performing two other
operations it's performing its
performing pooling and is performing
riilu now now we perform riilu to reduce
to increase the non-linearity of our
model that makes it easier for our model
to learn nonlinear functions uh which is
in which is the case for this it turns
all of our negative values into zeros
okay
and then it which improves non-linearity
I will talk about that when we when we
point going to that in a second but uh
then we're also doing max pooling or
pooling is an operation where we just
take the best parts of what we've just
calculated and by best the max values
which is those work it's kind of like
involving or from every part of the
matrix
we're just saying what is the max value
from this matrix so if there's like six
seven eight nine we'll take nine and
there there's our there's an entire
field of research over pooling and we'll
talk about that later on but it's taking
the max value so back to where we were I
just wanted to define those two
processes that we're doing okay so back
to this the convolutional layer we're
going to have an of shape and where the
shape is going to consist of all of
those hyper all of those parameters that
we define what is the size of our
filters the number of input channels
which is one because this is a grayscale
image okay and the number of filters
these are our weights now that we've
actually created helper functions and
and this is the this is the actual
initialization step right well for a
convolutional layer so our convolutional
layer is actually a block and it
consists of three operations remember a
convolutional layer isn't just a series
of matrix multiplications and a
summation of all this it's also it's
also a
right it's also a it's also real ooh and
it's also well sit beside real ooh it's
also sides real ooh it's also uh using
cooling so there are three operations
that are happening in a convolutional
layer okay three three operations and so
this is the default tensor flow this is
the default tensor flow convolutional 2d
uh function
okay and we're wrapping this in our own
function so that we can add biases and
we can add uh we can use biases and we
can use up weights now here's another
thing now in this convolution layer we
know about our input we know about our
weights water strides strides are think
of striding like in real life right
whatever what it what is it stride by
the way like a like a normally a stride
is when you like kind of you take a big
stride you take a big ass step you take
a big step and that's what striding up
is in internet it defines how what are
the intervals that we are multi up
creating these multiplications right so
is it every pixel is it every two pixels
is it every three pixels that's what we
define as strides so the smaller
destroyed the more accurate it's going
to be but it's more computation so
there's a trade-off more accurate
classification for more computation then
we're defining padding it's a massive
big step and then we're going to define
padding so padding is going to be same
which means the input image is padded
with zeros so the size of the output is
the same okay so that means that the the
difference between the input image and
the filters the matrices aren't the same
size so we'll pad it so look at the
smaller one with zeros so they're the
same size we want them to be the same
size so it's easier to perform matrix
multiplication and so that is what the
output is going to be the same that's
why we perform padding then we're going
to perform cooling and so down sampling
is what we is is what we call this
process of cooling down sampling is the
name of this process we call all sorts
of pooling max pooling average
pulling all sorts of pooling we call it
uh max we call it a down sample and
Chancellor tentacle has a built-in
function for this called max pooling
when we define the size of the up of the
pool and the strides as well for the
pooling so it's similar to convolution
it's similar it's a similar process it's
a kind of like a flashlight that's
shining over the image and the part that
it's looking at remember it is called a
receptive field whatever it's focused on
is the receptive field okay so now and
so then we'll define our right so then
our third operation and this is the
third operation in every convolutional
block is going to be rectified linear
unit or reload which is going to
calculate the max max value which turns
all the negative numbers to zero so it's
kind of similar to absolute value now it
allows it to learn more complicated
functions than just linear regression
those are the three operations that
occur in a convolutional block any
traditional CN n 3 blocks are there
right this happens three times and then
you squash it and it outputs a
probability this is going to return our
layer which is what we the resulting
layer and the resulting weights after we
perform these three computations on okay
now we're going to do that three times
and then then we're going to flatten the
layer we're going to flatten it using a
fully connected layer so okay so let me
just talk about the steps here to
flatten the layer to flatten the layer
marking talking for a second to flatten
a layer we're going to first see the
layers shape okay but what is the shape
of that layer what is it what is the
dimensions that it's going by and then
we're going to say how many features
does this layer have okay what are the
features and to get that we're going to
use the num elements function we're
going to use those features and those
features are what we want to squash and
the word squash means reduce the
dimensionality okay we want to reduce
the dimensionality to a two-dimensional
vector which we can then reduce even
further into a scalar and that scalar
which is a single value is going to be
probability it's going to be our
probability I am made up of a drive to
solve intelligence because I have travel
and I have seen all the problems in the
world and it is the most important thing
the entire world I would die for this
cause I will die for this cause so layer
flat is that last is that last shape
once we reshape that number of features
layer flat is going to be that
two-dimensional vector okay and then we
were going to return that okay so then
so then for our fully connected layer
we're going to perform matrix
multiplication and then focus focus
focus arriving so hold on a second hold
on a second hold on once we flatten the
layer we input it into the fully
connected layer as a dimension and it's
going to output a is going to output a
the fully connected layer a probability
that's that's what is helpful it's going
to help with a fully kind of layer which
is a probability okay so so don't so we
all we did so far we define our helper
functions now let's build our graph
using these helper functions and because
we define all these complicated helper
functions now we can easily define our
computation graph so we start off with
our placeholder value the placeholder is
going to take in that image right the
image is going to be a two-dimensional
grayscale image right it's going to be a
number so it's going to be not just the
image we're also going to input these
lay people a label so we have two
placeholders for the input image and the
label then we're going to input it into
our convolutional layer yeah I'm going
to I'm going to do a Q&amp;amp;A session after
this to explain things more okay so so
write down your questions okay all right
so then let me remove me so I can so so
that's gonna be that so then so let's
define a first convolutional layer let's
define our first convolutional layer
using our helper function that we
created our
first convolutional layer is going to
take the images and input and create a
up set of filters and each of those
filters has a width and a height equal
to the filter size that we define and
then finally when we list you down
sample it using max pooling so it's
going to be half the size that's going
to be our output it's going to be that
feature map that's going to be our first
filter and we can look at what happened
here right it's a it's a tensor flow
object TF tensor we define riilu on it
and it's going to be a shape of size
question mark 14 14 by 16 and then the
type is going to be float 30 tiers say
matrix of values right a matrix is a
table of values of a pixel values we'll
do that again for the next convolutional
layer right so there are two
convolutional layer we took the output
of that first layer and we fed it into
this next layer then we're going to
flatten it we're going to flatten it um
what would you flatten it using the uh
this the we're going to flatten it over
here uh right into it's a four
dimensional tensor I'm going to find
that into a two dimensional tensor using
that that help her to that helper
function we created alright so if we
were to then visualize that flattened
layer it would be seated now it's the
shape is going to be two dimensions
right for the first time it had four
four numbers here right because it was
four dimensions and now it's just two
dimensions okay so now we're going to
look at the number feature which we have
a lot of features and so now it's time
to flatten it flatten that flattened
two-dimensional vector into a
probability and we're going to do this
twice we have two fully connected layers
okay we have two fully connected layers
here and for each of them we're going to
define them we already define the helper
function so what we're going to do is
we're going to run them both on that
input data and it's going to output uh
this one it's going to output this
two-dimensional vector which we can then
squash using a soft max function
okay the softmax function is a sigmoid
function it's going to output a
probability one of ten classes this
probability is going to be mapped to one
of ten classes
and the way it's going to be mapped the
way our network knows where to map this
is by using an optimizer and a loss
function okay so this is the this is the
gradient descent and an optimization
step okay so for our gradient descent
and optimization step we're going to use
okay what is what is that what is our
loss function where it's going to be
called cross entropy cross entropy is
our loss function here's a great um pora
link for the cross entropy function like
learning about it so basically it's
similar to softmax
except instead of stock in a softmax
function we're determining the type of
activation layer but in cross entropy
we're using we're using it to measure
the error at the softmax later it's
similar to softmax but difference
because we're using it to measure the
error at the softmax layer so it's a
loss it's a loss function because it's
going to output an error value and we're
going to use this error value to error
value that it's that it's outputting
right uh between the real and expected
value so right so it's going to output
it's going to output a prediction right
so be like your CC percent chance that
it's a point 70 but there in reality
it's a one hundred percent chance that
it says that it's that the number is
there's a 60% chance of the number is 7
but in rally it's a 100 percent chance
at number seven so that difference
between the 60% prediction and the 100%
reality that error value is what we want
to minimize and the way we minimize that
error is using an optimization method
now Adam is just another word for
gradient descent it's a type of gradient
descent optimization function now
gradient descent determines the
direction that we want to update our
waves our weights in this case are going
to be those filter values how do we
update those filter values so they are
more accurate that they give us a more
accurate result that these that they
that they are more closely aligned to
features that would be in the image can
we how can we modify those initial
matrices in those filters such that they
output a more accurate prediction and
in dissent is going to give us that
because it is using the chain rule
because okay so hole I mean so gradient
descent I mean that could be a whole
explanation so let's let's talk about it
let's talk about greatness in a little
bit so for gradient descent we're using
the chain rule so what is a chain rule
we're taking the derivative of the loss
function and we are recursively taking
the derivative of each layer back
propagating our loss to each layer and
then we're updating our weights using
the gradient um try to explain this a
couple times in a couple videos but I
you know honestly I need a video just
for back propagation and that's going to
come up very soon but it is back
propagating weights by taking the
derivative recursively across each layer
okay and that's probably a great huge
for this back propagation recursive
recursive derivative it's all about
recursion when it comes to back
propagation I mean the chain rule is
just recursively taking derivative of
derivative of derivative but and so and
that's from statistics so knowing the
chain rule is very important you have to
know how back propagation works because
it is e it is the optimization method
that is used across a neural networks
so with tensor flow we can basically
just plug and play it's a plug and play
model to just type in whatever type of
optimization function we want to use and
then it's going to minimize that loss
over time okay so then we'll use the
reduce nice so then we're going to use
Tentacles reduce mean function to
measure the classification accuracy okay
so it's a binary yes or no value 0 or 1
true or false ok so then we're going to
run our session we always have to run
our computation graph once we build it
so then um so once we run our session so
these are help this is us running our
this or this is running our session and
then we're going to plot our values
after a set of iterations
okay we do
with you a set of iterations these are
all just printing out testing accuracies
and and training accuracies but really
what i want to get to is we've we've
created our graph we've run it and now
we want to see that accuracy so after
one run so after one set of small set of
batches we have a 13.7 percent accuracy
which is very terrible it is worse than
linear regression it is terrible
we never won a 13.7 percent accuracy
right but after training it even more
one optimization iteration we about
fourteen point three percent right so
after a hundred we have fifty nine point
eight percent so you see that the more
we run this optimization the more
accurate after a thousand we have ninety
percent okay and so a thousand would
take probably on a standard grade 2015
MacBook with a two-point you know
probably the 2.7 gigahertz CPU a
thousand iteration that this would take
like 30 minutes or less okay so this is
all computationally possible on a on a
local machine right now we don't have to
use the cloud for this stuff
okay this is multi-class classification
that's what this is considered
multi-class classification so let's
visualize those weights that we created
right so this is this is using matlab to
visualize those weights okay because
those are matrices so we're going to
visualize those matrices using map fault
line and if we look at those weights
that we just visualized so let's look at
those visuals okay this this is an image
from the test set these aren't the
actual visual lists these are the
visualizations so we've colored the
relevant uh record the relevant parts of
the matrix red and the irrelevant or the
irrelevant blue okay binary right so so
why is it that that so why is it that
you know this line is what's considered
to be a good feature why is it that it's
considered this bottom right part to be
a good a good feature you don't know
what I but what happens is it's going to
be more and more accurate it's these
features are going to be more and more
uh close to what we want to make the
accurate prediction so that's what it
looks like it's a matrix of pixel values
that it learns at these weights these
weights these filters right between each
of
the convolutional layers and so if we
plot these what's happening in these
like this is the feature map this is
what it looks like and see it's
highlighting different parts of the
image it's going to combine all of these
to output a probability ok and see and
so in the next convolutional layer we
have even more filters we have 16 by 36
even more picks up features and those
are the results and then we close the
session I always close it when we're
done okay and then there are more
exercises so so yeah that's it for our
convolutional layer and let's let's guys
stick around for a second stick around
stick around let me let me go back to
stop screen sharing okay so go ahead and
ask questions guys cuz we have some
questions to ask there okay the the
notebook is in the description I'm going
to add the readme in a second I am so
proud of all of you guys for being in
this live session for watching this
video it is awesome to see you guys here
this stuff is not easy it's not easy but
it's very important and knowing this is
going to put you a huge step above
everybody every other developer just the
fact that you even recognize little
things about convolutional nets is very
important okay for your career for your
academic career for your for any sort of
yet you want to do it's very important
because all successful startups all
successful companies all successful
academic research in computer science is
going to be at the bleeding edge it's
going to get the most attention is going
to involve deep learning at this point
and then we're going to build on that
later insulins get even better so I'm
very proud of you guys for being here
okay you guys are awesome I am honored
to be a part of this community okay so
the room is soundproof and next week I'm
going to actually improve the quality of
this livestream so I'm very excited for
that okay we are growing so fast we have
500 developers joining this community
every single day on average we are going
so fast so it's a very exciting time to
be here and I'm just going to get better
at explaining and you guys are just
going to get better at doing these
things I mean it is just incredible the
amount of coding challenges you guys
have done it is incredible
what you guys have done it is just
incredible so okay can I share resources
to help you Brock sin I'm going to put
so many CNN resources in the description
it's not even going to be funny within
the hour
if you want the image to be manipulated
do I add another matrix to affect the
color channels have an output
multi-channel image the image to be
manipulated if you want the image to be
manipulated we're going to perform eight
entirely different optimization process
not gradient descent but gradient ascent
and that's what's used in neural style
transfer and we're going to get to that
three videos from now have you heard
about good ai ai challenge no can this
be done with Python with same level of
difficulty Python this was Python okay
guys can you do more stuff on financial
subject yes it's coming out on Friday is
max pulling done across channels or a
specific feature frame Matt's pulling is
done across all channels it's every
Wednesday at 10:00 a.m. PST also turn to
the next slide straight because it's
going to be even better quality I'm not
going to use Google Hangouts it's going
to be even better quality
okay thanks close Chikara we have so
much to do we have so much to do guys
remember to share your share your
victories with all of us one victory is
a victory for all of us
you are a very special person for
knowing neural networks you don't even
understand how important you are to the
future of the human race even every
commit counts on github every commit to
every repo counts because it's like the
butterfly effect you guys do the
butterfly whip effect if a butterfly
flaps its wings in Colombia it's going
to cause a tornado later on somewhere in
a different continent it's like that for
code and machine learning
the rate of discovery that is happening
the fact that you even push any open
source code to github is going to be
even looked at by any other developer
it's very important because then they're
going to build and they're going to
build and they're going to go and other
people gonna build so it's all it's all
it's all connected everything we're
doing is connected here okay so it's
don't think that anything you do is
meaningless in this field everything
that you open-source everything that you
output is going to have immense value to
the entire world as machine learning
propagates across every industry
okay so let's cost tornadoes with code
let me answer two more questions and
then we're done with this live stream
okay
can you make a video on gaen I have a
video on dancer gender battery channel
networks I'm gonna do another one it's
gonna be even better any flat strip or
puzzle we can all discuss yes I have a
slack channel is in the description of
every single video of mine
join the saw channel ok there's a lot of
great people in there ok so that was one
question one more question how do I
learn besides content practice how do I
learn I input a diverse set of sources
so I have you know if I'm learning about
convolutional legs so like for this past
video for example I had Andre Karpov
ease of the unreasonable effectiveness
of neural neural networks epic blog
posts up in one tab and then I had
another tab from I think Chris Ola great
blog by the way first hola Andre karpati
two of my favorites um blogs and then I
had you know other tabs since what I'm
doing is I'm going between different
tabs and and I'm max pooling ah I'm I'm
I'm taking the optimal information from
each of these and I'm and I'm saying
every time I have a question one that's
looking another four so you have to have
multiple diverse sets of learning
resources okay that so that's what so
that's how I learned and I and I just
and I truly love what I'm doing so I
think it's very important to love what
you're learning because that is such an
important thing and I believe in myself
I believe in myself that's such an
important thing and you guys all need to
believe in yourselves as well because
you are very very important ok you guys
are awesome I'm so honored I'm so
honored to be here every Wednesday
making videos for you guys and the
community is getting better as well our
community is getting better people are
helping each other in the comments the
number of coding submissions are going
up every week and the quality of the
coding submissions are going up every
week as well so we are all getting
better it's incredible and we are a
force to be reckoned with God you are a
force to be reckoned with
ok so ok so that's it for our questions
and so now I'm going to
it with a motivational rap okay so um uh
okay so any uh guys and by the way
there's so much money to be made here
and we're going to talk about all the
money to be made as well from each of us
okay we are all going to get very rich
not that that should be the driving
factor that and that is a good thing
it's good to build well the driving
factor should be to make as much of an
impact as you can and as a side result
you will make money because this stuff
I'll make a video on that like ways to
make money with machine learning I know
it's gonna be important so freestyle ok
d3.js that's that's that's much for you
sell d3.js
ok hey yo when I visualize matrices I
use d3 I do a wig JavaScript can't you
see I see all these matrices in a vector
it's like 3d it's like 2d 1d scalar
vector tensor all these words man I
don't know call me Victor I was looking
at all these statistics I was looking at
linear algebra with six I was hitting it
with my brain every day I went back yo
it's gonna be ok ok so that was it
everybody's on you're rich ok wasn't in
the side channel it's our programming up
- wizards that's it for the video
thanks for being here guys for now I've
got to make the sickest a stock
prediction video using LS TM networks so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>