<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Navigating a Virtual World Using Dynamic Programming | Coder Coacher - Coaching Coders</title><meta content="Navigating a Virtual World Using Dynamic Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Navigating a Virtual World Using Dynamic Programming</b></h2><h5 class="post__date">2017-11-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5R2vErZn0yw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and how does an
AI get from point A to point B in a game
world that's the question that we're
gonna answer today using the magic of
dynamic programming okay this is a type
of reinforcement learning and we're
gonna apply it to the open AI frozen
lake environment that you see behind me
basically it's a really rough text
version of a frozen lake and we've got
we've got an AI an agent that's got to
get from the starting point to the
ending point without falling into a hole
okay and there's a there's an optimal
way of doing this right there's it
there's an optimal path that this agent
can take such that it doesn't fall into
any holes and it gets from point A to
point B as fast as possible and we're
gonna use dynamic programming to help us
do that okay so let's get right on into
it the first thing I want to talk about
is this environment so this is what it
looks like right so you've got you've
got a grid right it's a three by four
grid or a four by four grid and it
represents a frozen lake and we have us
we have a series of letters right so s
means the starting point that's where
our agent starts f means frozen that
means that an agent can totally walk on
it slide on it do whatever you want it's
a frozen lake right H means a hole you
do not want to fall into the hole and
then G means goal that's where the agent
wants to be so it's the goal for the
agent is to just travel through all
these squares until it hits G without
touching any of the h's and it's going
to do this through a process of trial
and error
aka reinforcement learning so how do we
frame this problem right so the Markov
decision process is the most used way of
formally defining an environment with an
agent it's the most used way of formally
mathematically defining a problem
setting well we have an agent we have an
environment we have a set of actions
that this agent can take in this
environment and we have a set of states
that will occur sequentially as the
agent takes new actions in this
environment the reason that we define it
as such instead of just saying yeah we
have an agent and it's in this game
world and yeah it's gonna do some things
it's because
just mathematics is beautiful right it's
it's a way of describing all of these
different processes as variables and
what can we do when we break down these
processes into variables well we can
perform all sorts of operations on these
variables we can find relations between
these variables we can create formulas
and equations that tell us what the
optimal value function for example or
policy for example what are the best set
of rules for an agent to perform in
order to complete this objective that's
the reason that we use what's called a
Markov decision process which is a
mathematical way of describing all the
relations between these variables
because it makes things easier later on
as we're trying to construct these
functions these equations later on so in
RL in reinforcement learning we have an
agent that's interacting with an
environment in some way and at each time
step every second or whatever interval
this agent Rai is performing some action
jumping moving forward hitting a ball
whatever it's gonna lead to two things
one it's gonna change the environment
State that means that okay so Mario
moved forward one inch the new state is
the environment with Mario one step
forward and the agent possibly receiving
a reward or penalty from the environment
did you fall in the hole minus one did
you not fall in the hole did you get a
coin plus one right and the goal of the
agent is to discover the optimal policy
so that is the word of the day policy
okay so policy means what are the
actions to do in each state right that's
a policy what are the set of actions
that this agent could do in each state
so you can think of it as a table that
this agent is looking at it's like okay
this is the state of the game oh this is
the action that I should perform let me
do that right and we want to learn the
optimal policy that means the best
actions for that given state such that
the agent is completing its objective as
efficiently as possible to be more
specific to maximize the future the
total future discounted reward but we'll
get to that
so Markov decision processes are a way
of describing this agent environment
loop in a formal way we have s which is
a set of states the state of the world
writes and we can describe States in any
number of ways right the number of
Koopas the number of you know BOTS that
the layout of the grid do you know the
state right it's a very abstract way of
thinking about all these problems that's
why it's so generalized that's why it's
used so often we have a a set of actions
we have the probability of a given state
given us an action and a previous state
this is called our transition function
and we want to estimate it right what is
the most likely the next state given a
agent takes a certain action in a
certain state we have our starting state
distribution which we can skip for now
we don't need that we have a discount
factor and we have a reward so the
reward is the value given some action in
a given state do we get a +1 do we get a
minus 1 now sometimes right
sometimes a reward sometimes doing some
action will give us a reward whereas if
we perform a different action entirely
we get a longer-term reward what do I
mean I mean let's say I'm Mario right
and I'm in this game world and I see a
star and the star is bouncing towards me
and instead of shooting the star I
choose - I choose to step on the Koopa
so I get a plus 1 so that's that's
optimizing for short-term reward but if
instead I decide let me let me instead
get the star and not the Koopa if I get
the star I'm not getting any +1 but it
means that I'm more likely to get plus
ones later on so I instead of optimizing
for the short-term reward I'd be
optimizing for the long-term award so
the discount factor tells us it's a way
of weighing what what how how optimal an
action will be to maximize for a reward
and the agent learns this right it
learns what the optimal discount vector
will be right so what we're trying to
compute is this policy so policies are
generally they are denoted like this pi
symbols and the idea is that the policy
is a function that takes a current
environment state to return an action
and it's denoted by this symbol right so
let's differentiate between the two
different types of environments that we
could have so we could have a
deterministic environment or we could
have a stochastic environment these are
two words that you should just know in
general for machine learning because
they're used all over the place it's
really simple determinants
means you can predict what's gonna
happen stochastic means you can't it's
random simple as that but to be a little
more specific in a deterministic
environment the next state is completely
determined by the current state and the
actions performed by the agent that
means the agent has a has a real effect
on what's gonna happen in the
environment whereas in a stochastic
environment which is real life right we
can't predict what's gonna happen right
and outside of a vacuum that doesn't
matter what an agent does that the
environment is gonna do whatever it's
going to do you can't predict it right
so clearly stochastic environments are a
little harder to learn the optimal
policy for but deterministic ones are
easier so in this video we're just gonna
focus on deterministic environments
right so like I said earlier I kind of
hinted at this this is the this is the
formula for the total discounted reward
okay so don't give don't get afraid by
this math where I'm gonna go over it in
a second so the goal of the agent is to
pick the best policy that will maximize
the polder rewards received from the
environment so let's see what this is
this is a Sigma notation what this means
is it's the sum of all of these values
right here together so war I equals 1 up
until T where T is the horizon the
episode length right which can be
infinity right an episode for a game
like you know from start to finish let's
take this discount and reward times the
reward so what this would look like if
we were to expand this equation it would
be a discount
under reward plus a discount times
reward plus a discount times a reward
right for each value and so we sum all
those rewards up and over this count
generally makes a reward smaller and
smaller and smaller the farther and
farther we go into the future or we can
split that and they could go larger and
larger and larger and this is what we're
trying to maximize for wizzy what is
that we're trying to optimize for what
how do we how do we maximize this value
what do we optimize for it to maximize
for this value so ok so the solution to
a Markov decision process is called a
policy right and it simply specifies the
best action to take for each of the
states but although the policy is what
we're after what we're actually going to
compute is a value function because we
can easily derive the policy from the
you function right so a value function
is similar to a policy except instead of
specifying an action for each state it
specifies a numerical value for each
state right so policies are very
straightforward rights for a given state
this is the optimal action we should
take but for the value function tells us
just a numerical value for each state
and using that value we can compute what
the optimal policy should be so it's
kind of its so the value function is so
the policy is a subset of the value
function and we can derive if we know
the value function we can easily derive
what the policy is so there are two
fundamental methods of solving Markov
decision processes that are model-based
that means that the agent knows the
model of the world beforehand and these
are policy iteration and value iteration
algorithms and these are considered
dynamic programming algorithms right so
they assume that the agent knows what
the model of the world looks like later
on we'll discours will discuss model
free methods as in the agent doesn't
know what the model of the world is and
that is q-learning and we'll get into
that in a later video but we're trying
to learn what the optimal policy will be
and we're going to use either value
iteration or policy iteration to do that
right so let's get into this okay so
value iteration okay this is this is
this can be non-trivial it might take a
few tries but eventually you're gonna
get it just keep looking at it over and
over look at my notes after this video
if you don't get it look at the code
it's all there you just gotta look at it
for a while a few hours and boom you'll
get it but anyway let's let's go through
this these are two separate processes so
for value iteration what we're gonna do
is we're gonna compute the optimal state
value function by iteratively improving
the estimate of the value function V of
S which is the value function right so
the algorithm initializes V of s two
arbitrary random values it repeatedly
updates the Q function if we're talking
about a q function and then it's
guaranteed to converge converge on the
optimal values in our case though we're
not going to talk about the q function
right now we're just going to focus on
value iteration without the q function
so here's what it looks like we start up
by choosing an initial estimate of the
optimal value function just writes
whatever
estimate is it could even be zero right
we repeat this process until the change
in values is sufficiently small for
every given state we calculate the
maximum expected value of neighboring
states for each possible action we then
use the maximum value from this list the
Arg max over that to update the estimate
of the optimal value function and after
we do that then we can calculate the
optimal value function it's okay if you
didn't get that I'm gonna go over it
again in a second in a more plain
English way for Policy iteration we're
gonna first of all choose an initial
policy and a value function then we're
gonna repeat this process until the
policy is stable there are two parts
here policy evaluation and policy
improvement so for policy evaluation
we're gonna repeat this process for each
state we're gonna calculate the value of
neighboring states when taking an action
according to the current policy then
update the estimate of the optimal value
function and then for policy improvement
then we're gonna update it to get a new
policy right so policy iteration instead
of repeatedly improving the value
function estimate will redefine the
policy at each step and compute the
value according to the new policy until
the policy converges policy iteration is
also guaranteed to converge to the
optimal policy and it often takes less
iterations to converge than the value
iteration algorithm so check this out
here is the best plain English
explanation that I can give you for the
difference between these two I'm just
gonna read this out okay just take just
just listen to this in a policy
iteration algorithm you start off with a
random policy right it's just random
just some number and then you'll find
the value function of that policy this
is the policy evaluation step then find
a new improved policy based on the
previous value function and so on in
this process each policy is guaranteed
to be a strict improvement over the
previous one unless it's already optimal
in which case we can just stop iterating
because we know now we found the the
optimal policy right
but in value iteration you start off
with a random value function and then
find a new improved value function in an
iterative process until reaching the
optimal value function and notice that
once we have that value function like I
said earlier we can easily derive the
optimal policy from the value function
why because it's a
in the in the it's one variable that PI
sim will in the value functions equation
we can easily drive that so what's the
difference here policy iteration is
generally faster than value iteration as
policy converges more quickly than value
function but it depends right it depends
on the environment and it's good to try
both out so let's look at some code here
so we have open a ties environments to
do this right so I'm gonna go over this
code and then I'm gonna go over the I'm
gonna go over the high level code and
then the the lower level code so let's
just let's just go to this let's just
step through this code what we're doing
is we're building this frozen lake world
and we're gonna apply both policy
iteration and value iteration algorithms
to this code so to start off we're gonna
import Jim right open a eyes Jim
environment that lets us test out a
bunch of different game environments and
then we're gonna import numpy to do some
matrix math so to start off we're gonna
have some action mappings write numbers
that correlate to keys that the agent
can perform I up down left or right okay
so in this play episodes function we're
gonna assume that we know the optimal
value function the optimal policy so
here's how it would go if we knew those
two right we've already performed value
and policy iteration so for every
episode in the game while the game is
still running select the best action to
perform in a current state we're taking
that right from the policy and we're
using Arg max to find the greatest value
for the action the action that's going
to give us the greatest value then we're
going to perform that action and observe
how the environment acted in response so
the great thing about Jim's environment
the Jim environment is that we can just
use the step function on a given
environment and to perform an action and
it's going to report it's gonna return
the next state if there's a reward or
not yes or no boolean did the game
terminate or not and then some logistics
some logging info that's it then we'll
render the environment at every time
step so we can see what it looks like
and we'll summer will summarize the
total reward remember that equation I
showed you about the total reward and
then we'll update the current state as
the next state right because we just
stepped into the next time step we'll
calculate the number of wins over each
episode and we'll get the average award
as the total reward divided
a number of episodes and return the
number of wins the total reward and the
average reward so that is the that's
actually how the agent plays given its
knows everything you know the optimal
policy and the optimal value function so
then we have a number of episodes to
play let's just say 10,000 and these are
both of our solvers where we're gonna
have a policy iteration solver and a
value iteration solver that I'm gonna go
into later so so for each of these
solvers load up our environment frozen
lake okay and then we're gonna search
for an optimal policy using policy
iteration okay and then we're gonna
print out what that optimal policy is
and then we're gonna compute an optimal
policy using policy iteration and we're
gonna use both value and policy
iteration for because these this is
looping for each of these solvers and
then we'll print out the the rewards the
winds and everything for us to view
later on right so let's go and see what
these two functions look like under DP
dynamic programming okay so let's start
off with value iteration right so for
value iteration we're trying to compute
that optimal value function right and if
we've got that value function then we
can compute the optimal policy so here's
what we're gonna do we got the
environment our discount factor a theta
value the theta value is a stopping
threshold and the max number of
iterations we want to perform this right
how many times do we want to iterate to
get that optimal value function we're
gonna start off by initializing our
value function randomly right and then
for the number of iterations we defined
we have our stopping condition we're
gonna update each state we're gonna do
one step look ahead to calculate the
state action values in the next time
step and that's going to give us this
action value then we're gonna select the
best action to perform based on the
highest state action value so we'll
perform and P dot max to find the
largest number for the action value and
that's gonna be our best action value
we'll calculate the change or the Delta
right between this current state and the
and our best action value and then we'll
update the value function for the
current state we found the change the
difference in those two and then we'll
check this is our stopping condition
right we'll check if we can we can stop
we've converged and once we have that
value right then we can create a policy
using that OP
value function down here so for each
state so we'll first initialize our
policy randomly and then for each state
we have we'll look ahead one step to
find the best action for the state using
our optimal value function then we'll
select the best action based on the
highest state action value and update
the policy to perform a better action at
a current state at the very end we
return the optimal policy and the
optimal value function right so the
value function is at the core it's at
the center and from it we then derive
the policy so now let's look at policy
iteration so for policy iteration we
start off with a random policy right
right in this case it's going to be the
number of states times the number of
actions divided by the number of actions
right and so now we have a counter to
account how many times we want to
evaluate each policy we're gonna repeat
this until convergence we say evaluate
the current policy right remember this
two steps policy evaluation and then
policy improvement we're gonna say
evaluate the current policy and that's
going to give us a value function we're
going to go through each state and try
to improve actions that we're taking
we're gonna choose the best action in a
current state under the current policy
by performing our max what's the max
value in that policy for a given state
and that's gonna be the current action
well then look one step ahead and
evaluate if the current action is
optimal and we're gonna try every single
possible action in a current state and
we're gonna select the better action of
the two and if the action doesn't change
then that means we've found a stable
policy there's no change in the action
and then we we will update the policy
right greedily but we'll talk about what
greevey means later this will update the
policy and then we have a convergence
criteria at the end right so that's a
difference here right value iteration is
focused on the value function first and
foremost where it has policy iteration
is more focused on the policy first and
foremost and there are two differences
sometimes one works better than the
other generally generally the policy
iteration algorithm is faster but you've
got to try out both right and both of
these are examples of dynamic
programming and this these are model
based methods if our agent knows what
the world is going to be like yeah
it looks like this right you know a
better view of this would be just for me
to remove this render and then just look
at our results over time right here are
our policy right it's a collection of
directions that our agent should move
over time using policy iteration you can
see how many policies it's it's
evaluated and it's going to keep
repeating right telling and now it's
like okay here's value iteration and see
when each of these converged you could
see the optimal policy derived from
value iteration and yeah that's it
please subscribe for more programming
videos and for now I've got to optimize
my chest hair so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>