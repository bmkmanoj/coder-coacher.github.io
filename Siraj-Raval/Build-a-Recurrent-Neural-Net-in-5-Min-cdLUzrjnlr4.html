<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build a Recurrent Neural Net in 5 Min | Coder Coacher - Coaching Coders</title><meta content="Build a Recurrent Neural Net in 5 Min - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build a Recurrent Neural Net in 5 Min</b></h2><h5 class="post__date">2016-08-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cdLUzrjnlr4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and today we're
going to code up a three layer recurrent
neural network that can predict the sum
of two binary numbers in under 80 lines
of Python but first I want to show you
guys what a genius I am I'm going to
recite the alphabet backwards ready okay
okay
zy4 damn it the reason it's so hard to
repeat the alphabet backwards or
sing-songs backwards is because we learn
these concepts as a sequence and when
data is represented as a sequence
positional memory matters let's say we
wanted to train a neural net on a video
of a top spinning on the table each data
point would be a frame of the video so
if we wanted to predict where the top
would be in the next frame it would be
super helpful to know where the top was
in the last frame sequential data like
this is why we build recurrent neural
networks plain old neural nets expect
fixed size inputs and have fixed size
outputs so the state of the hidden layer
is based only on input data as it all
flows in one direction or feed-forward
in a recurrent net we incorporate the
concept of memory in order to do this at
each time step we input a combination of
the input data and the hidden layer at
the previous time step for cursive Lee
so if we try to predict the next word in
a song that contained phrases like I
like eggs and I like toast
brilliant songwriting I know and our
network tried to predict the next word
how would it know what follows I like it
need to know what part of the song is in
and hidden recurrence helps it do that
we're just using copy to help us copy
things and numpy to do math we're going
to start out by writing a sigmoid
function this function will help us map
any value we feed it into a value
between 0 and 1
it helps convert numbers to
probabilities we'll also write a
function that generates the derivative
of the sigmoid the derivative is the
slope of a sigmoid at a given point the
derivative will help us calculate our
error during training later on next
we're going to create a lookup table
that map's integers to their binary
representations we'll initialize it as
empty and set a max length the binary
numbers will be adding then we'll
compute the largest number possible to
represent with the binary length we
chose 8
our lookup table is then filled with
binary numbers so that we can easily
return the binary value of any integer
we input now it's time to initialize our
input variables first we'll set our
learning rate then our input dimensions
since we will be adding two numbers
together we'll be feeding in two bit
strings one character at a time so we
need to have two inputs to the network
one for each of the numbers being added
hidden dim defines the size of our
hidden layer that will be storing our
carry bit and lastly we define our
output size since we're only predicting
one sum we'll set it to one will
initialize three matrices of synapses
the first synapse matrix connects our
input layer to our hidden layer so it
has two rows and sixteen columns the
next synapse matrix connects our hidden
layer to our output layer so it has 16
rows and one column the last synapse
connects our hidden layer in the
previous time step to the hidden layer
in the current time step it also
connects the hidden layer in the current
time step to the hidden layer in the
next time step well just keep on using
it so 16 rows and 16 columns next we
need three variables to store the
synapse updates for each of the matrices
once we have enough synapse updates will
update the matrices alright let's start
training it to kill humans I mean
predict binary subs yeah
we're going to iterate over 10,000
training samples we'll start by
generating some random addition problem
well initialize our first variable to
use our lookup table to find the binary
form of it and store it in a then we'll
do the same thing for B we'll sum both
integers and get the binary encoding for
the sum C we want to store the neural
networks predictions in an empty binary
array which we'll call D well reset the
error measure to zero every time
this will help us see how each new bit
of data contributes to our predictions
we then initialize two lists they will
keep track of the layer 2 derivatives in
layer 1 values at each time step and
since time step 0 has no previous hidden
layer we initialize one that's off ok
now we're ready to iterate through the
binary representation will generate an
input first this is a list of two
numbers one from a and one from B Y is
the output and is the value of the
correct answer either 1 or 0 now it's
time for the real magic step of neural
networks we construct our hidden layer
by propagating our input to the hidden
layer then we propagate from the
previous hidden layer to the current
hidden layer we sum both of these two
vectors and pass them through the
sigmoid function after both the previous
hidden layer and
put have been propagated through their
various matrices we sum the information
well then construct our output layer it
propagates the hidden layer to the
output in order to make a prediction we
can't forget to compute how much the
prediction mist will store the
derivative in a list holding the
derivative at each time step then we'll
calculate the sum of the absolute errors
to track propagation we'll end up with
the sum of the error at each binary
position will then need to round the
output to a binary value and store it in
the designated slot of D we copy the
layer 1 value into an array so that at
the next time step we can apply the
hidden layer at the current one okay so
we've done the forward propagation for
all time steps we've computed
derivatives at the output layers and
stored them in a list now it's time to
back propagate starting with the last
time step and propagating to the first
we index the input data just like before
and select the current hidden layer from
the list and the previous hidden layer
then the current output layer it's time
to compute the current hidden layer
error given the error at the hidden
layer from the future and the error at
the current output layer yay all of our
derivatives have been back propagated at
this current time step so we can
construct our synapse updates we're not
actually updating them yet those ones
back prop is done then we update our
synapses using the learning rate we
initialize then MTV update variables
let's see some results the predicted
value is pretty off the mark at first
but with each iteration gets better if
you want to learn more about recurrent
neural Nets check out the description
for links and hit that subscribe button
for more programming videos for now I've
got to go synchronize some go routines
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>