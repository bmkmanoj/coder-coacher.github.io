<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make a Neural Network - Intro to Deep Learning #2 | Coder Coacher - Coaching Coders</title><meta content="How to Make a Neural Network - Intro to Deep Learning #2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make a Neural Network - Intro to Deep Learning #2</b></h2><h5 class="post__date">2017-01-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p69khggr1Jo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">how do we learn although times may
change some concepts stay the same
unchanging information outlast the body
it's stored in our brain but can be
passed down from generation to
generation our brain is capable of
synthesizing the diverse set of inputs
we call our five senses and from them
creating a hierarchy of concepts if
we're lucky we can learn a task while
being supervised by a teacher directly
while interacting with our environment
we can feel our surroundings see our
obstacles and try to predict the next
steps if we try at first and fail that's
okay through the process of trial and
error we can learn anything but what is
it that gives our brain this special
capability unlike anything else in
nature everything we've ever experienced
or felt all our thoughts and memories
our very sense of self is produced by
the brain at the molecular level our
brain consists of an estimated 100
billion nerve cells called neurons each
neuron has three jobs receive a set of
signals from what are called its
dendrites integrate those signals
together to determine whether or not the
information should be passed on in the
cell body or soma and then if the sum of
the signals passes a certain threshold
send this resulting signal called the
action potential onwards via its axon to
the next set of neurons hello world is
suraj we're going to build our own
neural network in Python the rules that
govern the brain give rise to
intelligence it's the same algorithm
that invented modern language Space
Flight Shia LeBeouf it's what makes us
us it's what allowed us to survive and
thrive on planet Earth
but as far as we've come as a species we
still face a host of existential threat
to our existence
there's the impending threat of climate
change the possibility of biochemical
warfare an asteroid impact
these are non-trivial problems that
could take our biological neural
networks many generations to solve but
what if we could harness this power what
if we could create an artificial neural
network and have it run on a
non-biological substrate like silicon we
could give it more computing power and
data than any one human would be capable
of handling and have it solve problems a
thousand or even a million times faster
than we could alone in 1943 too early
computer scientist named Warren
McCulloch and Walter Pitts invented the
first computational
of a neuron their model demonstrated a
neuron that received binary inputs some
them and if the sum exceeded a certain
threshold value output a 1 if not output
a 0 it was a simple model but in the
early days of AI this was a big deal and
got computer scientists talking about
the possibilities a few years later a
psychologist named brink Rosenblatt was
frustrated that the McCulloch Pitts
model still lacked a mechanism for
learning so he concedes a neural model
that built on their idea which he called
the perceptron which is another word for
a single layer feed-forward neural
network we call it feed-forward because
the data disclosed in one direction
forward the perceptron incorporated the
idea of weights on the inputs so given
some training set of input-output
examples it should learn a function from
it by increasing or decreasing the
weights continuously for each example
depending on what its output was these
weight values are mathematically applied
to the input touch that after each
iteration the output prediction gets
more accurate the best understand is
process we call training let's build our
own single layered neural network in
Python using only numpy as our
dependency in our main function we'll
first initialize our neural network
which will later define as its own crash
then print out its starting weights for
our reference when we demo it we can now
define our data set we've got four
examples each example has three input
values and one output value they're all
ones and zeros the T function transposes
the matrix from horizontal to vertical
so the computer is storing the numbers
like this will train our neural network
on these values so that given a new list
of london zeros you'll be able to
predict whether or not the output to
deal 1 or 0 since we are identifying
which category it belongs to this is
considered a classification task in
machine learning well train our network
on this data by using them as arguments
to our train function as well as the
number 10,000 which is the amount of
times we'd like to iterate during
training after it's done training we'll
print out the updated weights so we can
compare them and finally we'll predict
the output given a new input we've got
our main function ready so let's now
define our neural network class when we
initialize the class the first thing we
want to do is seat it well initialize
our weight values randomly in a second
and seating them make sure
that it generates the same numbers every
time the program runs this is useful for
debugging later on we'll assign random
weights to a three by one matrix with
values in the range of negative 1 to 1
with a mean of 0 since our single neuron
has 3 input connections and 1 output
connections next we'll write out our
activation function which in our case
will be a sigmoid it describes an
s-shaped curve we pass the weighted sum
of the inputs through it and it will
convert them to a probability between 0
and 1 this probability will help make
our prediction will use our sigmoid
function directly in our predict
function which takes input as parameters
and passes them through our neurons to
get the weighted sum of our inputs we'll
compute the dot product of our inputs
and our weights this is how our weights
govern the attention of how data flows
in our neural net and this function will
return our prediction now we can write
out our train function which is the real
meat of our code well write a for loop
to iterate 10,000 times as we specified
then use our predict function to pass
the training set through the network and
get the output value which is our
prediction
well next calculate the error which is a
difference between the desired output
and our predicted output we want to
minimize this error as we train and do
this by iteratively updating our weight
we'll calculate the necessary adjustment
by computing the dot product of our
inputs transpose and the error
multiplied by the gradient of the
sigmoid curve so less confident weights
are adjusted more and inputs are 0 don't
cause changes to the weight this process
is called gradient descent yeah
descending that gradient Oh we'll also
write out the function that calculates
the derivative of our sigmoid which
gives us its gradient or slope this
measures how confident we are of the
existing weight value and helps us
update our prediction in the right
direction finally once we have our
adjustment we'll update our weights with
that value this process of propagating
our error value back into our network to
adjust our waves is called back
propagation let's demo this baby in
terminal because the training set is so
small it took milliseconds to turn it we
can see that our weight values updated
themselves after all those iterations
and when we set it a novel input it
predicted that the output was very
likely a 1 we just made our first neural
network from scratch anyways about
backpropagation I
and back propagates an update waste
backs off the gates update waste that
propagates update waste that property
update waste the 1/2 nose nap suppose in
one inputs go in and waste get some past
that to my sigmoid function get
that era what's real and prediction and
that's why I use gradient descent and
give direction and it doesn't pretend
update wait and repeat ten thousand
times outputs their list I'll be doing
just fine
so as dopant Rosen what idea was in the
decades following it neural networks
didn't really give us any kind of
noteworthy results they could only
accomplish simple things but after World
Wide Web grew from a third project to
the massive nervous system for humanity
that it is today we've seen an explosion
in data and computing power and a small
group of researchers funded by the
Canadian government helps back to their
belief in the power of neural networks
to help us find solutions from this data
when they took a neural net and made it
not one or two but many layers deep gave
it a huge data set and lots of computing
power they discovered that it could
outperform humans impacts that we
thought only we could do this is
profound our biological neural network
is carbon-based sending electro
chemicals like acetylcholine glutamate
and serotonin as signals an artificial
neural network doesn't even exist in
physical space it's an abstract concept
we programmatically created and it's
represented on silicon transistors if
it's like the complete difference in
mediums
they both develop a very similar
mechanism for processing information and
the results show that perhaps there's a
law of intelligence encoded into our
universe and we're coming ever closer to
finding it so to break it down a neural
network is a biologically inspired
algorithm that learns to identify
patterns and data back propagation is a
popular technique to train a neural
network by continually updating weights
via gradient descent and when we train a
many layer deep neural network on lots
of data using lots of computing power we
call this process deep learning the
coding challenge winner for last week is
Ludo blonde little made a really slick
ipython notebook to demo not just 2d
regression but 3d regression as well on
a crime
change data set wizard of the week and
the runner-up is a medulla Tariq he
completed the bonus with great results
the challenge for this video is to
create a not one not two but three layer
feed-forward neural network using just
numpy post your github link in the
comments and I'll announce the winner in
one week please subscribe and for now
I've got to update my weight so thanks
for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>