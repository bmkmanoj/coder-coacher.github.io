<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Do Sentiment Analysis - Intro to Deep Learning #3 | Coder Coacher - Coaching Coders</title><meta content="How to Do Sentiment Analysis - Intro to Deep Learning #3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Do Sentiment Analysis - Intro to Deep Learning #3</b></h2><h5 class="post__date">2017-01-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/si8zZHkufRY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and today we're
going to use machine learning to help us
understand our emotions our emotional
intelligence distinguishes us from every
other known living being on earth these
emotions can be simple like when you get
so hype all you can hear is gasolina by
daddy yankee and we've invented language
to help us express them to others but
sometimes words are not enough like some
emotions have no direct English
translation for example in German vault
I'm some kites is a feeling experienced
when you are alone in the woods
connecting with nature in Japanese Moana
know a lot it is the awareness of the
impermanence of all things and the
gentle sadness at their passing emotions
are hard to express let alone understand
but that's where AI can help us an AI
could understand us better than we do
analyzing our emotional data to help us
make optimal decisions for goals that we
specify like a personal life coach /
therapist / Denzel Washington but how
would it do this there are generally two
main approaches to sentiment analysis
the first one is the lexicon based
approach we first want to split some
given text into smaller tokens be that
words phrases or whole sentences this
process is called tokenization then we
count the number of times each word
showed up this resulting tally is called
the bag of words model next we look up
the subjectivity of each word from an
existing lexicon which is a database of
emotional values for words
pre-recorded by researchers and once we
have those values we could then compute
the overall subjectivity of our text the
other approach uses mishima if we have a
corpus of say tweets that are labeled
either positive or negative we can train
a classifier on it and then give it a
new tweet it will classify it as either
positive or negative so which approach
is better don't ask me
know yet totally ask me well using a
lexicon is easier but the learning
approach is more accurate there are
subtleties in language that lexicons are
bad like sarcasm it seems to mean one
thing but it really means another but
deep neural Nets can understand these
subtleties
because they don't analyze text at face
value they create abstract
representations of what they learn these
generalizations are called vectors and
we can use them to classify data let's
learn more about vectors by building a
sentiment classifier for movie reviews
and I'll show you how to run it in the
cloud
the only dependency we'll need is TF
learn and I'm using it since it's the
easiest way to get started building deep
neural networks well import a couple of
helper functions that are built into it
as well and I'll explain those when we
get to them the first step in our
process is to collect our data set TF
learn has a bunch of pre-processed data
sets we can use and we're going to use a
data set of IMDB movie ratings I'll load
it using the load data function this
will download our data set from the web
we'll name the path where we want to
save it the extension being pickle which
means it's a byte stream this makes it
easier to convert to other Python
objects like lists or tuples later we
want 10,000 words from the database and
we only want to use 10% of the data for
our validation set so we'll set the last
argument to 0.1 load data will return
our movie reviews put into a training
and testing set we can then further put
those sets into reviews and labels and
set them equal to x and y values
training data is the portion our model
learns from validation data is a part of
the training process while training data
helps us fit our weights validation data
helps prevent overfitting by letting us
tune or hyper parameters accordingly and
testing data is what our model uses to
test itself by comparing its predicted
labels to actual labels so test yourself
before you wreck yourself now that we
have our data split into your sets let's
do some pre-processing we can't just
speed text strings into a neural network
directly we have to vectorize our inputs
neural nets are algorithms that
essentially just apply a series of
computations to matrices so converting
them to numerical representations or
vectors is necessary the pad sequences
function will do that for our review
text it will convert each review into a
matrix and Pat it padding is necessary
to ensure consistency in our inputs
dimensionality it will Pat each sequence
with a zero at the end which we specify
until it reaches the max possible sequin
plane which will set to 100 we also want
to convert our labels to vectors as well
and we can
really do that using the two categorical
functions these are binary vectors with
two classes one which is positive or
zero which is negative yo all those
Becker's got me feeling like a
feed-forward neural net got me feeling
so upset only six sides inputs like a
small number said so I dropped it and
instead I use recurrent net which I made
four sequences like times up over index
back confirm my text into vectors and I
see the inch for retraining word vectors
that are embedded in repin word
numerically we can you generically a
one-to-many mapping that I bring to you
very clear on we can intuitively define
each layer in our network at its own
line of code first will be our input
layer this is where we feed data into
our network the only parameter we'll
specify is the input shape the first
element is the batch size which we'll
set to none and then the length which is
100 since we set our max sequence length
to 100 our neck layer is our embedding
layer the first parameter will be the
output vector we received from the
previous layer and by the way for every
layer we write we'll be using the
previous layers output as its input this
is how data flows through a neural
network at each layer it's transformed
like a seven layer dip of computation
oops I mentioned to 10,000 since that's
how many words we loaded from our data
set earlier and the output dimension to
128 which is the number of dimensions of
our resulting embeddings next we'll feed
those values to our LST M layer this
layer allows our network to remember
data from the beginning of the sequences
which will improve our prediction will
set dropout to point eight which is a
technique that helps prevent overfitting
by randomly turning on and off different
pathways in our network our next layer
is fully connected which means that
every neuron in the previous layer is
connected through every neuron in this
layer we have a set of a learned feature
vectors from previous layers and adding
a fully connected layer is a
computationally cheap way of learning
nonlinear combinations of them it's got
two units and it's using the softmax
function as its activation function this
will take in a vector of values and
squash it into a vector of output
probabilities between 0 and 1 that sum
to 1 we'll use those values in our last
layer which is our regression layer this
will apply a regression operation to the
input
specify an optimizer method that will
minimize a given loss function as well
as the learning rate which specifies how
fast we want our network to Train the
optimizer will use this atom which
performs gradient descent and
categorical cross entropy is our loss it
helps to find the difference between our
predicted output and the expected output
after building our neural network we can
go ahead and initialize it using TF
learns deep neural net function then we
can call our models fit function which
will launch the training process for our
given training and validation data will
also set show metric to true so we can
do the log of accuracy during training
so the demo is we're going to run this
in the cloud using AWS what we'll do is
use a pre-built Amazon machine image
this ami can be used to launch an
instance and it's got every dependency
we need built-in including tensor glow
CUDA lil wayne's deposition video if we
click on the orange continue button we
can select the type of instance we want
I'll go for the smallest because I'm
course still but ideally we use a larger
instance with GPUs then we can accept
the terms in one click next we'll go to
our AWS console by clicking this button
and after a while our instance will
start running we can copy and paste the
public dns into our browser followed by
which is the port we specified for
access for the password
we'll use the instance ID now we're in
our instance environment built with our
ami and we can play with a jupiter
notebook hosted on AWS we'll create a
new notebook and take our code in there
now we can run it and it'll start
training just like that so to break it
down there are two main approaches to
sentiment analysis using a lexicon of
pre-recorded sentiments or using
state-of-the-art but more
computationally expensive deep learning
to learn generalized vector
representations from worse feed-forward
nets accept fixed size inputs like
binary numbers but recurrent neural Nets
help us learn from sequences of data
like text and you can use AWS with a
pre-built ami to easily train your model
in the cloud without dealing with
dependency issues the coding challenge
winner from last week is Ludo blonde
little architected his neural net so
that stacking layers was as easy as a
line of code per layer wizard of the
week and the
runner-up is CGS soon he accurately
modified my code to reflect multi-layer
backpropagation the coding challenge for
this week is to use TF learn to train a
neural network to recognize sentiment
from a video game review dataset that
I'll provide details are in the readme
post your github link in the comments
and I'll announce the winner in one week
please click that subscribe button if
you want to see more videos like this
check out this related video and for now
I got to figure out what the f hi court
is so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>