<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Autoencoder Explained | Coder Coacher - Coaching Coders</title><meta content="Autoencoder Explained - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Autoencoder Explained</b></h2><h5 class="post__date">2018-01-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H1AllrJ-_30" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and there are so
many different neural architectures that
I feel like I could just talk about them
all day on YouTube dreams do come true
an autoencoder
is a neural network that's not just
really useful for a lot of tasks it's
also an easy entry point to learn more
complex concepts in machine learning
let's go over some theory and code in
order to grasp this model capable of
everything from image colorization to
dialogue generation fully if we've got
data that's properly labeled be that
images or audio or text we're in luck
deep learning works really well with
labeled datasets that's because there is
always a function that represents the
relationship between both columns it's
easy to conceptualize this if our data
set is numeric like if our input data
was a bunch of numbers and the labels
defined whether or not that input data
was an even number or an odd number the
function that represents the
relationship between these two columns
is simple if the input data is divisible
by two the number is even else it's odd
all data types be that video or text can
be represented numerically and as such
there is always a function that map's
the relationship it's just a more
complex function than the one we just
discussed so while it's kind of
incredible that we can speak to our
computers now and they're able to
transcribe what we are saying be that
Siri or Alexa or Google now okay Google
do you love me hahahaha no speech
recognition is just a result of deep
learning on labeled datasets if a team
of developers is trying to create a
speech recognition engine they use a
data set of audio clips with their
transcripts as the labels every single
byte of the audio can be broken down
into a series of numbers and so can the
text transcripts some combination of
operations will convert the input to the
labels and that combination is the
function neural networks can slowly
approximate or get closer and closer to
this function through an iterative
optimization process also called
training in short it's minimized
an error value at every iteration so
that given a novel audio clip it can
easily predict what the transcript for
it would be deep learning is essentially
performing a to be mappings that's it
a more accurate way to say this is that
it's performing universal function
approximation meaning with sufficient
data it can approximate any function
input a loan application output the
likelihood a customer will repay it
input an email and output the
possibility it's spam or not spam input
usage patterns for a fleet of cars and
output where to send a car next like the
dumpster if it was made by GM since
there are an endless array of
applications for this deep learning has
gotten really popular but while deep
learning is good at finding a function
we don't already know but have training
data for it's surprisingly useful to
find a function we already know and then
look at how we found it all neural
networks are composite functions that
means they are functions of functions
the more layers a network has the more
nested functions it has for a three
layer network we multiply the input by
the first weight matrix apply an
activation function to it and repeat the
process once again this time using the
output as our new input input times
weight activate the result is our output
this can be represented as a composite
function since we're using the output of
the first function as input to the next
function but let's say our goal wasn't
to find a label Y but instead to
reconstruct the original input X meaning
if our input was an array consisting of
a few numbers our network should output
that same input with those same exact
numbers after applying a series of
operations to it we can call the first
part of the network that compresses the
input into fewer bits the encoder and we
can call the second part that
reconstructs the image the decoder so
why should we care about doing this well
we don't care about the output it's just
a replica of the input what we care
about is the hidden layer
the network can reliably reconstruct its
input the hidden layer must contain
enough information to represent the
output if as is typical the hidden layer
is smaller than the input and output
layers what it represents is the same
information in a lower density it's a
much more dense representation of the
input data one that is learned over time
although it turns out that there are
better techniques for data compression
autoencoders are still really useful for
some tasks like dimensionality reduction
once we have a more condensed
representation of some multi-dimensional
data we can easily visualize it in just
two or three dimensions for further
analysis we can also use it for
classification the idea is that we train
an autoencoder to reconstruct its
instances of a particular class we don't
train it on any instances of any other
class then to classify new instances we
feed them to the auto encoder at the
input layer data reconstruction in the
output layer and compute the
reconstruction error which is usually a
measure of distance between the
reconstruction and the input if it
generalizes to a new instance and we
constructs it properly then it's likely
to be of the same class as the instance
it used a train on anomaly detection is
another use case we first train it on
normal instances so that if we feed it
any anomalies they'll be detected easily
if we train it to recognize anomaly
instances in our training set it would
only find the ones that look like
anomalies it's already seen in many
cases we have very few anomalies in our
training that set but when using an
autoencoder this isn't a problem
so auto-encoders are just neural
networks where the target output is the
input we don't actually need any new
code if we're just using a super simple
scikit-learn like interface we'd simply
train our model by changing a single
parameter instead of modelled bit X&amp;amp;Y we
just say model dot fit X and X all the
usual training strategies work with
auto-encoders including back propagation
and regularization and dropout it's fun
to take an existing neural network
library and see what kind of low
dimensional representations we can come
up with
if we were to build a really simple
auto-encoder using the Kaos deep
learning library to reconstruct a
training set of images notice how a
single fully connected neural layer acts
as the encoder and as a decoder and this
model is just a simple neural network
we're only calling it an auto encoder
because we're feeding it the input data
as the labels since we don't have any
labels the reconstructed results will
look very similar to the original input
data after we're done training sometimes
though the model could over fit on the
input data and in order for it to be
able to learn a better more robust
representation to the input data we can
manually add some noise and this is
called a denoising auto-encoder the
amount of noise to apply to the input
usually takes the form of a percentage
there are so many different types of
autoencoders we could make but one in
particular that I really like is called
the variational auto encoder this learns
a latent variable model of its input
data instead of letting the network
learn some function we're learning the
parameters of a probability distribution
that models our data then we can sample
points from this distribution and
generate new input data samples meaning
a VI e can be considered a generative
model this lets us create all sorts of
new images and videos that have never
existed before image colorization
chatbots V AES are right up there with
generative adversarial networks as one
of my top five favorite deep learning
models three points to encode in your
biological neural network from this
video neural networks can slowly
approximate any function that map's
inputs to outputs through an iterative
optimization process also called
training if we set the output to be the
same as the input we can call this
neural network an auto encoder because
it encodes a more dense representation
of the input data and there are many
types of autoencoders we could make a
more recent generative model is called
the variational auto encoder which
learns a latent variable of its input
data this week's coding challenge is to
create a simple auto encoder using the
chaos deep learning library poster of
github link in the comments section and
the winners will be announced in one
week please subscribe for more
programming videos and for now
I've got to solve AI or die trying so
thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>