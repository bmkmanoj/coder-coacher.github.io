<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction to AI for Video Games | Coder Coacher - Coaching Coders</title><meta content="Introduction to AI for Video Games - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction to AI for Video Games</b></h2><h5 class="post__date">2017-11-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/i_McNBDP9Qs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and welcome to my
new AI for videogames series for the
next 10 weeks I'm gonna teach you how a
really popular subfield of AI called
reinforcement learning works and we're
gonna be using all kinds of game
environments to implement the theory
behind the algorithms we learn the
majority of the recent advancements in
AI have been due to the use of
supervised algorithms namely neural
networks applied to big datasets and
using lots of GPUs for computation
neural nets have been around since the
50s and they're considered universal
function approximate errs that means
that given any set of inputs and outputs
if given enough examples they can learn
the function the mapping that relates
both of them together we can then use
this function to predict new outputs
given some set of inputs everything from
medical imaging for hospitals to
license-plate detection for governments
to identifying crop yields for farmers
these incredible applications of deep
neural Nets have been due to the fact
that the data set used to train the
algorithms had an Associated set of
labels a lot of times though if labels
aren't available data scientists will
hire humans to hand label their data
sets using services like Amazon's
Mechanical Turk but ideally we don't
need labels we can just train our
algorithms on unlabeled data since the
vast majority of the world's data does
not in fact have labels so if we want to
train algorithms unsupervised meaning no
labels then we can use techniques like
clustering and anomaly detection these
are fast improving but there's also room
for another class of learning techniques
that are based on trial and error in an
environment setting this is called
reinforcement learning the basic idea is
that in reinforcement learning the
labels are time delayed and instead of
calling them labels we call them rewards
while supervised learning
tells you how to achieve your goal
reinforcement learning tells you how
well you achieved the goal there are
lots of problem settings where the idea
of time delayed labels makes more sense
think about if you were tasked with
creating an AI that learns how best to
control the temperature in a data center
how are you going to tell your algorithm
what the correct setting of each
hardware component is at any given time
step using reinforcement learning you
can use feedback data such as how much
electricity was used at a certain time
period or the average temperature this
is literally how Google reduced the cost
of cooling its data centers by a massive
amount so it's the real deal
unlike magic leap the human brain
probably implements all three of these
learning paradigms together the
neocortex could be similar to a stack of
auto-encoders learning low-level
perceptual feature detectors close to
the sensory input with no correction
signals these networks possibly get
fine-tuned by top-down feedback in a
supervised manner using immediate and
reliable correction signals and the
supervision and behavioral control
mechanisms could be largely learned via
reinforcement learning with plenty of
different reward signals but who knows
right the only way to unlock the secrets
of intelligence are to do some
experimentation because reinforcement
learning revolves around using rewards
in some kind of environment instead of
using a premade data set robotics has
been a testbed for
RL research for decades there have been
several successes in getting RL agents
to learn to play sports navigate a
helicopter autonomously gain robots to
walk and getting them to full of laundry
we've created some seriously capable
robots that are theoretically able to do
any task a healthy human could do but
the reason they're still so limited is
because of software robotics is a
software problem not a hardware problem
in parallel to the robotics world game
environments have also been a testbed
for
RL since they are safer than the real
world
the barrier to entry is just having a
laptop so anyone can test out their
algorithms two of the most popular AI
research institutions in the world
open AI and deep mind extensively use
game environments to train and test
their algorithms and their world-class
algorithms like alphago zero and the
dota 2 BOTS
both of which beat world-class players
for the first time ever heavily used
reinforcement learning so let's say we
have some game environment the simple
game of tic-tac-toe where the goal is to
be the first to successfully create
three in a row and we have our AI which
we'll call an agent our goal is to have
this AI learn how to become really good
at playing tic-tac-toe against humans
rather than just hard coding in a bunch
of if-then statements how can we
formalize this problem that's where the
mathematics of algebra and probability
theory come in we can use the variable s
to define a set of game States these are
all the possible configurations the
board can be in at any given time step
during the game we also have a which is
a set of actions that our agent can take
in this case it would be the position on
the board they'd like to place their X
on P represents the probability that a
given action a in a given state s will
lead to another state it's a measure of
how likely a board state will be a
certain way after the agent plays a
certain move or represents a reward
value it's what the agent gets after the
game transitions from one state to
another after a given action this is the
time delayed reward we were talking
about the signal that will tell the
agent that the action it's taken is
either good or bad which it can then use
to further improve itself
lastly we'll have a discount factor
which represents the difference in
importance between future rewards and
present rewards we can call this
formalization a Markov decision process
it's a way of framing a problem where at
each time step an agent is in some state
and may choose some action this will
probabilistically transition the agent
into the next state and
some reward as a function of the
transition crucially the transition
obeys the Markov property meaning the
transition and reward probabilities are
only dependent on the pair s and a and
not dependent on the entire history of
previous states in other words the state
s should encode all of the important
information to be able to make good
decisions on which action to take
so reinforcement learning is a method of
solving this kind of process and we can
break down RL methods that do this into
two groups policy iteration and value
iteration methods policy methods perform
a search in a policy space and value
methods try to estimate the value
function the value function is a
function that tries to estimate the long
term utility of either a state or a
state action pair and the agent just
selects the Arg max action over this
function it represents how good is a
state for an agent to be in it is equal
to the expected total reward for an
agent starting from state s the value
function depends on the policy by which
the agent picks actions to perform among
all possible value functions there
exists an optimal value function that
has higher value than other functions
for all states the optimal policy is the
policy that corresponds to the optimal
value function value function algorithms
in reinforcement learning generally
follow the current policy as defined by
the current value function and whenever
a reward is received the agent
propagates it back through its history
assigning each action sum of the reward
over time this gives the agent a good
idea of what states and actions result
in good rewards so value iteration
computes the optimal state value
function by iteratively improving the
estimate of V of s both value iteration
and policy iteration algorithms assume
the MDP model is known by the agent
comparing to each other policy iteration
is computationally efficient as it often
takes considerably view
a number of iterations to converge
although each iteration is more
computationally expensive so there are
three important points to remember from
this video reinforcement learning is a
technique that lets a eyes learn to
complete an objective in an environment
using time-delayed labels aka rewards as
a signal we can formally call this a
Markov decision process which relates
States actions and rewards foreign agent
in two fundamental ways of solving MDPs
is by either using a value iteration or
policy iteration algorithm this week's
coding challenge is to create a simple
value iteration algorithm for an AI
using the open AI gym environment I'll
announce the top two submissions next
week so make sure to post your github
links in the comment section and I'll
review them personally please subscribe
for more programming videos and for now
I've got to go hack my Nintendo switch
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>