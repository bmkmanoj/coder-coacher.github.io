<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Generate Music with Tensorflow (LIVE) | Coder Coacher - Coaching Coders</title><meta content="How to Generate Music with Tensorflow (LIVE) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Generate Music with Tensorflow (LIVE)</b></h2><h5 class="post__date">2017-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pg9apmwf7og" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj how's everybody
doing I'm in this new space I'm at the
upload VR space
so I've got this whole code behind me
thing so it's not all ghetto with me in
Google Hangouts so I'm very excited
about that
let's see who is in the chat room and
let me give my shoutouts to people who
showed up alive and then we're going to
do our 5 minute Q&amp;amp;A okay so we've got
Brandon we've got a bleep got three
Harsha kepala Aditya Anuj Abe a lot of
cool people in the house right now okay
we are doing this legit okay I am done
with the Google Hangouts I am here to
make this quality for you guys okay
Mohan we got a lot of cool people in
here right now okay uh so Ellery this is
awesome I know it's very awesome so I
was inspired in part by Dan shiffman's
livestream you know the way he does it
and so we're gonna have the code happen
behind me okay so I'm here this is my
computer we're gonna have the code
happen behind me okay
I finally got into this yes thank you
I'm glad you guys like the new setup
okay so uh Santiago in vineet and Jai
and I'll say one more name Joyce okay
that was it for the names
okay one more name uh there's so many
notes so I'm going to choose done DAA n
that's a cool it's a cool name Don and
he said konichiwa bitches great okay but
women are not that okay so now we're
going to get started with the code okay
let's do that well actually let's do the
five mini Q&amp;amp;A what am I think about so
shoot me your questions and then we're
going to do this okay we're going to
generate some music we are going to
generate some music it's going to be
dope we're using tensorflow to generate
music okay tensorflow music generation
is no joke okay we are using a
encoder/decoder model for this okay so
what are you using for this I'm using a
open broadcaster service OBS to stream
this okay
yeah okay I gave Shipman a shout out
great quality thank you okay
questions about machine learning guys
come on questions about deep learning
what's been burning your tensors in your
mind okay what do we got here I know I
know we're gonna we're going to get some
good questions
mini are you going to use Gann I'm not
going to use again but we can use again
for music generation that is some very
very new stuff okay I am definitely
gonna do a live so I have had so many
requests for Gans in fact I'm not just
going to do one game video I'm probably
going to do like four or five okay
because there is a lot of stuff to talk
about back propagation with tensorflow
ROG back propagation video specific back
propagation is coming within a week okay
or two weeks but it's coming the music
okay what else can we use similarity
detection on music sets absolutely so
remember if we take any data set and we
convert it into a vector we can then
find the similarity between these
vectors because we were representing it
mathematically okay these are matrices
okay any sessions for again okay LST m
yes we are using LST MS because we are
trying to remember long-term
dependencies okay music is very long it
can be multiple pieces it could go
through multiple phases I will do Jan's
genre for this music it's going to be
ragtime music okay it's going to be
piano ragtime music okay I'm going to
take two more questions and then we're
going to get started with this code
which is the best server to use to run
Python code all the time for sending
some automated something it just went
away uh the best Python server the best
server app would be Heroku in my in my
opinion Heroku is the one Heroku is
pretty reliable okay so two more
questions can we work on multiple data
set to arrive at one result uh yes you
can you would want to create one big
data set using all the different data
sets okay and then so one more question
what technical skills should I know
you should know
so this specific one we're going to need
to know a little bit of music theory but
I'll talk about that okay and also a
tensorflow basics and syntax okay so
that's it for the questions we have 307
people in this livestream okay so we are
about to get started right now let's do
this
deep reinforcement learning not today so
okay what are we going to do here so I
was looking on github for a bunch of
music generation code and I didn't so I
found there there are some that uses
tensorflow specifically so there are
some but the problem is that a lot of
them don't compile a lot of them don't
compile and why is that because for
music specifically music doesn't have
good libraries in Python right now that
we haven't it's not that we don't have
good music it's not that the bridge
between music and computer science
hasn't been built it has we've been
working on this stuff for 40-plus years
the the bridge that hasn't been built is
between up is between Python 3
tensorflow and Python music libraries
there is magenta but magenta is ok so
first of all about magenta I admire
magenta let's let's look at magenta
right let's look at let's look at
magenta for a little bit so Google has
this effort to uh talk about music
generation in tensorflow and I really
admire magenta but what I think magenta
needs is better documentation I mean to
me I mean when I see this it's just like
run this you know it's essentially like
a look at this MML generate it's
essentially like a bash script it's like
a bash script ok and you you don't want
a bat I mean if you're going to make a
bash script you might as well have a web
app what we want is modular code like
some simple examples similar to how
chaos has simple examples maybe 10 to 15
lines of code so while I admire magenta
I think we need better documentation for
it okay so in the models folder for
magenta for the nth melody RNN it has
you generate a melody right so okay so
this is the file melody RNN so
so let's let's take a look at melody RNN
for a second that melody RNN module we
have let's see this code is showing up
behind me isn't that so cool guy this is
amazing let's so there's a lot of stuff
happening here so I mean look at this we
have magenta protobuf generator magenta
music dot one hot event sequence
encoder/decoder
there's a lot of stuff that is is
encapsulated and abstracted away where
is the actual RNN where are the hyper
parameters where are the layers okay
there is too much code for simple models
okay I agree with Brannon there is too
much code here this is not a problem of
magenta alone this is a problem of all
music machine learning models this stuff
is not trivial because we are talking
about music theory here there are people
who study music theory full-time who
don't fully understand how to compose
music by there are human minds ok and
yet here we are doing it with machines
but we can definitely do it it's
definitely possible ok so that's enough
about magenta let's talk about the code
that we're going to use so I found this
code okay and there's so much code right
there's so much code here so uh so I
found this code and this is what this
guy tried to do ok so what he tried to
do and we're going to write code okay
we're going to write code but before we
write code I'm going to talk about I'm
going to talk about what he did here so
his first idea was to try a basic
recurrent Network and why did he try
that because it's a sequence of notes
right sequence notes we use recurrent
Nets it makes sense predict the next
sequence okay that that makes us that
makes sense right
so he tried that okay he's a basic
recurrent net and he was he was inspired
by on drake her Papi's blog post the
unreasonable recurrent that's by the way
andre karpati andr
EJ Karpov he is amazing and you should
definitely look up anything he's written
that guy's awesome okay he has he knows
his so he tried a basic recurrent
Network and he was we're not using care
or using tensor flow he used a basic
recurrent network using two LS TM cells
and he trained it on ragtime music and
let's listen to what he generated using
just three songs okay
so he just trained it on three songs
let's listen to this
you
so it sounds great right it actually
sounds pretty good the problem is that
it was over fit in fact this song that
was generated was an exact almost an
exact replica of one of the songs that
it trained on and we that is overfitting
right so that is a problem we don't want
to over fit we don't want to just repeat
whatever we've learned we want to create
novel music so what he did was he said
okay instead of giving it three songs
let me give it 400 songs so then he gave
it 400 songs okay and so when he gave it
400 songs this is what it generated
okay so that no one's not that bad
actually using a recurrent using a
recurrent network with 2 LS CM sells on
400 songs it's not that bad but there
was a problem so the problem that he
found was that when he even when he used
400 songs
the network still didn't get the
relationship between notes so in music
theory so let's look at some music for a
second in music there in music we have
scales right it's music sheet music
we have scales right so notes are
related to each other each note is
related to each other so we start off
with a note you know I look at this
first note here right so this is a C and
then the next note is a B so it's down
one one half-step okay this is a short
little lesson on music theory these
notes are in relation to each other
they're just ways of mapping out pitches
they're ways of mapping out sound waves
okay and this is how we write out those
differences in in sound waves those
differences in pitches what his model
did was it couldn't recognize that there
are that one note is actually related to
another note okay so this is actually
one half-step down on one step up and he
and this was that means that if he
inverted the notes it would still
predict the same thing so you know if it
was C then B it was it would output
something that was similar to as if we
were to input B and C so even if we
inverted the notes it would still still
output the same prediction so to fix
that he thought okay how can I improve
this model even more so then he said
okay so to improve this model even more
we're going to use a different
architecture entirely we're going to use
an encoder decoder architecture okay so
I haven't actually talked about this
before and I will the next uh you know
to weekly videos are going to be talking
about the encoder decoder architecture
but to give a brief overview just for a
second and I'm sure most of you actually
know what this is the encoder the
encoder decoder architecture takes two
recurrent networks okay so look at this
green set of squares and this yellow set
of squares
there are two different recurrent
networks two different LS TM or current
networks and each serves a different
purpose
one is the encoder okay the screen one
is the encoder it takes an input
sequence and we feed it into this
encoder okay the second one the yellow
one is the decoder so you might think
okay so we just take that encoders
output and we input that into the
decoder no we don't take the output
because this is not what we do is we are
trying trying to take the hidden state
okay so if you look at this fourth
square right here and it goes up and it
says W we were trying to take that
hidden state because we're not trying to
take the output because it's not a
classification we don't care about
classifying we just want that hidden
state from the encoder we take the
hidden state which is a set of vectors
that it learned feature vectors and we
take that hidden state and we put it
into our decoder okay so we put it into
our decoder we feed that we feed the
state itself into our decoder and then
once it's once we feed that input into
our decoder the decoder will then take
that feed that vector and then decode it
into a okay so the first we take the
first hidden state and we put it into
the decoder and the decoder takes the
hidden state and it outputs the next
what would be the next note in the
sequence or the next value in the
sequence whatever it is and so why do we
do this why why have an encoder decoder
architecture in general instead of
having just one recurrent Network
well this for the same reason that we
have different networks for different
tasks like we have convolutional
networks that do you know a certain task
we have a that that look at that look
for images we have recurrent Nets for
sequences we have feed-forward nets for
binary classification and think about
even our brain our brain isn't just one
neural one type of neural network it's
several types of neural networks so
having an encoder decoder architecture
lets these networks specialize and
having them specialized increases the
predictive capability of these networks
because they are so specialized on a
task
okay so because they're so specialized
they perform different tasks well okay
so that's that's the the high level and
it and so he tried that and it worked
even better so that's the high level of
what he did okay and it was too
recurrent networks one wasn't in color
and one was a decoder and now we're
going to look at the code so there's
actually a lot of code so this so
there's there's a lot of code here and
you know four different modules one for
the connector and for the keyboard but
what we're going to do is we're going to
just write out the meat of the code okay
we're going to write out the model
itself and then we'll talk a little bit
about more about what else is happening
here okay so let's start out by just
writing out the model itself let will
write out this encoder decoder model so
let's start off with importing our
dependencies here let me make sure that
this is visible okay and remember the
code itself is in the the code itself is
in the description so let start off with
our dependencies okay we're going to
start off with a few of his dependencies
that he loaded here and I'll explain
what each of them does so okay so okay
so this module loader is going to do
several things we'll talk about the
module loader when we get to it but this
one this next helper class is going to
predict the next key the next key in
this in the sequence okay so we have
hello from hongkong okay great hi and so
we have our keyboard cell and we're
going to use our keyboard cell so what
this does is it just predicts the next
key and we'll talk about that more and
we have one more helper class which is
going to encapsulate the song data and
so that we can run things like so we can
run get scale we can run get relative
note we can run a bunch of methods on
our data okay so this is kind of a data
pre-processing this is a data
pre-processing step okay thanks Jerry
okay so he is the original author of
this code okay so this is the best code
that I
that I found okay so okay so let's let's
start off with this so we're going to
import deep music using song struct as
music so let's let's write this out okay
so these are our these are our only
dependencies and they're there's not a
paper there is a write up I'm going to
link to that when we get to it those are
our dependencies let's write the code
okay do we have to pick these up yes
we're gonna have to pick them up but not
be specific
these are helper classes you'll have to
pick these but you do have to pick
tensorflow
and the cons whatever the name is in the
readme of the code in the description
okay so let's do this let's write out
our code where was I
okay okay here we go with this okay
oh there's a what else we got oh there's
actually two more dependencies we have a
numpy because we're going to generate
random numbers generate random numbers
and there's of course tensorflow alright
tensor flow for flow wing okay so the
first thing we're going to do is we're
going to write out the method for
building the network okay actually Dave
Patel looks like me it's the other way
around
okay so let's build let's build our
network and let's build our network so
the first thing we're going to do is
we're going to create our computation
graph we're going to create the
computation graph and it's going to
encapsulate our tensor flow session and
the graph initialization okay so we're
going to initialize our graph and so any
time we use tensor flow we create a
session and a graph right these are just
basic initialization steps we do it
every time and the great thing about
these help these helper libraries is
this guy use the module loader to build
this batch builders dot get module okay
so what does this do this this line
right here basically just encapsulates
that graph creation and that that
session creation okay so that's the
first step
so we
our computation graph initialized now we
want to start building our model okay so
before we build our model or we need to
create our placeholders right these are
our gateways for data to flow into the
network so we'll create our first
placeholder okay our first placeholder
is going to be for the inputs okay
and these are going to be the notes
themselves this is for our notes okay
this is for our note data we'll just
call it no data
okay so self input and remember this is
in MIDI format
it's MIDI format all right so for self
inputs these are the inputs to our graph
TF dot placeholder how can I get deep
music I'm in math class it's okay Bianca
all look what my brain is not even
working today it's all good okay it just
take it in cerebrally take it in and
just subconsciously you're going to
start to get this stuff this is this is
not trivial stuff but just follow along
and we're going to we're going to do
this okay so so now let's let's talk
about this
so we have our placeholder and the first
thing we're going to do is we're going
to say what what type of data do we want
to feed this well this is going to be an
integer data right this is numerical
data because new MIDI me I can't believe
I called a MIDI format on the video but
thanks for correcting me some guys
corrected me so that was good I know one
that I've never actually heard it said
before so it's like mid aya MIDI
whatever so it's numerical data so we'll
use a float 32
okay we're using a float 32 for this and
the next thing we're going to do is
we're going to say what how much data do
we want to use and so these are our up
so these are placeholders for how much
data we want to give it to like what is
a size like how many the batch like how
many so batches are like if we have a
basket of data how much do we take out
at a time how much do we take out at a
time and feed into a model those are our
batches and we'll define this with our
batch size okay which is going to be one
of the arguments
ah but we we are not going to talk about
that right now
right yeah mid I I'm glad you like that
Brandon okay so okay where were we so
that's our that's how much data and then
we have what about we've got one more
thing actually we've got the input
dimension and then we've got the name
well what are we going to call it we're
going to call this input we're going to
call it input implored sense flow as TF
thank you
Aditya exactly and the input and okay so
that's it for our placeholder input and
so now that's so that's our note data
now we're going to define our targets
okay yes okay now we're going to define
our targets what are our targets they
are the classes okay so these are these
are whether or not a key was pressed so
this is a binary classification part
athe binary classification problem okay
so each of and this is not just this is
not inherent in the MIDI data this is
just what we are feeding it we are
saying there are 88 keys on a keyboard
right we're looking at piano music and
is that key pressed or not if it's
pressed it's a 1 if it's not pressed
it's a 0 okay so that's what we're going
to define for our target values so we'll
say with TF dot oh it's not TF that name
actually it's TF name scope and name
scoping is a way for us to encapsulate
some a TF object and give it its own
name so we can reference it later on
spell check on song struck thank you
struct as music okay so with TF dot name
scope we're going to say this is going
to be our placeholder targets remember
targets are for the the labels we're
going to consider this a supervised
classification problem using an encoder
decoder architecture so so let's go
ahead and talk about the targets that
we're going to use for this one so for
this one we're going to say okay we'll
use a TEFL placeholder like before and
it's also going to be an int it's not
going to be a float it's going to be in
32 and why are you using an
32 instead of a float 32 because it is
going to be either a 1 or 0 so it's less
memory to use an INT than it is to use a
float also useful for tensor board ok ok
so where were we
TF in 32 so right so it's going to be
either 0 or 1 and so what else do we got
here we got TF 32 we have 2 sighs so
self dot batch size okay and that's the
size of our and input this actually goes
over here okay okay so what else we got
here we've got the target that's the
name of this thing right the name is the
target oh my god there is so much naming
happening here yes siree equal sign
thanks Nick where's the equal sign
there's an equal sign somewhere isn't
there whatever okay okay so yeah all
right great so that's that and okay so
that's that and wait to latina are you
serious they're not four bytes each in
different languages it's in different
languages it's different for what it is
float values are they take up more space
generally self targets equals self
targets
yes that is the one Thank You Nick that
is an equal sign okay so okay so guess
what we're going to use one more
placeholder because this is a recurrent
Network we are not just feeding in our
inputs what else do we feed in to our
recurrent Network we don't just feed in
the input data one we're training we
also feed in the drumroll please think
about it hidden state our previous
hidden state we are feeding in both our
input data and the previous hidden state
there are two things so we're going to
create one more placeholder because this
is a recurrent neural network and not a
feed-forward network so we'll use the
placeholder for and they'll call it
youth breathe okay
you should definitely bet on AI when
choosing a career because you will not
you will definitely get employed wall
you will get employed so
we'll say okay so we'll use the previous
and I did import STF okay so I guess
float and int are both four bytes so I
was wrong but if you use a short it is
two bytes okay but don't quote me on
that checkup you know we could argue
about this later we'll see what the you
know post a link or something anyway so
so okay so the for the placeholder for
the placeholder value this is going to
be TF top boolean should we use the
previous value or not yes or no okay and
then the name will leave the size empty
because there's not really a size
associated with the boolean and then the
name is going to be use pre use previous
right yes okay so those are our three
placeholders in-state okay our hidden
state our note data and then our target
values these are our labels okay what
else what else got D keys we got people
wrapping in this chat so we have
everything happening right now it is
chaos it is absolute chaos but that's
what we like we are agents of chaos in a
good way creative chaos constructive
chaos so the next step is to define our
network we are going to define our
network we have our placeholders and
we're ready to build the hell out of
this model I can barely program in front
of another person you're a brave man for
live-streaming Suraj Shotgun thank you
and you can it just takes practice it
just takes practice like all other
things we have to train our minds to do
these things
okay self dot loop processing module
loader what the hell am I writing what
am i writing does anyone know I don't
even know what I'm writing right now you
know it's just it's crazy it's crazy I
know what I'm writing what this is is
it's a part of the module loader class
and so what this is is we are going to
create a loop manually okay because we
are not using care offs we are using raw
tensorflow could you please explain what
is going on
so that is what I am trying to do so
what I'm going to do is I'm going to
continue doing what I'm doing and maybe
I'll even try to explain harder so I'll
turn up my explanation so okay okay so
what we're doing is we're going to write
a loop function and the loop function is
used to connect one of the outputs of
the network to the next input it is the
actual loop so it is that code that
takes that output and feeds it back in
to the the input placeholder so we're
going to manually create this loop okay
we're going to manually create this loop
this is too deep for me why is your hair
black we have some amazing comments in
this so okay very entertaining so so you
get the code you get the entertainment
this is this is what it's all about
these are why these live streams are
awesome okay so let's see what are we
going to feed this and we're going to
feed in the previous value and we're
going to feed in the current value okay
the reticular the two values for our
loop okay for our loop what is after
self all right okay ARDS we don't so
that's that's actually for the command
line so we could define if we want it to
be a loop or not but that's not the case
here so let's not even worry about that
we're not doing it from the command line
or the command line is we already have
default values for that so now what
we're going to do is for our loop we're
going to say well we'll take our next
input and we're going to use this loop
process processing method to take in the
previous value so we'll take in the
previous value to predict the next input
okay and hey Suraj what age did you
start coding when I was 14 I was this
script Kitty who would try to do
whatever I could and what was cool at
the time was to deface like forums a not
be face but like you know D face it was
so stupid when I was 14 also hacking
halo2
and stuff like that so that was that was
when I first started but when I really
started coding was like when I was I
think uh twenty so six years ago so you
know you know we all have to find our
way
but now I'm full white hat I'm brown hat
actually ok so let's return some so so
we're going to use the TF conditional
object of tensor flow and what this is
is it's a conditional statement so it's
going to either return one or the other
of the parameters that we give it and
I'm going to talk about what we're going
to feed this in a second so so I'm not
going to get distracted okay I'm not
gonna get distracted so we're going to
feet let me just let me just type this
up and I'm going to explain exactly what
I'm typing up so the lambda value here
and we have one more input and that is
how okay and then we have one more okay
so okay so on so on training we're
forcing the correct input and on testing
we use the previous output as the next
input so this is a conditional depending
on on training versus testing okay
training versus testing okay a training
versus testing so that's what that's
what this is it's it's it's a
differentiate between training versus
testing but we're just focused on
training so we don't even care about it
being conditional right now we're just
focused on training lambda function
lambda parameter so many comments I wish
I could just respond to them all but
time is of the essence isn't it back
propagation through time back propagate
to update Waits
okay so now we're going to do the
sequence of sequence now this is the
cool part this is the actual model part
we have defined our placeholder values
we have defined our loop function okay
and and so now we're going to build our
sequence of sequence models so we're
going to build our sequence to sequence
model this is it this is it guys this is
it for the sequence of sequence model so
tensorflow fortunately for us has a
sequence of sequence function built in
okay it has a sequence of sequence
function
Elton and we are going to talk about
what it is doing but let's let's write
out what it's going to output first it's
going to output its when to output our
prediction value what is our next note
and we're going to minimize the loss
here and I'll talk about what the
minimizations okay exactly exactly
questions happen at the end of the live
session remember all your questions I'm
going to answer them at the end of this
session we're focused right now so this
is going to output our our predicted
note in our final state okay and the
final state is a final hidden layer for
our decoder network and so on okay
so let's build this thing so we've got
our sequence to sequence model okay and
we're going to build our decoder and get
okay so we're going to build our decoder
and I'll talk about our encoder in a
second so the decoder is going to take a
series of parameter inputs so the input
is going to be are the inputs that came
out of our encoder okay the the inputs
that came out of our encoder that our
encoder and the initial state is none
because it's defined inside of the
keyboard cell let me because defined in
keyboards so I'm going to explain this
in a second let me just type this out
and Excel is going to be keyboard cell
it's going to be a little confusing as I
type this I've got one more line to type
and then we're doing this we're doing
this live as bill o'reilly said we're
doing it live loop r and n loop RN n
that's what's up okay that's our
sequence of sequence model right there
okay we didn't define a loss function we
didn't define our optimizer but with
this one line we said tensorflow we want
to use your built in sequence of
sequence model that is two recurrent
networks that we could just write out
every layer manually we could write out
the weights we could write out the each
of the you know computations happening
at each layer we could write out the
activation functions but tensorflow has
itself encapsulated this for us and so
we're what the reason we are giving
we're defining a decoder here is because
our encoder was already defined and our
encoder was defined in our helper class
and also by this loop function that we
just wrote so the input to the decoder
is going to be the output from the
encoder okay what does even have okay
okay so guys I also have another code
sample that I'm going to show you at the
end of this that's going to make even
more sense than this okay but it doesn't
compile that's that's the problem
because some dependency issue with the
mid ah I almost said it again the MIDI
the MIDI uh Python module okay I'm going
to show you another piece of code at
this because piece of code for this okay
also I have videos on music generation I
have to and build an AI composer and
generate music intense your flow okay so
okay I don't understand a single line
beginner that's okay that's okay
eventually it's all going to make sense
it's time from now we're going to do the
the training step so is it okay hold on
okay
okay so now the training step so for our
training step we're going to say let's
define our loss function training step
so let's define our loss function let's
define this thing so for our loss
function we're going to say let's define
our loss function well we could just
manually write out so first let's talk
about what loss function we want to use
here
so because notes can can occur at the
same time right let's let's listen to
this for a second
so you notice that some notes are
pressed at the same time right so it's
not a it's not just a classification
problem it's a multi class class
classification problem we wanted to
output multiple notes at if necessary
right and so because we it's a
multi-class classification problem we're
going to use cross-entropy as our loss
function so when we have multi-class
classification appreciated Nikki's
channel 13 when we have a multi-class
classification problem we then use
cross-entropy
okay if we don't then we were then we're
going to use a sigmoid you know a
softmax function but because be but
because steel melts at 3,000 degrees
what are you guys even talking about
okay we're talking about music guys okay
so where were we because it's
multi-class classification we're going
to use cross entropy as our loss
function and cross entropy basically
measures the difference between two
probability distributions and the two
differ or it can even be multiple
distributions and these distributions
are for each of our predicted values
each of our predicted notes and the
reason we're using cross entropy is
because okay we can have multiple notes
and we want to predict the difference
between different notes
I appreciate it Vijay okay cross entropy
cross entropy and would it be easier if
it was monophonic yes yes it would be
easier absolutely but we don't like easy
things do we so let's write out this
function so guess what tensorflow is
like okay Suraj I know you want to use
sequence of sequence so what I did for
you Suraj is I said I'm going to build
in a sequence loss function into the
sequence the sequence module attention
flows neural net class and hold on to
all your questions I'm going to answer
it after this so we have our outputs
okay and so so it's saying like I know
you're using sequence of sequence and
let's define a loss function
or that built-in sequence the sequence
model you're using so if you're going to
use a module of tensorflow life
sequences sequence use it for everything
use it for your loss
use it for your optimizer and all that
stuff
difference in what okay okay let me let
me just check on what's going on here we
have three hundred and sixty six people
alive now so we have even more people so
this is a good sign
this is a good sign hi everybody across
the world you are awesome thanks for
watching so we have our outputs and we
have our target so what it's going to do
is it's going to we have a sequence of
notes we have a sequence of notes and
it's going to sample a sequence of notes
right so it's gonna so we have a huge
sequence of notes right so we're going
to give it like in batches say the first
sequence right here and it knows what
the next notes are going to be it
already knows what the next notes are
going to be so it's going to predict
give it a note what the next note is and
then the difference in value between the
the predicted note and the actual note
is what we want to minimize with our
loss function the loss function will
minimize the difference between these
two notes and minimize by minimize I
mean we represent these notes as
numerical values so a C would be
something like you know 0.23 whatever
and the actual note is a B which would
be like point three seven so we want to
minimize that the difference between
those predicted outputs so the idea is
that we have a huge sequence of notes
and starting from the beginning in
batches we feed it in batches of note
sequences and we're predicting what's
going to come afterwards and over time
we're going to minimize this loss so
that the predicted next note and the
actual next note are going to be very
similar then when we feed it a when we
we try to generate novel sequences is
going to generate novel notes in the
style that it already
it already generated okay so we have our
output so we have our targets and now
we're going to generate we're going to
not generate we're going to define our
loss function as cross-entropy
okay it's cross-entropy and so it's
saying so the sequence
sequence sequence loss-of-function is
asking for a function to use with both
of our with both of our sequences that's
why it's part of our sequence the
sequence module okay this is the same
loss function Wow Gregory I really
appreciate that
it's the same loss function for both of
them
I am going to use cross-entropy for this
with logits okay it such a long
unnecessarily long name I mean with
logits what even is with logics I I mean
I looked this up before but I remember
was now you guys can see like how I
search for answers I just like google it
I'm sure all you guys do as well but
Stack Overflow is a great lodgest simply
means that the function operates on the
unscaled output of earlier layers and
that the relative scale to understand
the units is linear it means in
particular that some of the inputs may
not equal one and the values are not
probabilities oh okay so it means that
the sum of the inputs are don't always
have to equal one because it don't
because it and the values because
because the values are not probabilities
okay so that just means that our MIDI
notes can be like 15 20 right so anyway
so cross-entropy
that's the name of the game
cross-entropy okay so now we have two
more we have two more functions here
time steps equals true we have what else
we got we have average across
batch okay how long is the session can
be it's going to be a 15 more minutes
max okay we are trying to cap it at an
hour and we're almost done all right
we'll have a five-minute Q&amp;amp;A at the end
but of course we're going to sample this
we're going to run the training we're
going to do it all it's going to be
amazing okay I know I'm a little cut off
it's all good this is our first stream
okay like this this high quality stuff
okay and so average across batch equals
true now these are just okay so we want
to take the average value across the
batches oh wait
average across time steps in fact I
honestly don't even think that we should
even be we should even need these two
parameters because because we should
just by default want to use the average
values unless we're really really
fine-tuning this thing we we don't need
to do it so that's our loss function
sequences sequence cross entropy now
we're going to minimize it and we're
going to minimize and this is our last
line I'm gonna go over everything I just
wrote right at the end so if you stayed
if you stuck through this long you are a
hero okay or heroine if you stock
through this long
you are amazing because we are now at
the last line this relatively complex
code actually if you are looking at this
stream you are looking at the bleeding
edge of computing if you are even unable
to understand ten percent of what I just
wrote you are like one of the top people
okay you have that top potential first
of all you have the interest in this
second of all you have the that is that
that alone is amazing that is so
valuable okay okay
did I write that twice yes I did I wrote
twice Thank You Nick okay so now our
okay so our last step
our last step initialize the optimizer
minimize the loss minimize the loss okay
so we're going to minimize this loss so
to minimize this loss we're going to use
Adam Adam is our optimizer step and I'll
talk about why we're using Adam but
first let me write out a little bit of
magic numbers okay these are our hyper
parameters and honestly that we
shouldn't do it here we should have
defined this earlier this is bad
practice I'm just saying it right
officer upfront this is bad practice to
write these out here but that's
we're going to do this these four these
three lines okay and I'll talk about
what this is in a second all right so
these are our values Epsilon what else
we got we have 1e - oh 8 okay boom
that's it
that's our that's our atom that's our
atom optimizer and so let's talk about
atom okay but before we talk about atom
let me write out the literally the last
line of code this is the last line of
code love you to our I'll this is the
last line of code so our last line of
code is to just say op dot minimize our
loss that's it we minimize this loss and
this is dart actual training okay and
what we're done with this we're going to
later on in different modules we'll save
it well we can then you know predict
values and this is our training okay so
let's talk about atom for a second so
atom is one of our possible optimization
functions and I definitely need to make
a video that describes the difference
between different optimizers I
definitely need to do that but that's
not this we're not going to talk about
all the different optimizers just know
that atom is a great optimizer for a for
specifically for sequence to sequence
models so in general what hyper
parameters should I use what optimizer
should I use what a loss function should
I use these are these are meta questions
we are not at that phase yet where we
can just learn to learn where we can let
our network learn what optimal optimus
asian strategy to use where we can let
it learn what loss function to use where
we can let it learn the hyper parameters
to use that is phase two that is where
we as a that we as a community are
getting to we're going to start learning
when let it learn for itself right now
we should define this ourselves
so for atom optimizer so okay so it's
actually this is a great link but it's
not even this is actually a little more
complex
I mean how can I best explain this okay
so I mean it's it's very similar to
gradient descent but there are several
key concepts that are different that uh
it's okay if you're like Fernando it's
all it's all good don't don't worry
about it
just know that atom is what's given us
our best I'm still figuring out how to
explain optimization techniques without
writing out a lot of math and codes yeah
I mean yeah so it's it's very similar to
gray in descent and I have I've talked
about gradient descent several times but
okay so the point is that we are so
that's what we're using for that so
let's let's keep going here for a second
we have a so that that is our model
function so what I just done is I wrote
out so in the in the actual code these
are all the different classes okay
they're there there's all the different
classes here so in the model class okay
he's got a class for the model and he's
got several policies for different
things for like for the you know how do
we want to choose our weights how do we
want a sample so this I honestly think
this guy went like overboard with this
stuff because he's going at it at such a
granular level for for an example but
for actual research I mean this is
amazing so interestingly enough this
model so let's let's talk about what
happily let me let me let me compile
this okay let's let's let's uh so we
have five songs that we're training on
okay I'm definitely gonna do a video on
TF optimizers that's a great idea
thank you guys I will definitely go
video on that I want to start releasing
more videos that are focused on so I'm
going to actually stop this because this
is going to take a lot of compute to
calculate this and a lot of time this
will take on my macbook pro 2016
somewhere around one to three hours okay
to train on up
100 songs ok 1 2 3 hours and then
linearly increase that time depending on
how many songs you do in the future so
interestingly enough for this for this
code what this guy did is he said at the
end I had big ambitions but I needed
more GPUs so I'm going to keep this
model in mind until I have more GPUs so
literally the blocker for this code was
amount of GPUs that's can you believe
that we need better computation ok we
need more affordable computation do we
need GPU you don't need GPUs but if you
want to speed up training like you could
do this on the CPU ok but if you want to
speed up training by like 10x GPUs are
nice I recommend AWS I have a great
video on how to use AWS and with a
prebuilt Amazon machine image
it's got tensorflow built in a bunch of
great things literally you just you just
upload your model and go and the video
on that is which one is that tensorflow
build attention flow image classifier in
5 minutes that's the video it was
blocked by the GPU so let's see what the
result was so I I trained this earlier
and where was this ok so let's play this
do I have a MIDI player or MIDI player
yes GarageBand GarageBand sucks this is
on 5 songs is trained on 5 songs
okay a little dance I'll do a little
dance okay it sounds amazing because
it's overfit on five songs that's that's
the reason okay it's over fit that's why
it sounds amazing
so it's interesting we have that curve
right so it starts up really so if we
had a graph of like how well a model
fits to how how much data we've given it
it's going to be like up here in terms
of like quality and that's going to go
down the more data we give it because
it's not overfitting and then when we
give it even more than is going to go up
again so it's kind of like a you in
terms of quality over you know so y-axis
is quality and then x-axis is a you know
amount of songs we give it so that's
that's what that is rap Suraj I will rap
over this of course can someone give me
a I remind you a lot of Daniel shipment
you guys we are getting it wrong these
people remind you me know I remind
Daniel Shipman reminds you of me okay
let's let me rap about this big data
using Python yo I love big data wait
this is actually not a rapping beat this
is like I totally not around me so so so
so that's it for this code and now what
we're going to do is we're going to
answer some questions so please ask
questions about machine learning about
this code this is our last five-minute
QA and what I'm going to do is I'm going
to so please us you know say the
questions and I'm going to wrap the
answers to it okay can we save a trained
model on can we save a trained model on
what can we say the train shut the hell
up can we save the Train models so we
can load it and save the time for
training again and again each time let
me answer that this beat sucks balls
hold on
our or sucks um anything yes it's the
best of course you can't save it that's
what I do I'm not just turf in I'm
telling you the truth of course you can
save it the weights are so true you save
it you load it you do it again you
initialize the variables look I'm your
friend I'm gonna tell you the truth when
you save those wastes tensorflow has a
way for you to do it okay uh what other
questions do we got can you give a quick
summary
let me wrap a summary okay first we
imported our dependencies yo we went and
started building our network so we had
three names Cooke scopes okay one for
the input and one for the target one for
the previous that's men I'll do it I'm
lost it a logic function is what I use
at the end okay I got a loop recurrent
net I went back off edit my previous
hidden state and that's went back to
back sequence the sequence models where
it came back I gave it an output in the
previous state I wrap all that stuff
every day I try to minimize the loss
function okay I use softmax
cross-entropy
okay and now I'm going to minimize that
with Adam yay
okay so that's what I did for that let
me end the rap okay thank you okay so
that's that's that was my summary of
what we did and let me let me summarize
it one more time in plain English so we
imported our 10th our tensorflow machine
learning library and three helper
classes okay and so what we did was we
built a network using three placeholders
one for the input data one for the
that's our notes and then one for our
targets and those are binary
classifications was this note pressed or
not and then for our hidden state
because this is a recurrent Network so
we had three placeholders and then we
built our sequence to sequence model
that is our encoder
our decoder what we didn't do is we
didn't define our encoder here that's in
a one of our helper classes but we did
define our decoder here okay so that's
what we define using sequence of
sequence so what we did was we fed our
encoder our initials note set it created
hidden a hit a hidden state after
training on that and the hidden state
got better and better right and then we
fed the hidden state to our decoder we
didn't feed the output probability to
our decoder we don't do that in
sequences who wins we feed the internal
hidden state to the decoder and we use
that decoder to take that hidden state
to create our multi-class classification
like the notes are going to be B and a
or just B right depending on whether or
not it's got likely a chord or a you
know something else okay so that's what
we did and then at the end we said well
what is that loss function depending on
the batches of sequence data we give it
what is that loss function that we are
going to use to minimize difference
between the predicted note and the
actual note and that is a cross-entropy
and then we minimize it using adam okay
and Adam will definitely be explained in
a future video so that's that I still
don't understand what a hidden state is
so a hidden state is let me know me okay
so neural network let me just quickly
explain this for a second all right so
so for our hidden state so in our neural
network we have a series of hidden
layers and these layers are we could
think about them as operations okay they
are just matrices and we of weights and
in an operation like a summation or a
multiplication or they can be any number
of operations that we define and so what
what's happening here is when we're
feeding our data in at each of these
states okay at each of these up we could
also call it hidden layer so state is
equal to layer at each of these hidden
layers or States we're computing some
operation some matrix operation it's all
linear algebra it's all matrix math and
we are taking these states and we're
multiplying it by that input and that
the state itself is going to be updated
through an optimization technique like
atom or you know whatever else and these
weight values are
hidden state so we feed those weight
values back into the network in
combination with our input okay so in by
combination we're using a summation
operation okay so we're not just feeding
in the input data like we would in a
feed-forward net we're also feeding in
the previous hidden state and this this
is because we're doing it over time
right this is back propagation through
time how did you feed in the songs
didn't get it okay so I what I didn't do
is I didn't talk about the
pre-processing of the actual songs
you're right that's a great question I
only talked about building the model so
for our songs we have you know two more
minutes so for our songs we had our
music data class and so MIDI is a
sequence of notes and there's there's a
number of pre-processing steps that
happened here there's there's actually
quite a bit of pre-processing well so so
let me do this let me do this
so there's actually one more so I said I
was going to give you guys one more
library so let me paste this in the chat
thanks Colin so go to this link right
here
bhakti Priya slash blues so this is a
great example of a very easy to
understand music
laia ipython notebook okay it's
literally just this it's one Python
notebook but the reason I didn't use it
is because of dependency issues so
remember when it comes to Python we have
a little bit of work to do for our music
libraries they're not good enough right
now they you know Python MIDI only works
for Python two we need to update it for
Python 3 which is what I use now but
what uh what she did is she she uh
buckteeth she took these MIDI notes and
she converted them into matrices of
values right they're already matrices
right and so we use and the matrices
were fed directly into the network they
were vectorized notes so that's kind of
you know a one line of how data in
general should be fed into uh into a
network okay
so check out that
link bhakti Priya / blues and yeah two
more questions and then we're done for
the day
okay Java is all right no it's not guys
can you guide someone who is starting
out what's the best way to go about
learning my videos start with machine
learning for hackers then move on to
learn Python for data science then I'm
huge now so I'll just the way I like it
for Q&amp;amp;A
exactly she is a sheet okay and then
okay
actually to to to not to do to two more
questions
she's looking at you she likes you how
to use LST MS as sequence classifier
Karros has a great example on the main
read me on using Elysee m4 a sequence
classifier and in fact I have videos on
this what was my last LST M video art
generation but that was more generative
Ellis t m4 sequence classifier yeah just
use a double stack LS cm and have your
last layer be a fully connected layer
followed by a soft max so you squash it
okay and then alright one more question
we have one more question and then we're
out of here how to learn the mathematics
of deep learning great questions so I
answered this in my first video of this
series but I'll answer it again linear
algebra calculus statistics do you need
to know everything about calculation
linear no you just need to know the bare
minimum and I have link to cheat sheets
for each of those in my first intro to
deep learning video check out those chi
Chi's as in do you need to know the
inner the integrals for calculates no
you just need to know the derivatives
why we use it for back propagation okay
you just need to know the basics am I
sitting here studying calculus linear
algebra no I'm focused on machine
learning on deep learning so I only need
it I need only need to learn what I need
okay and that's a mentality all of us
should have let's only learn what we
need to learn okay
and nothing more okay because there is a
lot of stuff to learn so we don't want
to waste our time our time is very
valuable your time is very valuable okay
so that's it's on for this session
session on Bayesian deep learning that
is going to be at the end of this course
I cannot wait to talk about Bayesian
optimization Bayesian learning one-shot
learning that in some advanced stuff but
we will get there when we get there I am
so excited to have all you here a part
of this journey thank you for showing up
we're going to learn everything okay
we're going to learn everything we're
going to learn to learn we're going to
learn to learn to learn ok so thanks for
watching I love you guys and for now
I've got to go learn to learn to learn
so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>