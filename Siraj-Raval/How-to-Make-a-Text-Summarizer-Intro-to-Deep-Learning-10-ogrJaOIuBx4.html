<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Make a Text Summarizer - Intro to Deep Learning #10 | Coder Coacher - Coaching Coders</title><meta content="How to Make a Text Summarizer - Intro to Deep Learning #10 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Siraj-Raval/">Siraj Raval</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Make a Text Summarizer - Intro to Deep Learning #10</b></h2><h5 class="post__date">2017-03-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ogrJaOIuBx4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello world it's Suraj and we're going
to make an app that reason article of
text and creates a one sentence summary
out of it using the power of natural
language processing language is in many
ways the seat of intelligence it's the
original communication protocol that we
invented to describe all the incredibly
complex processes happening in our
neocortex do you ever feel like you're
getting flooded with an increasing
amount of articles and links and videos
to choose from as this data grows the
importance of semantic density does as
well how can you say the most important
thing in the shortest amount of time
having a generated summary lets you
decide whether you want to deep dive
further or not and the better it gets
the more will be able to apply it to
more complex language like that in a
scientific paper or even an entire book
the future of NLP is a very bright one
interestingly enough one of the earliest
used cases for machine summarization was
by the Canadian government in the early
90s for a weather system they invented
called fog instead of sifting through
all the meteorological data they had
access to manually they'll and fog read
it and generate a weather forecast from
it on a recurring basis it had a set
textual template and it would fill in
the values for the current weather given
the data something like this it was just
an experiment but they found it
sometimes people actually prefer the
computer-generated forecast to the human
ones partly because the generated ones
use more consistent terminology a
similar approach has been applied in
fields with lots of data that needs you
man readable summaries like finance and
in medicine summarizing a patient's
medical data has proven to be a great
decision support tool for doctors most
summarization tools in the past were
extracted they selected an existing
subset of words or numbers from some
data to create a summary but you and I
do something a little more complex than
that when we summarize our brain builds
an internal semantic representation of
what we've just read and from that we
can generate a summary this is instead
an abstracted method and we can do this
with deep learning what can't we do with
it so let's build a text summarizer that
can
a headline from a short article using
care of we're going to use this
collection of news articles as our
training data will convert it to pickle
format which essentially means
converting it into a raw ice-cream
pickling is a way of converting a Python
object into a character stream so we can
easily reconstruct that object in
another Python script modularity for the
win we're saving the data as a tuple
with the heading description and
keywords the heading and description are
the list of headings and their
respective articles in order and the
keywords are akin to tags but we won't
be using those in this example we're
going to first tokenize or split up the
text into individual words because
that's the level we're going to deal
with this data in our headline will be
generated one word at a time we want
some way of representing these words
numerically and he'll coined the term
for this called word embeddings back in
2003 but they were first made popular by
a team of researchers at Google when
they released word Tyvek inspired by
boys to men just kidding
word - Bek is a two layer neural net
trained on a big label text corpus it's
a pre trained model you can download it
takes a word as its input and produces a
vector as its output one vector per word
creating word vectors lets us analyze
words mathematically so these high
dimensional vectors represent words and
each dimension encodes a different
property like gender or title the
magnitude along each axis represents the
relevance of that property to a word
so we could say king + man - woman
equals Queen we can also find the
similarity between words which equates
to distance what de Becque offers a
predictive approach to creating word
vectors but another approach is count
based and a popular algorithm for that
is glove short for global vectors it
first construct a large co-occurrence
matrix of words by context for each word
ie row it will tell how frequently it
sees it in some context which is the
column since the number of contexts can
be large it factorizes the matrix to get
a lower dimensional matrix which
represents words by features so each row
has a feature representation for each
word and they also train it on a large
text corpus both perform similar
we weld like glove trains a little
faster so we'll go with that will
download the pre trained glove for
vectors from this link and save them to
disk then we'll use them to initialize
an embedding matrix with our tokenized
vocabulary from our training data we'll
initialize it with random numbers and
copy all the glove weights of words show
up in our training vocabulary and for
every word outside this embedding matrix
will find the closest word inside the
matrix I'm measuring the cosine distance
of glove vectors now we've got this
matrix award embeddings that we could do
so many things with so how are we going
to use these word embeddings to create a
summery headline for a novel article we
feed it let's back up for a second then
geo squad first introduce a neural
architecture called sequence to sequence
in 2014 that later inspired the Google
brain team to use it for text
summarization successfully it's called
sequence of sequence because we are
taking an input sequence and outputting
not a single value a sequence as well
we're going to encode then we decode
we're going to encode then we decode
when I feed it a book is set to rise and
when I decode that I mesmerize so we use
two recurrent networks one for each
sequence the first is the encoder
Network it takes an input sequence and
creates an encoded representation of it
the second is the decoder Network we
feed it as its input that same encoded
representation and it will generate an
output sequence by decoding it there are
different ways we could approach this
architecture one approach would be to
let our encoder network learn these
embedding from scratch by feeding it our
training data but we're taking a less
computationally expensive approach
because we already have learned
embeddings from gloves when we build our
encoder lsdm network will set those pre
trained embeddings as our first layers
weights the embedding layer is meant to
turn input integers into fixed size
vectors anyway we've just given it a
huge head start by doing this and when
we train this model it will just fine
tune or improve the accuracy of our
embeddings as a supervised
classification problem where the input
data is our set of go cap words and the
labels are their associated headline
words will minimize the cross entropy
loss using RMF
now for our decoder our decoder will
generate headlines it will have the same
LS pm architecture as our encoder and
will initialize its weights using our
same pre-trained glove embeddings it
will take as input the vector
representation generated after feeding
in the last word of the input text so it
will first generate its own
representation using its embedding layer
and the next step is to convert this
representation into a word but there is
actually one more step we need a way to
decide what part of the input we need to
remember like names and numbers we
talked about the importance of memory
that's why we use lsdm cells but another
important aspect of learning theory is
attention basically what is the most
relevant data to memorize our decoder
will generate a word as its output and
that same word will be fed in as input
when generating the next word until we
have a headline we use an attention
mechanism when outputting each word in
the decoder for each output word it
computes a weight over each of the input
words that determines how much attention
should be paid to that input word all
the weights sum up to one and are used
to compute a weighted average of the
last hidden layers generated after
processing each of the inputted words
will take that weighted average and
input it into the softmax layer along
with the last hidden layer from the
current step of the decoder so let's see
what our model generates for this
article after training all right we've
got this headline generated beautifully
and let's do it once more for a
different article couldn't have said it
better myself so to break it down we can
use retrained word vectors using a model
like love easily to avoid having to
create them ourselves to generate an
output sequence of words given an input
sequence of words we use a neural
encoder decoder architecture and by
adding an attention mechanism to our
decoder it can help it decide what is
the most relevant token to focus on when
generating new texts the winner of the
coding challenge in the last video is
Jieun C he wrote an AI composer in 100
lines of code last week's challenge was
non-trivial and he managed to get a
working demo up so definitely check out
his repo wizard of the week the coding
challenge for this video is to
a sequence the sequence model with
Terra's to summarize a piece of text
oats your github link in the comments
and I'll announce the winner next video
please subscribe for more programming
videos and for now I've got to remember
to pay attention so thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>