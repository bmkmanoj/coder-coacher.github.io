<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Is Hadoop a Necessity For Data Science? | Hadoop Tutorial | Big Data Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Is Hadoop a Necessity For Data Science? | Hadoop Tutorial | Big Data Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Is Hadoop a Necessity For Data Science? | Hadoop Tutorial | Big Data Training | Edureka</b></h2><h5 class="post__date">2015-09-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1NFQxf6_lIg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's Hadoop and necessity for data
science so that's our topic for today
so moving on the agenda is going to be
pretty straightforward we have a very
clear agenda today so we will see what
is big data what is Hadoop so this has
been going making several rounds in the
last couple of years
lot of people are you know it is still
remaining a buzzword and a lot of people
are you know trying to gain rather you
know they they want to get more you know
footprints
many corporates many corporate many
companies are trying to gain more
footprints onto this big data Hadoop and
all that and many people are very
curious right that curiosity level has
really gone up okay
so what is a data product we are going
to see what is there are science right
in general what is data science right
this is something we will discuss and
why Hadoop for data science and what is
the necessity for data science and you
know will also you know I'll tell you a
small problem towards the end you know a
simple problem you know how we can see
that in Hadoop right that is something I
am going to if time is time permits I
try to show you that as well right so
here is our agenda for today and moving
on what is Big Data and Hadoop right
let's try to demystify these terms right
so I think if you some of you may may
have noticed right people with you know
people who I think many of you must have
at least look through the internet
Google and various social media sites
and YouTube and all that there are
scores of videos available pertaining to
Big Data and Hadoop and there are lots
and lots of things you know related
topics in internet and all that so it's
all available in different
you know manifestations I would say but
you know is that enough clarity still
there you know that that's that's a
question you know which requests to be
answered right what is Big Data and what
is Hadoop so can can you you know can
some of you just take a quick guess and
you know type it out in your question
window what do you think is Big Data
right I just want to keep it a little
more interactive here what is Big Data
friends anything that is big
anything that is large anything that is
scalable
what's your you know view about Big Data
and what is Hadoop you know just give me
one answer right
just one more answer data within data
okay
shriek insists that anybody else just
take a guess okay awesome
lot of people are yeah I'm just reading
the ones we know which are coming closer
framework to analyze huge data volume
huge volume velocity variety managing
high volume not samples entire thing
huge data huge amount of data
manipulation large amount of data which
can't be processed with regular database
also I think I mean I'm very happy to
you know get lot of great answers from
all of you I think you guys are not
starters for sure right you I think a
lot of you have unstructured data
beautiful right data in terms of set
abides and yeah brilliant guys Hadoop is
a framework to handle Ashura yeah thanks
thanks all so friends if someone asks
you tomorrow what is Big Data what is
Hadoop you should know the answer so
anything that can be you know the data
is that this big right so if you can you
know something which is of high volume
coming from different varieties and
which is coming at an alarmingly high
velocity right so that is going to be a
big data not only that many of you
pointed out it is also unstructured data
right data today it's predominantly
unstructured when I say predominantly
it's more than 80% of the data
is unstructured data right things are
coming from social media you know ever
since the advent of social media right
so data is coming from various quarters
and we cannot just ignore the data right
so you know in back in good old days I I
can still remember like two thousand
five six types right when someone is
like doing something in Orkut I think
that point you know Facebook was
probably not there output was you know
one of these sought-after things someone
was doing that you know us people will
look at that person like an outcast you
know maybe he's he's not a you know he's
not very aligned with the job or he's
just you know it's just like a waste
really kind of image right so today
social media cannot be ignored right so
we do sentiment analysis right so lot of
major decisions are based on sentiment
analysis what we do in Twitter right and
we look at you know lot of product
insights come out of this lot of things
keep coming right so major product
decisions are made based on the
decisions you know based on the insights
what we derive out of this social media
and various kind of you know channels so
we cannot just ignore this so times have
changed that's what I am trying to say
so huge data unstructured data
everything's big data and hadoop is a
framework to handle this big data yeah
so coming to this you know look at this
exponential growth of data fine social
enterprise data right it's just a
culmination of everything Big Data put
to be there so big data can be either
structured data or completely
unstructured right so under structure is
something you know something like your
text data your audio video you know
something that cannot be structured like
all these are unstructured data and
this kind of unstructured data is
becoming more and more you know we are
getting more and more of such types
compared to structured data so that's
the story about Big Data today right so
we cannot and the traditional systems
whatever we have been having over all
those systems are just not able to scale
up right that's one of the reasons why
we are looking at you know big data lot
of people are you know they just want to
jump into this bandwagon because they
don't have a choice now right so most of
the enterprises or you know earlier they
were dipping the toe now the curiosity
is you know even more you know it has
gone up and they really want to get on
with big data right can someone tell me
like okay what is the social media they
cannot ignore and all that can someone
tell me what could be the factors for
companies corporates enterprises to show
the sudden you know shift you know it's
like a tectonic shift I would say like
why is there a tectonic shift of the
mindset of this corporates companies to
jump into the big data you know we
cannot say that big data was you know
just came like yesterday right it has
been there for the last few years right
probably for five years right so why is
the sudden rush can someone tell me now
like just can someone guess awesome
great answers better business handle all
types of data predict what that will be
interested in buying excellent via
unstructured mental okay cost-effective
genome okay I like this answer right
cost-effective mining the data right so
friends are data analytics nearly 90
percent unstructured data yeah
all these are where all of you are
telling very very valid points one
couple of things you know which really
makes it more interesting for the
customers is one is for us to process
this kind of data right the other amount
of data can you all you know if you for
us to buy even a you know a ramp a 640
KB you know 640 mb ram you know those
days I'm talking about you know it was
seriously expensive I'm talking about
90s mid 90s and all that so those
feather days you know and today the
mobile phone what we are you know just
for 20 grand right you okay you get a
fantastic mobile phone with octa-core
you know it's phenomenal speed right the
and it is really you know I I would say
that most of the tablets what we hold
today they are faster than many of the
laptops available right they are really
faster and you know most of the work
gets done through tablets itself right
so it's things have changed times are
change you know we are just moving ahead
and you know we are in a very fast-paced
environment things have changed and it's
going to be constantly changing right so
that is let's not forget that right so
what is powerful today may not be
powerful tomorrow right so big data
itself will be a moving target right
it's going to be a moving target and the
other thing is the cost to process the
data right these kind of the hardware
cost has really plummeted right what is
the cost of processing right the
hardware those things have really come
down right so the companies you know
they don't have the apprehension to try
out this because you know they cannot
also the most important point is you
know they just cannot ignore this big
data any longer right so most of you
have pointed out that you know it is put
for predicting the you know pattern bias
pattern then whatever business they are
on right so it's critical for
organization to keep in pace with a fast
changing world
giant this point of great point yeah so
yeah these are all valid points friends
look at this three weeks and if you look
at you know IBM I think they have they
have got they have added one more week
called veracity which is the your
quality of the data itself right so
volume variety and velocity is there
look at this vitality level of
importance how critical it is how robust
it is fault-tolerant monitoring
transparency forensics fraud detection
for visibility a big data is like you
know it just it's like a central theme
for all this bubbles
validity corresponding accurately to the
real world statistical modeling
everywhere right the data is growing
data is big right so big data even
though I can say that you know it is
going to be moving down it but it's
going to stay for quite some time now
right because all these data science
even our you know our experiment with
IOT right everything you know has got to
do a lot with the data right just a
minute
so harder piece of programming framework
you see this this adorable yellow
elephant large process large datasets in
a distributed computing environment so
look at this I really like this this but
how do was the first SIL the best tool
to handle big data friends let's not
forget one thing today I think if you if
you have seen how do then we people are
talking about spark people are talking
about storm people are talking about
flink
there are several flavors there are a
lot of projects you know that are in you
know some are still in the research
stage some have come out and some are
you know they will become a efficient
project a little later there are lots
and lots of flavors of you know Apache
you know open source those things are
coming out but you know if someone
learns Hadoop Hadoop still remains the
favorite you know to handle large amount
of data because there are a lot of
fancier options like you know in-memory
computing people say different things
but how do piss the fundamental thing
right it still remains one of the widely
used frameworks to handle big data brief
history of Hadoop look at this
it was established yahoo create steam
and E for team to work on Hadoop it
became a project operate at scale
Haughton works data platform I think
some of you must have experienced this
Haughton works as you know they have
this sandbox and everything you know you
know that's that's also gaining
popularity today so cloud era yeah yes
shakin thanks so cloud aura is there in
fact a lot of people you know offer or
do platform right how do prevents an
open source because you know Hadoop
remains popular one is it is open source
secondly it is very very success
fully I mean it's cost-effective it is
scalable its horizontally scalable it is
flexible okay
provide scale on economy you can just go
on you know you can go on adding this
Balakrishna is asking is Hadoop a
technology or a framework it's it's a
it's a technology framework I would say
it's more a framework of allocation to
handle big data right because how do has
it's a frame it's a framework which has
got two vital ingredients called a
Hadoop file system and the MapReduce for
processing the data right so I will come
to that I'll come to that
I'll come to that it sir so it also
provides stability and all that stuff
right how do so look at this what is
horizontal scalability sure
yeah Stricker does ask this question
I'll answer this then I'll answer them
other squish any language processing
tools I'll come to that yeah so let me
answer what is horizontal scalability so
friends what I try to say is like let's
say you have a mission right you have a
mission which has got about two
gigabytes of RAM right you process 100
gigabytes of data right so let's say it
becomes very slow because of the disk
i/o switch disk i/o you know it keeps
switching the i/o for the ramp to load
the data things like that right over a
period of time it becomes very slow
crippled what you realize hey it cannot
process this kind of data this volume of
data you try to juggle around you
upgrade the RAM you try to make the RAM
- let's say 64 gigabytes it becomes
faster right tomorrow you bring on
another file which is probably half a
terabyte what's going to happen the same
problem will come in right because it's
not able to you know the RAM is not good
enough to sustain or to handle a file
which is of much larger size right it
gets crippled so what we do we keep go
on upgrading how far you can upgrade you
tell me that right
it's you will be restricted I mean you
cannot go on upgrading this so the other
option is you handle you know you
probably have a system you know which is
probably of let's say four gigabytes of
RAM right if this system is not able to
handle a large file you add another five
systems right these are commodity
Hardware right so yeah they're not going
to be super expensive you just go on
adding it right so it the way it is
going to be handled it's you know it is
growing horizontally right not
vertically you're not growing to you
know go on bubbling up once particular
machine right you rather add more
terminals which are far more cheaper and
which is going to perform distributed
computing versus computing on a single
machine single powerful mission this way
now what is the workload is distributed
so it becomes far more efficient am i
answering your question shaketh are you
clear now okay so he said related any of
language processing friends how do
itself was written in Java just for your
information so but how do doesn't mean
that you know Hadoop has got lot of
ecosystem tools it doesn't mean that you
know you need to understand more of you
know Java or anything
MapReduce of course you need you know if
you have Java expertise it's good but
you know it doesn't you know there are a
lot of other languages like you know pig
is there hive is there right so you can
use these languages as well right which
are completely you know which abstracts
the complexity of MapReduce Java and all
that right it makes it far more simpler
so then you focus on the problems versus
getting stuck on Java related issues and
all that right so that's the intent here
so do we still use MapReduce or yawn
okay
mrinmoyee okay I think it is up to us
yawn is more like a platform friend so
it's more a platform and you know Hadoop
two uses yawn right it provides yawn and
MapReduce runs on top of young right so
there are other frameworks that can also
run on top of yarn that's how it is so
we can use MapReduce on top of yarn that
is what I want to say so look at this
distributed file system self-healing
datastore yeah
MapReduce so what we have done very
intelligently today but you know with
Hadoop is these two are combined right
what is the storage another one is the
processing right these two are coupled
fine in if you just remind your memory
how it was handled you know in a
traditional system right so we always
take the data that is stored in the
storage area to the processing front
right so when we take it to the
processing or compute layer let's put it
that way right storage layer to compute
layer when you take it what happens when
you take it itself it can cripple the
network if the volume they W miss pretty
high right so what we want to do is we
we have merged these two things together
in Hadoop so one of the fundamental
shift I would say in Hadoop compared to
the traditional system why it is faster
is the processing is taken to the data
right versus the data is taken to the
processing in the good old days right
the data is taken to the processing like
you are shifting the data where it needs
to process and all that today in the
processing program or whatever that is
taken to the place where the data
resides look at the
look at the shift so the data doesn't
need to be moving along the network
right so we have made this super
effective by this way let's move on from
here computation co-located with the
data right so that's very very critical
here co-designed co-develop to work
together process another thing process
data in parallel
thousands of commodity hardware nodes
failure handle by software which is
self-healing so when we say parallel
processing friends what we mean by this
is you know for us because data is
stored as chunks or blocks in Hadoop
cluster so what we have let's say we
have a file which is of size 200
megabytes so if the default
block size what we said in a Hadoop
cluster is 128 megabytes how many blocks
we will be storing in the cluster two
blocks right one is 128 another one is
probably 72 bytes megabytes right so
when we process these files right how do
you want to process you know it will be
processed like imagine a file which is
running two terabytes it will be
processed as individual blocks it come
already servers and multiple servers
will be processing this blocks in
parallel so which is a parallel
processing processing which makes it
very very efficient then pretty fast
compared to you know the traditional
systems however powerful they where they
still cannot handle a very large volume
of data this ends for one right and
multiple reads there are no random
writes right optimize for minimum seek
on hard drives right but when we say
minimum C C the other one thing is
Hadoop has got very high latency in the
sense C it is not suitable for a yltp
kind of processing suppose you want to
get into
a bank and look for okay mr. Rajesh
Agron has come in to verify his account
you know if they look at his passbook
they want to pull all the - hey current
details what did they just go by the you
know a code number a code name something
like that right they will have a primary
key and all that they are quickly able
to get the data Hadoop is not designed
for that Hadoop is more analytical in
nature it has got very high latency
meaning you cannot go on zero in on a
particular row of data quickly that is
not possible okay
ah DBMS can do that for you very quickly
because it is indexed it has got primary
key foreign key and all that
whereas Hadoop has been designed to read
data which is a very high volume of data
right if you want to read 20 terabytes a
DBMS will take a day or two and still
may fail how do can do it in minutes for
you right so it is been designed for
such kind of purpose friends are you all
clear so far
am I going in the right pace
all right thank you thank thanks for the
confirmation Yosh blocks okay shake on
this asking what are blocks nothing she
can see let's say you know you know just
give me a minute
I just show you I like my I mean I like
to use like a quick whiteboard okay I'll
take a notepad see what I try to say is
let's say a file which is called test
dot dad right 200 megabytes is the size
of the file and block size is 128
megabytes this is a default right so I
love B 1 block 1 p2 right so this is 128
megabytes
this one is 72 megabytes
fine this is how it be stored so these
are blocks basically blocks so the
tester dad has been fragmented into two
different blocks so b1 could be stored
in server 1 I'm just taking you know I'm
just taking a giving you a very simple
example okay
p2 should be 70
okay so someone Balakrishna says default
block size is 64 MB okay sure belong
Stasi friends just want to tell you it
how do one the default block size is 64
MB in Hadoop to the default block size
is 128 MB fine of course this block size
it is not a sacrosanct field it is
configurable so in another way I can say
you can go to Hadoop one and make the
default block size to 64 or to 128 and
202 you can go and set the default boxes
to 64 you can do anything
oh you can even make it 300 as your
default block size so this is a
configurable parameter but by default
Hadoop one is 64 Hadoop 2 is 128
megabytes yeah so that is something you
need to understand this Thanks bollocks
on
thanks for the raising the question
blocks saucer no no frickin blocks or
data they are basically data blocks
stored in server you got it
stored in server 1 stored in their
physical data but it has been
distributed that way it's like a chunks
of data right you're just pulling part
of the data and skipping it in server
one pulling part of the data keeping it
in server do so by pulling this we are
able to process when we process we do
parallel processing you got it divided
into blocks exactly exactly speak up
they are divided - yeah thanks
so that is key power of Hadoop Simona's
one is the data locality data locality
which is nothing but they are co-located
with the data computation co-located one
thing and again processing data in
parallel these two are very very vital
you need to understand this data product
software system whose core functionality
depends on the application of
statistical analysis and machine
learning to data so this is a data
product so that is also it is used for
so look at this you know you people you
may know I think what does this does it
ring a bell do all of you what is this
basically it's a LinkedIn right people
you may know and all so ok can someone
tell me friends what is what could be
the logic here people you may know what
is this basically
any guesses here associations matching
skills other correct brilliant money
she's saying recommendation engine yeah
that's exactly the word I was looking
for million so yeah other answers are
also correct but you know this is the
you know this is the concept here
recommendation engine friends any idea
see just want to give you a small
picture about that there are about
300,000 active members in LinkedIn right
I you know the figure could have gone up
by now okay
but now see the point is if you need to
go and connect with people right you
need to go and connect with that many
number of people I think you'll probably
drop drop LinkedIn and you know go to
some other side right people will lose
patience right it's it's just not going
to work what they have smartly done I
think some of you have pointed out
commonality they look at commonality and
you know pick up some of the common
attributes patterns right and they try
to link you up whether you have worked
in a you're worked in the same company
okay same profession same designation
you know something like that same
technology they try to do all this mod
matching right associations they do all
these things
okay then they prompt and it so happens
you know I would say at least you know
sixty more than sixty percent you know
you will you know it should be like a
familiar face to you right or family I
would say that you know sixty percent
you will go for it you will go for it
and you know think that you know this is
working right that's just the other you
know J if this is something you know
it's more successful and you know is
that is something you cannot ignore
right so they have done their job that's
what I am trying to say do you all agree
with me friends on this part the
recommendation engine of LinkedIn
okay thank you spell correction yeah
again see do you want results for this
right so they do this as well what it
relates to Hadoop seeketh I'm coming
there I'm coming there I am coming to
that part what is data science I think
yeah chicken so your question will be
answered pretty shortly so what is data
science right so extracting deep meaning
from data data mining finding gems in
data data mining is hard work but there
is lot of lot of gems hidden in there
how many people think that data means a
lot right they also say you know one of
the trendy statement people make is you
know they're recent
you know they also say the black the you
know lot of people say that you know the
data if it is analyzed you will find
gems okay if it is properly analyzed if
you don't properly analyze the data
you'll find nothing it's as good as like
a dead data dump data right it means
nothing so the data analysis right
that's a pretty vital thing okay so
let's talk about data analysis versus
data science little bit I just want to
give you my perspective here see data
analysis it is you know somebody from a
strong business domain right let's say
that you know you are given insurance
data right do you think a non insurance
person somebody from a different domain
let's say no you are from financial
domain right will you be able to make
sense of insurance domain data s no okay
some of it okay no no no in fact okay
such it I think you're I mean I I'll go
with you some of it could be relevant
yeah
you're right but yeah somebody from
there let's say retail domain yeah
with insurance yeah you'll not be able
to connect much right so because each
domain it's a mighty ocean by itself
right and it has got their own domain
experience X you know expertise and all
that so somebody from retailing you know
they will be very strong in that part so
they can do extensive analysis they will
know which algorithm to apply to get the
best output or the best results and all
that right there as if you give it to a
financial person on the financial some
financial data to a financial analyst he
can do an excellent job right so it's
very very clear on that part so that is
Maura you know data analysis part data
science I just want to be you know want
you to have this difference very clear
data science is more you know it's more
science than the analysis part right so
an analyst will know which algorithm to
apply for this business domain okay
whereas a data scientist will know how
to optimize the algorithm itself right
so it's more a scientific job right said
you know it data scientist will know you
know should we use this clustering
algorithm or should we use some other
algorithm right random forest algorithm
okay should I take this linear
regression or logistic regression right
so Dana scientists will not only know
which kind of regression technique or
which kind of logic which he has to do
he will also try to fix an optimize an
algorithm right say you know that's more
you know more a scientific job is what I
want this and be here
now look at this descriptive and
predictive right clustering so you all
must have heard about it
clustering is nothing but you know
grouping or classifying the data data
classification is nothing but clustering
outlier detection so you know outlier is
you know again you know you're doing you
are taking a average of salary for a
district right so let's say this
thousand dollars per person on an
average right and suddenly when you are
analyzing the data one person is shown
50 million right so it is an outlier
it's a it's a data anomaly or if you
analyze it it could be like you know
high profile sports person or you know a
movie star it's possible to so some
analysts will be able to point out that
affinity analysis look at the occurrence
of co-occurrence of patterns so that is
possible right these are all the
descriptive aspects classifications
predict the category you know someone
mentioned in the very beginning stage
arrows they can do a you know prediction
why why they are using big data right
predict a category regression predicting
a value yeah recommendation predict the
preference right all these are possible
in this data science world now look at
this building data products right
delivering gems on a regular basis look
at this when the data goes through this
processing tunnel right we are building
a model and we are passing it to a data
store which is a persistent medium here
right so we process the data and we send
it to the data Mart let's say data Mart
or you know like a warehouse where we
get lot of bi reports processed and all
that okay it's just a depiction on that
aspect
why hurdle for data science okay sorry
exploration right of datasets so friends
one thing I want you to understand
before I go a little deeper on this is
data science data scientists right if
you look at even if you google it and
all that most of them are like you know
they have strong background in
mathematics and statistics and you know
they come from PhDs and you know
operational research you know they come
from such a bad road right so because
because they need to tune the algorithm
they need to optimize the algorithm you
know they work on things they need to
understand the you know how the
algorithm works and all that so they
need to have such a background plus you
will be very very surprised
most of the data scientists spend I
would say at least 60 to 70 percent of
the time in data preparation in other
words data preparation involves going by
you know cleaning up the data because
data comes in myriads of sources right
it comes in different down with us so
they need to clean up make sure that
data is ready for analysis right even
for them to get on get to that stage
takes them awhile takes time quite a
while right because data comes in
unstructured packets so they get you
know they spend significant amount of
time doing that right so if you look at
this data processing exploration of data
sets so exploration is one of the you
know data exploration again you know
they they may do exploratory analysis
they can mine the data mining the data
is like looking for insights and you
know looking for outliers they look at
the data dictionary and look for you
know as I told you right you cannot
simply do an outlier you know just like
that some knowledgeable person will know
my you know an anomaly versus a proper
you know a meaningful data as well right
someone should be able to point the
so all these are you know the analysis
part of it right it's it's it's quite a
significant part before it is you know
properly you know before we can make out
any insights out of the data first thing
is we make the data the structure of the
data meaningful so that it's ready for
analysis so that preparation stage is
like very vital then we take it for to
analysis there are a lot of steps
involved here mining of larger data sets
as I told you data mining you know there
are different models for data mining as
well different techniques used for right
so people use you know some of them use
SAS some of them use Hadoop and some of
them use R so lot of techniques are
there in we our relevance here is like
you know why Hadoop right so we will
look into the Hadoop aspect today so
what do we do better outcomes right when
we do this more data better outcomes so
friends Wow the other thing is like you
know the more and more data you unearth
you make analysis sort of more data
right because even in your null
hypothesis and all that we always look
at historical data okay and we try to
discover patterns so one quick example
friends something you know the data
science how it has evolved today right
so most of the let's say retail outlets
now out of this business domains of
enormous Lee got benefited and I would
just give you a small example back in
2000 or you know at least 10-15 years
back right in in supermarkets in us also
which was fairly well equipped and all
that right so people used to give lot of
coupons lots and lots of coupons most of
them will not make sense to you suppose
I go to a super
right they'll give me coupons on you
know home products baby products you
know I'll just trash them without even
taking a second look okay yeah I'm
coming
Upendra is asking how market researcher
could use this Hadoop I'm coming to the
Dipendra yeah
so now as times have evolved right
because most of them you know you will
just trash them right this coupons and
all will not make sense now people have
started observing the customers buying
patterns they are making deriving
insights out of it inside sort of it
right
they are deriving the buying patterns
now they are sending coupons they are
sending you know relevant coupons to
relevant customers are buyers right so
it really makes and they you know it not
only gives you know customer loyalty it
also gives you know customer retention
customer you know more customers you
know they go happy customers it's a
win-win for retailers as well
right it's become a very year success is
a huge success story for thee in the
retailing I'm I'm just saying I mean I
can replicate this story for so many
other domains as well now the answer
open the last question how market
researchers could use this Hadoop can
someone take a guess friends I told you
a story ok which is which is a real
story only but how do you think openness
question can be answered I think you
already know the answer in a way why do
you think Hadoop will fit in here any
guesses here friends look patterns ok I
did define patterns ok ok yeah ashish
has got a point
it says Hadoop can store patterns of
each customers okay behaviorally buying
other patterns okay so one thing friends
parallel process okay great
see how dope can store this data
okay we are talking about a very very
large database right because if you look
at some of the major supermarkets in us
like Cole's right
these Sears these guys have moved on
from the traditional Oracle to Hadoop if
you go to Sears if you see their data
architecture they would on to you know
Hadoop cluster properly right so data is
stored together associated yesterday
Thea may be correct so data intelligence
data because we have got lot of analysis
tools in Hadoop which can be used to
derive to analyze the data right we have
seen data science is all about you know
preparing the data and analyzing the
data so Hadoop can be very well used for
data analysis and storing the data and
of course data analysis right so we need
to store the data over and above I mean
on that data we analyze the data right
we analyze the data that is stored so
Hadoop can play a significant role major
role in keeping the data like storing
the data which is set up which comes at
a very you know low cost right plus we
also do extensive analysis using the
data is that clear friends what are the
analyzing tools yeah funny
analyzing tools we have high we have big
right this hive can help us analyze and
there there is Impala I'm not going to
go there but you know I'm telling
there are lots and lots of tools there
and you know dayley there are you know
different versions coming up but you
know the major ones are hive and pig I
would say tableau is it's more a
visualization platform finish funny yeah
yeah so that is more a visualization
it's it's a brilliant platform and you
know in fact a blow can work with her do
you know just to let you know tableau
can work with Hadoop also you can
integrate it for reporting purpose
absolutely correct large-scale data
preparation look at this friends 80% of
the data says okay okay I was written
off I said 70 percent right so 80
percent significant look at this joins
entity resolution stripper may HD you
know you need to prepare the data that
needs to be mined or analyzed right so
the raw data whatever comes in will be
completely unstructured you need to
really make the data you need to prepare
the data so that you make it more
meaningful for analysis and you cannot
simply analyze the data
just imagine friends you know if the raw
data comes in which is your high
diffident the data absolutely but same
time it needs to be if you want to
analyze it you need to bring some
sanctity to the data right so you need
to take care of these aspects okay lot
of aspects right sometimes the data will
come in later I'll give you a very small
example data comes in like you know one
source it is coming the day it is coming
as DDM my way another source that is
coming from us it is coming coming as
mmddyy right so you need to sort out
these things using you know you know you
need to use either a MapReduce tool or
something like a some processing engine
right can you can even use Pig
it's an ETL tool to do transformation to
make it more consistent right you can
use a regular expression you can do all
those things to do cleaning up the data
so that it becomes consistent then once
the data is far more structured you can
do it do the analysis yeah so this is
how these cyclists accelerate data under
one innovation so today
one other thing friends there are not
you know the Apple what we have you know
I may be talking something
completely out of the back but I still
want to tell you this see most of the
companies if you look at the marketing
strategy right they all come back and
say that you know they do a survey
proper survey they do this pistol
analysis there's a lot of marketing you
know strategies how you study the market
how you look at the segments
segmentation customer segmentation and
they look at their locality they look at
the location and they look at the people
they knew various things right before
they launch a product not many of them
or you know Apple is like a
one-in-a-billion right Steve Jobs had
the foresight he said I am NOT going to
go by the market I will define the
market right how many people have the
guts and how many people have the
conviction to still succeed right iPhone
succeeded we all know that but most of
them you know most you know if someone
says something out of thin air of course
Steve Jobs had the financial muscle as
well the launch a product like this
particular product like this but how
many people will have that leverage
right think about it so if someone gives
a statement like that and you will say
can you show me some backing source can
you prove it with your data right data
plays a very vital role so we always
want to ensure that you know it is
supported by some data facts something
we just did are driven right so
data-driven innovation is far more I
would say that you know trustworthy or
you know it's going to be both
believable or it's it can you know the
chances of succeeding using data-driven
innovation is going to be it's going to
be higher than you know something which
is not backed by data
right so sorry I went to that example
you know just to code you that people
you know something which need not be
data-driven also can succeed right I
want to tell you that look at this
schema and right change is expensive a
DBMS right
a barrier for more data-driven okay
Upendra is asking more data okay
anything you know Upendra I'll tell you
let's say you are World Cup soccer 2014
I don't know how many people watch
soccer right the World Cups of soccer
ASAP analytics from Germany right they
did the extensive analysis on the data
they looked at players profiles they
looked at you know how you know Germany
was really I would say that you know I
think I I don't exactly remember the
number but you know I can tell you the
you know the you overall strategy what
they did they figured out that the ball
position right ball position of every
player in Germany right it was about six
six to seven seconds or or rather five
seconds I would say they brought it down
to buy one out of seconds so it became
like three and a half right for every
player will be possessing the ball
within themselves so they were passing
faster than how they were passing
earlier this completely disrupted the
opponent players who have just not
prepared SFA analytics gave this you
know they they did this analysis and
told the German team if you do this you
are you know you can easily penetrate
into the opponent's defense and you know
the rest is history Germany went on to
win the World Cup right and you all know
how they ate Brazil for their lunch
right the semi-finals it was like seven
one or six one I don't know what was the
score line they just thrashed them and
Brazil you know surprisingly Brazil team
had 51% of ball position in the first
off okay
Germany had just 49% the first off but
they were passing and you know they
completely demolished the you know the
defense of Brazil see I'm saying you
know data
Upendra there are so many things you
know you could say you know how people
do the patterns you know the buying
patterns whatever I told you in Sears
supermarkets they're all data driven
just for your information
everything is data driven is a clear
Upendra yeah sure so new data finally we
start collecting is it any good schema
see look at this right schema and right
our DBMS we all know it uses C it is a
very very strict schema right the schema
is defined well in advance then we start
inserting the data fine
then finally we I mean we start
collecting more and more data is it any
good you know it's like high barrier for
data-driven innovation is like if you
want to change the schema after some
time it's going to be a problem right
because it's all fixed right whereas
it's a high entry where I mean high
barrier for innovation whereas if you
look at Hadoop right you can just put
anything there you can put up you can
just push an elephant over do right
schema and read low barrier okay so it's
is simply going to put it in HDFS you
can write any kind of data any kind of
unstructured data also it's a schema
really this is schema and right for our
DBMS right so that way you know how to
provides distinct advantage of you know
the data being stored in a unstructured
format and all that right it is team
under real so this mix makes it lot more
flexible and flowing for data driven
innovation so how do provides that
operational advantage I would say in the
long term right so thanks any questions
before I show you a quick piece you know
any question so far schema unreal okay
okay so friends what a what we say is
schema and readers see when we are can
you insert data without defining the
schema in our DBMS can you just insert
the data s no
into our DBMS without defining the
schema no the answer is very clear all
of you know this right in HDFS I don't
need to define anything I can blindly
insert data right I'll simply go and
insert the data that is possible in a
loop right HDFS which is your Hadoop
distributed file system whereas in our
DBMS it is schema
right that's why you can trust the data
that is coming from a DBMS meaning you
are imposing the schema which is going
to act as the gatekeeper right so it is
going to have all the integrity data
integrity everything is there it's going
to have this asset property and all that
when it comes to hadoo schema is nothing
but your design there is nothing but you
decide let's say in a table vanish
quick thing is in a table you have rows
and columns right all up one I am going
to say name age gender okay name is
Ganesh ages let's say 20 gender is male
right so this is your schema fine so I
need to have this very strictly defined
inner or DBMS right so this is scheme on
right in HDFS or Hadoop I blindly insert
the data that's all
but while reading fine while reading I
can define the schema at that point
right suppose I want to load data
through pig pig is one of the ecosystem
tools of Hadoop when I want to load the
data through Pig I am going to put thee
I am going to load it right at that
point I need to define the schema that
is schema reading okay I have another
question my previous question does
Hadoop has any relation to natural
language language processing tools
individual need that actually see MLP
it's more to do with text finding the
magrav so I'm not an expert in NLP but I
can tell you that you know you can still
use Hadoop as a medium to process the
NLB you can still do that that's
possible and it is not like you know you
need to have the NLP - I mean it's if
you are really doing the you know
natural language processing yes you need
to have the skill set but you know if
you are not doing that you don't need
that skill set you know pretty simple
like people who all do text mining you
know they need that skill so in other
ways it's not needed right digital
marketing how much Hadoop system that's
a great question
digital marketing sorry whether I'm not
quite sure but I will try to find out
you know if because a fantastic question
yeah because digital marketing I know
that it's the one of the in things today
but I don't know I don't know where I
can place Hadoop there maybe can how to
process the data from s AP yes max it's
possible because how do best you know I
am just guessing open the row theory you
know max I'm guessing
the point is there are hadoo connectors
for most of the leading databases so I'm
pretty sure si P will have a connected -
so Phaedra s demystifying digital
marketing is nothing but online
marketing yeah it's pasta Utley yes
Upendra so
Pruss so your question on the visitor if
it is online marketing I can say
Hadoop's how can we use yeah then I can
say that you know online marketing you
know let's say customer retention you
know looking at the customer
segmentation you know you can customer
profiling you can use various things in
you know these are all marketing
exercises right so you can very well use
it there security of atmos know Hadoop
is fairly secure it's it's quite secure
good boy how can we use Hadoop in cards
and payment domain it is very well used
in you know but I would say in paper I
would say and you know we saw all these
guys are actually using it and they use
Hadoop clusters in their organizations
and of course it is secured oh how does
it help yeah I mean see people are going
to Hadoop primarily because you know
it's not for anything else it is very
very cost effective right because most
of these guys are you know burning
dollars and dollars in costly hardware
infrastructure right this has become
completely you know highly cost
effective right and it is and of course
it gives you gives a very good
performance tool there are many flavors
basic how do a she she has got a very
long pressure if so what flavor what
happening you better to an open sources
of flavor managed by different witness
okay of see Ashish I would say plain
vanilla Hadoop is
great right you just need to know how to
install that and installing is not a big
rocket science you can easily get it
done right I can clearly say that so and
there are you know these guys you know
most of some of the corporates they
prefer you know Microsoft has got its HT
insights and IBM insights is there and
the cloud era has got its own Haughton
works is coming and pushing their own
you know all these guys are you know
coming up with the flavors of you know
how do just like Linux distros right
they people have their own flavors right
so I would say you know if you are so
comfortable using the plain vanilla are
dope
that's great other ways you can you know
you can always go for cloud or and all
these providers you know they come and
offer you this so but I would I am very
comfortable you know I've been using the
plain one so which is very comfortable
good on right compared to a DBMS
absolutely yes Mirage absolutely yes how
can we use it in Linux yeah you can just
use it the less you know Hadoop is
available for you know for Windows as
well quite recently they've launched
that you know it is by default it it's
suitable for Linux
opened rosovsky Hadoop can process any
kind of data how can serve and protect
password and ID no it goes through a
secured shell open there's no problem
about this yeah it's not an issue goes
through ACH okay
so friends are you know definitely and I
need the feedback but before that I just
thought I'd show you a very quick you
know a quick processing if you can hold
me hold for another ten minutes right
five ten minutes I'll quickly show you
what a you know is that fine with all of
him yeah excellent so just bear with me
sure so what I close this you know of
course towards the end friends after I'm
done with the a spawn lab I request your
feedback right that's very very
important you know as clearly stated
here being a compliment the suggestion
or complaint you know be very up friend
and you know provide your thoughts
because you know I that's definitely
going to II you know we believe in
continuous improvement right so please
help me on that part sure thanks thank
you Pedroia so I will go to the you know
her lab session so what I want to show
friends see here just give me a minute I
want to show something like I have this
see I open the terminal okay can I'll
try to answer this question can how to
be helpful for Java developer absolutely
Livi there's no doubt about that someone
is Java right it's going to be supremely
helpful no doubt about that
so I want to show you something you know
very very interesting here yes give me a
minute okay I got it
so okay so friends okay I have a
question here
what about non Java absolutely funny
there is no as he told you right
Java non Java it does not matter at all
a lot of people have reservation can non
Java guys pick up Hadoop my answer is so
overwhelming yes so not to worry about
that because pig how there are so many
flavors in fact I understand Java and as
much as possible I try to use the non
Java part of it the reason is it makes
it even more quicker right you don't
need to learn Java for that so that's
pretty much it and what are the
different type of job roles in Hadoop
Anil Kumar yeah I mean I think you know
you can write right from that being a
developer you know analyst and you can
be a architect we can be a data can be a
big dinner architect and you can be a
dictator programmer and not of multiple
roles it depends where you exactly want
to which no single best to learn that's
like us okay Mira great question there
are about 150 no sequel databases right
there are 150 no sequel databases
approximately says no nothing like best
to learn or anything
I would suggest take one no sequel right
you can probably start off with HBase or
one of these things right understand the
nuances of no sequel why how is it
relevant how is it different from a
traditional a DBMS then understand
one no sequel database then you know
that will give you the big idea of what
you know what is going on then you know
then you can move with that yeah so the
others should be like you know they're
different flavors available Mongo will
be better scoped yeah possibly yes Mongo
yeah so friends I want to show you
something very quickly here I won't take
much time yeah there are categorization
indras asking yeah there are graphical
databases there are columnar databases
there are you know what to say a
database which provides high consistency
versus high availability right so
different kinds of databases are there
she you're absolutely right
categorizations are there so it's not
like you know one you learn now this
will be but you know you at least
concept of lis I would say that you know
there will be you can at least at least
you can you know get to some point but
you know there are like you know car you
know like Google table is there and you
know hyper table and some of them are
derivatives of this kind of these
families of databases there are
different families but you know at least
no sequel you know from that aspect I'm
saying if you learn one one or two right
it should give you a broad perspective
of what and no sequel database is that's
my point
how do we get data video formats in real
time yeah sure
key value that's right in draw your key
value pairs and all that what's the best
if I'm looking for key value key value I
think you're ready sits there in the
rough yeah Redis is there and Cassandra
is there yeah these are all based on of
key value pairs I would say how do we
get slides and video recording
sure slides of which one budget this
this session is this session is recorded
Rajat you may need to check with in
Eureka support do you just need to find
out the other ones you know you may need
to find out you know whether this
becomes available I think you can check
with the support I'm sure they will help
you yeah so friends just want to tell
you this is like a a quick problem right
I am NOT going to go explain the overall
things right but this is a sip you know
a simple analytical tasks how Hadoop is
playing a major role I want to just tell
you this right look at this number of
flowers right can you all tell me your
observations just a high-level
observation they all look alike in a way
is that correct names are different okay
there are a lot of just majority look
like it look alike correct yeah friends
you know I want to you know just want to
lead you on to this problem there are
these are all types of flowers right and
this one this is nothing but the
duplicate of this okay this flower is
getting duplicated nine times right see
in other words I have how many flowers
here 19 items selected right
so I have how many duplicates here one
is here and nine times of this so 10 so
in other words if I the business the
problem statement for this exercise is
to get rid of the duplicates it
pretty scientific problem so let's say
you know we are talking about you know I
need to get nine images right let's say
you are given one hundred million images
fine and someone tells you that you know
you are working for a cryptography
project which has got like fingerprint
scanning and all that stuff right it's a
very high sensitive project they want
the data to be treated with utmost
confidence now they are looking at you
know where your fingerprints they've
given you now they are telling you to
get rid of some of the fingerprints that
have come yes duplicates okay and they
say the number of duplicates could be
maybe 100 or 200 how do you do this any
vague idea you guys have any vague idea
anybody has this idea fence image
processing tool okay matching pictures
Upendra says that any any other guesses
friends okay so well I'm not what I am
trying to say is it's a very scientific
problem how Hadoop uses you know I I
just want to showcase that part
beautiful London cometh he says md5
value message digest value right so
that's size of the file may not you know
said it
the point is size of the file is a very
misleading attribute right it could
still the size may be the same but the
fine type so many things can be you know
different right say MDF is one of the
you know algorithm right so that gives
you like a hashing value using that you
can be able to find the top wickets
right so that is something you know I I
just sort of showcase that quickly I
will not be you know taking much time
here so what I want to show is MapReduce
input you know I have already put these
files this images into this folder right
now I want to get rid of this I don't
want to physically remove these files I
rather know instead of getting 19
entries here my output should have only
nine in threes right ignoring those end
duplicates right overall so this is my
it has the URI a spell where the flowers
are stored so my what I want to convey
here is like you know I will I have my
Eclipse just give me a minute I can show
this quickly a little bit wait okay I
got something here so friends
differentiate the duplicates okay reject
I am coming there different chip so what
I want to convey here friends
differentiating the duplicates is you
know the md5 algorithm right what it
does is see I'm going to first if there
are 100 million images right I am going
to put that in a compressed format right
first I am going to put that as a
sequence file which is nothing but a
binary file then how do how do pull dot
you know I am using Hadoop framework to
do this right
using a MapReduce program which is
nothing but you know it's part of the
Hadoop so I'm using MapReduce framework
here so what I want to do here is see I
have this first thing I want to keep all
these files read the files completely
there are two steps process to this
friends okay two steps process just give
me a minute I'll explain you this then
you can ask this question fine
there are two steps process two step
process oneness I want to convert
everything into binary
so let's not I am NOT going to go too
much deeper into the source code but
what I want to convey at the high level
is I want to read the file one by one
based on the URI right I only showed you
the
input right then it's gonna rain create
a byte stream out of that fine you got
it so that is the first step I want to
do then the second step okay that is the
first step second step is this is my
reading that file reading that file
right
I want to process image by image I want
to create an empty file you know here I
want to create calculate the md5 message
digest fine fine so that is going to
generate a hash value which I want to
convert it into a hexadecimal fine
if the hexadecimal matches okay I want
to just drop that so those are all
duplicates so again there is something
called Map Reduce in Hadoop right Map
Reduce is nothing but you know map is
processing reduce is nothing but
aggregating the presence of the
processing okay so if I have 20
documents coming in right with each one
of them you know having different
different results when I want to combine
it and aggregate right that is what
gives your reduced process fine so now
when I do this reduce right first you
know I want to get in and you know just
do this
I process only the you know the first
time because if I have 20 different
identical values I want to get in the
first time and do a break so that you
know I don't get to process the
duplicate once so I'm using the Hadoop
framework very effectively the Hadoop
MapReduce framework any questions
friends in this how to run the same
program without Java like ping sure I
mean we need to write logic for this
funny it's going to be lot more you know
simpler in Pig it's going to be you know
fewer lines and because it internally
uses MapReduce Pig internally
uses MapReduce so piggish more like
it'll do the you know you don't worry
about how it is done internally and all
that you just tell what needs to be done
that big will take care of that right so
that is something you know big is
extremely useful for but you know if
there are completely unstructured and
you know audio/video files and all that
I would suggest to use MapReduce at
least at the first level because there
are more support and libraries and you
know tools sophisticated tools in Java
compared to other areas right so you can
exploit that then you go into Pig in for
the second level of analysis you can do
that no no it will not be time Indra is
asking Pig is something like SQL PL no
intra thing is more like a you know yeah
it's closer to SQL I would say closer to
SQL if the memory which duplicate is
present is very large no no it will not
be time consuming
Rosetta it is not an all be why we want
to do this into binary format is to we
want to process the by a binary you know
we see I told you about you know the
step initially you can process things in
chunks right if 100 million map process
you know runs because these are all
distinct images right do you want to run
100 million processes in different
computers is that very efficient yes or
no because these images are very tiny
that zip files jpg few or bitmap they
are all tiny files right is it good to
process that many files in you know as
separate process threads is it good to
process one hundred million files no
right you all know this it said no
because it's going to be overheads in
connecting them and all that right so
you want to create a bug because it's a
very small file right you may want to
process it as sequence files which is
nothing but you know I want I want to
compress them and you know keep them
tidy so that I
in fact processing speed improves
overall right you got it so let first
thing whoa do you all understand the
overall structure of this the program
what I mentioned folks first thing I
want to convert that into a binary
sequence file second thing I want to
read the file
get the md5 value of each file then go
to this of you know the aggregation part
and you know I am because I am NOT
physically removing the file I am only
putting the path first time I do this
for the same thing you know for the rest
of them I jump out of the loop by doing
a break or you are getting this logic
overall do do you understand whatever I
am saying so far okay why we do yeah
binary why we do this is to simplify the
you know you need to process it right
you can't do a you know I don't want to
treat them as you know separate files
and process because that's going to be
very very time-consuming so I want to
compress them so that's why I am
treating converting them into binary
fine
secondly when you are when you want to
process these images right I need to
have it in some format so binary format
is the you know compressed and shortest
format I can have Mohamed are you clear
with this that's why we are doing this
then only we can build some hash out of
the file first thing is we need to read
the file itself right then convert that
into binary and all that yeah is that
clear yeah okay friends but overall I
want to ask you are you understanding
the what we are trying to do with the
overall files right the overall
structure you are understanding what we
are trying to do is that correct
okay brilliant
so what I will do I will execute this I
am going to convert this into a jar very
simple job in a minute okay I've done
something let me go and check it out
just give me a minute have the jar file
right there so I'm going to do JA ok
first thing
and movin it my input is sequence good
five more input sequence second good
text and I want to write the output to
September 9th
yeah that is a file
okay
yeah I think I gave this Ahmad all Mr
input oh there is a folder in okay
minute
thanks.thanks tomography thanks for
suggesting that yeah so boy is asking
distributing file in multiple slaves
then how do we compare each image is
duplicate up no I actually read more the
point is see now it's going to be yeah
it's going to be distributed I agree
with you but you know when the binary
file whatever we are tributing right if
it is stored as stored in as distributed
right the point is we are going to keep
this right we are this the the file okay
let me show you instead of talking about
this way let me show you this part this
part right this is my output this file
this is my binary file okay if this file
can be distributed across the cluster
that's possible this file itself part 0
0 right which is sever which is our
cluster okay this this file can be
distributed if the threshold size of
this file if the file is above let's say
300 megabytes it can be distributed
across the cluster so overall the the
framework Hadoop framework will take
care when it processes it goes and you
know it can still processing as
individual blocks that's possible so
I'll tell you how it can be because when
we are doing this right see my second
part of my second part of the processing
right this is my first part right I have
converted that into binary this is my
second one okay what will be the input
for my second part the output of the
previous thing which is nothing but this
right now I am going to
let's say I want to do output September
9th say final let me say that fine so
let's try this
so admire my pointers if that file the
output file which is what you saw here
if that file is large enough right it'll
get distributed across the cluster okay
and the Hadoop framework by default it's
going to follow the thumb rule of data
local data locality right which is
nothing but it is going to take the
program which is this jar file and all
these things configuration files these
are all the programs it is going to take
them into the respective slave missions
where it is good or run the you know
processing so now I just done this right
okay now if I want to go back to the
output September 9th
you see a final look at this the
duplicates have been removed these are
all the extra decimal you know those
values you can ignore that but you know
my point is you're getting only one
flower right one auto load these are the
ones which got duplicate right it's hash
value will represent aggregate right yes
yes indra yeah see the the hash value or
the md5 algorithm fine so those image
when you are comparing two images you
cannot go by the size you cannot go by
the types and all that but the hash
value the binary format right once you
create the hash value using the binary
it cannot be different I mean it will be
the same for two identical images that's
my part so we have gotten rid of the
duplicates is that clear friends overall
see Hadoop can play a very major role in
exercises like this you can probably you
know you can also how to avoid noise in
the bar conversation that's a very
famous program too right we can use
machine algorithm machine learning
algorithm for that right so the again
Hadoop can play a major role there is a
clear to all of you
okay so friends we have come to the end
of the session and of hope you found it
useful if the images in the crop of the
images I don't understand this question
Roger images is the crop I'll try to
answer this question then I'll find it
up so friends again I say requested hope
you enjoyed this webinar overall so hope
you found it useful so I really want you
to you know provide the feedback as you
know as a requested earlier
yeah sure says it so budget maybe if we
can sure-sure Indra maybe indica can you
know help you to you know solve that I
mean provide this information new majors
crop of previous images will it be
yeah-heah
see any image identical image it's going
to have the same value that's my point
yeah you image your world image right so
it's going to be you can you can find
out this value and ID be the same that's
that's very much there yeah so thanks
folks funny I think you know I would
suggest you know you can check with the
indirect or support they're ruthlessly
efficient so I don't mind giving my
number but my point is like you know you
would be better off because if I don't I
may not attend to your urgency sometimes
right I may not see your urgency you
know you may get delayed so please talk
to the support team I would suggest you
know they will be very happy to help you
any doubts you guys have yeah so I give
them a lady folks but my recommendation
please you know I give I'm a lady
my recommendation you know please talk
to the support you know they are more
than happy to help you there yeah so I'm
sending to all of you so thank you so
much
hope you found it useful please please
please provide your thoughts feedback on
this so I will be very appreciative if
you can do that thank you so much
friends</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>