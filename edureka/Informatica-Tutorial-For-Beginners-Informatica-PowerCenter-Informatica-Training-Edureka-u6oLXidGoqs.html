<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Informatica Tutorial For Beginners | Informatica PowerCenter | Informatica Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Informatica Tutorial For Beginners | Informatica PowerCenter | Informatica Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Informatica Tutorial For Beginners | Informatica PowerCenter | Informatica Training | Edureka</b></h2><h5 class="post__date">2017-06-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u6oLXidGoqs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone this is me from Erica and
welcome to this informatica tutorial
session so let's not waste any time and
let's move forward and look at today's
agenda now we will begin this session by
first trying to understand what exactly
does businesses need today then we move
on to understand how business
intelligence helps in providing what the
distances need and then you'll be deep
diving into business intelligence where
we will be talking about extract
transform and load that is the ETL
process as well as another core concept
of data warehouse from there we move on
to talk about in traumatic a power
center we'll be talking about the
architecture of informatica power center
and then we'll be looking at the various
informatica power center clients finally
to complete the business intelligence
cycle we'll also be talking about data
visualization so why you guys came to
this agenda can you give me a quick
confirmation all right so I've got a
confirmation from everyone let's move
forward into the first topic of the day
that is what do businesses need today
now before we actually move ahead let me
ask you all the same question what do
you feel that a business should meet
today ok so now he's telling me more
revenue definitely some Porsche is
telling growth sometimes in terms of
growth what exactly are you referring to
do you mean organizational growth or
growth in terms of revenue growth in
terms of its strength ok all of that
that's great so to summarize everything
growth is the perfect answer to it every
business today needs to grow but how
exactly do they do that to achieve
growth every business needs to have an
insight as to how the organization is
doing as an individual as well as how it
is performing in the market as much but
to understand all this it needs to
gather data from different sources this
could be data from social trends it
could be data from inside the
organization it could be market research
it could be customer feedback
so you have different sources from where
you can get this data once you have this
data by performing register kinds of
analysis you can get a great business
insight which in turn can give you a
guiding path which will lead to your
business growth so to answer the
question
every business today needs to grow but
becomes
the question on who needs these insights
or who exactly are the right people for
this insight any answers to this okay
Henry Sales Manager Krupa says top level
professionals definitely that is correct
now talking about professions who need
these insight there are mainly two
categories of professions
this is executors that are top-level
professionals and this is analysts
this is analysts are usually the people
who work around with this data to gather
all this insight who then go on to
provide this information to the business
executors once in the business executors
have an idea with respect to what
exactly is needed to be done this is
done for a brainstorming session to work
run with different possibilities and
come with various possible business
ideas which can lead to business growth
so I just clear your trigger can you
give me a quick confirmation okay some
push has given me a confirmation so has
Henry Dave Kripa Gowri that's great to
see I've got a confirmation from all of
you guys so going forward let's try to
understand what exactly is the problem
in achieving these in fact one of the
major problems most analysts face is
that the data that is being gathered
from these different sources cannot be
directly visualized can anyone tell me
why do we need to visualize this
analytical or this insight that's
perfectly correct Kripa visualization is
the best means to help anyone understand
what exactly these insights up so let's
take at this pin if I show you the sales
number in an excel format and at the
same time I wish lies it in form of four
line graphs or a histogram which would
you understand better definitely the
histogram of the line graph so our eye
processes the visuals move faster than
this proko with numbers okay so that's
exactly why data visualization plays a
very important role in today's industry
and is one of the most growing trends
and technology in today's world so the
key problem being here is I can gather
my data from all the sources but I
cannot directly go ahead and visualize
this nor can I directly integrate this
data because they could be coming from
different sources
you could be using an Oracle database to
store your data it could be a Microsoft
sequel server you can also be taking the
unstructured data from social media as
well so combining all this data to
analyze and get an insight becomes
highly unprovable
although there are tools which help you
gather data from different sources and
directly visualize it like tableau but
again it's quite limited in that aspect
as well because the amount of analysis
as well as the norm amount of
integration there is quite limited
so are you guys clear with respect to
the problems that people face while
achieving this insight okay that's good
enough now there's always a problem so
what is a solution going to be to this
so now we have a problem at now comes
the question how we can solve this now
there are various ways that you can come
across to try and solve this problem but
the most preferred and the most trusted
solution in the industry presently is
business intelligence which tends to be
the perfect solution to all your
problems in achieving in facts now this
is intelligence basically is a
combination of various techniques as
well as tools which help you gather the
data from different sources and then
help you to combine this data and store
it into a data warehouse once you store
this data into a data warehouse then you
can perform your various analytical
process which can then be taken forward
to visualization which will make it
easier for the business users other
business executives to understand these
insights but again how do I combine the
data from different sources and then go
ahead and keep it under create a
warehouse that's where the process of
ETL comes into pitch now what do I mean
by ETA ETA L stands for extract
transform and load ETL is one of the
most popular technique in the industry
presently to achieve data integration
and help you combine your data from
different sources and store it into a
data warehouse okay so Krupa has a
question here creep is ask me is ETL the
only way through which we can achieve
business interest
grappa okay to give you a better
understanding with respect to this
grippa ETL actually is a process under
the stage of data integration in
business intelligence now what do I mean
by data integration it is a way through
which you combine your data from
different sources and then going on to
store this data into different
warehouses or cubes as per your
requirements now to achieve data
integration there are again different
ways you can go with data modeling you
can go with it a warehousing data
cleansing data profiling and then you
have ETL or ELT process among all this
ETL is the most preferred and the most
effective processes and that's exactly
why most organizations today follow with
ETL process now talking about what
exactly is the ETL process there are
mainly three phases that is extract
transform and load now your first phase
that is extract mainly comprises of two
parts where you have capture which gives
you sample idea of what data is present
in your source system and then you have
scrub scrub basically is a process in
which you're going to check your data
for any inconsistencies or any invalid
input with respect to that say this is
done using various AI techniques to help
you understand if there's some data that
you can correct and take it forward or
if the complete rules has to be rejected
once you have completely accepted all
the data presenting your so system then
you go on to perform the operation of
transformation now what exactly is a
transformation transformation is quite
simply put a set of rules that you
define which help you convert the data
to a required form now let's say I have
50 lakh rows present with my sources now
for my analysis purpose I don't need all
50 lap rows I just need five thousand
rows which meet a certain condition so
what am I going to do I'm going to write
these set of rules which will help me
transform the source data and give me
only the required data once I have this
required data I go ahead to load it and
index it into my data warehouse so are
you guys here with respect to the ETL
process and why we were using it in the
scenario of business intelligence
okay so Davis asked me how is ETL
different from ELT okay that's a very
good question d2 those of you who are
not aware of what LDS ELT is quite
similar to eat here
but where instead of you perform extract
transform and load what you will be
doing is you extract the data load it
into a data warehouse and then be
performing the transmission of pressured
spread but in terms of performance now
let's take a real-time scenario let's
say the same number I have 50 lakh rows
and then I'm directly loading into my
data warehouse firstly my data warehouse
needs to be that huge and then in that
50 lakh rose I just need 50,000 so once
I have loaded these 50 lakh goes I need
to perform the transformation operation
to just segregate the five thousand rows
that I want so just because I need that
five thousand rows I need to ensure that
my data warehouse can store 50 lakh rows
so you see the problem here right it's
the problem with respect to the memory
requirement now most enterprise data
warehouses do occupy a huge memory but
again on a real-time scenario with
respect to your performance don't you
feel that if this is a downgrade that's
great so I'm glad to see that all of you
are agreeing with respect to this so
does that answer your question Dave
great again so one thing I want to put
across right away is that at any point
if any concept or any topic that I am
explaining is not clear to you please
make sure you put across your query in
the chat box so that I can clarify I
have no problem if I have to explain a
certain concept five times or ten times
but I want to ensure that all of you are
quite clear with respect to what we
discussed because this is quite
important for you to understand before
we actually move into informatica power
center - now don't worry we'll be seeing
extensive demos as part of today's
session so I request you to be a bit
patient with respect to that thing so
again even while we're discussing demo
there may be certain things that you may
not be clear so don't worry please put
across your questions at that point so I
can help you clarify it is that here all
right great so I've got a
formation from a be so moving forward
let's talk about the important concept
here about data warehouse now we've
talked about how you're taking data from
different sources and then storing into
a data warehouse all the way from the
start of the session but what exactly is
a data warehouse I was expecting at
least one of you to ask me this question
so does anyone want to give a try ok
some tor says data warehouse is used to
store data from differences that's good
push but let me ask you this how is the
data warehouse different from your
normal data base now you know not just a
question to some toad business to all of
you think for a moment and then let me
know if you don't know the answer don't
worry I will be explaining that
okay so VP's got it almost screams so
let me answer this question in a way
that everyone can understand the main
difference between a data warehouse and
a database can be based on two factors
first is their usage and the second is
their architecture now coming to the
usage a data warehouse is mainly used
for analytical purposes now it's
something that is created with the main
objective of performing various analysis
in the data that has been stored whereas
a database is mainly used to store it
thereby giving you the second difference
that your database mainly focuses on
writing data to it whereas your data
warehouse mainly focuses on reading data
from it so your architecture as well are
quite different there by their
difference in architecture also comes
into picture a data warehouse is created
in such a way that the reading and the
analytical process performed on it is
quite faster as compared to a database
but a database in terms of writing data
into it may or may not be quite faster
than a data warehouse so it again
depends on what kind of database you're
using so I hope you there is a clear
with respect to the difference between
both okay that's great to see now going
by the definition of bury table the data
warehouse is a simple complete and
consistent store of data obtained from
different sources made available to
end-users in a way they can understand
as well as use in business context so
here you are extracting data from
different sources then you are making it
available to end-users such that you can
understand and use it in business
context that is it becomes easy for an
analyst to understand this data as well
as perform previous different analytics
to the state now comes the question as
to what a data warehouse is being used
there mainly three purposes as to which
a data warehouse is going to be used
first comes your information processing
your analytical processing as well as
data mining now let's be dive into each
one of these talking about information
processing data warehouse these are
mainly used for basic query
apart from that they are also used for
basic statistical analytics and
reporting using crosstabs tables charts
infographics and more around the
information presentation layer so they
are again quite straightforward then
when you come down to talking about the
analytical processing data warehouse
this revolves around mainly your OLAP
operations like a slice and dice to love
drill down and pivoting with the summary
table Leftwich finally we have data
mining process now this process mainly
revolves around finding hidden patterns
and associations in your data
constructing various analytical models
as well as performing classifications
and prediction of your data now once you
perform these operations these can again
directly we taken ahead to visualize so
are you guys clear with respect to the
types of data warehouse and what exactly
is the data warehouse
okay so dave has a question here Dave is
asking me do we have to select one of
these three options while creating a
data warehouse notice these three data
warehouse may seem similar but how they
are created slightly base a data
warehouse that you would be creating for
information process and a data warehouse
that you would be creating for data
mining would be completely different
because again that purposes are
different so the data that you're going
to be storing in them how you are
exactly storing the data within them
they all quite differ so I hope you get
the idea now don't worry we'll be
extensively talking about data warehouse
how we'll be creating a data warehouse
and all that in our next session so in
case if you're not clear with this then
definitely we talked about this
extensively in our next session and at
that point if you have any doubts we'll
take it up that does that sound good
Dave but I hope you have a simple
understanding with respect to water data
warehouses and how it plays an important
role here
okay that's great now comes a question
as to why use informatica again there
are different tools that help you
achieve data integration and you also
have different tools which help you
perform ETL operations among all that
you must be wondering why use
informatica percent the answer is quite
straight for informatica is the market
leading end-to-end data solution
provider now in terms of market strength
informatica schools control over 70% of
the market and is growing every day even
you get a competitor you may have to
consider maybe ibm's data stage or Aven
issue but again what you need to
understand is that when informatica is
holding 70% of that market how much do
you think these other tools could get a
hold on the main reason here is mainly
because of the complete product life
cycle that is being provided by
informatica boxset let's say you want to
maintain the quality of your data
throughout your process so you have
informatica data pocket which is also
referred as idq so let's say you want to
work with cloud and integrate your data
on the clock so there you have a
traumatic as data cloud integrate let's
say you're working on an application
development as
there again you have informatica
lifecycle management tool which helps
you completely through that or let's say
you wanna integrate your data between
different organizations so let's a huge
amount of data that you integrate for
that you have informatica enterprise
data indicator now let's say you want to
exchange data you have informated as b2b
data exchange a person that another very
popular tool in the market today from
informatica is informatica as master
data management so this again is one of
the most popular tools from informatica
obtained from attica power center and is
quite rapidly growing at the point
so again informatica is something that
is always keeping up with the market now
again if you talk about cloud storage
which is one of the most hot cakes in
the market empress informatica is also
making its presence felt there and it's
also gone on to form a tie-up with
Salesforce and symphysis also gone on to
invest a few billions on informatica
just a few years back as well so again
informatica is the leader in the market
where data is being dealt with but that
exactly shouldn't be why you need to go
with informatica power set let's talk
about what exactly is informatica power
center and its key features this will
help you understand why informatica
power center is being the market leader
and is dominating the present path now
informatica power center is not just
used to perform data integration but
also helps you migrating your data from
different data bases to a new data bases
apart from that let's say you want to
integrate your application data then you
can do that as well using informatica
powers or the major topic that we have
been discussing that is the creation of
a data warehouse informatica can do that
without a problem so if you look at the
example here you're taking data from an
Oracle database you're taking data from
s AP HANA
your Microsoft sequel server as well as
different databases all that can be
combined and stored into a single data
warehouse using informatica power set
and later on in this session you'll be
seeing how easy it is to do all this so
I hope you guys are clear with respect
to Y go with informatica and why
informatica power center is the market
so any doubts
expect to the same or or you guys are
not convinced as to why informatica is
so good okay that's great to see move
our let's look at some of the domains
where informatica has helped solve
various problems now let's talk about
the banking duck now we all know the
amount of data that is being processed
on a daily basis in the back and after D
monetization statisticians tell us the
growth has been close to 180 percentage
now that's a huge growth with respect to
transaction indeed now all this data
needs to be verified and you also need
to centralize this data because we go
with core banking facilities these days
that's where informatica power center
comes into picture and help you do all
this without a problem and makes it
quite easy now let's look at the
insurance domain now here one of the
major challenges is validating the
claims and information from the
customers as well as you need to migrate
the data in case of a merger or an
acquisition now this is quite common
with respect to the insurance term as
well as when you look at publication
domain as well there also you need to
migrate your data between different
industries when there is a takeover or
there's a process of migration
happenings
now all these domain problems can be
fall very easily using informatica
powerset now how that happens we'll be
seeing in the upcoming slides now to
help you understand how informatica
power center works let's look at five
different use case now let's say you're
a retail organization and you have
different sets of data you have thousand
customer details you have about 3,000
products and 50,000 transaction details
from this we are mainly going to try to
achieve five different insights first is
mainly to identify your most loyal
customers so that you can go on to
provide them with additional distance
now this is something that most
organizations do to improve their
customer relationship after that let's
try to identify the regions which
require more marketing that is regions
where we are having leases once you have
identified this we can try out different
marketing schemes to improve our hold in
these regions now
the third problem is to identify
customers who've made low number of
purchases from our organization once
you've identified this we can try to
offer them a special discount to
increase their chances of coming back to
our organization and increasing our
sales now the fourth problem that we'll
be talking about is to identify our most
selling product and it's corresponding
category once you have identified this
then we can focus movie around this and
then improve the quality of products
that are provided in this category
finally what we'll be trying to
understand is the requirement of the
customers region wise so that we can
provide better discounts better
marketing strategies across each reach
so either I Square with respect to the
problem state
okay that's great now this is a
representation of how our data is going
to be processed now we have the details
of the customer in a flat file the
product details in a CSV file and the
transaction details in our database all
these details is going to be taken and
loaded into our informatica power center
from which we'll be processing the data
transforming it and then finally going
to store it into a sales data warehouse
once we have done this then we can go on
to visualize this data so I guys clear
with respect to the process okay now the
solution again can be broken down to
four stages mainly around the
informatica power center client tools
the first two we'll be using is
informatica power center design now in
fiying a power center designer is used
to load the data from all their sources
cleanse this data and then define a
mapping now what exactly is a mapping
mapping basically is the flow of the
data from the source to the target
definition through various
transformations so this is basically a
connected design that will be create
then comes our flow management now
invoke your manager what we'll be doing
is that we'll be creating a work flow
and then we go on to executed now a
mapping in itself is not directly
executable so the transfer of data does
not happen to do that what we need to do
is we need to create a workflow to which
will associate a corresponding map once
you have done that when we execute the
workflow we can monitor it
using the informatica power center
workflow monitor there we can see the
number of data present in the source the
number of data that is being processed
the speed at which this data is being
processed as well as the amount of data
that is going to be stored into a data
warehouse finally once we have the data
we go on to visualize this data so I got
clear with respect to the solution to
this problem and how we're going to use
each of informatica power centers tool
to achieve this okay so I've got a
confirmation from Henry Kravis Santosh
Krupa Dave DT that's great to see so
let's talk about the first tool that is
informatica power center designer so as
we've talked informatica power center
designer is mainly used to load the
source metadata create different
mappings and then go on to also create
target metadata or load an existing
target method it okay so Kripa has asked
me why metadata okay creep up varies we
work on our metadata because it gives us
a better understanding as to what a data
is present in the database rather than
using a schema the metadata will give us
a better insight with respect to a book
now in this process of transferring the
data from the source to the destination
we'll be using various transformations
to process the data into a format that
we choose to so let's move forward and
look at informatica power center design
but before we look at informatica power
center designer we looking at a tool
that is called informatica power centers
up a stream manager so let me go ahead
and show you informatica power center
repository manager first so let me
launch it from Attica power center a
forestry management
so once you install informatica on your
system inside the informatica folder you
can find all the client tools here and
the first tool we will be exploring is
informatica power center repository
manager now again you might be wondering
why we are discussing about informatica
power center repository manager first
this is mainly to help you understand
how all the files that you will be
creating would be stored and organized
as such now again repository is the
place where all the projects that you're
working on would be stored on in an
organization usually you have multiple
repositories because different teams
will be working on different projects as
such so in that case you would be having
different clients as well so to make it
easier for management you work with
different pathways and to configure as
well as work around with all the
repositories you use the informatica or
Center at Bostrom item so I have already
configured a tapestry lifting just
connect to it and show you so once you
click on the connect option here after
selecting or a pathway it asked for you
for you username and password once you
specify that just click on connect to
connect your a possible so here are my
different work folders now I'll just
give you a simple example
now let's see it Eydie record
transformation so here are all the
sources that I have used would be stored
now again once you open this you can see
the sources so these are the different
sources that had used in this project
similarly the targets that I had defined
the mappings as well as the workflows
now don't worry if you're not clear with
respect to what our mapping this and
water both use we'll be seeing them
practically and I am quite sure you'll
be clear but even if you're not here at
that point please make sure you put
across your questions and I'll be able
to clarify it then in that so I guess
clearly here
so what we'll be doing is we'll be
creating a new work folder okay so
gallery is asking me
in case of source does the complete
source get stored no going so here again
an object is created so everything that
you see here are basically objects be it
your source be it your target definition
and these objects intern
our metadata representation of your
source data so not the complete source
would be stored only an object would be
stored which has the metadata
information of the source that you're
working with same with targets as well
as mappings does that answer a question
Gowri all right so to create a new work
folder just go to folders tab and select
create option
name's informatica tutorial
and one thing you need to remember is
while naming anything we work folder be
it your mappings be it your target
technicians do not make use of white
spaces always use underscore instead of
white spaces this is mainly because
informatica does not support white
spaces in this naming convention so
there are certain naming conventions
that will be following I'll be
explaining them as part of the session
ok so once you specified this you don't
need to change anything else
click on OK and our work folder gets
created so if you actually see the work
folder right now there's no details
present here so what we need to do is we
need to go and start filling it up so
the first climb tool that we'll be
exploring is informatica power center
design now when we talk about line two
zero mean III client tools power center
designer power center workflow manager
and power center workflow monitor each
has its own purpose that we have
discussed as part of the solution phase
so the first phase that we'll be solving
is in using informatica power center
design so just click on with B I can
present here and automatically
informatica power center designer is
lost
so this is what your informatica power
center designer looks like so here again
you have all your work folders so this
is your workspace there are mainly five
different workspaces we'll be talking
about that little head and this is
basically our output console window so
any operation that you perform
corresponding output would be generated
here and will be also talking about a
special feature of informatica then
always comes into handy while you're
working with mappings now coming back to
the workspaces you mainly have five
workspaces you have your source analyzer
workspace this workspace is mainly used
to load your sources as well as modify
or create new sources as well so if you
want to modify any details with respect
to your source data then you can do that
in this workspace second comes your
target designer workspace this works
with as the name suggests is used to
either create a target definition or you
can load or modify a target definition
so any changes with respect to your
target definition can be done here
ok creepers ask me what do I mean by a
target definition cripple by target
definition I basically mean this
structure of how my target data should
be stored now let's say your source data
has about 20 columns and in your target
data you speed pre corpse so what I can
do is I can define a target that
consists of only three calls so that
modification can be done in this target
designer workspace does that answer your
question Krupa okay now the third
workspace that we'll be talking about is
the mapping designer workspace now this
is a very important workspace where we
will be designing our mappings which
again consists of our source definition
our target definitions as well as
various transformations now apart from
this you have the map lead designer
workspace and the transformation
developer workspace about VC workspaces
we'll be talking a bit later in the
session so for now let's go back to our
source analyzer workspace and let's
begin by importing our source now before
that to any of you actually remember
what our first use case problem was
okay so in case if you don't the first
use case problem that we had to solve
owes to identify the most loyal
customers so that we can provide them
with additional discover now again if
you look at the numbers here we have
10,000 customers now how do you identify
the most loyal customers any suggestion
guys
okay so BP tells me people who have
purchased most products definitely
that's correct with e so to identify
customers who've bought a lot of
products from us we actually need to set
a benchmark that is the minimum number
of amounts they need to buy to be
considered a loyal customers so in our
case what we'll be doing is we'll
consider customers who purchase more
than 10 products from our organization
to be loyal customers now this is the
number that I have just assumed as per
your requirement you can change it
accordingly okay so what we need to do
is we need to identify the most loyal
customers and then provide them with
additional discounts now we mainly have
three source here we have customer
details we have product details and we
have transaction details who can tell me
which data sets we need to use do we
need to use all three of them
okay Gowdy says all three sort of
Santosh Dave says only customer details
Henry and to pass a customer and
transaction so I see a lot of confusion
between you people so let me clarify
this out so let me clarify it for you
all let me show you how these tables
look like that way you get a better
understanding with respect to what is
present where so first is my transaction
table that is present in my database
Here I am using an Oracle database to
store all my data and you can configure
any database of your choice but this
needs to be configured while you're
installing informatica power center so
let me connect here so this is my Oracle
SQL Developer and here I have different
databases so my source transaction
detail is present in my Oracle HR
underscore SRC database and if you go
inside tables here you can find
transaction so let me show you the data
present in science so this is the schema
of the table present in the database if
you have invoice number store code
description of the product the quantity
invoice date your enterprise customer ID
and Product ID so do you think we need
to include the transaction table as part
of our input yes we need to now apart
from the transaction table which do you
feel should be imported up from product
and customer details definitely customer
details itself so we need to use
customer details which is present in a
flat file and the transaction details
which is present in our database and
identify the most loyal customers who
purchase more than 10 products from our
organization now let's go back to our
informatica power center designer so
here let's begin by loading the sources
to load any source make sure you're in
the source analyze the workspace and you
have a tab here called sources click on
it and you have option import from beta
base and import from file so first let
me import from beta base so here you
need to select the ODBC connection
object so for each database you need to
create a corresponding ODBC connection
object now how do we do that we'll be
discussing as
our next session but for now all you
need to understand is that you need to
use a corresponding object to connect to
each database now my username is HR and
once I click on connect you can see it
has connected to my database now here
from tables I need transaction table so
I select that and click on ok so here
you can see the transaction table has
been included as part of my sources now
apart from this I also need my customer
details now it's time to load the
customer details from a flat file so for
that click on import from file
so now my customer file is actually
present in the default directory where
informatica looks for source files now
where is that in my PC Google Drive that
you have installed informatica inside
informatica folder go to the version now
I'm using nine point six point one and
the latest version prodigy is
informatica Tech there is not much
difference with respect to both there
are certain differences with respect to
UI and certain new features which we
also see in the next session
where we will be working with
informatica power center tech so just go
inside the folder and here go into the
server folder inside this go inside info
under spa share and here inside sr3
fights so this is the default directory
where informatica text for source file
so here this change the file type to all
files and you have your different tips
so I want my customer table which is in
a flat file format so just selected and
you have a flat file import wizard now
this basically is an import wizard which
makes it quite easy for you to import
your data from a flat file now first
what you need to do is you need to check
what kind of flat file it is whether it
is a delimited file or a fixed now mine
is a delimited file which is a comma
separated file and the second thing you
need to check up in the first step is
that you need to import the field names
from the first line now the main
objective of this is that informatica
actually samples your data and then
creates a metadata if the first field is
not imported as free link then what
happens is the sample data that
informatica checks from the file turns
out to be incorrect
because the first look may actually
contain field names which could be of
variable type but when you come down to
the actual data that is present in each
of these columns they could be numerical
values so it actually messes it up so
always make sure if your first row
contains field names you click on this
option import field names from first
line I will clear in here any doubts
with respect to how to import a file
from a database and a flat file ok so
I've got a confirmation from everyone
good to see that you guys are following
so just click on next now here again
when you say delimited file you can
specify what kind of delimited file it
is and if you are using text qualifiers
what kind of
codes the text qualifier specified it
just click on next and you have a sample
data present here let's say you want to
make any modifications with respect to
the data type of a column then you can
modify it here itself while you're
importing the date
so again you can see a sample data of
your data file here we're talking about
500 rows have been imported from your
actual customer details of 10,000 rows
so once you've done with this just click
on finish to complete the import so here
you have your customer details as well
as you have your transaction details now
it's time we go on to create a mapping
which will help us identify the most
loyal customers so for that lets go to
the mapping designers workspace and
let's begin by creating a new mapping
now to create a new mapping just click
on the mapping tab and select create
option here now what you need to do is
that you need to name your map now again
coming back to what I had mentioned
earlier by convention all mappings
should start with a small M followed by
name of mapping so here it's most loyal
customers and again no usage of
whitespace
so once you click on okay a new mapping
has been created so again these are the
different transformations that you can
use as part of your mapping so we'll be
getting into that so as soon as you
create a new mapping itself the
transformations get unlocked as well as
the mapping is I will be here you can
see my present mapping is M underscore
most loyal customers now you get clearly
here any doubts okay that's great now
what we need to do is that we need to
import our sources so select your
customer details putting it onto the
workspace just drag it drop it similarly
do it for your transaction table as well
now you may see along with the source
definition this another tab that has
been associated with it so this is
basically your source qualified
transformation now those qualified
transformation is one of the most basic
transformations you will be seeing in
informatica while you're working with
relational tables or flat packs now the
main purpose of a source qualified
transformation this will help convert
the various data types to informatica
supported data types so it is not the
most important and basic transformations
in informatics
so now that we have our sources it's
time we go ahead and create our mapping
so first what we need to do is that we
need to identify the customers who have
purchased more than 10 products from our
website
what we'll be doing this will be
creating our first transformation that
is the aggregator transformation now to
create any transformation you can go to
the transformation tab select here go to
create option and then you have option
of selecting multiple transformations
here which meet your requirements so you
can select any one of them I definitely
recommend that you explore each one of
them try on playing with them and you
get a good idea with respect to the
thing so again I will get to
transformations is what we will be using
so the naming convention usually follows
that is name of the transformation and
then the purpose for it so here what I
need to do is that I need to process my
transaction so I call it process trance
and then once you click on create you
can see the transformation is created
once you are done now what you need to
do is you need to connect your
aggregator transformation to the source
qualifier of transaction this is mainly
to create a connectivity for the data to
flow from source definition to the
transformation now what you need to do
is just right click on the source
qualifier transformation select all from
here you can drag it and drop it to the
aggregator transformation this will
ensure there is a copy of each column
present in source qualifier
transformation created in the aggregator
transformation and they are
correspondingly linked as well so if you
look at both the transformations let me
just expand this I have invoice number
that is also present in my aggregator
transformation and they are connected
correctly
similarly for stock code description
quantity invoice date unit price
customer ID and Product ID so everything
that is present in my source qualify
transformation is being transferred to
my aggregator translation now in my
aggregator transformation I need to
perform the logic which will help me
identify the most loyal customers so to
modify any transformation first thing
you need to do is this double click on
the transformation and automatically
this window that is the edit
transmission window will pop now here
you have the name of the transformation
as well as the type of the
transformation now this is an aggregator
transformation and you see a window here
that is me reuse now we not be
discussing about
right away but I will tell you what
exactly this option plays a role in the
next example so let's go to the port's
tab now here I have all the available
ports in my aggregator transcription now
what do I need to do is that I need to
group all this data based on the
customer ID that is the details of a
single customer would be blue
correspondingly with respect to all the
data so you have the details of all the
transactions performed by a single
customer as a group
so what into group any data you have the
group by option now apart from this what
I need is that I need to have a count of
the total number of transactions
performed by each customer now to do
that what you need to do is that you
need to create a new port which will
actually count the total number of
transactions so what you need to do is
click on this option here that is add a
new port and then let's call it let me
just rename this let me call it count
now we have added a new port of count
and we need to change its data type now
by default it's been assigned string so
we need to make a decimal because this
is going to store a numerical value that
is the count of number of times a
transaction made by each customers now
coming to the next part here you can see
there are three columns i/o and we the
first is basically input port output
port and variables now if a port has
input port checked that means that the
values for this sport is being received
as input if it has output port option
check that means that this value has to
be provided as the output now coming
back to our card we know that the value
for count is not something that is
received this is a value we need to copy
it so you need to uncheck this and
immediately when you uncheck this you
can see here the expression option for
count gets unlocked now what exactly
does this mean is that the value for
this column is something that needs to
be computed using a specific formula and
this formula is something that you are
going to specify now to specify the
formula just click on the expression
option here and you find this arrow once
you click on it the expression editor
opens up now to give you a simple
understanding of the expression editor
there are several functions that you can
perform as part of the aggregator
transformation now to just give you a
simple idea these are the complete list
of functions that you can perform now
apart from this let's say your function
is something that includes a value that
is being fed as input to your aggregator
transformation in that case if you go to
the port's tab you can find all the
input ports available to your aggregator
transformation finally you have
variables now variables basically you
have three kinds you have either
built-in variables mapping variables or
mapping parameters so for now let's not
look at this let's come back to our port
now who can tell me the expression based
on which I can identify the total number
of transactions made by my customer can
anyone guess it that's right there I
need
to count the customer ID so what you
need to do is that the formula for
counting discount itself now you need to
pass the parameter which is customer ID
so you can just have a picture and it
gets added here close the parentheses
and one important feature that
informatica provides you is that it lets
you validate your formula or your
expression that you have defined to
check if it's correct or not let's say
there's certain formula that you're
writing but you're not sure if it gives
you the right value or if the syntax is
crap in that case you can always use the
option of validate this text if the
expression is correct and if it can be
computed now in our case is correct so
click on OK and click on OK so are you
guys clear to hear as to how we are
upping the details based on the customer
ID and how we are counting the number of
customers to identify the total number
of transactions made by the customer
any doubts to the earth with respect to
the aggregator transformation as well
all right if create to see that you guys
have for now what do I need to do is
that I need to identify the most loyal
customers so does it make sense to have
the details of the invoice the stock or
the description in my further processes
no right so I don't need any of these so
let me just unselect this
I don't need description quantity
invoice date unit price but I need the
customer ID I don't need the product ID
and finally I need the customer comp as
well so once you're done with this just
click on apply and click on booking so
till here what have I done is that I
have grouped the details of transaction
based on my customer and then I have put
a count that helps me identify the total
number of transactions made by each
customer but again to identify your most
loyal customers what you need to do is
that you need to check if they have made
more than 10 transactions now how do you
do that is that you create a new
transformation called filter
transformation again if you remember to
create transformation go to the
transformation tab and select create
so here we need to create a filter
fealty our most loyal
click on create the transmission gets
created
now here I just need the customer ID and
the count so once you've selected both
of them just drag it and drop it to your
filter transmission so till here I have
identified the number of transactions
made by a customer now what I need to do
is that I need to filter out the details
of customers who have made more than 10
transactions so to do that what I am
going to do is that I'm going to specify
a condition in the filter transformation
now we click on the filter
transformation
go to properties
the filter condition so this needs to be
modified so to move true and what we
need to do is that specify the value of
count has to be greater than 10
or let's say it has to be greater than
equal to 10 that is if they have made 10
transactions or what let us consider
them to be a loyal customer so here you
have the various operators present as
well as a numeric keyboard to help you
make the operation easier so again you
can validate this expression has passed
successfully that means it's correct so
click on OK
and now we have filtered out the details
of customers who have purchased more
than 10 products from our website so is
this complete so can we go ahead and
load these details to a data warehouse
no we can't we still need the details of
the customers because we only have the
customer ID so what we need to do is
that we need to extract the details of
the customers from the customer table so
click on apply and we need to join this
data with our customer details so we've
already loaded our customer table here
to select this
join the data from the filter
transmission to the details of the
customers through the source qualifier
transformation we use our fourth
transformation today that is a join a
transformation
now the join a transformation is a very
useful transformation it helps you join
the data from two sources the source
could be a transformation or your actual
source as well you can join the details
as long as there is at least one
matching column present between both of
them if you have that based on that
matching column the details would be
pressed now we need all the details of
the customers present here so select all
you can drag it and drop it to your
joiner transformation similarly I need
these two details from a quilter
transformation select all drag it and
drop it here now what do I need to do is
that inside my join transformation I
need to specify the condition so once
you click on condition tab you can
always add the condition so you can see
your customer ID from my filter
transformation should be equal to the
customer ID present in my customer table
once you've done that it's quite simple
click on apply
so once you've done that it's quite
simple just click on apply and the
customer details has been added click on
ok so now we are joining the details of
the customer who have purchased more
than 10 items from our retail
organization finally what do we need to
do is that we need to go ahead and store
it into our target data warehouse now
before that we need to actually create a
target technician that needs our
requirement so there are mainly two ways
you can create a target definition you
can either create it from scratch as per
your requirement or what you can do is
that you can model your target
definition similar to a transformation
to create a target definition quite
similar to any transformation just
right-click on the transformation and go
to the option create and add target this
will actually create a target technician
that means your target require that a
similar pure transformation so any
doubts through here
okay it's great to see that you guys
have followed now before I go ahead and
connect this target definition I need to
make certain changes with respect to
that set so to do that let's go to the
target designer workspace
once you click here you need to just
load the target definition that we have
just created so this can be found inside
the target folder in your work folder so
just drag it and drop it to the
workspace that will click on it and you
can modify so again there is a naming
convention that we need to follow for
fog and definitions as well so begin by
renaming it let me call it PG t most
loyal customers
and click on okay
now since my output table is going to be
stored into a database the database type
this auric but let's say is going to be
stored into a different database you
have multiple options present here you
can choose one that meets your
requirement if you can apply we are done
so before we actually go on to complete
the mapping there is one important thing
that you need to create now you need to
create a target table in your database
which can store this data from the
source now to do that go to Target
option and click on generate execute SQL
option here what you need to do is that
first you need to connect your target
database so I am going to store it in my
Oracle HR underscore PGT so here again I
need to specify the username and
password so it is HR underscore TGT
and you can connect to your data
that's done click on create table option
and click on generate and execute
options here
so in your output window you can see the
option where the create tables SQL is
being executed and has been successfully
executed so to validate this you can go
to your SQL database you can reconnect
and in your tables you can find the most
loyal customers present so a table has
been created to store this data now
before we go ahead and complete our
mapping one last thing we need to do is
that inside our target table we need to
remove the duplication with respect to
the customer ID so just select this and
click on remove option and that column
gets removed so click on apply and ok
and let's go back to our mapping
designer drag and drop the updated
target definition and let's connect it
so select all and correctly link it
now if you see here it's not linked
correctly that is the customer ID from
reputation is getting linked to the card
so you need to check this always that
the link is correct because if you do
not perform the correct link what
happens is incorrect data gets connected
so always make sure you correctly check
the link while you are connecting to
transformations so with this you can go
on to save your mapping and every time
you save it that is by pressing ctrl as
informatica actually checks and
validates your mapping so as you can see
here now that I have saved it it has
completely checked my mapping and then
it has declared that this mapping is
loyd so we've completed our mapping any
doubts key here
ok so d please ask me what happens if we
do not create a table into a database so
between that case what happens is it
processes the data but finally when it's
going to load this that time your
informatica power center throws an error
so you workflow interface now don't
worry about what workflows will be
seeing that next so for now all you need
to understand is that if you do not
create a table into your database then
what happens is informatica does not
know where it needs to put this process
data and it ends up failing
does that answer your question bTW ok
that's great
all right so now as I had mentioned
earlier your mapping in itself is not
directly executable here what you've
done basically is that you have defined
how the data should be taken from the
source how it needs to be processed and
where it needs to be stored but we need
to actually process this that is we need
to execute this map to do that what we
are going to do is that we are going to
create a workflow now a workflow is
something that is associated to a
mapping which helps you executing in
this map now to create a workflow you're
going to use the second informatica
client tool that is informatica power
center workflow manager to do that you
have the option here that is informatica
power center workflow not now all your
informatica power center tools have the
option of the other client tools which
makes it easier for you to switch
so just click on the W I can hear and
automatically informatica power center
workflow manager get launched
percent of workflow manager similarly
like a power center designer there are
mainly three workspaces here for now
let's concentrate on workflow design now
in our workflow designer let's begin by
creating a new workflow suit creating
your workflow click on workflow tab and
select create option again the naming
convention here is that the name of the
workflow will begin by WF followed by
the name of the mapping that we have
used M underscore most loyal customers
now don't change anything else just
click on OK and your workflow gets
created to understand that you have
successfully created a workflow you can
see the start I can present you now it's
time that we add our mapping to this
workflow to do that what you need to do
is that you need to add a task which is
associated to our work now there are
multiple tasks in informatica power
center work flow design so you have a
session task you have a command task you
have email tasks and then you have
various option of decision assignment
timer control so these are the different
operations also available along with the
three major tasks for now let's
concentrate on the session task which
helps you associate to or not so once
you have selected the session task this
left-click on to your workspace and then
it asks you to which mapping should this
session be associated that is which
mapping should be executed when its term
of this session to be executed so we
need to execute our first mapping so
click on ok and a session has been
created now if you look at the name of
the session as well it again has a
naming convention that is s underscore
the name of the mapping you have set so
these are some of the naming conventions
you need to follow in informatica power
set now what we need to do finally is
that we need to link the start to the
session icon so that the execution of
the workflow follows a sequence so you
basically have this option called linked
tasks present here once you selected it
the start point has to be linked to the
session
so any doubt still here okay so Davis
asked me can we execute multiple
mappings do we have to create different
sessions or can a single session be
associated to multiple numbers okay
that's a very good question now in a
real time scenario you're not going to
be working with a single mapping you're
going to be working with multiple
mappings of such in that case what you
need to do is that you need to create
multiple sessions because each session
can be associated to only one mapping in
such cases you will have a complete
workflow in a following structure where
you have a sequence of which mapping
should be executed first and which
should be executed later now let me give
you an example let's say you're working
with multiple mappings where the output
of your first mapping is going to be the
input of your second mapping and the
input and the output of the second
wrapping is going to be the input for
the third in such a case what sequence
would you follow for the session session
associators the first mapping comes
first session associated to the second
mapping comes second session associated
in the third mapping comes from great so
but let's say you have mappings 4 &amp;amp; 5 as
well mapping 4 has its separate input
and mapping 5 takes input from mapping 4
then you have another mapping 6 which
takes input from both mapping 3 and
mapping 5 so in that case you can create
a branch where while you're executing
workflow 1 2 &amp;amp; 3 workflow 4 &amp;amp; 5 can be
executed and finally the output can be
given to workflow 6 ok so again it
depends on your requirement I definitely
recommend that you try playing around
with this and it will be a really fun
experience for you guys as so any other
doubts - yeah are you guys clear with
this ok that's quick now before we go
ahead and execute this workflow there
are certain things that you need to
check with respect to the session
properties to do that this double click
on the session icon and go to the
mapping tab here now here first what you
need to do is that you need to
whether it is X taking the data from
valid sources and then is this storing
correspondingly to the valid output
database as well so let's see in case of
our customer now customer was a flat
file so it is actually using a file
reader now one thing you need to
remember is let's say your flat file is
not present in the default directory
it's present at a different location in
that case what you need to do is that
you see the option here that a source
file directory under the properties tab
here you need to specify the path to
that file so this is something you need
to remember if your source file is not
present in the default directory so here
you need to set the corresponding part
now let's check for transactions table
it is using a relational reader here
because it is reading from a database
similarly let's check for our target now
for the target I need to change the
connection object because or app with a
char underscore SRC is to my source
table and I need to change it to Oracle
HR underscore PGT once you click on it
let's check all right it all seems ok
just click on apply click on OK and
let's go forward save this workflow that
is by pressing ctrl s and similarly like
we had seen with power center designer
workflow Marja as well checks if your
workflow is valid or not once you have
done that it's time to go ahead and
execute this workflow to do that you can
either go to the work your option and
select start workflow or you can just
right click on your workspace and select
start workflow from here now once you do
this the third phase of your solution
comes into picture that is informatica
power center workflow monitor so
informatica power centers workflow
monitors is a client tool that helps you
check the status of a workflow and helps
you see whether there is any problem
with the execution of it or if this or
if the workflow has executed
successfully now let's go ahead and
execute this select start workflow
option here
so again click on start workflow and you
can see here workflow manager gets
automatically launched so if you look at
this workspace here
this is the various workflow that you
are executing as well as the details of
this session so I have really a single
session so it's showing the status of
the session now to get more details with
respect to the save you can double click
it or right click and get run properties
so this will give you the complete
details of your session properties now
to get a better understanding go to
source target statistics now here you
can see my customer details had 10,000
rows my transaction details had 50,000
rows but my final output has only 344
roots that is from my 10,000 customers I
have identified 344 customers who have
made more than 10 purchases from our
retail organization now let's say you
want to verify this even further and
what you can do is you can go to your
database and check this details
so let's do that for the Oracle database
let's say select star
most loyal customers I'll just excuse
the statement
and here you have the details of your
customers so customer ID the name of the
customer details everything that we want
along with the cart now if you look at
the count column you can see that only
customers who have purchased at least
ten or more products from our
organization are present here so with
this we have stored our first use is and
we have seen the different phases so any
doubts with respect to that thing
because from now onwards we'll try to
speed up things a little bit so that you
can see the various transformations as
well as get an understanding with
respect to how informatica power center
works but any doubts with respect to
each of the client posts that we have
seen
okay that's great to see that you guys
are following and clear with respect to
that sale now let's look at our second
use case call what we need to do is that
we need to identify the regions where we
need to require more marketing next
question which datasets do we need to
use we need to use customer product and
transaction or we just need product and
customer product and transaction of
simran transaction or just transaction
who can tell me okay Henry tells me
customer and transactions Andy can you
tell me why
just a simple justification okay that's
good enough effort we need the details
of the transaction to identify the
customers from the region which has
least number of sales so again what
we're going to do is that we need to
first identify the customer details then
we need to check the number of
transactions made and then we need to
divide it region wise so earlier we had
grouped it based on customer ID here
we'll try grouping it based on region so
eighty dots here okay so let's go ahead
and try it so let's go back to a power
centered aligner and start creating a
mapping for the second use case
to create a new mapping
I will call it marketing reasons
so here again the steps are quite
similar we're going to load our
transaction details then we connect it
to an aggregator transformation so these
are shortcuts to your transformations
you can select any one from here as well
so once you've selected it just click on
the workspace to add the new
transformation I will select all this
from my transaction table loaded to my
aggregator in my aggregator I group it
by customer ID so that I can understand
which customer has bought which product
now this makes it easier for me to
identify the regions from where my
customers converse now again I'll add a
new column that is count let me call it
count and change the data type I'll
change it to decimal it's not taking an
input so let's write the expression
add in the customer ID
validated and click on so any doubts
with the step we've done this before as
well I hope you guys are clear why we're
doing this
so click on apply and okay we've
completed our aggregator transaction now
similarly let's add a filter
transformation now in the filter
transformation earlier what we had done
is that we had identified customers who
had purchased more than 10 items from
our organization now what we'll do is
we'll identify low purchase customers so
what we'll do is we'll keep the
threshold has three so any customer who
has purchased three or less than three
items their region will try to market
more and try to bring them aboard and
make more transactions so before I link
the aggregator to the filter
transformation let's just modify this
once more reports we don't need invoice
number stock code description we need
the customer ID we don't need the
product ID and click on apply
drag it and drop it to the filter
transformation here in the filter
transformation we need to identify the
customers who have purchased less than
three item so count let me add it from
the port so
is less than equal to three
validated and it's tough
so till here we have identified the
customers who have purchased less than
three items from our organization now
it's time to identify from which region
do they come from so for that what we'll
be doing is we bring in the customer
table and again we create a joint
raishin
join transformation comes all the
customer details as well as Beatles on
the filter transformation
so here you have details of customer and
the details from the filter
transformation in conditions tab let's
add the condition customer ID from the
filter table should be equal to the
customer ID from my customer tip click
on apply and in ports tap the pin just
make sure that this doesn't go as an
output
cly
so to hear what we have done is we've
identified the details of the customers
who basically have purchased three or
less than three products from our
organization now what we're going to do
is now we're going to group them based
on their region
and then what we'll be doing is we'll
try to identify those reasons where we
need to provide immediate marketing
attention so for that again I'll add
another aggregator transformation
here I will take all the details present
here
now in my port tab what I do is I group
them by city and country so this will
give me a better idea with respect to
the reasons where I need to concentrate
more exactly of why we're grouping them
based on their city and country okay so
I've got a confirmation from everyone
good to see that you guys are following
now here again I don't need the details
of the customer directly so let me just
uncheck all this option
I need the details of the city the
country in I do need their contact
number email and I need the count as
well so click on apply now here comes
the question on how do you identify
regions where I need to provide
immediate marketing attention who can
tell me the answer
that's correct day we need to identify
the region with least number of
customers so for that what you need to
do is that you need to sort this data
based on that cup and to sort this data
we are going to use a new transformation
called sort or transformation which will
help us sort our data based on a
specific value so you have sorted
transformation here if you click on that
now we need city country and count along
drag it and drop it or sort or
transformation and in the sorter
transformation if you go inside the
port's tab you need to identify the key
option key option basically is the value
based on which you're going to sort your
data so I select my count to be the key
value and it has to be in an ascending
order that is the row which has keys
count number has to count first the city
and country which have the least count
should come first before the others so
he be sorted in an ascending order any
dots with respect to this or
transformation that's great to see all
of you understand this because it's a
very simple transformation and
informatica has made it quite easy for
anyone to understand it as well and how
effective it is
you'll be seeing ahead so finally what I
need to do is I need all the data to
come into my target table so to do that
I will again model my target table to
meet the same requirements as a sorter
transformation so right-click it I'm
going to create an ADD act let's go back
and modify this target definition
so let me first clear this and let's
modify our target definition here let me
show you a different kind of output file
let's call it a flat file okay now I am
going to create a delimited flat file
that is going to be a comma separated
file so just click on OK I'll just
rename this so click on rename again TGT
underscore marketing regions so if you
click on OK and then apply we are done
with this any doubts with respect to how
to create a flat file as an output
okay since all of you seemed clear let's
go back remove this target definition
and update the target definition finally
let's connection
and let's save this map so you can see
the mapping is well now rather than
manually creating a workflow for simple
mappings like this you can always use
the automated process to create a work
now how do you do that is you click on
the workspace and select generate
workflow from here
now workflow with non-reusable session
is what you need to go so you can find
the details here let's click on next
just click on next and we are done
now if you go inside your informatica
power center workflow manager
inside your workflow you can find a new
workflow created that is WF underscore M
underscore marketing region just open
this and you have your session already
created for your map so let's this check
the session properties
so the transaction details are read from
our database and details of the customer
are read from the slap-a- finally when
you're creating a target file to be a
flat file there are two things you need
to do first is go to properties here now
in the properties tab scroll down to
find the header option here what you
need to do is that you need to set the
output field names to the headers if you
do not do this there would not be any
headers for your data and the second
thing you need to do is that you need to
change the extension of the output file
now by default informatica creates the
output file to be a dot out file so we
need to change it to a dot CSV on sit
down that this click on apply okay and
save this work for you so we are all
safe to go ahead and execute our
workflow select executors right-click
select start workflow and in a workflow
manager you can see the status so it has
successfully executed to get the
properties just double click on it so
here you can see there are two thousand
fifty two customers that you need to
address who are in our low marketing
reach now here comes the question on
where is the output file create now if
you go back to your informatica
installation folder
in the same location that is inside
informatica your installation version
shared in far under score shared you
have another folder called PGD files now
if you open this you can find your
output file present here now again this
is the default location where
informatica creates its output files to
be present so just double click on it
and here you can find the city as well
as the country with the corresponding
card so it's also being sorted into the
required format so that you can get a
good idea on where you need to start
investing right okay
so if you can see here regions up to
three have been considered as part of
the state so any dots with respect to
our second use case so again if you look
at the numbers here we've actually
processed 60,000 rows and then brought
it down to just two thousand fifty two
routes and all this data processing has
happened in only two seconds so again
this is something that you need to
understand how fast informatica is
processing such a huge amount of data
and then giving you the result so are
you guys clear with respect to how we
solve the second use case let's can we
look at the third use case now can I get
a quick confirmation in the chat box
I've got a confirmation from Gowri
Santosh Dave Henry Krupa DP it okay
great to see that you guys have Paulo
now let's go back and look at the third
use case
and on third use is we want to provide
the customers with low number of
purchases the special offer so so this
is again quite similar to what we have
actually done with respect to the second
use case here what we are trying to do
is that we are trying to identify the
customers who made slow number of
purposes so again what we can do here is
that we can modify the same mapping that
we have used to meet our requirement so
who can tell me what modifications we
need to do so you can look at the
mapping here lift is a short out
so this is your complete mapping another
way to represent this mapping can be an
iconic format where just click on
arrange all iconic Li and this is your
mapping or is it iconic format so what
modifications do I need to do here who
can tell me the answer to that okay so
these he has got an answer your VP is
life we don't need the aggregator
transformation or the sort or
transformation who agrees with VP you
forgot who can tell me what exactly were
we doing in this aggregator
transformation we're grouping them by
city and country so we don't need to do
this to get the details of our low
purchase customers so what you can do is
we can remove these two transmissions so
just select them and click on delete
and we do also need to remove the target
definition so this target definition
does not meet a requirement so what we
need to do is that we need to create a
new target definition with all the
details of the customers along with the
count of products that they have
purchased now by default this in turn
brings in customers who have purchased
three products or less so here what I'll
do is I'll filter a little more so what
I want this customers who have purchased
less than three products not just three
products so modified here click on apply
now this gives me customers who have
purchased at least one product but not
more than two products right click
select create an ad table let's modify
this here it from here
let me just rename this
I will call it low customers
click on ok.now do you want me to write
it to a database or create a flashback
now since I have Oracle database present
here on my system I can either write it
directly to my Oracle database or my
flat
okay so dave has a question here Davis
asked me which other database can i
connect my informatica so he did there
are a lot of our databases you can
connect to so you have your teradata you
have your semis you have your info mix
Microsoft sequel server you have IBM db2
as well so again there are different
kinds of databases that you can connect
apart from that you can also connect to
your Salesforce platform and import data
from there as well now here what I'm
doing is I'm going to create it as a
flat file sir and just to show you how
to import from different sources if you
go to the sources tab you can find you
how you can import from PeopleSoft your
repositories now as disabled to capacity
option here apart from that you can
proportion various web service providers
you have power X a in salesforce.com so
again there are different sources from
which you can import your data from and
work with in informatica power center
come back to our mapping designer
workspace let me just remove this
bring in the updated target definition
connected to my mapping
finally let's say this
and now what you need to do is let's go
back to a workflow now here we made a
modification to our mapping now do you
think we can directly go on and execute
our work through to get the output
correct the correct answer to this is no
we need to also ensure that the changes
that are made in the mapping gets
reflected here as well now to do that
what you can do is that you can select
the session to which the mapping is
associated right-click it and select
refresh mapping when you do this you see
the yellow icon present here this means
that there has been changes made to this
mapping so just go ahead and save it and
you can see bi can be change again let's
this check the properties put a nothing
let me check the output details again
needs header and I need to change the
output file extension
save it
the sensi now you can see it's executing
my workflow manager and it has
successfully executed now earlier they
were two thousand two fifty two rows
now there's one thousand 176 this is
merely because I have not considered
customers who have bought three products
from my website
let me show you the output file
who have purchased slow products from my
website now what I need to do is I need
to create a marketing campaign which
will bring them back to my website for
better office so I guys care with this
any doubts with respect to that okay so
now what we'll use we'll check out the
fourth use case now in the fourth use
case what we need to do is that we need
to identify the most selling category of
products now for this we also need to
include our product details table but
should we include our customer details
no we don't need to include our customer
details now let's go back I'll begin by
creating a new mapping
most selling category if you click on
okay now let's go back to a source
analyzer workspace I clear all this and
let's load our details of product
so again I will import it from a file
changed file type to all files I have
product dot CSV
and finisher now coming back to my
mapping first thing again to identify
the most selling product which table
should I use first transaction or
product correct Kripa we need to use
transaction table first so let me load
my transaction table who can tell me the
next step what do you think I need to do
to identify my most selling cattle so in
my transaction table I have invoice
number stock code description quantity
invoice date unit price customer ID and
Product ID as well so what should I do
here okay so the P is telling me I need
to group them by product ID how many of
you agree with that okay so since most
of you consider that is an option to be
used let me put it this way as well what
if we bring in the details of the
product and then group them by the
category in that case what do we do
which do you feel is more suitable
category grouping right great so now
what I will do is I will bring in the
details of the product as well
drag and drop it
and then
from here I'm going to use a diviner
transformation in the joini
transformation I'll bring in all the
details from the transaction table as
well as details from the product table
the condition based on which it's going
to be joined is going to be the product
ID
during the following Pope's
Rep price is not needed seller
the department is needed department here
basically refers to the category from
which it belongs to so that's about all
we need so click on apply and you can
see a product or we have joined these
two tables now what we need to do is we
need to connect it to an aggregator
transformation
and in the aggregator transformation
let's bring in the product ID and the
product department I will drag it and
drop it to the aggregator transformation
here I group them by Department also
what I'll do is I'll add a new column
that you keep the count
now let me add the expression for count
it's going to count the number of times
the department ID comes
so count department that's validated
click on ok kept comply and this so with
this we get the count as well but what
do we need we need to identify the
top-selling categories in our case what
we lose we'll try to identify the top
three selling categories so for that
what we're going to do is we're going to
add a new transformation which will help
us sort this so we have used this
earlier with to the transformation yes
it's the sorter transformation so bring
in the sorter transformation let me
bring all these three in the sorter with
no cell count to be the key and instead
of ascending we are going to use
descending here so all this done I have
all the rules sorted in a descending
order but I need to still identify the
top three selling category for that what
you will reduce you're going to use a
rant transmission now a Ryan
transformation helps you select a
certain range of rows from either top or
from the bottom of the tape so let's go
to transmissions create prank
transformation
and draw now in the rhyme transmission
we need to specify we need the top three
so here you have option to select from
the top or from bottom so we need from
top we need three
click on apply
and finally it's time we create our
target definition before the let the
storage of this
now let's create a target definition
run transmissions create an ad target
let's modify this
top categories
so I created as a flat file click on
apply
technician
and let me connect
so our mapping is valid let's create a
workflow
and let's go back to our workflow
manager so here let's disconnect and
let's reconnect
so you have your more selling categories
if you open it
session properties
as earlier I had the header and change
the extension factor
save it and let's execute this workflow
so here if you see the properties from
3,000 rows back where they're my product
table and 50,000 my transaction I have
got top three selling categories so
let's see the output so these are my top
three selling categories so any dot with
respect to the same so let's you get our
last use case so here what you need to
do is you need to understand the
requirement of the customer region vice
so what we need to do here any cases
it's quite simple guys we work around
with this so again you need the
transaction details any details of the
customer and then what we do is city
wise what we try to understand is what
exactly is their requirement so that way
we will get a good idea with respect to
the thing
so let's go back to a call center
designer and I will create the final
market
so let me create a new mapping i'll call
it m underscore region requirement
customer
so here let's bring in the transaction
details let's bring in the customer
details
let's assign them
you
the joining condition would be customer
ID equal to customer ID so just click on
apply now one thing we do not need is
that we do not need the invoice number
know the product ID
so you need the customers name you don't
need this address we need his city and
country and we don't need the contact
details so apply so let's create an
aggregate or transmission
we need the customer ID the city country
drug it
drop it to the aggregator transformation
in our aggregator transmission what
we'll be doing is we'll be grouping it
based on city
one thing let me add in the product ID
as well so that way in case of future if
you want to know the details of the
product as well so we can include that
as well so product ID as well I'll be
including here so from this I create my
target definition
let's modify it finally clear this
TGT region
tomorrow the climate
they're gonna play
and I create this also as a flat file
because it's easier for me to view it
say this
and you can see it showing us invalid
because it is not yet so let me remove
this
let me begin the new target a fish
connected now if you save it you can see
it is showing touch palette so finally
let me create a workflow
go back to the workflow manager here let
me disconnect and regard
so once I've connected let me just open
it
open this so we have a workflow created
coincide in mapping target definition
it's to be updated
that's here too
you cannot play save it and let's
execute
has successfully executed C properties
we have 3000 321 rules present in our
target definition so let's see that so
these are the product requirements in
the following regions so any dots with
respect to this example so we've talked
about five different use cases let's
recap them so first case we saw how to
identify the most loyal customers so
that we can provide them with additional
discount second case we saw I would
identify the various regions which
require more marketing third case we saw
how to provide customers with lower
number of purchases for a special offer
and then we saw how to identify more
selling category of products and finally
we saw how to understand the
requirements customer region wise so we
need out with respect to the use case
that we have discussed alright great so
now that I've got a confirmation from
Evian let's move on to the informatica
architecture
so now that you've seen the different
informatica power center tools it's time
we look at informatica power centers
architecture to understand how these
tools fit in together
so we have the informatica power center
client tools which we've already see you
have the repository manager the power
center designer power center workflow
manager and power center workflow map so
you know what each one of them does now
all these are directly linked to each of
the corresponding cells now as we've
discussed the integration service is the
major service that helps you in
transferring the data from the source to
the target destination apart from that
you have the appositive service as we
had mentioned earlier a positive service
is the service that maintains your
repository as well as stores all the
data that you'll be creating into the
repository database then you have the
informatica domain now this is something
that you will be seeing mainly in your
organization where you have a single
informatica server and various
informatica nodes connected to it now
when you talk about this in an
organization you will have the
informatica power center client tools
installed on your system and one main
system will have the informatica power
center server tools installed on it so
there you mainly have informatica power
center admin console that would be
working around now so let me show you
how the admin console looks like so let
me go ahead and show you the
administrator console
so this would be administrative login
page so this is what your administrative
console looks like so this is how your
administrative console would look like
after you've logged it to it now usually
in an organization most people who are
working on the administrative side will
be having access to this so not everyone
may get an access to work on this but
since I have configured both my server
and client on the same system I have the
direct access to now since I have just a
single note there's this a node details
present here inside my domain and same
with my license signature now apart from
this domain you can also see the various
connections that you have specified now
since again as I've mentioned it's all
on my same system but in case of a
server client configuration as in
organizations then you will have the
complete connection specified here ok so
again with respect to both my services
now let's say at one point what happens
is my integration service faces
uncharged so this is the place you can
come to and then restart your indication
service and work around with it so it
definitely is something that helps your
with respect to let's say now apart from
this you have the logs tab now log
basically is used keep track of what
every person is doing as to what message
is being shared or war operation you're
performing who is performing which
operation so all that can be configured
here now it can be in my domain it can
be with respect to a service any user's
activity or even the log aggregate can
be used to aggregate a lot now apart
from that monitoring helps you to check
as to what each person is specifically
doing as part of your domain now again
these are mainly used by administrators
as you may have figured out by now so
you don't have to worry much about it as
a reports again is something that you'll
definitely be using for creating
different kinds of license management
reports Web Services reports and so
forth now one thing is if you are
configuring your informatica power
center for a base service then you can
find the details present here it can be
even informatica cloud it can be
salesforce cloud and so far now the
final tab that you'll be discussing
about is the security tab now here the
major use of the security tab for an
administrator is to assign privileges to
each of
users as per their neck now a team
member may have only access to the
client tools but a team leader could
also have access to the depositing
manager based on which you'll be able to
manage and process it now again all that
is actually defined in the security tab
and we'll be seeing how to do that in
our next session so you guys clear with
respect to how the administrative
console looks like any dots with respect
to that I hope you've got a simple
understanding as to how it works now you
don't want to definitely work with the
administrative console when you are
trying to install informatica on your
system and if you not install
informatica and then we also have a
session for that upcoming this weekend
so now that we've seen informatica power
center admin console I hope you guys
have understood the informatica power
center architecture can you give me a
quick confirmation in the chat box ok so
I've got a confirmation from Santosh DT
gallery Dave Kripa that's great to see
you guys so let's talk about this final
stage that is data visualization now in
data visualization is the quite simple
process you have your data and you need
to convert it to a visual graphics again
we've talked about how important data
visualization is now there are different
ways through which a data can be
visualized it could be in terms of a
graph it could be a plot it could be a
geographical map it could be a heat map
so again you have a lot of data
visualization tools using which you can
perform data visualization so will not
go into detail visualization as part of
this session but we'll be talking about
it in our next next session where the
data that we have processed will
directly be loading into tableau and I
will show you to work around the visuals
so with that we come to a conclusion of
today's session and to summarize what
we've learned today we started talking
about what do business needs and how
exactly business insights plays an
important role then we moved on to talk
about business intelligence and how it
helps you achieve these insights from
there we talk about
ETL which is a process which helps you
achieve business intelligence and then
we talked about informatica power center
architecture problem statement and its
solution phases and then we even moved
on to see informatica power center
client tools so any questions with
respect to today's session anything that
you guys are not here with you want me
to repeat
all right so I've got a confirmation
from everyone that's great to see so
hope you guys have a great weekend and
see you for the next session thank you
I hope you enjoyed listening to this
video please be kind enough to like it
and you can comment any of your doubts
and queries and we will reply to them at
the earliest to look out for more videos
in our playlist and subscribe to our
Eddie Rica channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>