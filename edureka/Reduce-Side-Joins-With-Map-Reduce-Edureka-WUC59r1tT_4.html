<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reduce Side Joins With Map Reduce | Edureka | Coder Coacher - Coaching Coders</title><meta content="Reduce Side Joins With Map Reduce | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reduce Side Joins With Map Reduce | Edureka</b></h2><h5 class="post__date">2015-09-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WUC59r1tT_4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so folks today we are having a Wigner on
big data and hadoop session so let me
quickly see who who is joining for
today's women oh okay so they're pretty
much everyone has joined today's webinar
so folks let me quickly get started over
here okay so before we actually start
discussing about the webinar itself I
would like to give a quick overview on
the LMS potent which of a drea how it
works basically so folks here is the LMS
portal all that you are supposed to do
is just log into the portal and just go
to the courses link and click on the my
courses tab so basically we will find
all the courses you guys have registered
for so since this is going to be Webner
I am sure that our so far no one would
have been registered so I just wanted to
walk you through how the system works
basically even if you guys are
interested let's go through this all
courses lap you will find a list of all
courses over here okay currently going
on under it Rekha so however welcome on
board folks so basically for one of the
most happening Jason's Big Data and
Hadoop okay so let me quickly pull up
the presentation for today so the topic
of discussion for today will be very
technical in nature I will not go
through any of the theoretical aspects
we'll just try to discuss some of the
technical aspects so before that I would
like to introduce myself so folks my
name is Mahesh Biscay I am a big data
expert working in the big data industry
since six years having versatile domain
experience as well so I'm basically a
data scientist applying both data
processing techniques plus data analysis
as well okay so I just want to keep the
introduction very short so that we'll
have more time for discs in the actual
technical aspects folks so if everyone
is able to see my screen and hear my
voice please quickly say yes or no the
chart window
that I can quickly start with the word
description excellent so it looks like
pretty much everyone is able to see the
screen
okay so folks today I'm going to talk
about ready sight joints in Map Reduce
so what is the importance of giants
within the Hadoop framework so how
basically giants could be achieved
within the MapReduce okay
so however there are multiple frameworks
around for us within the ecosystem so
our point of discussion I mean which
allow you to basically do the joins
today our point of discussion will be on
top of MapReduce and see how to achieve
the giants within the MapReduce
framework okay so the objective for
today is basically we'll try to
understand what is a reduce I'd join why
reduce I'd join is more important and
then where we actually use MapReduce
then a quick walk through on the
MapReduce workflow after that I'll
actually talk about the steps involved
in implementing a MapReduce program
finally we will also go through an
example program as well to implement the
reducer joins folks okay so that sounds
like a good plan so let's move on them
okay so why we need to join the data so
first of all we need to understand the
difference between the Hadoop framework
and the tradition and the data
warehousing frameworks so data
warehousing our relational databases
pretty much they come in hand for doing
the data joins by default because you
know a to the relational databases where
you are actually strictly defining the
schema and binding the schema to your
data and you are actually validating all
vertical records again is the schema
upfront you load the data into the
tables so with all those things in place
which I mean in fact relational
databases also maintains
indexes as well so with all those you
know things in place it's very easy for
you to actually achieve the joints with
between two different tables in either
relational databases or in data
warehousing kind of data warehousing
tools however when we are talking about
the Hadoop framework
so basically Hadoop comes with both
storage engine as well as you know the
compute engine combined together so when
I say storage engine we have got HDFS
Hadoop distributed file system and we
have got the compute grid we just call
as basically add now and stands for it
another resource negotiated so there is
there are a few other acronyms for admin
as well
okay so however let's stick to some
standard definition over here
so now that you are actually having the
data in the form of the files within the
Hadoop framework it will be quite
complex to achieve the chance however
with the advent of new DSL specific
languages on top of Hadoop make our
lives easier to achieve the joints like
say hi our big kind of frameworks
basically all of you to perform the
joins quite easily as opposed to
MapReduce framework itself however it's
tedious to understand how the MapReduce
framework how within the MapReduce
framework you could achieve the joins
it's because all these DSL languages we
are talking about let's say hi our pick
they actually depend upon MapReduce in
the sense that whatever the queries you
write within the hi framework will get
converted into MapReduce job and the
MapReduce job will get executed on the
Hadoop cluster in the same way whatever
the command you execute within the big
framework so that command will get
internally transformed into your
MapReduce job and that MapReduce job
will be executed on top of Hadoop
cluster folks so that's why it's very
crucial for us to understand I know the
underlying programming jargons within
the MapReduce itself
so let's take a simple example over here
let's consider that I have got two
different files even though the I have
actually organized the data in the form
of a table over here for you were
understanding I mean for your intuition
however the data will be in the form of
a file so if you consider these two
files over here we have good customer
related information one file basically
consists of customer ID name and then
the item then the other file consists of
customer ID city and phone so to keep it
in a simple for today's discussion I had
taken a very basic example over here
because our intent is to understand how
to implement it inside join ok so
however Map Reduce is designed to handle
most complex data sets okay I mean most
unstructured data sets as well to get
the complete details and one needs to
join both the data files in order to get
the information from both these files
you need to somehow join the data
between these two files not really
tables I'm just calling it as files
using giants we can achieve data we can
generate data which would be useful and
sensible based on some key here which is
customer ID so basically considering
these two data sets obviously you know
we can easily identify that we could do
the join between a column called as
customer ID within list 2 files ok so
let's move on here so I just have shown
a sample data over here how the data
will look like after you do the join on
top of the customer ID ok so before we
talk about what are the different ways
to achieve shines within MapReduce
framework
it's quite important for you to
understand you know the significance of
the joints itself the joints are kind of
you know the basic fundamental
programming I'm a fundamental way our
technique for you to do the data
analysis so very quickly especially
because you get the datasets from
different sources okay so when you are
getting the data from different sources
it would be it would become quite
obvious that you might have to do the
joins between those data sets seamlessly
so that you could analyze the data sets
okay so that's I mean that's anyhow I
mean even in relational world a data
warehousing world or even in the you
know big data world so it's pretty much
has a similar impact so however as we
all know it achieving the joins again in
a the relational are in data warehousing
it's quite simple in nature since we are
dealing with the raw data over here in
the form of the files so it is quite it
is a little bit complex and achieving in
MapReduce but not really true because we
have got multiple other frameworks which
help us to do the joins
however since our target is to see how
MapReduce implements it virtually okay
so here are two important ways I would
like to highlight two to which you can
achieve John's one is map site joints
and the other one is review side joints
okay one is map side join and the other
one is reduce I join okay so guys my
assumption here is basically you are all
at least a bit familiar with the
MapReduce framework so that's why I'm
using these terms like mapper and
reducer however I do understand that
some of you may not be you know even
knowing basics offer
my producer give me a little bit of time
so that I could explain about what is a
MapReduce framework what are the key
components involved within the MapReduce
framework and what is the workflow
within the framework itself okay so then
we will go ahead and implement the
program have you were here I would just
like to bring up some level of insight
our understanding on achieving the joins
in MapReduce so as I mentioned we have
got two different ways to achieve the
Giants in general it depends upon the
data that you are dealing with and the
state of the data so that based on these
two aspects you would choose any of
these types of joins again one is it
depends upon the types of the data sets
itself also it depends upon the state of
the data as well to determine which kind
of joins it would would be more helpful
for you to perform okay so let me take
let me start with map set joins first
before we you know go back to the do
side charts so so far are we all
following guys okay I can see Warren
could not hear me and Samuel Samuel has
a question by the way okay so I think
Warren can you are you able to hear me
now looks like pretty much everyone is
able to hear my voice loud and clear
okay so it looks like there is some
issue from your side so I would expect
you to reconnect this to the verb now so
that they she will be solved again okay
so Samuel says what kind of knowledge is
required to understand this webinar
don't worry Samuel I do understand that
you we're all I mean at least you all
know new to the system most of you at
least are new to the system the give me
a moment let me first explain in it the
problem statement and then let me go
into the fundamental so that's why it
will be more clear okay so folks please
try to concentrate here and try to just
understand just try to correlate
whatever I am speaking here with you
were in a single relational world or you
know housing world basically
okay so as I was talking about you know
which types of joints could be used at
what scenario first let me start with
the Maps I join and try to understand
where all it could be applicable okay so
let's take this case there are two to
three different types of maps I joints
okay there are two to three I mean there
are three different types are three
different ways to achieve map so join so
what are those three different ways
basically so one is called a site data
distribution and the second type is
basically called as using distributor
cache on the third type is basically
used by using a a class within the
MapReduce framework all as composite
input format okay so I'm sure you know
most of you are being getting afraid of
whatever the terms I've been using but
don't worry I'll just try to keep the
things more simple as simple as possible
so if I just come out of the technical
world if I try to explain you in simple
language here so basically one of the
key
he approach of choosing a map said join
will be let's say if you are having two
different data sets one with very used
data and other with very small data for
example if you're processing or if
you're analyzing weather related data
okay so obviously we all know that the
Vidur a related readings captured by the
census of very high volume nests in
nature so you were dealing with very
large data sets over here at the same
time if you try to join that weather
related data with the stapes of that
particular country
okay adults the stations where these
where the recordings has been recorded
okay so the amount of the station
specific data will be very very less in
nature maybe somewhere around one and
br-2 in me kind of volume okay so you
are having some terabytes of data
sitting in the HDFS also you have some
states are stations related information
which is very small okay so this is a
very good use case for going with maps a
joy okay this is a very good use case
really should consider maps I join as
the as your best friend and it's very
easier to achieve the maps I'll join and
this kind of datasets as well then the
second use case is basically if you want
to perform a very efficient joint so if
you do not really have more time to wait
to get the joins okay assuming that the
data is already pre processed then them
again the match a joint is the right way
to implement you know the joins on such
pre-processed data
so these are two key considerations for
map sight joints so it happens on the
map side itself so don't worry what is
some map where these reviews I'll just
talk about them in a moment and then
done within memory when data is big
other data is worn just now explained
and then it's a very expensive operation
because it expects the data should be
pre-processed for some use cases okay
then moving on to the new section
basically this is how this is this is it
is the inbuilt mechanism within the
MapReduce framework itself to achieve
the joints so it's basically in bulge
there is nothing assets you are trying
to do here to achieve the joints because
the framework itself has the ability to
do the joints by default all that you
should do is you should know how the
framework behaves based on that you know
you should make sure that the data will
get chai and based on your requirement
so today our point of discussion will be
radius adjoin as I mentioned so it
happens on the reduce on the reducer so
done off memory so the greatest
advantage doing off memory is basically
you can board the data sets could be
highly voluminous in nature so no matter
if the frost date as it is some
petabytes of data and the second data
set could be terabytes or petabytes of
you know data set still you could easily
perform the joins between these two data
sets with radius I join okay and it's
very inexpensive operation in the sense
that you know need to really write a
really write a very complex code it
because it's involved within the
framework itself it's very easy to
achieve okay so I'm sure most of you
would have got confused about
what is map what is reduced so why we
are talking about giants happening at
map site and reduce and so you give me a
moment so do me a favor
I just for waiting for another couple of
minutes other five minutes I'll talk
about all these things in detail okay so
where should I there should reduce I try
and be used so these are different key
considerations for the new site join
basically if I would like to highlight
joining data is arguably one of the
biggest use cases of herding so joining
petabytes are terabytes of data is one
of the biggest use cases of hurdle
because most of the analysis o happens
over joining the reticence and then when
one needs to implement joints joints
simple steps okay reduce site joins our
stride forward due to the fact that
Hadoop sends identical piece to the same
reducer so by default the data is
organized for us so I I know that this
is very complex statement but it will be
crisp crystal clear in a moment for you
I'll explain about that in detail folks
ok so let's move on here first try to
understand what is on map or and reducer
then we will come back and try to read
those points so that you know will get
good enough understanding so before we
go ahead with reduce I'd join let us
refresh quickly about what is a
MapReduce what is MapReduce framework
what is a mapper what is a reducer how
mapper and reducer will communicate with
each other so let's try to understand
all these points over here so even
before that so let's try to understand
what are the applications of MapReduce
ok so the simple applications of
MapReduce framework a MapReduce
programming paradigm is everywhere is
everywhere but I just wanted to
highlight you know by taking a very
simple
use cases over here so MapReduce can be
used for analyzing the weather data sets
and to predict you know the weather
itself and to do the weather forecasting
basically you can liveries on MapReduce
kind of programming paradigm why why
should I live race on MapReduce kind of
programming paradigm is it really new
kind of technique which is being
introduced with hurdle absolutely no no
because MapReduce programming paradigm
has been existing since ages with very
older languages as well like Fortran if
you consider any of the traditional
native languages most of them also
already had or at least used MapReduce
kind of programming paradigm but what
makes her loop so interesting no
interesting let's try to understand that
here so first of all even before that
let's try to understand what is actually
a MapReduce framework is so folks
MapReduce programming paradigm is all
about instead of sending the data to
computation you just pass the
computation to the data itself so that's
the core paradigm of the MapReduce okay
so you are not actually sending the data
to the competition instead you are just
sending your program are sending your
competition to the data so that you are
analyzing the data in its location
itself to which you are achieving high
performance okay also high performance
processing high wall luminous addresses
when you're processing high volume the
statuses also most importantly MapReduce
follow something callers divide and rule
kind of policy where it tries to operate
or it tries to process the data
by chunking down the data into different
pieces and going to each piece and
trying to process each such piece
independent to other piece of data okay
so that's how it basically I choose the
parallelism infinitely let's say if your
data is sitting across thousand nodes
you can just write a simple map news
program and let the MapReduce framework
take care of processing the data
residing and all those thousand nodes in
parallel at the same time okay so that
you were see you at if your probe your
processing time will be reduced
drastically okay so he was seamlessly
racing on all the resources computing
resources of all those nodes okay so why
are basically for whether it is because
whether data sets are very high volume
as in nature so you are dealing with
some terabytes of data so you need to
have a MapReduce kind of parallel
programming technique to process these
huge datasets in parallel then
de-identify personal healthcare
information so this is another very very
interesting use case especially because
you are nominees with the advent of in a
cloud with the advent of Internet
basically we are able to capture the EHR
our EMR records electronic healthcare
records you know from different
hospitals the volume of these EHR
Auriemma datasets are very high in
nature so especially if you consider one
of the interesting use cases in
healthcare to mine is if you want to you
know outsource that data from one
company to other company you can't
simply shave the data as ease to the
other company for various reasons
because health specific data is highly
sensitive in nature so that's where you
need to encrypt the data and then you
could be able to shape that data sets
box okay so this is another interesting
use case where again MapReduce comes
into picture okay so folks are you able
to hear me properly so far because I can
see three says she couldn't hear me
properly three are you able to hear me
no okay so looks like she has an issue
from okay so now he is able to hear me
okay so let's move on folks okay so
we're MapReduce is used so let's try to
understand you know where MapReduce is
coming from and what kind of use cases
it's halls so if you see here MapReduce
framework basically it's been published
by Google so Google has actually
published a couple of papers when is gar
first Google file system which was the
which is the distributed file system are
running on top of the commodity hardware
okay and then MapReduce which is called
as Google MapReduce so these two papers
they have published long before okay so
what Apache Fame I mean what Apache did
is basically so not really a party let
me talk a little bit about our who also
here so what ahoo did is basically they
have taken that white papers as a
reference and they helped build a
framework called as a loop okay so I who
has created the framework called as a
loop and they have out used Hadoop
framework through Apache so now so
Apache is a I mean Apache Hadoop is a
core framework available for us as open
source framework however there are many
third-party distributions for MapReduce
books I'm sorry howdy folks
so Apache Hadoop basically come
presence of many ecosystem frameworks
but whenever I am saying Hadoop it
basically means it's nothing but HDFS
Hadoop distributed file system which has
been developed on top of GFS and then no
Hadoop's MapReduce so it has been
implemented on top of Google's MapReduce
and then now we have got you know an as
well and basically now is a compute grid
for Hadoop for this generation too and
then there are some ecosystem frameworks
as well like pick high on HBase so
Apache actually a woman where appache
you know now we can access the open
source Hadoop then used him so we're
actually MapReduce is used
it's basically for indexing and
searching classification of the data
recommendations and then analytics so
these are most complex used cases
however you know if in order to explain
you in simplest terms here here are some
easy examples for you to understand for
summarization basically you need to
build an inverted index so that's very
bad videos could be useful for you then
top end records within the
classification to identify the top-end
records and then sorting kind of use
cases under recommendation so there are
two ways to look at all this high level
use cases like recommendation
classification and summarization so most
of these are I know similar to that of
the mission learning kind of terminology
where we talk about most complex use
cases especially if I say
recommendations we all know Amazon
YouTube kind of you know internet our
e-commerce kind of websites basically
how built very intresting recommendation
platforms are even Apple kind of in a
companies say analytics could even stand
for very a simple analytics like doing
the choice and selection so coming back
to MapReduce basically what is some
MapReduce so MapReduce is nothing but in
a MapReduce
job essentialy you will have two
different phases one phase is called as
mapper phase and the other phase is
called as the user phase it's nothing
but mapper is nothing but a simple
function reducer is nothing but another
function okay mapper is a simple program
reducer is a simple program where you
write your business logic into the
mapper R into the reducer so basically
for you to understand what is MapReduce
basically MapReduce is nothing but there
are two different functions or one is
mapper function
the other one is reducer function RL is
just a program okay then what are the
features of the MapReduce framework
basically it's a programming model it's
been designed for a large-scale
distributed in a data modelling and it's
basically you know has a fundamental
feature of doing the parallel data
processing as I explained just before
okay so let's get into the details of
how MapReduce works okay so so for any
questions guys are we all good so far I
can take a few questions here okay sure
excellent excellent
so let me proceed here so let me try to
explain you basically what's the
workflow of a MapReduce program okay so
for this what I am going to do is I'll
take a very simple problem which is the
most fundamental problem which is called
as word count problem okay so what what
do I like to do here is basically given
say one terabyte are tender by two file
to me I would like to count the repeat
count the words within that prepare file
or files okay I would like to extract
the
number of times every world repeated in
that file are in the pot okay so that's
my key problem so let's say if I how a
statement like a breaker has you know
advanced courses breaker is the say most
reliable online course course let's say
I just taken some randomly some example
here to explain basically what is what
count so in this file if I want to count
so there will be say it rekha the count
will be two okay so and then has count
will be one advanced okay count will be
say 1 again let's see if there are these
lines are repeated in the same file
let's just take for example if these
words are lines are repeated then this
will become 3 okay so let's see if I
take YZ YZ will become 2 because I'm
just trying to count the words number of
times it has repeated in the in that
file are in the number of file so this
is the problem statement let's try to
understand how MapReduce tries to
operate on this kind of problem let me
take a very simple data for you to
understand very quickly so let's see if
I how some data set like say rework beer
and then say near then car early were
near so beer ok car that's a free work
ok so I'm just taking some sample there
does it here ok select me just randomly
pick another 2 lines
okay okay
so folks let's say that your file
contains these words you could ask me
now so why are you use taking just a
small file of when MapReduce is supposed
to process some terabytes of fine
definitely MapReduce is not designed for
this kind of small files instant
MapReduce is designed to process very
huge amounts of data even though you
could process even the small files as
well but for the sake of simplicity to
understand the problem
this dataset route is suffice for
understanding the problem statement here
folks ok so let me explain here so what
MapReduce this is in short basically it
will divides the actual file into
multiple pieces let's call those pieces
us say input splits so I'm just calling
this pieces of data as input splits so
once that I have two inputs splits
readily available for me then what I
will do is whatever the map of function
and reducer function I have created I'll
just try to run that map random user
functions on top of this independent or
on top of this input splits so what
happens is basically let's just take
some assumptions here because we are
just having one however it's very hard
for me to cover everything but let's
let's try to either get some try to take
some assumptions as well let's take an
assumption here saying that say for your
mapper the input will be a key and then
a value let's take the key as a dummy
key so just ignore about what the key
will be but the value will be aligned in
each iteration so for every time your
mapper function will get line as the
input a single line from your file as an
input so
what mapper will do is napa will split
that line with white space characters
and emit intermediate data okay which is
again a key value pair so once I split
let's say I'll take map were here okay
so mapper one I'm just calling it as map
of one and then say mapper to here and
then I'm just calling this as a map or
three because there are three different
input splits and my function has been
you know divided into three and then
it's been running in parallel on all
these three input splits so that if I
take this piece of data here if I split
this line over here with whitespace
characters or basically Elliott River
and then beer and then here so what I do
is once I get this place I'm just
imaging some intermediate key value
pairs where my key will become the
actual word and the value become a
hard-coded value call us one because
this one actually indicating that I have
seen this term or this word river once
so that's why I'm just omitting this key
as the work and then the value as the
count are the occurrences the number of
occurrences of that term which is one so
I'm just hard coding that value here and
I'm omitting that for the first
hydration for the second iteration it
will take the second line obviously it
will image it will split it will perform
the same operation so what is the
operation of mapa
it is split and then emit split and
image so once it split this car river
and new so there will be three keys one
is car remover an ear and a hard-coded
value of 1 1 1 so obviously the output
coming from this will be will this will
look like this mapper will look like
this ok so this is what it is going to
do my mapper so exactly same way all the
mappers will perform the same
operation but on different data sets so
this mapper also obviously is going to
do split and emit okay say if it does
the same thing so there will be beer one
car one and so on
beer one car one and so on
so it basically Oliver mappers are
operating on individual pieces of data
and they're processing the data and then
emitting some intermediate records so
all these intermediate recalls will be
pushed back to something called as a
reducer function okay so something
called as reducer function and at the
reducer side let's see if there are two
reducers defined for your MapReduce job
say reduce one and reduce to so what am
I going to do here is basically I'll
take some data let's say River here as
the input River with the list of values
here okay so with the list of values say
what are the values associated with
River so let's take this River as one
here and then there is no other
repetition here so let's see here say
again there is a river two times here so
there will be 1 comma 1 comma 1 and then
River East appeared here is here as well
so again 1 comma 1 so ideally what is
happening is internally MapReduce
framework is joining based on the key so
it is taking the key and then joining
all the values and then just emitting a
key and the list of values to a
particular reducer to a particular
reducer let's say we will reduce 1 so if
the reduced 1 is getting everywhere so
always it will get that free one okay
and then it might if it is getting car
so car will be here with all its
associated values because framework
internally is taking care of joining all
the values are so
with the third particular key no matter
from which mapper that key has been
emitted okay so now that reducer has
caught the data what reducer will do is
it will iterate over the list and do
count say this I trading and count so
what what will happen here so when it I
tripped on all this list so it'll this
is a list right so if it items for
symmetry get one second I'm one third
time one full timer and fifth time one
so meanwhile it is also being counting
yes well so if it does it counting or if
it does that some let me change this
instead of count let me call it a sum so
if I I try ten count sorry do some
basically one plus one plus one plus one
so it will become five right so reducer
also will emit a key value pair ideally
the key will be the term or the word and
the value will be the actual sum so
finally after your MapReduce job is
being successfully completed you get the
count of each of those words available
in the in that particular file I'm sure
there are a few things which will be
quite confusing for most a few of you
it's because of the nature of the
session itself this is just to explain
how in high-level how the MapReduce
works but we are not really going into
the detail understanding here folks okay
so now that we understood how the
MapReduce works so basically it's a
high-level flow over here you have the
list of Furth you have the file so some
each input split is operated by a Mapple
like this and mapper does some
processing basically madmapper is
splitting based on the whitespace
character and emitting the tree and then
the value so those all those are
actually going into your face called a
shuffle phase which I have not
introduced you guys it will be more
confusing if I introduce you so just
imagine that shuffle is the face wave
basically the join happens by default
upon the key so once the data has been
joined then Ollie will chime key value
pairs like this the key and the list of
values will go to the reducer and the
reducer said you can apply some more
business logic let's say in our case we
are just doing the sum and finally
empting the final research okay so let
me move on here quickly MapReduce job
submission flow see how the input data
you are splitting it into two different
nodes so let's say node one has an input
splitter node two has an input split
when I send node it's basically nothing
but it's fundamentally it means a
machine again so since you are having a
cluster of machines let's say we're
having two machines you are just
splitting the file into two pieces and
putting it across two different machines
which I am calling it as NORs
so input data is distributed across the
nodes and then in the next step
each map tasks work on a split of data
so there will be n number of mappers
based upon your n number of input splits
if there are n input splits you have you
know n map was running for your
MapReduce job okay so parallely
processing each of those inputs splits
of the data the map or output some
intermediate data it does some
functionality based on whatever the
requirement you have you just key in
that functionality into the mapper and
it just in its and into out image some
intermediate data so that intermediate
data is going through some phase called
a shuffle phase and then coming back to
the reducer so where the reducer is
performing some operation and then
emitting the final result so this is the
whole workflow however we have actually
hidden lot of the technical aspects over
here to make our to keep the session
simple and quick okay so let's move on
here so how does how it works renew
sides so now let's come back now that we
have got at least some rough idea on
what is a map reproduce
framework in the sense that mapper is
some program reduce it is some program
are function okay and then you can apply
those on each record I mean each key
value pair so if you understand that
that's pretty much sufficient so now
that we understood that so let's try to
talk about the reduce I'd join here
so apart from T's we are trying to tag
her to identify the source of the file
from which the reduce side joints so
apart from the actual T's what we'll try
to do is we'll just try to tag so let's
do this here instead of just going
through this text
let me first quickly you know show the
data sets and and then we'll come back
here and we'll try to understand that
will be more in to do okay so I just
try to quickly pull up the datasets here
okay so I have this customer related
data which has this customer ID first
name last name and some information over
here let's assume the customer age and
then here the customers designation and
then I have second dataset over here
which basically has again the customer
ID then some date of birth of the
customer oh sorry this is not customer
ID probably this is social security
number and this is some date of birth of
that customer and this is customer ID
then this is the amount of transaction
and this is something like the category
then here is what the customer has
purchased then some bla bla bla
information I mean the city and in the
state and finally whether it's a credit
transaction a debit transaction so now
that we understand the data sets as well
let me quickly go to my virtual machine
and start the Hadoop cluster okay so I'm
just logging into my whole do my virtual
machine which I'm running on top of my
Windows machine okay so now that my
virtual machine has already started okay
so let me let it's taking a little bit
of time here to load okay so now it's
loaded so now that I have started let me
quickly open up the terminal so this is
basically my VM is running a whuppin to
environment I can say I'll do some time
in the end to take your questions let me
just quickly finish this program okay
and then uh I just will go to your
directory where I have set up everything
here then let me quickly start my Hadoop
cluster okay Hadoop to that full of one
and then it's been slashed taught all
that I say so this basically starts my
Hadoop cluster in the sense that he can
start the data node it will start the
name node it will start the second
rename would also resource manager and
then node manager under the an framework
so
the services from in the background
meanwhile let me quickly go through the
actual program so how am I going to
achieve the joints here reducer join
here so folks if you see here this is a
Java program I mean you're not from the
Java background so import some classes
I'm importing some Java specific classes
which is I were exceptions and then you
can see here most of the aps are
imported from Hadoop
you
most of the ApS are imported from Hadoop
like configuration path text so all
these classes are from Hadoop so in my
class which I'm calling it as a reduce
joint let's start with the main method
there is there are three key components
when it's main method when is mapper
class and reducer class so in the main
method
I'm just instantiating in an instance
called as configuration instance which
is required for the job to be run in the
Hadoop of Hadoop platform and then I'm
instantiating the job instance college
job and then you know describing it does
reduce I join save Java class name so
I'm setting this class name as the
Choctaws reducer class what is the
reducer class for my program and what is
the mapper class I can sit the mapper
class as well okay so let me quickly set
the map of class a job judge set mapper
class which is say let's say this okay I
got it I got the idea okay so basically
what I'm trying to do is I have actually
set the classes here customs customer
class and then the transaction class let
me explain you a little bit over there
so after I said that in user class then
set output Nicolas has text an output
value classes text okay so I'm just
specifying my framework that what is my
output key output value now that I have
defined most of the stuff I had to now
configure my mapper so in this you have
got two different files when his
customers file and the other one is
transactions file again there should be
two different mappers processing these
files if you are having two files with
customers data you no need to have two
different map was since you are
processing two different data sets here
I'm using a map with two different
mappers so in order to configure this
mapper I am calling I'm using something
called as multiple
inputs for this job the odd input part
I'm passing the job instance and then
the part where my file is located and
then the input format type as text then
the mapper classes my customer mapper
and then input part same thing but with
transactions mapper then I'm
instantiating the output part as well
file output format that set output
partner and sitting the output path and
finally submitting the job to the
classroom by calling the wait for
completion method so if I go back
quickly here I have my customs my
customers mapper class which is
extending the mapper class from Hadoop
framework and then overriding a method
called as map method so in this
basically what I'm trying to do is I'm
taking the value converting it into a
string and then splitting it based on
the comma because you would have noticed
that the data is a comma separated value
file CSV file
and after that I have split the data
with the comma then I'm just omitting
the first part of it which is this one
first part of my customers file which is
this customer ID and then the remaining
part over here but I am just hard-coding
some text like see you with tears over
here before I am in parts one so parts
of one basically condense this first
name over here okay and then in the
transactions mapper
I'm doing pretty much the same thing I'm
splitting the the each line based on the
comma and then omitting parts of two
which is basically you can see here so 0
1 2 so 2 is basically my customer ID so
I'm everything that has the key so you
might have understood by now so how the
join is happening because the key is
same here emitted by this mapper and
this mapper is same and then text so
basically I'm hard coding something
called as transactions and then the
parts of three so parts of three
basically contains no amount of
transaction here so in the reducer what
I'm trying to do is basically I am
getting them I'm just defining some
variables over here I trading through
the list of values based on the join so
we know that reduced
I mean framework itself will disjoint
based on the key so if it sees two
different customer ID is the same
customer ID coming from customer mapper
and coming from say transaction mapper
it will just automatically does the join
and just omit those lists of values to
it so now that I am i iterating with the
list of values I'm just checking whether
the value is starting from transactions
then I am just doing the sum if not I am
just you know hard coding it that the
hard coding that value is named over
here and finally omitting the name and
then the amount of transaction over here
the count in total so basically I'm
doing the count as well as amount of
total amount of transactions a total
number of transactions and then the
actual amount of transaction as well so
now that I have prepared my job and now
that we understood the
I'm just quickly exporting this job I
said char file Java archive file I'll
call this jar file ass let's say reduce
side dot jar so now that I have defined
the ready reduce ID jar
okay so here is my reduce ID jar which
has been exported into my desktop here
I'm just copying it into my VM so CP
let's say mod and then he refers desktop
reducer char here so now that my jar
file is available here I can now run a
MapReduce job here folks so in order to
run the MapReduce job so I need to have
that files located in my MA Hadoop HDFS
filesystem so let me quickly see where
that files are available for me and more
- test otherwise I can quickly copy
those two files so I think it looks like
I don't have those files it's better I
will copy these files quickly so I'll
just save again on my desktop as
transactions and then on my desktop as
customers okay so now that I have both
my files in my host machine so let me
copy those files as well here okay and
then here yes we are almost done okay so
now all that I'll do is hardware first -
put I'm just copying this customers file
into your pod say your mark test okay
and then I'll copy the other file as
well quickly say which is the file
transactions into the amat test again so
now that I have copied the data into the
HDFS part as well so and all that I'll
try to do is Hadoop
char then I'll just pass the jar file
name which is reduce side the char and
then the input path
the first input part should be the
chance hi I think customers so then the
customers curse dot txt and then second
input path will be transactions so
transactions dot txt and the third will
be the output mod test reviews join
output ok so that's it folks so now it's
his class not phone ok um my apologies I
just forgot to specify the class name
here so the class name here is reduce
join ok reduce joy so that's it so it
has actually started a MapReduce job
which we could monitor from the web
interface as well to see what's the
status of the job folks so even on the
console itself we can get the output as
well so he can go to the ant cluster
HTTP colon localhost a generated cluster
it will basically go to the resource
manager and we'll try to pull up the
status of the status of all the jobs
currently running on see here you can
see that particular application reduce a
join has actually been picked in the
progress bar also has been started you
can see the actual progress of the job
as well here
so once the job gets completed so now
you can see mapper is hundred percent
completed and rhodesia will start
immediately so after the mapper after
all the mappers are completed then
reducer will start so before it comes
out so basically let us go ahead and see
what's happening here so files have been
taken in the mappers and then we are
just tagging either it's a customer file
or it's a if that record is coming from
customer file we're just are attacking
it as cursed otherwise we are talking it
does say transactions so in this are to
shuffle automatically will take care of
joining the records based on the key
where the key was a
customer ID and then we are just doing
the counting and summing so that's what
basically all about doing the joins
folks so now you can see the giant has
been successfully ran and the output
will also be there I'll do the first -
Alice which is a more - test /r abuse
style
I think reduce join say output it may
have generated some part files so there
is one part file okay if I say Hadoop FS
- text it's like printing the house file
okay if I do just say pipe mo it will
quickly emit so the here is the person
here is the number of transactions he
made and here is the amount of
transaction so how will this analysis is
helpful for me it's basically if I want
to understand let's say top loyal
customers top 10 loyal customers I can
write this kind of MapReduce job where I
can do some sorting as well and then can
get the top 10 loyal customers from the
from my database or from my HDFS file
system so folks so that's how you
achieve the reducer giants anyhow it's
very short and fast I know there will be
a lot of questions as well are there
could be a lot of confusions around
because in one however you understand
right so how much we could we can cover
so ideally the idea over here is
basically to explain you how reduce ID
shine works so folks if I if you have
any questions I can pick up your
questions and then probably we can call
this issue
okay so Neela says please explain the
reducer in detail
so however reducer is nothing but it's
just a Java program it's just another
program Neelu all that you are trying to
do is you're just defining your business
logic in the reducer so whatever the
intermediate data which has been
generated from your mappers will go into
a reducer but before that what MapReduce
framework is doing is basically it is
aggregating based on the key and sending
the list of values to the reducer
respect to e value 3 and list of values
is that kind of clear Neelu and then she
versus can you show me an example
so here I did write maybe this you have
asked before I shown the example called
manases first is it wilt avoid as
blogsite absolutely calm on e it will
first device the data into blocks okay
and then on top of it it has a concept
our notation as for input split so there
is a slight difference between a block
and an input split so block is physical
in nature input split is more logical in
nature so does that answer your question
Coulomb on Albani I'm sorry if I didn't
pronounce your name properly
Komachi says based on what criteria this
input splits assorted so input splits
are not sorted as such but MapReduce
framework whatever the output has been
is emitted that has been sorted based on
the key but your input split will never
be sorry okay
it depends I mean if your input split we
are getting from other MapReduce job it
would have been sorted based on the key
used by that particular MapReduce shop
is that clear Komachi and then Ravi
conscious what if same values repeat in
the same line it does not matter Ravi
Khanna it just emits
that same value I mean same word and
then the occurrence as one that we can
then share cherish is kindly provided as
a reference material stalling
Hadoop on VM definitely cherries we
already have enough material within the
Hadoop LMS portal itself there you will
have a step by step explanation on how
to set up the Hadoop on VM you can even
request the support team as well
Hadoop at 80 record Co okay Raj says can
you also show this color program for
this Suraj
now showing the scholar program is quite
difficult with one hours time frame so I
believe you will understand about it
however scholar comes into picture only
when you are using spar kind of
framework okay so in Hadoop there is no
native support for Scala hope is purely
a Java based framework however the
things are changing slowly maybe sooner
or later he might find a Hadoop totally
built on top of scholar but not now at
least does it answer Suraj she says how
can we have reduce I join and having and
and or okay good question so that logic
has to go anywhere reducer itself okay
so you can define that logic if you want
implement and and or you just define
that logic or within the reducer itself
Ashish because reducer is totally under
our control right even map as well so
che Tricia's difference between Hadoop
versus park so simple difference is
basically Hadoop is a batch processing
system where it mainly relies upon the
file system and the disk to do a lot of
operations SPARC is mainly an in-memory
data processing system okay so I know
it's kind of very vague
you know the difference however
considering the limitation of the time
that's what we could cover
for today cherish then Krishnam says
could you please do a short description
and what is partition and calm manner in
MapReduce partition is the one which
basically decides how the keys will be
distributed across different reducers so
that's the definition of partitioner and
then come burner is the one which makes
it will job more efficient in the sense
that it does map side joins as well I
mean map side aggregation sorry not chai
and abseiled aggregation so that the
amount of intermediate data being flown
from mapper to reducer will be
drastically reduced through which you
obtain lot of performance I don't
remember who is that Krishna I think
Krishnam vision browser then Komachi
says how the input splits are made line
by line or block by block oh it's not
really line by line it's basically a
block only for you just for fun time
being you can just imagine like it's a
block but it's slightly different than a
block
Komachi because it's a logical division
of the data so gousa densest will
shuffling a curve first are starting
occur first very good question so
sorting basically people most of the
people are under the impression that
something happens at reduce aside but
it's not true
sorting first happens in the map aside
itself and also happens in the user side
society happens in both the phases after
the mapper has completed and before the
reducer is about to start
okay so shuffle and sort what happens at
the same time after the mapper emits the
intermediate data and that output been
generated from that particular map
actually the data will be partition and
then started then the shuffle phase
happens at the reducer side where we
were based on the key he'll aggregate
the values at the reducer side okay so
for this to understand in detail we need
to actually understand the the
architecture our understanding the inner
workings of the it's hot and cheerful
phase itself but in shot or for you to
assume let's say SART will happen first
and then sit shuffle will happen goes of
them mono man Mohan Lal basically says
joins and renew side are done off memory
means Ram is not used for it joins on
radio sign are done off memory means Ram
is not used for it it's not really true
Ram is still there but actually it is
been happening and the partial data said
but not they are not an entire dataset
Mon Mon Mon obviously it can't load and
headed into ramps but it loads a partial
it are required to do the join and then
does the join it will happen in memory
itself obviously then Shoshanna says or
what is the input split and HDFS block
this is life difference I mean now I it
will be very hard for me to explain just
imagine that you know block is like a
physical block and inputs abilities like
a logical split on top of a block
Shoshanna then I'm it's a sir I'm sure
okay so how do I get the recorded video
you can contact the support team a myth
you can get it if once you contact them
first else's Pro still says the number
of mapper is equal to the Block in
achieve is not really block but input
splits number of inputs plots ok
situation this has any difference okay
so she Shana I just explained a very
high level I hope you understand then
she says for and it is is it possible
can you tell an example coding or
condition okay so it's basically
whatever we have run now it's basically
an R condition itself okay in your
reducer let's see if you are getting two
regards coming from mapper a and map of
B let's say if one record is missing it
doesn't matter we can still process that
and emit whatever the intermediate data
you have it you have in your producer
so you no need to necessary filter that
particular record in the reducer logic
that's how you achieve our attrition she
was another says what is the identity
mapper identity mapper basically takes
each key value pair from your input data
and just image as is without being any
changes okay then there will be added to
the user as well krishnam raju say
starting happens first and then
shuffling yep right true one more loss
is what sorting technique is used for
sorting the data on mapper side so if
I'm not wrong it will be it will be
general starting but I know about the
user side reduce the side its it will be
basically mod sat at the reducer side
but the map aside it could be same but
we'll saw the whatever quicksort may be
so it depends over there
then Shoshanna says okay so very concise
this is one of the good training which
helps a lot
it helps a lot unclear thanks we can
thanks for that so folks that's pretty
much it from my side hope you guys have
enjoyed this webinar
I tried kind of I tried to keep more
information as possible considering the
time limitation for us
okay so we tried to just understand at
least one use case for Hadoop it is they
decide giant at a very high level okay
so thanks folks thanks for joining and I
hope you guys all enjoyed decision okay
she wishes thank you so much thanks for
thank you as well thankfully thanks
Komachi okay so bye everyone time okay
have a good night and have a nice day as
well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>