<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop MapReduce Example | MapReduce Programming | Hadoop Tutorial For Beginners | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hadoop MapReduce Example | MapReduce Programming | Hadoop Tutorial For Beginners | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop MapReduce Example | MapReduce Programming | Hadoop Tutorial For Beginners | Edureka</b></h2><h5 class="post__date">2016-12-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x-PCNX4prLA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is we need from Eddy
Rekha in today's session we are going to
focus on Map Reduce examples let's move
on and look at today's agenda the very
first thing that we'll do is we'll
revise the Map Reduce way that we
learned in our previous sessions going
on we'll explore the classes and
packages that are available in Map
Reduce
using which one can write a MapReduce
program I'll be explaining you the
various components of a MapReduce
program like the mapper the reducer and
the MapReduce driver okay we will be
looking at MapReduce examples on
analytics and we will also explore
MapReduce examples on testing ok noise
are you clear with today's agenda if you
can write down on the chat window that
would be great
Omar says yes matt says yes Himanshu
says yes Jessica says yes Himanshu says
how many practicals are there
Himanshu don't you worry the entire
session is based on practicals ok so
you'll have at least 2-3 practicals for
today's session
does that sound interesting guys Manchu
says great so let's move on and we start
off with Map Reduce example onward come
we already saw this example in our
previous session however let's refresh
this so that we do not miss out on
anything as you saw we had input file
which was divided into input splits
these plates were passed on to the map
functions the output of the map
functions was passed on to reduce
functions and an aggregation task was
performed at the reducer finally the
complete output was sent back to the
client
ok now this was the example that we
looked at the entire input was Deer Bay
River car car River deer car pier we had
three inputs plates deer be a river that
is one car car River that is second the
third one was deer car pier there was a
mapping phase in which the count of 1
was assigned against every word that was
there in the input split so deer comma
one beer comma one River comma 1
similarly for input split two and for
input split 3 in the shuffling phase all
the keys had a list of values that was
assigned by going through all the
results that was given out
the mapping function similarly so for
beer it was one comma one because beer
one and then another beer one was there
in third split
similarly happened for car so it became
car comma 1 comma 1 comma 1 the same
happened for deer that is 1 comma 1 and
finally happened for River that is River
1 comma 1 in the reducing phase for
every key the list of values were summed
up and the result was beer comma 2 comma
3 deer comma 2 river comma 2 and finally
this was the final output on the right
which was sent back to the client so
this was the word-count process using
MapReduce guys I hope you can clearly
recall what we learned in the last
session before we move on can you give
me a quick confirmation Omar says yes
Himanshu says yes Jessica says yes what
about Matt Matt says ok let's move on
then guys before we move on and explore
the input and output classes in
MapReduce I wanted to tell you that the
input data said that you push into your
mapper functions or the output that is
given out by the MapReduce program all
together can only be in certain formats
now these formats are nothing but the
data types like you had in Tangela
ok you had string in Java similarly you
have certain data types that are
available in MapReduce in these formats
only MapReduce can accept the input or
can give the output finally ok
so let's explore the classes that are
available first we'll go through the
input format now we have an input format
which is a superclass which is present
in the package urg Apache Hadoop dot
MapReduce we have three different types
of input formats with an input format
that is the superclass we have file
input format we have composable input
format and then finally we have DB input
format ok file input format is the most
commonly used input format that is there
which has a list of input formats under
it that is like combined file input
format text input format key value text
input format n line input format
sequence file input format sequence
while input format is something which
takes sequence files as an input
and line input format takes n number of
lines as an input altogether text input
format takes one singular line which is
in text form right similarly we have
composable input format which can be
used and then finally a DB input format
so guys please don't worry about this
right now I am going to take up certain
examples in which I will be using one of
these input formats which will give you
a better perspective how to use them
okay are you clear
good thanks Omar so maher says can't we
have custom input format absolutely Omar
you can also have custom input format
however we have not going to discuss
that in today's session it is because
first we need to understand how to use a
normal input format and then finally
write a MapReduce program okay once we
are done with that in our later sessions
we can certainly go through this topic
as well is it clear so we have a
question from Matt and he says according
to the diagram file input format and
composable input format has two
attributes as input that is K and V
whereas DB input format has only one
attribute that is T that is a very good
question Matt and let me tell you at
this moment that is composable input
format takes two attributes which is
nothing but the key and value okay and
this could be either of the data types
that are supported by Hadoop like in
writable long writable and so on okay
whereas DB input format has only one
attribute which is of type D be writable
which is nothing but reading a data from
a table from a database is it clear to
you so Matt says okay great let's move
on and explore the output formats that
are available which can be used in a
MapReduce program now this is the
superclass that is output format which
is present in Watchi apache hadoop
mapreduce package now there are
basically four types of output formats
file output format the most commonly
used null output format in which there
is no output that has to be given DB
output format which gives out DB
writeable kind of an object filter
output format okay that is the fourth
one file output format can be divided
into two types that is takes
output format which basically dumps out
the output as text second is the
sequence file output format and I'm sure
you can clearly tell me what will be the
output from sequence file output format
can you please tell me Oh Marcy is
binary files absolutely binary files are
stored as sequence files so guys are you
clear with the available output formats
that are there can you give me a quick
confirmation good so let's move on and
let's understand the packages and
classes in a word count MapReduce
example in our previous session we
understood how to execute a MapReduce
example that is present with the Hadoop
package itself however in this session
I'll tell you how to write a MapReduce
program of your own using Java as well
as I will tell you what are the packages
and classes that are required to do that
so the very first thing is to explore
the classes that are used for writing a
MapReduce program right let's move on
the very first thing is the list of the
packages that needs to be imported which
is there in the Hadoop jar file now
these are the packages or libraries that
needs to be imported from the Hadoop
package so that we get the required
classes for writing the MapReduce
program first we are importing the Java
are you package and the util packages
that are available in Java moving on we
import the packages that are available
within the Hadoop common store chart ok
so basically we are importing Hadoop FS
path we are importing Hadoop conf and
then Hadoop ru packages now all these
packages are required for example
input/output is required for taking the
input and giving out the output so there
are certain classes for that which needs
to be used in the MapReduce program
similarly Hadoop dot count defines the
configuration of your Hadoop job so FS
dot path enables you to access a path
that is present in Hadoop file system as
well as create certain directories in
the Hadoop file system or or read from
that right moving on we have another set
of packages that needs to be imported
which are present in Hadoop MapReduce
client core jar ok now both these jars
are available in your Hadoop package
itself ok so what we are doing is we are
importing text output format
okay which is a kind of output format we
are importing text input format that we
saw in the input format list right so
these are the list of packages that
needs to be imported for writing a
MapReduce program Himanshu says do we
need all these packages in our MapReduce
program absolutely yes these are the
list of packages which are immersed that
you will minimally require to write a
MapReduce program okay
is it clear amongst you okay
matt says okay what about you Himanshu
Himanshu says so there are more packages
absolutely yes mount you as per the need
you need to import those packages okay
you can certainly go on and explore the
jar file and using that you'll be able
to get to know what are the other
packages that are available now in this
class we are understanding we need to
create a map class now in order to
create the map class we need to extend
the superclass that is the mapper class
defined in our hadoop package now this
mapper class takes four arguments as
input and they are of certain types so
we need to define the types of the
arguments so the input key is long
writable they input values takes the
output keys takes and the output value
isn't writable so guys I hope you're
clear with this Omar says yes what about
the rest of you thank you very much you
Manchu and the others let's move on now
we come on to the second important class
that needs to be implemented in a
MapReduce program that is nothing but
the reducer class now I am defining a
reducer class of the name reduce which
needs to extend the super class that is
reducer just like the mapper class right
that we saw in a previous slide
now this reducer class also takes four
arguments as inputs that is the input
key the input value the output key and
the output value now we need to mention
the types of these values now as you can
see here the input key for reducer is
said to takes the input value isn't
writable the output key is text and the
output value is in try table now this is
how we are declaring the reducer class
so guys are you clear with this
matt has a question so does the output
from Mapple goes into reducer absolutely
yes Matt will understand this better
when I will explain you the entire word
count example okay Omar says the
input-output data dives here has been
set corresponding to the word count
example is it so yes more you are
absolutely right okay so guys I hope you
are clear with two important classes
that needs to be declared and how they
needs to be declared for writing a
MapReduce program in this case it is
word count so let's move on so it's time
to see some MapReduce example to start
with let me take you through the word
count example itself okay so it's time
for some practical guys so it's time for
some practicals I'll take you to the
Dirac a VM now so this is a clips guys
this is the ID for writing a Java
MapReduce program so the very first
thing is you need to create a Java
project
so you can go to new click on project
select the type of project that you want
to create and then click on next give
the name of the project and then click
on finish okay so I have already created
my projects this is the word count
project in which I have my word count or
Java file okay so the very first thing
that we saw in the presentation was we
were importing certain packages from two
jar files okay so as you can see I have
added two jar files that is Hadoop
MapReduce client code and Hadoop common
okay
these jar files can easily be found in
the Hadoop package itself okay and the
part for these files are right here that
is user Lib Hadoop 2 point 2.0 or
diversion that you have under that you
have share folder within that Hadoop and
then within that MapReduce you'll find
this jar file there similarly you will
find comments jar under the common
folder which is present in Hadoop and
then which is present in share folder ok
how can you add it to the build path the
build path is nothing but build path is
nothing but telling the MapReduce
program where to find these jar files ok
so what you need to do is go to the
build path configure build path ok
go to libraries click on add external
jars now locate your jar file I will go
to file system user Lib Hadoop now since
my Hadoop was there in user lab so I
went there within this you need to go to
share folder then you need to go to
Hadoop folder the very first jar that is
hadoop mapreduce client jar is present
on the MapReduce right here this one
okay the second jar is present under
common and then this is the jar file you
click OK and this gets added right here
in finally you click OK okay so this is
how you need to add both the jar files
in your class path once you are done
with that you can come here and you can
import all these packages right here
these are the same packages that we saw
in the presentation that is file output
format input format text input format
text output format we have got mapper we
have got reducer class which is nothing
but the super classes ok so let's see
the major classes that are present in
this word count or Java file if you
divide the entire program we have got
one main class that is called word count
which has got one class that is map and
the second class that is reduce map is
the class which extends the superclass
that is called mapper which takes
attributes that is long writable text
text and in writable ok so this is the
input key this is the input value this
is the output key and this is the output
value similarly you have another class
reduce which extends the reducer class
the superclass which takes 4 attributes
the input key is of text type the input
value is of interactable type the output
key will be of text type and the output
value would be off in try to Bill type
guys are you clear with this this is
what we saw in the presentation ok it's
great I'm getting lot of yeses good Oh
Marcy is why have we specified the input
key data type as long writable
it is because the input key would be of
long writeable type okay it would be a
hexadecimal number which is a long
writable number I'll be explaining it to
you in a while okay now if you see the
output key value types of the mapper
it's exactly the same as the input key
value types of the reducer okay can you
see that everyone Omar says yes Himanshu
says yes so guys this is the answer to
your question that is the output of the
mapper class is sent as a input to the
reducer class and that is why the types
of key value pairs of the mapper output
is exactly the same as the input key
value types of the reducer is it clear
guys do you want me to explain again
good now let's see what is written
within the map class okay so after you
extend the mapper class you need to
implement the map function this is the
map function which will have the actual
logic of your mapper okay the input
attributes are long writable key text
and value okay so we are giving the name
of the variables and this is the type of
the data that can be stored within this
variables so this is exactly the same
long writable which is nothing but the
input key of the mapper class the text
is the input value of the mapper class
correct now the last thing is the
context now context is another class
which is used to write the output of the
mapper class okay now let's understand
the logic which is present within the
map function in the very first line we
are converting the value that was passed
into the mapper to string and storing
within a variable that is called line
which is of type string okay so let's
take an example okay let's say the input
text was this is a dareka class three
okay so the key here would be the byte
offset of
every line so every line has got its own
byteoffset which is nothing but a
hexadecimal number okay and that is why
the input type of the key is long
writable okay so this answers another
question that was asked in the session
previously okay so this would be the key
the entire line would go into the value
okay which is this is a dareka class
three okay so which is nothing but of
next time so that is why what we are
doing is we are converting this to
string and storing within a variable
that is called line okay guys are we
clear till now good pose that will be
using a string tokenizer now this line
variable is B now this line variable
will be passed to an object of string
tokenizer class now string tokenizer is
used to extract the words on the basis
or spaces that are present between them
okay now what I am doing here is I am
creating this object tokenizer and I am
passing the variable line which contains
the entire line
okay the value now once it is passed we
need to run a loop and I traitor which
is a while loop here now this particular
statement is checking whether the
tokenizer has got any more words or not
till the time the tokenizer has got
tokens or words within it the while loop
will keep on running okay now once we go
within the while loop what we are doing
is we are using a variable value dot set
yes assigning a value to the variable
value and the value would be tokenizer
dot next token what we are doing is we
are picking up the next word that is
present in the tokenizer
and we are assigning it to the variable
that is called value okay
after that we are using the context
object that was defined right here and
we are calling the function right within
that we need to pass two attributes one
is the key and the second is value so
the key right here is the word okay that
we extracted from the tokenizer
and then we are assigning a value of one
against it if you recall the explanation
that I gave you previously we need to
assign one against every word to find
the word count right so what is going to
happen is the very first value will come
we'll beat this ok and what will be
written would be this comma one
similarly the second value would be is
and what will be written from the mapper
as output would be is comma one the
third would be a dareka and the value
that would be written out would be a
dareka comma one similarly will happen
for class so it would be class comma 1
and then 3 comma 1 once it is done
tokenizer dot has more tokens will
return a false value and it will exit
the while loop and then finally the
mapper would be finished guys are you
clear with the logic that is written
right here if you can give me a quick
confirmation and if you have any
questions you can ask me right now omar
says yes matt says yes what about the
rest of you Himanshu says yes great just
remember the output here this one is one
at Eureka one class one and three one
now we will move into the reducer
implementation just like the mapper we
have a reducer class which extends the
main class or the superclass that is
called reducer and these are the
attributes so we have input key s takes
and input value as in writable which is
nothing but the output that was given by
the mapper right the mapper output was
of type text' key and in writable value
now the key value output from the
reducer would be of type text and entire
table respectively right now as soon as
you extend the reducer class next thing
is you need to implement the reduce
function okay now the reduced function
will take three attributes as input
texts second would be values which is of
type I travel which has values of type
int writable okay and then finally the
context context as you as you know is
used to write the final output from the
reducer or the mapper okay a way to
understand what is I tribal here now a
tribal is nothing but the list of values
so if you recall in the reducer phase
the input is key and a list of values so
this is nothing but the list of values
that are against a particular key guys
are you clear with this so mod has a
question he says we are writing a Hadoop
MapReduce program using Java and Java
happens to be the native language of
with Hadoop so why isn't we are using
the same data types that are present in
Java with her to Omar this is because
the data types that are there in Hadoop
are specifically designed for a
distributed file system and then finally
which can be used as an input format to
a MapReduce program ok the generate data
types that are there in Java is
something which cannot be used right
away
ok so guys I hope you are clear with the
reduced function and the attributes that
are passed to the reduced function now
let's understand the logic that is there
the aggregation that is done in the
reduced function okay so first we are
defining a variable that is called int
sum equals to 0 this is nothing but to
calculate the frequency of the key or
the word okay now now we are running a
for each loop okay so we have values of
in writable type so we are defining a
variable int writable X we are passing
on the I triple that is values so we
will read every value that is present
within this I triple okay and will keep
on adding this using this particular
Java statement okay so the very first
value is read from values I triple and
get stored in X it comes here and it
gets added to some then again the for
loop is repeated the second value comes
into X it comes here and it gets added
to the previous value of sum and this
loop will keep on executing till it
reaches the end of the values I terrible
so once all the values are read from the
values I Tribble
it exits the for-loop finally what we
are doing is we are writing down the key
which is nothing but the word and the
sum of the values that was received from
the fault function as the output from
the reducer function right now let's try
and take up the same example that we
were dealing while understanding the Map
Reduce function so this was the output
so what we have here is so this is what
we should receive as an input okay so
this was there only once so it will be 1
itself similarly is comma 1 would be
there and similarly the rest of the will
be there ok so within the parentheses
actually explains that this is a list
okay this is what I am trying to signify
there now let's say there was another is
in the input file so it would be is
comma 1 right so what will happen here
is instead of is comma 1 in the list
there would be another one so this is
what is received as an input by the
reducer guys are you clear with this
Hamas says yes great
finally let's come down and try and pass
it from the for function so for the very
first value that is this one what will
happen is this comes here so the very
first key and list of values that will
come as an input to this reducer would
be this one the value that gets stored
into sum would be one with so list of
values right here contains only one okay
so the context output would be txt comma
1 similarly is and this list would be
passed to the reducer the value of sum
here would be anybody would like to
guess what will be the value of sum for
key is absolutely right everyone it
would be 2 because 1 plus 1 will become
2 here okay and the output that would be
written would be is comma 2 right
finally for others also it will happen
like a dareka comma
on then it would be class comma one and
then finally it would be three comma one
and the final output would be given on
the output screen this so this is how we
have written the reduced function as
well now let's understand the main
function okay this is what is driving or
executing your entire MapReduce program
so what we need to do is we need to
define an object of the class that is
called configuration so confident object
of the class configuration right here so
using this object we can define the
entire configuration of our word count
example or any MapReduce example okay
now next you define the job now right
here we are defining the job that needs
to get executed on the Hadoop cluster to
this job we need to pass on the
configuration of our MapReduce program
along with the name of the MapReduce
program this could be anything but under
double quotes which is nothing but the
string finally I am setting the jar by
class so this is the main class that we
defined on the top that is word counter
class right here this is the main class
that we defined so we need to set this
class that is word counter class right
here so set jar by class we are
mentioning the main class of our word
count example next we need to set the
mapper class that is mapped out class
right here this was the map so here we
are setting the map dot class dot class
is nothing but the Java extension of
class file similarly we are setting the
reduced or class right here for the job
so what we are doing is we are setting
the main class of the job right here we
are setting the mapper class of the job
and then we are setting the reduced
class of the job we are telling the
Hadoop framework to use these classes
while executing my MapReduce job right
we need to set the output key class so
the output key class was text right here
because the output that was received was
of the text format output value class
was of type in writable the input format
class was text input format okay and the
output key format was text out
format as a whole okay finally right
here I am setting the input path of the
input file that needs to be used by the
MapReduce program so if you can see path
argument 0 so what I am telling the
MapReduce program is I'm going to pass
the arguments from the command line and
the very first argument is the input
path
similarly goes with the output path
right here argument 1 so let me tell you
what is this argument 0 an argument 1 so
while executing the word count example
what I need to do is I will be executing
this command so that is Hadoop jar the
name of the jar file let's say word
count dot jar and then I will be
specifying the input file let's say it
is input and the output directory is
output so this right here is nothing but
the argument 0 which will be back slash
input okay and this back slash output
will be argument 1 guys are you clear
with this Omar says yes what about the
rest of you Himanshu says yes matt says
yes jessica says yes great Himanshu says
let's run the program sure Himanshu ok
so since you are clear with this now
we'll just export the jar file so what
do you need to do is right click on your
project click on export select the jar
click Next give the name of your jar
file so what I will do is I'll name it
as word count or jar ok
I'll click on next again next now this
is a very important step in which you
need to select the main class which has
the main function there ok so I click on
ok so what count was the main class and
then finally I click on finish ok now
I'll just show you the jar
find that was created right here the
word count or jar okay I'll open my
terminal okay so it's time we execute
the Hadoop MapReduce program however
before that first we need to create the
input right so what I will do is I have
a Python file which I showed you in the
last class using which I am scraping the
Apache Hadoop page okay
I'll simply execute this desktop scrape
dot py and enter it is going to scrape
the entire source code of the Apache
Hadoop page and it will create a file on
my desktop right here as you can see
Hadoop Apache okay I will make the
screen full now can anyone tell me what
is the first step before executing the
MapReduce program anyone I hope you
remember from your previous session put
input file on the HDFS absolutely right
Himanshu correct Kumar so I will move
the input file I will write Hadoop DFS -
put and then I'll give path to my input
file that is present on desktop and the
name of the file was Hadoop Pachi right
and I will put it on the root folder of
Hadoop okay it's time we execute our
word count program so Hadoop jar the jar
that I exported is present on my desktop
solid desktop word count or jar next I
need to mention the HDFS input file path
so it will be back slash Hadoop Apache
this is the file I copied and then I
need to mention the output directory
that will be created on Hadoop HDFS and
let's name it as my word count right
this is the first program that we have
written on our own and I will press
Enter
so job has been submitted and the mapper
has started to execute the mapper has
finished now and it will be the reducer
which will be aggregating the results
now right
Mars is yes and the reducer has also
finished and the job is also completed
successfully okay so let's go on to the
name not health UI and look for the
output so the name of the directory that
we created was my word count right here
as you can see in the bottom I will
click on this so we have the part file
and this is the word count output as you
can see this particular term came thrice
it is 3 so this is the entire output of
the source code that was present with
Apache Hadoop right strong Hadoop came
four times right so guys I hope you are
clear with writing a word count program
or a MapReduce structure overall good
now let's go back and see the next
example that will be executing today so
MapReduce has got wide range of
applications where MapReduce can be
utilized however if we try to classify
it we can broadly classify it under two
major applications one could be the
analytics and the second could be
testing okay so using MapReduce lot of
people perform various kinds of
analytics over the big data as well as
MapReduce can also be used for testing a
particular MapReduce program okay using
a my unit so let's move on and to start
with we'll see examples of MapReduce on
analytics right the very first example
is MapReduce temperature example okay
now let us understand what it is the
problem statement of this example is we
need to analyze the weather data of
Austin to determine the hot and cold
days that were there ok now what we are
doing is we are using the data set that
is present at NCI ok so which is nothing
but national centers for environmental
information ok so it has got huge
historical weather
that is present there and using this
data we can do lot of analysis on the
basis of which we can forecast the
weather as well weather the next day
would be hot or it would be cold okay so
we can come up with certain patterns and
we can analyze this data to start with
let us go to the website and see this
data set so this is the website where
you can find the entire set of weather
data set from 2000 till 2016 okay so
what we have done is we have picked one
of the data set that is present from
Austin okay let me just show you the
readme file which explains the details
of this data set okay which will be
required for you otherwise you will not
be able to understand any of the data so
I will quickly take you to the details
of the fields that are present within
the data set right here the first field
is v bar number the second is last date
the third is CR xvn fourth is the
longitude fifth is the latitude six
there is the daily maximum temperature
seventh is the daily minimum temperature
so there are many more fields right here
the fields that we are going to use in
our MapReduce example would be the daily
max and the daily min along with the
date okay so what we are trying to find
out is whether a particular day at
Austin was cold or whether it was hot
okay so now I will quickly give you a
glimpse of the data set as well so guys
this is how your data set will look like
right so now I hope you can understand
the importance of reading the readme
file
guys did you understand it good so right
here the fields that are important to us
as the date the second column and then
the sixth column which is nothing but
the max temperature and the seventh
column which is the minimum temperature
okay let me just take you back to the
presentation so that we can understand
it easily as you can see right here the
sixth column represents the maximum
temperature on that particular date and
the seventh column represents a minimum
temperature on that particular date
using this we are going to calculate
whether we can classify a particular day
as cold or hot in Austin so
interesting guys right it's good right
good thanks now let's move on and see
the MapReduce program that we've written
to find out the cold and hot days in
Austin right so this is the MapReduce
program that we have written to find out
the hot and cold days in Austin right
I'll open up the maximun Java file so
the very first thing is we have all the
packages that we need to import just
like we did in the word count example
the next is the main class which is my
max min we have the mapper class that is
max min temperature mapper we have the
reducer class that is maximum
temperature reducer and then finally the
main function which has all the job
properties right so I'll quickly take
you through the mapper logic so what we
are doing is the very first thing we
have defined as in missing double line
double line so this is a value which is
present within the data set which is
nothing but an inconsistent data so we
will be taking care of this in the
program itself the very first thing is
we are converting the value to string
and storing in a line then if the line
is not equals to 0 that is if there is a
value in the line we'll come here and
we'll pick up the substring 6 and 14 so
why are we picking 6 and 14 it is
because from character 6 till 14 is the
date of that particular day ok so I will
quickly show you the data set right now
so that we can understand it better so I
will go to data sets and this is the
data set that I am looking at so this is
the input file ok so if you see this is
the sixth character from the left right
so this is one two three four five and
six so after the sixth character starts
the date of that particular day and it
ends at 14 ok as you can see this is
seven eight nine ten eleven twelve
thirteen and fourteen right so this is
why what we have done is we are picking
up the string from 6:00 till 14 and
storing it within the date variable guys
are you clear with this
and you give me a quick confirmation on
the chat window Omar says yes Matt says
yes good Himanshu says understood great
next what we are doing is we are picking
up the max temperature from sub-string
39 to 45 right so max temperature can
always be a decimal value and that is
why we are converting it into float
similarly we are picking up the minimum
value from character 47 to 53 once we
have it so what we are doing is if the
max temperature is greater than 35 we
are classifying it as a hot day and if
the minimum temperature is less than 10
we are classifying that day as a cold
day okay so let's go back to the
terminal and execute this so before we
go and execute let me just quickly show
you the driver class I am sure that you
are pretty much clear with the driver
class already so what we are doing is we
are defining the configuration you are
defining a job and passing on the
configuration and name of the example
okay we're setting up the main class
we're setting up the output key class
output value class you are setting up
the mapper class then the reducer class
and so on okay setting up the input path
and the output path so it's time we
export the jar and execute the program
so I'll click on export jar next I'll
rename the file so this would be hot and
cold okay I'll click Next
next I need to select the main class so
it would be my max min and finish let me
show you the jar file so this is the hot
and cold jar now let's move the data set
that we have into the Hadoop distributed
file system so it's Hadoop TFS - put I
will give the part to my data sets it's
desktop data set folder and within that
I will find my weather data set and I
will move it to the root directory of
do and I will press ENTER okay it is
copied now I'll execute the weather
example right away or do jar and the
Java file is present on the desktop so
I'll write desktop hot and cold dot jar
right next I need to mention the name of
my input file that is weather underscore
dataset which is present on HDFS and
finally I will mention the name of the
output file so let's name it as my
weather okay and I will press Enter
the mapper has finished even the reducer
has finished now and the job is also
executed let's quickly go and check out
the output let's browse the file system
which is nothing but the HDFS file
system and look for my weather right
here I'll click on that let's see the
output so as you can see each day is
classified as cold day or hot day and
against that you have the temperature
value written so one point one it was a
cold day again this was a cold day so
you'll see most of the values are of
cold days except the last value that is
there which is a hot day with 69.6 right
it is because it this data set belongs
to Austin ok so which is supposed to be
a cold place correct so guys are you
clear with this can we go back to the
presentation matches right Hamas says
sure thank you now we come to a next
example that is last dot FM example okay
so last door FM is nothing but a music
company which provides songs to the
subscribers ok to the users at last dot
FM you can go and listen to several
songs ok now in this what last door FM
does is it has data for every user or
every song that is played ok this is how
the data is stored we have user ID we
have track ID track ID is nothing but
the song ID we have shared shared means
that whether this song was shared or not
so if one is there it was shared 0 is
there it means it was not shared radio
it was listened on radio or not so if
the value is 1 it means it was if the
value is 0 it wasn't
similarly we come on to skip if the
value is 0 the song was not skipped
however if the value is 1 the song was
skipped ok so this is how the data is
stored at last or FM so just imagine
every song that is played by n number of
users n number of times or it is share
or skipped every detail gets recorded at
last dot F M okay so we are going to
analyze that so there could be n number
of analysis that could be done over this
data however in this particular session
we will try to find out the unique users
for every track ID so essentially what
we are trying to find out there is the
popularity of a particular song so if
you have multiple unique users for a
particular song and the number is high
that means the song has picked up very
well right guys did you understand the
problem statement can you write down on
the chat window
Aman Shu says yes Matt says it's good
now let's go to the VM to understand the
MapReduce program to find out the unique
users for a particular track idea for a
particular song right so this is the
MapReduce program that has been written
to find out the unique users for every
song that is there at last order film ok
so right here this is the data set I
will quickly open it so this is a so
this is our last door FM stores it data
right we have the user ID we have the
track ID whether this track was shared
or not whether this track was heard at
radio or not and finally whether this
track was skipped or not ok so let's
quickly go back to the code again we
have imported all the packages right
here as you can see ok we have a main
class that is called unique listeners ok
this is what we are essentially trying
to find out so we have defined a class
that is called last.fm constant it
contains nothing but a list of constants
using which we will try to traverse
through every record but before moving
on let me quickly show you the
significance of these constants that we
are declaring for that I will go to the
data set and I will copy one record ok
so if you see the pipe is the delimiter
right here right every value separated
by the pipe the first value is the user
ID or you can say the 0th value correct
that is
user ID has a value zero the second ID
is the track ID or index one right so
track ID has a value one similarly the
Shared has got the index two next is the
radio has got index three and scaped has
index four so this is nothing but
defining the index and what kind of
value you are going to find on the index
right so at times index gets confusing
and hence what we are doing is we are
defining constant so we'll find user ID
at index zero will find track ID at
index one will find shared at index two
and so on
guys are you clear with this it's a very
simple concept okay great thank you
we'll move on so this is our mapper
class as you can see right here in which
I am defining a variable track ID user
ID because this is the final output that
I would like to give from my mapper
function correct so what we are doing is
we are splitting the string or the input
value on the basis of pipe right because
it was the delimiter and then again we
are picking up the track ID and the user
ID and finally dumping it out from the
mapper function so very simple right
guys are you clear with this what we are
doing is we are dividing the input on
the basis of pipe and we are picking up
the track ID and the user ID so track ID
is nothing but the key and the user ID
is the value so guys if you can give me
a confirmation great good now we move on
to the unique listeners reducer okay so
this is the reducer logic right here in
this what we are doing is we are
creating a hash set so let me tell you a
unique property of hash set hash set
cannot contain a duplicate value okay so
if you push any number of values even
though you are pushing a duplicate value
into hash set it will only keep one copy
of it okay so I hope now you can relate
why we are using hash set it is because
we want to find out the unique users for
a particular track ID or song right so
what we are doing is we are travel
singing through every user ID that is
there for a particular track ID and then
we are adding it to the hash set that is
user ID said even though this user ID I
tribal may have duplicate user IDs
however since hash set cannot contain
duplicates it is going to store only one
copy of every user okay and then finally
what we are doing is we are finding out
the size of this hash set which will
give me nothing but the actual count of
the number of users for a particular
track ID and finally I am giving it as
the output from the reducer is that
clear guys
Himanshu say is absolutely right it is
to find out the unique listeners
absolutely correct Iman shoe Omar says
yes Matt says yes great so I will not be
discussing the driver part it is exactly
the same I will just export the jar file
from here I'll click on export next I
will name the file as lost fm
next next I will select the main class
unique listeners and I will click on
finish and okay Himanshu says from where
we can find this code amongst you this
code will be there in your elements you
can always download this code and
execute it okay now let us go and check
out the jar file so this is the jar file
that is created that is lost
FM dot jar the very first task is to
move the data set to the Hadoop cluster
right so I'll write Hadoop TFS - put I
will give the path to the input file
that is desktop data set and then last
FM underscore sample finally I want to
move it to the root directory of Hadoop
and enter the file is copied now now
it's time you execute the last.fm
MapReduce program so it's Hadoop jar
next up last.fm jar I need to mention
the name of the input file
which is last.fm underscore sample next
I need to mention the output directory
where it will store the final result so
let's name it as my last FM right it
sounds very weird right let's enter
the mapper has executed so we have the
final output written on HDFS let's go
and check let's click on browse the file
system and the name of the output folder
was my last FM I'll click on that you'll
click on part file and this is the
result track two not one was played by
14 unique users track 2 not 2 was played
by 16 unique users and so on there are
many other analysis that you can do over
this data set ok that is something I'll
leave up to you to imagine and finally
write a MapReduce program on that in
case you need help guys you can always
reach out to me or the support team ok
so guys are you clear with this can we
move on great thanks so now we look into
the MapReduce example for testing ok in
this we'll be using the mem our unit a
my unit is nothing but a way or a
framework using which you can get the
advantages of J unit on a MapReduce
program so what happens is you are
basically executing unit tests over a
MapReduce program ok in this what you do
is you pass a set of inputs and you
define the final output that should be
received after processing the input if
the output of the MapReduce program
doesn't match is the absolute output
that we mentioned earlier in that case
what happens is the logic of MapReduce
program is not correct however if the
output of the MapReduce program matches
exactly with the output that was
mentioned in the starting then your map
dis logic is absolutely right ok so this
is how the mi unit works on a higher
level are you guys clear with this great
thanks for the confirmation guys now for
implementing the mi unit in MapReduce
there are certain drivers that can be
utilized one is the map driver for
checking out the map function the
reduced driver for the reduced function
Map Reduce driver and then pipeline Map
Reduce driver ok
now let's quickly go back to eclipse and
check out the Map Reduce mi unit program
okay so this is the mi unit test example
that we have prepared for you now there
are a set of libraries that are required
for running a MapReduce program okay
most of the libraries can be found
within the Hadoop package itself except
the mi unit jar okay this is a jar that
you need to explicitly download from
Internet okay
so it can easily be found on google you
just have to download it and add to the
class path of your ma unit test program
okay the rest of the jar files can be
found within the Hadoop package itself
now let's go and see the MapReduce Java
code for mi unit so again we have a list
of packages in this program as well
there are two new packages that has been
added they are my unit produced and the
J unit test package correct now we have
a test example main class which extends
the superclass that is called test case
right so we are defining a map class
which extends the mapper class so this
is the mapper logic that we'll be
testing in this mi unit program okay it
is nothing but a word count logic itself
correct so we are defining an object of
the map driver class so as you saw we
have got four different kinds of drivers
that can be used with mi unit map driver
is the driver using which you can test a
mapper job okay or a map class right so
next is we need to define the set of
function within which we are creating an
object of map test which is nothing but
the mapper class that we defined right
we are also passing the mapper object to
the map driver class correct so what we
are doing is map driver dot new map
driver and we are passing the object of
my mapper class that is not test finally
we have defined a test case with which
goes with an annotation that is at the
rate test so the test case is test
mapper within which we are defining the
input and the output values okay as I
said we need to specify a certain input
as well as we need to specify a certain
output so that the output of the mapper
can be matched with the
put that as mentioned with the map
driver right so right here we are
setting up the input the input value is
1 comma Sunday comma eh comma holiday
since the logic the mapper logic is
nothing but of the word count so what we
are trying to find out as you find
trying to find out the occurrences of a
particular word in the input file or the
input line right so we are trying to set
up the output and we'll check whether
the occurrences of this particular word
that is sunday is 2 or not ok let's try
and execute guys can you predict the
answer whether this test case will pass
or fail before we actually execute it
Himanshu see is wrong he says it will
fail what about you Matt do you think is
right Kumar do you think is right you
can say anything Matt says he's right
okay let me just execute this for you I
will right click I will click on run as
and click on J unit test okay so it's
executing now okay as you can see it has
failed okay and just read the message
right here
missing expected output sunday comma 2
at position 0 received unexpected output
that is sunday comma 1 okay so what
happened as the output from the mapper
function was sunday common one however
the absolute output that you mentioned
in the program was Sunday comma - right
right here I will quickly show you so
you are expecting you are telling the
program that the expected output is
sunday comma 2
however the mapped is logic scent Paks
and a comma 1 so the problem is not with
the logic it actually gave you the right
answer that is sunday only came once
however the result that you were
checking was not correct right shall
make it 1 so sunday should only come
once right will execute the program once
again we click on Ranas j-unit so it's
executing now
so failure zero and we have a green
signal right that means the unit test
got executed very smoothly it was
absolutely right the program gave the
right answer okay you can add as many
test cases as you want this is one test
case for you to understand so I hope you
guys enjoyed this example can you please
write down on the chat window Manchester
is very nice example he enjoyed it great
Matt says it was good thank you Matt I
hope everyone else was fine Matt says
thanks for showing some real-time
samples Winnie it was wonderful
experience great there are much more
examples that are there in your LMS as
well as Matt what I suggested you should
download open source data sets like you
saw the weather data set use it and come
up with your own problem statement and
finally implement the MapReduce example
I'll be there to help you out if in case
you are stuck anywhere ok so let's get
back to the presentation now so this is
the end of today's session guys before
we end the session guys I just want to
reiterate you have a set of blocks that
are there for you to learn more things
and enhance your learning so we have a
set of Hadoop tutorial series that you
can go through as well as there is a
MapReduce tutorial written specifically
on MapReduce so guys please go through
that and let me know in case you need
any help ok so if you have any queries
now guys you have the time please ask me
I'll try and answer them any other query
if you have Hamas says all the questions
are answered great even though guys if
you have any more queries you can
certainly ask me or take the help of a
dirac our support team okay also guys
there are set of assignments on
MapReduce around five to six assignments
on MapReduce this is a very important
topic please go through that attempt the
assignments and we'll discuss those
assignments in the next session thank
you everyone and have a nice day I hope
you enjoyed listening to this video
please be kind enough to like it and you
can comment any of your doubts and
queries and we will reply to them at the
earliest do look out for more videos in
our playlist and subscribe to our ed
rika-chan
to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>