<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talend ETL Tutorial | Talend Tutorial For Beginners | Talend Online Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Talend ETL Tutorial | Talend Tutorial For Beginners | Talend Online Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talend ETL Tutorial | Talend Tutorial For Beginners | Talend Online Training | Edureka</b></h2><h5 class="post__date">2018-02-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_8Ws03Yk6DY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is Jessica on behalf
of Erica
this webinar is all about to tell you
what is ideal give you a brief
understanding of ETL and why it is
needed and then we'll take you all for a
walk through for talent ETL how talent
is helping you for doing ETL stuffs so
that is the intention of this webinar so
the agenda for this webinar is let's
understand why ETL is needed and what is
ETL what does it stand for what is the
reason it came into picture and what are
the ETL tools in market today who is
doing good and then where is talent
standing according to its capabilities
in the market and then we'll see a short
demo on how talent can help you in doing
it will task right so let's first
understand why it you are listening it
so as we know today in real world we
face many data problems so data is
scattered across the locations so
business will be in different
geographical locations and each of them
will handle in different formats and
different business rules will be there
and data is stored in different types of
sources may be one customer is giving
you an flat file and another customer is
giving you an Excel and one more is
giving you in an XML file and one is
just putting the data in UI and you will
have to extract it something like that
and then volume of data keeps on
increasing so today you have some data
and tomorrow you get along with the old
data the fresh set of data or something
corrected in the old data so in either
of the cases volume of the data is
increasing data can be structured
semi-structured or unstructured and if
you are following some structured way of
putting the data in then you will be
getting them in structure format
sometimes it will be semi structured or
it could be an unstructured as well
right so for all these reasons where
data management is a problem we need to
have someone who manages all these
problems you know enables us to manage
it in efficient way and manage the data
that is why we need ETL so ETL is
one-stop solution so ETL provides and
one-stop solution for all the problems
which I am going to see now so extract
so as I mentioned in the previous slide
the head
true genius sources are available as I
said one in flat file one in Excel one
he can tell to connect to my DB and
extract the data so they are all
different kinds of so system where I
will have to connect and get the data or
read the data right so extraction is the
first problem wherein we have different
structures and different formats and
then transform so after I get all these
information and I read it by not
providing a lot of effort on the source
system I should not be affecting that I
should be extracting with minimal effect
to this whole system but transformation
after I read it according to the
business rule or if I have to know get
one new column
maybe timestamp every time when I load I
need a new column which is timestamp
something like that and I maintain an
active or inactive flag and I apply some
business rule if you know I'm getting
some employee department data then this
department has to get some bonus or
something like that so all these things
comes under transformation anything for
that matter I am just giving you a
general example so all those business
rules on whatever you are going to
change the data from the source whatever
you are reading all those things comes
under transformation right so we'll have
to take the raw data or the source data
will have to transform them according to
the business rules and if we want to get
some you know new columns over there and
derive some columns based on the source
columns all of them comes under
transformation right and then once you
have finished all your business rules
and you have separated the valid records
invalid records so you take the decision
to load that data into a target system
so if everything is fine you directly go
on load them into your target system and
business decisions can be taken on the
target data right if you find some
mistakes maybe you will capture them
with the right reasons and check with
your data owner what could be the
mistakes over there and you correct them
and ultimately by doing this in
iterative manner we'll be able to manage
the data and get all of them corrected
and we have an efficient set of data
which is reliable to take business
decisions on that right so for these all
three steps is ETL is one-stop solution
for you so what is ETL so we understood
what is in
I mean reasons why we need ETL and also
the steps involved in it you know
extract transform load that is where it
will stand for and now we see what is
that actually what is a process so
extraction is a process of extracting
the data from the various homogeneous or
heterogeneous data sources based on
different validation points so as you
can see in the slide so we have txt file
we have XML file and we have XLS file
and we have a data source as we have we
replace whatever it can be so all of
these are it can be either homogeneous
or heterogeneous data sources which I
will have to connect to them or read
them by you know not affecting the
source system which not be you know
hitting them again and again because
somebody would be dependent on these
data sources for their business purpose
so we should take that data and have a
temporary storage right so this step is
called extracting steps in ETL so that's
more so second step is transformation
what is this transformation the entire
data is analyzed and various functions
are applied on it in order to load the
data in the target database in a clean
and general format okay so we have a
rules like our few columns will be
mandatory and few columns should
specifically have only these values and
we'll have to do some lookups for
getting the other value so for all those
things transformation is a solution and
you can clean them we can filter out
them if we are deriving we can enrich
the data values we are able to split the
data you can split the data and if you
want to join the data you can join it so
all of these together is transformation
on your business rules and ultimately we
have a cleaned and formatted data for
your further process right so this is
transformation and the last step is load
loading is the process of loading the
data which you already process in a
target repository using minimal
resources that is what even when we read
the data we should have minimal effort
and even when we write will have to use
the minimal resource when loaded right
so transformed data can be again in any
format you could have written to a file
you could have written to a DB whatever
it is and then ultimately for
reporting purpose usually it goes to a
data warehouse your target can be
anything like depending on your business
rules okay so once you have the clean
and formatted data you can directly load
that data into your target so what do
you mean by Mineral Resources you can do
a bulk anode to it so it takes
efficiently it loads if you are doing
one by one row by row it is not that
efficient but if data is really less
then doing bulk is not good so we will
have to go for oh by room something like
that so depending on your business rules
you will have to decide how you can load
the data to the target using minimal
resources so combining lis so extracting
with not giving the effect to the Sol
system transforming based on the
business tools cleaning you know
filtering splitting joining and whatever
you want to do derivation enrich and
then putting that data into your target
is the ETL complete right so what are
the ETL tools in today's market which
are doing good in their own way right
each of its tool will have their own
features wherein we will have to decide
which one to use for our business
requirement right so let us have a look
at them so ETL tools are the tools which
combine all the three processes that is
extract transform and load into a single
program into so I'll have a ETL tool so
which will enable me to read an excel
file read a text file connect to a MySQL
database connect to a service system
something like that so I can just
configure them read the data and
transform it and load it back to my
target right but what are the features
of it so when do you call a tool as
completely ta little it should have few
features which we are telling you right
now so easy to use so the learning curve
for that tool should be very low you
should be able to understand it and use
it I know it's very easy to use it is
very I know psychologically designed
where you can understand okay if I do
this option now it is very clear for you
so that is the ease of use it is
graphical user interface right
so usually ETL tools will be a gif
waster so user friendly to a that is
what I was telling you so gee I will
give you different windows where you can
choose some components you can configure
it in another window you can run it in
another window you can see the locks in
another window and it is like very
interactive to you and very user
friendly right and then inbuilt error
handling so few things which are
standard in ETL or I know Java based
tool means it will have its own standard
error handling messages like then
another retail tool it will have its own
components if you are including those
components they will handle their as
they will log it as well for the
repository default tables something like
that
so inbuilt error handling should also be
there in that tool and then economical
tool so if it's doing right now we have
a lot of competition in the market so
giving a lot of options for retail and
having an economical price is what makes
that product a successful product so it
reduces expenses that is one way of
looking at it in the business way but
indirectly in as an ETL tool if it is
reducing expenses means if you are
manually doing some work if you are
converting and if you are filtering and
doing some when or writing some a stored
procedure to do some action so
ultimately you are spending on more
resources or hardware or software
ultimately ETL way is doing everything
internally so it reduces your expenses
tasks connectivity to everything and you
can do it with only one tube and thereby
it is reducing your expenses as well or
also resource management and then better
data management so previously it would
have been very difficult to log errors
or they see the data which has passed
and take out report out of it
so ETL tool is like configurable you can
design jobs in such a way that every
time when there is an incremental load
you check the new data or if you want to
can check the full data you report for
the both ways something else so you have
a better data management in an ETL too
and then of course increasing
performance ETL tool whenever you using
this UI and dragging and dropping the
components ultimately it is generating
an optimized code in the backend and
also it has options like if you have a
great in a very large lookup then you
can have it in cash a and some you know
increase in the Java memory something
like that few options which can increase
the performance where a manual coding or
the manual way of handling data
management things would have been a
costly code but
it generates in very optimal core right
so you'll understand is how in
background detail tool generates an
executable code or a Java code so it
depends on which tool we are using so
summarizing it has to be very easy
user-friendly error handling should be
available it should be a low cost and
you should you know enable me to reduce
resources or reduce the time we are
investing on data management and it
should give me a better data management
solution a better view of looking into
the data and of course it should
increase the performance so if we have
all these options in a table it is a
very good retail tool to go ahead and
give solutions for my business problems
right so ETL tools you know various ETL
tools which are available right now and
which are very famous few of them we
have put over here so informatica power
center which is kind of a leader in ETL
so it comes with a license cost and then
we have SAS data integration and we have
the ODS Business Objects for s AP and
SSI as SQL Server integration services
ODI Oracle data integrator IBM
infosphere information services and
ultimately
Talend open studio for data integration
so talent open studio will also be
available in your open source and also
there is an enterprise version of it so
now let's understand talent as an ETL
tool so what does talent offers you for
ETN solution so talent is an open source
software integration platform or a
vendor like when we say platform or a
vendor talent itself is a company and
which has various products and one such
product is talent open studio for ETL
right so it offers various data
integration and data management
solutions so talent open studio for data
integration so that is the OS for di
data integration is widely used as an
ETL tool so right now it is a very
booming tool which has very you know all
these options which we discussed are
available in talent so TOS acts as a
code generator which converts all the
underlying programs in Java at the
backend so we will have a GUI where you
can drag-and-drop the components and
will have a filter component say for
example you have a join component so you
just drag and drop them and connect them
and configure them ultimately when you
do this job at the back end it is the
thing button Java so when you run the
job the backend Java code will be
combined and executed and it is platform
independent you can develop in Windows
running Linux oh it gives you that
flexibility so devious accessing code
generator which converts all the code
into Java at the backend so you don't
need to worry about it it does
automatically and of course that back in
Java code which is unwritten is non
editable
we'll have to come back to your designer
and that is like GI and you'll have to
correct if there are any mistakes but
the ultimate code generator is not
editable TOS can be easily combined
convert and update data present in the
various sources it is very user friendly
now psychologically designed so if you
want to do a filter just search for
filter and get all the related
components and you can drag and drop
them configure and use it for as per
your business requirement so all these
features talent provide C so what are
the benefits of using talent yes these
features are also available in all the
other tools
what is that talent is giving me extra
or how it is very easy so talent has an
ETL tool know using this TOS you can
easily manage all the steps involved in
ETL that is extracting it has dedicated
components for files dedicated
components for different databases MySQL
Oracle whatever you name so they are
using this customized options say for
example in Oracle we have indexing
concept so in the component itself in
the Advanced Settings will have all the
options which can make that process of
reading very optimized reading or
writing as I mentioned we can do a bulk
upload we can do one by one we can tell
whether do insert first update for us so
it has all the components design and a
very customized and optimized way so it
provides you all the options for ETL
extract transform and load it is
developed on the top of eclipse
graphical development environment so
Java if you or aware of Joey will be
knowing about this tool so eclipse tool
is the tool which they use for
developing java code so already
available aps will be there so you will
just you include those packages and you
start coding for your business
requirement and talent ETL is developed
upon this framework so it is like just
giving your GA where you can drag and
drop
the components but then ultimately it is
a Java code at then so it is built upon
the Eclipse in graphical environment you
can easily map the data between source
and destination system with a simple
drag-and-drop so we'll have this
automatic options if the names are the
same in the source and target you can
just automatic and just drag and drop
you can do your business rules as I said
you can introduce a new column get some
default values or the dynamic values
over there can do anything in a very
simple way so talent openstudio for da
provides an improvised data integration
so very user friendly you know a lot of
options involve customized option
involved for that particular component
so we don't have just one component for
all the databases we have different
components for different databases
different file formats so that the
options for optimizing options for that
particular DB that particular format of
file is available for you so it always
strongly connectivity easy adapter
Liberty and smooth flow of extraction
and transformation process so once you
you know see the demo we will understand
for a simple business requirement it is
very easy to you know drag and drop
configure them and convert your
functional requirements into technical
ETL steps and have a solution for it and
that solution will be very optimized and
very professional like you can divide
the data you know can divide the error
reports you can send an email of that
error report to the data owner so it has
a complete package filled so talent open
studio for data integration is what we
are going to see now so this is how your
talent TOS for data integration looks
like the recent version is six point
four point one so talent openstudio for
data integration is extensively used for
integration between operational systems
ETL process and data migration so they
are famous for all these things so it
can do any ETL process if you have you
know where as an experienced
professional I have done migration from
mainframe to NCP
so mainframes are so no legacy data
types and SME has the modern data
arrives so converting them you know a
few of the binary things are there and
you have a different languages to handle
which take multi bytes so since it is
integrated with Java it is very easy to
do
something which is not possible in ETL
maybe if you take an example of another
ETL tool few options are not available
and you cannot provide a solution for it
but in case your talent if you're not
having it in a standard way in talent we
can write a Java code or a scale script
anything for that matter and you can
integrate them in the ETL job itself so
that you ultimately provide a complete
solution in detail it's right so teo is
for data integration so palette is the
place where I get all my component and
they are very neatly organized and the
names of the components are
psychologically designed so you can say
if you want to be a filter you can just
type filter if you want to do a bulk
load you can just type well float all
the related components for that category
will be popped up over there okay so in
palette of the TOS you can find various
component which will help you in
designing the ETL process right so TiVo
is provides more than 900 components and
built-in connectors so 900 components
for all these you know different
categories which are being shown up here
so for example data quality will have
some you know matching algorithms all
the stuff components fuzzy logic
components and then file read all the
files related will have different
components for the file which I'll be
showing in the demo and we have a logs
and error caching components so each of
this there are more than 900 components
it's not like we'll have to know all the
900 components it depends on our
business requirements so I'll be using
big data and I am using some I know or
transformation components with that
knowledge I can develop the job right so
it has for anything and everything your
name so using Thailand openstudio can
easily bridge between the file systems
web services package interior
applications and then data warehouses
well app software as surveys cloud-based
anything for that matter they have a
connector for you so let's now have
brief demo of talent as a data
integration tool and tell you how we can
download the tool how we can launch the
tool and how we can develop a simple ETL
job take any use case so we'll discuss
about the use keys I will first tell you
how talent can be downloaded and
launched and then I will tell you about
the use case and we provide a solution
for that use case right
so as I informed you will have to
download the talent or first so how do
we download the talent tool is to just
to log on to talent corn and this will
be your welcome page so in the Welcome
page you will be having the download
section you can click on downloads and
when the download page appears we can
choose the product which we want so for
us now we are dealing with talent data
integration that is TOS di talent open
studio for data integration you can just
click on download free tool and then you
can click download over here which
downloads you a zip file which has the
complete binaries for your talent open
studio data integration right so once
you have downloaded it you'll have a zip
file downloaded like this for the demo
purpose so I'm showing you and I have
downloaded TOS that is talent open
studio for big data that means talent
data integration plus big data is Talent
open studio for big data ok so it is a
superset so it also has all the
components which is there in Thailand
Open Studio for data integration so we
can either download talent open studio
for data integration or you can download
the talent open studio for Big Data it
will have all the components which are
available in talent integration as well
so when you download the talent open
studio for big data you are actually
downloading the super set of data
integration software right so once you
download it you'll have a zip file like
this which is TOS BD 6.4 version once
you unzip this you'll have a folder like
this so which will have all the binaries
like for Windows or Linux or Solaris all
the executables will be available this
the only step we'll have to do and the
prerequisite is Java should be installed
and Java home should be set that's the
only prerequisite and you just have to
double-click on the required a such file
or the exe file based on your OS so that
is the only step required to launch your
talent isn't it very easy yes it is so
once I double-click on the Linux version
or SH file for my 6th
but binaries then it will open up the
talent open studio for big data for me
so once you have launched the talent
studio you will have to create a project
where you are going to develop your ETL
jobs right for that we have an option
over here to create a new project so we
are having an ETL demo from a Tirico so
I will name it as ETL the move Eddy
Rekha and I see just create so it will
create a project for me in my talents to
do so once it has been created I just
select this project and I tell finish so
it will open up the tool for me with all
the prerequisite for that project and I
can start developing the ETL job over
there
so this is the welcome page for you for
Thailand Open Studio I'll just close
this welcome page will open up all of
the windows for me so this is your
talent open studio for Big Data which is
the superset and it has all the
companies which is there in Thailand
open studio for data integration right
so this section is called repository
wherein you can create business models
it's just like another paint editor kind
of thing where you'll be having
different shapes and connectors so you
can create an ER diagram or a workflow
of the ETL job to just to have an
high-level idea about what your ETL job
is doing ok and in job designs you are
actually going to be designing your ETL
jobs context means if we need any
variables required for our ETL job where
it has to hold some value for you it can
be either used in only one job or it can
be used in multiple jobs we are going to
create it in context so that is context
in Tallin language and if we have to
embed some code in the PPT I was telling
you if something is beyond ETL
capability and we'll have to embed some
of the java code or SQL routines you'll
have to do it in the code section and we
can call that code in the ETL job and
similarly for the same reason we have
the SQL templates as well and whenever
we are dealing with structures
connections we can create the metadata
where we create ones and use many times
so all those section comes under my data
and you can create documentation of the
job or we can even attach the document
which are required for the job in
documentation section so all these
features are available under repository
window for you and talent and this
section is called your workspace where
you actually create your job design your
job designer and code window whenever I
create a new job all these sections will
be enabled so let me quickly create one
job and let these windows be activated
so I just come to job designs I
right-click on it and I see create job
and I just give a meaningful name saying
job and is the first job whichever I am
doing and I will give you an ETL
handsome
so purpose of this job is to provide
insight off tl job and direct so same
can be the description so how given all
the mandated thing so author will be
automatically populated this will be
provided in the initial section where I
have downloaded the talam and we can
maintain versions over here major
version I can click on capitolium
smaller version I can click on small M
you can observe the changes over here
and if you want to maintain the status
of the job whether it is in development
testing or production you can just keep
that status and if you want to provide
any documentation path over here you can
provide it or wherever you want to store
these things you can select the path of
it okay only these things are mandatory
and with this I just say finish it will
create an ETL job under job designs and
it will enable all the windows which are
required for developing the jobs so as I
said they have been activated so this is
a designer section where I can drag and
drop the components and connect them as
per my business requirement and as I
told you where I was explaining in the
PPT that whatever I drag and drop it is
ultimately a Java code at the background
so that Java code can be visible when I
click on this code section right so here
is what I create my ETA job so this is
designer window and code window and this
window is palette window where all my
categories of components have been
listed I can go into each of these
categories use the components from those
categories and whatever I do with the
job so whatever I gave the name of the
job or the recreation date modified date
all the information about the job will
be available in the job window here and
context whatever context I create in the
variables which I create for this job
will be available over here whenever I
drag and drop a component that component
properties will be available here we
will see it in the demo and after I have
finished the development of the job I
can run the job I can kill the job or I
clear the locks over here all these
actions can be done in run window so
having the knowledge of two widely used
sections in the talent windows so we'll
start with a use case to develop one ETL
job so let us put on a simple use case
I'll show the input files and what is
the expectation out of the input files
is what we'll discuss now say for
example we have a sample input file
which is used for joining okay so let us
see what is the data inside this sample
input file so I have a data of a product
it's a product data so I have a product
ID which is a numeric and I have a
product name what is the name of the
product and how much is the sales rating
for that product so how well it is going
in market so they have a rating of it
and I have this data where they say what
is the Product ID name and what is the
sales rating of it okay having this
input data
I haven't look up data as well where in
key becomes Product ID I'll show you the
look of data so look up data has the
Product ID and what is the total
investment or the expenditure which has
gone for that product so that detail is
available in another file right so what
is the expectation is you'll have to the
ultimate business goal is they have to
know which is the highest sales rating
product and which is the lowest sales
rating product so that they can improve
the sales of it or whenever they are
getting to know which is the highest
sales rating product they can have more
demand or the production for it
right so they understand that there is
more demand and more sales rating is
there they can increase the production
and for the less demand or the less
sales rating they can reduce the intake
or the production and they can
concentrate on selling the existing
stock so that is the high level a
business requirement but then when it
comes to ETL we'll have little bit of
problems in data so we'll have to check
the business rules and maybe they are
only looking at sales rating as a number
they will not be able to understand will
have to tell whether that rating is good
or bad based on some baseline that
baseline can be changing any time so
that is a requirement so my a single
line requirement would be to take the
lookup give me a data set wherein
product ID product name sales rating and
also expenditure look the value is also
available so I'll have to create one
master data like that and after I create
that master data I'll have to draw
conclusions out of it so which is the
highest
sailes rating and which is a lower sale
rating that is one part and also have to
create a new column in my master data
saying what is the index of it whether
it is good index or whether it is bad
index so I can take a threshold like if
you can observe there is starting from
6:00 till 10:00 I have the sales rating
maybe 7 above 7 is good 7 and below 7 is
bad so that is what a baseline we can
think of and we'll have to tell the
management which of these products are
good and which of the products are bad
and also tell which is the highest and
which is lowest so having that in mind
let's start developing it and let's get
to know what other problems say for
example first problem we encounter here
is we have around 6 data set over here
where in 1 or 6 Product ID is not
available in my lookup so I will not be
able to get the total expenditure for
that product so in this situation what
should ETL do it should actually take
out that record and say to the data
owner that lookup for this or the total
expenditure for this is not available so
I have to make a meaningful learn it
asset and say what is the rejection
reason for it so even that I have to
take care and also few ETL rules like a
product name cannot be you know dari
underscore products so we can replace -
underscore with space or you want to
print all the product names in capital
letters something like that can be done
transform data can be done and then when
you are filtering out so they need all
the good products in one output and all
the bad products in another output right
so in that way we can filter it out and
give it to them so which are the good
ones going in sales and which are the
bad ones going in sales so having this
understanding in mind so let's go back
and handle all these negative scenarios
and check if ETL makes my life is here
ok so the first step what we have to do
is to read this file right so let's get
back to Talon so reading file will have
a component so what I am doing as I said
the component names are very
psychological EDI no design and we can
easily find it so I am actually all the
components in Thailand are starting with
steel so what I am going to
is a file so T file and what kind of
file is it it is input file so T file
input and which kind of file am reading
it is a delimited file as you know it is
a comma separated value so whatever we
saw till now it's a dot CSV so it is a
delimited file so this is how it is
designed okay P file input D limited so
I get a component over here and if you
can see it the help or the tooltip is T
file input delimited reads a file
row-by-row with simple separated fields
so that is what I want to read it right
now so I can choose this component how
do I get this component over here is
I'll just drag and drop it so this is my
T file input delimited and if you
observe the component window know I'll
have all the attributes needed for this
component I should be able to configure
them and read it okay so this is how I
take a component to read a file so what
it is asking our property type will be
built in right now I'll introduce it to
a repository in few minutes
so after this is which file I have to
read so I can just browse and take my
file over here so it was and I have kept
them and demo input so sample input file
I have to read so I will just say I have
to read this file and the raw separator
is slash and the field separator since
it is comma separated values it will be
comma and I had a header over there
so my header will be one and I did not
have any footer and if you want to limit
the number of Records you can give the
number of lap words do you want to limit
from the first record and if you come
here it always ask for a structure or
schema to read the file it yes I
understand the file name is this and the
row separator is this and field
separator is comma but how many feels it
has and what is a data type of those
fields so that is one of the major
information which my component needs so
if I tell edit schema it will show up a
window for me wherein I can come and
manually create the data right I mean I
can create five fields I can give the
same name in the file and I can define
the data types so this is actually a
manual work right so why don't we you
know talent provides as an option to
read it from the file itself and it all
so predicts the data types and the
length whatever is available in your
data set and it will give you for a
review you can review that and then you
can accept it so how do I do it
I'll show now so rather than doing it
manually as I already explained all the
metadata where I create the structure
once use it many times so I can come to
metadata section so which file am
creating metadata for it's for a
delimited file so I just right click and
tell create delimited file and I say it
is metadata for products so and I'd just
say take the file which is my demo input
and sample input file so just by giving
this file my talent tool is able to read
this file and if I tell next it will ask
for all the attributes over here what is
the encoding what is the field separator
what is the raw separator whether you
have an heading so customize options I
was looking at is one of the example is
like this whenever you are reading
usually the first row will be heading so
you have an customized option here
otherwise if you have more than one you
can click here and give the number over
there since in my case it is in the
first row I can just come to a
customization option here and if you can
see here it is automatically click and
set one after I click this so I say
refresh preview so it asks taken
although so I see you know it is
separated by a semicolon so let me
change it to some column and I say
refresh preview so I have all my columns
listed over here so I can tell next so I
have product ID product name sales
rating so all of this are taking a
string if you want to change it to an
integer you can do it right away here so
if I know all my product ID is integer
so if you can see here so all of them
are integer and even says rating is
integer so if you want to maintain it as
integer you can maintain it as integer
and just see finish so you have your
metadata ready here so now if I see the
repository and I can choose the metadata
which have created now so I can just say
ok and now it will take the scheme over
there previously it was empty
whenever we check this edit schema and
now we can
the schema which has been created so I
did not manually create it so just by
giving the file talent itself created
and it gave it for my review and in
review if I want to change anything I
can certainly do it so in this way I
have configured my input component which
is reading my input file right and then
I'll have to join another file which I
look up file right even for lookup fat
I'll have to create a metadata quickly
we can create it so this is for lookup
so metadata for lookup I can quickly
give the lookup CSV and even this is
semicolon separated so I can tell next
and I can say semicolon and header is
the first row and I say refresh preview
so it has taken the two columns and data
will be available over there so current
ID and expenditure both are numbers if I
want I can take it to an integer and I
say finish and now first time I showed
you taking the file from palette which
we're components will be available and I
configured them here by mapping them to
repository right but another way to get
the component is directly from metadata
I can drag and drop so it will give me
the list of components where I can use
that structure or the file properties so
I can just click on T file input
delimited it will pre-populate all the
values which is required for me right so
if you observe the attributes of this
component it is already taken the file
name
it's all grayed out because it is a
repository if you want to change these
values you have to go to repository and
change it suppose if I want to try let's
click on this to change it will give me
an option either you can change it to
built-in that is manual option or you
can go on update it in the repository
because it might be used in many other
jobs right and all these values will be
pre-populated so we know two methods of
doing it either take it and manually
configure it or directly take it from
the metadata and now we have the input
file I can even rename this to input so
I can say products just by clicking on
it so product input and I have the
lookup now I need to join and map these
columns to the target which I need in a
different format so for mapping we have
a column called de map so I can just
drag and drop them and also if I want I
can just click on the designer and start
typing the component name I'll get the
list over here itself so I can take T
map which allows join column filtering
or row filtering transformations
multiple outputs it is kind of a large
component a heavy component which can do
many options like this many features are
available on this component so I'll just
take this team up so whichever I joined
first right so product input I am
joining first to team up so that will
become a main row main row usually
carries the data and even here I want to
take the data so it is main row but I'm
giving that we're mapping component okay
so mapping component treat this row as
lookup so it will be a lookup though the
main data is coming for team up
it is a lookup so that should be the
understanding right the main row will be
solid like and the lookup will be dotted
line so now if I go inside this team up
I can see all my metadata which are
created is available here and the lookup
also it is available here so how do I
join these two so there are options to
join over here so what kind of join I
should be doing is the inner join
right I'm looking up the data and only
data which is available should come in
over here okay so I can select inner
join and I can click on okay but what is
the key for joining as we know the
product ID is the key from the input
file I'll have to drag and drop to the
lookup of product so from the input file
that product ID should be equal to the
look of product ID and inner join will
happen on this right so after this inner
join I should be able to take all this
column from here and also the lookup
column from here so once you have
defined the joining key and you have
told what kind of join you are making
you can create the output over here name
that output
like you know products out and then you
can simply drag and drop the columns you
need so the mapping will be created and
then from the lookup I am trying to take
the total expenditure so total
expenditure will be taken from the
lookup so in my output now I have four
columns which has all the data which is
required right but what about the inner
join rejects so as I have previously
told you will have to join the I mean we
have to capture the inner join digits
with a valid reason so how will you
catch the inner join reject is the
question so we will have to create
another output say inner join reject or
products Ridgid and of course when it is
inner join widget I will not be getting
the total expenditure value because no
lookup you know will be available for it
so I'll have to only take for which
product I am not getting the lookup
value so I'll only take these values of
course my total expenditure will be null
but even then we can take it and display
that this lookup value is null similarly
like how I have done it for the project
which will be going for output but I'll
change one settings over here that is
catch inner join
reject is true so this output only
caches which are the inner join rigid so
I have inner joint conditions which will
go for output in this products out
section in products rigid I will catch
the inner join rejects and tell them
this is because of the inner join reject
that total expenditure is not up right
so with this we have completed the
mapping section of T map
okay so after this I will have to put
them into some target say for example
our target is file right now so I can
just take T file output delimited
and this is for products which are pass
and I can just copy paste another
component that I can name it as rejects
so the first output which should be
coming on the map will be products out
which I will be giving it to product
pass and I can configure this or put
component say I can go and create
new folder
that is demo output
so inside demo or put I can create this
file so maybe I can tell what is the
file name over here I can just take the
path of it
this is home a drink I'm demo output
so it's all magical and demo put
and I can see it is products output dot
cs3 and then for the rejects I can take
the reject output product reject and I
can give it to do the reject one and
similarly I can copy this path and
change the file and rename it as
products reject
and also as I said I can do some
business logic over here I told product
name I can just come here and click on
this editor and if I want to replace the
- or you can replace the make it you
know uppercase all the stuff I we have n
number of string handling functions
available so I can take replace when I
can double click on this so I'll get the
string handling over here so the string
which I am looking at as input is Row
one dot product name which is my input
and if I find a - I can replace it with
a space so I can do this so that my you
know diary products will be not diary
underscore products but then diary space
products so that should be the output so
this is one simple example where you can
transform your data right and also I
said we can have another extra column
here created say for example category
and you can take sales rating again and
you can do a transformation on it I am
just dragging and dropping sales rating
but I will write a condition over here
if sales rating you know if it is
greater than 7 then I'm using a ternary
operator I can tell it is good category
else I'll have to just use crushing Mac
first then if it is not greater than 7 I
can make it as bad so that can be a
derived column for me based on sales
rating so I'll get a category also over
here right so in products out I'll be
getting the category as fine so let's
now run this job so we have just given a
solution further simple use case which
we define we are reading the data from
input we are looking at the data with
product ID key and I am joining them and
the joint records successfully we'll go
to product pass and whichever does not
join the data which is not available in
lookup I will be capturing it them in
rejects
so this is the simple solution which we
can give so now let's run the job and
check if there are any errors and we'll
solve them so now the job has successful
run so let's go and check whether we
have the expected output in demo output
folder so in them or put folder we have
products output created let's open it
and as we have already seen a diary
product was having underscore now it is
removed and we have a space introducer
over here and based on seven or more
than seven we have good category and bad
category introduced over here so
management can have look at it and this
is the output which we were expecting
for products output dot CSV and then if
you come to products rejects so we did
not have a lookup for one so it is a my
you know foot way which was having sales
rating as six it doesn't have the lookup
so this product is rejected and I have
all the details to get to know which is
the product which is product ID right me
and product sales category so in this
way I can separate the filtered out data
to my products or to my rejects so going
forward if management wanted based on
category we wanted to divide the data
then we can simply call T filter row and
you can take the same data which was
given to your product pass and you can
introduce a condition if your product
category is equal to good I can just
check whether exact case sensitive is
available so it's capital G and capital
B so if it is good I'll filter out to
one of the delimited file I can just
copy paste again and I can use it and
change the name so I can give it to this
filter and I can chill products output
good I can bring any metals good and
whichever gets rejected that will be my
bad so I can just right click on this
and filter rejects I will be giving it
to this so this is product passed but it
is good and here it is product paths
but it is bad so I can come here and
change the file name too bad and I say
sing columns because filter reject will
also give me the reason why it has been
rejected so that is a standard error
message which will come along with you
are rejected so it is displayed in green
because it is standard
so I'll just go back and run this job
so it is successfully completed and now
I have my good CSV and batch yes way as
well so this is all with your good
values you can see it over here and
similarly we'll have for bad which has
all the bad categories right so this is
how you can again in bad category we
have the error message because the
condition which we give that category
should be equal to good is failed that
is why it has been rejected and it can
be your filter condition can be more
than one in that case it makes sense to
have this reason which is fail for what
reason right so this is how with the
simple solution for this ideal we were
able to read the file we were able to
join the two files we were able to
capture the Richard we were able to do a
small transformation and also we
introduced it in new column and then we
also filtered few of the business
conditions and gave the management or a
separate file for good products and
supply it file for bad products so with
the help of all these a good business
decision can be taken and that is how
ETL is making your life easy hope you
liked this demo please follow us for
more updates thank you one and all I
hope you have enjoyed listening to this
video please be kind enough to like it
and you can comment any of your doubts
and queries and we will reply them at
the earliest do look out for more videos
in our playlist and subscribe to any
Rekha channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>