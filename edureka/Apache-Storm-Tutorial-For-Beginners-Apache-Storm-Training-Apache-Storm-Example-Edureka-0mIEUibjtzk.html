<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Storm Tutorial For Beginners | Apache Storm Training | Apache Storm Example | Edureka | Coder Coacher - Coaching Coders</title><meta content="Apache Storm Tutorial For Beginners | Apache Storm Training | Apache Storm Example | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Storm Tutorial For Beginners | Apache Storm Training | Apache Storm Example | Edureka</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0mIEUibjtzk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey what's up guys welcome to Apache
storms certification training and I
welcome you all to Ed Eureka I will try
to make as much engaging in fun as
possible and we will have lots of live
coding sessions demos and assignments
hope you all have fun right so what are
the objectives will after completing
this you should be able to know the
basics of Big Data and loop and if you
already know it you'll be able to recall
it and you will be able to understand
what is patch and real-time analytics of
big data and how do they differ we will
see what are the shortcomings of Hadoop
and we'll understand what is lambda
architecture and why do we need it you
will be able to develop a basic
understanding of Apache storm and its
components and this will act as a
excellent starting point for writing
storm topologies in future and I'll also
explain the use cases and key
differentiators of storm why it is
preferred over some of its competitors
and why you should pick storm and where
does it fit and where it doesn't so
let's see the definition of storm so
storm is an open source computing system
used for processing real-time big data
and analytics wow that's a pretty
convoluted statement so let's break it
down a bit so first let us understand
what big data analytics is then we will
understand what real-time is and then
maybe this whole statement will make
more sense so what is big data big data
is anything where you have huge amount
of data so as the definition says big
data is the term for a collection of
data sets which are so large and complex
that it becomes difficult to process
them using your regular database
management tools like you have your
MySQL PostgreSQL
and any other traditional data
processing applications that you have
and the challenges are plenty I mean you
cannot
captured that amount of data when you
have terabytes and petabytes of data
produced every maybe day or month it
becomes really challenging to capture
that data and the creation of that data
and the storage of that data because
most of the data is unstructured you
know you cannot store that type of data
into your regular on-hand databases and
the sharing of data because the data is
in terabytes and petabytes you cannot
share or transfer the data over the wire
easily and you cannot analyze that much
amount of data because you cannot bring
it all into memory and most of the data
is unstructured in big data world for
example you have log files log files
will have so much amount of unstructured
data I mean almost 80% of the locks that
we have are not used for almost 80
percent of the time so you can see how
we have so much amount of data which we
don't need and it's very difficult to
extract some meaningful data out of that
unstructured data so let's take an
example of a big data system so you have
your stock markets which generate about
one terabyte of new trade data per day
to perform optimal stock trading
analytics to determine trends over
optimal trades so people are buying and
selling stocks all the time and if you
start saving all of that data because
the data might be very useful and as we
will see and you can extract so much
information out of it
that might be of use to you so how can
you use that data once you have
collected it
there are basically two aspects of
analytics one is discovery that means
you want to identify interesting
patterns in the data whichever makes
sense to you and another one is
communicating you want to provide
insights in a meaningful way to your
upper management so that so they can use
those insights into some productive work
and the goals of analytics are to
increase the sales of course that's the
primary goal but also to make your
company more efficient in terms of cost
saving and people saving so unstructured
data is exploding day by day you can see
the graph here we started collecting
data after 2000
mainly but after 2010 the data exploded
at an exponential rate and that is one
of the reasons why machine learning is
picking up so much because it requires
lots and lots of data and you can see
the numbers here are pretty appalling
2500 examples of new information in 2012
with Internet as primary driver and
digital universe grew 62% just in one
year to 800k petabytes and it will grow
to one zettabytes this year so of course
the number are pretty dramatically and
relational data of course is not
increasing at that much space because
most of the companies are moving to no
SQL databases and secondly there is lot
of unstructured data you can see with
big data analytics we are targeting this
orange zone that is your application
data so iBM has his own definition where
it says if you want to identify that the
data is a big data it need to satisfy
three V properties which is volume
velocity and variety volume means you
should have very huge data velocity
means it should be produced at a very
rapid rate it should not happen that you
do have large amount of data but it was
collected over last 20 years it should
be like you have produced huge volume of
data at a very fast rate and then your
data should have a variety if your data
doesn't have variety if you have a very
plain grows and it is not unstructured
you can probably put it into any new SQL
database and start querying it so unless
your data is unstructured and you have
lots of varieties of data that data
isn't considered big data I'd say when
you have your data you have mainly two
types of analytics one is business
intelligence where most of the data will
have multiple dimensions and you will
try to find insights using those
dimensions and you will even do machine
learning using these dimensions and so
for example you will have dimensions
like this stock price was increased by
this much at this time so you have three
dimensions in your stock data that is
one is stock symbol another one is
percentage change and third one is time
so now you can play around with these
dimensions like what was the average
increase in banking sector at 2:00 p.m.
yesterday
or if you had ecommerce you can say like
which shoes were sold most in month of
May in Delhi area these kind of analysis
can simply be done by writing a simple
program which aggregates on your data
and and finds out these simple business
insights for you another one is
predictive analysis which is getting
very popular these days and it works on
statistics and machine learning so you
can predict user behavior from it it
includes things like clustering
classification and linear regression and
you can say things like these things
were frequently bought together like you
have on Amazon and you can say like this
phone and this screen guard were
frequently put together so let's talk
about what is batch and real-time
processing let's first talk about batch
processing so we just saw that batch
data processing is an efficient way of
processing high volumes of data where a
group of transactions has collected over
a period of time so data is collected
and entered processed and then the best
results are produced batch data
processing requires separate programs
for input processing and output so an
example is like payroll which usually
occur on monthly cycles and something
that might occur on days is well it
doesn't have to be monthly only so other
example can be you're trying to get some
consumer trends over the past few years
so past few days maybe so you can ask
questions like which shoes were most
popular among adults in last 30 days so
these all are batch processing tasks so
let's see what real-time processing
tasks are in real-time data processing
the data is coming continuously and
our processing and outputting the
results of that data so data must be
processed in a very small time period so
for example you have radar systems you
have customer services and suppose when
a shopper hits the ATM you need to
update his bank account immediately and
it should be so close that the customer
does not even notice the delay so you're
processing the events as they are coming
in another very popular example that you
will see everywhere is to analyze your
data within a rolling window so suppose
if I ask what were the most popular
hashtags in last 10 minutes of course
you can do a batch process and give that
to me suppose our batch process is very
fast it doesn't take any time and you
get the most popular tweets in last 10
minutes instantaneously but the next
minute that data is stale because you've
got new data in and last 10 minutes the
new popular hashtags in last 10 minutes
are now changed so what you have is
stale you need to continuously keep
updating the count of your hashtags as
soon as the data is coming in for that
you cannot use your batch process you
need something that is processing the
data as soon as the data is coming in
and giving you the live view of data all
the time in that case we will always use
real-time processing so let's see what
are the advantages of batch processing
first so it is ideal for processing
large volumes of data and it increases
efficiency rather than processing each
record individually you can write your
code much more efficiently if you know
that data will be coming in batches
rather than one at a time and it can be
done or processed independently during
less busy times of course at day time
your servers are busy serving the live
traffic if you are trying to do
real-time analytics you'll need to buy
new servers but if you are using your
batch service you can free up some of
your servers at night and use those
machines to batch process your data
let's see what are the disadvantages the
only disadvantage of batch processing is
the time delay between collection of
data and
getting the result so as I gave you the
example of rolling window of course that
time delay is not acceptable let's see
what our real time processing advantages
so advantages there's no delay in
response of course it is very very
useful in social networking so how can
you use this in social networking with
your own big database and then you can
measure the immediate impact to your
site traffic from any social media so if
you have posted an ad on social media or
you have launched a new blog post or you
have to eat something or you have put
some reward on liking or commenting on
your page on social media you can
measure the impact real time and of
course knowing this information
translates to a better conversation and
more effective online campaigns of
course the SAS companies that means
software as a service companies which
are serving many clients at a time can
measure user behavior and act upon it
for improving customer satisfaction and
conversion rates which means immediate
increase in revenue and that is what
most of the companies care about
financial services so determining in
real time whether your portfolio is
losing money or there's a fraud in your
system you can prevent disasters before
they are current and not after the
damage is done and secondly you can have
sentiment analysis or you can correlate
three or four parameters to to give you
live insights into the market to
maximize your profit so let's talk about
a real life need for real-time analytics
so the problem statement is you need to
find the total number of pageviews of
Eddie Erika's block over a range of time
so the range can be give me page views
of this particular block of last one day
so Google Analytics can provide this
information to you right so example for
a particular day the data can look like
this where these are number of views on
each type of blog these are the
percentages of total number of views
what is the challenge here querying huge
amount of historical data of course as
soon as you get the event of
doing a blog you put it into a log write
you possibly won't keep viewing events
in your relational database so the
problem is if you are putting everything
into your logs querying that huge amount
of historical data is gonna be slow so
if you have all the data and that data
has become a petabyte scale suppose you
want your aggregating over last one year
of data that will be too slow right so
what you want so you want to aggregate
the data so that you don't have to save
all the data so you will pre compute the
data and then query on that so for
example Google Analytics might have to
keep the historical data aggregated for
each hour as pre-compiled view so
suppose this is all your page view data
you will keep it into a database like
this so this data has been aggregated so
for this particular URL this was the
hour of the day and this were the number
of views about this blog and maybe you
will have date as well now when you
query you can you just need to aggregate
on all of these and you will have your
data of course you can do this thing
using your Hadoop branch process you
will run a Hadoop a chop on all of your
data and then you will pre compute the
views and then you can query that data
but what about the data generated that
was after the precompiled view so
suppose you ran your Hadoop shop
yesterday you won't be able to see the
data that you have today so what about
that data you can keep putting your
real-time data into an application just
forget about this ellipse as of now and
just think of this as a black box
application you are putting your
real-time data into this application and
you are computing real-time views on
that data and then you are saving it to
data base of your choice so you'll need
to do two things now you are sending
your live data into your Hadoop shop so
that you can pre-compute and run the
batch jobs daily and then you will have
one more application and that will pre
compute your real-time view that is
today's view working on events as they
come in and then when you query you can
merge these two results and you
the complete information in your query
so this sort of architecture where you
send your data to your bad jobs and to
your real time jobs is known as lambda
architecture and it is because this
thing it looks like lambda as you might
have seen a lambda is a Greek symbol
which looks like an inverted V so this
is what a lambda looks like so Nathan
Mars is a guy who was the inventor of
storm invented the term lambda
architecture so what you will do is that
every time your new data is coming in
you will send it to two places one is to
your batch layer another one is to your
speed layer so bash layer has two
functions managing the master data set
so all of the data that you write in to
your batch layer will be immutable that
means you will just be appending to same
files again and again without disturbing
the data you already had whatever the
software's and processes and MapReduce
jobs you will write on that master data
set you will never ever change your
master data set you will just read it
compute results on it and save it
somewhere so that if anything goes wrong
at any point of time you can just rerun
your job and you will get the exact same
data again this property is known as
idempotent that doing same things again
and again on a particular piece of data
will produce same amount of data once
you have produced the views after doing
some computations on your master data
set of course you can query these data
sets in a low latency ad hoc manner now
let's talk about speed layer so the
speed layer composites for high latency
of updates to serving there and deals
with recent data only speed layer will
only work with the recent data as I told
you about the rolling window your speed
layer will only work on the data that
you collected after the pre compiled
view so that you don't have
cross-cutting data so now you have a
real-time view which has suppose your
job was run yesterday at 12 p.m. you
have data up to last 12 p.m. here and
the new data after 12 p.m. is here
so whenever you want to query now your
query will merge the results from last
12 p.m. from here and desserts prior to
that from here and you will have
complete information again alright guys
so now that we have seen what is batch
processing and what is real-time
processing
let's see what is storm and how does it
fit in so as the definition says storm
is a distributed reliable and
fault-tolerant system for processing
streams of data so anytime you need to
process your real-time streams to
produce meaningful data within a
sub-second latency storm is your guy so
to understand storm let's take a simple
analogy suppose you are running a
company and you are interviewing people
for some particular job so now your job
is to filter candidates out of thousands
of candidates that you are receiving so
what will the scenario look like you'll
have a main gate by which your
candidates are entering then you will
have a given room in which four
panelists are sitting so first panelists
only checks your name and checks your
resume a that you have everything that
you need and then he sends you to the
next person so this guy only takes up
one minute of your time so then now the
next guy takes your technical interview
he takes suppose half an hour of yours
then you have a third guy which is an HR
interview and which only takes up here
five minutes of time and he just wants
to see whether you did right or not and
this is the final filtering funnel and
then after that you go back to where you
came from and there will be someone
sitting at the reception who is
coordinating all of this so that if guy
sitting in the room tries to take a
break then he will halt people at the
main gate and whenever they are ready he
will keep sending more people now this
looks like a phase strategy but what's
happening here is that until one
candidate has come out you cannot send
another candidate because even though
the first guy is taking less amount of
time the second guy is taking
half an hour so if something is taking
half an hour people will keep piling up
there and he will not be able to handle
the loot and the third guy is also
sitting idle for 25 minutes after he has
processed the first guy in five minutes
so what can be the solution here you'll
have one more room in which you will
have same three guys and now the
receptionist will start sending people
in from gate to this room as well after
that if you get even more candidates you
can have one more room and you'll have
three rooms doing the same task with
same number of people and the person
sitting at the reception will keep
sending guys to the corresponding rooms
as soon as they come out but there's a
catch here which I didn't tell you about
the catch is all of the three people
sitting in this room are all technical
people so first person can take the
interview as well so what's the problem
with this approach the problem with this
approach is that the first person and
the third person are sitting idle most
of the time so now you are having a very
long line at you gate because three
rooms are still not efficient what can
you do about that you can do one thing
notably that I can see suppose this is
your input gate so forget about this
what is powered in bolt for now and just
think of this as your input gate which
interview candidates are coming so as a
solution what you can do is that only
assign two people for the first task who
is checking the resuming and identity of
the people and then assign four or five
people for the interview process the
main interview process which is taking
so long time and then assign maybe only
one person for the last process and
depending on how much time that process
is taking so this will be a great
solution because now the number of
people are dividing this will be a great
solution because you can increase the
number of resources where your
performance bottleneck is performance
bottleneck was the interviewing part so
now you have more people there so this
is what storm is about you have your
input data which you want to process
somehow you can divide it into the small
number of steps and define what
step will do and then you can define the
parallelism at each step that this
process only takes small amount of time
only assigned to threads through this
this process takes longer amount of time
assign three threads to this and the
last process is very small just assigned
one thread to it and maybe sometimes
assigning more number of threads to a
particular process might not be easy
because that person might need to
coordinate all the outputs so that he
can write down which all candidates were
selected and he can count that your
selected candidate quota is fulfilled or
not because if you send them to
different rooms you will have to
coordinate between them that how many
candidates have been actually selected
this is what you will find in these
points as well the work is delegated to
different types of components that are
each responsible for simple specific
tasks so this guy is task was just to
note down the identities and check the
resumes this guy's task was to take
interviews
this guy's task was just to count the
number of people that has been selected
the input stream of storm cluster is
handled by a component called spout so
the first guy who is taking people in
from the gate and checking the
identities and making sure that everyone
goes to the right person this guy is
called a spout this guy's only job is to
handle in input data and tag that data
if you want to and then divide that data
to the next processes the spout passes
the data to the component called bolt
which transforms it in some way so now
you have bored the guy who is taking the
interview you might have second round of
interview as well so you might have more
bolts in here and have as many number of
steps as you would like so all those
steps will be called bolts and you can
just name your bolts that this is step
one Pole this is step two bolt this is
step three bolt and all of them are
transforming the data or interviewing
the data or doing anything that you
would like with the data a bolt either
persists the data in some sort of
storage or passes it to some other bolt
like we are doing it here
so this guy did not persist anything the
last guy suppose he checked down whether
this candidate was selected or not and
if he was selected he just saves it down
to a database so this is what all storm
is about so there are only two main
components that you need to worry about
one is spouts one is pulled all right so
let's see what components this storm
have so the components we just saw that
were spouts and bolts that were from
developers perspective that you need to
care about but now let's see how storm
handles all of the things that you gave
him and how does that coordinate between
all of those processes think of those
machines as rooms that you had in your
interview center where you were
interviewing the candidates so each room
can have multiple bolts each room can
have multiple spouts and each room is
known as a worker and there will be a
person who is called supervisor who will
be sitting in each room and will take
care of all the people who are coming in
this supervisor is not one of your bolts
and this supervisor will be a demon
process which is run by storm just to
take care of all of your spouts and
bolts within that worker storm mainly
has three sets of nodes one is Nimbus
node another one is zookeeper node third
one is supervisor so Nimbus node is like
the receptionist who was sitting at the
counter and who was coordinating
everything if any person in any room is
on a break
supervisor will let the receptionist
know which is Nimbus and Nimbus will
stop sending candidates to that
particular room in fact if any person
has to go somewhere
Nimbus will send the replacement of that
person so it is the person sitting on
the counter who is responsible for
everything so it is like the job tracker
that you have in Hadoop so Nimbus is the
guy who uploads competitions of
executions so whatever code that you
have written you submitted to Nimbus and
it's Nimbus is responsibility send that
code over to all the supervisors that
you have with you it also distributes
the code across the cluster
it launches workers across the cluster's
workers means rooms and it monitors
computations and reallocates worker as
needed so I think you get the idea what
Nimbus does so you don't need to go into
too much detail of nimbus zookeeper and
supervisor because this is what storm
handles and you have nothing to do with
this of course this type of information
can be handy while you are debugging
your system if you don't know what
zookeeper is zookeeper is a central
configuration management system so
Nimbus talks to supervisors who are
these zookeepers so that even if Nimbus
goes down for some time he knows where
every supervisor is running and pick it
up where it left the last time
supervisor read the status of what
Nimbus is saying while zookeeper and
everything they want to coordinate about
they will coordinate why a zookeeper so
that you don't miss the state if
supervisor goes down it should know
where your Nimbus is and when Nimbus
goes down it should know where your
zookeeper is and what state they are in
right now so you have your supervisor
notes which communicates with Nimbus
through zookeeper so let's see what
components does Tom have so there are
five things in total that you need to
take care in order to write great storm
topologies so one is tuple so whenever
you are sending a data from one spout to
next board each unit of data is known as
tuples so now the interview candidate
that you are sending from here to here
and here to here is a tuple and in
general when you're sending the data
from here to here
that will be a grouped amount of data
think of it as a row in a DB or in an
excel sheet so whenever you will write
your topology you will define the
headers that and you will be emitting
and then in the subsequent Emmet's you
will keep sending these tuples one after
they another so you will define the
column headers first like first one as a
b c d and in the subsequent requests you
will just send some comma separated
values known as tuples and the bolt will
understand that the first value
corresponds to a 1 corresponds to B this
corresponds to C and this correspond to
D so you cannot change the data format
once you have
so once you have declared four quadrants
you cannot send three columns
another one is streams it is unbounded
sequence of tuples so this pipe that you
have opened for sending data from here
to here is known as a stream so you can
keep sending the data via this stream as
long as you have the data then you have
spouts spouts are the guys who are
reading the data from outside world so
this is like the gate that you had and
you are getting data from all over the
world people are coming why a taxi wire
buses wire trains by their own vehicle
and spouts is responsible for taking
that data in and sending the data to
your bolts and in real life you will
have something like your logs that are
being read by the spouts or your Twitter
API which is sending you continuous
tweets then you have bolts bolts process
input streams and produce output streams
as you can see here they can run
functions they can run filters they can
aggregate they can join data they can
talk to databases they have all the
power in the world and they can do
anything it's really in developers hand
what you want to write here you are
given a method that is called execute
and you can do anything to the data that
you are getting in and overall after you
have defined all of these steps and
defined how they connect to each other
that will be essentially a directed
acyclic graph that thing is known as
topology so the overall calculation
represented visually as a network of
spouts and bolts is known as your
topology so let's see if they use cases
of strong as we have already seen storm
is used for real-time stream processing
and there is no need to have queues when
sending data from one component to
another so if the second bolt is not
able to handle the load storm will take
care of that itself and after few of the
events have piled up on that bolt storm
will stop sending data to that bolt
instance until all that data have been
processed so that queue is sort of
producer-consumer queue so strong takes
care of handling of the producer and
consumer queues between poles of course
it is used for continuous computation
the data you are getting in you can just
keep computing the data and the
cause you are just adding the numbers
that you are getting in you can keep
adding those numbers and show them
somewhere maybe on a dashboard as you
would like and you can do all sorts of
analytics on that sky's the limit really
then you can have BR PCs this is a very
fine thing that has come up in last few
years and what BR PC is so suppose
someone has called your API and you have
a task which is very CPU intensive like
some sort of image processing or
something so instead of doing it on your
machine which will take a long time you
can send your operation to remote stomp
cluster and storm cluster will paralyze
the operation compute the result and
send the result back to you so all your
CPU intensive operations will be done by
stop in a very quick time so we have
already seen how it can be used in
financial social in retail world so you
can use storm to prevent certain
outcomes or to optimize certain
objectives so of course you can save
security frauds and compile insulations
order routing pricing you can optimize
your offers you can just the performance
of a particular offer using storm and
real-time events and if that particular
offer is not doing good just take that
offer out and you can set your pricing
according to the pricing of your
competitors so suppose you are getting
the price events of your competitor
websites from some stream you just need
to pass that and whatever prices you
have on your competitors just set the
product price of your product according
to that so of course you can have all
sorts of functionalities that you see
here you can go over them one by one I'm
pretty sure they make sense from the
outset so what are the key
differentiators of storm that make storm
unique well it is very simple to program
and the complexity of distributing the
data and fault tolerance and resilience
is all drastically reduced while
programming with storm you will see how
simple it is to program with storm and
it's easier to develop it in a JVM paste
language maybe Scala closure or Java but
you can basically write your spouts and
bolts in almost any language that you
would like
so if you have suppose a machine
learning library that is only available
in Python you can write your storm and
bolts in Python and leverage those
libraries and the most important feature
is it is fault tolerance so if anything
goes down storm will reassign those
tasks some other node so you won't have
to worry about whether something got
processed or not the stone technology
stack storm is mostly written in Java
and closure and some of the parts of
Java are also written in Python storm
runs on JVM and it is written in
combination of Java and closure and
storms primary interfaces are defined in
Java and the core logic is implemented
mostly in closure so most of the code
that you will be writing in storm will
be in Java storm is highly polyglot
friendly that means you can write your
spouts and bolts in almost any
technology possible but you cannot skip
the Java code you will have to define
your topology in Java but you can write
your spouts and bolts in almost any
technology some parts of storm are
written in Python so all the storm
daemons and management commands are run
from a single executable file that is
written in Python so all the
command-line tools that you will see are
mostly written in Python this also
includes the Nimbus and supervisor
daemons
that we talked about in our last lecture
and we will see that all the commands
that you need to manage and deploy your
topologies are written in Python due to
this reason you need a Python
interpreter installed on almost all
storm machines that are used by the
storm cluster now let's move into some
interesting stuff storm has basically
two running modes one is local mode one
is remote mode local mode as the name
suggests is used to run storm topologies
in your local machine this is generally
used at the development time and you
will use local mode for testing almost
all of your topologies remote mode is
the mode where you submit the topology
code that you
have just written packaged it into a jar
and then give it to the storm cluster
that is running remotely and that will
distribute your code on all the nodes
and analyze your data so local mode
although it can paralyze your data it
will just use one machine your local
development machine and you don't have
to worry about whether this thing will
run smoothly on remote or not because
Tom takes care of that for you so you
will be writing a simple java program
which you will be running just as a
simple java program and when you give
that program the storm cluster it will
take care of all the parallelism fault
tolerance and resilience for you isn't
it awesome so it takes away almost all
the headaches that comes with
distributed data processing so let's
talk about the local mode in detail
first so local mode is used for
development purposes and the strong
topology that you will write will be run
on your local machine in a single JVM
you can debug your code you can test
your code and and after you are done
writing all the code you can submit it
to the remote storm cluster in the
remote mode we submit our topology to
the cluster the storm cluster which is
composed of many processes usually
running on different machines and you
can add more machines to the cluster
anytime you want and you can remove some
nodes from the cluster anytime you want
and remote mode doesn't show debugging
information so which is why it is
considered production mode so all the
debugging information is reserved only
for local mode now in remote mode we can
adjust parameters that enable us to see
how our topology is running in different
storm configuration however you can also
create a remote mode storm cluster on a
single development machine and I think
it's generally a good idea to do so
before deploying it to production storm
cluster directly it lets you make sure
that there won't be any problems running
the topology in production mode so let's
create our first storm topology you know
in fact I think before creating the
topology of counting words let's first
create
topology of hello worlds so that we can
understand what do you mean by a
topology and what are the components
that are required in a topology so let's
dive into a live coding session okay so
now you will see how easy it is to write
your first storm topology first we will
write a simple hello world
storm topology and then we will submit
it in local mode after we are absolutely
sure that our topology does not have any
bugs and it is outputting what it is
supposed to be we will submit it to a
storm cluster and we will set up that
cluster on our local machine as well I
am using IntelliJ here but you are free
to use any other IDE like eclipse so
first we will create a maven project so
let's create a new maven project click
on maven select next just give any group
ID that you like Kondor test and
artifact ID will be tutorial dot storm
next and our project name will be let's
leave it as tutorial dot storm and press
finish so after our project is created
we need to import storm let's go to web
search for storm core maven dependency
go to first link get the first
dependency it doesn't matter which
version you get copy the dependency
switch back to IntelliJ add the
dependencies tag and paste your
dependency I'm also using the
presentation manager which will let you
see all the shortcuts that I'm using so
that you can see what sort of shortcuts
I use and maybe you might find some
useful ones for your own benefit now
since we have important storm we will
need to change the scope from provided
to compile so whenever you are using
your storm in your local mode you will
need to provide storm yourself that
means the scope of storm core will be
compiled but when you will be submitting
this to a storm cluster we will change
the scope to provide it because in that
case remote storm cluster will provide
it the storm jar so that you don't have
the version mismatch now let's create
our first spout so
we discussed there are two main
components of a topology one our spouts
another one is bold we can read the data
in the spout from any of the cues like
Kafka rabbitmq or a file or a database
or anywhere you want it to read the data
from but right now in our hello world
spout for simplicity we will just keep
generating the data so what our hello
world topology will do is that our
spouts will keep producing some random
integers and then it will pass those
integers to next board what our bolt
will do it will just multiply whatever
it gets with two and then outputs it to
the next port but I think for
simplicity's sake we will just write one
board now that bolt can push the data
doing some data base by which you can
show it on some dashboard or maybe you
can alert someone based on that but for
simplicity we'll just be printing the
data out of our board let's create a
first Java class let's name it integers
now all our spouts needs to extend Base
rich spouts we will also talk about the
hierarchical spouts in a bit of time but
for time being all of your spouts need
to extend based which spouts so let's
extend it from here it's about let's
implement the methods it has got so now
you will see we have three methods that
we need to override let's go through
them one by one so open is the method
which is sort of constructor for your
spouts you will be passed in the map of
your configuration and you will be
passed the topology context and you will
be passed in spouts output collector so
anything that you want to produce from
this pout as the name suggests it will
go to the spouts output collector so we
need to save an instance of spouts
output collector let's create an
instance of spouts output collector
that's assigned its power to our
collector this power collector
so that's it we don't need any fancy
stuff here yet
now let's first talk about declare
output fields if you remember in my
previous lecture I talked about tuples
so what a tuple is basically you define
the name of columns at the beginning
when your spout is initialized and in
the subsequent calls you just keep
omitting the rows so if you declare five
column names at first you will have to
omit five comma separated value in every
subsequent call so in this method will
declare the headers of our columns so
right now as we discussed we are only
outputting random integers so what we
will do is that we will output a field
and we'll name it field so for example
we will do output fields declare dot
declare new fields field so we have
declared the name of our output field it
will be field now we just need to
declare random integers from here let's
do one thing let's emit integer from 1
200 and then stop because we don't want
we don't want to keep this topology
running forever so just initialize an
integer here and it's and let's
initialize it to 0 now what we will do
in next tuple so now your next tuple
will get called whenever your previous
entry that you emitted is processed by
all the bolts this is like that
gatekeeper who is standing there and
whenever your interviewee candidate has
returned from an interview room your
gate will be sending the next person so
we omit the next entry by using spouts
output collector dot emit as easy as
that so you use the fields class to
declare the field names
similarly you use values class to
declare the values and we will just
increment our index I think this should
make sense so every time your next tuple
is called it will just emit the new
value and increment the index first time
it will emit one the next time it will
emit two then it will emit 3 4 and so on
up until infinity so let's give this a
limit if
index is less than hundred
all right so our next task is to create
our board let's create a new Java class
let's name it multiplier port now every
bolt needs to extend some sort of bolt
in our case we will use base basic port
let's implement the methods now we only
need to implement two of its methods in
most cases extending base basic bolt
will be enough but we will discuss more
about bolts hierarchy so that you can
make sure which bolts needs to be
extended when so I think both of these
methods make sense so you will be
declaring the output fields just as in
spouts here what you will be emitting
out of this bolt in this execute method
you will be writing that logic that you
will perform on the tuples that you get
in so this is like the person who is
taking the interview let's get the value
out of this tuple which was emitted by
our spouts tuple dot so now you have
couple of options
either you can take things into string
and then cast it yourself or directly
take it integer now you can access the
values by indexes code access the values
by their names so what was the name that
we emitted here the name was field we
can get it by a field or we can get it
wire index since we only have one value
let's get this by index it's easier to
get it by index let's assign it to a
variable and let's name it our number we
need to multiply this number by two and
now we need to omit this number to the
next bolt and if you don't have any next
bolt it will just be informed back to
your spouts that this entry was
processed so even though you don't have
a next bolt we will omit this entry so
that your spouts can know that the entry
was processed and it can send the next
entry in so it is always important to
omit things out of your last bolt and if
you notice one more thing we don't need
to save the basic output collector like
in our spouts because this will be
passed in to us in every call so let's
Emmit
let's declare the output field that
we'll be sending
now that we have implemented our spouts
and our bolt let's create our topology
so topology is like a simple Java
program in which you will say that this
pout will be my entry point then after
this my data will go to this so policy
builder will basically create that graph
in which you will specify how your data
will flow in your spouts and bolts so
let's create that main topology I know a
lame name but never mind so this is just
a normal Java program you don't need to
extend anything here just write public
static void main and let's fill it
apology the first thing you need here is
a topology builder so let's create
topology builder
now using this builder he will set your
spout you can give any name to your
spout let's name it your integer spout
and the spout name can be anything that
you will use later on this is sort of an
identifier for that spout let's create
your integer spout now that you have set
your spout let's set your gold let's
name it
multiplayer port now you must be
thinking that we need to assign these
spouts and bolts in sequence so that
this bolt knows that I need to get data
from the spout but that's not the case
actually you need to tell builder that
this bolt will get data from this pout
explicitly and how do you do that using
groupings so in groupings because he
will have multiple instance of bolts you
need to tell that what instances of
spouts should go to what instances of
bolts so to understand groupings let's
talk about that interviewing analogy
once again so suppose what you want is
that you want to send all the candidates
whose name starts from a two-room one
candidates whose name starts from B to
room 2 and candidates whose name starts
from C to room see now how will you do
that or suppose there is some other
strategy that you want you want the
spouts to send people randomly to any
room whichever has free bandwidth or
suppose you might have another strategy
in which you say people from computer
science background should go to room na
people from mechanical background should
go to room B so you can have so many
strategies that which candidates should
be sent to which room similarly you can
have many strategies here in which you
can define which instance of bold should
get what data and you will see how
important this is when we build some
complex topologies further right now I
just want to say give me random tuples
to any of the bolts that you have so for
that we will say shuffle grouping
integers
so that means this bolt will get data
from this pout in a random fashion now
you can also pass in some configurations
to your topology at runtime so this conf
object that you are getting here you
will get your configurations here so
this can be file paths or maybe some
URLs or or the number of threads or
anything for that matter so similarly
you can have this in volt I haven't
shown you the method where you will get
your configurations but we will get to
that for now we will just create an
empty configuration let's set the debug
to true so that we can see what's going
on in our topology now your topology is
built the configuration is created let's
submitted to the local cluster as we
talked about we have two modes in which
a topology could be run one is local
mode one is remote mode so let's submit
our topology to local cluster let's
create a local cluster cluster mister
dot submit apology let's give some name
hello
Kaji pass at the configuration and then
build a dot what this is config also
let's put this in a try method and
finally we want to shut down our cluster
as easy as that
so let's try putting in a sleep for some
time so that the topology doesn't finish
before it is and you just need to do
this in local mode you don't need to put
a sleep in your remote mode and there
you go you have your first stomp
topology built in a CPC right let's try
running it so
see what we have all the spouts and
bolts that we have are basically
executors so storm runs spouts and bolts
as executor processes so you can see the
executor that ninety eight was emitted
ninety nine was emitted ninety seven was
emitted ninety six was emitted now you
can search for integer spouts and you
will see that it emitted 95 94 93 up
until 99 and when you search for a
multiplier bolt you will see it is
multiplying the numbers that were
streamed for integer spouts with 2 and
outputting those numbers you can also
try a cleaner method for testing like
just print these out to your sister and
comment debug logs and then when you
will run it you will probably see much
cleaner logs so that is a better way to
test here yeah so you have all the
numbers that are outputted by your
multiplier port I hope it makes sense so
let's see the course outline that we
have so this course is divided into six
parts where the first part will be
giving you all the background about big
data analytics and we will see why we
need storm and where does it fit in the
big data space then we will dive into
storm installations and we will see what
are groupings and then we'll go to nuts
and bolts of storm but in case of storm
we don't call them nuts and bolts and we
call them spouts and bolts and then we
will see what is Kafka and why do we
need it how does it fit into this then
we will see what is that right in
topology why do we need it we'll also
see D RPC and and then we will have
practical sessions of Apache storm which
will be self-paced which will help you
to kick-start the process of writing
topologies in storm I hope you have
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and we will reply them at the earliest
do look out for more videos in our
playlist and subscribe to any rekha
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>