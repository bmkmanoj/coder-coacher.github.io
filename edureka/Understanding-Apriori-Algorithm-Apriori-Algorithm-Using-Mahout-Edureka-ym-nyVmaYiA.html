<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding Apriori Algorithm | Apriori Algorithm Using Mahout | Edureka | Coder Coacher - Coaching Coders</title><meta content="Understanding Apriori Algorithm | Apriori Algorithm Using Mahout | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding Apriori Algorithm | Apriori Algorithm Using Mahout | Edureka</b></h2><h5 class="post__date">2015-02-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ym-nyVmaYiA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the most simplest one that you can
find in the literature is a priori
algorithm okay there are two steps
finding all the frequent items in item
sets that means the ones that have
support greater than the threshold of
minimum support okay they are called as
frequent itemsets obviously and using
these frequent itemsets to generate
rules and as we said we will focus only
on the first part because 99.9 percent
of the time is spent in this phase alone
okay so like for example you from here
chicken clothes and milk is a frequent
item set and using this item set you can
generate a rule of this sort from this
from this item set okay that's the first
phase in the second place now let's talk
about the first part which is finding
all the item sets and we already said
computing enumerated all possible
subsets is very expensive right like
whatever we draw on whatever we wrote in
the text text pad which is same as this
right at the lowest level you have one
item sets and then you have two item
sets combining multiple of this and then
by combining multiple of these you can
combine and you can compute three item
sets and so on you can go on right so
all of this is called a lattice of item
sets okay
so essentially you are taking all the
subsets and then putting them in a nice
nicely structured manner so for example
a and B together form a B a and D
together form a D and so on and then
similarly a be a D and BD form a BD
right so in other words the a B D subset
consists of three two subsets which are
a B a D and VD and each of these contain
two one subsets okay
so all the lines essentially are the
subsets related to this thing abd okay
so this is a lattice of item sets and
what is the name approach that we talked
about we simply go to every possible
item set in the lattice and then
enumerate its frequency okay
and obviously we said it's very
inefficient and now we want to find an
efficient way to traverse through the
lattice okay like the the nice intuition
which is the dynamic programming comes
from this fact and if you know the
frequency of a and B you can use it to
compute the frequency of a B similarly
if you know the frequency of a B ad and
baby you can compute the support for a
BD and so on right so one simple
mechanism is to go in a level wise
manner bottom-up first I find all the
item sets from the first level then I go
to the second level and compute them and
then third level then fourth level and
so on that's a reasonable strategy right
and the a priori algorithm essentially
follows that level voice search in the
last lattice okay
is it clear so forth
okay now what I'm going to tell you is
the most fundamental property of
frequent items at mining every single
algorithm that is presented for this
problem use this particular property
okay and what is that property it's very
simple
it's basically based on the notion of
frequency okay so let's say a B is
frequent okay
that means a B support is greater than
the threshold it occurs in more than
threshold number of transactions okay if
I tell you that a B is frequent can you
say anything about a and B which are the
subsets of a B can I say they're
frequent or not frequent or can be both
can you say anything about it if a B is
frequent can I say
is frequent and B is frequent absolutely
right because if the frequency or if the
support of a B is greater than the
threshold then obviously support of its
subsets will be greater than threshold
okay it's a very very simple property do
you agree tonight okay
now let's look at the contrapositive of
that statement which is if is not
frequent okay let's say a occurs in only
20% of the transactions whereas our
threshold is 30% okay can I say anything
about a B if a is not frequent then
obviously a B is also not frequent
because frequency of a B is less than or
equal to frequency of 8 right and that
gives you a very great power right since
you are going in a level wise manner as
soon as you find something is not
frequent so what is the meaning of a is
not frequent then you know that a B is
not frequent a C is not frequent a D is
not frequent ABC is not frequent a BD is
not frequent a CD is not required so you
can prune out all this part of the
search space you don't need to enumerate
and evaluate these item sets right so
you are able to skip a lot of these item
sets for evaluation because you know
that they will not be frequent ok this
is called as pruning the search space I
have exponential search space which is
this entire lattice as we said it's an
exponential size 2 to the N and we are
pruning out some parts of the search
space based on that property if a sub if
an item set is not frequent then none of
its super sets can become frequent okay
so just by knowing a is not frequent I
can cut out lot of these guys I can cut
that I can cut that I can cut that I can
cut this and cut this you can add that
okay
now suddenly you just have four more
things to evaluate instead of seven and
ten right from ten you quickly went to
seven now if you know that let's say de
is also not frequent okay then you can
cut down even further this is not
frequent this is not frequent this is
not frequent so you just have bc to
evaluate okay so you see how quickly we
are able to remove some of the things
from the search space okay so that is
called as a priori property are also
called as downward closure property okay
downward closure because essentially if
a be is frequent then all its downward
things are also frequent so it kind of
encloses those things so that's why it's
called downward closure but the property
is essentially any subsets of a frequent
item set are also frequent in other
words the contrapositive of that
statement is if an item set is not
frequent then none of its super sets can
be frequent
okay that's the contrapositive which is
the most important property okay do you
understand that
okay now if you know that then that's it
a priori is done actually so what you
need to do essentially that I eat rative
algorithm so you find all the one
frequent itemsets that's very easy right
I tell you that my Walmart data set
contains 10,000 items I simply count in
one scan I can count all the frequencies
of all the 10,000 items right I can
simply create a hash map of 10,000 items
or array of 10,000 items and then I keep
scanning every transaction and then I
increment the corresponding comes and at
the end of my scan I will have the
frequencies or the support of all the
10,000 items that I have okay and then I
can quickly from once I compute that I
can find which of them are frequent and
which of them are not frequent right
because I simply count which ones have
frequency greater than the threshold in
in our example 30% and which of them
have less than 30% okay so let's say I
have six items okay let's for the
purpose of an example so I have a comma
B comma C comma D comma F these are six
items okay now what is the first step
I'm saying find the support of all six
items okay and the second thing that is
this requires one scan of the data
entire data okay
then I can simply find the list of
frequent items okay in other words
frequent one item sells are you with me
so far
okay now let's say in this example I
have a c e and f are frequent okay and
then b d and what's other one no that's
it
or not frequent okay
now given this which of the two item
sets so i am done with the first level
that contains one item sets now i want
to go to the second level okay but we
can use our downward closure property
and remove some of the things from
second level right so if i know that
these four are frequent and these two
are not frequent can you list the list
of item sets to item sets that we need
to evaluate right like for example
should I evaluate a B no because b is
not frequent can I avoid CD no can I
evaluate EF yes okay so
you know I mean you understood the
concept now can you list out all the
item sets that need to be evaluated to
item cells
okay subrata your answer
yogi so both of you were absolutely
correct I need to evaluate these deaths
I simply copy pasted a nice answer which
is same as a transfer as well that means
I can I need to only look at my frequent
subsets and then create the candidates
for the next level okay so these are my
candidates now what can I do
once I compute the candidates the next
step is find the frequency of all
candidates and how would I do it I can
do another scan of the data
one more scan and then compute the
frequency of all these six two items
okay and then I can again divide that
into frequent or not frequent so let's
say for the purpose of example a see a E
and C E or frequent but however a F what
are the remaining C F and here are not
frequent okay that I can do after my
second scan now the next step is
obviously I have to compute three
candidates three items at candidates
okay so what would be the candidate
subset I need to evaluate here
let's even make it interesting let's
make this also frequent okay there are
four frequent and - not frequent okay
now how can i or which of the three item
sets that i need to evaluate in my next
level
can I evaluate CEF the answer is no
because EF for example is not frequent
and therefore CF cannot be frequent okay
so see if I want a value it and let's
take a CF do I need to evaluate it a CF
is it what are the subsets involved in
there is CF CF no AC is frequent but AF
is not frequent so even if one of them
is not frequent then I can quickly
remove the entire guy
so since AF is not frequent I will not I
cannot have a CF to be frequent right
that is the power I mean you can quickly
prune out a lot of things so similarly
AE depth
do I need to evaluate that AE gets no
absolutely
how about AC e
this would be yes because it is frequent
I mean it is a candidate because what
are all the subsets in there AC e C and
if you see all those three are frequent
since all of them are frequent it is a
candidate okay so that means I just have
a single candidate that need to be
evaluated so essentially what you can do
is simply by looking at these just like
here right how did you come up with this
list you looked at these and then you
took all combinations you see a AFC a CF
EF right so similarly take this and then
compute all combinations okay so a CAE
what's the combination AC e but also I
have additionally constraint that for AC
e I should also have C so C is there so
AC is a candidate now let's try to
combine AC and C which will come into
the same thing now let's compute a C and
C F that means AC f but it's not
possible because AF is not that so
similarly you compute this and this
which is AC e which you already have a E
and C F that's not a three three
combination because it's a large one
they see if there's four combination so
we'll look at the fourth level and then
compute C E and C f/c EF and again
that's not possible because EF is not
that okay so you see the method right
you can simply look at the frequent
items take all the combinations and then
compute the next level candidates okay
that's it you keep repeating until
you're at the final level are you don't
have anything more to try out like for
example this is my candidate now what is
my next step
fine the frequency or the support of a C
let's say AC e is not frequent
do I need to evaluate any four
candidates for item said candidates no
because there is nothing to expand on
right so that would be my end observer
Oracle's
so after this my output is simply from
the first level we got a CES from the
second level we got these another four
so the total output of the algorithm is
eight eight subsets that are frequent
okay
so you can see right how many
computations you did in the first level
you computed frequency for all of the
six in the second level you computed the
frequency of another six items here 6
plus 6 12 and the in the third level you
computer the one one only one item set
which is a c-13 so out of 24 or 23 not
even 23 so what's a combination total 2
to the 6 minus 1 is equal to how much 63
so out of all the 63 combinations you
simply evaluated only 13 ok you can see
the power of the simple rule
okay that's it that's your a priori
algorithm you simply find all the
frequent one item sets then you list all
the two item sets and so on okay so in
the next slide I have a detailed example
you want to see which is similar to what
we just talked about but with respect to
a particular data set so you can scan
item sets so you get candidates of one
item sets that's where this is called a
c1 and then what
here the minimum support is two out of
four transactions so this guy is one so
you cut it out and then these are my
frequent items and this is not frequent
item and then you combine these frequent
items to compute see two candidates from
the second level okay and then you again
scan the database cut something out and
then you find all the other guys that
are frequent and then you combine these
to make the third item c3 candidate to
three and then you keep progressing okay
any questions on the algorithm
as I said I mean the algorithm is very
simple right once you understood the
concepts of support and confidence
algorithm is fairly straightforward it
is this one okay so in our example the
one that we have here
how many scans on the data that I have
how many scans of the data that we did
overall
how many scans is it one two three four
how many scans do we have so we said for
computing the frequency of individual
item sets we need to scan the data to
compute the frequency right so my
question is how many of those scans that
we did in this particular example
there are three scans of the data no not
seven three because in the first level
we did one scan and then found all the
frequencies of all the one items then we
computed the candidates from level two
then we did one more scan to find
frequent and not frequent of level two
then we computed the three items at
candidates and in order to compute that
frequency we will need our third scan we
compute the frequency of AC E and then
decided it's not frequent and we stopped
so that means we did one two and three
scans right make sense subrata so in
other words we are making a scan for
every level in the dataset right so if
you think about it in there is a lattice
context here we make one scan here one
scan here once can hear one scan here
and so on where until wherever you stop
right so the downside of this algorithm
us no it's very simple and very powerful
the downside of the algorithm is that we
are making repeated scans of the data at
every level ok and you can imagine right
if you have 10,000 items you will go up
to many levels maybe 30 levels 40 levels
if I have terabyte or 10 terabytes of
data I have to scan this data 40 times
that's a lot of time right okay so
that's a negative point of this data
many scans over the data set and further
research is on how can we cut
the number of scans while computing
these items it's okay there are many
many many many algorithms and the most
popular one is called as FP growth which
is extremely efficient I mean it has
some limitations but it lives in general
it's extremely efficient and it makes
only two scans over the data guaranteed
just by scanning the data twice you
compute all the item sets okay and
therefore it is very efficient okay but
I mean we won't go into the details of
that algorithm which is little more
complex but this is algorithm that gets
that cut implemented in mouth okay
any questions does that so that
concludes our algorithm description of
the entire thing
any questions
okay so yes that's what my next step is
showing it in not okay so let a via Slee
in order to do that we need to have a
data set let's look at this particular
data set okay so here I have the first
column as a basket ID something that
transaction ID okay and I have a bunch
of items here conditional lemons
standard coffee frozen chicken wings 98%
fat-free hamburger sugar cookies onions
ham blah blah blah blah many of them
right there a lot of them these are all
the different items and there is a false
that means this transaction did not have
this item and if you look at for example
this is a true right that means the
transaction number cat5 82 has items
sugar cookies and onions and also ice
cream and maybe some other ones you can
see it in the right side of the screen
okay so wherever there is the item that
was bought in that particular
transaction there is a true and
remaining all the things are false okay
so it is like a 0 1 matrix if the item
was bought in that particular
transaction you have a 1 otherwise it is
0 okay so this is the different
representation of the data do you do you
understood understand this
subrata is it clear so it simply says
see 85-82 contains sugar cookies onions
and ice cream okay so that's why we have
true associated with those items and
remaining all items are false okay so
essentially if I look at our example
data this one right I have how many
items here one two three four five and
six clothes okay so there are total of
six items
beef chicken milk cheese boots beef
chicken milk cheese boots and clothes
okay there are total of six items that
means I am representing this as seven by
six matrix seven is a transaction six is
a items okay
clear-rite beef chicken meat cheese
boobs and Loubs okay
now t 1 will be beef chicken and milk
so essentially true so let's put 1 and 0
1 0 episode 1 1 0 0 0 and T 2 will be
what beef and cheese so 1 here 0 0 0
this is 1 0 okay you get the idea
right so this is another representation
of the data and in this case there are
some implementations which take this
representation and there are some
implementations which take this
implement at this representation which
is just simply the list of items okay
and mahute implementation requires this
in this representation okay list of
items as opposed to this boolean
representation okay but since our data
is in this format we have to convert
from here to here and then feed it into
mahute okay
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>