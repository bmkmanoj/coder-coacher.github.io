<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding Lucene Analyzers | Types of Analyzers | Apache Lucene Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Understanding Lucene Analyzers | Types of Analyzers | Apache Lucene Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding Lucene Analyzers | Types of Analyzers | Apache Lucene Tutorial | Edureka</b></h2><h5 class="post__date">2014-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I9bgz8_bM20" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">basically what we're going to cover
today is we will be talking about the
analyzes since it is very important so
let's quickly jump onto the analyzers
part of it so an analyzer is the initial
step in our indexing or the search
process flow so yesterday we had seen
the system flow right once we acquire
the docum once we acquire the content we
build the document what was the next
step which is the first step in the
analyzer when tissue you have any idea
so the first step in the analyzer is the
tokenization process once the analyzer
picks it up so it has to do some
tokenization tokenization is nothing but
splitting the stream of tokens or stream
of words into words or tokens okay so
that is the process called as
tokenization so most of the time of the
tokenization accepts a string reader
which you can pass it to the tokenizer
and the tokenizer tickets takes it from
there and splits the words or the tokens
and gives all the token stream so now it
is a token stream would be the output so
there tokenizer always takes a reader
and emits a token stream as a output so
that the next filters series of filters
based on what you have chosen it will
pick up and it will do some sort of
filtering say for example if we have
chosen lowercase filter so each token
would be converted into lowercase filter
and just in case if we have along with
lowercase filter if we have used any
synonym filter so it will expand the
each token if we have any synonyms to
multiple tokens so those kind of filters
we can use it in conjunction with the
tokenizer so there can be more than one
filter okay then eventually the output
of the token will be given to the index
rider which will take care of writing
all the tokens into the index fine and
we also said it will be four different
languages it's not that lousine can only
handle for English or any particular
language so it can handle all the
internationalized languages or localized
content as well okay so fine so we have
seen some analyzers and some filters so
I will not dig into this slide a lot
rather we will jump into the actual
analyzers itself so let's start with a
very simple analyzer so yes Wendish so
for whenever we wanted to expand our
token stream using synonyms so we will
cover in that slide say for example I
have a synonym for a word XYZ so my
token string will be expanded based on
the mapping what I give it to the filter
okay we will see that example very soon
fine so let's start with the simple
analyzer so the simple analyzer what it
does is it picks token a stream of text
and it splits all the tokens based on
space by default and only emits the
characters data stream so by default it
ignores all numeric data or numeric
characters and also non characters or
special characters so in nutshell it is
a very simple one it just uses two one
tokenizer which is a letter tokenizer
with lowercase filter fact lowercase
filter
that's it so even if you have any ear or
any numeric text all of them will be
ignored
so if you have any use case where you
only have to retain the character stream
and ignore all the numeric data or
numeric string in your index or in your
documents so the simple analyzer is the
best choice for to use in those use
cases okay so you might have noticed
yesterday so the very bare minimum way
of instantiating any analyzer is the
ones which are provided by losing core
and the analyzer component is simple
analyze just say new instance simple
analyzer so the analyzer component or
the interface or the abstract class
remains same for everything and you can
just instantiate the simple analyzer
rest everything remains same as is what
we have seen from the yesterday's class
okay is that clear
when - when do we have to use simple
analyzer yep so the next one what we are
actually that one is not required so the
previous versions of Lucene ApS required
to mention the version so now you can
remove the version as well so I will
just show you so this is not mandatory
you can just ignore it will pick up the
latest version okay
fine so the next analyser we are looking
into is the standard analyzer so this
one is just an extension of the simple
analyzer but it has little bit more
features attached to it so in the
earlier example we have seen the simple
analyzer ignoring some of the numeric
characters and all of that stuff right
so the standard analyzer it it doesn't
bit little bit different from the simple
analyzer first it also uses lower case
filter and also uses a stop filter say
for example by default it has some stop
words like the all the common words in a
the an so all those common words will be
filtered so in the simple analyzer the
stop filter was not used so in the
standard analyzer there will be a stop
filter along with the lower case filter
okay so here in the standard analyzer
even the numeric characters will be
emitted from the tokenizer
okay that's the differences few
differences from standard to simple
analyzer okay fine so this is a again a
variation of the standard analyzer say
for example yesterday we spoke about in
some places you may have a a little bit
more control on the words the standard
analyzer restricts all the standard stop
words but it may not allow you to
override any your own bad words or
anything of that
so in those cases say if you wanted to
restrict a few words getting impure
index database you can create a list of
bad words or list of stop words along
with a standard standard analyzer or the
standard stop words you can create a
file or you can create a list of words
and using the stop analyzer you can stop
all those words getting into the index
so that is the primary usage of the stop
analyzer ok so we'll be seeing all of
these lab and labs and the variations of
it so that we can hear all the bad words
can be given in a string with spaces
actually it is not recommended to use
that in a string rather we should be
using it from a file so that we can
manage all of them in a file by line
separated thing so for the example I
just gave give it like that bad words
that's it ideally a file should be
preferred mode ok so this is a this is
very interesting one so at times say for
example we wanted to search some words
based on the route route understanding
of that particular word say for example
let us take a swimming swimmers example
say your document may contain swimming
or swimmers say what if if someone is
searching with swim and you still wanted
to match those documents and written it
back ok so in those cases you may have
to use something called stemming
similarly for the word like if your
document has cat-like or catty or
fishing fished or Fisher
if someone is searching for say fish or
Fisher or fished or fishing irrespective
of which word they are searching they
have to match this particular document
so if your application or use case has
this kind of requirement so in those
cases you have to use a stemming
analyzer so in the stemming analyzer
also it expands the token vector stream
so that you can search with multiple
terms even though that particular
document so so if I use standard
analyzer and search for sim I will not
get results yes of course you will not
get and will test all this all these
things only if you use the stemming
analyzer you if you use fishing fished
Fisher or fish you will be able to get
those results back okay so basically the
process is defined as to getting the
base word of whatever we are trying to
get a search from the index okay so we
will see at least couple of these
examples for this so that it gets pretty
easier okay
so ideally when we create the analyzer
so there is a algorithm by mr. Porter so
that's in that's the reason it's being
named on his name's Porter stem filter
so there is a very exhaustive list of
words what the porter stem filter covers
so if you pass a string the Portus stem
filter will give you the base word for
that particular string okay so using the
POTUS stem filter we'll be trying out
this example okay
so there there could be another
requirement or in some cases say for
example you may want it to add synonyms
for a list of words okay say if someone
has to search for a word say completed
over ended or finished alternatively so
even though your documents may not
contain all these index terms if you
expand by Peru by providing the synonym
list so you can search with any of these
words so that your users are not blocked
with only the term which the documents
are containing or only the index what
you are looking for right so it is a
very simple example right
someone can type in mobile phone someone
can type in cellphone so if your
documents only has mobile phone if
someone is typing cell phone you should
not stop bringing back those records
right when tsch so those are the kind of
use cases we may have to go ahead and
use synonyms so that we can go ahead and
allow the users to do whatever they are
looking for okay so a very simple code
snippet so I have just taken the same
example what we have provided here so I
have added three synonyms for finished
so for one I have added completed
overall ended so the user can now search
with either finished completed or over
or ended similarly I have added three
more synonyms for work as well so it job
labor or effort so it might be very
tempting to get confused with the base
stammer or the stemming thing but this
is completely different
a synonym and the stemming is different
because a stem derives the base word so
this is a synonym or a alternate word
which you are adding it for
searching for the users okay is it clear
five so I think let's quickly jump onto
the lab sessions so let's start with one
of them so all the lab documents I will
be attaching it in the zip file format
so that you can just extra X tracked all
the content yes all the syllable
synonyms should be given by us either in
a file or in the code so I have used it
in the code just to keep it a bit
simpler so you can provide it in the
file all the synonyms there are multiple
ways of creating this pranams so we will
see how to handle synonyms as well okay
so I am starting the first lap so let me
just go to the document I will be just
testing all the lab instructions as well
so that in case if there are any
mistakes we should be able to correct
that or the course of trying it out okay
so I could see maybe we can start off
with a stranded analyzer as per the
document okay so let's try the standard
analyzer so I will explain the code as
well so here I'm using the jar file
which I'll be providing you all the
source code so if you notice this is the
project which I have done it in my
NetBeans so since this is a maven
project so you guys should not be
worried about whether it is a NetBeans
or eclipse so it would run on any even
on the command line as well so when I
export this you would get something like
a jar file like this so you can use the
jar files to try the exercises and feel
free to go ahead and modify any API is
or anything which is required to be
tested and again redo the exercises on
top of your changes okay so the first
thing is I'm mentioning the classpath of
the jar files since I'm running from the
local folder I can provide the relative
path of my jar files so the first thing
this is the jar file which I have
developed followed that
yeah I don't use NetBeans console
because it means console again you'll
have lot of path issues and all that
dependency issues so I have eliminated
the NetBeans or eclipse equation just to
make sure people can execute all the lab
exercises from the shell or the command
line that way we are not tied up with a
path or ID kind of fitting nothing stops
us from even entering or ramming it from
the Eclipse or NetBeans as well okay
because see the moment we use NetBeans
or Eclipse there will be lot of path
issues where the NetBeans has been
installed how the folder structures are
done so eclipse may have bit different
folder structure NetBeans may have bit
different folder structure windows might
have something different clinics might
be something different so if we follow
this approach everything should run on
any platform or um any environment if we
have Java so that was the idea behind it
make sense linked ish fine so we are
providing our jar file followed by the
Lib folder where we have added our basic
libraries watch what is required or the
jar files ok so the documents is the
input files okay so this would be your
input folder we have some basic data I
don't have a lot of data right now so it
is this only two files we can just look
into it so this is the index file so
maybe I can delete this just to make
sure it recreates again so I am deleting
the index file and if you notice I am
calling my class the first one index
index and the path where I have to index
and where is the input folder so let's
go ahead and see it has created my index
let's check that one yes it has created
so now since I am testing the standard
analyzer let's go and
search the files okay
so let's let me search for ed Eureka
okay so I found it in two files so
that's the reason I I am showing the
file path as well okay so let me check
whether it is true or not in fact yes ed
Eureka is in both the files okay what do
you expect would happen pink - any
guesses would what would happen it won't
written okay I will tell you the reason
why it won't written so it depends on
the query what we are using to search
okay I will come to that topic very soon
okay so for now let us understand the
analyzers we'll get into the querying
aspect into detail fine
so let's see if I have any more examples
which we wanted to just quickly validate
yes numeric let's see numerals so the
standard analyzer has given us the
numeric outputs right so I think we can
try this standard analyzer with
different outputs of special characters
or anything and I wanted you guys to
come up with problems and so that we can
all go ahead and see if there are any
anything which we need to explore
further okay so let's go ahead and try
this one more time okay this time let's
try this with a simple analyzer so again
it has created so let me just go ahead
and do a search okay so let me try the
ED Eureka again back so great it is the
same results but let me try 2013 on the
same documents so straight away it
didn't return me any document because
simple analyzes does not get me any
numerix or anything of that sort so that
was the reason it straight away omitted
my results so let's just quickly see if
I have another test case okay let me
just check it out let's do one more
small example on this and then we can
compare the code differences as well
okay so let's add our more small file
fine three and
okay
so even if you have any numerix this
simple analyzer it is not accepting it
okay so that was the demo kind of thing
so let me just quickly go ahead and show
the actual code differences as well so
there is as I said there is not lot of
differences except for only one
difference everything remains the same
it was only just one small difference
here instead of using standard analyzer
we are using simple analyzer so in the
latest ones you can either remove this
entire thing
it is also valid or you can also say
latest okay so ideally it is now not
required to provide the version so
loosing internally figures that out okay
so let's see what is the difference from
the standard analyzer as I said it is
everything same except for the awkward
in it go
oh I can show here itself so except for
this one everything is more or less same
okay so and also you might have been
very curious to see what was this entire
thing all about why I was not able to
get the partially required token what is
it ABI come on
you
you
so that behavior is because of this
query parser and the query which I'll be
just taking it through very soon okay so
for now we have seen the standard and
the simple analyzer just a very basic
difference the only line of core of the
change in the lines of the code will be
just the construction part of it
otherwise everything remains more or
less the same okay so let's pick up the
next one next analyzers
okay so we we wanted to also do some
stop analyzer right so I think there is
this okay so let's go and try this or
maybe this was right
okay so let's first go ahead and
recreate the index you can see if I am
deleting it every time so that I don't
have to go ahead and do that over and
over again maybe not so let me just yeah
I'm here let me just run the index yes
it has created and now let me just go
ahead and do some search using the same
simple analyzer just to understand the
difference so here okay I am searching
some something like idiot let's see it
returns or not okay yes it did because
I'm using a very simple analyzer it
cannot understand whether he D it is a
stop would or not even if you use a
standard analyzer it will give you back
that document okay
so let's assume I don't want these kind
of things coming in so I wanted to go
ahead and stop all these things getting
into my index so let me just go ahead
and delete it for one more time and
again I will just go and run this time I
am running the m2 lab two index files
which is having a different analyzer
okay
so m2 lab 2 index files if you notice
everything remains more or less as usual
same except that I have a different
analyzer if you notice that should be
the only change Westall should remain
more or less same and if you see I am
using a stop analyzer and I am providing
all the stop words from a file let's see
what my stop words is on the part so I
said idiot something like a bad word and
rascal as my bad words so these words
should not go into my index and maybe I
have some bad words in my file ok see
training horses y2k bad word is the mmm
I asked him okay so these are my bad
words so I don't want some of these
things getting into my index so let me
just go ahead and see if it really went
in I could still search on those or not
so now I am not finding all of these
things so in case of you wanted to
overwrite or add your own bad words list
so that you should control your index
you can create an EM eliezer with a stop
foot and you can do whatever you want at
with that stop words okay
so that was
the stop words so let's see what is the
next analyzer I thought we had synonyms
right yeah it was synonyms okay
it was yeah stemming no worries anything
is fine the order doesn't matter
so if you notice everything will remain
again more or less same okay
so except that I have created my own
stemming analyzer and I am passing my my
class file to the analyzer that's the
only difference otherwise more or less
everything is same here see instead of
passing any simple simple or standard a
analyzer I am passing my own stemming
analyzer so this sorry guys so this
particular lab has got multiple
advantages first thing is we are showing
how to do the stemming and the second
one is we are also demonstrating how to
write our own custom analyzer okay so
yesterday I think you had a question how
do I use all the analyzers together at
one given point of time I said you can
use only one analyzer but you can create
your own custom analyzer and add all the
filters which various other analyzers
composes of right so using this you can
using this kind of method you can go
ahead and create your own analyzer okay
this is the core structure of it so it
should extend analyzer always okay so
once you extend analyzer it asks you to
override one method that's it okay this
is the method you have to
override so it returns a analyzer token
stream component and you should say
override the create compensated okay so
once you once you define this method and
override this one you can create any
number of filters starting with the
first
tokenizer so I said always in an
analyzer the first component is a
tokenizer which takes the token stream
in the reader format and then thereafter
you can have any number of filters okay
so I'm using only one filter here but if
required you can use any other filter as
well porta stem filter so here as I
mentioned for stemming we are using the
porter stemming algorithm to derive the
base words okay so now let's go ahead
and try some of these examples I think
this one is over I believe let's open
the next lab
okay so yes so the reader object is
nothing but the data which you are
trying to pass it to the tokenizer
that's it it could be a string reader it
could be a file object content passed as
a reader so there are multiple ways of
passing that reader object for now for
us this is very simple it's just the
stream of text what needs to be indexed
that's all okay so let me just go ahead
and create a new document okay I hope
it's already created yes this is for
what okay so here the idea behind
creating a document with so many words
is say for example I have either fishing
or fished or Fisher so irrespective of
whether what word it is I should be able
to search with fish okay
similarly swimming swimmers stemmer
stemming stemp so that is the same file
which I have created here and my
objective is irrespective of what what
words I have it in my file I should be
able to search with fish swim and stem
that is the base words for this three
records so that I should be able to
search using that okay so let's go to
lamprey
let me just make sure I have deleted
everything yeah okay so I'm just going
and indexing the documents now so the
documents are indexed now let's go ahead
and do a basic search okay so there we
go so let me just try finished first
okay was it finished there
oh sorry fishing okay
fishing yes I found okay what was the
other one fishing fished
so I found with fishing I found with
fished I found I should be able to find
with Fisher's okay so let me try if does
it return me with fish as well yes it
did if we have used any other analyzer
like standard or simple we would have
not got a match for it okay since we we
were using the porta stem algorithm we
were able to search with the bass word
as well let's try one more example let's
try swimming yes swimmers and I should
be also able to oh sorry swim yes hit
return the document which says it is in
the final three okay so this is the
basic use case when if you have a use
case where you may want it to search
documents based on the base words so you
may have to use a protest M fill it down
okay so for this part so the last
exercise would be the synonyms which
we'll just see over here so again this
is a custom analyzer so you can create
your own custom analyzer by providing
the similar syntax and overriding it so
here if you notice I am just doing the
more or less same thing I am in this
case I am using more than one filter so
I am using a classic tokenizer so always
the first one has to be a tokenizer
after that I am in this case I am using
three filters okay first one is the
standard filter then lower case filter
and the synonym filter okay the standard
filter
filters out all the basic standard
boards which is not required in the
default language which we have chosen
okay and then the lower case filter is
to convert it to lowercase and this one
is to expand the synonyms for every for
every word which we have all right over
here so I have added two synonyms and
three alternate for each synonym okay so
let's just go ahead try this out as well
okay so let's go ahead and check the
documents what I have got so I have
created it one more new document
finished and work okay so what should be
my objective so if you would have
noticed I should be able to search with
any of these alternates
so let
just firstborn try indexing the
documents then let me just go in to
search okay so let me just first try
something say finished yes of course I
found so also I wanted to search the
same document with the completed synonym
as well yes I found what was the the one
over ended okay
yes I found ending similarly I should be
able to search for work I should be able
to get job I should be able to get labor
should be able to get oh sorry I was
supposed to into something else
okay labor yes effort yes okay so
without those synonyms being added into
my expansion list I would have not been
able to get all the alternate ways of
querying the data okay so this is the
basic analyzers which you would come
across at least in most of the cases so
we have covered at least some of the
basics and I would say lot of basics on
the analyzers so is it clear
vintage on the analyzers
okay fine so I think let's move on to
the next topic
okay so the next topic we are going to
cover is completely on the query yeah go
ahead
vintage vintage you said you have some
question right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>