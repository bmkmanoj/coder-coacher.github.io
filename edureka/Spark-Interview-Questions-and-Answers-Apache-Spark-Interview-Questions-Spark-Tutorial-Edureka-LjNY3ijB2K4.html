<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Spark Interview Questions and Answers | Apache Spark Interview Questions | Spark Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Spark Interview Questions and Answers | Apache Spark Interview Questions | Spark Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Spark Interview Questions and Answers | Apache Spark Interview Questions | Spark Tutorial | Edureka</b></h2><h5 class="post__date">2017-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LjNY3ijB2K4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello folks welcome to spawn interview
questions I am Moo Tacoma the session
has been planned collectively to have
commonly asked interview questions
related to the spawn technology and the
general answer and the expectation is
already you are aware of this particular
technology to some extent and in general
the common questions being asked as well
as I will give introduction about the
technology as so if you get this
tranches so the agenda for this
particular session is the basic
questions we are going to cover and the
questions later is the spark or
technologies that's when I say spark or
that's going to be the base and top of
spark core we have four important
components which work that is streaming
graphics ma lib and SQL all this
components have been created to satisfy
a specific requirement a give
introduction about these technologies
and get into the common areas interview
questions and the questions also frames
such a way it covers the spectrum of
doubts as well as the features available
within that specific technology so let's
take the first question and look into
the answer like how commonly this
covered what is Apache spark and spark
it's with Apache foundation now it's an
open source it's a cluster computing
framework for real-time processing so
three main keywords over here up against
market open source project it's used for
cluster computing and for memory
processing along with real-time
processing it's going to support
in-memory computing so a lots of project
which supports cluster computing along
with that spark differentiates itself by
doing the in-memory computing it is very
active community and out of the Hadoop
ecosystem technologies Apache spark is
very active multiple releases we got
last year it's a very active project
among the Apache projects basically it's
a frame book wherein support it memory
computing and cluster computing and you
may feel this specific question how
SPARC is different than MapReduce or how
you can compare it with the MapReduce
MapReduce is the processing methodology
within the hydro Pico system and we can
Hydra keiko system beyond HD of a sort
of distributed file system MapReduce is
going to support distributed computing
and how span
different so how we can compare smart
with the MapReduce in a way this
comparison going to help us to
understand the technology better but
definitely like we cannot compare these
two or two different methodologies by
which it's going to work
spark is very simple to program like
MapReduce there is no abstraction or
this seems like all the implementations
we have to provide an interactivity it
has an interactive mode to work with in
Smok a MapReduce there is no interactive
mode there are some components like
Apache Pig and high which facilitates
has to do the interactive computing or
interactive programming and spa supports
real-time stream processing and to
precisely say within spark the stream
processing is called a near real-time
processing there's nothing in the world
is real-time processing it's near
real-time processing it's going to do
the processing in micro batches I'll
cover in detail when we are moving on to
the streaming concept I am going to do
the batch processing on the historical
data in MapReduce when I say stream
processing I will get the data that
getting processed in real-time and do
the processing and get the result either
store it or publish it to public
community you will be doing it let us
see Weis MapReduce will have very high
latency because it has to read the data
from hard disk at spark it will have
very low latency because it can
reprocess or use the data already cached
in memory but there is a small cash flow
over here in spark first time when the
data gets loaded it has to read it from
the hard disk same as MapReduce so once
it is read it will be dead in the memory
so SPARC is good whenever we need to do
an iterative computing so SPARC whenever
you do I treat a computing again and
again to the processing on the same data
especially in machine learning deep
learning all we will be using the
iterative computing she responds
performs much better you will see the
performance improvement hundred times
faster than MapReduce but if it is one
time processing and the fire-and-forget
that type of processing SPARC literally
it may be the same latency you will be
getting a tank MapReduce maybe like some
improvements because of the building
block or spark that's Nadi
we may get some additional advantage so
that's the key feature or the key
comparison factor of sparkin I produce
Moritz get on to the key features say
and key features of spark it discuss
about the speed and performance it's
going to use the in-memory computing so
speed and performance ways it's going to
much better when we do entity computing
and sub poly got the sense the
programming language to be used with
this part it can be any of these
languages can be Python Java are are
scalar we can do programming with any of
these languages and data formats to give
us a input we can give any data formats
like Jason back with a data format we
can if there is a input and the key
selling point with the spark is its lazy
evaluation the sense it's going to
calculate the entire cycle directed
acyclic graph deg because that is a deg
it's going to calculate what all steps
needs to be executed to achieve the
final result so we need to give all the
steps as well as what final result I
want it's going to calculate the optimal
cycle or optimal calculation what else
just needs to be calculated or what else
needs to be a fidgeted only those steps
it will be able to getting it so
basically it's a lazy musician only if
the results needs to be processed it
will be processing that specific result
and it supports real-time computing it's
through spark streaming that is a
component called spark streaming with
support real-time computing and it gins
with Hadoop ecosystem variable it can
run on top of Hadoop yawn or it can
leverage the HDFS to do the processing
so when it leverages the HDFS the Hadoop
cluster container can be used to do the
distributed computing as well as it can
leverage the resource manager to manage
the resources so spark and gel with HDFS
beautiful as well as it can liberate the
resource manager to share the resources
as well as data locality it can give
rich data locality it can do the
processing near to the data the data is
located within the HDFS it has a fleet
of machine learning algorithms already
implemented by from clustering
classification regression all those
logic already implemented and machine
learning it's achieved using Emily
within spark and there is a component
called graphics with supports in graph
theory so we can solve the problems
using graph theory using the component
graphics within this park so these are
the things we can consider as the key
features of spark so when you discuss
with the installation of the spark you
may come across this yarn what do we gon
do you make the install spark on all
nodes of gum cluster so yarn is nothing
but another resource negotiator that's
the resource managers within the Hadoop
ecosystem so that's going to provide the
disease management platform yarn going
to provide the resist management
platform across all the clusters and a
spark it's going to provide the data
processing so wherever the result is
being used that location response will
be used to do the data processing and of
course yes we need to have sparks
installed on all the nodes if star
clusters are located that's basically we
need those libraries an additional to
the installation of spark in all the
worker nodes we need to increase the RAM
capacity on the worker machines as well
response going to consume huge amount of
memory to do the processing if will not
do the MapReduce way of working
internally it's going to generate the
lag cycle and do the processing on top
of yeah so yawn and the high level it's
like resource manager or like an
operating system for the distributed
computing it's going to coordinate all
the resource management across the fleet
of servers on top of it I can have
multiple components like spark days
giraffe host path especially it's going
to help us to achieve in memory
computing so smart
Theon is nothing but it's a resource
manager to manage the resource of coffee
cluster on top of it we can have spark
and this we need to have fog installed
and all the nodes on where this park yon
cluster is used and also additional to
that we need to have the memory
increased in all the worker nodes so
next question was like this what file
system best path support when I say file
system when we worked in individual
systems we will be having a file system
to work with in that particular
operating system
cigarette cluster on the distributed
architecture we need a fine system with
which where we can store the data in a
disability mechanism a dope comes with
the file system called HDFS
it's called Hadoop distributed file
system where data gets distributed
across multiple systems and it will be
coordinated by two different types of
components called a node and data node
and spark it can use this HDFS directly
so you can have any finds in HDFS and
start using it within the SPARC
ecosystem and it gains another advantage
of data locality when it does the
distributed processing wherever the data
is distributed the processing could be
done locally to that particular machine
where data is located and to start with
as a standalone mode you can use the
local file system as well so this could
be used especially when we are doing the
development or NEP you see you can use
the local file system and Amazon Cloud
provides an affine system quad really
simple storage service we call that a
cs3 it's a block storage service this
can also be leveraged or used within spa
for this storage and lot other file
system also it supports there are some
file systems like Alexa which provides
in memory storage so we can leverage
that particular file system as well so
we have seen all features what all
functionality is available with inspark
we are going to look at the limitations
of using spark of course every component
when it comes with a switch power and
advantage it will have its own
limitations as well
so X question illustrates some
limitations of using spark spark
fertilizers more storage space compared
to I do when it comes to the
installation is going to consume more
space but in the wickeder world that's
not a very huge constraint because
storage cost is not very great or very
high and big data space and ever the
needs to be careful while running the
apps and spark the reason because it
uses in-memory computing of course it
handles the memory very well but if you
try to load a huge amount of data in the
distributed environment and if you're
trying to do join when you try to do
joint within the distributed world the
data are going to get transferred over
the network now
work is really a costly resource so the
plan or design should be such a bait to
reduce or minimize D there are signs all
over the network and however the way
possible that all possible means we
should facilitate distribution of the
data over multiple missions the more we
distribute the more parallelism we can
achieve and more results we can get and
cost efficiency if you try to compare
the cost how much cost involved to do a
particular processing take any unit in
terms of fuzzing on gb of data with say
like five iterative processing if you
compare cost ways in memory computing
always it's faster because memory it's
relatively come costlier than the
storage so that may act like a
bottleneck and we cannot increase the
memory capacity of the mission beyond
some limit so we have to grow
horizontally so when we have the data
distributed in memory across the cluster
of course the network France and all
those motor lines will come into picture
so we have to strike the right balance
which will help us to achieve in memory
computing whatever they memory computing
okay it will help us to achieve and it
consumes huge amount of data processing
compared to huddle and spa it performs
better than use it as a creative
computing because it likes for both
spark and the other technologies it has
to read data for the first time from the
hard disk our from other data source and
spark performance is really better then
it reads the data on to does the
processing when the data is available
indication of course is the dark cycle
it's going to give us a lot of advantage
while doing the processing but the
in-memory computing processing that's
going to give us lots of leverage the
next question lists some use cases with
spark outperforms Hadoop in processing
the first thing is the real time
processing how do cannot handle the real
time processing but spark can handle
real time processing so any data that's
coming in in the land architecture you
will have three layers most of the Big
Data projects will be in the land
architecture you will have speed layer
battery and service layer and the state
layer whenever the data comes in that
needs to be process is stored and
handled
when those type of real-time processing
starts is the best fit of course they
can have a pea coat system we have other
components which does the real-time
processing like storm but when you want
to leverage the machine learning along
with the spark streaming on switch
computation spark would be much better
so that's why I like when you have
architecture like a lambda architecture
you want to have all three layers
spatula speed layer and service layer
spark and gel the speed layer and
service layer far better and it's going
to provide a better performance and
whenever you do the X processing
especially like doing a machine learning
processing we will leverage nitrated
computing and can perform on a times
faster than I do the more the isolated
processing that we do the more data will
be read from the memory and it's going
to get as much faster performance than
additive MapReduce so again remember
whenever you do the processing of only
once say you're going to do the
processing Bundy once read process it
and deliver the result this path may not
be the best fit that can be done with a
MapReduce it search and there is another
compound called akka is a messaging
system or message coordinating system
spotty entirely uses occur for
scheduling or any task that needs to be
assigned by the master to the worker and
the follow-up of that particular trance
by the master basically asynchronous
coordination system and that's achieved
using acha acha programming internally
it's used by this park as such for the
developers we don't need to worry about
a couple of ganging up of course we can
leverage it but Chi is used internally
by the spark for scheduling and
coordination between master and the
worker
and with inspark we have a few major
components let's see what are the
various conferences by this bar the lady
conference of spot ecosystem star comes
with a good engine so that has the core
functionalities of what accepted from by
this Punk or father's Bank order these
are the building blocks of the spark or
engine on top of spark or the basic
functionalities our final interaction
five system coordination all that's done
by the spot put engine on top of spark
good engine VM and number of other
offerings to do machine learning to do
graph computing to do streaming we have
n number of other components so the
major we use the components of these
components like spark sequel stock
streaming Emily graphics and spark are
at the higher level we will see what are
all these components stock sequel is
especially it's designed to do the
processing against a structured data so
we can write cycle pace and we can
handle or we can do the processing so
it's going to give us the interface to
interact with the data especially
structured data and the language that we
can use its most similar to what we use
within the sequel against a 99% agency
and most of the commonly used
functionalities within the sequel have
been implemented within spawn frequent
and sparks swimming is going to support
the stream processing that's the
offering available to handle the same
processing and Emily base the offering
to handle machine learning so the
component name is called Emily and it
has a list of components a list of
machine learning algorithms already
defined we can average and use any of
those machine learning algorithms
graphics again it's a graph processing
offerings within the spunk it's going to
support us to achieve graph computing
against the data that we have like page
rank calculation how many connected
entities how many triangles all those
going to provide us a meaning to that
particular data and spark our is the
component is going to interact or help
us to leverage the language are within
the SPARC environment
our is a statistical programming
language where we can do statistical
computing in the dispatch environment
and we can leverage our language by
using this marker to get that executed
within the sparkle environment addition
to that there are other components as
well like approximate database it's
called a blink DB another things second
beta stage so these are the majorly used
components within spark so next question
how can start be used alongside Hadoop
certainly cease past performance much
better it's not a replacement to huddle
it's going to coexist with the huddle
right Square aging the spark and Hadoop
together it's going to help us to
achieve the best result a spark can do
in memory computing or can handle the
speed layer and atom comes with the
resist manager so we can leverage the
resource manager of huddle to make spark
to work and few processing we don't need
to leverage the in-memory computing for
example one time processing to the
processing and forget I just store it we
can use MapReduce so the processing cost
a computing cost will be much less
compared to spa so we can maximize and
get strike the right balance between the
backs processing and string processing
when we have SPARC along with harden so
it has some detailed questions later to
spark core within spark cool as I
mentioned earlier the core building bulk
of Sparco is our DD resilient
distributed data set it's a virtual it's
not a physical entity it's a logical
entity you will not see this entities
existing the existence of on today will
come into picture when you take some
action
so this on today will be used or
referred to create the dark cycles and
oddities will be optimized to transform
from one form to another form to make a
plan how the data set needs to be
transformed from one structure to
another structure and finally when you
take some against an oddity the
existence of the data structure the
resultant data will come into picture
and that can be stored in any file
system HDFS s3 or any other file system
can be stored and our IDs can exist in a
patient form the sense it can get
distributed across multiple systems and
its fault all of it and it's a fault
tournament if any of the oddity is lost
any partition of the oddity is lost it
can regenerate only that specific
partition it can regenerate so that's a
huge advantage of oddity so it's a mask
like for special advantage of added it's
a fault tolerant where it can regenerate
the last a disease and it can exist in a
distributed fashion and it is immutable
so since once an oddity is defined are
like it it cannot be changed the next
question is how do we create art it is
in spark the two ways we can create the
oddities one is is in the context we can
use any of the collections that are
available within this gala or the Java
and using the paralyzed function we can
create the oddity and it's going to use
the underlying file systems distribution
mechanism if the data is located in
distributed file system like HDFS it
will give raise that and it will make
those entities available in a number of
systems so it's going to leverage and
follow the same distribution an oddity
as well or we can create the anti D by
loading the data from external sources
as well like HP said the HDFS speed may
not consider as an external source it
becomes as a fine system of I do
so when SPARC is working with Huggle
mostly the file system we will be using
will be the HDFS if we can read from it
HP's or even we can do from other
sources like Parkwood file I have three
different sources a row we can tree then
create the entity next question is what
is the executors memory in spark
application so every spark application
will have a fixed shape size and fixed
number of course for the spark executors
executed is nothing but the execution
unit available in every machine and
that's going to facilitate to do the
processing to do the task into both our
machine say respective of whether you
use Y on resource manager or any other
may select resource manager every worker
mission we will have one executor and
within the executor the task will be
handled and the memory to be allocated
for that particularly executor is what
we defined
the hip size and we can define how much
amount of memory should be used for that
particular exhibitor within the worker
machine as well as number of course can
be used within the executor or by the
executor within the SPARC application
and that can be controlled through the
configuration files of SPARC next
question is different partitions in
apache spark so any data a perspective
of whether it is a small data or larger
data we can divide those data sets
across multiple systems the process of
dividing the data into multiple pieces
and making it to store across multiple
systems as a different logical units
it's called partitioning so in simple
terms partitioning is nothing but the
process of dividing the data and storing
in multiple systems is called partitions
and by default the conversion of the
data into rtd will happen in the system
where the partition is existing so the
more the partition the more parallelism
they are going to get at the same time
we have to be careful not to trigger
huge amount of network data transfer
answer every additi can be partitioned
within spa and the battle is the
partition going to help us to achieve
parallelism more the partition that we
have more distributions can be done and
that the key thing about the success of
the spawn program is minimizing the
network traffic while doing the parallel
processing and we're amazing the data
transfer within the systems of SPARC
what operations does already support so
I can operate multiple operations
against na dhih dhih so there are two
type of things we can do we can group it
into two one is transformations in
transformations Oddity will get
transformed from one form to another
form say like filtering grouping on that
is like it's going to get transformed
from one form to another form one small
example I can reduce by key filter all
that will be transformations the
resultant of the transformation will be
another na dhih dhih the same time we
can take some actions against inanity
that's going to give us the final result
I can say count how many records on thee
or store that result into in HDFS they
are our actions so multiple actions can
be taken against the RTD
so the existence of the data will come
into picture
only if I take some action against an
RDD okay next question what do you
understand by transformations in spark
so transformations are nothing but
functions mostly it'll be higher higher
order functions within scaler we have
something like a higher-order functions
it should be applied against that RDD
mostly against the list of elements that
we have within the RDD that function
will get applied but the existence of
the oddity will come into picture only
if you take some action against it in
this particular example I am reading the
file and having it within the rgd
contour data then I am doing some
transformation using a map so it's going
to apply a function so we did map I have
some function which will fit each record
using tip tab so the split filters tab
will be applied against each record
within the raw data and the resultant
movies data will again be another oddity
but of course this will be a lazy
operation the existence of movies data
will come into picture only if I take
some action against it like count or
print or store only those actions will
generate the data so next question
different functions of spark oops so
that's going to take care of the memory
management and fault tolerance of rdd's
it's going to help us to schedule
distribute the tasks and manage the jobs
running within the cluster and so going
to help us to or store the data in the
storage system as well as read data from
the storage system that's to do the file
system level operations it's going to
help us and spark poor programming can
be done in any of these languages like
Java Scala Python as well as using our
so core is at the horizontal level on
top of spark code we can have a number
of components and there are different
type of entities available one such
special type is pair or DD a next
question what do you understand by a
body it's going to exist in pairs of the
keys and values so I can do some special
functions within the pair are D these
are special transformations like collect
all the values corresponding to the same
key like sorted shuffle what happens
within the shortened shuffle of I do
those type of operations like you want
to consolidate or group all the values
corresponding to the same key or apply
some functions against all the values
corresponding to the same key like I
want to get the sum of the value of all
the keys we can use the pair a DD and
get that match it so it's going to the
data within the oddity going to existing
base keys n is all right question from
Jason what are our vector R duties in
machine learning you will have huge
amount of processing handled by vectors
and matrices and we do lots of
operations vector operations like
effective vector are transforming any
data into a vector form so vectors like
as the normal way it will have a
direction and magnitude so we can do
some operations like some two vectors
what is the difference between the
vector a and B as well as a and C if the
difference between vector a and b is
less compared to a and c we can say the
vector a and b is somewhat similar in
terms of features so the vector Oddity
will be used to represent the vector
directly and that will be used
excellence should be while doing the
machine learning and this thank you and
there's another question what is r GD r
NH so here any data processing any
transformations that we do it maintain
something called a lineage so how data
is getting transformed when the data is
available in a partitioned form in
multiple systems and when we do the
transformation if you rather grow
multiple steps and in the distributed
world it's very common to have failures
of machines or machines going out of the
network and the system or framework as
such it should be in a position to
handle one handles it through RDD
lineage it can restore the last
partition only assume like out of ten
machines data is distributed across five
machines out of that those five machines
one machine is lost
so whatever the latest transformation
the tag that data for
that particular partition the partition
in the last a machine alone can be
regenerated and it knows how to
regenerate that data on how to get that
resultant data using the concept of our
DD lineage so from which data source it
got generated what was its prior step so
the complete lineage will be available
and it's maintained by the sponsor aim
work internally we call that as a DD th
what is font driver to put it simply for
those who are from Hyderabad round yon
bedroom we can compare this to AK master
every application will have a spark
driver that really have a spark context
it's going to correlate the complete
excitation of the job that will coming
to this path master and delivers the RTD
graph that is the lineage to the master
and the coordinator tasks whatever the
task that gets executed in the
distributed environment it can do the
parallel processing do the
transformations and actions against the
RGD it's a single point of contact for
that specific application so spark
driver is a short link and the spawn
context within this part driver is going
to be the coordinator between the master
and the tasks that are running and
sponsor ever can get started in any of
Kiwi cricketer within spa main types of
custom managers in spark so whenever you
have a group of missions you need a
manager to manage the resources there
are different type of cluster manager
already we have seen the yarn yet
another assassin ago she ater which
manages the resources for Hadoop on top
of yarn we can make spark to work
sometimes I may want to have spark alone
in my organization and not along with
the Hadoop RNA the technology then I can
go with the standalone
spawn has built-in cluster manager so
only spawn can get executed multiple
systems but generally if we have a
cluster we will try to leverage various
other computing platforms or computing
frameworks like graph processing giraffe
pays all that we will try to give rich
that case we will go with yarn or some
generalized resource manager like missus
yarn is very specific to Hadoop time it
comes along the peridot maze of this
cluster level resource manager and I
have multiple clusters within
organization
then I can use missus missus is also a
sauce manager it's a separate top-level
project within apache x question what do
you understand by worker note saying a
tester redistribute environment they
will have n number of workers week on
that is a worker node or a slave node
which gives the actual processing going
to get the data do the fasting and get
us the result and marshall was going to
assign what has to be done by one perk
alone and it's going to read the data
available in this specific work under
gently the tasks assigned to the worker
node or the task will be assigned to the
worker node where data is located in
bigger space especially hadoop always it
will try to achieve the data locality
that's what we counted the resource
availability as well as the availability
of the results in terms of CPU memory as
well will be considered as we might have
some data in replicated in three
machines all three machines were busy
doing the work and no CPU or memory
available to start the other task it
will not wait for those missions to
complete the job and get the resource
and do the processing that will start
the processing in some other machine
which is going to be near to that
conditions having the data and r is the
rate of over the network so grants
estate or commissions are nothing that
which nests an actual work and going to
the port to the master in terms of what
is the resource utilization and the
tasks running within the work
commissions will be doing the actual
work and what is as pass victor the
favorites back i was answering a
question like what is a vector
victor is nothing but representing the
data in multi-dimensional form the
vector can be multi-dimensional vector
as well as you i am going to represent a
point in space I need three dimension C
XY and Z so the vector will have three
dimensions if I need to represent a line
in the space then I need two points to
represent the starting point of the line
and the end point of the line then I
need a vector it can hold so it will
have two dimensions the first dimension
will have one point the second dimension
will have another point at the same way
if I have to represent a plane then I
need another dimension to the version
two lines so each line will be
representing two points
see way I can represent any data using a
vector form as we might have huge number
of feedback or ratings of products
across an organization let's take a
simple example Amazon Amazon hang
millions of products not every users not
even a single user would have used
millions of all the products again
Amazon so only hardly we would have used
like a point one percent or like even
less than that maybe like few hundred
products we would have you generated the
products within Amazon for the complete
lifetime if I have to represent all
ratings of the products with a vector
and say the first position of the rating
is going to refer to the product with ID
one second position it's going to refer
to the product with ID - so I will have
million badges within that particular
vector out of million values I'll have
only values for 100 products where I
have provided the ratings so it may vary
from number 1 to 5 for all others it
will say 0 sparse pins thinly
distributed so to represent the huge
amount of data with the positional
saying this particular position is
having a zero value we can mention that
with a key and value so what position
having what value rather than storing
all 0 seconds to only non zeros the
position of it and that the
corresponding value that means all other
is going to be a zero value so we can
mention this particular sparse vector
mentioning it to represent the non zero
entities so to store only the non 0
entities this honest vector will be used
so that we don't need to waste or
additional space while showing this pass
vector let's discuss some questions on
spark streaming how is streaming
implemented in spark explained with
examples
stock streaming is used to file
processing real-time streaming data to
precisely say it's a micro batch
processing so data will be collected
between every small interval say maybe
of like 0.5 seconds or every seconds and
it will get processed so internally it's
going to create micro batches the
created out of that micro batch pecan
that is a D stream this thing is like a
oddity
so I can do transformations and actions
whatever that I do with a DD I can do it
with this remaster and spark streaming
can read data from flume HDFS or other
streaming sources acid and store the
data in the dashboard or in any other
database and it provides a very high
throughput as it can be processed with a
number of different systems in a
distributed fashion again streaming this
thing will be partitioned internally and
it has a built in fusion of four
tournaments even if any data is lost any
transformed or DD is lost
it can regenerate those oddities from
the existing or from the source data so
this thing is going to be the building
job of streaming and it has the fault
tolerance mechanism what we have within
the RTD so this thing on specialized on
DD socialize the form of a DD
specifically to use it within this box
streaming expression what is the
significance of sliding window operation
that's a very interesting one in the
streaming data whenever we do the
computing the data density are the
business implications of that specific
data may oscillate a lot for example
within Twitter we used to say the
trending tweet hashtag just because that
hashtag is very popular maybe someone
might have hacked into the system and
they used a number of feeds maybe for
that particular ask it might have
appeared millions of times just because
it appeared millions of times for that
specific a mini duration or like say 2 3
minute duration it should not get into
the trending tank or trending hashtag
for that particular day or for that
particular month so what people do we
will try to do an average so like a
window this current time frame and t
minus 1 t minus 2 all the data they will
consider and we will try to find the
average or some so the complete business
logic will be applied against that
particular window so any drastic changes
on to presently say the spike or dip in
a drastic spike or elastic dip in the
pattern of the data will be normalized
so that's the
because significance on fusing the
sliding window operation with inspark
streaming and spawn can handle this
sliding window automatically it can
store the prayer data the t minus 1 t
minus 2 and how big the window needs to
be maintained all that can be and easily
within the program hundreds at the
abstract level x question is what is the
string the x function is discretized the
screen so that's the abstract form or
the virtual form of representation of
the data for the smart streaming the
same way how are they getting
transformed from one form to another
form we will have series of oddities all
put together called as a D string so
this frame is nothing but it's another
representation of our DD Arlette group
of oddities because there is a stream
and I can apply the streaming functions
or any of the functions transformations
are actions available within this
streaming against this D string so
within that particular micro batch so I
will define what interval the data
should be collected on should be
processed it call there is a micro batch
it could be every 1 second or every 100
milliseconds or every 5 seconds I can
define that each particular period so
all the data received in that particular
duration will be considered as a piece
of data and that will be count as a D
string as cushion X time cashing in
spark streaming of course yes for
internal users in memory computing so
any data when it is doing the computing
that's killing geography it will be data
in memory but further if you do more and
more processing with other jobs when
there is a need for more memory the
least-used rdd's will be clear now from
the memory or the least used data
available out of actions from the IGT
will be cleared off from the memory
sometimes I may leave that later forever
in memory very simple example like
dictionary I want the dictionary words
should be always available in memory
because I may do a spell check against
the treat commands or feedback comments
a number of times so what I can do I can
say cache those any data that comes in
we can cache it or persist it in memory
so even when there is a need for memory
by other applications this
click data will not be as a mode and
especially that will be used to do the
further processing and the caching also
can be defined whether it should be in
whereof only by in memory and hard disks
that also we can define it let's discuss
some questions on SPARC graphics so next
question is is there an APA for
implementing crafts in spa then graph
theory
everything will be represented as a a
graph
when I say graph it will have nodes and
edges so all will be represented using
the arteries so it's going to extend the
RTD and there is a component called
graphics and it exposes the
functionalities to represent the graph
we can have a edge rgd vertex RDD by
creating the edges and vertex I can
create a graph and this graph can exist
in a distributed environment so same way
we will be in a position to do the panel
processing as well so graphics is just a
form of representing the data there are
graphs with edges and vetrix's and of
course is it provides the APA to
implement or create their graph do the
processing on the graph the apsl point
it what is page lines in graphics so we
didn't can have sex once the graph is
created we can add create the page rank
for a particular node so it's very
similar to how we have the page rank for
the websites within Google the higher
the page rank that means it's more
really important within that particular
staff it's going to show the importance
of that particular node or age within
the particular graph place a graph it's
a connected set of data all Vida will be
connected using the property and how
much important that property makes we
will have a value associated to it
so within PageRank we can calculate like
a static page length it will run a
number of iterations or there is another
page rank or dynamic page rank that will
get executed till we reach a particular
solution level and this agitation level
can be defined with multiple fight
videos and
a pas we recorded at the graph
operations and we directly indicated
against those graph and they are not
available as a PA within the graphics
what is lineage graph so the oddities
very similar to the graphics how the
graph representation every oddity
internally it will have the relation
saying how that particular oddity got
created and from base how that got
transformed oddities how they got
transformed so the complete lineage or
the complete history or the complete
part will be recorded within the lineage
that will be used in case if any
particular partition of the tardy is
lost it can be regenerated even is the
complete oddities last weekend region of
it so it will have the complete
information on water partitions there it
is existing water transformations it had
undergone what is the resultant value if
anything is lost in the middle it knows
where to recalculate from and what are
essential things needs to be calculated
it's going to save us a lot of time and
if that oddity is never being used it
will never get to the channel creature
so very calculation also triggers based
on the action only on need the base is
it will recalculate that's so it's going
to use the memories optimally then
Apache spawns provide checkpointing
especially locate the example life of
streaming and if any data is lost within
the predictor sliding window we cannot
get back the data or like the data
between the lost husband I'm making a
window off say 24 hours to do some
average I am making a sliding window off
24 hours every 24 hours it will keep on
getting tried and if you lose any system
assume there is a complete failure of
the cluster I may lose the data because
it's all available in the memory so how
do we calculate it if the data system is
lost
it follows something called a
checkpointing so we can checkpoint the
data and directly is provided by the
spark a PA we have to just provide the
location where it should get check
pointed and you can read that particular
data and back well you start the system
again whatever the state it was in we
can regenerate that particular data
so yes to answer the pushing straight
and practice path points check pointing
and it'll help us to regenerate the
state what it was really it's more to
the next conference Park Emily how is
mission learning implemented in sparks
the machine learning again it's a very
huge ocean by itself and it's not a
technology specific to spark which is
only is a common data science it's a
subset of data science would where we
have different type of algorithms
different categories of algorithm like
clustering reduction dimensionality
reduction all that we have and all these
algorithms are most of the algorithms
have been implemented in spark and spark
is the preferred framework called the
preferred application component to do
the machine learning algorithm nowadays
machine learning processing the reason
because most of the machine learning
algorithms needs to be executed
absitively and number of times till we
get the optimal result may be like say
20 generations or 50 I creation so till
we get that specific accuracy we will
keep on running the processing again and
again and SPARC is very good fit
whenever you want to do the processing
again and again because the data will be
available in memory I can read it faster
store the data back into the memory
again really trust and all these machine
learning algorithms have been provided
within the spark a separate component
called ml lid and within ml lid we have
other components like feature efficient
to extract the features we may be
wondering how we can process the images
the cool thing about processing HR audio
or video is about extracting the
features and comparing the future how
much they are related so
your vectors matrices all that will come
into picture and we can have pipeline of
processing as well to the processing one
then take the result and do the
processing to enhance the assistance
algorithm as the result of it we
generated a process result it can be
persisted and as a loaded back into the
system to continue the processing from
that particular point onwards next
question what are categories of machine
loan machine learning as such different
categories available supervised
unsupervised and reinforced by learning
supervised unsupervised it's very
popular with we will know so I will give
an example I will know well in advance
what category that belongs to as we
might want to do our character
recognition while training the data I
can give the information saying this
particular image belongs to this
particular category character or this
particular number and I can train
sometimes I will not know well in
advance assume like I may have different
type of images like it may have car like
cat dog all that I want to know how many
category available I will not know well
in advance so I want to group it
how many category available and then
I'll realize saying okay there's all
this belongs to a particular category
I'll identify the pattern within the
category and I will give a category name
say like all these images belongs to
both cattingly on look like a boot so
leaving it to the system by providing
this value or not let's say the cat a
different type of machine learning comes
into picture
such machine learning is not specific to
sparks it's going to help us to achieve
to run this machine learning algorithms
what has mark ml lead codes ml Lib is
nothing but machine learning library or
machine learning offering within this
mark and as a number of algorithms
implemented and it provides a very good
feature to persist the result gender in
machine learning you will generate a
model the pattern of the data we call
that is a model the model will be
persisted either in different forms like
path grid Avro different forms it can be
stored or possess it and has
methodologies to extract the features
from a set of data
I may have million images I want to
extract the common features available
between those millions of he pages and
other utilities available to process to
define our like to define the seed the
randomizing it so different utilities
available as well as pipelines that's
very specific to spark where I can
channels are arranged the sequence of
steps to be undergone by the machine
learning so machine learning one
algorithm first and then the result of
it will be fed into machine learning
algorithm to like that we can have a
sequence of execution and that will be
defined using the pipelines there's
already inbuilt features of spark Emily
what are some popular algorithms and
utilities in spark Emily so these are
some popular algorithms like regression
classification basic statistics
recommendation systems accomplish this
term is like well implemented all we
have to provide is give the data if we
give the ratings and products within an
organization if you have the complete
dump it can tell you the recommendation
system in no time and if you give any
user it can give a recommendation these
are the products the user may like and
those products can be displayed in the
search result
system purely works on the basis of the
feedback that we are providing for the
earlier products that we had what
clustering dimensionality reduction
whenever we do an something that the
huge amount of data it's very very
compute intensive and we may have to
reduce the dimensions especially the
Mexica dimensions within the Emily
without losing the features whatever the
features are available without losing it
we should reduce their dimensionality
and there are some algorithms available
to do that dimensionality reduction and
feature extraction so what are all the
common features or features are
available within that particular image
and I can compare what are all the
common across common features available
within those images that's what we will
group those images so get me whether
this particular image the person looking
like this image available in the
database or not for example assume the
organization or the police department
crime Department maintaining a list of
persons committed crime and if they get
a new photo when they do a search they
may not have the exact photo bit-by-bit
the photo might have been taken with a
different background different
Lighting's
different locations different time so
however present the data will be
different odd bits and bytes will be
different but look wise yes they are
going to be seeing so I'm going to
search the photo looking similar to this
particular photograph say input I
provide to achieve that we will be
extracting the features in each of those
photos we will extract the features and
we will try to match the feature rather
than the bits and bytes an optimization
as with in terms of processing or doing
the piping there are a number of
algorithms to do the optimization let's
move on to spawn sequence if they are a
module to implement sequence mark how
does it work so directly not the
sequence may be very similar to high
whatever the structure data that they
have we can arrange the rate on or
extract a meaning out of the data using
sequel and it exposes the API and we can
use those API to read the data or create
data frames and spank sequel has four
major categories data source data frame
data frame
is like the representation of X&amp;amp;Y data
are like Excel data by dimensional
structure data and abstract form on top
of data frame I can do the query and
internally it has interpreter and
optimizes any query I fire that will get
interpreted or optimized and get
executed using these equal services and
get the data from the data frame or it
can read the data from the data source
and do the processing what is a backward
file it's a format of the firebase the
data in some structured form especially
the result of the sparks equal can be
stored or returned in some persistence
and the packet again it is a open source
from Apache its data serialization
technique where we can serialize the
data using the packet form and to
precisely see it's a columnar storage
it's going to consume less space use the
keys and values and store the data and
all split helps cheaper access a
specific data from that packwood form
using the query the packet it's another
open source format data serialization
form to store the data or possess the
data as well as to retrieve the data
list the functions of sparks equal you
can be used to load the varieties of
structured data of course yes Oracle can
work only if it is structured data it
can be used to load varieties of
structured data and you can use a sequel
like statements to query against the
program and it can be used with external
tools to connect to this punk-ass one it
gives very good the integration with the
sequel and using Python Java scalar cool
we can create an RTD from the structured
data available directly using this box
equal I can generate the RDD so it's
going to facilitate the people from
database background to make the program
faster and quicker next question is what
do you understand by lazy evaluation so
whenever you do any operation within the
spot world it will not do the processing
immediately it look for the final result
that we are asking for it if it doesn't
ask for the final result it doesn't need
to do the processing so based on the
final action till we do the action there
will not be any transformations
will not be any actual processing
happening you just understand what our
transformations it has to do finally if
you ask for the action then in optimize
the V it's going to complete the data
processing and get us the final result
so to answer straight lazy evaluation is
doing the processing only on need of the
resultant data the data is not record
it's not going to do the processing can
you use for to access and analyze data
stored in Cassandra data piece yes it is
possible
cannot only Cassandra any of the new
sequel database it can very well do the
processing and the Cassandra also works
in a distributed architecture it's a no
sequel database so it can leverage the
data locality the query can be executed
locally where the Cassandra nodes are
available this is going to make the
query execution faster and reduce the
network load and spark executives it
will try to get started or responsive
executives in the machine wave the
Cassandra nodes are available or data is
available going to do the processing
locally so it's going to liberate the
data locality next question how can you
minimize data transfers when working
with SPARC if you ask the core design
the success of the path program depends
on how much you are reducing the network
transfer the network transfer is very
constant operation and you cannot
paralyze it gives multiple these are
especially two ways to avoid this one is
called broadcast variable and at
operators not considerable it will help
us to transfer any static data or any
information keep on publishing it to
multiple systems so I will say if any
data to be transferred to multiple
executors to be used in common I can
broadcast it I might want to consolidate
the values happening in multiple workers
in a single centralized location I can
use accumulator
so this will help us to achieve the data
consolidation or data distribution in
the distributed world the APL evans or
AG abstract level where we don't need to
do the heavy lifting that's taken care
by the spark for us what our broadcast
variables just now as we discussed the
value the common value that kidney
I may want that to be available in
multiple web visitors multiple work a
simple example you want to do a spell
check on the treat comments the
dictionary which has the right list of
words I have a complete list
I want that particularly sorry to be
available in hav Yuto so that's the task
when that's running locally in those
executives can refer to that particular
map task and get the processing done by
avoiding the network data transfer so
the process of distributing the data for
the spa context to the executors they
the tasks going to run is actually using
broadcast variables and so Billiton
breaking this part APA using this pass
KP we can create the brav cost variable
and the process of distributing this
data available in all its visitors is
taken care by the spock framework
explain accumulators in SPARC the
similar way how we have our broadcast
variables we have accumulators as well
simple example you want to count how
many error records are available in the
distributed environment as well data is
distributed across multiple systems
multiple executions each executor will
do the processing count the records
anatomically I may want the total count
so what I will do I will ask to maintain
an accumulator of course it will be
maintained in this part context in the
driver program let the driver program
going to be one per application you can
keep on getting accumulated and whenever
I want I can read those values and take
any appropriate action so it's like more
or less accumulative and profitable oops
opposite each other but the purpose is
totally different why is there a need
for broadcast variables when working
with Apache spark
read only variable and it will be cached
in memory in a distributed fashion and
it eliminates the work off moving the
data from a centralized location that is
a spawn driver are from but is a program
to all the executors within the cluster
where the tasks going to get executed we
don't need to worry about where the
tasks will get executed within the
cluster so when compared with the
accumulators broadcast variables it's
going to have a read-only operation the
executives can not change the value it
can only read those values it cannot
update so mostly it will be used like a
case that we have for the RTD next
question how can you trigger
automatically naps in spa to handle
accumulated metadata so there is a
parameter that we can set TTL that will
get triggered along with the running
jobs and enter very ok is going to write
the data result into the disk are
cleaned and necessary data or clean the
rdd's that's not being used the least
used oddity to get cleaned and akita
metadata as well as the memory ok what
are the various levels of persistence in
apache spark when you say direct should
be stored in memory it can be different
no you can be position so it can be in
memory only or memory and disk or disk
only and when it is getting stored we
can ask it to store it necessarily to
form so the reason why we may store all
processes I want this particular entity
this form of already little - for
reusing so I can read it back maybe I
may not need it very immediately so I
don't want that to keep on keeping my
memory I'll write it to the UM disk and
I will read it back whenever that is the
need and read it back the next question
what do you understand by schema oddity
so schema oddity will be used is to
sleep within these Punk's equal so the
oddity will have the meta information
built into it it will have the schema
also very similar to what we have the
database schema the structure of the
particular data and when I have the
structure it will be easy for me to
handle the data so data and the
structure will be existing together and
this schema entity now it's called as a
data frame but in small
and data frame term is very popular in
languages like ours as other languages
it's very popular so it's going to have
the data and the meta information about
that data saying what column what
structure it is it explained nestled
area where you will be using spock
streaming as if you want to do a
sentiment analysis of tweeters so there
will be streamed so we will use flume
sort of a tool to harvest the
information from theta and fit it in to
spark streaming it will extract or
identify the sentiment of each and every
tweet and market whether it is positive
or negative and accordingly the data
will be the structured data the tweet ID
whether it is positive or negative
maybe percentage of positive and
percentage of negative sentiment so it
in some structured form then it can
leverage this box equal and do grouping
or filtering based on the sentiment and
maybe I can use a machine learning
algorithm what drives that particular
tweet to be in the negative side is
there any similarity between all those
negatives and similarity treats may be
specific to a product or specific times
by when the tree was treated or from a
specific region the treat was treated so
those analysis could be done by
leveraging the ML label spark
so Emily streaming core on going to work
together all these are like different
offerings available to solve different
problems so with this we are coming to
end of this interview questions
discussion of spark I hope you all
enjoyed I hope it was constructive and
useful one the more information about
advisors are available in this website
it is a counter-coup on the best and
keep visiting the site for blogs and
latest updates thank you
I hope you enjoyed listening to this
video please be kind enough to like it
and you can comment any of your doubts
and queries and we will reply to them at
the earliest do look out for more videos
not playlists and subscribe to our at
Eureka channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>