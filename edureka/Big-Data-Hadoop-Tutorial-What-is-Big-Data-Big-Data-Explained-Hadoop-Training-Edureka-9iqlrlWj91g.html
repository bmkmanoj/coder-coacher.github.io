<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Hadoop Tutorial | What is Big Data | Big Data Explained | Hadoop Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Big Data Hadoop Tutorial | What is Big Data | Big Data Explained | Hadoop Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Hadoop Tutorial | What is Big Data | Big Data Explained | Hadoop Training | Edureka</b></h2><h5 class="post__date">2017-05-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9iqlrlWj91g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey everyone this is Rashmi from Eddie
Rica and in today's tutorial we're going
to learn about Big Data Hadoop
thank you all the attendees for joining
in today's session if you all can hear
me properly drop me a confirmation on
the chat window so that I could get
started alright so everyone is saying
that they can hear me properly so let us
take a look at the agenda so at first
we're going to start this tutorial by
looking at the Big Data Statistics we're
going to learn how big actually is Big
Data
and after that we'll learn what are the
reasons behind the growth of big data
for which we'll take a look at the big
data growth drivers that will understand
Big Data in detail and we'll also
understand how we can classify data as
big data by looking at the five es then
we'll see the Big Data domains the
domains where we can find or we can make
use of big data or from where the big
data is getting generated and then since
I'm talking about the problem of Big
Data I'll also tell you about the
solution which is nothing but Hadoop and
in Hadoop we'll study HDFS which is
Hadoop distributed file system and
MapReduce which is the programming unit
for Hadoop then we'll see how Hadoop
actually solves the big data problem and
in the end we'll take a look at the
Hadoop ecosystem we will study about the
different tools so any doubts in the
agenda all right so let us get started
with our first topic which is nothing
but big data statistics all right so we
have collected the statistics from the
white paper which is published every
year by Cisco and I know Cisco is one of
the biggest networking companies in the
world so if you see here cisco has
forecasted that in 2020 we'll be dealing
with 30.6 exabytes of data and that is a
lot of data because one exabyte is 10
raised to the power 18 so you can
imagine how much data we're going to
deal with in 2020 now this is just a
forecast
it might even surpass it and if you see
the statistics carefully you can see
that in 2015 we are only dealing with
3.7 X bytes and in just five years it is
going to rise to thirty point six which
is an exponential rise so this is the
thing we should worry about
cisco has research more and they found
out the major reasons behind the growth
of this data so the first reason is
adapting to smarter mobile devices all
of you use a smart mobile phone right
now use different apps you use social
media apps like Facebook Instagram and
then you also buy stuff and then you
also have applications for online
shopping like Amazon eBay so you're
generating a lot of data from your
phones also which early use to do only
in your laptops but now you can do it on
your phone so you're generating a lot of
data because of the smarter mobile
phones that everyone has got and then
the next reason is defining cell network
advances so the cellular network is also
advanced and that's why you use it
extensively it's very easily accessible
to us so that people have been using it
extensively in order to make use of
their apps and if you remember nowadays
every other app that you use needs an
internet connection and that's why the
cellular network is also advanced and
made it much more accessible to the
general public and that's why people are
using more and generating a lot of data
and the next is a reviewing period
pricing now the network companies have
also introduced tiered pricing shared
plans for everybody if you see in India
we have Network called geo and they are
giving free internet to everyone so you
can imagine how many people adopted that
because who doesn't want free internet
come on so that's what people are using
it extensively they're using more apps
that use Internet they're making a lot
of use out of the free internet so
that's why also data has been growing a
lot so now I'm just talking about that
data has been growing but let us find
out the exact reasons because of which
data is growing day by day and it is
growing out at such a large scale let me
tell you guys that nowadays almost
everyone owns a cell phone so if you see
the statistics you can see that by 2020
there will be 10 billion mobile devices
and in US over 70% people own the smart
phone and out of which 50% are the
people who immediately pick up and begin
using their phones after they open their
eyes in the morning and the majority of
growth in the
Media is being generated by the use of
mobile devices because you use your
mobile for everything right now it has
become an integral part of your life
your mobile your internet connection
your apps if you want to eat food you
order food from your phone if you want
to go out watch a movie you book your
tickets using your phone if you want to
go out somewhere you booked a cab with
your phone so it has pickup an integral
part so everyone is using it and that's
why data has been generating so much and
don't get me started on even social
media you know how much people love
social media everyone is sharing
everyone has got an account on Facebook
and Instagram and Twitter and we love to
spend time on social media so if you see
the statistics here you'll be amazed to
know that just in 60 seconds there are 4
million likes on Facebook and similarly
on Twitter in just a minute there are
three hundred forty-seven thousand
tweets on reddit the user cast votes go
up to 18 thousand on Instagram the likes
go almost to 2 million and in YouTube
and in YouTube every minute the user
uploads almost 300 hours of new video
now this is happening just in one minute
can you imagine how much data we've been
generating since the past years and how
much we'll be generating in the coming
years so all this data are getting
accumulated and it is growing huge and
huge day by day and that is the reason
the big data has been growing so much
now I only talked about things that we
had earlier and now we are making a lot
of use out of it but there are new
devices coming out right now we have got
IOT which is nothing but Internet of
Things we're not just talking about
smart mobile phones we are talking about
smart everything we have got smart aces
we've got self-driven cars we have got
smart watches and everything and all of
these things are interconnected among
each other so they will be generating
data in order to communicate because
they are connected to a network also so
self-driven cars they are generating a
lot of data because they study their
entire and
around them when they're driving they
will detect an obstacle the sign of the
distance and then react accordingly
similar with the smart AC that you're
using your house it was detected most
year around it study like is it a hot
day or a cold day and as I said the
temperature according to you it also
studies the body temperature of you also
because it has got sensors and
everything and that is one more reason
because of which data has been
generating so much so if you see this
status thing over here you'll see that
by 2020 the 14% of consumers plan and
buying IOT connected clothing so now
even your clothes are going to be
connected to internet so that's crazy
but fascinating and within five years
there will be over fifty billion smart
connected devices in the world fifty
billion guys and 90% of the cars will be
online compared with just two percent in
2012 so obviously I will buy a car that
drives by itself I don't have to worry
about anything I will just sit in the
car and just go
so obviously there's going to be a very
popular device and almost ninety percent
of the cars will be self driven cars and
the global wearable device shipments are
expected to increase by one hundred
seventy three point four million by 2020
so wearable devices something like your
watch so everyone is buying Apple watch
right now and Moto 360 and wearables
like that so this is also the third
major reason because of the growth of
big data so I hope that is understood
the major reasons behind the growth of
big data so can you think of some more
reasons if you come up with more reasons
please let me know on the chat window
and if you have any questions also you
can ask me alright so we'll move ahead
and we'll see what exactly is Big Data
so big data is the term for a collection
of data sets so large and complex that
it becomes difficult to process using
on-hand database management tools or
traditional data processing applications
so it means that big data is nothing but
a problem it is because our traditional
systems cannot process or store this big
data because it is too huge and it is
very complicated to process
and our traditional database systems are
not designed like that because earlier
when this database systems or our
traditional processing systems were
created we never thought that we'll have
to deal with such amount of data so
that's why it is a problem because big
data came in very suddenly it was not
growing linearly or growing slowly time
by time it is growing exponentially you
even saw it on the white paper forecast
by Cisco so that is why our traditional
systems are not ready yet for big data
so this is nothing but a problem
statement and that is why I call Big
Data as a problem statement and since I
told you that Big Data revolves around
the in capabilities of our traditional
system in order to store and process the
data and there are five fees which is
nothing but problems that help us
identify Big Data so this is a question
how can you differentiate data from Big
Data so the answer will be with the five
V's so now let us take a look at what
these five these are so the first fee is
falling which is again nothing but a
problem because Big Data is huge we are
generating data in very large volumes
you saw that we're going to deal with
30.6 exabytes of data but we are not
ready for a data like that we don't have
that much storage space and the
situation is exactly like this trying to
fit an elephant in a refrigerator which
is not possible and then the next V is
variety now volume is one problem we
know already that we are dealing with a
lot of data but then the data is also
not of the same type there are different
types of data and you know that our
traditional systems are only able to
store and process structured data only
that means which has got an organized
data format with a proper data scheme
defined something like which has rows
and columns in tabular form so you can
see tables like that these are
structured data we talk about semi
structured data it means they are
partially organized they don't have a
proper data schema something like text
files like your emails your log files a
JSON files
semi-structured and we talked about
unstructured data
it means completely unorganized data
they do not have any format like your
mp3 files your music files and your
video files your images all these are
unstructured data and let me also tell
you that all of the big data that we are
dealing with almost 90% of the data is
unstructured so this is a very huge
problem for us to process that
unstructured data because we cannot dump
unstructured data in your traditional
RDBMS it is not capable for that so
again the variety is a big problem and
the next V is the velocity now you
already saw that what happens how much
data is generated just in 60 seconds
from just social media now we have only
talked about social media there there
are many domains from where data is
being generated and the rate is so high
I mean 300 hours of new video every
minute can you imagine how high that is
and our traditional database systems are
only capable of receiving data or
storing data which is coming at a very
steady rate we cannot handle this much
speed so this is again a problem and the
fourth V is value so there is a problem
that we have got a lot of data but are
we able to extract value from it a value
meaning information that will be useful
for us so if you have data and you
cannot use that data for anything it
cannot be used for your business
development for making insight so what
can you do with that data it is nothing
but pure garbage so that's why you have
to make sure that your data sets or the
data that you're dealing with has value
so you should go through refining it
which is called data mining which means
C removing the unnecessary rows and
columns which you do not need so this is
also a problem because the data is huge
coming at a very high rate so how will
we able to process this data in order to
find out the value now let's talk about
veracity the veracity talks about the
sparseness of the data and in simple
words veracity says you cannot expect
the data to be always correct or
reliably
today's world you might get a few data
which has missing values you may have to
work with various types of data like
sporadic data or incorrect data or data
which may not always hold true so in
other words veracity means they have to
trust system and make the system to have
an understanding that the data may not
always be correct and up to the standard
so if you see here that certain values
are missing here and certain values
might be wrong because if you see that
other values in this field minimum field
which is like 4.3 2.0 so how can the
minimum be 15,000 so this is again an
example of incorrect data so this is
what veracity is so we've seen the 5 vs
which help us identify the big data
starting from volume which means huge
amount of data variety of data
structured semi-structured and
unstructured velocity of data because
data has been generated at a very high
rate that value it means that your data
is garbage if you don't extract the
correct value from it and veracity which
means inconsistency in your data so now
let us talk about the big data domain so
all these domains that you see we deal
with a lot of data on this particular
domain and this is exactly where the big
data is also used as an opportunity so
all the big data is analyzed and
companies get different insight so let
me just tell you one by one how big data
has been very much beneficial in each of
the domains so let's start by talking
about web at eat daily and retail now
these are nothing but online shopping
sites so this is where the entire
recommendation system works so let's say
when you go to online shopping website
like Amazon or flip card you will see
that there are certain recommendations
popping at the side of your screens so
it is because they already know your
browsing history and they have stored it
so if they have Godby browsing history
or the search history of every
particular user so they are using a lot
of data and let me can you how do they
make those recommendations sometimes
they do a customer to customer
recommendation so they find out
different groups of customers together
who have got the similar likes so let's
say that you are a person who buys shoes
and socks
the same time and there's one more
person was bought the same shoes but he
has bought a pair of slippers with it so
now how the recommendation system will
work that sends you to people bought the
same shoes so the first one will get a
recommendation of the slippers so the
other one bought and the second one will
get a recommendation of the socks that
the first one bought so this is how a
customer to customer recommendation
works but this also works on the types
of things that you search for there is
also something called remarketing that
happens and you can also see that after
buying something from Amazon or any
other particular website you will see
that if you go through social media
websites which pop up ads on the screen
you can see that the same thing will
keep on popping up because now they have
collected your likes and dislikes and
they will keep on recommending to you
until you buy it so this is where big
data is used as an opportunity and
companies like Amazon they are earning a
lot because of this recommendation
system and it is possible only because
of big data so now let us talk about
telecommunication so let's say that I
have got a network company let's say
Airtel so they want to identify the
customer turn that how many people are
leaving their service are switching on
to a new network provider and they want
to find the reasons for it so they will
go through all the customers they will
go through the mails or complains or
they would go through all the queries or
manage that the customer might have sent
them and there could be loads and they
want to identify region by region that
they want to find out whether it's
because of a power outage maybe because
they were not getting Network properly
or is it because of something else or
should they initiate more offers like
the other network providers are doing so
you have to deal with a lot of data you
have to have the details of each of your
user how have they used your network and
what kind of problems in what kind of
situations they were facing problem and
finally find out the reason because of
which your customers are switching on to
a different network provider so again
you're dealing with a huge amount of
data here in Sandy communication
similarly with government now until a
very interesting story how the
government made use of big data
politics so in the reason 2016 US
presidential election Donald Trump made
a huge use of big data so Donald Trump
hire the best analytics team in the
entire country and what they did is that
they collected personal data for every
person in the country like what kind of
newspaper is there subscribe to club
cards their social media tweets and
their status updates on Facebook
something like that and they made an
encourage them to target the top cities
so where they can get the maximum amount
of voters we're going to vote for Trump
and then they targeted messages on to
them on social media platforms like on
Facebook on Twitter and this is how dogs
are persuaded all the top cities all the
voters to vote for him and now you know
the results already here is the
President of the United States of
America so that was a very clever move
so this is how Donald Trump used big
data analytics to become the president
so again now coming to health care so
big data analysis can also be useful
here this will help us to identify in
what kind of regions around the world
what kind of diseases people are prone
to and similarly in finance and banking
so big data analytics makes it easier
for us for fraud detection so if some
kind of bank wants to understand the
person or what to identify a person that
if we are giving a credit card to will
be paid back to us so they will go on
and search the different transactional
records of the particular person and
they will analyze it so again big data
is useful in this case also so these are
the big data domains where there is a
lot of data to deal with and also a lot
of insights to make out of it so I hope
that if all understood this this is
fairly simple so we'll move on and now
we'll see the solution to Big Data so we
have so we have told about the problems
already so now it's time that we
understand how this problem was solved
and it was solved by Hadoop
so now let us take a look what is Hadoop
so Hadoop is nothing but this is a
framework and this allows us to store
and process large data sets in a
parallel and distributed manner since
you already know that the major problem
with big data was storing it and the
storage problem was solved by
HDFS so HDFS provides you with a
distributed way of storing data so we'll
see that in detail later on this
tutorial and the next problem was to
process this big data and MapReduce has
solved this problem so how does
MapReduce solve this problem that we
just introduced a parallel processing
system across a distributed file system
so we'll take a deeper look at both of
these together so for now let me
summarize this HDFS is the storage unit
of Hadoop which solves the storage
problem of Big Data and MapReduce is the
programming unit of Hadoop which solves
the processing problem of Big Data so
let us understand what is a distributed
file system so before learning what is a
Hadoop distributed file system let us
see what is a distributed file system in
general so distributed file system means
managing your data across a cluster of
multiple machines so let us understand
this with an example so in a de Rica
everything is managed in different
servers so we have got a dedicated
server for accounts for Finance for
customers and for reports so a
distributed file system what I can do is
that I can interconnect these machines
all together and store each file across
all these four machines so this will
give me an abstraction of a single
high-end server so multiple users can
access this cluster in order to store
huge amount of data since you're dealing
with big data now storing huge files
will be fairly easy with the distributed
file system even though it's particular
file does not fit in one server it will
be distributed across a cluster of
different servers which are
interconnected and addressed so whenever
you dump a file onto a distributed file
system it might get distributed it might
get divided into smaller parts across
different machines or it might also get
stored in a single machine but when you
try to access it you will be able to
access all of these four servers
together this gives you the abstraction
and this is very in handy when it comes
to dealing with big data so I hope that
you've understood the concept of a
distributed file system so let us go
ahead and now take a look at the Hadoop
distributed file system so similar to a
distributed file system in Hadoop
distributed file system we have a
cluster of
so let me tell you how the data is
managed in Hadoop so in Hadoop we have a
master/slave architecture so we have the
name node which is the master node and
the data nodes are slave nodes so the
name node over here maintains and
manages all the data nodes and whatever
file you store in HDFS they are
distributed across the data nodes the
name node is just meant for monitoring
the progress or monitoring data node is
managing how each of your files block so
this situation is very similar to a
supervisor at a team so the supervisor
always receives a request or receives a
project and distributes among his team
and then the team members always send a
track report and similar to that the
data nodes also do the same thing they
send hard beats to the main node and the
heartbeats are nothing but signals which
tells the name node that this data node
is functioning and it is alive so the
name node is the master demon and daemon
is nothing but a process and the data
nodes are known as the slave daemons so
the data node stores
actual data would ever read or write
request of a particular file that a name
would receive the data node actually
performs it so this is the entire thing
so this is how your data is managed
across a Hadoop distributed file system
so we have talked about how storing of
big data is done using Hadoop
distributed file system now let us take
a look at how the data is actually
processed in Hadoop and for that we have
got something called MapReduce version 2
or yarn yarn stands for yet another
resource negotiator so let us understand
MapReduce or programming in Hadoop with
help off an exam
so let us consider that we have a class
of four students and we have a professor
and now the professor assigns a task to
all of the students that count the
number of the word Julius in a
particular book so each of the students
has a copy of that book and they start
counting how many times the word Julius
appears in the book and it took them all
four hours in order to go through the
entire book to count the number of where
Julius has occurred so after they have
finished counting they start reporting
to the professor so the first student
says that I found it 45 times the second
says it's 46 tenth third says 45 and
fourth is also saying 45 so then the
professor assumes that he should go with
the majority since three of the students
are saying 45 so 45 is likely to be the
correct answer and so he produces the
correct answer as 45 but it took a lot
of time going through the entire book so
that's why the professor used a
different method now so instead of
giving them each with one whole book he
divided chapters among the students so
let us assume that the book has got four
chapters so he gave chapter number one
to student one chapter number 2 to the
second student chapter number 3 to the
third student and chapter number 4 to
the fourth student and now the students
only has to go through one chapter
instead of the entire book in order to
count number of times Julius is awkward
in the particular paragraph or in
particular chapter so that's why it took
them less time so instead of going
through four chapters is going through
one chapter and that's why it took him
one by fourth of the entire time that he
took before that is one hour and these
students are scanning through their
respective chapters at the same time so
at the time when the student one is
going to chapter one the student two is
going through chapter two and this
student is going through chapter three
and four so this is how it is happening
and in just one art they are able to
give the professor with the results that
how many times they found the word
Julius in their particular chapter and
so now the student reported that he has
found the word Julius 12 times in
chapter one and he has found Julius 14
times in chapter two similarly the third
student says that I found it eight times
in chapter three and eleven times in
chapter
and let us assume that in order to add
up all these answers together it took
the professor two minutes now these are
the small numbers you would take lesser
time for businesses for assumption so
now the whole time in calculating the
same result it comes up to one our two
minutes which is much much lesser than
the time we took before so this is what
actually happens in MapReduce in Hadoop
also we divide the tasks into smaller
pieces so this is exactly when the
students were given each chapter so each
of the students receive the chapter and
this process is known as map and finally
when all the answers were combined
together this is known as reduce so this
is what happens
finally different task takes place and
finally they're merged together in order
to provide you the final output so this
is the story of MapReduce I hope that if
all understood this is very simple
concepts that you have to go through but
these simple concepts are actually used
to solve such a big problem so now let
us take a look at what exactly is
MapReduce so the MapReduce is a
programming framework that allows us to
perform distributed and parallel
processing on large datasets in a
distributed environment since we know
that we're dealing with big data which
are large datasets
and for storage we're storing our data
in a distributed manner so MapReduce
allows us to process data that is spread
across a cluster or different machines
and this allows us to parallel process
data so let us assume that we have got a
particular input now what will happen in
MapReduce is that this input file will
be broken down into smaller tasks and
each of these malfunctions will carry
out a small part of the input and these
are known as map functions and each of
the math functions will produce a result
but this result are just intermediate
results not the final result so they
have to be combined up together and the
reducer does the task of combining the
intermediate results that is produced
from each of the maps and finally when
they're combined together you get the
final output so this is how a MapReduce
takes place in Hadoop so these were the
solutions in order to tackle
big data but now let us see how exactly
to solve the problem of big data so we
are dealing with huge data sets when we
talk about big data so let us assume
that I have got a text file which is
fairly large in size so when I input
this text file into my HDFS it will
break it down into different data blocks
and by default the size of each of the
data block is 128 MB so now assuming
that the file was 320 MB so the first
block will comprise of 128 MB the second
dog will comprise of 128 MB and the
third one will comprise of just the
remaining size which is need to be
stored which is 124 MV and then it is
distributed across my HDFS so let us
assume that the first data node will
contain my first block the second will
contain my second one and third will sit
in my third block so what is happening
is that my file is getting divided into
different chunks and it is stored across
different machines and now let's say
that if I want to store a fairly larger
file I'm going to sort file which is of
TBS or petabytes so if my cluster cannot
accommodate that much larger file what I
can do it I can scale my cluster I can
add more machines on to my cluster so
this is known as horizontally scaling my
cluster it means I'm adding more data
nodes in order to store much more data
so this is how Hadoop solves the storing
problem of data and the next problem was
dealing with the different variety of
data let's say that you want to migrate
from our DBMS into a Hadoop distributed
file system so in order to dump all your
structured data you can use a tool which
is known as scoop and for dealing with
unstructured and semi-structured data we
have got a similar tool which is known
as flume which allows you to dump all
the data into the HDFS cluster and you
don't need to go and install them
explicitly it already comes with Hadoop
so this tools come packaged with Hadoop
itself so now you don't have to worry
about storing different varieties of
data because no schema validation is
required while you want to dump data
into your HDFS cluster so you've got all
these tools to help you out so the next
problem the hadoop solves is process
the data faster so in Hadoop we have a
distributed file system so every time I
dump a file into my Hadoop cluster it
gets divided up into data blocks so now
let us assume that so my files are
divided into three data blocks and it is
spread across my different data nodes
and also let me tell you that the
processing of this different data blocks
also takes place at the same time so the
data block one will get processed in
that particular machine the data block
two will get processed in a different
machine and the data box three will also
get processed in a different machine but
at the same time so that's why it
reduces the processing time by one third
so if we was taking 3 hours before it
will now take one third of the whole
time which is one R and this is the
entire concept of MapReduce which is
nothing but parallel processing my file
by dividing it into smaller parts so
this is how the parallel processing
takes place ad in Hadoop we also have a
concept called as data locality so what
happens in this case is that instead of
bringing data into a processor the
process comes to data or basically the
data and the process reside in the same
machine so each of my data blocks here
are getting processed at the same time
and they are producing the intermediate
results and finally with MapReduce where
we have learned about the reducer which
combines all the results and finally
gives you the final output so this is
how we can use Hadoop in order to
process our big data faster so this is
the Hadoop ecosystem and this is nothing
but a set of tools which you can use for
performing big data analytics so let's
start with flume and scoop which are
used for ingesting data into HDFS now I
already told you that data has been
generating at a very high velocity so in
order to cope up with the velocity we
use tools like zoom and scoop in order
to ingest the data into a processing
system or our storage system because it
is getting generated at a very high rate
so flume and scope acts like funnel in
order to store the data for some time
and then ingest it accordingly
so Slom is used to ingest unstructured
and semi-structured data which are
nothing but mostly social media data and
scope is used to ingest structured data
like excel sheets Google sheets
something like
and you already know what h DSS is this
is a distributed file system which is
used for storing big data we have also
discussed about yarn which is nothing
but yet another resource negotiator this
is meant for processing big data and
apart from that we have got many other
tools in our Hadoop ecosystem so we have
got high V R now highs if used for
analysis so it was developed by Facebook
and it uses high query language which is
very similar to sequel so when Facebook
developed high and when they wanted to
start using it they didn't have to hire
people who knew hql because they could
already use the people who are experts
in sequel and it's very similar to that
now we have got another tool for
analytics which is pig now pig is really
powerful and one pig command is almost
equal to 20 lines of MapReduce code so
obviously when you run that Pig command
that one line pig command the compiler
implicitly converts it into a MapReduce
code but you have to only drive one
single Pig command and it will perform
analytics on your data
first let's park over here which is used
for near real-time processing and for
machine learning we've got two more
tools SPARC ml lib and modes again we've
got tools like zookeeper and embody
which is used for management and
coordination so a party embody is a tool
for provisioning managing and monitoring
the Apache Hadoop cluster and over here
is e is a workflow scheduler system in
order to manage Apache Hadoop jobs and
this is very scalable reliable and an
extensible system then Apache store this
is used for real-time computation which
is free and open source and with storm
it is very easy to reliably process
unbounded streams of data then we've
also got Kafka which handles real-time
data feeds and we've got solar loosin
which is used for searching and indexing
so these are the set of tools in Hadoop
ecosystem and according to your need you
need to select the best tools and come
up with the best possible solution so
you don't have to use all the tools at
the same time so this was Hadoop
ecosystem any questions or dau
all right so thank you for attending
this session I hope that you have all
learned about Big Data and Hadoop if you
have any questions or any queries or any
feedback we want to give it to me please
drop it on the comment section below and
I'll see you next time till then happy
learning I hope you enjoyed listening to
this video please be kind enough to like
it and you can comment any of your
doubts and queries and we will reply to
them at the earliest do look out for
more videos in our playlist and
subscribe to our Erica channel to learn
more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>