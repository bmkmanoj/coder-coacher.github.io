<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Docker Swarm For High Availability | Docker Tutorial | DevOps Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Docker Swarm For High Availability | Docker Tutorial | DevOps Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Docker Swarm For High Availability | Docker Tutorial | DevOps Tutorial | Edureka</b></h2><h5 class="post__date">2017-09-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ceqb53EXANk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and gentlemen my name is Vardhan and on
behalf of a Drakkar I welcome you all to
this exciting session on docker swamp so
this is a third session in the docker
series and a few people have not
intended Maya or your two sessions and
then I recommend you to watch those
videos after today's session okay so I
will share those links at the end of my
demo today and in the previous sessions
what I explained was what is docker what
is container ization what are its
benefits how to continue that services
and in particular I showed you how to
container eyes angular application and
also I use docker compose to container
eyes a mean stack application right so
today's session is the next in that
series where I will show you how to use
docker swamp okay and by the end of
today's session you will understand why
docker swarm is one of the best
techniques to achieve high availability
of the deployed web applications all
right
so please don't hesitate if you have any
doubts or queries you can put them in
the comment box and either my team or I
will myself answer them for you so sit
back relax and enjoy it Rekha live for
the next 30 minutes to learn how docker
swamp can be used to create a cluster of
docker engines for achieving high
availability right but before I get
started I want to ask you people a
couple oh I won't first of all know a
few people know what are docker
containers right so even though I cover
it in my previous session it's it will
be good if people already know it so
it's but otherwise if you still don't
know it then it's fine I will give you
an introduction it'll be a good revision
if you know it so please put your raw
comments coming in
you
okay so anyways I'm guessing there'll be
a mixed set of people here so some of
you might know what it is and some of
you do not so don't worry Abel as you
can see from the agenda I will first
give you a brief introduction to docker
containers okay I will show I will tell
you what are docker containers
what are docker services and after that
I will move on to what is a docker swamp
and what are its features okay we will
then discuss a few stocker commands
because those I would be using in
today's demonstration and finally would
be the demonstration part where I will
show you how to achieve high
availability of the services that you
deploy by our docker containers okay so
enough talk let's get started with our
first topic for today and that is what
is a docker container now a docker
container is an isolated application
platform which contains everything you
need to run the application except for
the actual operating system right so
I've said this before I'm saying it
again ok so your container here would
only contain your application software
its libraries and it's binary is all the
dependent dependent packages only these
things okay and this would be hosted on
top of the docker engine now your doctor
engine can also simultaneously have
other containers they can be many number
of containers based on the performance
of your server right and this docker
engine is in turn hosted on the server
the server can be anything host it can
have any host it can have a Windows
operating system it can have a Linux
operating system or can even have a Mac
operating system now in case you're
having the Windows or a Mac operating
system in that case then you will have
to use something called as a docker
machine to connect your docker engine ok
that's a separate interface but if
you're using ill enough then it's it's
as simple as using a terminal you can
just execute your comments your commands
from the terminal so that's that's what
a container is and now let's see how
docker containers are created they are
clear they are created from a docker
image ok now if you're running a very
simple docker raw you know application
ok let's not say docker application if
you want to run a simple web application
using docker like if you want to install
or run tom cat or Jenkins and
these things then you can just download
that docker image from the docker hub
okay and straight away execute that
image and you will get a docker
container the rocker container is
nothing but the active version or the
running version of your docker image
okay but if you want to customize the
application as for your needs you want
to install other set of dependencies and
packages on top of that image then you
can do so and the changes that you make
to that image will be recorded as layers
as you can see here this will be a base
layer where your where you this will be
the base layer which you download from
the docker up and on top of that
whatever changes you make they will be
recorded and that's how things will be
stored and when you execute this final
docker image you will get a docker
container right again this is the fact
this is the running instance and that's
how you get a docker you you get the
actual application that you know the
hosts on your system now I want also go
to details about how to create this
docker container okay now this is just a
basic of how our docker image converts
into a docker container but if you want
to go into details of how it works in
the industry then it works like this
okay
you start off by writing a docker file
in your docker file you specify which
docker image you want to pull from the
docker hub you specify that and then you
specify all the other applications that
you want to copy into the docker
container all the other dependencies and
library or all the dependencies and all
the other variables that you want to
install you specify all those things
inside the docker file ok and then when
you build that docker file you get a
docker image now this docker image is
even right now it's just an inactive
version or it's an inert file okay which
just has your all the dependent software
and all those things it's not active yet
and you don't have your application or
you don't have your service running as
yet but then you run that docker image
you get a docker container and this
docker container is the actual running
instance and this is where you get your
service but you do not upload your
docker container to the docker hub okay
now it is this same docker image which
you upload to the docker hub and this
docker once you upload it to the docker
hub other people can pull it from the
grubbé and execute that same docker
image at their end producing the same
server or the same service at that
system so in the industry it happens
this way you'll have the your developers
your who would push the docker image to
the docker hub and from the docker hub
you will have it operations team members
who would you know who would be pulling
that docker image and they would be
executing that docker image and
converted into a docker container either
at the staging end at the staging server
or at the production server so on a high
level this is how it works alright so
this was all the introduction that you
needed and these are things we also
covered in them in my previous sessions
ok but it sounds move on to our main
topic for today and that is what is
docker swamp so a docker swamp is a
technique to create and maintain a
cluster of docker engines ok now what I
mean when I say a cluster of docker
engines is that there will be many
docker engines connected to each other
forming a network
okay now this network of docker engines
is what is called as a docker swamp
cluster and as you can see from the
image over here this is the architecture
of docker swamp cluster ok and there
will always be one docket manager in
fact it is the docket manager which
basically initializes the whole swamp
and with the manager they will have many
other nodes on which the server should
be executing so there will be times when
the service will also be executing at
the managers end but basically the man
just primarily role is to make sure that
these services or the applications are
running perfectly on the docker nodes ok
now whatever applications or services
that are raw specified or requested they
will be divided and they would be
executed on the different docker nodes
now this act is called as load balancing
right the load is balanced between all
the other nodes so that's what happens
with the docker swarm and that's the
role of a dock and managers now let's go
and see what are the features of docker
swarm and why it's really important and
why it's you know the go-to standard in
the industry that's because with
dr. Swan there is high availability of
these services okay it's so much so that
they can be hundred person high
availability all the time right that's
what high availability means right so
how is that possible that's possible
because at any point of time even if one
node goes down then the services which
are running inside that node they can
start the manager will make sure that
that service is just that service is
started on other nodes right so the
service is not hampered even though the
node may be down the load is balanced
between the other nodes which are active
in the swamp so that that's what a
docker manager does and that's why the
document manager is heart of the docker
swarm cluster okay that's one feature
the other feature is auto load balancing
now again the order load balancing is
something that is related to high
availability itself where at any point
of time if there is any downtime at
those times the manager will make sure
that those services are not stopped and
they are continued and executed on other
nodes right so that the manager will do
but along with that a load balancing
also comes into the picture when you
when you want to scale up your services
supposing you have say three
applications and you have three nodes
for that right so including the manager
you will have four nodes because manager
is also technically a node okay so you
have a manager node and then you have
three different nodes so in this case
and the three services which you deploy
they will be running on three different
nodes and if you want to scale them at a
later point of time let's say you want
to scale them up to ten services then at
that time you the concept of auto load
balancing would still come into the
picture where in the tenth services they
would be divided between the nodes
alright so it would be such that you
will have three probably three services
running on one node three more services
running on the other node and the
remaining three services on the other
node and the one service that is left
out that would you know sometimes be run
on the manager or it would be a load
balance on some other node okay and the
best part of Doras you don't need to do
any load balancing it's all done on its
own right so there's an internal DNS
server with which the the token manager
managers and the DNA server makes sure
that it allocates it makes the DNS
server make sure that all the nodes are
connected in the cluster and whenever
any your load is coming it would balance
the traffic between the different nodes
okay so that's one big advantage with
Auto load balancing and another feature
is that of decentralized access so when
we say decentralized access it means
that we can access these managers or
these nodes from anywhere so if you have
these these managers or or or these
notes posted on any server then you can
simply SSH into that particular server
and you can get access to that
particular manager or node so if you
access the manager then you can control
what services are being deployed to
which nodes okay but if you log in or if
you as such into the server which is a
node then you can control or see which
service is running inside that node
itself okay but however you can't
control the other nodes if you are
inside a node only the manager node can
do that for you but anyways all that we
need is to login to already search into
a document manager and you know control
which services are running right so
that's all we need that can happen this
way and of course it's very easy to
scale-up deployment so I also spoke
about that earlier where you know you
can if you want let's say you already
have but answers and if you want to
suddenly scale it up to 50 or say 100
servers 100 services then what you can
do is you can just buy a few more
servers and deploy those hundred
services into those servers right so
it's a very simple or very simple
functionality where you can do it with
just one single command one single
command is all it takes to scale up your
number of services or applications to
the desired amount right and you will
have multiple services running inside
that same docker raw node so each node
can have a problem to probably have ten
or fifteen services running and it
basically depends on the number of nodes
that you have but ideally you would you
shouldn't do that you cannot have but
too many services running inside the
same node because that causes
performance issues right so all those
things you can do and finally is this
concept of rolling updates and rolling
update is by far the
most catchy feature because when we said
rolling updates we are what we mean is
these applications or services which are
running right they will have to be
updated at one point of time or the
other down the line you will have to
update it so at that time what will you
do you cannot you know upload you know
update manually in every single machine
right if you don't have docker if you
have a host of your web servers on
either virtual machines or on actual web
servers then what happens at that time
you would have to go to each and every
system and then probably update it
everywhere right or you might have to
use other configuration management tools
but with the help of docker
you don't have all those problems you
can simply you know specify the you can
use the rolling updates functionality
for that and you can specify a delay so
in the delay it would update one service
or draw each service which is hosted or
deployed inside every node it will
update each of those services one after
the other with the delay of the
specified amount of time right so
between so even when one service is
getting updated the other service is not
down and because of that there is high
availability since the other service is
still up and running you don't there is
no downtime caused right so you can be
sure of that
in spite of and rolling updates are very
simple also so you just again it's just
one command and you're all done that's
these are the benefits of docker swarm
and these are reasons why you should
implement docker swamp in your
organization if you have a massive web
application web service which is
deployed over multiple servers so that's
the big benefit with the docker swarm
right so moving on to the next slide
okay now it's time for the demo okay now
let's see how to achieve high
availability with docker swamp but
before I start with the hands-on
you
yeah sorry guys I'm back so I was saying
that before I get started with the
hands-on part where I would be showing
you on my virtual machines I want to
first go through what I want to show you
with respect to high availability okay
and how to achieve it with docker swamp
okay so first of all so first of all
when terms of high availability the
ideal definition is that you have the
application of the services deployed in
each of the web servers okay now look at
this architecture where I have got two
nodes and I have one manager okay and I
have docker engine running on each of
these each of these nodes and each of
these are all highly available okay so
at this point of time I don't have any
problem okay with respect to any
services and my application is deployed
in each of these servers okay each of
these servers or each of these nodes so
at this point of time if I access if I
try to access my browser and if I try to
access this port number in my browser I
can see my application running okay now
this is the application which I will be
showing you my demonstration on and this
is also the application which I executed
a couple of sessions back okay the link
of this application the demo of this I
will share it at the end of the session
ok but don't worry about that because
this session is all about docker swamp
so getting back what I was saying is
since these are hosted in each of these
servers I can access the application
that I have deployed on each of these
machines but look at this scenario where
my service is only hosted on one
particular node this time ok I have the
other services connected to my cluster
ok this is my swamp cluster where it's
all connected but the application is not
hosted on these two nodes so at this
time can you guess what happens
can anybody does anybody think that the
application will not be accessible on
these machines can you nobody tell me
that if you people think like that can
you just well if you think like that
then your people are wrong because
since it's connected in a cluster these
docker whatever is hosted on one
particular node
they can also be accessible on other
nodes so even even in spite of the fact
that these servers do not have the
application running the web port on
which this application is hosted right
this port number will be internally
exposed to all the nodes inside this
cluster and since the port number on
which it's running over here that is
full to double zero since that is
exposed to the cluster then in all the
other nodes in the cluster on the same
port number four to double zero the
application would be accessible right
same thing with even this particular
node so on four to double zero you can
access this angular application this is
the second scenario of high availability
but this is just a scenario where you
don't have your application this is when
this is the third scenario where high
availability is actually being
implemented okay now you have a scenario
where you have your raw three nodes and
two of your nodes or one of your nodes
goes down okay so this time you don't
have your application itself forget
about the fact that docker is not the
application or not hosted forget about
that fact think about this scenario
where your node is not accessible it's
down for some reason for some natural
calamity at that point of time do you
think you can't access it you can't well
that's because the though again the
nodes would be connected inside the
docker cluster the swamp cluster and the
port number would be exposed so because
of this reason you would still be able
to access or you would still be able to
view the angular application on these
servers right now that's a benefit of
handing a docker swamp cluster alright
so and this is how the high availability
factor is achieved with the help of
docker swamp and this is what I'm gonna
show you in my hands-on part but before
I go to that part let me just quickly
run through these docker swarm commands
okay so these commands is what I will be
using extensively in my demo and so also
the most common swamp commands that you
need to use when you're starting with
your docker swamp cluster okay so first
of all to initialize the swamp you use
this command
you say docker swamp and in it you use
double flag and say advertise adder ok
forward the followed by that you specify
the IP address of the manager machine or
that same machine where you're starting
this service
ok so the when you when you do this
whatever IP address is specified here
that particular machine would be acting
as a manager ok it is also ideally the
same machine on which this command is
running right the IP address of the of
what you specify or it should be the
same machine so that's the thing and
whenever you issue this command this
swarm would be initiated along with the
manager being this particular raw
machine which has this IP address ok
that's what happens when you do a
initializing the when you initialize the
swamp and of course when you initialize
the swamp you will get a token it's more
like the key entry key using which your
other workers can join your docker
cluster ok
but getting back to our docker swarm
once you finish lies your swamp you can
list the different services that are
running inside that swamp
ok you can list the different you can
list the different nodes that are
running you can check which all nodes
are connected to your swamp cluster you
can check what all tasks of services are
running you can check you can create a
new service a new service as in a new
container and then you can also remove
that new container and you can scale
them up using these commands ok so use
the docker service LS to list down the
services that are running then if you
want to drill down on one particular
service uncheck in which node one
particular service is running then you
can use the docker service PS command
ok so it lists down that process when
you shoot with the name of the servers
that you want to check for and then if
you want to create a new service then
you use this command of docker service
created then you specify the name of the
service in fact and you go to specify
the image which you want to which you
want to use to build that particular
container and service and finally to
remove a service you use this docker
service RM followed by the name of
particular service and finally if you
want to scale your services then you can
use this command docker service scale
and then you can just specify the name
of the service and you can specify the
number that you want to scale it up to
okay
so in this case if I had the service
which are which was which had two
replicas then by simply specifying is
equal to five against out of scale it up
to five different replicas right so
those are these are the swamp commands
which which are applicable from the
manager end and now going back to the
node n if you want to list down all the
nodes that are there in your swamp then
you can use the docker node less okay do
note that here it was all about the
different services okay and these
commands cannot be run on the docker
nodes okay they can only be run on the
docker managers okay so here you have
the docker node LS which lists listens
down all the managers and the nodes and
then if you do a docker node PS followed
by the service which you want to in fact
if you do a docker node PS it basically
lists down all the containers or
services that are running inside that
machine
okay now this command again it can be
run on even nodes okay this cannot be
run on all the nodes so the node LS it
can be only run on the manager and
finally if you want to remove a
particular node from your service from
your cluster then you can run the
command docker node RM followed by the
ID of that particular node okay but at
times you might not be able to do that
that's because the node might still be
connected to the cluster so in that case
what you have to do is you have to use a
docker swamp and leave command when you
use this command you can you can get if
you run these commands from the nodes
then the nodes would leave the cluster
and then you can just end your cluster
right
and finally you if you can just run the
docker swarm leave from the manager end
then you can end the whole cluster
itself so even the manager would leave
and manager would ideally be the last
instance to leave right so when there
are nodes there you cannot you cannot
have the manager leave with the Nords
being present so that's one thing and at
times you would be given a error saying
that you cannot leave the cluster
because you're a manager so at that time
you can use the flag force flight okay
so at this time you are wrong as a
manager you can leave the cluster and
your cluster session ends there right so
these are the top commands which are in
question so yeah I think it's time for
me to go to my session okay it's time to
go to my hands-on session where I'll
open up my virtual machines so guys
please confirm if you can see my virtual
machine now okay can you see my virtual
machine you guys okay great so for the
demo purpose I have three different VMs
okay and inside these Williams I have
three different docker engines and I
will be basically using two of the
docker engines as my node and I would be
using one of them as my manager okay so
this is my manager one as you can see
over here and this is the password okay
so this is the manager one which I'm
going to start the service the whole
swarm and the services from here okay
and if I go here this is my worker one
as you can see over here alright and
this is worker 2 now in these two nodes
I would be executing my applications or
services ok so first of all if you want
to create the swamp service then you
have to run the command docker swamp in
it advertise a door followed by the IP
address so the IP address of this
manager machine is 192 dot 168 sorry dot
one sixty eight dot one dot one hundred
ok
so great so my swarm is initialized and
as it says if you want to add a workout
with this one then you have to run this
token okay now this is the token let me
copy this token and run this add the
nodes end okay so I'm gonna go to work
of one I'm gonna paste this token and
when I get Enter it says this node has
joined this one as a worker now let me
verify that if I go back here and if I
issue the command docker node list then
it says that I have one manager which is
myself myself is being indicated by this
aspect okay referring to this own system
which is also the leader
so it says manager status leader correct
the status is ready and availability
it's active and recently I added the
worker node so it says even this is
available now let me go to the third VM
and enter the token here hundred says
even this one has joined as a worker now
if I go back to the manager and run the
same node list command you can see that
the worker 2 has also come in now okay
now that's because I have I you should
the join command at my node end so I'm
gonna just clear the screen and now we
can start creating our services so first
of all if you want to create a service
the command is docker service create
followed by the name of the name flag ok
so you specify the name of the service
that you want to give let's say I want
to say angular application I will say
this and followed by this we should
specify the name of the image so the
image name is a demo app 1 ok
and along with this I also want to
specify the port number on which I'm
going to do the binding because the
angular application which is being
hosted in one particular port number
inside my container that has to be
mapped to my browser port number right
if I want to access it on my web browser
that is this Firefox so for that reason
user - be flag and I'm gonna say four -
double zero of the browser port should
be mapped to photo double zero of my
container port so this is the command
okay now this command simply creates one
instance of this service angular app
which will be built from this image demo
app one okay and expose the port number
forward of zero from the container to
the port number photo double 0 of my
browser so let me hit an enter and let's
see what happens we got to give it a few
seconds because it's a big application
right yeah so now let's do a joker
docker service LS okay you can see that
one annual application is created now
this is just a warning okay you can
ignore this because this is the
confirmation that your service has been
started okay you can ignore such
warnings if you this what you need to
look for now if you get this image ID
then it means that your service has been
created okay this is the service already
basically so as you can see your right
now the mode is not replicated there is
just one single instance and the same
thing you can see over here it says
replicas is one the same name which are
you are specified the image that it used
and the port number where it's active
right now okay now let me do a dock at
PS command from the manager and check if
this application is running inside this
node so yes it says that this
application is running over here now
apparently let me go to my worker one
okay this is also connected to the same
cluster so I'm gonna do a dock at PS
over here you can see that I have got no
output so this means that there is no
container that has started inside this
node okay this is the work of one
similarly let me go to work or two and
say docker PS again there is no output
when it come you know with this okay it
says no container has started now if I
go back to the manager and verify I can
verify where in which node
application has started and the command
for that is talker service PS followed
by the name of the application that is
angular rap so when I hit enter you can
see that the name of the application is
this this is the ID and the image that
was there and the node where it's
running so it's posted in the manager
one in my system itself it's hosted ok
primary system the desired state is run
running and the current state is running
about a minute ago ok now let me go to
my browser and access localhost for to
double zero now as you can see this
particular draw this is the angular
application which I've hosted ok now I
have explained what this application is
about in one of my previous sessions I
would request you to go to that video
and get more details about this
application ok I want to just quickly
get back to my session here with respect
to Swan so since I've started my
application I can access it over here
now as I explained earlier all the nodes
in your cluster can see the application
that you've started right I explained
that or you right now let's verify that
by going to the other nodes so in spite
of the container not being hosted on
this particular node I can get the same
local I can get the same another
application over here because the port
number would be exposed internally
between the different nodes in that
cluster same thing with my docker
worker 2 right so if I do well I've
already done a docker PS you can see
there is no container your so let me
just quickly go here and do a local host
forward of 0 yeah so you can see the
application is hosted even on this
particular node now this is good news so
this means that your application is
successfully hosted on the cluster and
it's accessible on all the nodes right
now I'm gonna do a docker node LS and
yeah we have three different nodes it's
the application executed over here if
you want to verify that you can also do
this the docker
service LS okay it's just one
application and if I do PS with the
angle or name with the image name it
says it's running here great this is one
this is one of the scenarios which I
want to show you okay but I want to show
you another scenario where the
application can be hosted on multiple
nodes at the same time from the manager
okay and the command is not gonna be too
lengthy also okay so last time what I
did I basically executed the container
at one end right so that was executed
only over here let me but before I go to
the next scenario let me remove this
service okay so the command to remove
the services docker service remove
angular app so when you get this output
it basically means that app application
has stopped
you know the deployment has been removed
so if I try and refresh this forward of
the report it says it's unable to find
anything there and similarly you won't
be able to find it on any of the other
nodes also because the cluster itself
does not have access to this particular
angular application right now but now
let me go back to what I was talking
about the second scenario where I can
start the same service on all the three
nodes okay the same docker service which
I created I'm gonna issue that with a
slight modification so after my port
options after this flag I'm going to use
this flag of mode okay I'm gonna say
motifs global now with the help of this
flag the application which I am
deploying which I am you know hosting
this will be basically deployed on to
all the all the three nodes of mine okay
we can so I can show you that by first
hitting enter and let's see what the
status comes okay so we'll just take a
few seconds because it's being deployed
to multiple nodes right that's the only
thing so yeah again the service has been
created this is the
now let's do it docker PS and check and
there's one instance of this application
running in this same manager okay like
before it's running over here also and
let me verify that if it's running over
here this time by running the same
docker PS command
yes as you can see seventeen seconds ago
this application was created and
similarly if I go to the third VM and
run the same docker PS command
it's open over here also and this means
that the application this time was
deployed to all the three nodes parallel
you okay we can also verify that by
going to the folder double zero ports of
each of these machines so this time the
app is back up right it's running again
same thing I can refresh this and I will
see the application coming you're just
connecting and similarly over here also
I would have the same success scenario
okay it's connected great this is the
movie rating system that has been
deployed on all and across all the three
VMs now let me verify this give you the
way you can confirm this is by running
the docket service first you do the
docker service LS command ok so with
this you have the Morris global okay and
it says replicas is three of three
that's because they were raw three
different nodes connected and since its
deploy to all the three it says replicas
is three of three correct the only
difference last time was it was one out
of one the same as three or three and to
give you a four to drill down further
into details as to if it's running on
each of these nodes we can use the
command docker service PS followed by
the application name that is angular app
so when I hit enter as you can see it
says there's one instance running on
worker one one instance running on
manager one and the third instance
running on worker - great so this is
what we this is the real fun part with
docker right with once we command you
can do all these things so let me also I
mean
it's verified this right now now comes
the concept of high availability if any
of my node goes down then what happens
but I still get access to my application
over there right that question needs to
be answered so let's see if that is
gonna happen so for that let's say my
internet of my Avoca one right this is
my worker one right let's say my intern
of mine notice down and to get my node
down I'm just gonna do a disconnect
okay so right now it's not connected to
the Internet and if I go here and do a
talker node LS command which would list
down all the different nodes in my
system you can see that the status for
worker one is down okay all this time it
was getting a we were getting ready
that's because the status was that's
because the server server was up but
since I turned off the internet in my
work of one it's telling state is s down
but in spite of that I won't have any
problems accessing any application ok so
even though i refresh it you can see
that on this port number I could access
this application that's because in spite
of the fact that this node is connected
to the cluster I can access this right
and the very fact I can do that is
because all the machines or all the
nodes in the cluster will have the port
number opened right it would be exposed
between all the other nodes the same
concept I explained during my slides
right so in spite of the fact that might
not bring down I could do this now this
solves one aspect of high availability
right so in this case even if I have
like multiple nodes going down then some
of the nodes which are you know good
enough which are healthy those can
service they those can satisfy all my
services for a temporary period of time
but of course I don't have to bring up
my your nodes again right so this is how
one hi everybody can be accessed that
that's one thing so let me just go back
my worker 1 and any will internet again
ok so that I can continue with my
demonstration
ok I'm connected now so if I do a docker
node
dr. node LS again and let me just
refresh this huh yes now it says the
status is ready
great I'm gonna just clear the screen
now since I ran last last command where
I did it in global mode I had an
instance running on each of the nodes
right so this time let's say I don't
want to do that I have three different
nodes but I want to host the application
only on two nodes well I can do that
also I can set the number of replicas of
my service over there in the command
where I'm starting my service so let me
go back to that start command and
modified as per our needs so I'm going
to remove this mode global once you
remove this flag you can add you can add
the replicas and set the number of
services you want okay but before this I
would have to remove the service right
sorry my bad I just forgot to do that so
let's say docker service remove angular
app so I'm remove it now and I'm gonna
restart the service okay so now let me
start modifying this start command so
I'm going to remove this global mode and
I'm gonna say replicas and I'm gonna set
the replicas to two now this would
indicate that I will have two running
instances of this service between the
three nodes okay it will load balance
between the three nodes and the manager
will choose on its own it will deploy
the application on two of the
best-performing nodes let's verify if
that's happening
so yeah the success so it's successful
we can verify that by doing a talker
note PS okay this would basically is
down if the continuous present in this
node yeah there is one container or one
service running over here but to get a
detailed to get more details let's run
the talker service PS command
okay let me just clear the screen out
and run that command again for you so
when I do this it says that two
instances have been created right one
has been started on my worker one and
the other has been started on my manager
one right two instances between the
three nodes
let me also do this this for you let me
do a docker service LS sure to confirm
the replicas right it says the mode is
replicated mode and it's two out of two
correct so no no hassle anywhere here
right so if i refresh it I would still
have the angular application hosted this
is worker one its anyways hosted over
here so I don't need to verify anything
but to give you a confirmation I can do
that also by running the docker PS
command
so the docker PS would list down all the
containers and services running in this
particular node so when I hit enter I
have one entry here okay for once over
that got started however in this node to
the worker or to I do not have the
application right so let me verify that
by running the command docker PS okay
it's not running here there's no source
but in spite of that the application
would be running here so that's the
concept of docker cluster where in all
the nodes we get access to what's there
in the docker cluster so that's the fact
and now comes the concept of scaling up
and scaling down right this is one thing
which a lot of people have this doubt
because it's not always they're done
right and in spite of having a cluster
where you have only three nodes we can
scale it up to any number of
services that we want to so right now I
have two different services right if I
do a docker service and if I do LS you
can see that there are two services
running now if I want to scale it up to
let's say five services I can do that
too that would also happen and the
simple command to do that is docker
service scale and we should choose the
application and we have to set the
number we want to scale it up to let's
say I want to scale it up to five so in
this case three more services would be
added on to this cluster okay
I'm gonna go ahead and hit enter
and yeah so it says the app the
application has been scaled to five now
let me run the same docker service list
command and when I do that it says right
now
three replicas have already been started
and let's give it a time let's give it a
few minutes so that it can start on all
the other nodes okay in the meanwhile
I'm going to clear I'll clear the screen
first and I will do a docker service PS
and angular app this would tell me on
which nodes my applications are gonna
get deployed so it says out of the five
on worker one yeah there are not two of
those services will be running on worker
one okay as you can see our service
number one this is the service number
two right this is running on worker one
and again on work or two there will be
two services running you can see these
two services over here worker 2 and then
on manager one there is one service
running this is because I scale it up
from two to five now let me do a docker
service list command to check the for my
replicas are up yes so we've given
sufficient time and by now all the
services are up and running we can check
it over here but we don't need to
because we know for sure that it's going
to be hosted anyways so this is good
news correct so yeah this is this is how
we can easily scale up we can easily
scale down and we can achieve a lot a
lot of comfort by using docker correct
so yeah so yeah guys so this come brings
an end to my session to mine your
hands-on session about to show you how
to scale up and scale down I've showed
you the concept of high availability
also the whole concept of load balancing
happened here but I still there is one
more thing which I also want to add on
from my side okay and that is why will
the services be executed at the Manzo's
end right a manager ideally does not is
not supposed to do any work right that's
what the work has offered the manager
just manages so this is a question that
you can come up with it's a very valid
question so if I want to do that then I
can you know again run just one command
and enable that functionality also okay
and the command to do that is there is
docker node update I can use the
availability flag here okay and I can
say drain and I can choose which node I
want to drain so when I so drain
basically stops allocating services to
that particular node which is specified
so over here if I specify manager one
then and if I hit enter then from now on
the service which is allocated over here
right this would shut down and a new
service would be created on either one
or work or two that would happen or if I
don't want to drain my man job I can
also drain one of the workers I can
either drain worker one avocado but
let's say for our in our case we want to
down the manager so I can do that by
simply hitting Enter over here and yes
we've got this as the return return
value that's great so now if I do docker
service PS angular application which is
the same command you can see that the
manager one has been shut down okay and
there has been an additional service
that has started on worker one so right
now there is work of one running is one
two and three so three running on wakka
one and these two are running one worker
- right
so I'm gonna clear the screen here and
we execute the same command and also
show you what happens now when I do a
docker node list ok the node list will
basically list down all the nodes
connected inside that cluster right so
I'm gonna do a docker node list and over
here this time you can see that for my
manager one which is this ID the status
is ready
however availability is not active it is
drained ok even though it's a reader it
is drained so from now on if I scale up
the service or whatever I do even if in
case of high availability at that time I
cannot no services will be allocated to
my manager unless until I remove the
drain ok I can you know remove the drain
by again specifying the command of
active so let me run that command and
show you that so here instead of seeing
drain if I change the availability to
active then I can start allocating
services to my manager also so if I hit
enter it says I've got value and again
if I run the same token or LS command
availability is there and from now on
whichever if I if I scale up
I'd only at that point of time well my
manager start getting resources so these
would what are what are existing right
this will not get allocated to my
manager in case if there's any downtime
or if any of my node goes on then at
that time manager one will get access
right and yeah that would happen so this
is the simple demonstration which I want
to show you it sounds simple but this
solves a lot of industry issues correct
it's one of the one of the best tools I
have well Don docker and docker swarm is
one amazing technology that also AB
witness so I hope you've also understood
what kind of you know what I'm talking
about over here
correct so yeah that brings an end to my
demo session here do bring in your
questions if you have any do put them in
the chat box and the meanwhile I'll just
go back to my slides I can go to my
conclusions
before that let me also show you these
details now that we have come to the end
of the session I also want to tell you
people that oh yeah
we have an entire Gawker playlist over
here right it Rika has an entire docket
playlist along with a DevOps leyla
playlist right so you can check all of
the docker videos over here and if you
want to see one of my my earlier live
sessions then you can see that also in
this playlist ok so right now you can
see the docker compose where I've
continued rise demean sack application
you can see this video here and there's
another video which I would add on to
this playlist and you can also find that
video in this playlist so you'll have
three docker videos where I'm showing
the different functionalities of docker
ok these are the three things and figure
is want to be notified of future live
sessions do remember to hit the bell
button the bell icon over here okay and
right next to subscribe then you'll of
course be notified of all the live
sessions that are there in the future
right one in the meanwhile now if you
guys are interested in learning DevOps
or docker from Ed Eureka then you can
check out our raw landing page and here
you'll find it is about our patch you
will find the course details like the
price what is the price when when is the
next patch starting and you'll also find
what is the curriculum what you will get
to learn you can do that for both
develops and talker and yeah most
importantly since you guys have been
here for so long been so active till the
end of this session we have you can go
to this link and you can register over
here
if if and only if you want to avail an
extra discount on any ed rekha course ok
not just divorce or docker
but you'll get a discount which is only
for you right it's exclusive only for
you and you need to fill fill up this
form and you need to submit it if you
want to be eligible for some kind of
special discount all right so yeah these
are the things that I want to show you
and let me just go back to my slides
here
okay guys so yeah do dropping your
questions guys in the meanwhile
in the mean white
okay so I have one question from five of
he asks how should I configure load
balancer I mean which IP should be used
well vibe of dads the thing in the
meanwhile have to do the load balancing
on your own right it's got its auto load
balancer and the docker manager will
take care of that for you right so you
don't have any problem here whatsoever
so automatically the docker would take
care of that all right okay so I have
one question from vibe of he asks how
should I configure load balancer I mean
which IP should be used
well vibe of dads the thing in the
meanwhile have to do the load balancing
on your own right it's it's Auto a load
balancer okay on that please explain
your own apps a Tony problem Vetri way
to automatically my machine system
running on top of that I have I have</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>