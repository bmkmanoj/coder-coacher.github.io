<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>When to Use Hadoop | Hadoop Tutorial for Beginners | Edureka | Coder Coacher - Coaching Coders</title><meta content="When to Use Hadoop | Hadoop Tutorial for Beginners | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>When to Use Hadoop | Hadoop Tutorial for Beginners | Edureka</b></h2><h5 class="post__date">2015-05-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AyxXK6_U-P8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so let's go and begin to race there
bina so today maybe now is going to be
hall about on discussion of the Hadoop
platform and basically are taking a
scale of when to use Hadoop and the
primary applications where the Hadoop
can be used better the next one over its
going to be very interesting debate on
the hydro technology and sing some use
cases off I do so with this let's go and
begin today's webinar so before even
going and understanding now what about
Hadoop X that I just like give a very
small introduction of what is Hadoop
okay so that helps you a lot
so since we have only one hour of time
with us so the first 30 to 40 minutes I
might go over the theoretical aspect of
the webinar in the last 20 minutes I
like to cover some of the use cases with
Hadoop we're a primarily Hadoop can be
used very much effectively okay so maybe
I can give a very small introduction on
how to even before starting what is the
session is all about so let me give a
very small introduction of basically
what Hadoop and wired is all about
so Hadoop is closely related to the Big
E determine right so everybody is
already aware of why we use I'll do
because the primary intention of using
Hadoop is is because it is totally your
frame up which is used for big header
domain so prior to Hadoop right so what
makes Hadoop one of the most important
framework in the market today that is
the answer we need to understand first
even before understanding when to use
sort of event to not user do so the
primary thing of anybody has to
understand is why Hadoop
how come how do you become a very
important perspective in the peghead
Adame today so to answer this question
so primarily what happens was as big
data means means amygdala is nothing
lots of little right as as we start
accumulating more and more data right we
started facing problems like the first
thing the data storage so our effective
destroy is going to happen and also the
data analysis so typical our DBMS
Jennifer I hope everybody can able to
see the notepad right I just shared a
notepad okay yeah so so before as we are
discussing a why I do so as the data
gets progress to means as an
organization when I might have started
with a storage of two T GB of data right
so as my organization grows and the data
capacity also goes along with the
organization right so in that ski is
what happens so we need an answer for
this right as as the data keep screwing
is there any system can accommodate how
much data is going to come so I can't
define I it might be of 40 TB of data or
down the line it might go 200 TB of data
down the line it might go to 150 TB of
data nobody is in a position to define
alright so that's what the problem
started right if typical our DBMS cannot
solve this problem because an our DBMS
is fixed we are talking about something
called as totally and absolutely figure
which which we cannot come up it might
go to find a DB it might go to 600
tables right so that's the one problem
in actions the data analysis when we
talk on the analysis perspective so is
it possible to take the complete under
TB as an input right so this is what the
next question comes up which which I am
never able to achieve with my current
database problems right the current
database I cannot do this or in current
solution it may be any solution not only
a database perspective I can I consider
the complete under TV as an input or can
I consider the complete the word over
the data I am going to use as an input
if I say X amount of data okay so
finally the bottom line is the beginner
put a very simple problem is if that is
an X amount of data so can you store
this X and also can you do the complete
analysis onyx
right so this was the biggest challenge
but this is the challenge which was put
forward the complete industry so that's
what bigger I started creating the
problem the primary reason is we did a
base cannot scale up there's a
limitation of every every single person
is who are using the database is clearly
aware of why it cannot handle Android TV
and we don't want to discuss on that
also because that's not the primary
reason of using this correct okay so so
in order to answer this so we come up
with a framework called as nano so
that's well basically Hadoop came up in
the picture so it complete distributor
data set so that's the solution Adam
came up a complete distributor dataset
so that's what basically I'd have
started discussing on itself let's let's
see maybe a internal understanding of
how Hadoop is will to solve this part
this is what the this is what had to
post friend to slowly okay and this is
what Hadoop as solved and that is why we
are using I do it it gave us an answer
of storing any amount of data at the
same time of storing at the same time of
doing analysis to any amount of data
right there is no such thing like
restrictions there is no such there's no
certain restrictions on that big-haired
technologies with a do and how we do
such you let's let's see into a session
all right so that's what I want to put
forward first what what basically are
you is trying to swap right so it is a
distributed framework a completely
distributed framework which tries to
solve the picutre problem the problem
which we discussed all this amount of
data and as well as the complete
analysis of data okay so that that's
gives a very small introduction of what
Hadoop is horrible so maybe next we can
go all the slides and understand what
when do you use Hadoop and when not you
say okay it says everybody clear on this
getting an understanding of what I hope
is all about
okay fine so okay so ask you discuss so
that's what basically I do piss all of
what Exeter everything is fine so the
one important thing everybody has to
understand today what they think is that
because Hadoop as a framework is able to
solve all the problems right so it's not
something we should use Hadoop for
everything because we have a solution
like a framework called Hadoop which can
solve the distributed processing of data
right so we should not conclude
something like Hadoop is the solution
for everything so that's not the case
right there are some scenarios you
should not use a loop and there are some
scenarios you should use a right so
let's consider so now let's coming back
to those relations when to use a loop so
let's let's take some of the scenarios
when we are going to user so similarly
so we are going to use sort of as we
already discussed a primary reason of
using Hadoop is whenever the data size
is expected to be very huge right where
the data is going to be exponentially
doing and you can also perform data
analysis on anything it means you can
perform the data analysis unstructured
and the same structure and unstructured
basically we can do a data analysis of
anything right and also for future
planning of data so for example if
currently if your organization is our
intent EP of data and you are expected
to grow to a very greater extent right
so I do this with a perfect solution for
that because it can scale very much in a
very good distributed way okay so let's
go back and let's do a small MapReduce
example now let me show you a small
MapReduce example so technically I'm
going to I'm going to show you an
example of how to process structure data
and also I am going to show an example
of how to process a totally unstructured
data okay
so meanwhile I will take some of the
questions is there any major changes in
Hadoop 1.1 recent releases there is a
complete change
it's not even a major change there is a
absolutely
find out that's a complete complete
change in the religious okay I want to
perform the immensity production of
images I'm going to show an example of
unstructured data with images kashish so
I'm going to show an example okay
apart from previous like Linux we should
have now before starting learning about
what is the way to learn it so be sure
that's you can always join the Erica
cause we will guide you the exact way of
how we can be done okay we will provide
you the nice knowledge on basically how
to handle most of the things fine assure
I'm just I just show a very simple
mappers then I will go for a very
complex way of showing the Map Reduce
example next example I will take you go
through one of the unstructured friends
of you singers okay so first thing I am
going to use a very simple MapReduce
example of showing out easily MapReduce
can be done without buffered so
MapReduce is one of the data analysis
algorithm which is coming in built with
our ok it is one of the frameworks so as
I said you can you can broadly classify
into your Hadoop into three things so
one is something called as data analysis
and one is something called as data they
are storage and one is something called
s you're basically gaius Isis okay so
there are some ways something where the
data loaded into HDFS right so we talked
about how to load the data into HDFS
exit all those kind of things so data
analysis is something where we are going
to do some processing of data so what
about the data you store distributed you
are going to do that in the other phase
in something where your machine learning
algorithm comes up in the picture like
mahute exit or rush of the things right
so this is where your mouth machine
learning and accept all the things but
the most interesting thing is this part
can be used in all the three okay this
pack is one of the most important
features which can run on top of a loop
and it can basically solve the problems
of all the three in one shot okay so I'm
just
when you take the processing of data
which basically the initial way of
processing it as a MapReduce algorithm
selves I'm just going to show you a
MapReduce algorithm how Hadoop runs I'm
just going to give a document to a dupe
and trying to find basically how many
words has been repeated okay so once we
did with that particular application
next I will take you a completely
unstructured data maybe we can take some
ten images and try to find which images
duplicate it can we tell off your
MapReduce algorithm right so I'm not
going to discuss much on MapReduce
algorithm just maybe I can give a
smaller introduction MapReduce algorithm
is just nothing a concept of mapper and
reducer process so when you say mapper
and reducer process the data will be
split parallely into multiple smaller
sections and each smaller section will
be processed separately by a mapper say
once all the mapper has completed its
process the data will be combined and
given to the reducer so reducer is the
person who is going to take all the data
and process and finally go into produce
eraser so for example in this particular
thing so let me take the your
distributed file system
so this sorry so if you look at this
particular data set right so it's just a
normal day decide which is like a fine
system inside your Hadoop
so what what basically the mapper and it
is just essence I'm very in a very
simple way so mapper is something which
is going to run in parallel so for
example each and every line can be given
to a separate mapper on the least
scenario on a very least scenario so
what are the line you are you have five
lines right you can ask the mappers to
start processing all the file i'ts
so that's why Hadoop is so much powerful
so if you have fine lines of input you
can start five mappers and all mappers
can run family and you can go and it
means you can go and suggest what your
mappers should do right so you can
you're all you have to say is that you
have to write a code and tell back what
the mappers should do so that's all you
as a developer you should say right so
even if you're going to have some now
let's consider one like record or maybe
if you went you and I have some more
than a lakh record right even in that
kind of scenario if you look back you
can run any any number of mappers you
want ok there is there's nothing much
there's no restriction like you cannot
you should have only this much number of
mappers ok based on the availability
Hadoop can provide
how many mappers it can place on the
systems are available T etc so that
processing one lakh lines are processing
a 10,000 lines instead of processing in
a sequence way of going line by line so
Hadoop varies can process everything
parallely so it can split hundred lines
into ten line seeds and you can start
working on ten ten lines so once all the
ten lines has been done so once all the
parallel process has been then the
output will be collected and the output
will be collected and given to a final
section called reducer where the final
section of the output will be net so for
example in our particular scenario each
and every word for example if you had
five lines
and each and every Lane might add I
would call hello
so hello might have repeated in each and
every line right so once all the mapper
does this process the release or steps
up and basically combines all the Lo and
says okay that you have repeated for
three days that's so basically mapper
and reducer work so by either mapper
takes responsibility of running each and
every smaller section of data why is a
very so takes responsibility of
combining the map versa and dumping the
reader okay fine so where our mapper and
reducer events everything runs as part
of your Hadoop cluster okay a cluster
desiring a master and slaves so all the
mappers and reducers runs on slaves they
run you find prints basically run here
so that's what their primary intention
they are going to run your actual so if
you if if basically say for example if
I'm going to execute the same job so let
me if you open up the Hadoop shell and
let's execute the same job so maybe
after this example I can this is a
complete structure data so very simple
operation the next thing the next thing
what I will be basically going on is
after this particular example I'll just
show you the image files ok all the
image file works except other things so
just I'm basically executing some of the
basic Hadoop commands so that it makes
sure the data is there except all the
things so first I'm just making sure the
input file and output file everything is
existing so that's all I'm doing there's
nothing special than this so let me run
the job ok so I'm going to run the job
off I do since it is a Java program so
everything will be part of a jar file so
I am just running SH off I'd suggest I
am trying to process all the words and
I'm trying to find how many times each
words has been repeated
in the duration okay the same thing with
spark will will finish faster okay it
will definitely finish faster compared
to this so that's not one danger of
having spark because a spark runs
everything in memory okay so basically
when you look back and see the sequence
of output so when you go back and see
the output now everything again comes as
part of the HDFS file system yeah when
you see at output
it basically says it has process the
complete data which has been provided
this data can be of any size that's what
the point I'm trying to say as an
example we have taken only last you know
even though it's going to be a data of
any size it's going to X it's going to
perform in the same way okay because in
a typical production system you have a
large cluster which is expected to run
faster right
and it basically tries to tell us each
wood has repeated how many times okay
simple I said a very simple algorithm
it's a very simple concept where we are
basically running running the things
parallel even in storage as well as in
processing right maybe the next example
I am going to show you a completely an
unstructured data something called as
images we are going to process the
images okay so SPARC is for streaming
devgarh does it perform well for data
which is not live stream a spark is the
only best solution currently we offer
live stream causation the current
technologies the spark is one of the one
of the only best technologies which is
puff which can perform like shredding
apart from spark nothing nothing can
perform the streaming better than spar
no other technology currently in the in
the industry can't perform equal in T to
spark and no other technology saving you
good equal to spark frankish a so it
will perform okay so going back to the
slide again and so another usage of the
reason for using Hadoop yes it's a much
it is it can be used with along with
multiple frameworks so I can use it
Sparky can use with Python he can be
with MongoDB except type generally can
be used with lot of lot of things
right okay right
and so one more thing
Hadoop itself is having something called
as internal SQL query tools like hive et
cetera okay say Hadoop basically uses a
Caribbean technology something called hi
I use the most popularly as popular wise
SQL query based which we have and again
now as I said that is something called
spark SQL but Sparky school is something
again going to run on top of five okay
and so once the moment you join the
coast will be taking you all the sticks
what basically in hive Korea except all
those kind of things okay
if you are using the SQL table as input
file in what format is the MapReduce
perform so if you are using an SQL table
as input file so either we have
something called scoop scoop takes
responsibility of transferring the SQL
table data into HDFS so from there
MapReduce picks up defines whether
that's okay
difference between spark an SAP on a
completely different jet because Park is
something which can be used along with
spark can be used for normal processing
spark can be used for streaming spark
can be a spark is something can be used
for machine learning spark is totally
different it's not in a smaller
perspective okay but how does not
underline structure data yes I'm sure
yet I'm going to show you caches as I
said I'm going to show an example of
instruction reader
okay hi Arpi cannot handle instruction
it okay you if you want to handle
instruction header you must want to use
MapReduce there is no other option okay
yeah was use of a floom floom is again
one of the technologies which came
before then spark means for processing
live streaming of data in our course we
cover flume to have a concept on flu
telling you how how the live streaming
of data can be performed with flu okay
oh but storm come back to spark nothing
much only one thing I can say strong is
more matured in streaming spark is
definitely fastest growing technology
but
storm is more mature
in streaming but storm is only supported
for streaming and sparkies can be used
across everywhere okay Sparky is better
than a flu main processing live
streaming so uh to everybody I think
there's a lot of questions coming and
spark so what I suggest is one thing I
can tell you all is that spark is the
bettering everything listen you can't
compare anything it's path because this
is the one of the fastest clean
technologies in the world and for more
questions of spark I request all you to
go through this path maybe not on the
same we have a separate spark webinar
you can request the support team about
spark okay so so please please okay so
please verify with the support in
delegation but definitely there is no
comparison with spark spark is always
the best a better frame of compatibility
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>