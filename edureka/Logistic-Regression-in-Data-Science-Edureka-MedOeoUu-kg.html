<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Logistic Regression in Data Science | Edureka | Coder Coacher - Coaching Coders</title><meta content="Logistic Regression in Data Science | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Logistic Regression in Data Science | Edureka</b></h2><h5 class="post__date">2015-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MedOeoUu-kg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi there good morning grieving to all of
you from different parts of the world my
name is Annie Ruth color and I'll be
taking this webinar on logistic
regression alongside launch degredation
will also cover few basic concepts on
our and what is data mining and machine
learning what is the data life cycle in
a machine learning paradigm and how
exactly do we productionize a machine
that existed okay so let's start so
basically this particular webinar is
going to focus on how data science is
changing the behavior of Industry second
what is a data science project life
cycle in a production environment and
thirdly the tools and the until Gotham's
that constitute this new environment
call data Sciences and we will pick a
one and Garthim and we will see how
exactly we implement it with a real-life
case study okay so here we go so
objectives of this session would be that
after the session you should be you
should be knowing what is data mining
stages of data mining what is our what
is data science is what is needed of a
data scientist roles and
responsibilities over in a scientist and
last but by no means the least the
logistic regression exercise so the
actual exercise that we're going to do
today is going to focus on logistic or
LOGIN reparation
so there are since applications whine
recommendation well in in specifically
in France and in the northern coast
the eastern coast of North America you
will find a lot of people who have a
career our which relates to wine testing
and those wine testers are really highly
paid guys so they taste and they'll
figure it out if the the taste of wine
is good or bad but based on that they
also can also tell us what kind of
people would like the kind of file
so data science is helping figure out
who are the set of people who would like
a particular wine and let's say if I
like a wine of quality eggs what a
chances that I'll also like the wine of
quality or taste why and there I'm
making a recommendation engine which
helps in which helps in figuring out
what to be served to home well we also
use in assigns to bread accidents can we
actually do that is there a real core
causality or a correlation that can help
us predict accidents well the answer is
a yes we can do that and that is exactly
the exercise you're going to today are
using launched in regression so just
feed for that stages of analytics and
data mining so basically in a production
environment there are few stages that
you need to be very careful about now
this is not this is not a pretty much
similar to a software development
lifecycle so the idea is saying in the
idea is you know you taste something and
you evaluate it and then you you know
finally productionize it but these
stages are different so one stage which
is really important is data
understanding so for example the case
that I was discussing with you on the
excellence prediction we actually need
some kind of a domain knowledge also to
understand can that really be a
possibility to predict well in scenarios
like epidemiology or I will click
trading where you can figure out if a if
a particular stock is going to go up or
down
in 0.1 seconds then you can do or you
can sneak out some arbitrage in that
particular trading so domain experience
are the data understanding part is
really crucial because if you don't know
what exactly is lying under the hood
then you cannot figure out how we can
use the data
now think of an example where you are
supposed to figure out that offer that
you're giving to a particular person
will you redeem that offer or not then
you need to know what are the habits of
the purchase habits of that person then
you also should know what domain puts in
where so basically do you know the
retail supply of a retail supply chain
of a particular product do you know the
inventory of that particular product do
you know the market of that product do
you also know the customer segmentation
for that product
all these questions can be answered by
someone who comes in with the domain
experience on that particular domain and
thereby gives us the understanding of
the data the next is data preparation
now we already have the data then why
the hell do we need to have a data
preparation well the data preparation is
is something like where you figure out
the hidden data points now think of it
I mean think of the same example I just
gave where you need to figure out
whether a particular person will accept
an offer or not and let's say you were
given the entire history of his customer
office purchases in the last one year so
let's say you have an offer on a honest
on a soap then figure out how many
number of soaps
how many brands rather for up for soaps
has this person bought in the last one
year now that's a different column all
together now getting that column can
help us figure out if this person is
blind savvy or not so basically using
the same data set you figure out some
more attributes about that person that
is called a data preparation plus
cleaning the data is also part of data
preparation exercise also putting data
into different formats forms you have to
change something to a number or
something into a character so all that
data cleansing and data massaging part
happens in this data preparation stage
then comes the stage of modeling now
what is modeling
well and whether they in the world we
call it a modeling step so basically
what we do is we create analyte
analytical models which are nothing but
state-of-the-art algorithms that unknown
in the in the in the industry of data
sciences so regardons
could range from big complex neural
networks to as simple as in a regression
and all these regard them constitutes I
mean the algorithm that you put on on a
data it could be one it could be a
mixture of multiple models the multiple
Grantham's that will create a form
something as a model and that model is
something you can use for doing some
prediction doing some recommendation
doing some target analysis and once you
have the data model prepared you test it
or you evaluate it so basically you test
it on the data set you've not seen so
far and then you evaluate the
predictions of the model with the actual
values of the column that you are trying
to put it on in the evaluation stage if
everything is fine you go to the
deployment stage if your evaluation
criteria is not meeting up to up to the
expectation then either there is a
problem with your model all is a problem
the data preparation or you've not got
the business understanding part
correctly so you go back to business
understanding again and see if it's not
a big sense with the output that you
have gone from this entire exercise if
anything is fine we go to the
development stage well the components of
data signs by the way in case you got
any questions we'll have a session post
you know I compete all my stuff just for
the QA so if you got the question just
write it down and I'll take it after the
class so there are five very basic and I
would say it's not in fact the
components not the pillars if you miss
out on one you actually miss out on the
rest four so we inside with data
engineering now what is data engineering
I mean it could be
is it something similar to what we've
been doing in the ETL or the extract
transform load paradigm or something
similar to the VI the business
intelligence will some what that is and
that is because we still treat the data
the way we've been treating it just that
we massaged it in a different fashion
we might not put into facts and
dimensions we rather put into informant
we want to you know we want to rent on
then there is advanced computing well I
am sure you guys must you guys have
heard of something called as Hadoop
which is currently the most popular
framework for advanced computing rather
it's not at the one's company to be
honest it's more precisely battled
computing so parallel computing is is is
like when you've got a task you divide
into parallel runnable modules and you
put it on something like Hadoop or a
spark which is the new you know buzz
were in the market and do some really
advanced computations which were not
possible previously all they used to
take a lot of time the third part is
statistics so statistics plays more than
important role I think it's the
heartbeat of of analytics it's the
heartbeat of data science everything
that we do relates to statistics so if I
say Martin I don't really mean the
algorithm that you probably would would
have heard of while doing a course on
data structures because that's more of
data management you are them where you
manage the data in such a way that the
retrieval and their sorting and their
insertion is pretty optimized over here
we are trying to figure out something
statistically so all the algorithms that
we run there starts to be proven in fact
they are they come up from the branch of
statistics called statistical learning
then visualization well of course
whatever we do we need to present in
such a fashion that it makes sense so
with with data Sciences we go beyond the
basic visualization that we've seen in
the BI industry with
with tools like QlikView and tableau we
do have advanced visualization but in
statistics when you work on something
which which which grows beyond realm of
two dimensional three dimension that is
where you need something like more
advanced visualization that you can do
in our and there are some libraries like
d3 and finally which is again a very
important part is called the domain
expertise so you need to know the domain
for example I'm a data scientist and I
work mutually with the life sciences
data so i'm also an epidemiologist so i
have to deal with you know columns with
data set with you know with more than
10,000 columns or 20,000 columns and
that becomes really difficult for me
because running a analytical model with
20,000 columns is virtually impossible
so i need to have some domain experience
to figure out what could be more useful
in this entire set right let's move in
so components of data sense again so our
programming language what possibly could
are do here are is a programming
language well the answer is yes it is
similar to Java the answer is again a
yes because it has Oppo PS it could be
used as a o-p-s language but is it main
for the same purpose as Java the answer
is a big No
so we don't build we don't build any of
the of these scripts are our programs in
are just to serve a purpose of an
application development rather most of
the stuff that we do here is for
statistical modeling so R is also called
as in the statistical modeling language
then this this small picture that you
see is of non studio a screen where
you've got where you can write of
scripts and execute them in a console
which is a the bottom left of this image
on the right bottom you see a plot so
basically allows us to run all these
kind of plots they look really difficult
at times and the difficult sense the
to comprehend but they are really
meaningful and all these plots can be
pretty easily generated and are well in
a sense demand supply gap why the hell
should we think of switching our careers
to data science and big data and all
that stuff well I don't know if you've
been to Gartner I've been to Gartner
like thrice thrice specifically for the
BI and for big data and data science is
one skill that Gartner consistently
advocates on and that's because
industries are now figuring out they've
got data and they can do something
really important with the data so what
exactly is that is that a data scientist
will do where data scientists can
channelize you're not usable data or
unused data into something is really
meaningful so think of a scenario where
you got Rickard data about your parking
in particular mall let's say you got it
about which car was powered at one time
slot and when and what was the pitar car
type and all all that information about
the car can that be used by a store that
is built in that same on a retail store
or retail outlet off that's a Walmart to
predict what is the amount of ice-cream
breaks I need to store in my
refrigerators based on the information
that I get from the parking lot but of
course they have that information but
retain assigns now they can find
correlations that can help them
understand or come out with a number
that will ensure that they don't fall
short of these ice cream mix well in the
current scenario the industry currently
is in a big need of data scientists
which could be in fact and and all these
version of you see data big data
visualizer big data research analyst
very de engineers architect and analyst
whatever all these guys are actually one
or the other form of data scientists and
a data center is supposed to be actually
a combination of all of them
so you need to know how to hope you need
to know you need to know Big Data you
need to know no sequel or columnar
database like MongoDB or HBase which is
more columnar then you also need to know
the statistical power of it which is I
think the most difficult part the
gardens the data preparation that is the
data visualizations all that would
constitute you know the the work or the
skill set of a data scientist and as you
see currently only less than one third
are our you know other vacancy that have
been filled and companies are a to pay
higher salaries just because this is
very rare scale and this is known about
learning yet another technology with the
data science you need to master some of
the math need to master some of the
linear algebra you know master matrices
you need to master machine learning you
need master programming then there is
Hadoop and then there is spark and so on
and so forth so Hadoop and are together
so with our there is a problem of RAM so
are basically is in memory that means it
fits everything in the memory and then
runs so if the data set is not small
enough or it's not it's really big that
you cannot fit in in your RAM then you
need something like our in Hadoop
combination where you can run different
folks or different threads of the same
are processes on different machines that
is provided by a technology called as
revolution in analytics revolution
analytics body was bought by Microsoft
this way in the very starting of this
ear revolution analytics has done
nothing but if combined are and Hadoop
using a package called our loop which
the combination of our mr our HDFS a
knowledge base which made something we
not to worry about right now but a
llamar analogy are you deep as are the
ones that allows us to read from HDFS
which is the Hadoop distributed file
system and runner our program in
parallel on all the machines
obviously there are limitations to it
because the algorithms like
new network or algorithm like a
regression linear regression that can't
be done that can't be made to run in
parallel so think of a Fibonacci series
so that starts it starts with 1 and 1
again then 3 and then 2 then 3 then 5
and 8 and 13 and so on so basically it
sums up the last two numbers and comes
up with a newer one and then it sums up
the new one and then the second last one
and then comes with a new one so
basically so basically we don't have the
option so we don't have the option of
running it into panel because the final
step or the output step is based on is
based on the the the first step right so
that's that's one big challenge with
parallelization so not everything can be
parallelized but there aren't certain
rhythms that can be made to run in
parallel like support vector machines
and random forests so we can use them
within this concept call or a do and
thus minimize the execution time
well now machine learning in machine
learning we actually we actually don't
go beyond the data lifecycle that you've
seen the only difference is with that we
start is with a different data set so
basically we have an initial data set on
which we run and train our models so
this is called as a training data set so
whatever is going to training data set
that you wantem will we will understand
it so for example if you provide wrong
data it will give you wrong answers so
think of it in this way that you have a
image of a duck and you tell the ink on
the computer there are the algorithms
that it's a dog so the algorithm will
figure out this these are the particular
features of this image which is now
framed as dog so next time when you show
it they learn thumb the image of a dark
it we'll call it a dog okay so garbage
goes in and garbage will go will come
out okay
so that's called as the training data
set so we run our run our problems or
run our algorithms rather on the
training data set now we have got a
validation data set for which we know
the answers again but we ask the model
to predict the values for us and based
on that we get our predictions accuracy
now once we do that we run it on another
data set called as a test data set or a
primo data set so the purpose of this
data set is to keep it totally
independent from the one from the
developer who actually created those
leave anthems on that model and run it
in the production environment or a UAT
environment where the developer does not
have access to any of the ten scenarios
so machine learning focuses on the
development of computer programs that
can teach themselves to grow and change
when exposed to new data machine
learning basically has two components
what is supervised learning and the
other is unsupervised learning so one is
supervised learning Ivan I don't want to
actually go through this paragraph but
think of a case where you as the problem
is you have to figure out whether a
particular bank account will turn into a
NPA or not okay that's the problem so
first you need a data set that tells you
that these are 10,000 records and all of
them were NPAs and these were the next
10,000 records and these were not NPS so
then Gotham then figures out based on
the label which is MPA or not NPA that
this kind of a this kind of a feature
when used in this particular fashion
converts into a NP and vice versa but
let's say you don't know the the actual
the actual labels well that's when you
don't have the Y and that's form of
clustering so majorly what we do an
unsupervised learning is clustering or
dimensionality reduction
we'll see it once we get into the lab
well the common machine learning
algorithms so basically we've got two
two categories - supervised and
unsupervised that's what we are learning
then there is one called as regression
then there is one called as
classification then on this one but
instead is called as clustering and
dimensionality reduction and the file is
called this and final there's some more
cateura categorical unsupervised
learning algorithms which I'll call as
Association analysis basically figuring
out if X is what then what are the
chances of buying Y so basically in
supervised learning we do regression and
classification so regression is figuring
out what is the numerical value of a
problem so let's say there is a
prediction analysis and you figure out
what is the salary of a person okay now
that's a number that you to come up with
it's not high or low if not 1 or 0 so
let's not classify into anything so
basically when you predict a numerical
value it becomes a regression problem of
course the way to approach a regression
problem is is different - made to is
different than way to approach all a
classification problem now of course the
regression could be linear regression
cubic polynomial regression we'll see
that then decision trees random forests
well these are state of the art you know
rhythms classification is when you want
to classify a particular record into
categories and PA not n PA is a
classification algorithm except you
offer not accept the offer is a
classification algorithm good/bad
high-low all these are examples of
classification algorithms where you want
to categorize a particular record a
particular document a particular entity
into one type or the other then in the
unsupervised we got
a string and dimensionality reduction so
clustering is basically figuring out the
clusters within the data set so let's
say you develop Milgard them develop a
way to figure out how many what are the
salary standards of a particular country
so let's say you want to divide the
entire population to high or low but you
don't know whether particular 70 is high
or low so what you do is you divide the
entire population into two clusters so
one cluster is for the high income and
the second cluster is for the low income
and finally income to an association
analysis and the hidden Markov models
well these algorithms based on the
market basket analysis and you know if
one then two kind of a concepts okay so
now we come to the largest expression so
in statistics logistic regression or
logger decoration or logit model is a
direct probability model so I believe
that all of you understand what is
linear regression in case you don't so
let me tell you that a linear regression
is the one which has this one y equals
to a plus B X so Y goes to a plus B X is
the standard equation for linear
regression classifier oh sorry
linearization model so what is X X is
your independent variable and Y is the
dependent variable so X could be a value
it could be a variable to be matrix or
vector what is B B is the coefficient so
y equals to a plus BX is actually the
equation of a straight line in a 2d
geometry so a is the intercept and B is
the slope of that line so basically we
determine slopes using concept balanced
least squares
and that is how we figure out the
optimum value of y by fitting a curve a
straight line in a linear regression in
logistic regression we use the same
concept but we apply a sigmoid function
a function whose value is f of X is
equals to I'm sorry I'm writing through
my mouse pattern I'm at it's not that
great 1 upon 1 plus E power minus X so
if you now X in this case is our y equal
so F of Y is equals to 1 upon 1 plus E
power minus y and that is nothing but Y
is equals to a plus B X so we get 1 upon
1 plus E power minus of a plus B X that
will come down and finally we get P of X
which we define as a probability of a
particular event as ei upon E power a
plus BX upon 1 plus E power a plus BX so
after some calculation you'll get px
upon 1 minus px is e power a plus BX so
the quantity px upon 1 minus px is
called the odds of an event and KN can
take on values between 0 and infinity
that's because it's raised to the power
E Plus 3 upon e power it will be X well
values of the odds close to 0 and
infinity indicate very low and very high
probabilities respecting respectively
for any vector so let's say if you get
the value of odd as 9,000 865 which is
not a small number we say the value of
the probability of that event is very
high whereas if we get the value which
is like point 8 4 and you'll say the
value of of the probability of that
event is very low so when you solve it
this equation again you get I'm sorry
you get log of P X upon 1 minus px
equals to a plus BX so you see a plus BX
is the equation of our linear model and
that is why this particular function is
called as the log function or the logic
function on the log odds and that's why
we see that launched iteration is linear
with respect to X well this is the
elastic regression a sigmoid function it
has some very interesting features you
see so it is the value is closed between
1 &amp;amp; 0 and it crosses x equals to 0 at y
equals to 0.5 so when x equals to 0 y
equals to 0.5 so that means if you are
looking to figure out whether a
particular count will be an NP or not
then we just need to see whether the
value of the sigmoid function that we
are getting for it is it greater than
0.5 or not it was greater than 0.5 then
we say that that for all those value of
x the category is blah blah blah and for
the rest of them the category is blah
blah blah okay so we do lot of machine
learning a course in Arica and release
their modules of that particular course
we do basic reading emulation through
our we do lot of stuff in text analytics
we do we go through the state-of-the-art
machine learning algorithms right from
rainforest naive Bayes finally we go
over to integrating Hadoop and are the
auslan settings and stuffs on route and
machine learning in Hadoop and finally
turtle processing and our and then we
power project well
that's exactly what we've been doing so
far as part of the Renaissance course
and as you recall
well let's the end of the slides well
that's not the end of the session now
we've got to go into the hands-on
exercise for nonstick regression so I'm
going to open up my our session I hope
you all can see my ask our session
let me see God if you've got a question
it seems like some of your board
questions
so let me see how many questions you've
got okay okay so I see some of you guys
have the problem are very wise but it
seems that
which is it seems that uh some of you
are able to hear me okay so we'll get
the questions after the session okay so
let's start with our okay now the
problem that I'm gonna solve is
regarding a organization that wanted to
figure out that can we can we predict a
particular trip as risky or non risky
risky means whether there are chances
that that the the trip will have an
accident okay so this is a power company
that transports helium in big containers
and any blast nearby or even a collision
with that with that trunk on that
container can cause even bigger
explosion explosion that can go up to
100 meters which is huge so they want to
insure and they want to be extra
cautious about about figuring out a
possibility of an accident okay so we're
going to use some libraries one says
debris scattered so oops
ah I'm gonna so the moment I enter here
I see the output on my console so I have
entered this calc library and I've
entered into the card library's own I'll
use plier
I don't need spines and iPad so I leave
that so I've got a trip data set let me
let us see what is this trip data set so
we are reading from a CSV that's on a
machine so let's see what is there in
the trip set is it well if this is the
trip data set source is the is the one
who is actually creating the scheduled
distance is the distance of the the
triple supposed to make cycles like is
it good to be around cycle or is going
to be a one-way cycle complexity is
based on the highways traveling on then
cargo is it carrying a cargo or not how
many were of stops does it have in
between what is it the start what is the
month in which it's starting what is the
day of the month what is the day of the
week for example if someone is working
on a Saturday on the last week and the
chances are high because the compression
is already tired walking through all the
week start time is in seconds
then then this experience level ah in
number of years the number of months
so that's into pilot experience pilot to
in case you go out a pilot to okay that
that means you've got another driver
along with the main driver then how many
number of ours did the the pilot or the
driver drove in the last trip okay then
pilot duty as previously then the
distance the the route risk and a school
one Andrew Tristana to underscore to so
basically they were a risk measure to
every route and there let's say if there
are two routes to reach to a particular
particular destination then this could
be the route risk is the route risk is
the one for the first first route and
the second one is for the second route
whether it's is it good or bad is like
Cody
two one and zero visibility is taken to
numerical factor they have quantified it
using a number but is it a good or bad
visibility on that day then we've got
events like acceleration count the
acceleration count speed count stability
count even count so if we have any of
these value equal to one or more then we
define the trip as SK now clearly this
is a supervised learning algorithm okay
because we already have the risk
involved marked as yes or no so let's go
ahead so first of all I create a matrix
start matrix so I've just show up just I
just chose the column that I need for my
bonus prediction I am now creating
clusters using algorithm called k-means
cluster right now I'm in this way as
Center scale in my data set scaling the
data set means normalizing the data set
so basically you did deduct the standard
deviation and you divide by so you
deduct the mean and you divide by the
standard deviation then you create ten
clusters using algorithm called k-means
with this you find the Centers of those
clusters now you find the results of
those centers into a column called
results and then you find the distance
of each point from the center and then
you find out which are the most distant
points which we can point as outline so
I've chosen one to 1000 which are the
outliers lies in case if you feel like
this is not being much of a sense it's
fine because if you not from our
background if you not working on on any
machine learning algorithm so far it's
going to look like fringe but my idea is
to show you how we run things and how
practically or how you know how hands-on
are V with respect to a course okay then
we use we convert the the trips called
the trip data set into factors factor is
nothing but any any variable which has a
value other than number so let's say
male/female the height lower high could
be the variables which we can call as
factors so now I'm running something
called as a principal component analysis
which is reducing the number of
variables so so we've got we've got 23
columns but we can use only the first 13
to figure out those which gives us a
accuracy of point nine four to figure
out further predictions so that's the
beauty of that's the beauty of principal
component analysis of course we need not
to go into the reps of it see I did say
that we are running a supervised
learning algorithm but I've used two
Kwanzaa which are non supervised or
unsupervised which is principal
components and clustering okay
so I create a data frame with those
principal component has a principal
components and now I create now I create
something called as the case and the
control dataset so the problem with
accidents or problem with certain
machine learning problems are that the
act the required events are pitiless so
think of think of figuring out whether a
particular person will have a cans will
have cancer or not so we need datasets
with people who had cancer and Dana
certain people who who did not have
cancer so cancer is a cancer is pretty
common nowadays but think of it as very
rare disease and and and you know think
of it as as like 100 kind of a disease
so basically what you've got you learn
100 people and only one guy who's have
cancer or let's say you think about the
click-through rate so basically you are
given a you are shown an ad now if I
need to figure out the probability that
whether you can click on this ad or not
so to do that you need to first figure
out the features of those guys who click
on the ad now the ratio is
in 1001 thousand Forks only one would
click on the ad so basically the number
of actual cases are pretty less so what
we do is we do a sampling studies called
Isaac we run a something that is called
in the case control sampling strategy so
in this one we take all the data where
we did find a risky data set and we take
a part of the data where we find non
riskier assets okay such that the ratio
is not more than one is to five now we
create a training data set like this and
now we created testing data set so
basically the trip data set that we used
we divided into two parts one was the
training data set the other was a
testing data set so we run our model on
the training data set which is the
generalized linear model and when I say
family equals to binomial it runs a
logistic regression and I'm using data
constrain to train my model and here is
the name of the motion even the model is
mo de l pretty fast right now let's make
some predictions when I say type equals
response let's go to give me the
probability so let's see what does it
give me Fred
so it gives me some numbers 0.171 6-1
blah blah blah blah blah so basically
what it has given me is set of
probabilities okay for a particular
event but what to my beauties
so when I say 0.17 that means
probability for event is 0.1 7 and the
probability for the probability for one
class is point 1 7 so the probability of
another class is 0.8 3 but see which
probability is point 1 7 corresponding
to using this function called contrast
so it says for risky the probability is
is 0 so sorry for non risky probability
0 and for risky the probability is 1 so
what so so that means risk is the higher
side and non risky is less is like
I like on the lower side so what I've
taken is I've taken the threshold is
greater than 0.19 so in cases where the
probability value was greater than 0.1
nine I've marked those trips as risky
and anything greater than point one nine
so in less than 100 nine I've marked
OSes as non risky so how did I get this
numbers called 0.19 so basically I knew
that the ratio of my my actual risky
records / my total records was somewhere
around 0.1 line okay now we create
confusion matrix to see what was my
final prediction okay well I get
something like this so out of so six and
five zero nine are the records which the
agarthan said were non risky and they
were actually non risky eight 8 nine was
the records which the algorithm said
would non risky but they were actually
risky then three zero force and one
other acorns which the algorithm said
they are risky but they are not risky
and finally 747 are the number of events
that were risky and the algorithm also
figured out that it was risky okay so so
that's interesting isn't it because our
accuracy is like the total number of
risky trips were 889 + 747 and the total
accuracy is isn't that great right let's
see what's the Krissy is so and figure
out the accuracy which is seven four
seven divided by seven four seven and
eight 49 which is somewhere around one
three one three two seven four one five
three seven so let's mark it one five
four zero oops
I would see let me do it again
seven four seven divided by
one five four zero and we get an
accuracy of around point forty eight so
is that a good accuracy well that's a
mighty good accuracy if you look at it
because if you call setting trips that
were not risky but you call them risky
that's fine because then you can take
extra measures but it's important that
you are able to actually predict 50
percent or somewhere near to fifty
percent of the trips that actually is
key and you should be able to mark them
as risky so that's a pretty that's
pretty lenient or very a pretty
acceptable accuracy rate let's see the
summary of the model summary of the
model is it used thirteen columns from
pc1 to be studied and these are
coefficients so these are nothing but
the coefficient that we saw here so
these are those coefficients a plus BX
so a is is the intercept which is this
and B are these different different
variables so it's like minus one point
five four six one plus point zero zero
six minus point zero zero six into x1
minus point zero two nine into PC two
minus point zero two five into PC 3
minus zero point zero one six seven into
PC four and so on
and that's how we construct a model okay
find them so let's start with some
questions and you all can you all can
put up your questions now and let me see
if I can answer those questions right
now okay so they are mighty I mean the
plenty of question that you guys have
already asked so what is clear and clean
is your K nearest neighbors but I've not
used it Kennon basically figures out how
are your neighbors doing so for example
you've got a got them in you to figure
out whether a particular person is an NP
or not you find the record that are
nearest to that record and
and you try to see that you try to see
that what is the output for those
records then the question is what is the
advantage of using or against mahout
well R is a R is basically the statistic
modeling languages language mahout is is
running a statistical algorithm in
parallel or Hadoop okay
so well there are advantages of running
our R can run most of the items Mart is
still building up it does not have the
the the elaborate e the advantage or the
capability offering all those algorithms
I want to cry and scream from machine
learning so do our pack yes of course
you can apply SVM using a package for e
1 zero seven one there's another package
called currently lab that you can use to
run SVM we don't need to learn Hadoop
spark for a redesigned is well it's not
mandatory I mean it's not the primary
skill set but it's a now turning out to
be a very important skill set so the
answer is that first mastery or
statistics and then you go further than
at our for Hagen Spock how do you recite
the thresholds 0.19 well well accuracy
of 0.1 9 so the threshold point one nine
was based on the number of records that
were there in in the training and said
which was like so I had around 19,000
records which were risky and rest of
them will not escape how do you do mine
which attributes to include inflection
so that's a very good question and uh
there is no direct answer to it okay
there are multiple ways so PC one is one
look the the principal components and I
used there multiple other ways when you
find the correlations you find which a
particular set of columns are more
helpful and they algorithm still do that
so so it's one the most important
important important aspect of machine
learning and this actually is the one
that makes so Mario in tire model how do
we invite we normalize the data so we
normalize the Drina because a
in k-means clustering we figure out the
distances between two will garden to
records so basically if let's say the
features are salary for person at age of
a person okay so let's say sell you for
person is ten thousand an age of person
is 20 and then unless another person
with age I was twenty thousand and
salary as an age sorry
sadly is twenty thousand and ages twenty
two so the distance between these two
guys are huge because you when you when
you when you find the distance that's
twenty thousand - 1000 whole square plus
22 - to 20 whole square under root so
basically your age does not help in the
distance it's majorly the salary which
is figuring out the distance so you need
to normalize the data set so that each
LVH column has a say has a value has a
proposition to make
in the final model when to choose
elastic relations as well in which
languages Lang is the machining
algorithms written so see in our the
rhythms are most written are only but
the underlying language is C++
well we there are certain visit work
with Java they are certainly that work
with C++ but I'm a chill use are in
Python so ironpython are the most most
you know widely use a languages for for
machine learning when to choose Gnostic
relation means in which class case I
should okay let me just read it
completely okay so when to choose
Gnostic relation means in which case I
should choose the auscultation
production I were there's method like
random forints see there is no right
answer to it again okay you there are
certain properties of a logic regression
which makes it better than a random
friend so Gnostic regression based on
maximum likelihood
whereas random for instance based on the
vote based on the decision trees okay
so it's actually quench it's basically a
question when you move if let's see if
you can do things well with a linear
regression or a linear classifier then I
would go for a non stick generation but
if things but it is linearly not
separable then I would go I would want
to go for our random forest
svm or any ensemble how can you say it
is less than 0.5 is pretty good accuracy
so it's effectively if you see a deep
right
no that's Lakshmi so lecture if you see
that a deal Gotham wait if you see my
accuracy actually is not 0.945 it's
actually the a the specificity
okay so specificity is like point five
four five okay so someone is raising
hand so what about okay let me go back
to the questions so the accuracy is 0.45
0.46 whatever that means out of all the
ones that were risky 0.45 is offered a
person with forty five percent accuracy
I was able to figure out the three ski
trips my overall accuracy is point six
eight okay it is just the the specific
accuracy or the specificity as we call
it which means if let's say if I if I
was running a random classifier okay so
a random classifier will give me a very
low spaced accuracy my sorry specificity
my Christy would probably go up maybe it
would be seventy or eighty percent or
let's say I call everything is non risky
if I call everything is wrong risking I
am bound to have accuracy of to eighty
percent but my specificity would be zero
okay okay so my specificity is something
that I want to bank on only in short
that I do a good job as compared to a
random classifier so a random classifier
will not go beyond five percent okay of
course if you've not a few if you solve
these kinds of problem or you will
understand why and where the accuracy
beyond 0.51 something close to point
five is really good because that's
specificity and that's not accuracy so
let's say let's say you got a person on
our webpage you want to figure out
whether this person is going to click on
my ID or not now I already know that out
of thousand only one click on it so I
call this guy also as as not clicking
and let's say there were ten thousand of
the guys on call all them
as as not going to see me click on my
hand so outlays ten thousand thousand
expected as a hundred people will
actually click off ten people will click
on my ad but since I call every one of
them as known as someone who's gone not
going to visit my ad then I also
accurately classified nine thousand nine
and ninety miles because you are not
clicking so my accuracy is ninety-nine
point nine but that's not what I'm
looking at I want to figure out out of
those ten how many can I can I
accurately figure out and if I can
accurately figure out phone then I'm
then I'm I'm done five or six whatever
when I'm not doing bad okay
how can you say okay how to choose best
class way because we've got plenty I
think I've only answer that you need to
see the data set your data behavior can
I have to decision attributes I don't
understand this question but I think you
meant can we have more than two decision
attributes can we have more than uh can
we have more than two classes the answer
is yes absolutely we can I am both in
two classes how this Albert me algorithm
can help us with the accuracy while we
tray what is that trait basically I use
regression model find out the possible
higher and medium LM developer his talk
but most of it time it Scorpio on those
levels which have got three restaurants
obviously so see it's not that easy so
you need to find either the most complex
of the Northam and that's why the the
the algorithmic trading guys have paid
really high really high and and and and
to get to the accuracy where you figure
out the possible higher than the minimum
value of a stock it's not going to be
easy because we could do that be you'll
be a millionaire right so the answer is
is is yes then Gotham can help but how
exactly can help well there are
thousands of things that actually
constitute that entire model and not
just they got them we should know Hadoop
or spar for later scientists I think
already answered that question
any more questions we'll go to another
couple of minutes and then we'll close
of this webinar questions anyone so
man I swear ow I have already answered
this question we know you need to learn
in case I mean so the question is that
do we need to learn a doob allspark see
it's not the primary skill again it's
the second set of sales that you need to
acquire but do we need to the answer is
yes we need to eventually not on the
first other face of it the primary skill
sets already offer data scientists one
as I have already mentioned in
statistics you need to know their
statistics you need to know the math
behind the Gautham's because we don't
know the math then you not be able to
model those in columns accurately
because some algorithm do specifically
good with with specific kind of data
sets for a specific kind of problems so
that status six-second is either our
Python I would say I would say you you
do you go for both are in Python that
has to be the eventual goal but again
start with one of them third is a new
pen and spark and all that for four for
parallel processing and then you should
also have a good creative or a
understanding of the visualization part
you can learn d3 or you can do it in are
also there's other question which is as
are better or says I cannot answer that
question because says if I say are cells
going to you know is go to a sizable to
read only but are is open source it's
pretty handy and it's it's pretty cheap
even with the copied version and it has
got and you can do the most complex I
know you can run most the most complex
algorithm also in our are in science you
can still do that but sciences is now
confined to industry analytics and
doesn't go beyond linear regression or
GLM's I have not used random phone is
ever in SAS how would we should be
linear regression it's like how would we
should be in grammar if you want to
speak English linearization is a must
because it's simple and because it's
simple it is not minutes it's not
required because it's simple I would say
it's really important because that helps
you understand the rest of the
Grantham's plus the first step where I
solve any problem
is actually put up a linear regression
on that data so guys are we are done
with the webinar thank you so much for
joining in I hope this this was
informative and helpful for you guys
thank you so much for joining bye-bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>