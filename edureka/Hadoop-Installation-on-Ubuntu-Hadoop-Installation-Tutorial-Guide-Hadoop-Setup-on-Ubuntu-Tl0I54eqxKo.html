<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Installation on Ubuntu | Hadoop Installation Tutorial Guide | Hadoop Setup on Ubuntu | Coder Coacher - Coaching Coders</title><meta content="Hadoop Installation on Ubuntu | Hadoop Installation Tutorial Guide | Hadoop Setup on Ubuntu - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Installation on Ubuntu | Hadoop Installation Tutorial Guide | Hadoop Setup on Ubuntu</b></h2><h5 class="post__date">2014-07-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Tl0I54eqxKo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Holloway 1 let's start the installation
of Hadoop 2.2 101 - first of all what we
need we need a VMware Player for that we
can download it from the Google and just
install in our windows system once the
VMware Player is downloaded we have to
download the ubuntu image we once that
is downloaded we have to configure our
Ubuntu image into our VMware Player once
it is done we will get a window like I
have in my screen this is called a
terminal on which we can write that
commands how we can get the terminal we
can just go to our - this right term we
can write terminal and we can get the
terminal over here okay this is used to
type the command first of all what we
need over here we need to update the
repositories first for that we have a
command sudo space apt hyphen get update
it asking me to put a password the
password is password PA SS wo RT it
start working now it is updating the
repositories on my a bundle
it will depend upon the internet speed
it will not take more than one or two
minutes as you can see that it's done in
my machine now you can proceed further
with the other commands what we have to
do we have to download the open JDK now
for that we have a command sudo space
apt space so I get install space open
JDK - 6 - JD I'm downloading open JDK 6
to download or not I will give a
permission of here why you start
downloading as you can see that it's 0%
done it's 30 T remains 33 minutes
remaining
now we utilize our time we can download
the Hadoop jar file as well we can say
at our file side-by-side
what we can do for that you can just
right click on it and open a new
terminal a terminal will be open and we
can download the top fellow Hadoop 2.2
for that we have a command
I have already copied the command so I
will just copy and paste it over in the
terminal but for that we need a we need
to put a command W get what is the
purpose of W get it is used to download
the file directly into our Ubuntu or
Linux operating system from the server
after who have written W get after that
we have put a space now I have given the
path offshore download to Phi I will
press enter now to start downloading now
I have two things downloading side by
side one is JDK six second is Hadoop 2.2
let's wait for that till it downloads
because till it downloads we cannot
proceed further with the other commands
so we need to wait for that the size of
our open JD gates Mir about 170 MB and
of Hadoop 2.2 we have a size of around
one point 1 0 4 MB if you want to
download JDK 7 like I have used the
command open JDK - six - JDK instead of
you 6 you can use seven and the version
seven will be downloaded in your system
but I will recommend you to go with six
as of now for the Hadoop purpose
I hope to point to ISM you can say a
second generation of Hadoop in which we
are dealing with the latest versions of
Hadoop earlier we are dealing with
Hadoop generation one that is one point
two point one the one point two point
zero we have different different
versions for that but now we have
switched to second generation that is
Hadoop two so we are dealing with that
it now
Ubuntu is an open source in which we can
say a vanilla software in which we have
to download each and everything from our
own starting from the sketch
nothing will be present over here
as you can see that Java is downloaded
88% and Hadoop file is downloaded 66% so
let's wait for the Java to download and
wait for the Hadoop also to download it
I think Java will download first because
it is 98% done any 9% hundred percent
done now it will enter the packages of
that automatically we don't have to do
anything we don't have to write any
command for that it will automatically
do it as you can see that it's unpacking
judge Java files and all let's see
side-by-side what is the status of
Hadoop it's 77% done wow so we have a
remaining 22% remaining speed is also
quite good so it's showing maybe it's 4
seconds left let's wait for that till it
downloads in my second terminal it since
entering the packages of Java 6 Java is
done its untaught the packages as well
not with the command we can check it out
whether it is installed successfully or
not for that we have a command Java
space hyphen virgin put enter it will
show the version of Java download it in
your system right now I have downloaded
one point 6.0 underscore 31 so this is a
functionality of Java now let's wait for
the hope it downloads so that we can
proceed further with the commands of it
it's it in 90% done it showing 44%
remaining sorry 44 seconds remaining
93% on any foe only 6% left after that
we will see the real scenario of it any
7% non I did 97 okay okay it's done now
with the help of LS command we can check
it out whether it's downloaded or not we
can see that it is downloaded fully now
what we have to do we have to untie it
so for that we have a command dar space
- xvf space name of the file name of my
file is hard o - 2 point 2 point 0 dot r
dot ZZ let's press ENTER it will enter
the packages of that all the files of a
Hadoop - point 2 point 0
it's done now again we will go for the
command LS to check it see it is done I
dropped two point two point zero a
folder has being created with the name
now we will see the configuration files
of it to configure the path and all so
what is the status of that folder it's I
do I do - two point two point zero ok
let's see what all file it contains it
contains bin folder include Lib exe
notice dot txt s bin EDC Lib and all the
files and all so the configuration files
are present inside a TC directory for
that we use a command CD space a TC soy
I place SDC ok now I have been Hadoop
two point two point zero and EDC
directory let's see what all file it
contains now it contains only one folder
with the name Hadoop let's go inside
that and see what all it contains I mean
hydric directory see all the files are
present inside this I drop NV dot SH
kosai dot XML map it e NV dot SH mepid
rights odd XML these are the important
file which we need to configure it and
we will get the outputs after that means
we will see the demons running after it
and yeah this is also important file
yawn - side dot xml which we'll be using
to configure the path okay let me come
out of the older directories by using a
command CD
ok now what we have to do we have to set
the path of Hadoop and Java which we
have downloaded so that will be done
inside that dot bash RC file dot bash RC
file is a hidden file which we cannot
see
by using a command LS see I put I have
given the command LS are not able to see
that file but the
one more command as well to see that
file LS space - al space be a string it
will display all the hidden files
starting with the V name okay there is
some error in the command LS - al ok so
this is a bash RC file which we can
which we have to configure the path ok
instead I have not used a dot before be
that so I'd given me an error let me use
it over now LS space - al space dot be a
straight so I okay so we have only three
files with the name starting dot B bash
RC bash underscore logout bash
underscore history now we have to edit
the file dot bash RC to edit the file we
have a command sudo space G edit space
dot bash RC it will give a pass it will
ask a password password is password that
I put earlier so same password will work
for you a new file will be created we
can see not created a file will be
opened bash RC file will be opened now
we have to set the path inside that oh
let's go at the class now I will show
you what all paths we have to configure
in it it will be hydrogenated path first
of all we will be setting Hadoop related
paths first of all we have a command
export space I do okay sorry
Hadoop sorry export Hadoop underscore
I'm setting the Hadoop home path
I have said the dog home park now
now after that I will set the path of
configure directory means the files
which we have seen course ID dot
products ml map it dot XML for that we
need this we need to set the partner
group underscore come from in the score
directories for that we have a path I do
- two point two point zero it's a name
of the folder which we have downloaded
after that we have to proceed further to
e.t.c directory and after that we have
to move to Hadoop folder inside that all
the directories are present so it will
take the part automatically from there
now we have to set the path for map read
home for that we have a command Hadoop
underscore ma p I T in this Cove home I
do - to point to point to on dot C now
the map at home is set now we have to
set the common home now
instead of writing in the commands you
can also copy the command from the
installation guide which you have we can
easily copy paste the command in Ubuntu
there's no restrictions in that let me
close this now proceed further with the
commands
common home is done now we have to set
the path for HDFS export Hadoop under
Scholl HDFS and thus call home to home
directory ok HDFS home it said now we
have to set the home of yan yan
underscore home he goes to home
directory and the folder where its
present its present hadoop - two point
two point zero okay so these are the
path we have set for the hadoop purposes
now we have to set the path of Java
which we have downloaded for that we
have a command expert Java underscore
home directly which in which it
automatically downloads its user li be
JVM Java - was 6 - I will show you by
the tammana also the position of a
hardened JDK 6 you can verify the file
name of the file from there also so that
it should be present at the same means
the name should matches with the same
now let's set the path of Hadoop in
directory this is the last part we have
to set
in the bash RC fight I do path home
directly home Wow before that we have to
use but : so okay so the other part of
her to be upset this is the Java home we
have set and this is a part of Hadoop to
bind to we have set so these are the
only configurations we have to do in dot
bash RC file after that we can this
click on the Save button and close this
out it will take you to the command we
can type another command in that now
before proceeding further let me show
you the location of Java which we have
downloaded the location I have given CD
space us our live JVM inside that let me
put LS again so I have given this we
have JDK 6 &amp;amp; 7 but I've been used I've
downloaded Jadakiss 6 and it's present
over there and I have set the path for
it
come out of the sea directory and let me
clear this out sorry
okay now let's proceed further with the
other command dot bash RC means if you
are setting any path for that we can use
in any means suppose we are working with
the other installations as well so it
will work for all dot bash RC now we
have to set the Hadoop path as well
means we have to set the whole Java home
in Hadoop env dot s H as well so that we
can use it for Hadoop also so for that
we have a command CD space how do we can
use Hadoop underscore conf directory
means in which all the directories are
present we can give a path as L CD let
me take you to the longer path Hadoop
two point two point zero after that et
Cie after that Hadoop
okay we are inside that now we have to
edit the file first of all Hadoop in V
dot sh for that we have a command sudo
space G added hello - E and V dot dot Sh
okay so the file has been opened now I
have to give the path of Hadoop inside
that as you can see a Hadoop underscore
home has been given now we have to
comment this or we can utilize this line
only to set the path let me just comment
this and write a new line to set the
Java
sorry Java - six - open JDK - i386 okay
so this is the name of the file and this
is a directory let's click on the save
and close this out home now we have said
the Hadopi NV dot Sh let's proceed
further with the other directories that
we have to create now let's work with
the course I dot XML we are already in
Hadoop directory so we can directly give
the command sudo space G added space
Corps - site dot XML a cosign file has
been opened now we can give the
configuration inside that so what all
the configuration we have to give first
of all we have to define each and
everything inside the property I hope I
have given the spelling correct P ro PE
@ dy e ID Skype okay
in fact that we have some name of it and
after that it contains some values so
name is FS default dot name FS dot
default dot name after that we have to
close that as well let me just increase
the size of it so that our women can see
it easily okay now I have to set the
value well you in this we are setting
the SDF s path so for that as DFS
localhost
it will automatically take the IP
address of a local host and 9,000 is a
port number on which it will be running
okay and now we have to close the
property okay so it is closed so these
are the configurations we have to do in
a course i dot examine now let's click
on the save and close this out after
that we have to edit the file of SDF
Exide dot XML for that we have a command
sudo space G irate SDF s - site dot XML
s GF s site has been opened now we have
to set the configuration inside that so
let's see what all configurations we
have to do inside that first of all as I
have done in the course ID we have to
start with the property after that it
should contain some name and it should
contain some values so name is DF s dot
R application
I am starting their application effect
and inside that by giving a name now in
the value we have to give the
replication factor what replication
factor I have to give one two or three
values one I am giving up as a
replication factor again so this is done
now let's close the property of this now
we have to set the path of a name node
as well as a data node means SDF is
contain these two things let's set the
path for it so for that we have to again
open the property we have to give some
name name is DFS dot name node dot name
dot
okay so inside that we have to give the
value means the location of the file I
think I have missed one step I have to
create the directories so let me clear
let me do one thing let me just pause
this step over here and open a new
terminal and set the directory
side-by-side okay so I open a new
terminal to set the directories we have
to make a directory we have to use a
command mkdir - P and we have to give
the location where it has to be created
okay I am giving the home directory of
inside that a folder with the name
Hadoop to underscore data inside that a
folder with the name SPFs and inside
that we have a file a folder with the
name name node
okay so what directly has been created
the same way we have to create the
directories for the data node itself I
can use this upper cursor and just name
the change the name of data node instead
of name bro okay a director has being
created now we have to use the same path
inside the HDFS side dot XML let's give
the value over here file will be file
double colon okay I've already given so
now the location of it home underscore
user group two point two point zero Auto
to underscore the score data - SDF s -
name load now after that we have to
close
okay so a buck of this property is done
now we have to close this property as
okay so we have set the property of and
items I named not now now we have to set
the property of Dan'l so we can just
copy these four lines select copy and
paste it out over here this we can
change the name of data node to sorry
name node in the name we have to change
and in the value we have to change
apart from that each and everything will
be same we can cross-check ones because
it's we have said lot of configuration
so we can just cross check once before
closing the file properties open name we
have open after that we have given the
name we are given the value there is one
mistake I have done so I will just
rectify it
properties close now again a property
has been opened a name name has been
closed value it has been closed property
has been closed again a property has
been opened
okay each and every thing is fine now we
can close it and proceed further with
the other files core side is done let me
just close it out and go to the same
terminal we have set the path of Hadoop
env dot s H or site dot XML is done s
GFSI dot xml is done now let's set the
path in young side dot eczema for that
we have a command sudo space g added
space eon - site dot xml okay a young
site has been opened now inside that we
have to set the properties of a one
known manager you must be thinking that
in the previous two files we have not
received this line so it's it's not a
issue much if you
you can delete this line as well or just
keep it as a same it will not get any
issues to you okay now first of all we
have to set the path of a node manager
again what we have to do we have to open
the property close this out we have to
give some name yon dot node ma na GE r
dot aux - services okay a name has been
done now we have to give some values it
is the value of it let's see value will
be map radius underscore shuffle about
given the spelling correctly yes correct
okay our value has been given now we can
close the properties and proceed further
okay we have to set one more property
inside dot of hadoop mapreduce part
which contains which will be linked with
yarn dot note manager only a name has
been opened what will the name Yan sorry
dot node manager dot aux - service own
dot Map Reduce dot shuffle dot class
so the value will be our G dot a batch a
dot I do dot map right home but shuffle
handler this is the value we have given
now we have to close this value and we
have to close a property no ok so this
has been said we can cross check once
whether each and everything is fine or
not ok it's look like each and
everything is fine as should be capital
over here oh I did charge Apache Hadoop
dot map right dot shuffle handler ok
it's fine you can close it out and we
can proceed further now what we have to
do we have to edit the map it's on light
dot XML file this is the last file we
have to set up you have to set now we
after that we will be going for the
installation we will really studying the
demons and check it out so let's open
this file with ok before we open this
file because by default map it's ID dot
XML will be empty so what we have to do
we have to copy the template of map it's
i dot XML dot template to map it
sideroad XML let me show you one thing
LS so so inside that we have two files
so where are it is ok
okay so one file is this mapping - sigh
dot XML dot damn plane okay so we have
to copy the template to map it - site
dot XML because by default map its ID
dot XML will be empty let me show you
one thing sudo space G add a map break -
side dot XML okay as you can see that by
default map right side is empty so we
have to copy the template of Muppet side
dot XML dot template to this file and
let me show you the file let me close it
and open the map it template dot XML
okay so as you can see that it cuz
somewhat like the previous files that we
have open HDFS side dot XML core site or
dive XML and yang side dot XML so we
have to use basically in further for
studying the demons and all we have to
use map its I dot XML but by default it
will be empty so it has created the
template file to compile it template
into that okay so to copy the template
we have a command CP map rate - one side
dot XML dot template to map lit - side
dot XML what we'll do it has copied all
the template from map right side dot XML
dot template to map it side dot XML
now let's open the file of map its ID
dot XML to configure the properties okay
now let's see the file of map it's I dot
XML so that we can configure the path
inside that for that you will use a
comma
and sudo space G added space map paid -
site dot examine earlier you have seen
that map it's AI dot XML was blank
now once we have copied the
configurations and all the templates -
from map its site dot XML dot template
to this file
it has copied each and every content to
it now we can set the path and
properties inside that easily so what we
have to set inside dot first of all
again we have to open the property after
that we have to give some name name is
Map Reduce no dot framework dot name
after that we have to give the value
values yeah once it is done we have to
close a property now ok so this is the
only thing we have to set in map right
side dot XML Map Reduce dot framework
dot name it is the name after that the
value will be yeah save the file and
close it out ok so this has been done we
have set all the configurations that are
necessary for the Hadoop lupine 2.0 now
let me just come out of etc' and hadoop
directory
let me do one thing let me just come out
of the whole directory idle to run the
demons and all we have to use some
directory Hadoop's means the file hydro
- two point two point zero after that it
should be s been okay so what we have to
do we have to father come on how I do -
name node we have to format the name
node first it choose command not fun so
let me check okay before starting the
demons and all we need to set the
configuration of a SSH key means so that
you should not ask the password while
starting the demons for that we need to
come out of Hadoop directory to send SSH
key we have a command SSH space not
space - key gen-5 space hyphen T space
RS a space - P P should be in uppercase
space double quotes they should not be a
space between the double quotes this
step should be followed very clearly let
me just repeat it once again SSH - qijin
after that they the space after that - T
space RS a space - P space double quotes
let's say it will generate a key it will
ask enter file in which we have to save
it so by default the location is Hadoop
underscore Hadoop user SSH so let it be
the same let's we have to just press
ENTER and it has been created now we
have to move that file to authentication
key so for that we have a command cat
space location of a home directory now
we have to move the ssh dot SSH ID and
the skull RS a dot pub to home after
that authorized key authorized under
skull keys okay so it has been copied
now now what we have to do let's
cross-check once all the files that we
have configured co site SDF s ER and map
rate so for that we have command sudo
sorry CD space
how to to point to point - 0 after an
atc after that I do so let's start with
the course I dot XML know we have
configured Hadoop e NV dot SH first so
let's go with that first Hadoop - II and
V dot SH what we have done in this we
have just set the home path of a Java so
it has been correct now we can close
this out let's check for code site dot
XML generally we need to cross check
that I mean we have closed all the
demons or not okay we have found one
mistake over here that instead of value
we have given name so let's rectify it
property name and all okay first each
and everything is fine save it and close
it after
let's open HDFS I was okay property name
value fine fine okay it's quick so we
can just close it slave and close it up
after that we have to use young okay
we found one mistake over here as well
we have not closed the property by
putting the backslash let's say close it
now the last file we have to cross-check
is map red dot method side got eczema
property name value property okay it's
fine
so we have cross-checked also each and
everything now we can proceed further
with the formatting of a name node and
then and the last we will start the
demons of it so for that this come out
of the Hadoop directory directly give
the command
I do name load - format ok choose come
on not form so let me check what either
we have to move some directly to it to
make it run so this lets me let me cross
check once okay to format the name node
we have to move to Hadoop folder it is
Hadoop to point to point 0 so I it
should be CD ok after that we have to
move to bin folder inside that we have
to format the name node for that we have
a command I dope space
space name no space - format okay so
it's formatting now okay it's done
okay to start the demons we have to move
to s bin directory demons are present
inside that so let me come out of this
and move to s been okay so we are in has
been directly enough so let's start the
demon of data node first or that we have
a command idle space - demon dot sh
start titanal okay so our data node is
starting okay it's we can give the
command JPS to check whether it's
turning or not okay it's running fine
now we will proceed further to start the
other demon now let's start name node
for that we have a command idle - demon
dot SH space start name no okay so our
name mode is starting now okay it
started let's go to the JPS to check
whether it's running okay so that I know
this started earlier now we have started
name node so both are running fine now
let's start resource manager as resource
manager this a demon of a yarn so for
that we have a command key on space
demon dot sh start
resource manager let's go check the
spelling was okay I missed hard over
here okay let's start again starting
resource manager let's see okay so this
was manager I started that I notice
started Nemo death started now let's
start node manager now for that we have
a command Yan - demon dot SH space start
node manager okay so let's press enter
and check it out after that only one d1
is left that is history server to start
before going positing further let's go
JPS and check the status of it okay so
for all the four demons are running fine
let's go with history server now to
start with we have a command mr - job
history
I find demon dot SH space start history
server okay history server is also
starting let's go with JPS and check it
out out
so the source manager started dead I
know they started name would have
started
non-manager started job is to serve I
started okay these are the five D ones
that we have to start in I do two point
two point zero so it means that our
cluster setup is ready and we can
proceed further wither and so on path we
can check one more thing to the browser
whether the health is fine or not so for
that we have a you let's go to Mozilla
Firefox first</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>