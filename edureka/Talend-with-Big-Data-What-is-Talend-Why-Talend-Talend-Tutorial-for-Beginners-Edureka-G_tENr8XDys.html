<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Talend with Big Data | What is Talend | Why Talend | Talend Tutorial for Beginners | Edureka | Coder Coacher - Coaching Coders</title><meta content="Talend with Big Data | What is Talend | Why Talend | Talend Tutorial for Beginners | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Talend with Big Data | What is Talend | Why Talend | Talend Tutorial for Beginners | Edureka</b></h2><h5 class="post__date">2015-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G_tENr8XDys" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is what exactly is ETL with big
data or talent with big data it's a
graphical abstraction layer which is
sandwiched on top of the hadoop
application yeah
now the surprising buzz going on up in
the market these days that introduction
of Hadoop big data is the tomb for the
ETL which is not true this is really a
myth all right now the surprising stuff
about this current universe is this is
really not practical and these are all
outlined is comments made by some named
statements yeah now the typical
assertion as I said how do eliminates
the need of ETL seriously that's not
really true now what no one seems to
question in response of these sort of
comments is the neighb assumption these
statements are based on is it realistic
for a moment let's think is it realistic
for the organization to you know
transform their whole enterprise data
from from the a legacy platform to the
hadoop system there are a lot of
challenges so it's it's really not the
evolution of her ETL but we will see
soon that how ETL and big data they come
all right in between guys adjust for
your understanding how we have planned
the webinar we are going to finish up
the session in in half an hour's time
then we have reserved the question
answer you know questions around for all
the participants or at least for 15
minutes to 30 minutes all right so by
the end of this webinar we are going to
take up you can write down your question
on the question board and by the end of
this webinar me mike your your webinar
presenter your tutor or the course is
going to take all the question one by
one you can trust me on that
all right now moving you know moving
now this modern data integration tool in
platform they ensure the timely trusted
relevant and secure
now modern integration technology use
optimizes to process the information in
both scale up and scale down
architecture now push processing into
database man system and not just the
data into the Hadoop sorry
they also integrate from end to end
now they broker and publish the data
layer that abstracts processing such
that multiple application can consume
and benefit from the secure and curated
data set
ETL no doubt needs to continue of course
to evolve another to develop reference
at performing but it also takes care of
the lepton see need of the modern
application see honestly speaking hunter
will just another engine upon which en
and its associated technology like data
quality data profiling can run well
renaming
what is commonly referred to as ETL or
was ignorant dismissing the data
challenges and enterprise wide data
needs is just irresponsible well this is
one of my favorites lag which explains a
lot of myths in yes or no is writing ETL
scripts in MapReduce code is still
easier
that's right yes it is still it here now
is each one running faster in few cases
and slower and others on Hadoop is
eliminated yeah no absolutely not in
introduction of hadoo changing when
where and how
idiot happens yes now the most important
part with ETL big data is the question
is not really what are we eliminating
that are we eliminating ETL or not but
where does ETL take place and how are we
changing the definition and how does it
really complement the big data alright
they go hand-in-hand
all right now what is this ATM
all right now ETL they present stability
to consistently and reliably
extra
data with the high performance and
minimal impact to the throw system
represents the ability to transfer one
or more data set in the bad or real time
into the consumable format
now obviously the load stand for loading
the data into the persistent or the
virtual stood now this is more or less
anyone who is not from the background of
VPN can understand this is more or less
what the ETL does moving forward the
most important slide in the webinar why
do you need to use why do we need to
combine the ETL with Hadoop and most
importantly what do we need to combine
talent with detail Talent with Hadoop
sorry now now how learning this ETL
along with the big data is in the major
business challenge now right major
business problem it is addressing now if
you see this entire product suit offered
by talent we have big data we have data
integration we have data quality with a
master data management we have recently
introduced Enterprise Service bus
we have Business Process Management
anything that you talk about data is
part of Ellen products should and when
you combine it with Hadoop which we are
going to sleep that demo and fill in
minutes that writing how talent for big
data is abolishing or almost making the
mat reduce programming so much easy for
any developer whether you are starting
friend whether you are program manager
whether you are an ETL he or whether
you're technical lead whether you senior
staff it really doesn't matter
learning talent for big data is just so
easy now why this is this is one-stop
solution now talent is of course a whole
ecosystem which complement most of the
environment whether you talk about the
Hadoop ecosystem in terms of period in
terms of hive in terms of school in
terms of HBase exit log Cassandra of
no sequel database like MongoDB you name
it talent has a platform to establish
the connection and make the job up and
running in that environment all right as
you see most of the top class brand the
market leaders in the Big Data industry
whose map are cloud era they all have
done the partnership with Talan knowing
the capabilities that it has to offer
through its graphical user interface and
it gives you so much of time rather than
doing the MapReduce and coding which
gives you obviously a upper edge now it
improves the efficiency of the Big Data
job design with graphic interface
abstract and generates the code run
transform inside hadoo native support
for HDFS scoop HBase mahute Big Five
MapReduce as I said Apache Allah is true
embedded in hot rocks data platform now
they are obviously as I said certified
by the three market leaders cloud there
are a power bottom box alright now we
are closing on to the action white Allen
this is what the Talon Corporation has
to say because the more connected the
data the world becomes the more quickly
a business must adapt very underlined
statement yeah because the more
connected the world becomes the more
quickly a business was done imagine if
I'm only working on big data environment
as as a potata user or I'm a MapReduce
programmer or I just know hi or I just
know pink but I really do not understand
my business data it really doesn't make
sense now why does learning talent helps
here because Talon learning with the big
data gives you two operands one it makes
you winner of the leader of how to
integrate your data to understand
I think from the source where the data
is coming from plus it also gives you a
control on the big date idea all right
now that makes you the winner of both
the world obviously talent is only the
graphical the only graphical user
interface tool which is capable enough
to translate an ETL job to the MapReduce
job the Nietzschean job gets executed as
a MapReduce job on Hadoop and get the
big data running up and running in
minutes and make you repeat it a
developer in minutes this is a key
innovation which helps to reduce the
entry barriers in Big Data technology
and allows ETL job developers to carry
out the data warehouse offloading to
greater extent
now it's strong that lead Eclipse based
graphical workspace talent open studio
for big data enables the developer and
data scientist to leverage and bring and
processing technology I guess the FSH
base hi Pig without having to write
Hadoop application core Hadoop
application seamlessly integrate within
minutes using the talent now before we
go to the next slide let me just have a
quick look on the question board all
right I am new NIT
I have different background obviously I
took all right all right
all right gasps and now Z just within
minutes we are going to finish up the
slide then it's all demo and it's all
questioner all right now as you see on
the slide
why talent but still we still justifying
why trial and anyone who comes on you
who the pressure who comes from a
different background because there are a
lot of news floating on the market
should I go to the MapReduce programming
where should I learn it through tool
plus pecans talent for Big Data is
awesome true when we are going to see
the
a demo in minutes you will be just
mesmerized that what it is capable of
doing in minutes now by simply selecting
the graphical components from the pallet
and arranging them and if you know the
right set of configuration you can
create a Hadoop job in minutes and to
load the data into is DFS which is
Hadoop distributed file system it will
use the Hadoop pick to transform the
data and the load data into the Hadoop
hybrid data warehouse it can perform ETL
or e.e
leverage the scoop scoop is nothing but
it will take out the data from
traditional atopy on the state of the
system and pushes it into the data
moment
Wow
Talon opens Studio Talent open solution
plus the power of a do makes Talent open
studio for big data right for Hadoop
application to be truly accessible to
your organization they need to be smooth
integrated into your overall data flow
now talent opens for big data is the
real tool for integrating the Hadoop
application into your broader data
architecture now one of the primary
reason why talent is so powerful it has
more built-in connector component than
any other data integration solution
available visited in the market with
their 800 closed connector that make it
easy to read from or write to any major
file for my data base package Enterprise
Edition you name it most of the area and
it covers anything they don't cover they
keep on upgrading in their newer
versions say for example Talent open
studio for big data you can use drag and
drop configurable components to create
data integration flow that move the data
from delimited log files into Hadoop
hive or form the operation I'll extract
the data from hi will remind sequel
database there is n number of
possibilities guys see of course more
and more Enterprise wanted to scale up
in Hadoop Big Data technology with the
use of existing pool of talent this is
where our IT industry and most of the
area they are struggling
but going on that you need to have Java
background and only a Kalani and how
about my existing pool of Rousseau's how
am I going to train them on a big data
project now these are the kind of
critical question that the business need
to address and one of the solution is
talent or Big Data now high-rise job
trend in the data scientist data
analysis talent also comes with basic di
transformation which complements this
area and reduces your dependency on the
simple extra in DES Polk PA to Gartner
is telling is the best technology in the
market for data integration in Big Data
now these three major players in the Big
Data industry Orton walks cloud era
mapper have already tied up with talent
for Big Data solution and now mostly any
level of person in the industry can
quickly get started on this without much
political taste as I was saying the
width
I don't know java programming how this
goes help me learn and excel in Big Data
the biggest at one ditch that you get
with talent OB is there is no preview
cities trust me to learn this concept
whether you know whether you come with
prior knowledge of Hadoop or not it
really doesn't matter you can be a big
data experts and up and running with the
data integration environment in minutes
guys this course has some or other thing
best things to offer each and every one
well that's it
Big Data in ten minutes that what a big
that for the talent claims go from zero
to pro in Big Data just under ten
minutes you can be on hand coding the
talent for Big Data sandbox is ready to
run the virtual environment which we
will see in minutes that includes the
talent platform for Big Data this is at
the popular distribution as you see on
the crown top of the crown they have
cloud era hot invokes they have map are
they also have Apache Hadoop as I said
anyone who can use talent for Big Data
starting from the fresher the fresh
graduates in IT industry
starting from professional starting from
developers that the managers anyone can
get started on this obviously I am
having experience on 10 years I'm a
manager I how do I start with big data
it really doesn't allow me to
flexibility of now start lining the
programming of the MapReduce programming
now so what should I do
go put that in a big data by the time we
will do a hands-on we would realize that
this is everyone's cup of tea well about
to see the hands-on demo we are about to
see the big picture now rather than
wasting any more time it is all quickly
see what talented doing minutes
reducing the man-hours in doing the
MapReduce programming in Hadoop shall we
I'm going to show use case all right now
for that use case is very important you
know the environment that I'm using I'm
using talent open studio for Big Data
5.5 I'm using a pre-configured sandbox
environment which is my Hadoop
environment 1.3 in order to make it run
up and running I'm using Oracle Virtual
Machine sandbox obviously my machine my
laptop is running on Windows 7 64 bit
and I'm just using a normal standard
hard work configuration of OG bhuvah and
i three processor
alright now with that set of set up
let's quickly go and see fresh demo I'll
just explain you what is it that we are
going to do in the demo yeah I am just
logging into my sandbox give me one sec
dad said
all righty I just show you the file or
the data that I'm using for this demo
I'll show you this this particular use
case is based on a unstructured web
block data that we had extracted and on
this particular unstructured web block
data people are talking a lot about big
data say for example this has been
extracted from our Facebook page of a
company website who does a lot of you
know analysis you know test data all
right
righty that's it okay
I'm using this is my sample file that
I'm using well I let me quickly see the
questioner window okay all right all
right let's just with this and then we
can get up and running in minutes guys
how is the audio
how are the PPT because we are about to
start the hands-on can I get a yes from
each and every one
all right thank you sand yeah thank you
now see everything that you see on this
demo it's all open source it's all
downloadable you just need to know how
to configure them nothing you need to
pay nothing there is like sense all
right everything you have downloaded
from the word alright again thank so
each and everyone can see my screen
there is a file now that you would be
able to see and this is a normal log
file and these are just simple text
right
everyone is talking about everyone is
giving many comments on the big dinner
now one of the primary requirement of
one of the you know weblog industry is
they want to study this log and they
want to find out what is it that people
are talking about the most the most
hyped works the buzz words out of this
unstructured log data using talent for
big data without doing any MapReduce
programming we are just going to do that
in minutes all right if we are going to
do this in typical MapReduce programming
we may end up writing some 100 200 300
lines of Java programming whereas using
talent for big data we will see that
this is just a cakewalk all right
there are some questions now hang on
guys all right yeah so the word count
dot PHP file is present in HDFS that's
right and I'll just show you you can I
can view this if some of you would be
aware about few of the commands that we
normally exercise in Hadoop all right in
order to view a file I use Hadoop FS
minus cat and I'm going to view the same
data from my command prompt word 16
bingo
does that answer your questions and yes
it is there in the head SDRs yeah this
is not ordinary file this is the file
which is there on HDFS all right now
quickly you're welcome
now picking what I'm going to show here
is alright now what have I done here all
right
I am doing I'm connecting to the Hadoop
environment okay can you see my full
screen now okay I am just connecting to
my Hadoop environment up from talent
connecting to the HDFS then what I'm
doing is and transferring this file this
file is actually there in my local
system and transferring this file from
my local to the HDFS if I show you in my
local directory see this file it their
web extract all right it is the same
file
alrighty
yeah it's the same file so what I'm
doing in this is I'm going to copy and
rename this file so that I can show you
in them all that
how does talent move file between local
to SD others as well in minutes I am
going to rename this file with extract 1
all right with extract 1 now guys there
is no such file called WebEx chat 1 in
my head
we'll extract one all right now let me
see no there's no such fire there's
neither with extract one so this file
doesn't exist as of now okay so what I'm
gonna do using talent I'm going to
transfer this file from my local system
to my SD office all right and then I'm
going to read that file which I have
Angele transferred to my SD of a system
all right and I'll need me dance work
loud one okay move with extract one
toward club one and then I'm going to
read over twelve one that's good that's
it all right and rest off the
transformation process what I'm going to
do is I'm going to arrange this
unstructured log file in a particular
manner then I'm going to map it using
typical dialing components and these are
very easy to use graphical drag and drop
components kind and I'm going to do this
we are going to analyze the web extract
data and produce the most frequently
used or the spoken word and that we are
going to dump the data back into what
cloud one or cloud underscore output one
into our HDFS all right then I'll just
go and quickly run my job please pay
attention while the job is running this
job is actually running on HDFS platform
has a MapReduce program
on the backend but I'm not writing any
MapReduce programming here I just did
drag and drop tick-tick-tick job done
happy happy it will take some time guys
there is many job running on my machine
many stuff I keep on doing here and
there all right by the time the job is
running guys surprisingly I did not
introduce to everyone all right
this is Mike and I have almost a decade
experience close to 10 years of
experience in ETL big data business
intelligence industry evolved in many
geographic location including Europe
London and Middle East
and we have helped so many clients in
developing and building the big data
application the ETL application any
application which provides a business
friendly solution so that more or less
for me and this analysis job which I am
showing if the real-time example from
from one of my web customer who one who
were very interested to design their
work route but they need to know which
are the keywords which actually frames
their work around I believe all of you
guys you do understand what is the word
cloud do you do you guys understand the
workflow
yeah I will show Sam the business logic
in minutes I will I just saw the output
first nosey
yeah that is the login that I use into
the sandbox yeah that's that's the root
user to the sandbox so it let me make my
my environment up and running now
understand one thing talent is a
separate entity this big data area is a
separate entity so if I want to take
advantage of a drag drop technology
where I do not have up doing any hand
coding then of course I need to
establish a connection between this
interface to the Hadoop interface right
now my job just ran
it's uh it's alright I also have made
some prohibition in my jobs so that
before I go and check my file I can
literally see the output here that is
going to come wonderful so these are my
top rated work and these are the most
high in my love big data has been used
six times in this file in this web
extract is has been used six times
Hadoop has been used four times data has
been used four times so has been used
for ten okay I have not made any
provision to remove blanks that is the
reason something you can see after so we
can plug and play one component to
ignore all the blanks that's not a big
deal and now in is the most frequently
used work in our web extract now the
similar thing in E has already gone into
what was the file name we had given word
cloud under school
war-club output I'm going to show you
this from the compound as less
misspelling the fine work cloud is a
little test return dot txt that's it
bingo I can see my file from here I can
go to my web interface test data or
cloud output 1 there you go and you see
the time stamp is the right now I have
logged in from Indian time zone where we
are on 916 p.m. we just ran it three
four minutes back and the sandbox has
created this data wonderful alright
that's it
so how much time it takes to build this
job hardly two or three minutes
no the logic which are used to analyze
this complex unstructured log data
because this is not coming in a row and
column format is okay someone is asking
something Wow but if we want to
calculate Mac pressure from specific row
or column how we let them know what to
work exactly
all right now see that everything is
possible what I'm showing you on
Thailand is a very small them or what it
is capable of
if you see it can have such a control on
such unstructured log file can you
imagine the kind of control it will be
having on a structured file
okay now the coming back what logic can
have used is I have just treated all my
unstructured log separated by space is I
have using a normalized component I did
not normalize everything I normalize I'm
tired data I arranged each of the world
one by one one after other to make them
each an individual record that is the
reason if you see my screen my 11 rules
which is then there are my source data
they have become 130 row because I split
everything because I wanted to
individual control on each of the world
then I'm not removing any to click it
because that is not what I mean in this
ROM rather what I have I have brought
everything to one platform I did a case
so that if I'm looking at big data small
big data over the large big data board
everything is big data all right so I
have used all the focus while mapping in
talent so that everything is on single
platform then I have used a creator
transformation what I have done is I did
a group by on each individual world but
by that time everything is enough kids
so though if I show you my log file if
you see my log file all right
you see here there is something called
big data and these gaps here and there
is something called big data being small
here anyone can come on web and he can
misspell she can misspell children they
can write in any format obviously
control of the control of the audience
emotions of analytics is not in our hand
you can do be can use the tool to is
best to analyze this kind of
structured data bring them massage the
data and then I use aggregate but my aim
was to see the top works the top
frequent words so I arranged them in
order where I wanted to see the count
the bigger count at the beginning and
the lesser count at end and I'm just
using a display to make sure that what
if what am I doing here I'm doing
perfectly fine there
I came I'm pushing my data back to I do
HDFS so I'm bringing I'm reading
phenomena the effect and I'm doing the
transformation and everything image D of
this platform and then again I am just
recording or storing the data in HDFS so
that these results can be later used for
business analytics or big data analytics
am I making sense here all right where
does the competence the computation
exactly yes and now right now I am
running on a standalone installation of
a single node cluster all right so the
instruction will go to the job tracker
then the mother lode will decide to
which node the typical it is running on
the Hadoop platform but the advantage
that it gives you is any kind of complex
requirement you have you do not need to
write those thousand lines of not
pudding you talk about I'll just show
you in minutes the capabilities of
talent for Big Data
if you all see my screen here you see
can you all see that is a family called
Big Data here what does it what does it
doesn't support it almost supports
everything you see it supports SD occurs
and I got a lot of components within
that to do my job
it also supports hi which we have in our
in our regular curriculum
we do a lot of transformation on high we
transform the data we create the table
dynamically on the fly everything we do
within high then you can you see the pig
we do have big scripting job but do not
need to ride the big screen so
indirectly talent for big data is called
the swiss knife of the non programmer if
you want to focus on better technology
and you're running short of time
Alain for big data is the right tool for
you if you think MapReduce programming
is too much for you and one for big data
is the right tool for you if you think
you come from a background where you do
not want to put all your previous
experience IT experience or technical
experience into vain and start from
scratch
talent for big data is the right route
for you can you see MongoDB
can you see text catalog can you see
HBase can you see people people maybe
query can you see Cassandra can you see
comes DB you will name it we have a
component here all right can you see
scoop anything everything is there and
their core top some 800 or connector you
can write any kind of social system
complex XML we have seen just
unstructured normal text log file we
still have a control on this kind of
file we have all the kind of database
you name them starting from Oracle my
sequel LDAP my sequel server everything
we have over here and the best practices
we can parameterize everything we do not
need to hard-code anything there is
context then we do have the concept of
we do have the concept of routines and
code where we can write the usable piece
off for transformation which can be used
again and again so primarily what you
see on your screen now is stand for big
data is open studio version this is the
palette window this is where we design a
job this is where we see the law so it's
a one man army show everything happens
here all right
now coming back to our slide this is
what we had used we have performed the
demo
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>