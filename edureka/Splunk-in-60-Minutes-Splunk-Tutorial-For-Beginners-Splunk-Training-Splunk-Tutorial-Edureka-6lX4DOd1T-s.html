<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Splunk in 60 Minutes | Splunk Tutorial For Beginners | Splunk Training | Splunk Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Splunk in 60 Minutes | Splunk Tutorial For Beginners | Splunk Training | Splunk Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Splunk in 60 Minutes | Splunk Tutorial For Beginners | Splunk Training | Splunk Tutorial | Edureka</b></h2><h5 class="post__date">2017-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6lX4DOd1T-s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so good evening ladies and gentlemen my
name is Vartan and on behalf of Federica
I welcome you on to this live session on
Splunk so in this youtube live session
i'm gonna explain everything that you
need to know about Splunk to get started
with that tool right so the next 30
minutes you're gonna get a demonstration
for 10 minutes and I'm gonna give you a
theoretical introduction towards Blancas
and by the end of the 30 minutes I'm
pretty short you will know why Splunk as
the go-to tool for law panel analysis
and besides that you will also
understand why log analysis is really
important ok so without wasting much
time let me get started and these will
be the topics that I will cover so first
I will talk about the problems with log
data and tell you why is there need for
a tool like Splunk right and after that
I'll tell you what exactly's plonkers
and how it is the ultimate solution for
log processing and then I'll talk about
a few of this plug competence that are
involved and I'll tell you how this plan
works and finally I will give you a
demonstration of collecting logs from a
remote instance and then putting them on
to your Splunk instance and then
performing analysis and visualization on
that data right so I hope that has set
the agenda for today and without wasting
any time I'm going to go to the first
topic and that is why is it why is there
a need for you know why is there a need
to explore logs there's a famous saying
that logs are to go to archives for
gaining company-wide
operational intelligence now does
anybody disagree with this you can
please put in your comment box and if
there are people who who agree with this
then I would also request you to tell
tell us why you think so because I have
an answer with me and it might be the
same it might not be the same but
anyways you can get your answers coming
in in the meanwhile let me tell you my
half of it so logs are something that
get generated on computing devices and
non computing devices right and these
devices nowadays in today's world they
are really important for businesses
actually for those of businesses which
are online which are on the Internet
take for example in Toorak I itself it
requires an online education place which
is run on the Internet
right so our business is on the internet
now in this case do you think how we'll
have a lot of logs which will manage
we'll have a lot of servers which will
be generating a lot of logs and we have
a lot of servers because we'll have to
manage a lot of people who will be
accessing our website at any given time
correct so we have a lot of logs for
that purpose sorry you'll have a server
for that purpose and then we love loss
which we get which get generated inside
those servers now now you can ask me
what exactly these logs container and
why are they even relevant correct I get
to that the fort but before I get to
that let me tell you even systems have
logs any basic system even the machine
which you're using to view this
videoblog session of mine right even
that will have logs being generated and
that will be stored in one location and
one directory but the thing is we cannot
we cannot read those logs they are not
in our or readable format but they have
something really important stored in
them they have details about every
single transaction and every single
operation that happens on that device
okay
be it a computing device or beat a
non-competing device so you have details
about those things inside your knobs now
what will be the operations and
transactions that I am talking about
they are these in case of your server
you will have materials about what what
is your background of your customers
what is the IP address of them of those
people which is the location or the
geography room from where they are
accessing it we have those details and
then you will have details with respect
to there being any security threats are
there any vulnerabilities to your
servers to your network are there people
accessing it from a very strange place
are people eavesdropping on your network
you have answers to all these questions
but initially they are not just answers
they are just details along with all the
other data you have these data related
to even these
these facts inside those logs okay and
now when it comes to system you will
have details about your system
performance
what is the CPU usage what is the load
how many users are accessing your CPU at
what time what did they try to access
and which application orbit software is
running in your system and because of
that how is your system responding to it
so all these details right so anything
that happens any transaction any
operation that happens on your device
that gets registered in your logs now
since your logs have so many things it's
like a treasure it's very tough so I'm
pretty sure you people would have played
the treasure hunt during your childhood
right so logs are something like that
you can even think of gold mines for
example they are like gold mines okay
you have you have to dig deep to get the
actual benefits but when you dig deep
when you get the actual data you're
looking for then you have this nailed it
you go to the next level where you know
exactly what your problems are what how
your operations are being performed and
if you tweak a few things here or there
then how that will impact your business
now when you have such answers then
you're definitely gonna gonna be
operating at a very high level and this
very act of operating on your this very
act of understanding your business
understanding your data and improving
your operations is called as operational
intelligence correct and the solution or
the details about how to achieve
operational intelligence it's all hidden
in your logs and that is why exporting
logs is really really important
especially in today's world where the
internet and you know internet is ruling
correct this is the most important time
that we need to leverage our logs but
like I said exploring these logs
understanding these logs are not easy
because they look something like this
correct they are not in human readable
form imagine this this cat when you put
yourself in her shoes what would you
feel when you look at these logs and
durbin you are in a corner and you have
to make a decision supposing you are
informed that there is some secure
breach okay and you know that what the
security breach is what the problem is
you know that it is present in the logs
so how will you fix this problem can you
read these logs and get some meaning out
of this log can you definitely not right
it's an impossible task and such logs
they get generated massively almost
thousands of such lines of codes get
generated every single minute in fact
and what happens is every single
transaction let's take for example the
web server of any e-commerce company or
you can even take example of a Tory car
here so every single user that is trying
to access Enriquez web site or something
like amazon.com or flipkart.com their IP
address will be recorded along with that
whatever clicks whatever links they open
whatever whatever action they perform
all that website those get record and
they are stored on the web they are sold
on the logs inside the web server okay
now that is an example of this at that
kind of a time if you if you are told
that there's such a security breach and
you have to identify where the problem
is from and fix that problem then how
will they do it at a sudden you know
instantly how will you fix it impossible
right you cannot check any threats real
time you cannot analyze any business
metrics also in real time you cannot
understand how many customers want to
buy what products you cannot do that
what you need for this is you need a
tool which can understand your logs and
explain to you in a very simple manner
and that's where Splunk fits in ok and
as far as I am concerned Splunk is the
ultimate log of collection and analysis
tool why because Splunk does all these
things
Splunk does real-time log forwarding now
when we say real-time not forwarding it
means that it can collect logs from one
particular instance or one particular
server and forward those logs to a
remote instance ok so you can see that
this system here right you can assume
this as a server probably a better or a
router and from here whatever logs are
collected they can be sent to the remote
location they can be sent to researcher
where the end users will be
looking at the data and they'll be able
to get insights it will be a very human
readable form at that time minutes here
so I will give you an example of how
read abilities on Splunk and how non
readable it is from a server point of
view when when I show you the hands-on
okay but for now just understand this
part that a salon can do remote
forwarding of data and let you visualize
and get actual insights it makes your
job hand a lot of easy that's one thing
the second point is that Splunk does
real-time syslog analysis now again
syslog analysis is something like you
know server analysis itself but here it
can be installed on any system of yours
and it can monitor any application based
on whatever raus system logs are being
generated in real time and we can
perform analysis on that and similarly
you can install Splunk on any servers
and you can perform monitoring and
understand what is the IP traffic how
many people are there on your website
what are they trying to perform what
actions are they trying to perform are
they trying to eavesdrop on your network
are they trying to you know are they in
German customers were trying to buy
something and if they're if they are
buying something then based on these
dogs you know what are their activities
so how do you think ecommerce companies
are able to target you there's something
called as target marketing right
retargeting so all that happens based on
whatever is there all your logs you get
insights about your customer behavior
you understand if your customer is poor
if he's probably trying to view a
particular product a number of times
then it's an assumption that okay he
wants to buy the product and based on
that you get a hint that okay you can
give em an offer and uh close that deal
so that's what accompanying ecommerce
companies are doing nowadays right so
that's one thing that's one example of
how real time server monitoring will
help you and besides this I also
explained how you can analyze security
threats and all those things right so
that's one working so another another
application of first knock is that it
can do real time it can give you
real-time alerts and notifications so
these are also notifications would come
in handy when there's an actual security
threat or probably when something
is about to happen or something strange
is happening to your servers if someone
is accessing your network from from a
very unreliable
you know source then immediately you can
configure Splunk such that it would
throw you an alert when it when it
realizes that there's and there's a
request coming it from that particular
IP range and that can be done and
probably you can also installs block on
some system and monitor the CP
performance and whenever the CPU
performance or the usage crosses a
threshold then your raw system might
crash so at those times when it crosses
the threshold immediately
Splunk and give you an alert and tell
you that ok something's wrong that's
gonna happen over here so do the needful
and you can immediately fix any of the
food coming losses you can avoid them
altogether
that can be done but most of all there's
one under you know under valued a
feature of Splunk that is historical
data store and performing analysis on
that data
alright so people always think that it
is real time real time and real time but
no you can also do this whatever data
comes in at real time that can be stored
in your raw Splunk indexes which is
nothing but the database was blocked and
from there you can perform analysis so
you will have a set period of time till
when you can perform analysis right now
let's say there's a concept of buckets
ok and you can see that you set your
bucket size for 30 days that means that
whatever logs have come in in the last
30 days that will be very easily
accessible and maybe beyond that 30 days
then you will have another bucket where
it will be stored and in that bucket it
might be stored in a compressed format
or in an arcade format and things like
that
but however they can lead to even but
beyond a certain time you cannot store
it right you will it will definitely get
deleted so something like that so you
can make use of all these historical
data lock store and you can gain a lot
of business insights and a lot of
advantages and I might also want to tell
you at this point of time that wench
blocks towards data it's George it on a
compressed form so it's not like
whatever data comes in its going to be
stored straightaway
no it's going to be compressed at one
level at to one point of time and then
later man gets archived it will be
companies
more so these are the different benefits
that you get it's Blanc and that's why
exploring locks is very very easy with
Splunk and now I would recommend you to
do that okay now that is just about what
Blanc is but now let's talk about some
of the customers that have used Blanc
there okay let's understand a few use
cases here few of this companies might
be really popular here water phone a
Domino's ing bank these are some really
popular companies but there is one one
New York ad break right so this is one
one organization which is basically the
u.s. they are part of the u.s. railroad
Association right now they are another
subsys touring and they are the most
magnificent stories that I've heard
about
Splunk success story let me start
explaining them one after the other what
its world water phone achieved by
implementing spell water phone basically
are using Splunk to manage big data and
map key performance indicator any idea
how your phone is generating big data
and what you're doing with that big data
anybody okay
so we all know that water phone is a
telephonic service provider correct so
what happens every call any customer
makes any user or whenever you send any
message there will be a lot of logs
generated all right so the telephone
industry is one undersea where a lot of
logs get generated and it's very tough
to handle them so that's where water
phone has covenant they have salary
links plant to manage these logs which
is nothing but big data and based on
whatever happens there based on customer
behavior based on their pattern of usage
they can optimize it and give customers
offers if someone is probably trying to
you know use a lot of internet and then
when his balance is going low or if his
net pack is getting over then at that
time
what if okay water phone can come in and
give a very customized offer to that
person and closing deals right so on a
high level that's an example I'm giving
you but yeah that's how water phone is
using Splunk and this new york add break
this is one other excuse me guys so New
York add break it's one it's
it's another company which huge Splunk
and they got it they go they say around
1 billion dollar in just one month by
implementing Splunk well what did they
do
the ghost is plumbed to monitor the logs
of how the breaks you know the logs of
breaks being applied in the drains
correct so we have the you a New York
railroad right if they are part of that
organization and whatever our trains are
and whenever brakes are applied those
logs are rod those logs were recorded
and
and basically based on whatever dinner
they collected they analyzed that even
breaking for a few seconds during the
journey of a train resulted in consuming
a lot of fuel later when the brakes were
released and acceleration was applied
correct so by just applying brakes and
releasing them and you know you're
increasing the speed by hitting on the
gas pedal they realize that a lot of
fuel was consumed and this they got it
through Splunk by analyzing the brake
logs and what did they do
they saved 1 billion dollars in just one
month and it was us is a railroad
organization which saved so much money
that is about them and then there is
Domino's Pizza which of course is
another popular case you know use case
and I've written a case study about
these people and I've written a blog on
them so if you want to if you want to
read more about Domino's then you can
definitely go to one of my blogs on
Enriquez page and you can view it there
okay but what the owners did was they
understood the behavioral patterns of
their consumers were was buying their
products they understood at what time on
what dates their customers preferred to
buy edible items and based on those
details Domino's also gave you know
certain offers which helped them close
these faster and healthier clothes
customize and do target marketing so
that's one thing and then there is ing
bank right so ING Bank are they are one
of those few bands which have come
forward and said that they are using
Splunk because a lot of companies a lot
of banks have limited Splunk but they
don't really reveal it and ing bank have
been pretty brave and revealing that and
they've said that they use plans for
faster troubleshooting of four key
application and yet insider to customer
behavior well it's not just these things
not just troubleshooting certain
applications are getting insider to
certain customer behavior it's also
about monitoring the bank accounts and
are there any fraudulent transactions
happening are there any failed attempts
after which there's some kind of
transaction happening and if the
happening they are there to make sure
the transaction doesn't
right so they can do that but how will
they do it how will they realize that
you know there's a fraudulent
transaction happening by basically
analyzing logs are coming at real-time
correct and who's going to do the
analysis in real time it's Splunk Splunk
is going to do the analysis and give it
to the end user the end users here is
nothing but AG Bank and they can you
know get insights into it and take
actions accordingly and similarly there
are a lot of other domains where Splunk
is being used and in Splunk is one such
tool which is used almost in every
single business because almost every
business today are on the internet they
have an internet presence right so
that's where it's plug really benefits
and that's where even companies using
splott they also get a real big benefit
out of it so that was about their
customers and use cases let me go
forward and actually talk about
houseplant works okay let's talk about a
few of this plug components off now the
most important components are the
primary components in Splunk are those
of forwarders indexes or searches okay
in a single instance deployment you have
these three components which you would
be dealing with but when you go to a
more distributed deployment of plan core
or doose's plank in a very vast manner
at that time you will have multiple
forwarders in place you have multiple
indexes again multiple searches and
along with this you will have a few more
components which would manage these
basic components and they are nothing
but a deployer cluster master and
deployment server okay let me explain
each of these components one after the
other
I want to forward all do forwarders are
basically responsible for collecting
your data and forwarding it to another
Splunk instance and in this case it
would be the indexer the indexer is
where the data is being stored whatever
logs come in in real time or maybe not
real-time that will all be sold in the
indexer but when it's inside the indexer
you can't just straightaway look at this
data we got to use the search head which
will in turn
access the data that is present in
in Excel and it will let you do the
analysis or visualization and reporting
give you alerts notification all those
things so that happens at the serger so
we interact with the search at the
search it accesses the data from the
indexer and the indexer gets the data
from the forwarders okay if these three
if these three components do this match
then what about the other three
comprehend you see on the screen what
did they do you can have the question
now that's where the concept of a
distributed deployment comes when you
have a massive infrastructure when you
have a lot of radar coming in then one
indexer one forward and one so chain is
not enough because your data needs to be
safe correct you cannot have data loss
or you cannot have downtime so for that
purpose you have other components like
deployer clustered master and deployment
server in place what is it deploying
into and the deployer is basically
responsible for making sure that any
updates to configurations or any updates
to operations like server save searches
like reporting all those things are sent
do the clusters okay now all these
clusters which are there here there are
also called cluster members and these
classes together they are called search
at clustering okay this concept is
called
resource at clustering and the deployer
is basically responsible for scheduling
or sending any updates and monitoring
all the clusters all the searches in
this search actress rate and similarly
you have all the indexes right so they
are called indexes or peer nodes in a
disability environment and these are
basically monitored by your cluster
master you know cluster master is the
most important component in your drug
distributor deployment and your master
will always be responsible for two
things
checking your indexer clustering this is
called an extra clustering okay your
trust master will be responsible for
making sure all your peers in your index
and cluster are active they are up
there's no downtime this replication of
data that is happening a lot of you
might be aware of data replication so
data replication is nothing but
replicating you know having multiple
copies of data so that you have higher
value already so that's what this does
and your cluster master ensures that
all these nodes are up and running at
all the time and there's represen
happening and your smoke instance is
behaving as per your expectation and
besides that it also manages your raw
different search heads and it tells your
searches where to look for data
supposing a search it wants to search
for data in the indexer which would
which is against run in a remote
location which IP to go to which indexer
to access and get data so all these
answers the classroom master would give
to the captain over here in this circuit
cluster okay and then now you have the
deployment server which is similar to
deploying the deployer would issue
configuration updates to the cluster
members right but the deployment server
will do a similar job to the forwarders
if there are if there's any change in
the kind of Reno you want to pull from
the source and sent to the destination
then those updates those application
updates or consecration updates all
those updates you would be sending and
scheduling through your deployment
server right that's how the changes
would take would be brought into effect
inside the forwarders so on a high level
this is how things work okay these are
the different components and this is a
very complicated diagram if you're if
you're new to Splunk right but don't
worry we will have a couple more live
sessions on Splunk where I will explain
things in a more better fashion
incremental e I'll explain the different
components right but before I do that
before I go on to talk about what I'll
do in the next session I want to wrap up
today's session with a demonstration
okay
so here I explained the forwarder the
indexer and the searcher so what I will
do in my demonstration is that I will
collect data from my forwarders and send
it to another Wilson which is nothing
but my indexer now since I will be using
a single instance deployment then it's
just going to be one index around now
once I've said so I can do both those
things from my the instance where the
data is coming to okay so that's what I
am going to do and this is called of
course log collection with the help of a
universal forwarder so
let me get started with my demonstration
and for my demonstration I'm going to
use a double use instances that I have
installed Splunk okay over there I will
configure these logs to go from one
instance to another instance and I'll
show you how Splunk will houseplant
plays a major role there right so I'm
going to first fall open up the areas
instances and guys do give me a
confirmation if you can see my screen
here okay
I suppose my your screen is visible here
so this is my Erebus instance where I
have a couple of in fact I created this
whole cluster of instances for my bank
disappear deployment okay but however
I'm not going to explain each of them
today and if you remember from the
slides you can recall all these
components there's a deployer and
deployment server there's a cluster
master there are two indexers in this in
my deployment here and then there are
again two search heads and then there is
one universal folder now what I'm gonna
show you is I'm going to show you how we
use a universal folder to collect logs
and send it to one remote instance on
which I'm gonna perform analysis and
analyze that system logs okay I'm going
to take two intervals instances sorry
I'm gonna take to a turbulence instances
for that purpose and let me first start
off by bringing up my servers okay let
me first okay
refresh my revel use console and I'm
going to log into my inner base
instances with the help of putti since
I'm using Windows okay now this is my
cluster master this is the IP address
and let me put in the IP address and
login to the instance okay this is the
key that I say
okay successfully logged into my cluster
master now let me login to my Universal
forwarder so this is the IP address let
me create another putting instance so
here I'm choosing my private key
I say open and my connection is
established for my forwarder also okay
so great if you can recall then this is
my what you see on the left is basically
my cluster master and this is my
Universal forward up okay and this is
gonna maximize these two right now what
I'm gonna do first is I'm going to show
you how to install Splunk okay in fact I
already have it installed but enemies
gonna show to you because it's a simple
process
I'll just showed you and to end from the
beginning but remember that when you
whenever you have to run spam through
the CLI then you have to always be
logged in through root user so always do
this command sudo su and then start
performing your operations or commands
right you need to root privilege for
that okay so anyways moving forward
let's go to the opt directory where and
where we need to install any add-on
applications to your server or to your
system right looks that's how it works
so when I did an LS you can see that
there's already block in soil so what
I'm going to do is I'm just going to
remove this plug okay I'm going to use
the command
Splunk
okay so I've removed the Splunk and this
is just the tar file right so similarly
let me go to my universal forward
okay I'm gonna do a CD and move to the
opening directory then over here there
is the spunk forwarded already installed
so I'm gonna remove this okay great so
now I'm going to install both these are
the universal folder and my spunk
enterprise instance from scratch and our
since I have a tar file I can run the
command tower - xvf and the file name
the tar file name okay and when I hit
enter my fine we'll start getting
extracted
okay so we're learning go to my
Enterprise instance and do the same here
let me see
dar - xbf it's plug okay so even this
has even this is getting extracted now
in the meanwhile let me go to my
Universal forwarder and start performing
my operations so I'm gonna clear my
screen once now guys you got to remember
one thing okay before you before you get
your block instance setup you have to
make sure that you set the right
permissions for your folder so you have
to always give the read/write/execute
access to your to this folder even if
whether it's a root user or if it's any
other revision okay and the command to
give the complete access is this this is
the command chmod Drupal 7 and the
folder name so Mike is it gonna be
Splunk formula right so I've got my
permissions over here let me do the same
thing on my block Enterprise instance
also LS okay I have my folder here I'm
gonna say chmod - R and this folder okay
since the permissions have been set
let me start bringing up my instances
okay
this is my Enterprise instance which is
going to do my ass which is going to be
my index and on my search right so what
am
- here first is able to start the Splunk
instance to start this pumpkin since you
have to you have to go to the bin folder
inside Splunk directory okay
and over here if you do an LS you can
see we have a number of files and
scripts and there is this Splunk which
we have to start this demon okay so I'm
going to say God slash Splunk start but
since I'm starting Splunk for the first
time after downloading and installing it
yeah you have to remember you have to
give this flag of accept license okay
well even if you don't give it it's fine
but it's just that when you don't say
when you don't give accept license
you'll be prompted to you know keep
scrolling down until you agree to the
complete document so it's just a shot
away to war you know get yours plant
instance up and running so it says my
web server is up at this IP address so
let me go to my plant Enterprise
instance okay so that was my cluster
master right so this is the public IP I
can access it through my public IP okay
I'm gonna just paste the public IP here
and port number 8,000 okay so there you
go
for number eight thousand of this plump
servant of mine I have my Splunk
enterprise and since running okay and
this enterprise instance is what would
be my indexes and search it for this
demonstration and remember guys every
time whenever you install this plug the
first time when you're logging in you
don't ask you to change your password so
first time it's going to be admin itself
and then the password would be changing
the first ever you login okay after you
log in it will prompt you to change the
password so do that also I am entering a
password here and when you do it for the
first time you have to do something
similar to that see your password and
then this is basically my Enterprise
instance okay this is how Splunk looks
like and you can easily search for data
from this application and before I get
to this okay let me show you this first
of all when I clicked on that app search
operand reporting let me show you that I
don't have any data right now in my
instance okay to give you an example I
can search my index okay because index
is where always your data is stored okay
the default index will be main so I'm
going to search for the main index and
I've checked for data all time and you
can see that you can see that there are
no results from that because I have not
sent any logs or any data to this
particular instance now to do that to
send mine your system logs or my server
logs which is on which my account
forward is installed after first send
data right that is the first step so let
me do that since I've extracted the
folder here I can straight away start
the strong concerns over here also so
let me first go to the Splunk folder
inside here there's a bin folder okay I
go in here by doing a CD command and in
here when I do an LS you can see that I
have so many files and again there's a
Splunk which script which I need to
execute okay I am gonna do is plug
start this time I am NOT using the
accept license I'm just doing it just to
show you that know how pinyin it's a
painful task to do if you do not see
accept license you can see you're right
it says more 4% so you have to keep
hitting Enter
Angela until you read the entire
document so we can stop this by hitting
ctrl C and to just avoid this particular
law to avoid this thing having to scroll
down come and until you completely read
the file you can use the flag accept
license right so this is the flag and
when you hit enter okay
oh here I have a spelling error here so
that's why it's now now I should be good
yes so the same it just completely
accepted my license by when I specified
accept license it assumed that I accept
the license and then at this stray a
star in my gas tank instance now if you
noticed when I started my Enterprise
instance it said that my web interface
is active at this port number on this IP
address correct but however for my
Splunk forward
that did not happen and this says that
it started maya Splunk server demon but
it did not show me because for your spam
folder you do not have a GUI you cannot
access it through the web it's only the
CLI and the configuration files through
it you should access splob or make
settings and configure data in right so
you will not have a GUI but however I
will whatever logs are any to send I can
send it by this running car through my
CLI and that's what I'm going to do but
what which data am I gonna send to my
block Emacs off that let me first figure
out
let's see where my system logs are
stored so this particular server system
logs since it's a UNIX machine it would
always be stored at this particular
folder right inside my bad folder there
is live so let me do a CD Lib correct
okay not Lib there should be a log
folder system log will be present over
here so I'm gonna say CD log and anyone
I do LS you can see that I have a syslog
file okay now I'm gonna do a cat command
to show you how big the logs would be
okay when you hit enter you can see this
right the this data is on structure and
this is definitely not unreadable format
and you have this data that will keep
getting generated every few seconds and
there's simply a lot of data now is it
readable definitely not correct so what
is the solution if I want to read this
and get insight or of this log data send
it to the Senate to my stock in their
instance to my Enterprise instance which
is gonna act as my index error and
search it and then perform analysis over
there correct so that's what I'm gonna
do
since it's present my slash bear slash
log folder
let me go back to Splunk bin folder and
then execute the command to forward data
so I'm going to say slash o pd / Splunk
forwarder and bin okay you're I'm gonna
run the command dot slash Splunk ad
forward server forward - server and then
after give the IP address of the
instance - which I want to send this
data right so the IP address of this
particular instance I can find it over
here so this is the cluster master
correct so this is the IP address I'm
gonna copy this go back to my instance
and when I paste it here along with the
IP address have to specify the port
number where is that plump instance
would listen to data correct there are a
number of ports which will be open in
Splunk and for receiving data the
default port number is data per line
seven correct so I need to specify that
over here
so by specifying this I'm basically
telling this particular founder of mine
to add a particular server to where I
want a forward data okay and the server
address is this one and send to port
number triple nine seven of this
particular server address correct so let
me hit enter then it says your session
is invalid please login okay the
username is admin and the password would
be change me correct because the first
time when you try to log in it should be
admin and change me after that you will
after you login it would be you can
change the password but however for a
forwarder you cannot change the password
so at first logon you have to change the
password so let me just retry that okay
okay sorry
let me just execute this command want to
go to work clear the screen okay I'm
gonna add enter this command and the
username is admin by default and the
default password would be change me see
much en GE yeah me enter there you go it
says add it forwarding to this
particular IP address and to this port
number in that IP address so the data
forwarding pipeline is ready but at this
point of time of the still not mentioned
which file to forward correct that's
what I'm going to do now let me do a
clear command and run the command to
forward data and the commander of
forward data is again dot slash Splunk
however the same it would be ad monitor
okay add monitor and then after I
specify the path of the place where the
file is present and if you remember it
was there in my root there log syslog
correct the slock was the name of the
file and it was present in this path if
you remember and along with specifying
which file you want to send you should
also
specify which index you want to store
this data in okay the data would go to
my index and instance and over there the
index that I want to store this data in
should be mentioned over here with a
flag so - index and I'm going to say
let's just total index main ok and then
I'm going to specify the source type so
source type for those of you who are new
to Splunk source type is basically used
to identify what to identify the type of
data coming in right so it's pretty
obvious from the name it's a source type
type of data so I'm gonna say it's my
forwarders server server log so let me
give the name my universal forwarder
server logs okay I am going to say enter
and what time I told I'm told that it's
added the monitor of this particular
file so at this point of time the data
might be going in to my instance but
there might be one problem right now the
data might still not be available at my
in necks of beakers over here I have not
opened the port number triple nine seven
from where it can be listening for data
correct so if I want to accept data then
I have to open the port number triple
nine seven and to do that the command is
Splunk enable listen and followed by the
port number that is nothing but triple
nine seven okay I need to log in here
also let me say admin and this time it
is a different password because I
changed my password at first logon okay
it says listening force plug data on the
TCP port number triple nine seven very
good so now if I open the GUI of this
particular law index or instance I can
see all my data okay so this is that
particular instance right so let me just
go back to the home home dashboard okay
refresh it and when you go to the search
and reporting option that you see over
here so everything here is categorized
as applications okay so this is the
default application that you get that
snug so if you want to search what data
is there in your index then you got to
click here and when you go here you'll
get a lot of options so along with that
it's also saying that I have got four
thousand and twenty events indexed okay
and less than a day ago that's because I
sent it now in real time
correct the latest event that was
generated what was sounding minutes ago
correct so let me start searching for
data if you remember my data I was
storing it an indexed main so I'm going
to say the index where I want to search
for data and I'm going to say index is
going to be mean and over here I'm going
to say search and as you can see
whatever I entered over there
whatever logs hours I wanted to forward
from my for instance that has been
forward over your if you remember I gave
the source type as my universal forward
server logs correct and it's also giving
me the same IP address I can show you I
can verify that also remember the
private IP is 172 170 2.30 not 0.123 now
if I go here and this is a cluster
master correct so here if you scroll
down you can see the private IP address
which is nothing but okay sorry this is
the private IP address DNS name right so
this is where it's being sent
oops sorry I think I explained it wrong
what am i you're saying the data is
coming from a universal forward correct
so my Universal folder IP address would
be this one 70 2.30 dot zero dot one two
three and that is the same that you see
in my instance of indexer IP address is
the same where slash log slash syslog is
the source from my readers coming in
this is the host and this is the source
typist my Universal forest server logs
okay perfect now whatever events are
coming in so in your logs you will have
events right so you have multiple events
coming in and those events would be
different and they would have different
values for at four different
transactions so the difference would be
only that and that is what is what a
common here as different events and if
you want to identify each of them you
can simply see it here if you want to
drill down and filter the data or then
you can choose which you want you can if
you see on the left hand side you have
selected fields which are nothing but
the default fields and then you have
interesting fields so if you choose the
host then you will only get all the data
from this particular host okay when I
checked on that the automatically got
filtered over here in this case I am
getting data in only from one particular
source so I have only one host over here
but however in the real instance if you
have course you got multiple for us then
you can you will have multiple host over
here and you can check which host send
you which data or what data that you can
do similarly the same thing with source
if I have only configured one input that
is the thing but however you have other
fields where there is a change in data
so you have different options here so
the date the are in match the event
human okay so you have different values
for it so you can choose that you can
choose one of them and you can choose on
which month of the day your events are
coming in so you can see that values are
16th and 15th it says on the 15 these
many events have come in and on the 16th
these many major
now that is because if you see that is
because this may be running I'm running
this particular instance in in a US
server so for that because of that
purpose it might be showing me two
different time zones that's one thing
and then it has other filter saying at
which minute how many events came in so
at the 56th minute of the are they we've
got 730 events correct and on the second
minute of the are we've got 727 events
so if I click on this 56 then I have a
few more events correct so it says from
this IP address and on the 56th minute
of FDR maybe two o'clock and three six
minutes three o'clock or 56 minutes at
these times we have these many events
coming in if you want to filter this
data even more you can do that also on
the 56th minute
you can choose on you know you can
choose on how many choose from the
number of days that you have so right
now it's showing that only I've got data
and only on the 16th right this is the
month of Dale so if I choose 16 it's
slower filter too many things but yeah
and that's another filter that can be
applied but if you want to filter this
data even more then you can click on
this date second okay so here it shows
that on the 24th second of any minute
we've got these many events coming in
and on the 25th second and so on these
are the different seconds on which we
have multiple events coming so on the
24th of and if I if I filter it for that
time then you can see that they all have
a similar timeframe and these are all
those logs okay and then you have the
difference can be viewed here but of
course this is just a system log and
here there aren't much of insights which
I can show you and I cannot do a there's
no point in showing you're showing you a
report over here okay but however in my
next life session I would give you a
more detailed explanation of this plank
search and reporting commands okay these
are some of the certain reporting
commands these are just a few of them I
will give you a detailed explanation of
a number of other commands in my next
life session I am the teacher mode
hopefully we can learn a lot from each
other you can ask me a couple more
questions and I'll get back to you on
that and that is going to be one session
and that session is going to be
knowledge objects okay because these are
all knowledge objects at the end of the
day that's one thing and I'll also
probably have another session where I
will show you a distributed cluster
Abell so I will show you everything from
indexer clustering to search a
clustering and to setting up your remote
forwarders so whatever deployment
whatever components are you see here
right all these components I would
insinuate on different instances and
show you a complete end-to-end
distribution of Splunk and that is what
is a daily activity of a slump Atlas
reader and I don't need to remind you
how valuable strong catalyst traders are
in organizations right so these these
topics are what would be coming in my
upcoming sessions but till then stay
tuned guys this is pretty much the end
to this particular demonstration of fine
so let me just go back to my slides and
current load today's session and in the
meanwhile you can put in your questions
and I will pick up a few of those
questions okay and these are the topics
that I covered this is just my session
in a minute I spoke about the input
importance of logs and the need for
Splunk I spoke about what exactly is
plant I spoke about the different
components and I give you a
demonstration of how to collect logs
from remote instances and perform
analysis on that data
fantastic right so on that note let me
conclude today's session but if you guys
have requested put any questions let me
just take them up let me see what
questions are common
okay so I cannot see a number of
questions you have any relevant
questions so anyways guys any doubt you
can of course comment on this video
itself and later I'll get my technical
team to reply to those questions alright
so let me just show you one more thing
before that I want to show you that I
want to let you know that there's a
survey form which you can fill and when
you fill the survey form you stand a
chance to win to avail an exclusive
discount right I would request you to
fill in this form and someone from my
team would be sending you the link to
this on the chat box so pick it up from
there and fill in fill this out and you
can also of course mention if you want
to be notified of any other of your live
sessions on and on which topics you want
to be notified of so you will get those
details and for those of you who are
serious about becoming a Splunk and a
straight red power user then we have a
trigger we have a course over here and
you can check out the course from this
URL and you can check out all the course
details here and of course this was the
blog which I was speaking about Splunk
use case Domino is a success story you
can read the entire block to understand
how Splunk was a big help to Domino's
and growing their business during their
young years and finally we also have a
youtube presence and now you can check
out our raw Splunk tutorial playlist
here right you can check out on all
Rodriguez channel and guys most of all
remember to subscribe to our Channel and
hit on the bell icon if you want to be
notified of any live events in the
future okay so I think that's it from my
side I'll be covering today's session on
that note and I'll see you again next
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>