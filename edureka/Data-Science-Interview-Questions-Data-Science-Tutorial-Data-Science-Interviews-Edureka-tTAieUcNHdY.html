<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Science Interview Questions | Data Science Tutorial | Data Science Interviews | Edureka | Coder Coacher - Coaching Coders</title><meta content="Data Science Interview Questions | Data Science Tutorial | Data Science Interviews | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Science Interview Questions | Data Science Tutorial | Data Science Interviews | Edureka</b></h2><h5 class="post__date">2017-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tTAieUcNHdY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey I all lost to the session on the
panning for data science interview
questions I'm Karthik so I've been like
working in this data science domain for
now close to seven eight years with
experiences in companies like Reckitt
Benckiser Snapdeal hike messenger and
currently I am running a company called
probate Oh
where we develop R&amp;amp;D driven data science
solutions for some really complex
problems particularly faced by various
industries so in this session we are
going to focus on certain fundamentals
questions and the thought processes you
need to sit for and data science
interview and then from there on will
make the questions a bit more complex as
we move on and we will try to answer
which kind of skill sets people normally
look at when you appear for a data
science interview in any company so
there are like various roles depending
on that the complexity and the variety
in the questions might change but this
session will try to cover things in a
fundamental level so that it prepares in
a very common ground irrespective of
which role you are applying for you you
might be able to at least get a
perspective of what data science
requires as such right so with that note
let's see how we going to structure the
various questions that's going to come
up in this session today majorly
focusing on statistics data analytics
machine learning and probability so
these are the four headers under which
the questions will come and I will
discuss the question and how to approach
a particular question in the structured
approach right in a structured manner
and once we understand the problem we'll
we will see how does the thought process
of coming out with the solution really
works right so all of you you know write
data Sciences in booming state many
openings are coming up in across the
globe in various companies irrespective
of which company it is which particular
business it deals with there is a
potential of some applications from data
science right they have lot of data
different systems of the businesses
generating the data with
an enormous amount of volume velocity
and variety so with that sort of data
asset that every company has now got
they want to now leverage it towards
growing the business to the next level
so you have some of these stats here
like this is more than sufficient
motivation that getting into data
science would definitely land you in
some really good professional career
okay so let's start with the questions
here directly fill in the beginning
start to focus on some of the
fundamental questions which is more for
you to understand what is data science
then like a particular interviewer
asking you that question
for instance many people wonders what
data science is all about right though
there are many online sources and blogs
which describes data science in a very
nutshell this is what it boils down to
right a person who is very good in
understanding the computer algorithms a
person who understands the statistics
and mathematical ideas and applying
these to knowledge is from the computer
science and mathematics into a
particular application right a business
application where somebody sees a value
coming out from the data so how do you
apply that so that's how kind of data
science approaches so when you combine
these two powerful concepts coming in
from computer science and mathematics on
a real-world application the sort of
outcome that comes from particular data
science project goes in a direction
where people see our return on
investment right so the people you bring
in the technology you bring in the ideas
you are working on all of it should kind
of gives you some return on the
investment you have put in so that's
where industries out started to looking
at data science so various subjects
which are very important for you to know
data sticks computer science applied
mathematics and then subjects like
linear algebra calculus and a few more
right so fundamentally from computer
science algorithms and data structures
might be very useful from the
mathematics or statistics and things
like calculus linear algebra matrix
factorization and concepts like that and
from the application it is more
from your experiences from the industry
if you have worked for retail you know
how the business process in retail works
right so people often also ask from the
technology and do we need any sort of
experiences in Python language right
Python or like for example arts
programming as well so python is one of
the most looked out for kind of
programming skills particularly when you
want to build solutions in data science
domain and with availability of
libraries like numpy pandas a Python has
now established its compound very
strongly in giving a very robust
framework for designing data science
solutions right and in particular things
which it has like lists dictionaries
tuples and states are one of its own
sort of one of its capabilities which
sets Python in its own league of
programming language is suitable for up
coming out with a design solutions so
there are many more other libraries as
well apart from this for building
machine learning algorithms but these
are some common libraries that you would
not really find people using it and also
with distributions like anaconda Python
has shown its capabilities even for a
protection grade solutioning rake
wherein you make sure that all the
dependencies that one particular library
has for building a data science Ellucian
is all in one place so quite a popular
programming language in the recent kind
of happenings though our programming is
also fully good in terms of producing a
quick prototype for most of your
modeling tasks but Python is moving
itself into a production grade where
things can be deployed after the sort of
prototype into a production environment
and it can face the customers from day
one so that sort of capabilities are
coming up with Python right so let's
talk something a bit more specific with
the data right so when people are doing
any sort of data analysis they normally
face something that we know by the name
selection bias right so what this is
actually a selection bias the
fundamental place where you start doing
a data analysis is by selecting a
representative
simple right so that's where we like
normally start doing any analysis so
when you like working for a company
which has let's say 1 billion records in
their databases like 1 billion is like
very very large number which represents
various customer customers data or it
might also be depending on which feature
you are working on and so on so if you
collect all of those it might very
easily come to one billion records which
is nothing but in a structured form
number of rows so with that an enormous
amount of volume in the data any
analysis that you take up might have to
have lot of filters like saying I only
want to analyze a particular feature in
my products let's say like and I won't
only want my customers from X Y Z region
which is let it op for region or top.i
regions so you might like put many
features or filters like this but later
on if you would like to do a kind of an
analysis which covers most of your
customer base right there comes the
tricky situation of not being able to
use the entire volume of 1 billion
records at the same time you want to do
a really good study or analysis based on
what data you have so in statistics we
will normally use this idea of doing a
kind of a randomized selection right so
with this randomized selection we make
sure that out of these 1 billion records
we are choosing a small subset let's say
of 1 million which is a true
representation of the entire population
right but what happens with this is
while we do this 1 million record
selection in a randomized way there are
chances that you might have certain bias
in the analysis obviously because you
are not using the entire population so
selection bias is this particular sort
of a characteristics while you are doing
a sampling on a large population of data
very common example for this is if you
want to do a exit poll analysis of
particular election even before the
election results are coming up and you
have not chosen a representative sample
for doing that exit poll analysis right
by that you I mean you only have asked
some questions for a selective view from
the particular constituency and they
have opinion towards one candidate but
that doesn't represent the entire
opinion of the population in that
constituency so selection bias is like
very important to handle
and most of the time people employ
things like randomized selection or
selection sampling techniques like
satified sampling and so on so by that
you can minimize this selection bias so
these are some very generic questions so
let's start with some sort of
statistical questions and also how to
deal with different types of data okay
so doing any sort of data with a
structured information so structured
information I mean there are many rows
and like many columns and it looks more
like a tabular data right so with such a
data in in place there are like these
two different formats one saying the
long and the white so let me like show
you an example here you have a record of
two customers right and you store just
two values which is the height and the
weight so these are like in columns so
with the these two customers these
height and weight being a separate
column is one format like a long format
let's say transform this by having only
one particular column which will say
attribute right by that I will bring in
these two columns as one column by
calling that as an attribute and put the
values in one column so this sort of
format is called the long one so what
really happens is instead of having two
separate columns for two of your
attributes you put or both of those into
one column and by doing that there are a
lot of benefits with respect to the task
you have in your hand
particularly in data visualization
certain data visualization would need
you to not put your attributes as a
separate column but rather as the one
column which can have the attribute
names so which might then go into
building your legends right so these
kind of techniques are kind of very
common formats between like be longer
and divided and very frequently will
people like deal with both these data
formats depending on what task they are
doing particularly when we are building
the visualization dashboards okay
talking a bit more on the data analysis
perspective of people like know that in
stats normal distribution kind of is
that one like The Godfather of
you have many distributions which
normally people try to see if four is
present in my data or not but the moment
people kind of see normal distribution
coming up in any data things are kind of
bit easy to understand and in typical
case of any distribution any data
distribution that you would like to find
out given data for analysis it gives us
a lot of characteristics around what the
data is about time let's say analyzing
the salaries of the employees in my
company right I might see that there are
some employees who are in that like that
thick crust in the center where majority
of the people are sitting with a
moderate level of salary ranges and then
there are these sort of extremes on the
left and the right so you are like very
commonly people refer whenever you talk
about salaries to a bell curve right and
then they start talking about top 25
percent of the performers in my company
the bottom 25 and the middle one which
you like are sort of the normal
performance so this sort of bell curve
the distribution is very commonly
understood and as well as used in doing
many data analysis so is the other
distributions as well but normal
distribution has its own significance so
when somebody asks you around anything
around not normal distribution the first
thing you should visualize is the
symmetrical bell-shaped curve right and
the moment you get that bell-shaped
curve in your imagination start thinking
of certain properties like what is the
mean of a normal distribution what is
the standard deviation of our normal
distribution and in particular a special
case of normal distribution which we
call the standard normal distribution so
in that standard normal distribution we
know the mean is exactly zero all the
time and the standard deviation is
exactly one right so there are different
places where normal distributions are
kind of used and if you are comfortable
with ideas like central limit theorem or
the law of large numbers so you might
want to relate that as well to normal
distribution particularly the central
limit theorem right but the idea is a
distribution which is symmetrical around
the mean so that is what a normal
distribution is
so depending on which variable you are
trying to analyze even a given data set
whether it could be like an employee
salary or your sales in a for a business
of yours right or your number of let's
say interactions of the customers on
your product so any variable that you
define can have a symmetrical
bell-shaped distribution which we
normally refer by a normal distribution
and the moment we understand that
something follows a normal distribution
all these properties of that
distribution is like revealed so that's
sort of the importance of doing any
analysis around the distribution of data
a normal distribution like very common
one and in many statistical techniques
in even model building exercises if you
have anything in a normal distribution
many other possibilities of applying
certain modeling techniques comes out
evidently there and also there are many
other modeling techniques in stats and
machine learning where there is an
fundamental assumption that things
should follow a normal distribution if
it is not following then the model is
wrong there are many use cases of
knowing what normal distribution is but
in simple terms it's a symmetrical
bell-shaped curve right a be testing so
quite a popular approach people
particularly who are working with
product right and what happens is when
you are as a company having blocked many
features inside a product for instance
if LinkedIn is a company which has a web
page right it has a lot of features
inside that you have some jobs portal in
willing then you have places where you
can connect to your professionals in
this similar industry you can also read
about the post people are doing in the
portal and so on so there are different
places in the website with many features
so what happens is if LinkedIn as a
company is looking for some changes for
instance changing the entire website's
design in aesthetics design and
aesthetics or changing one particular
feature inside their website right so
these changes are normally accompanied
by sort of a process called a be testing
so what happens as an analyst you might
be working with Lincoln and say on a
fine day they might come out with a new
feature new design or new sort of
changes in their website so you as a
person would tell okay this is my
framework for testing
you change by defining a metric so my
metric will be in simple terms saying if
I change this website from A to B is my
number of footprints on the website
going to go down or not so this is my
metric and if I successfully establish
the fact that after rolling out this new
website my number of customers who are
going to visit my website is not going
to go down I can be confident that ok
fine this works now roll out this new
feature so in this framework we'd
normally have two set of users to
identify the particular risk associated
with getting this new feature in go into
the platform and in which we in a
randomized way put one user group and
expose them to an older website and
another user group and expose them to
the newer new features right or the new
website and when we compare the results
of a particular metric like the number
of clicks or the number of purchases and
so on we should be able to see that
these two groups are either exactly the
same or quite different and if it is on
the negative side of the difference we
said the feature is not good and even if
then the difference is not at all there
we say even if we bring this new feature
nothing is going to happen
so this a/b testing framework is quite
rob robust in its own way right and a
very common question you have like
worked as data analyst or if you are
expecting to be like sitting for this
data analysis and of the role Unknowing
a/b testing framework is very important
ok so sometimes also when you do these
kind of a be testing sort of analysis
people normally come out with sort of
saying what should be the sample size of
my users whom I should be getting to
participate in my a B testing framework
but also when you are building some
models you might see that there are
certain statistical measures so which
has to be evaluated by the end of model
building exercise like if you're
building a machine learning model let's
say and you want to see if the pose
matrix one which I am evaluating are
really good or not right so in that
sensitivity is one of those methods or
like matrix which we normally evaluate
and I'm going to show you some something
that we normally refer by the name
confusion matrix so I will spend some
time explaining this and then come to
what we mean by like sensitivity so
let's say you are building a model right
a model for predicting whether a
particular customer is going to purchase
from my platform within one month or not
it's a very simple problem statement
which might include many variables that
we would bring in and then finally build
this model saying ok this is my final
model which says with 90% accuracy
whether my customer is going to buy from
my platform that's an online e-commerce
platform within next month or not so
this is my model so now as we without
going into the details of the model
let's assume that after you build the
model you have got the results so while
we analyze and evaluate what the result
is all about we might come out with a
confusion matrix so what it says so
obviously when you are building a model
which follows a supervised way of
learning you will see from the
historical data after people have
purchased in the platform within next
one month whether they are buying or not
so I can obviously create a really good
training data set right which will
contain the information after people
let's do the first transaction with me
in next one month whether they are
buying the next product or not
so with that data set I train my model
and the wave confusion matrix puts that
is by saying my actual data say
something and you have predicted
something right so let's just kind of
get into the details of that so this
particular box which says my actual data
says the customer will buy and you are
also predicting the same so this we call
the true positive the prediction is
actually true right in a positive
direction saying the purchase happened
now like move that or diagonally
opposite to this TP which is this TN
just true negative which means the
prediction of your model saying the
customer will not buy is actually
matching with the actual data as well
right so both of these values the true
positive and the true negatives for the
two cases is the right predictions from
your model but consider the off-diagonal
elements the
false negative and the false positive
and these two cases this is sort of an
error why because your actual data says
the customer is not going to buy in this
case let's say but your prediction says
he is going to buy so the prediction is
actually positive whereas the actual
data is on the negative side so this is
a false prediction right so you have
your in this case your F P which is D
false positive cases and on the off
diagonal element if you look at this one
which is f n which is your false
negative for the cases where you are
predicting the customer is not going to
buy but at the actual data it says that
customer actually has brought the
product so in this case the model is
wrong so the type 1 and type 2 errors
needs to be taken care of when you are
building any machine learning model if
these errors are low then your model is
going to move towards that hundred
percent accuracy mark but normally any
machine learning model has its own
limitations so particularly there is one
metric which we prefer by calling
sensitivity so what happens is these
true positives and true negatives needs
to be controlled right if my model is
very good and true positive ones like
the positive cases of when the customer
is buying but it is doing very bad job
then the customer is not buying the
cases in which the customer is not
buying then the model has some sort of
issues right it is doing only good in
one place but doing very bad on the
other case so I need to find out that by
some bad trick so sensitivity helped us
to find that out which is in simple
terms as a ratio between the true
positive in the denominator we have all
the cases of positive predictions so
imagine now if the type 1 error is going
to grow my sensitivity is going to come
down so if my true positives are like
very high the sensitivity will also be
high so this is sort of what we call the
statistical power if this sensitivity is
really good I would say that my positive
cases are predicted well and the exact
opposite of the sensitivity is what we
know by specificity so we need to make
sure that in a very good machine
modeling model sensitivity and
specificity both are balanced right so
in very simple terms this is what would
be like
here the ratio of true positive by the
total positive events there and as I
mentioned both of these sensitivity and
specificity playing good roles when you
want to evaluate a models output right
and one more common problem so these
questions might be immediately following
one another when people ask you about
sensitivity and specificity as you know
it's about the machine learning models
output right then the model is done you
understand whether the model is good or
not so in those cases we also come
across some kind of issues like
overfitting and underfitting given
machine learning models right so these
words are very common and the idea is
depending on the complexity of your
model you might see that you want to
adapt very sort of exactly to your data
points or you might want to do a
generalization so for instance here if I
have these red and blue dots here right
and if I draw a curve like this which
separates the red from the blue and when
this separation happens I am building
actually a classifier using some sort of
modeling technique but now imagine by
drawing a smooth curve like the one
which is given in black you might be
over generalizing it right by which I
mean there might be some red dots on the
other side of this boundary you can
obviously see that but the moment I am
kind of a bit more flexible by drawing
this green boundary which actually
covers all that issues which is coming
out with these red dots on the other
side of the boundary so this green
boundary has like taken care of that but
the problem when we are building any
model right the idea is you need to
generalize to the pattern found in the
data so if you don't do that
generalization well you are under
fitting but if you do that
generalization like too specific you are
overfitting so this curve might be
represented by some polynomial right but
that zigzag kind of a polynomial might
be bit more complex then a smooth curve
like the one which is shown in the black
so you need to be very careful when you
are building a model particularly in the
cases of regression models where it is
represented by a line and
polynomial you need to make sure that
the polynomial is not so complex at the
same time not so simple then you will
either end up in an overfitting
situation versus an underfitting
situation so we need to have a good
balance between these two right so in
summary when kind of statistical
questions comes in it would mostly
covering things like some basic
statistical properties as you would be
like very aware of like things like
standard deviation averages how to
interpret median how to interpret
quartiles right the first quartile
second quartile and so and so on and
what do you mean by percentiles right
these are some basic questions a bit
more complex in nature might be
discussions around sensitivity
overfitting under fitting these are like
testicle ideas so you want to prepare
maybe from a basics level using the
properties like standard deviation mean
and so on till things like overfitting
under fitting statistical the
sensitivity and specificity kind of
ideas so that will like make your
scrounge a bit more stronger when you
are going for the interviews and these
are at least the bare minimum for you to
understand in these statistical concepts
right if anything less than that you
might like face some difficulties in the
interview okay but let's also talk about
now questions which are related a bit
more on data analysis so let's see how
what kind of data analysis questions
which might pop up in the interview okay
so some generate questions like this
people normally do analysis on
structured data which is in rows and
columns but there might be cases when
the data is not so well structured and
those places the data might be textual
for for instance in Twitter right if you
were doing any sort of algorithms like
sentiment analysis quite commonly known
algorithms so in that case the sentiment
analysis could be for a brand for a
election campaign or maybe something
else around your product features and so
on so text analytics in its own or a
little large domain and in Python as
well as are there are a number of
libraries so in particular are has
libraries like TM right
the
whining package Python as well we have
packages like pandas packages like the
numpy ones right and also packages like
NLT k which is built only for a natural
language processing so it can deal with
many different sort of text mining
approaches or text analytics approaches
so in comparison if you talk about as I
said the robustness in Python is much
more than in R but in terms of features
both are powerful enough with the
libraries and packages that it offers
right
one of the fundamental sort of starting
place when you do any analysis so then
you are given a data set and you are
asked to do some sort of basic analysis
of what that data tells you may be
typically of questions like I am in a
retail business and my sales in a
particular region is going down so this
is an analysis that is expected out of
you and you need to dig through in
understanding what really is the problem
in the sales going down so if this is
sort of the data this which is given to
you you might want to first look at the
transactional data which is present in
the system then you might also want to
go to the outside of your network maybe
you might get the sentiments of your
customers from social media platforms
and so on so there will be different
sources of data that you will collect
but oftentimes collecting the data is
not only the task right and not like
only building a model or doing
statistical analysis might like come
very later in this stage but what comes
before that after you have collected
your data is to make sure that the
integrity of the data is maintained you
get rid of all the unwanted noise from
the data and then finally prepare the
data for doing the sort of modeling
exercise or doing descriptive analytics
on top of it so this cleaning and
understanding the data doing a lot of
explorations with plot in essence takes
close to 70 to 80 percent of your time
in any data analysis task so if a
company maintains the data in a very
well-structured way this kind of heavy
wet time which we spend on data analysis
might be ready where the data cleaning
might be reduced otherwise you need to
like take this up for any new project
that you take take up which for which
the data is not available in a prior or
you don't have like any pipeline which
do this the screen you
- write it down of your own so very very
important if you don't do the cleaning
part and understanding the data well the
analysis or the models that you build
might end up giving you a very very bad
performance it's so very important as I
said 80% of the time people normally
spend on this task right and oftentimes
when you are analyzing things like the
example I told my sales are going down
what do I do it is not possible to come
out with such answers to complex
problems like this with just one
variable right so you might also want
sometimes to move beyond one variable
and talk about let's say how to do
multivariate or bivariate kind of
analysis so oftentimes this question
comes up where you like us to
distinguish between this univariate
bivariate and multivariate analysis and
the idea is very simple in any sort of
analysis it is not only one variable
which kind of decides so the end output
of your analysis but there are multiple
factors involved so when there are
multiple factors involved you might also
want to look at things like correlation
there are multiple variables you want to
see if there is any correlation between
these things sales are going down but
because of what is it because my sales
representatives are not going to the
market or is it my products are bad or
is there some other reasons so with all
the variables in one place you might
want to go and dig deeper to see if
there is any relationships coming in the
variables or not and when we
collectively get all these variables
together and do some sort of current
analysis around the problem you come out
with a really crisp answers to what you
are trying to analyze right and moving
on there is also times when people do
some sort of grouping right with the
data you do a sampling right you get a
data set in your system or in whichever
servers you are doing the analysis but
in that there might be locked many
number of times when even the randomised
sampling of getting the true
representative from the population like
that might not work well right so in
those cases you might want to do some
sort of systematic sampling or maybe a
cluster
based sampling as well there in you
might decide to say I want to analyze
the issue with only five regions in my
mind and with the five regions I am
going to form different clusters or in
the systematic sampling you might also
want to say that with the five regions
that I have got I might want to analyze
only for one product right which is not
doing that good in details so these kind
of sampling techniques or like the
cluster base one or the systematic
sampling techniques and there are
different names for this people might be
able to give a very good interpretation
of what really went wrong in whichever
sort of analysis you are doing so one
example is like sales going down but you
can adapt this to other analysis as well
but the idea is instead of doing a
randomized sampling by which we are not
very sure which kind of data is coming
in the data set which we want to use for
analysis but if you do it in a cluster
basis or cluster or a systematic
sampling you know exactly which clusters
or which sort of regions in in this
example you are like analyzing and in
your end of the analysis you'll be very
able to say this is not like a
randomized sample that I have taken but
from these five regions so there are
many different ways of doing clustering
cluster orders or sort of the systematic
sampling which kind of helps in this
particular final end results of your
analysis in the right perspectives right
instead of doing a randomized sampling
okay one more quite a useful sort of an
idea what kind of widely borrowed from
the field of linear algebra and this is
a bit related to what we earlier saw
between moving from one variable to
multiple variables right an eigenvalue
and eigenvectors kind of a concept
borrowed from linear algebra helps us to
bring in some in some way a linear
combination of different variables
together for instance in some complex
analysis it might happen so that given a
data set it might have many columns
right let's assume you have a data set
with 1 million rows and let's assume
10,000 columns so in these 10,000
columns are some C
there are complex problems like that but
in most often like most of the time not
all the 10,000 variables are useful
right the input variables so what we can
do is we might want to transform this
data set in a lower dimensional space by
which we mean this 10,000 columns can be
reduced to let's say only 100 columns
right so eigen value and eigen vectors
are these ideas which helps us in this
transformation and the idea is can this
100 variables be represented as some
sort of linear combinations of the
10,000 variables and if I'm able to do
that my dimensionality is reduced the
time I take to do the analysis is kind
of also reduced and the represent
ability which will come with only 100
variables will go up right so quite a
powerful idea agent values and the
eigenvectors and as I said the egan
vectors is kind of that linear
combinations of many kind of variables
there and this calculations around egon
vectors normally happens for a
correlation or covariance kind of a
matrix which as you know the measure
correlation is also about how to two
variables are related or how cool
strongly two variables are correlated
right so that's why we are also saying
of this eigen vectors can help us to
compress the sort of data that we have
right and that's because of one
eigenvector can be representing a 100
column 100 variables together right so
that's sort of how it works so quite a
powerful idea and commonly use methods
for reducing the dimensions of a large
data set like the PCA a principal
component analysis is actually based on
like in value and eigenvectors so if
somebody asks you about eigenvalue and
eigenvectors and interview also talk
about the pca principal component
analysis which is actually based on
these two concepts so that gives them a
good idea to the interview we let you
know about you can values and
eigenvectors and you are also able to
think of its application like in pca
right so we
talk about this false positive and the
false negative cases in our confusion
matrix example so this is exactly the
same you also talked about the type 1
and type 2 error okay so but let's now
also drill further and say examples or
kind of scenarios when the false
positives are important in scenarios
where the false negatives are sort of
important and the by the term importance
we mean are we like allowed to do this
mistake if you are building a machine
learning model are we even allowed to do
of mistake on either of the cases the
positive or the negatives so for
instance here if I take an example in a
medical domain where we have let's say a
process called chemotherapy which is
normally given to cancer patients which
is a radioactive kind of a therapy which
kills the cancerous cells right so it is
very focused sort of a therapy on the
cancerous cells so what will happen if
you like predict let's say you are
building a model for detecting cancers
right given a CT image and this model
would obviously not be hundred percent
correct all machine learning model has
its limitations
but you are here required to predict
this whether a patient has cancer or not
and based on that radiologist might
decide that whether the chemotherapy is
right for this patient or not but
imagine now if you have predicted
somebody to be positive for cancer but
the patient is actually not having the
cancer cells there right so in those
cases you might end up saying let's go
ahead with the chemotherapy but the
side-effects of chemotherapy are like
very very adverse right because you are
giving these therapies on the healthy
cells if the patient is not having the
cancers so in these cases sort of the
false positives becomes a bit more
important so it will be absolutely fine
if your model says the patient doesn't
have cancer if even if there is like
this slight possibility of cancer
present in the cells of the patients but
in that case you are not like exposing
the patient with a chemotherapy there
right which is like more harmful than
saying that the patient does not have
both
cancer right so in these cases the false
negative though the file false negative
itself is not so good in this case but
it leaves with this particular example
the false positive gets the importance
then the false negative but both are bad
as you know right from the confusion
matrix discussions so in simple terms it
is better to not expose the patient with
a false positive with a treatment like
this chemotherapy like treatment then it
is like much better than saying you
don't have cancers okay and a very
similar example in some other context
might also come up so if you would like
to think of some other examples in the
same context okay so that is the other
case now which is the false negative
right so we talked about the false
positives importance but there might be
also cases where false negative might
become a bit more important this and for
examples like this if you are let's say
building a model where you want to
convict a particular criminal basis all
the records and the arguments which has
happened in the code right and let's see
what would happen if you make a criminal
go free right because your model says
it's false negativity though the person
is actually a criminal but because your
model predicted based on all the
evidences you had the person is not a
criminal so you are letting a criminal
like walk free in society so that kind
of is more harmful than convicting that
person and maybe for a prolonged period
you might also want to get more
evidences and build a stronger case so
it is fine to keep a suspect in behind
the bars for a longer period then
letting the suspect go free there when
we like know that that it might be a
case of a criminal going free from the
judicial systems there so in these cases
the other case becomes more important so
keep in mind it is very easy to get
yourself confused between this false
negative and positive but if you keep an
example in mind always and like don't
give any room for confusion there so it
is a confusion matrix of the based on
which these two ideas comes in you will
be able at every time put these examples
in front and talk from that so if you
start to explain what false negative is
in terms of the formula you might get
confused but if you take an example and
then explain things are much more clear
for you as well as the purpose
hearing that in the interview right in
cases when both are important
typically whether the one which relates
to banking industry you are building a
model which will decide whether to give
a loan to a person or not basis many of
the input attributes there around which
you have like collected the application
from the customers but here you say if
the customer is really good and you are
missing the opportunity there of not
giving the loan versus the customer is
really bad in terms of its his or her
credit history and you are giving the
loan in both the cases in one case you
are losing the business in the other
cases you are taking a risk in which you
will lose your money right so in this
case and in this example both kind of
has an equal role so if you are positive
or false negatives are in either of this
is high you will end up losing some
chunk of your money there keep these
three examples in mind and every time
you get to like here false positive
false negatives things should not be
like confusing at all okay so now let's
also talk about building a machine
learning model so so far we have
discussed about what happens after
building the model right but let's step
one like get one step back and see how
do we normally build a machine learning
model and what kind of processes we not
very follow so when we are like building
a machine learning model we know that we
need to given a data set we need to
divide it into different buckets or
different parts so the commonly known
divisions that we like have is your
training data right then we also divide
the data into something called test data
and sometimes we will also divide the or
keep one portion of your large data set
which is called the validation data so
often times people confuse between the
test data and the validation data right
so what happens is in the training
process there are certain models wherein
while you are training you will use the
training data obviously but in the
process of training you can also involve
something like a validation step right
which will make sure that during the
process so there is one part dedicatedly
given for the validation of the model
and when the model is done you might see
that the final model is
well trained on the data at the same
time validated but when the model is
completely done then only you get into a
process that we call testing right so
you can imagine like this you have a
data of thousand records you keep some
700 records for training hundred record
for validation and the remaining 200
records for testing so there are three
splits right if I would like to explain
this with an example this is how it will
happen so there is a process called
k-fold cross-validation in which K can
be any number like between 5 and and
mostly there are like standards saying 5
fold cross validation or 10-fold
cross-validation an idea is when you are
building your model you will work with a
training set right and a validation set
in which a small portion of the data you
keep for validation of the rest you use
for training so what you see here test
set can be like replaced with a
validation set right and you can see
that this is a rolling sort of subset
and you keep changing it in each fold so
you go for the first fold of the
iteration you keep one validation set
and the rest of it is the training set
and the next fold you move this window
to another subset and the rest is
training data and so on and when this
model is done using this k-fold
cross-validation approach in the end you
will get a model which you can then use
on the testing data to see if the
accuracies are good or not so this kind
of brings in lot of performance
improvements people have also found the
validation set to be an really good way
of tuning the parameters in many machine
learning models as you might know there
are something called parameters
typically in the neural network models
and these parameters need to be tuned as
the model is a kind of proceeding so we
cannot use the testing data set for
tuning this parameter so validation set
kind of comes very handy in those cases
all right so we just talked about the
cross-validation so when you keep moving
this validation set in each of the fold
first fold second fold third fold and
the validation set is keep changing you
kind of do this process of
cross-validation right and the idea
behind doing cross-validation is
is to see how well is your final model
generalizes to the data that you have so
independent of which data you use for
training your model should generalize
because oftentimes it happens when you
train a machine learning model it works
very good in the training part but when
it comes to the testing the model does
very bad the same problem with you
overfitting and the under fitting cases
so in this approach of cross-validation
you have made sure that your trained
model has trained on various subsets of
the data right and in the process we
also have this small validation set so
that every stage of the cross-validation
process you are able to use a different
subset so that means you have trained
your model very well so irrespective of
which data you use your model is going
to do well in the testing cases so
that's how cross-validation brings in
the capabilities okay so with the two
pillars like the statistical analysis
and the basic data analysis I hope you
are getting some sense of how people ask
a particular question coming from either
the model building perspective or from
normal data analysis perspective right
so we're going to know now go a bit more
deeper into questions which might relate
directly to machine learning so these
are all the most questions that you have
seen so far are either saying how do we
do analysis after the modulus build or
how do we normally perform simple data
analysis like a be testing frameworks
and so on but what if you are asked
something very particularly some of
machine learning domain type so the next
set of questions will cover those part
like people might also start with the
basics like what do you mean by machine
learning so the idea of learning must be
very clear you are given a set of data
points particular to a given domain
right and you would like to build a
learning algorithm which will take the
historical data and predict something
for the future
so as we talked about many examples
finding whether a convict is actually a
convict or not like predicting is given
the evidences if person is a convict
there or not or predicting whether we
should be giving a loan to a customer or
not right predicting the onset of cancer
in a given patient
by using the queuing patience historical
records and so on and now this
algorithms are now even becoming more
complex like it is starting to work on
speech data face data which are mostly
used for biometric authentication
systems right so many use cases coming
up from various industries right so in
machine learning the most commonly used
two types of learning disapper wise and
the unsupervised learning there are like
two other types as well the
semi-supervised and the reinforcement
learning and the idea is around if you
are given a set of input attributes do
you have a label which can help to learn
the input attributes around any data
points that you have or you don't have
right so if you have then the approach
could be a supervised learning approach
versus if you don't have then the
approaches more an unsupervised so some
examples of the algorithms are like
support vector machine regression a
based decision trees all of these are
the supervised learning algorithms so in
a very simple terms if I say you are
given an input attributes for
identifying let's assume a sort of given
an image of different fruits so based on
the characteristics of the fruit can you
like identify whether it is apples
bananas or the oranges so if I am given
that label with me the model will run
keeping in mind that given these
characteristics this is an apple this is
an orange and this is a banana but in
the other case if you go for a
clustering approach where such a label
of saying that it is apple banana orange
is not available then we might just
simply segregate the data points with
the input features maybe with color with
texture or with the shape like with that
we can maybe say that particular fruit
with this shape is like you know into a
bucket which we can like call banana or
another kind of a spherical shape it
might be apple or an orange so depending
on the presence of a label we can say
either to use supervised or an
unsupervised learning and both of these
approaches are quite common and
sometimes there are certain algorithms
which can have the both ways it can also
learn and supervised manner and the
supervised manner so depending on how
you model the problem the fundamental
difference comes from the fact on
whether we have the label or not okay so
when we talk about the supervised
learning algorithms other name for Super
was kind of one of the types in the
supervised learning algorithms is
classification right and the
classification is around given a set of
input attributes
and the label is like categories right
for instance if it is fruit bananas
Apple and oranges like if it is a
customer whom we want to predict into
either kind of a defaulter type or a
good customer so the classes are to like
one which is saying the customer is
going to be a defaulter the other says
the customer is going to be a good
customer same is true for when you want
to build an classification algorithm for
detecting cancer whether the patient has
a cancer or not has a cancer right or if
you want to detect let's say a malicious
content or a malicious file which might
be a virus frozen or warm or something
else right so in that case the classes
are now many so more than one class can
also be there but the fundamental idea
is we are following a supervised
learning algorithm but the type of
problem we are solving is a
classification problem so instead of
saying classification algorithm we can
also say it's a classification problem
using supervised learning algorithm so
these are the various types of
classification algorithms like being a
linear regression decision tree then you
have the support vector machines and so
on right so now let's talk about one of
the types of classification algorithm
called the logistic regression right so
very commonly used algorithms and banks
or companies as big as like American
Express so sort of have leveraged these
logistic regression algorithms to quite
a extent and they have like built a
really really robust implementations of
these algorithms particularly in banking
sector cases like predicting whether a
customer is going to be a defaulter or
not given if I issue the customer a
credit card
or give a loan these kind of decisions
can very robustly be taken from a
logistic regression algorithm and these
algorithms are best suited for two class
problems or a binary problem right where
you have either Y yes or no quite a
common technique as I mentioned and in
all the possible cases wherever you have
these per binary classes of problems you
might use a logistic regression a
political leader winning a collection or
not somebody getting a success in an
examination or not and as I mentioned
whether to give a loan to a customer
basis whether he or she oak is going
somebody fault or not and many of these
like binary types keep in mind lodges
regression works best for classification
problems with to class so one of another
widely used algorithms like the
recommender systems and like I think
this particular algorithm doesn't need
introduction so this is that common
nowadays take an example in Amazon you
have a product which you are browsing
the products which comes in bottom of
the widget which says you may also like
or customers who brought this also
brought this right so these sort of
recommendations is actually from a
recommender system which is running in
the back end if you like take YouTube
example if you watch one video the next
videos like starts to come one after the
other
that again is a recommender system
working in behind the scene or if you
take Netflix if you watch a particular
movie it starts to adapt right to a
movie which you might like Netflix also
uses recommender systems the
applications are not coming more and
more as sophisticated systems are
building in right facebook uses it for
recommending friends right you have a
set of friends and based on your data
coming from the contact mailing list
Facebook starts to curate friend
suggestions then all of these are
algorithms which might benefit the
business in some way or the other for
Amazon it is if you give a
recommendation below a page people might
buy more than one product in a
transaction for Facebook they will grow
their network of people right their
connection between the users are going
to go stronger and hence obviously the
kind of adds that Koufos Facebook wants
to sell will also starts to grow right
and more their users more the
connections more is these sort of
interactions you know about the
connections as well as the behaviors
that people show in a social network so
the fundamental idea behind all these
recommender systems is to get a
meaningful comparison between two users
or between two items right for Amazon
between any two products what's the
similarity if the similarity is really
high recommend that product to the for a
given product and in
or if you find that two users are very
similar in let's say Facebook you might
want to show to each others that you
have another friend whom you might want
to connect right so there are like many
such use cases which comes out the
moment you get into the deeper
understanding of recommender system but
the fundamental idea is how do I compare
two items the items might be product
people movies and so on and how do I
compare two users in simple terms so
this is what a recommendation system
works on so there are quite famous
examples like the collaborative
filtering approaches user base
collaborative filtering algorithms or
the item based collaborative filtering
algorithms both of the algorithms are
quite commonly used in recommender
systems and nowadays people have also
moved on to Latin factor based models
like the SVD single value decomposition
and many others so we talked about
classification problem and then we said
logistic regression is a binary class
problem right so you might also be asked
something around linear regressions so
what if I don't want to have a class of
a particular user or a patient or
something like that right but instead if
I would ask you can you give me a crisp
value instead of a class it's when I'm
like classifying a given file into
good/bad in which bad can be virus forms
cousins these are the classes but if I
don't want class but rather for example
if I want to know the exact value of a
house in a particular locality in my
city how do I calculate that right so
linear regression models in machine
learning is one such technique which can
regress over a given input data which
might include the properties of the
houses like number of bedrooms the area
in square feet and so on and finally
predict a value a crisp value which will
be exactly in terms of let's say dollar
or in any other currency the value of
the price and the idea is once again
that you have a training data with you
which has labels so from the past data I
know that given these attributes of the
house what should be the idle price of a
house I'll use that as my training data
and then build my model for future so
now with any such similar pattern in any
data which is going to be coming in
future for maybe our new property which
is built in
XYZ location I can use the model and
predict exactly because these features
are somewhere similar in that locality
the prices might be in a particular
range so model like linear regression
we'll learn those patterns in the given
input attributes and try to predict the
price of the house right and the idea is
if I have a set of data points I want to
build a very generic model like drawing
this line which is as close as to all
the points right so you can like drum
infinitely many number of points if you
are given a set of data points like this
in a two dimensional space but the one
which is the serratus or the closest to
the points is the best line so the red
line which you see here is like far away
from all these points right so this is
not a best line but if I see this blue
line which is very close to all the data
points is like one of the best lines
which I can get from this points right
so the idea here is to fit a line
passing through as closely as possible
to all the data points that I have and
minimize these so called the error which
is nothing but the sum of all the
distances right so there are simple
linear regression ideas and the
fundamental idea that we are following
here is your variables are having a
linear relationship which means with the
increase of one variable the other also
increases but if you have a pattern
which is not so linear in shape like if
you are like not able to draw a generic
line but the representation or the sort
of points are aligned in a way that you
can only create a module which is
polynomial so in that case linear
regression will not be so useful because
the relationship is not linear anymore
right so in that case you might want to
go for some other regression approach
maybe like a polynomial regression which
has a nonlinear relationship between the
independent attributes so quite a common
approach and it has like a really large
chunk of its explanation coming from the
statistical ideas statistical ideas like
hypothesis testing p-values confidence
intervals and so on so if somebody is
asking you a nonlinear regression it
would be to start with saying how you
are build a linear regression model
right and then you might give some
examples of it so at best if you are not
comfortable with these ideas like
p-value or hypothesis testings you might
to refresh that before you let go for
any interview because of linear
regression comes up these concepts needs
to be a bit more explained type so when
I talked about the recommendation
algorithm I mentioned something about
collaborative filters right the user
base collaborative filtering or the item
based collaborative filtering so these
are the two commonly used sort of
recommendation algorithms right normally
referred to as item based so IB CF and
the UBC F as idea as I mentioned is to
compare to users or to compare to movies
or let's say items in particular so the
item can be anything a product a movie
or a person and the sort of weight base
the model is given lot of users and
their particular let's say in this
example their rating to a movie we now
need to find out can we recommend for
some users based on the behaviors of
other users or their ratings to the
movie some suitable movies or not so for
example here the for Carol Carol has not
seen the movie 21 right so that is the
question mark there so can I predict
this value if it comes to be let's sale
like close to two I won't recommend the
movie but if it comes to let's say
somewhere three four or five yep the
model says yes recommend this movie to
Carol okay so one more fundamental
problems on when you build the models so
as I mentioned in the earlier discussion
that when we are doing some analysis
with the data after you collect the data
there requires a lot of cleaning right
and the exploration as well so in that
process of cleaning and exploring the
data sets you might often find there are
some extreme points so for instance if I
am building a regression model for
predicting the house prices and there is
this one house which somebody has sort
of able to sell for a very high price by
means of maybe some auction or some
other sort of marketing gimmick there
the point might mislead the model to
predict or get itself towards that
outlier point right so we don't want to
like move towards an outlier point but
we need to deal it separately so if you
don't have a better explanation for why
that outlier is in terms of an input
attribute better is to remove it
so for instance if I am like analyzing
any data in an e-commerce world when I
am going through all the products and
the sales that the product has seen in
last one week and on the last one week
there was a particular day when a kind
of for sales day was there like nowadays
e-commerce companies do this a lot but
in those sales days you would have
obviously expect that the products
purchase is going to go very high right
but does that mean that it's an outlier
to me not because if I am able to
explain an outlier by saying that this
was a discount day I might be able to
handle it separately either I can like
simply take all those points which are
for the discounts day or sales day and
keep it separately or if I would like to
have the variable like saying whether
the given days are sales then keep the
outlier as well then the analysis goes
in a different direction so it's
important you handle the outlier before
you start to build your models or do any
analysis otherwise your insights or your
models output might totally give you a
different direction and there are very
different ways to handle the outliers so
some people use approaches like removing
any data points which is like outside of
the range of mean plus three standard
deviation right or sometimes people also
use the percentile way of doing it any
point which is greater than the 99th
percentile can be like removed so these
are like you are removing the stoppers
from the data points of yours of let's
say a Sat examination or a cat
examination so the outliers are
sometimes can cause certain issues in
explaining the model so you can
obviously imagine this in very intuitive
terms also if you have a set of scores
for candidates who appeared for an
examination and there was one outlier
candidate who scored really really high
so do you have a way of explaining that
outlier you might be simply calling that
person a talented person but does that
like explain the models may be difficult
right so better is to keep those
exceptional cases separate and do the
analysis with the rest of it so which
gives you the good pattern or good
insight from a data so that's how you
like normally handle an outlier right
and this quite often is an important
question to answer that if you are given
an analytics project with lot of data
normally approach it right in typical
cases the first step is to really go
deep dive into the problem in hand and
the problem needs to be defined very
crisply so know never define a problem
which is broad in sense so for example
if you are building a model for customer
segmentation using a clustering approach
so instead of saying build our customer
segmentation model for all the
categories of products that can be like
a broad problem but if I say build a
customer segmentation model only for
fashion category of products right then
the problem becomes crisp so defining a
problem statement and it's understanding
is before most tasks and then comes the
the kind of exercise of exploring the
data in which you will identify outliers
missing values and if you need any
transformations like converting from a
log format to a wide format or the vice
versa
you do all that steps in the second and
the third step and once we have found
out that the data is very good now after
we have removed all the outliers and the
missing values and so on you then start
to understand the certain relationship
like given input attributes relate in
some way or the other right so this is a
stage where you start to prepare for any
further insight building exercise or
like model building exercise and let's
say if you build the model in this step
the immediate step is to validate it
right whether the model is really good
or not by using the testing data right
and once let's say all of these is done
and you are either coming out with a
insight or a model you would like to see
in long term how this model is going to
perform so it might happen so because
every model is not static your data is
growing on daily basis so if you want to
build a really robust model on a growing
times it should upend the model should
update based on the new data which is
available right so we're a period of
time you should track and analyze how
good the model is performing on a real
world data and if the performance is
going down then it's time to retrain the
model and maybe come out with an updated
model on the data that you are already
given right okay so one more task as I
was explaining in the cleaning process
how do we treat the missing values so
there are quite
a number of techniques to do that so for
instance if you have an attribute let's
say H right and you are analyzing this
age in various segments of people people
who are teenagers people who are
professionals people who are still in
their college and so on right and there
is one value missing in one of these
categories of people let's say teenagers
so if the age is missing because I know
I am analyzing a group of people who are
teenagers by looking at the average age
in the teen age group right
I can maybe impute a value so instead of
discarding the entire row because I
don't have the value of eight I might be
able to impute it by some simple
measures like this calculating the
average in that group and putting the
value there which will not be completely
wrong because I know there is a very
strong evidence teenagers would be more
or less in the range of let's say
sixteen to twenty right even if I am
wrong in my average calculation it might
not be so high it might be like just
plus or minus one or two years
so which I am like fine with if I want
to keep my data retain and you might see
that in your many applications sometimes
having a kind of discarding a particular
row because of missing values might be
very costly because the data is limited
in number so that's why people normally
do this or to sort of mean minimum or
maximum kind of a value or they also
calculate the average and impute the
value there so there can be some other
pattern based imputation also possible
but I'll just give you an example and in
other cases if nothing is possible by
putting a value if everything is going
to be misleading then better better is
to like remove that but that can only be
done if you have a surplus of data with
you if not then be cautious of removing
any values particularly the missing ones
okay so this question particularly
pertains to a machine learning algorithm
called k-means right every time you run
the algorithm you have to define what
should be the value of K right so there
are approaches like elbow curve which
plots the kind of a plot scatter plot
between the x-axis which is the number
of clusters versus y-axis which is the
WSS or within sum of square which is
also known by the
name distortion the idea is when we are
building the k-means clustering
algorithm if we find at one point if you
like keep increasing the value of K than
one point we find that the distortion is
low which means the distance between the
data points within a cluster is as close
as possible and the distances between
two cluster points like a point in
cluster 1 and a point in cluster two is
as far as is possible so if that's the
case then the distortion will be like
very low but if your points inside a
cluster itself is very spread out and
your clusters are very close to each
other then the distortion is going to be
very high so in those cases the value of
K needs to be may be further increased
and at one point using an idea called an
elbow curve which will show you a small
kink from where there is a sharp dip in
the distortion values and that is an
appropriate value for K while we are
building a k-means algorithm so this is
how it looks like so you can see here
the x axis is the number of clusters and
your y axis is your within group of sums
of square and at one point you can see
there is a sharp dip here here and after
this point which is like circled in red
the values sort of almost saturates so
this maybe I can use as the appropriate
value of K so value of K can be like 6
in this case so I hope you are now
getting comfortable with questions
around data analysis statistics and even
the machine learning part so the next
pillar which might very closely be
associated with statistics as well is
around probability right so it is like
sin sometimes in most of the standard
literature probability and statistics
comes together this is not inseparable
anytime so in the name Bayes algorithm
like this is one of the machine learning
algorithm which is based on the Bayes
theorem a probability ideas are quite a
lot used and there are some really neat
probability concepts like the
probability graph models which is
actually based on the basics coming from
Bayes theorem and the fundamental
properties around probability I will not
go to that detail of probability and
interview questions if you have an
expertise around probability and
probability graph models Ornette based
kind of algorithms
feel like confident to speak about it
but most of the time the probability
questions are quite basics and
fundamentals like one the one with is
which is asked in interviews depending
on obviously in which place you are
giving the interview it lets also like
see some sort of approach in attacking
of probability problems okay so I like
read it out for you what the problem is
here so it says in any 15 min minute
interval there is a 20% probability that
you will see a shooting star like a good
example rather so what is the
probability that you you see at least
one shooting star in a period of NR so
there is unknown information given to us
that every 15 minute interval there is
this 20% probability you will see at
least one star so we now want to
calculate the this probability in a
period of one knot right so what is the
sort of approach we would take here so
let business systematically so we know
one set that probability of knots is our
scene the shooting star in every 15
minute interval is 20 percent which like
comes down to 0.2 in probability terms
20 percent chances or 0.2 probability
and as you know probability is always
between 0 and 1 so if I would like to
calculate what is the probability of not
seeing the shooting star in 15 minutes
which is like the opposite of this right
if I take from so these are like
independent events right so if I take 1
minus of the probability of seeing one
shooting star 0.8 will then become the
probability of not seeing the shooting
star in 15 minutes interval okay what I
need to do is then probability of not
seeing this shooting star in another one
are because these are now independent
events like seeing the shooting star in
first 15 minute interval the second and
third and the fourth in an arse period
is independent of each other
we can multiply this probability like
this so 0.8 into 0.8 into 0.8 into 0.8
which comes down to a value of 0.4 so
the probability of not seeing the
shooting star in an period of 1 r is 0.4
right and if we know the value of this
probability here
will I have to do to get the value of eh
we are interested in is which is the
probability of seeing the shooting star
is 1 minus the probability of not seeing
it right so the probability here comes
down to 0.5 quite a simple approach in
terms of how it works right
so there might be like similar tricky
ways of putting the same questions but
if you know one approach and having the
idea of independent event is right you
may be able to get the understanding or
clique formulation in this way and also
keep in mind given sort of a sample
space and many events from the sample
space the sum of the probabilities
cannot exceed 1 right so that's why we
are able to do this if I am saying
tossing a coin the probability of tail
is going to be 0.5 and the probability
of head is going to be the other event
which is going to be 1 minus the
probability of tail right so similar is
the case with here as well we are like
dating this as a binary problem so
depending on which problem you are going
to a tackle the approaches are going to
be the same but the language and sort of
the trickiness and the question might
increase right so this is how you can
attack that ok so there is this one more
question which is around generating a
random number between 1 to 7 with only a
die ok so let's also think about this or
we can like approach this problem so we
have like seen random how we generate
random random numbers from a given set
of points but this kind of says that we
want to do this with a die right by
rolling a die we want to find out a
random number between 1 to 7 so
obviously we know that the die has
numbers between 1 to 6 so the
probability of getting any one digit in
the die is 1 by 6 like if I do 1 2 3 1
divided by the total number of
possibilities that is 1 by 6 so that
will be my probability of choosing any
one of the digits there but it is saying
it is we want to generate this random
number between 1 to 7 so where do we get
the seven so obviously the requirement
is that we can like roll the dice twice
right then the number of possibilities
increases so as I said there is no way
we can get the seventh one right because
we have
six digits but if you roll the die twice
the number of possibilities of
generating this seven random numbers
increases so in this case we have now 36
different outcomes possible you can like
do the match there which is nothing but
six cross 6 1 2 1 3 1 4 1 5 1 6 and for
the rest of the digits the same way
right 6 cross 6 so that's the different
number of outcomes we can get so now for
us to get this 7 sort of random numbers
and imagine that this idea actually
relates to a sampling technique as well
when we say we want to get the numbers
between 1 to 7 all of those numbers
should be equally likely right
otherwise it will happen that I might
get one and maybe quite a number of
times versus one particular digit like 2
only once or so on but if I want to do a
very good randomized selection I need to
be making sure the probability of
picking any one digit between 1 to 7 is
equally likely if I am NOT doing that I
might have a bias right so we have this
36 different outcomes so how do we now
make sure that this is like kind of
equally likely so first we have to like
find out a number which is divisible by
7 which is close to 36 so maybe we can
exclude one of the possibilities from
the 36 combinations we have and keep
only these 35 possible outcomes so like
excluding maybe 6 comma 6
this possibility if we remove we are
left with the remaining 35 possible
outcomes so with this one reduction in
the possible outcome all we are left out
with is all the possible computations so
starting from 1 1 till 6 5 right now if
you divide this into 7 parts where each
like contains 5 possible outcomes so
what will happen is with the 7 parts
each having 5 outcomes then ever any one
outcome comes in these 7 parts I am
going to assign a value to it so for
instance if I assign the first 5 because
we are going to divide it into 7 parts
where each part will contain 5 possible
outcomes so maybe the first 5 outcomes I
will assign to the part 1 so whenever
that 5
five outcomes comes up I will say the
random number which I am generated is
one in the second part I will keep the
next five outcomes and whenever that
five outcomes comes in rolling these two
dice I will assign that a value 2 and so
on so this way what we have done is we
have made sure that the random number
that we are picking between 1 &amp;amp; 7 are
equally likely if you don't do that then
there will be a bias so you see here how
sort of statistics is like merged with
the probability that this is how the two
ideas and subjects travel together okay
so one more question on the same line so
for instance here if you have a couple
and let's say they have two childrens so
at least one of which is a girl so this
is our given scenario so the question
asks you what is the probability that
they have two girls right it becomes a
bit more like tossing two coins problem
right where there are different
combinations so what you can see here is
a couple has two children so at least
one of which is a girl so we have a
known fact with us so now all we need to
do to kind of calculate this probability
is to consider it all the possible
combinations the first child being boys
the second child being girl the first
being girl the second being boy and so
on right if you write that down this is
what the possible combinations are and
both all these four are equally likely
as you could see so with this case we
have like in the equation at least one
of the children is a girl so that means
only three combinations are left out
with us the combination where both the
children were boys is excluded so in a
very simple manner you can like skill
just go ahead and calculate the
probability of having two girl child
will be 1 by 3 so there are total three
possibilities which comes in the
denominator and you have like the
probability to choose so there is only
one event which favors this probability
like saying two girls out of the three
that we have so one by three so let's
this question here which says the jar
has a thousand coin of which there are
like 999 coins which are fair right
which means the head end there is one
head and one tail and there is one coin
which is like tampered with and both the
sides of that coin is head we need to
now pick a a coin at random from this
jar and toss it 10 times right so given
that we have seen then 10 heads already
off in that 10 times of tossing the coin
what is the probability that the next
toss of the coin is also going to be
head and we are given our statement here
that 10 heads are already seen what is
the probability that the 11th toss would
be your head okay so let's see how we
approach this so let's first put these
two possibilities in place there is this
999 fair coins and one double headed
coin right double-headed coin so we need
to see the probability of how many times
we can get that double-headed coin so if
I look at from the jars perspective
choosing a fair coin from this charge
there is 999 times out of all the
thousand possible coins so which comes
to be 0.9 so high probability but for
the unfair one there though there is a
small probability but it might also
appear some point right so the
probability of selecting ten heads in a
row would then be either you would get a
fair coin and tossed and head
continuously so this is one possibility
the second possibility is to select an
unfair coin where the possibility of
head is like hundred percent because
there are both the sides of the coin in
his head so the probability of selecting
ten heads in a row would be the
selecting fair coin multiplied by
probability of selecting ten heads which
is nothing but each coin which you are
picking up is independent of each other
getting a head there is also independent
there so the probability is 0.5 that's
why we are able to multiply it so you
have here the probability of picking a
fair coin to be 0.9 and multiply that
with 0.5 ten times and that's your like
probability which you get after that so
this probability of a is selecting a
fair coin and getting ten heads and the
probability of B is selecting an unfair
coin and we know whenever we select that
the surety
yes we always get ahead so then that
probability of B becomes 0.001 so now
what we need to find out is this
probability where we can sort of say
given we have these two cases what is
the probability that we are able to come
out with the levant head right so how
are we going to do that is by selecting
this kind of combining this
probabilities into a sort of form like
this where we are calculating given
these two possible cases what is the
sort of likelihood of with a fair coin
getting ten heads and with an unfair
coin getting that one head right so
those two are the probabilities written
here 0.49 and 0.5 0 so finally these two
probability is known to us the
probability of selecting one more head
would mean that in the fair coin the
probability is 0.5 right so which you
already know and in the unfair coin the
probability is always going to be 1
because we have both the sides as head
and both of these probabilities we are
just going to multiply with the
probability of the fact that already ten
coins have been seen in its head so
that's what we are like multiplying in
front of it so if you put this into one
formulation you will finally get the
probability of selecting another head is
0.75 so which is like slightly more if
you didn't had this sort of one
double-headed coin if that coin would
not have been present the probability
would be a bit more or less here because
all the coins are then fair ok so thanks
a lot for listening to this session hope
this helps for you to prepare for your
interviews so the reiterating the fact
give a very good emphasis on basic ideas
some stats and probability also spend
some amount of time understanding
machine learning models and basic ways
of doing data analysis and things like a
B framework testing and so on so these
are some broad concepts you need to know
also spend some time on linear algebra
kind of ideas like the weekend values
and eigenvectors it might also be
helpful on certain questions so all the
very best for your interview and hope
you all get a really really successful
carrier India
thank you I hope you enjoyed listening
to this video please be kind enough to
like it and you can comment any of your
doubts and queries and we will reply to
them at the earliest to look out for
more videos in our playlist and
subscribe to our retro Rica channel to
learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>