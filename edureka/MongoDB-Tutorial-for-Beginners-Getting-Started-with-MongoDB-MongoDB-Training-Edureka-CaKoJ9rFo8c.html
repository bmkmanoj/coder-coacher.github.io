<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MongoDB Tutorial for Beginners | Getting Started with MongoDB | MongoDB Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="MongoDB Tutorial for Beginners | Getting Started with MongoDB | MongoDB Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MongoDB Tutorial for Beginners | Getting Started with MongoDB | MongoDB Training | Edureka</b></h2><h5 class="post__date">2018-04-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CaKoJ9rFo8c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is our first session
for with the Mongo certification course
in first module that means today's
session so we'll be exploring the
introduction to MongoDB and orders the
basic needs for exploring any of the new
SQL stores what other various types of
our DBMS and Wes Kuehl stores that we
have and where exactly does one go fit
in and what are the various
classifications of the data stores and
types of data that we can preserve and
what is the structure that Mongo
actually comprises of and a brief
overview of the various kinds of
activities or our various kind of sores
that we can play around with starting
with the first session it's the
introduction to Mongo store and the
objects use that we have from this
module that means for this session is we
should be able to understand what are
the various differences that are there
we're considering the various kinds of
data stores that we have in place and
how exactly they differ from each other
and what makes sense in which scenario
and what kind of store we actually
preserve in there and then understand
the basic need for Mongo and the design
of Mongo in itself lists the various set
of tools that Mongo has how to do the
installation of Mongo on various
environments setting up MongoDB
environment in on a specific OS what is
the possible difference between a JSON
and a beast on starting with the
fundamentals database primarily I mean
what exactly why exactly do we need it
at a base and how does the database to
look like and what kind of things what
kind of advantages that a database cue
us that's always a very important and
challenging question that we should ask
ourselves before actually choosing a
data source or deciding that there's a
need for data source C the first thing
is when we're actually talking about
large amount of data that when we
actually have high volume of data or
large amount of data and there are
possible connections in between the data
that we have we need to organize that we
need to manage that and we need to
ensure that there's a very high volume
of rates and right that's where
did a source or database actually comes
in picture that's something it abates
actually helps us to solve that kind of
problems or smooth enough for our steps
in there and then giving us a better
well-defined platform variant we can
perform the data analysis with more
convenience and ease or if we talk about
giving you a disciplined approach
towards managing the data I'm keeping a
more organized forms of data that we
have in place so I mean and then the
biggest the most important is keeping
fault tolerance in mind and we're
ensuring that we're able to handle
security will able to restrict users
from accessing various parts or radius
sections of data based on their rules
and rights were able to devise various
plans on the restrictions or the various
sections of a specific data source and
then recovering the data in case of
failures or backup as well so now we
talked about the introduction to
database there are various category is
that a data source can actually be
expected in weather we talked about the
oil TP or well ap or being no SQL you
know SQL is primarily where an Mongo
actually be looking for so then we can
have probably there is Cassandra there's
couched its radius there is react and
elasticsearch its so on and so ago then
in OLTP we have it's primarily the RTV
gamers and more of real-time stores so
we have article it's probably the most
popular racket source we have post gram
s SQL MySQL and so on and so and the oli
P it's primarily it's very specific it's
very peculiar with respect to the kind
of data that we can put where in the
world because it's primarily a DSS
stores so you will not find many popular
implementations and all ap because all
TP is more popular and that and most of
the times you will see there are a lot
of implementations of wall TP as
compared to all ap for a while 18 and
that is Antonovich quite popular so you
can actually expect that you were
actually storing data in for in the
specific possibility on a specific
industry
so I'm sorry a novelty and if we talk
about what exactly in all TP is and what
kind of information what specifications
does this word bring on board
that means I'll choose and all TP or and
no SQL or aap see the quality ap is
primarily for during the real-time data
so I mean the data that actually is just
coming in as soon as the data is
actually being pushed the data is
actually being sent in from the user
probably from somewhere from any of the
possible locations which is cool and we
push it there too that's a primary
reason we're an all TP will actually be
well it's expected of more relevance and
since it's more of a transactional
stores its so primarily when we talk
about working in the banking and
financial domion whether we talk about
the accounts the managing balances
transactions handling the transaction
history ensuring there's a commitment
while we're transaction or in case of
possible failures of one of the possible
steps while actually executing a
transaction so you can make a decision
while in there and then since it mostly
works on the real-time stores so you can
always expect the data is always pushed
for a short-term storage and then
probably it's actually being moved on
even though I mean it depends it's not
necessary that you will always have a
short-term storage there are still a
story and we keep on pushing the data
and we preserve it there for a very long
time hours it depends it's conditional
but yes the both of the possibilities
are quite observed in an old TP store
how are in case of an Ola P it's
primarily based on the processed data
that we have it's homogeneous in nature
so I mean it's abhorrent so it's
probably unique so the type of the data
that actually shorts is very specific
then the access to we can always
leverage well they people are actually
interacting the various DBMS is
considering that we don't have extra we
do we're not working with the transaxles
a data source we are not working with
relations that's where an oil TP will
works certainly since there is no
concept relationship so the possibility
of preserving a data is primarily
long-term
you will always have high volumes of
data and at the duration of the present
and the lifetime of the data is
prominent primarily very long in case of
the no SQL no SQL is primarily again
it's agnostic in nature so because since
we its schema less so it doesn't have
any forms of structure it's you can
always store any form of unstructured
source its regardless of what is the
volume and what is the velocity of the
data that you're actually pushing in
here however in case of the
transactional store that's something we
cannot primarily achieve in the new way
in the no excuse except for the graph
data sources that actually help us if it
the SM reflexes or is it the acid
principle hours other than that - that
we talked about
mangu on its whole or any other high
school shorts that's column based or
document base those stores they don't
primarily looking for such specific
source however on the other hands we
talk about the volume of data can always
be very high so you can always expect
that there will be the amount of fields
actually being preserved in the know SQL
stores can always be very long term and
the expectations are clear the source
can be you ready I'm expected because
it's something that might be exists at a
very later point of time primarily if we
talking board from an odbms
or NT VM a store the SQL is the more
conventional standard of actually
specifying or executing the set of
commands to process the data or to
actually retrieve the data or to perform
any specific set of operations on top of
the data that we actually have in place
in a DBMS or in a data source
color that's all TP or well ap so that's
what the structured query language or
SQL will as we call it it's being
specified so analyzing the data we can
have the analytic queries we can define
these structures of the data the
structure of a table structure of the
database the structure of the columns
how we're actually retrieving the data
how we actually specify the
relationships how we actually want to
retrieve specific chunks of data how we
want to ensure update or modify specific
sections
tables that's warp run that's where SQL
will actually help us out or than we if
we talk about the integration scripts or
if we talk about the administration are
modernizing or monitoring tables ours D
data tables or the complete database on
a tour in the SQL rule can can come into
picture or if we talk about performing
various kinds of sanitize operations
where and we're actually looking in for
processing any levels of database
operations whether we talk about the
data definition operations or the DML
see the data manipulation operations
changing the structure of the stores
that's a database or a table or actually
modifying the data inside the tables so
in there something where the SQL will
actually come in picture however in case
of a little more advanced flavors less
like a no SQL that's something when we
actually really don't require SQL so the
major reason in there is that we
actually don't have a conventional data
source in place so we don't have the
relationships defined in there and then
we have a distributed architecture in
place because the data is actually
primarily distributed across multiple
sections or across the globe spread over
different levels of Nords that's
something that's more common from NoSQL
store as compared to an SQL store and
there's always a there's actually be
preserved and then primarily most often
aware schools are actually completely
free of course and it's very simple to
access the API is there is no concepts
it's unstructured so it completely
agnostic asked the schemas in place then
the scalability is finally the
horizontal scaling is out to be very
easy and simple that's worth primarily
you know SQL is now if we actually have
a comparison of sq databases were still
schools the SEO has been evolved it's
very stable and it has been there for a
very long time because it actually used
to suffice majority of our expectations
were suffice within the SQL environment
however considering the high volume of
data in considering the stability of the
systems that we have in the industry
right now so the problem is that even
the intensity of the data
the data interactions and the data
operations that are actually happening
around in the industry right now so the
volume of data that actually gets
processed eventually is actually very
very high and three events that's where
that's why big data comes in picture now
in the SQL it's primarily we just have
only SQL it's a standard standard query
language that you can use that you can
use to perform operations on top of
various for narvi be races even though
there might be specific differences
slight deviations with respect to the to
the specific database server or specific
implementation however in case of no SQL
there is no specific type there are
multiple types we have the column base
we have the key value we have graph we
have document so it depends on what kind
of a know SQL store is that we are
actually working with so based on that
we can actually decide how exactly it
looks like then we if we talk about the
SQL was developing SQL has been evolving
almost three to four our tech it's now
it's almost four decades now more than
four decades now so those are around
1970s when SQL actually came into
existence however no SQL is relatively
new it's almost less than a couple of
decades a decade or so it's almost
around the 2000s the y2k boom so and
that's where no SQL is actually started
to come into limelight even though
actually honestly no SQL is actually
started getting used or more realistic
and possibilities probably around 2003
or 2004 and that's our way around where
no SQL actually started making more
sense to people who are actually working
around with it obviously for sql's we
have a lot of implementation a lot of
and most of them are very popular
they've been industry for a very long
time talk about oracle microsoft SQL
server mysql both scream and so on less
schools or not in case of and most of
them with SQL the challenges majority of
the SQL czar primarily paid for their
license now in SQL we always have a
well-defined schema in place that's
something that doesn't get changed off
so once we
we have the schema and that's something
that we have to work out with it's
actually something that's actually a
restriction in the SQL however if we
talk about the new SQL
no SQL they can always stretch they're
always very versatile in nature so we
can always modernize or we can always
change the structure of these stores in
there because the advantages that didn't
primarily the new SQL are unstructured
in nature and they don't have they don't
actually abide by a standard schema so
we are not restricted or confined with
whatever the schema actually looks like
so we can always change it we can always
change around with it and that's the big
advantage that we have in no SQL now on
the other hand if we talk about these
scaling the scaling of sql's
sq later sources even though I mean it
possible and that's actually something
that we actually do quite frequently
nowadays considering because primarily
we don't actually expect to be working
handling just a single instance we
always have multiple instances working
together in collaboration to respond to
a specific volume of data our disk
ailing the challenges primarily is
scaling is primarily vertical in nature
so I mean we can always scale up give
them that specific stance or not the
classroom management is not primarily
one it's not applicable to all of them
so there are a few sources that actually
primarily don't support the distributed
environment they are not primarily
distributed in nature however in case of
no SQL no SQL the primarily distributed
in nature so the horizontal scaling is
always easy and make sense in there as
well because the collaboration with
different instances and embel defined in
cluster and very organized data
transformations are something that
actually work that better could actually
have a handle on the core of the know
SQL or on top of now if we talk about
the day of models did you have mortals
we can always have the development
models we can have paid as well as read
then the support as well it depends I
mean what kind of instance is we're
actually working on so based on that the
support can actually change as well
however in case else say if we talk
about no SQL no SQL so finally open
source so the development is finally
open source so it's roryd actually
commercialized except for you just like
new
or Janu 4js not all of the features are
actually available in the community
version so there are a few that are
specific to the want to price version so
for that not all of the court will be
actually available openly then SQL is
primarily for transactional stores so
the STC principles are actually being
implemented very strongly and stiffly
our in case of no SQL databases this
primarily works on bass now will
actually go through
I mean how acid works and how bass was
said is a principle that actually stands
on four major pillars comity
considerations the isolation and
durability the atomic prime LG o H
actually being exited to ensure the
transaction has been executed
successfully on Lauren or it's actually
being executed partially let suppose
that we're actually executing an insert
statement however we were actually
updating the complete rule YouTube but
some of you who somehow some of the
reasons as well actually we updated the
first two fields the transaction
actually fell off or we actually
executed a couple of insert statements
the first one actually executes
successfully our the next one was not
actually it fell off to just ensure that
either both of the insert statements
being executed successfully or none of
them actually get so me there's no
possibility of acceptance of partial
executions or partial failures of
transactions to ensure that this cancer
obesity in the actual transactions
that's what the atomic city principle
actually talks about the consistency is
actually about the integrity constraints
or so insure that I mean that actually
the the integral conference or the
constraints are that are actually being
managed with the data sources that's
actually what comprises our that falls
under the consistency to ensure that
that the data is actually being
maintained consistently so that so that
there is no failure of data and if the
data is actually being read or how they
did data is actually being written to
the database south that's what the
consistency talks about
the isolation it's more popular for
example it's opposed the multiple
transactions are actually happening
simultaneously in the same within the
same time frame to ensure that they
don't actually come in between and they
don't come actually in in each other's
paths and they don't actually interact
with each other so every transaction is
actually being handled separately for
example is let's take an example and
let's say that we were actually
inserting we were actually executing an
insert statement into our table and
jaemin search statement or actually
probably calling a delete statement at
the same time probably for a different
table and what we're actually doing the
insert we're actually getting the
maximum number of rules that we have in
here to actually increment the column ID
for the unique identifier for example
let's say there's a delete transaction
that actually gets executed separately
you however you see any of the possible
reasons they actually know about each
other and they actually communicate with
each other in such a way that we're not
really sure which of the two statements
should can execute it first and in which
order and then we're not really sure
what is the last possible value so there
won't be any specific consistency in the
values and we'll be expecting from the
Select Mac statement for example let's
say so in order to make sure that each
of the transactions that are actually
getting executed are that are getting
processed on the database server are
working regardless of each other they
don't actually really interact with each
other or they don't really depend or
they don't really modify the state of
the other transaction that's actually
getting executed
that's what isolation is about the
durability is to ensure that if we ask
for example let's say that the data that
actually have that we actually pushed
into the source and push into a database
however the data is actually yet to be
written to the actual files of the
database I mean the data is probably
somewhere in between company and then in
memory or some temp memory before the
actual process could get executed
successfully so due to any of the power
simple reasons the data is still under
light it has reached the database but it
has not been the commit for that day the
source has not been executed
successfully near any of the possible
reasons to ensure that we don't actually
lose that data so to ensure that we have
proper and viable durability of the data
source that we are interacting with
that's what durability is approach to
ensure that it was supposed in such a
scenario due to any of the system
possible system failures that we can
come across that the data is and that
didn't actually get execute didn't get
processed ordering to get executed
successfully so what happens is that
we're not really able to execute the
data successfully in there so what we do
is we wait for the system to actually
come up again and once the system is
back up again then the data actually
gets crossed a successfully that's what
your ability is so these were the
principles of the acid lea thomas city
of the data what is the consistency how
integral data is actually being
maintained with a database then the
isolation how about the transactions are
happening or they are getting handled in
between and the durability how durable
the data is if we talk about the base
property you from the new SQL store base
is a little different than the assets at
the base is primarily the basic
availability so how the data actually is
available to actually being processed at
a specific time to ensure that the
database is primarily available at most
of the time so there's always a high
availability of the this is what the
high availability this is the basic
availability so deep data is primarily
available up to most of the times this
soft shade is to ensure that all the
database stores should be consistent in
writes at every possible time because
there is no proper at Tama City in the
or there's no it's not a transactional
system in nature by defaults for a no
SQL store so it's probably it differs
there's a possibility that at some point
of time you might need to have a high
read read consists of
see or write are why see worse are you
you might need to have higher right
consistency over reals in some other
scenarios so it depends on what kind of
operations we're talking about then the
eventual consistency to ensure that the
data are always being responded
consistently until a specific point
because it doesn't really it depends I
mean how optimum the cluster is how we
have actually specify it's been a lot of
factors involved in order to defend all
of those base attributes in the real
implementation side when we talk about
from a no SQL store because for SQL
primarily is its dynamic in nature
second it primarily expects horizontal
scaling so unless we are not manage
horizontal scaling and we're not defines
horizontal scaling properly so there's
always possible to see our failure in
this state now every no SQL other than
base works on the cap theorem so the cap
theorem says that every low SQL store
will not be actually able to defend will
or will not be able to support all of
these three at the same point of time so
that means that there is no real time
possibility that a store will be
consistent it will be highly available
and it will be partitioned or fault
tolerance at the same point of times now
if you talk about consistency
consistency it ensures that the data is
always consistent regardless of how many
number of operations get performed on
that data or how many times the same
operation is being performed
it's regardless of that or in case after
the change the state change of data are
lots of pause that there was a
modification in the data and certainly
you know suppose if there were five
different instances within a specific
question of Mongo per se so now suddenly
by Act one of the instances they did
actually got changed and it got
circulated to one of the instances and
in the meantime the same data is
actually being retrieved probably from
some other instance and now to ensure to
make sure that no sense in this Rio
knotch the same data is not available on
all the instances they it's actually
available on one of
instance this information is not being
circulated across all instances based on
I mean how good it is actually being
managed
based on the sharding and partitioning
principles and there but now in this
case we need to ensure that the data is
actually being pushed in there and it's
consistent to make sure that all the
nodes should be able to get only the
updated data they should not expect them
to return the all the data or the
historical data that's what the
consistency principle says
engage availability we are unsure that
in case off with the data out it is
actually being sent across and it's
being spread across while the bull Lords
in the cluster so it's important that we
ensure that it has spread across these
ports is always available so for example
let's say that we have three different
nodes and one of the nodes actually goes
down or it's being shut down now the
amount of data that was actually
available on node 2 that date that the
amount of data should still be available
if we execute the same query and we
process it only on two nodes as compared
to in the earlier possibility of
executing the same query on three
different nodes at the same point of
time we should still be able to retrieve
that data we just still get the same
copy of the data so obviously that means
that I mean how the data is actually
being balanced across and the three
different nodes and how the data is
actually being managed again so that's
what the availability principle says now
when we talk about the partition is the
system actually ensures that we are when
we are actually I understand that within
the cluster we ensure that how we want
to differentiate how we want to
segregate the data across multiple nodes
based on which principle and to make
sure that once the data is actually
being segregated with each other then
based on what principle different nodes
will be able to communicate to each
other to understand that this chunk or
this section of the it is actually
available with this specific Nord that's
where the partition tolerance is all
about so this is what the gap through
actually talks about to define the
consistency how it actually looks like
what is the availability how highly that
cluster or how highly the data is
actually available to the consumers
the data is actually being spread across
multiple lowers how the partitioning
really works in 4d cap theorem is all
about there is a possibility just like I
told you if not all caps learn all the
most you'll support can support all
three of them at the same point of time
so there's a possibility that we can
have a database that's consistent as
well as malleable then there's a
possibility that we can have the
database as consistent or an partition
tolerance or we can have availability
and partition tolerance so this is
primarily that actually works and then
it actually differs from how that's
actually again now which one of them
will be actually applicable to which
database it's primarily depends on the
kind of databases because the
consistency levels in cassandra might be
different and they are actually
different so as compared to the
consistency levels in Mongo we talked
about and the availability principles
and the partitioning strategies across
different no SQL stores are completely
different now if you actually talk about
the consistency and the availability so
it's primarily that means there might be
a single site to a store so that means
all the nodes are always available and
they're always in touch they might be
always available in nature however if we
talk about the consistency and
partitioning the consistency and
partitioning that's primarily if we talk
about that some parts of the data might
be available so there's a possibility
that there might be some parts for some
sections of the data that might be
actually accessible or it might be
available in some states but the data is
always retrieved whatever the data is
actually being responded back with the
number of instances that we have it all
will always be consistent and accurate
because it supports the consistency and
the partitioning principle and now if
you talk about the availability and
partitioning it means that the system is
primarily available in fling in case of
partitioning hour that might is a
possibility that we might have in
accuracy there might be some
inconsistency in the data that we can
actually responded back with or data
that we actually retrieve back so there
is a huge possibility around that
now in the availability and consistency
that means primarily the sorority BMS
neo4j and where this for the consistency
and partitioning is probably Mongo the
HBase BigTable the availability and
partitioning that's probably couchdb
pachi Cassandra react the no SQL
databases there are some bunch of
features that can that no SQL that the
base can actually have the first is
probably in structured of titre it can
always support unstructured are even
semi structured even the semi structured
data big data and data formats are
considered as an structure so we can
call it as who's managing the
unstructured data and then the
concurrent transactions or when we talk
about the concurrent reads and writes so
the high volume of reads and writes is
always it's actually a real time
possible expectation from NoSQL store
and that's something that no SQL
actually can perform or actually get to
do a bit with a bare minimum complexity
or a concern and then the response time
how much of time it actually gives the
response back to the consumer who is
actually trying to access NoSQL store is
is very immense and then with with
optimal high availability and
reliability features in mind so the
downtime in no SQL stores is almost nil
most of the times and that's actually
how it should be we should always have a
nil amount of downtime that we are
actually expecting for no SQL stores to
make sure that the availability of the
cluster on its whole is very high and
there's very little amount of downtime
that's available in there and then if we
talk about changing the requirement with
intense you updates so the transition
for updating the stores
updating the database and the
expectation for our the transition for
actually modifying the changes or stuff
that's something that's very easy in a
noisy old store no the how exactly the
database stories actually works in
Oracle is primarily
the one AP or LTP if we talk about the
transaction so we can always store the
data that's actually coming in from LTP
or you can directly push in data from
the our dbmss
to NoSQL or big data or probably from
the oil GPS just like retail or ATMs we
can again directly push that to no SQL
stores now the major advantages of NoSQL
is with the bees that we have before our
five years and that we have a new SQL
that if it's volume velocity variability
or veracity so for volume and date are
so what is the amount of data that's
actually being pushed in and the amount
of data that has already been pushed
into a no SQL store and how the data is
actually being managed now that is that
would be what we are talking about the
volume of data that okay probably to
date right now we have probably
terabytes or or exabytes of data that we
have stored in the SQL and at which
velocity that probably there are there's
ten megabytes of data that's actually
being pushed in per second or there's a
higher volume of data is actually being
pushed into the data probably as for the
number of records that need to be pushed
into a no SQL store only I suppose some
hundred thousand or drive cars need to
be pushed in every second would be
database that's what we will ask to use
about the variability as the various
forms of data that we have in place
since there's always a possibility that
we can have since unstructured there's
no specifying some possible structure
that we can expect the data in so there
is always possibility that we can have
data in various formats we can have data
probably in the form of movies we can
have data in the form of JSON we can
have a music file we can have a zip file
we can have a blog you can have any
forms of data that we can expect at the
same time even part is same source
that's variability and then the veracity
so the veracity is probably it
inconsistency or the uncertainty that
can exist in the data that we're
actually drawing are the anomalies that
we have in data or earth there is some
latency or delay in the data that we're
actually getting so there's not proper
consistency in the date or accuracy in
the data that we are actually getting
that's what the veracity is all about so
these are the basic four B's that we
have in no sql's no other bigger
advantage is that we can have various
kinds of schemas we can have a key value
we can have documents we can have column
based we can have a graph and so on and
so we can have we can replace some of
the possible use cases of an our DBMS
with no SQL storage and even with graph
we can actually replace almost most of
the or DBMS stores with the no SQL
stores with higher volume of the eater
or higher volume of distributed data or
distributed structure or distributed
management of data in there no SQL
actually makes a pretty good fit and
then the unstructured data are probably
huge amount of data or bigger chunks of
data and unpredictable or unexpected or
undefined format or very new formats
that's something that data is actually
being pushed in there as well no SQL
makes more sense the noise school can
actually be very helpful in there as
well and then since we don't primarily
have the tables or we don't have a
specific defined structure and no SQL so
we you'll always play around how we
really want to traverse the data the
traversal of data or the transcription
of data can always be changed based on
our expectations there's no one way of
actually traversing or retrieving the
data and that we have in noise actually
ends up with a bigger advantage it may
actually helps us off then the major
advantages we can have is it's like
agile in nature we have it similar to
the newest you're right it's
unstructured we don't really have a
specific expectation in there in the new
SQL stores the various categories or we
have document based and
midbass so we have we have a singular
key that is actually being linked to a
specific complex data structure or
probably next level of data structures
being uniquely identified or represented
through a key something similar to how
Mongo actually preserves data then the
key value formats probably something
like we have in Redis so we have similar
to a map for every unique and part
there's a value that is being actually
being shown then DeGraff store the de
Graaff is primarily storing the data and
the farmer craft so there might be
multiple trees who can actually define
so there might be Belle defined
relationships in between so every node
will be uniquely identifying or
represented with few set of attributes
and regardless how its counterpart node
will look like so we can always preserve
date on those Nords that's what the
graph is for then comes the white column
stores so I mean white column store is
primarily where and it is actually being
stored in case on columns as compared to
rules that we store data in rental ID
PMS column based structures are more
optimal and more consistent in nature
their performance is little better as
compared to graph or key value Cassandra
is one of the popular column based
stores or talked about Dino now based on
the kind of data that we have that we
are trying to preserve and thence the
kind of reads and write operation that
will be performing in the upcoming
future keeping that in mind we should be
able to understand that how exactly the
data is actually being stored how
exactly the data will be preserved off
and which form of the SQL store I will
be using so there's a possibility that
in for a few of the use cases or might
be going the document base and for
others who might be going with key value
and in some cases who might be going
with graph or the column based so it
primarily depends on towards expectation
and need right so types of SQL databases
that we have in document B of Mongo we
have couch and with Cloudant these are
the popular ones in
value with memcache current rate this
graph we have the Oracle no SQL stores
we have Orient we have neo4j in column
base we have BigTable HBase Cassandra
and a few of the commercial ones like
the aid of Lists DynamoDB and differ now
the selection I mean which ones to
actually use and how exactly to define
implementation to make sure that the
right fit for our storage and this ready
makes sense and how exactly do we
implement that we primarily come up with
three major steps that we have to keep
in mind based on which we can decide
that this is the no SQL store that we'll
be going ahead with and that's how
exactly we can take a decision on that
we need to have the correct data model
in place we need to make sure that this
is the kind of data that we pushing in
then we need to keep in mind of the pros
and cons of the consistencies what is
how what kind of what levels of
consistencies we're looking for or what
is the expectation of consistencies and
then what are the basic features that we
actually have in our DBMS when we are
actually considering NoSQL so most lot
of times since most of our systems are
already in place so we need to keep in
mind that there's a possibility that we
might actually do a transition from an
existing or our DBMS Stewart to the any
of the new no SQL store so to make sure
that the transition is more simpler so
we need to keep in we need to make sure
that there are a few of the features
from the our DBMS side as well under no
SQL now this is this is more on the
generic on the new SQL side if you know
we talked about how the brief about the
Mongo and how exactly Mongo really looks
like Mongo is primarily we talked about
it's one of the most popular open source
document based systems and it has a very
impressive performance even though the
performance actually keeps on going it
has a very consistent high availability
and it has a very extensively abrupt
performance as
well it's scalable it's it's very easy
to scale Mongo up at any point of time
these scalability features in Mongo are
relatively very simple and easy to take
care of in order to make sure that the
new sql's in order to make sure that the
amount of processing times and the
execution of more complex queries gets
more simpler that's where in Mongo
actually really fits in
Mongo is primarily most of the data is
being stored in the form of documents
are the the set of collections that we
have so it's primarily in the form of
chaise horns that preserve the data even
though to make the processing on the
data a lot more faster since it's an
unstructured source so the data she will
not find across the concepts of the
identity columns or the primary and
foreign keys in here even though we have
the key that actually uniquely
identifies the data that you're actually
pushing in to a specific document now
the basic features of Mongo Mongo
primarily but you can stores the JSON
data modeling with primarily the dynamic
schema so we don't really need to the
predefined static schemas in place can
always go with the dynamic scheme must
we can have with the aggregation
frameworks and we can interact with the
native or MapReduce Hadoop framework as
well we can do these secondary indexes
even the induced spatial queries as well
we have there's a lot of building
features and Mongo for the replication
and high availability we can always do
the auto scaling after the data are so
the horizontal scaling and sharding and
how to do the partitioning of data
across multiple lords to ensure that the
data is not being pushed consistency
across one single Lord and it's being
balanced all the thoughts in the cluster
are being balanced very accurately and
very properly that's what some of the
key features of mango then if we talk
about this the law of community this law
when we support that's available that
actually makes Mongo very convenient to
define as implementations because a lot
of the communication with Mongo is
relatively very
easy and simple so there's a lot of
community support available and the
drivers and API is that are available in
there that actually give us and that
actually make it a good fit to actually
interact with using any of the
application layer languages the reasons
I mean it's always important thing I
mean what are the reasons why we should
actually make use of Mongo
obviously the scalability is a big
factor and thence the streaming of the
data so I mean the development to
enhance the developments board our
development lifecycle
that's all that's another big reason
that Mongo actually make becomes a good
fit because they read tribal for example
this lay the retrying mechanism for
rights to make the data more consistent
and to ensure that we don't lose rights
Mongo has every tribal a right so to
ensure that we have a more managed error
handling so we don't come across with
too many errors in case a lot of few of
the rights actually missed are from the
list they're still being actually retry
they're still being considered for retry
mechanisms that's what the retry is and
then create different data pipelines to
stream the data and we can always change
the stream of those data as how those
teams will actually really look like
when we're actually pushing data from
Mongo to the outside were all from
different worlds to Mongo in itself then
the shard awareness of the secondary
reads to ensure that indeed a
consistency is there was supposed to any
of the possible reasons that the data
that's actually available on the primary
short do any of the reasons that shard
actually goes down off and now since the
day but shard has gone down off to make
sure that we don't lose the data based
on the replication factor that we have
specified any of the secondary shards
will come up and be elected to respond
to the queries that were actually being
sent to the primary char and then based
on the geographic locations as well that
which one of these secondary shards is
there in case the secondary shard is the
newer one who's actually responding to
the request basin that's will be able to
execute or will be able to respond to
that specific query or especially
expectation of that query ensure that
there's consistency in the data that
we're actually pushing on from different
charts then if we talk about the
manipulations or the processing in Mongo
is is quite easy as well a lot of
inbuilt functions or methods they're
actually smooth enough with the process
when we are dealing with the retrievals
updated and in there and we can always
have the complex possible combinations
when we're actually doing when we are
talking about the manipulations our
cater aren't the updates well in the
development phase for the scaling up
certainly we can always scale up even
with the simplest configurations we can
scale up even to a very high probably
even more than 10 X or made it's
relatively very easy and then the
management of the JSON structures the
interval instruction of how Mongo
actually really preserves the data are
using the JSON structures it actually
really made it very easy to scale up for
the system in nature these are the
primary reasons for Mongo tour if you're
actually trying to use Mongo then we try
to speed up the development or to
enhance the scaling then if we talk
about the running anywhere because Mongo
is globally distributed we can run Mongo
as a service I'm majorly using through
Atlas so or if we are working with cloud
so we can always run Mongo as a
clustered Nords cluster of Nords in the
public cloud and we can directly access
it from anywhere and the automatic
scaling actually removes off the
development operations overhead by
actually provisional and provisioning
the additional clusters whenever it's
actually required so we can we always
have some initial Astor's really need to
go and have an explicit DevOps team ours
and then ops tools link to that for the
insights we can always have if we talk
about the aggregations on analytics we
have there a lot of pipelines that we
can enable off for analytics it's very
limited amount of explicit core that we
acquire majority of the
pipelines are itself very strong and
flexible enough to handle the majority
off the operations when we talk about
the aggregations or analytics Mongo it
actually provides Lord of business
intelligence integrations if we talk
about the SQL bi is the the SAP objects
the colloquiums tableau ours by any
other popular BI tools no primarily
tableau is more popular than their the
Mongo actually provides a good
integration but are as well or the
native libraries as well they can help
us even with the native libraries we can
do some data mining and statistics and
it analytics there's a very good
integration where they are available for
Mongo as well that we can use to get
some more additional and agree mental
insights as per the market trends that
has actually really Quorn the you know
SQL market is actually is used to grow
up to almost four billion dollars by
2020 as first CCS you are 35% awful
growth from to 2014 to 2020 Mongo is
primarily one of the leading no SQL
that's no doubt among the fortune 500 or
the global 500 a lot of developers
prefers or intend to use Mongo or worse
the others different no SQL stores
because it's relatively very easy and
simple to configure and use from as you
can see the data storage and data
processing how it has actually really
cool off from the no SQL stores as
compared to the any other store the
popularity of Bangor or if we talk about
elasticsearch pulls grass a lot of these
tools already popular these days now and
Mongo actually has a lot of reliable and
a huge list of a very long list of very
renowned and reliable customers who have
actually been testing and who have been
actually working up with Mongo
extensively and experimenting and
leveraging the best out of Mongo
and giving us debate the major insights
and various possibilities where and we
actually really feel that Mongo actually
makes sense whether if we talk about a
Google or Facebook our SAP and then we
have a lot of other users as well
there's a long list pretty long list and
it's actually going that every passing
moment sectors I mean primarily in which
areas we primarily use Mongo in for if
we talk about this service surveillance
data aggregations crime data management
analytics citizen engagement platform
program data and healthcare record
management or risk analytics and
reporting time series data portfolio
management and order capture reference
data and market data management these
are some popular sectors the hard
sectors for Mongo implementations or if
we talk about the content management and
delivery user data and digital asset
management mobile and social apps
content archiving
360-degree patient view lab data
management and analytics well that's the
healthcare side mobile apps for doctors
and nurses the e-commerce site we talk
about the customer data management
real-time price optimizations rich
product catalogs new services and
digital coupons then in the in the AI
and the m2m or at the IOT side the
customer cloud and product catalog the
customer service improvement
machine-to-machine platform real-time
network analysis and optimizations
the advantages are fusing Mongo
it's very easy to scale it can scale up
very easily
it's document based so it's schema is
primarily agnostic in nature so you get
always have different structures of
schema Mongo supports a lot of dynamic
queries on top of document does have it
doesn't really require any complex
joints or intersections to retrieve the
data structure is
primarily the basic structure is an json
or an object in nature that the
accessing of data is relatively very
fast primarily by using the in total
memory in here if we actually try to
retrieve the data and that's read in
Mongo and the performance tuning and the
optimizations within a Mongo are
relatively quite easy we can easily put
do the configurations there are very
very simple set of configuration steps
that we need to perform in order to
achieve those conversion and or mapping
off and the the actual objects to
database objects it's actually really
not required since primarily we can
actually always push the data into Mongo
primarily through the application layer
so we can directly push that data
without actually making a change on or
without actually really making a
conversion to the daytime there I know
the standard terminology is and some
commonly accessible or commonly supposed
they using tools that exist in Mongo by
default it's always the database the
database is primarily a group of
collections and collection is primarily
database is primarily just a major
high-level container where there can be
various sets of collections and each
database primarily will have its own set
of files that could actually create it
on the background in the file system on
it the disk obviously a single server
can have different databases we can have
more than one database in nature Mongo
DS is the primary source for the Mongo
database system it it's the initial
source that is actually in there and
then for actually handling the data
requests we have the managed formats we
can have the background management
operations as well we can have all the
various requests the formats our
expectations we have the collection now
in the database then inside the database
we can have collections collections is
primarily group of similar or related
different documents
can be preserved inside a single
database even though collection doesn't
primarily impose or enforce a specific
schema or style or pattern of storage
structure in nature so there can be
different documents with different set
of fields regardless of how they look in
nature the only difference is that they
might be primarily alike or they might
be related to each other
that's where primarily where the
documents will actually make more sense
so I mean if we try to make a comparison
from an our DBMS perspective to a Mongo
database in our DBMS a similar identical
throw database in Mongo table in our
DBMS is primarily identical to a Mungo
collection and the rule the actual
record rule the RDBMS table is similar
to a document the columns that we have
are our DBMS are similar to fields and
the table giants I mean there are
similar to embedded documents so what we
can have we can have a nested documents
we can have a document object within
another document object the one that
actually links to each other
that's what the documents are actually
linked are and so because there's a
possibility that we can have different
kinds of document since we don't have a
realistic relationship there's no
possibility of actually tying different
documents together that's where the
embedded documents really makes sense
the primary key in case of an our DBMS
we don't really have set the primary key
in Mongo even can servers be my support
for example their code in Mongo it's
primarily the Mammootty and then the
actual command in our DBMS we primarily
have the MySQL and there so that's heard
so I mean from an odbms perspective to a
Mongo
we have the corresponding or from a
database with respect to our DBMS we
have a database in Mongo with respect to
a table we have a collection in here in
comparison to a row we have a document
in here it with respect to the column we
have a field in here
with respect to various depend on tables
or the join table
in our DBMS we have embedded documents
in here in case when we talk about the
server and client the server is
primarily the MySQL D or the article we
just simply run the article in G so or
DBMS is in Mongo it's Mongo D Mahmud II
actually runs the daemon --thread for
executing the default server in there
and Mongo runs declines however in case
of s compared to the our DBMS is Margo
doesn't primarily really actually have
the concept of primary Keys however we
have the key ID that is very identical
to the primary key and our DBMS so
that's where these are some Santa
terminology with respect to the our DBMS
and Mongo now with respect to the tools
there are various set of tools that we
have and these tools actually are very
useful when we are actually working with
Mongo they help us in various sections
or in various different steps in the
core processes the primary tools are
primarily mangodi Mongo s and Mongo they
are primarily are the tools that we
actually have in there then in case of
data export import and export we have
Mongo import we primarily have Mongo
export for pushing the data out of the
Mongo instance to to the important
export primarily makes sense if we're
actually trying to take a backup or
restore from a backup the Diagnostics
tools the mongrel stats the Mongo ops
this sniff wash the performance so they
are primarily used for the monitoring
and the performance tuning to actually
get a health check of the instances and
to understand how the instances really
look like
that's where primarily these instances
or these tools will actually help us out
in alright so the windows services we
primarily have the Mongo T this is the
actual service and the Mongo s these are
the background services then for the
binary imports and exports there
faster than the data imports and exports
so we can take the Mongo dump the
restore the beast roaring down the
Mongoose blog or the Mongo files
that's the grid files
the introduction to JSON and the B Sun
the JSON is primarily the JavaScript
object notation I'm sure most of you
will be orally acquainted with JSON it's
a very popular term everywhere whenever
we are doing at the the transactions
when we were actually pushing the dayit
are in the form of request or response
but then we are preserving data it's
it's a very popular structure of storing
data certainly it's primarily it's very
lightweight so de France mission of JSON
is really convenient and clear it's that
the this index for the storing
exchanging is relatively very simple
it's primarily based on the JavaScript
standards so they are being implemented
with more softie languages that are
around there so if we talk about the web
services the communication on web
services if we talk about FRA matelassÃ©
is simply in the form of JavaScript
itself where in the browser is since
most of the browser's actually support
JavaScript and/or they are actually
being written on top of to execute
JavaScript properly that's why this one
is very popular it's primarily used for
sending communication from web servers
to the app in between servers and web
applications to make the transmission
little lighter and faster in nature
that's what primarily JSON is actually
really looks like so it doesn't really
matter how exactly I'm sending because
it's a standard format it's regardless
of the language in itself well on the
other words the the beasts horn is
primarily the binary form of the chase
or it is just the in coordinate
serialization of the simple JSON
documents so that means the actual JSON
documents that we have in place we just
do the encoding for them in the binary
format for the faster processing since
there are binary in nature so these
support in the embedding of documents
and areas within others documents and
areas to make it more optimal and to
understand to ensure the complexity of
the embedded documents that we have in
Mongo since primarily the old data that
actually one will shortz is actually
being within the form of binary JSON so
to ensure that we support stamina
documents so we can always have
extensions the flavors that are not
actually available
the types that are really not available
in json and itself or some extensions
and probably it's being extended from
some other source that's something that
we can actually add in DB s-- on as well
be saunas primarily it's more efficient
in busan as compared to simpler jason's
and the traverse souls to ensures our
and the lightweight it's primarily it's
where you lightweight since the overhead
for any database the presentations is
primarily to ensure that there's a lot
of overhead when we are actually doing
the transformation over the networks to
ensure that the processing speed while
we're actually sending the data for the
transmission is very light and it's very
fast and that's worth the major
characteristic strain sure that nissan
is very lightweight since it doesn't
actually send much of the configurations
because it's conventionally similar to
DJ songs that we're actually sending off
the traversal did it helps us in
actually smoother and worse luminously
doing this our searching a specific
document within the list of various
documents that we actually have in place
the bayer the beasts ons can really help
us out in a little faster speed the
traverse all done really very easy in
binary jason's efficiency off actually
the encoding and decoding of these ones
are primarily very very easy and it
takes very little amount of time and
that's it's obviously because most of
the mangos actually being written in C
or C++ so and as we know that the C and
C++ always have give us a very efficient
performance in nature so it's always
very easy to make sure that Nissan is
very efficient and actually doing the
conversions weathers we talked about the
encoding or the decoding the simple
example I mean how exactly this might be
looking like there's like we have a
simple JSON in nature and this is
actually how the beasts on one primarily
look like this is a conventional
structure I mean it's not necessarily
this will always look like depends on
ways factors as well same apply the
licensing installation Mongo is
primarily it has obviously every most of
the data sources have multiple editions
they have the open source one the free
one the more
moody and the commercial one the our
enterprise MongoDB it actually comes at
Enterprise one primarily comes in with
some advanced on the security side and
the management tools and some additional
integration API is ready to use
integration api's these were something
that are primarily not available in the
open-source flavors remaining other
features regardless of all these
specific supports I mean it's like we
have the advanced security management
support certified or support on-demand
training entreprise software integration
these are primarily in both plugged in
or shipped with the order price version
that's something especially when we talk
about the support you would on get a
commercial support in the community
version so let's sometimes gets a little
challenge first the downloading we can
always download Mongo from the specified
URL that we have that's mentioned in
here we can also download the no SQL
client it's available in github to have
the UI representation for Mongo now we
can download the MongoDB from the
browser so let's try to do that these
are these steps for the installation or
Windows you download it from the
browsers once you choose whichever
installation you're actually looking for
then you can follow the visit that's
mentioned in there once it's installed
and you can execute you can start Mongo
server and you can access the data go to
the other folders whatever folder you
are actually looking for run Mongo
servers the Mongo D and once 2701 seven
in different command prompt you can runs
the Mongo client and you can actually
start accessing then you can probably
download the new SQL quite if you want
so let's try to do that lets me download
Mongo Mongo DB downloads
see the downloads
so that's how you actually go to the
download center see there are various
features we have Atlas database as a
service we have community servers we
have enterprise ops and compass go to
the community one we have this is the
installation how you're actually trying
to install so install the Downloads dgz
right so the GC is actually getting
downloaded now once it gets downloaded
off then we download the USQ point
go to the no SQL client
you can choose any of the clients that
we have that are mentioned there
underpriced virgins they have various
flavors that actually inaudible there so
you can choose any of those see you
download which to rescue aversion you're
actually looking for right Dino SQL
client wants you downloads the no SQL
client one the Mongo client primarily
right once if your mug was actually
already installed so it will
automatically detect for - or if it's
not then you can actually connect now
the embargo is primarily actually
looking for that specific object in
there so now let's say right so let's
try to see if that's downloaded still
here we what we can do this we can
install so even download any of the
MongoDB DUI clients so whichever
actually suits you are you can we can
choose any of those so let's see so this
is no SQL client this is just to give
the user interface to our client so once
you actually go through the wizard like
since I am on you know the Download
Center we have various options in here
we have the cloud one cloud one is for
using Mongo as a service then the Atlas
one then that's what the Atlas is for
community community one is community one
is primarily and we are talking marks
the free one then the entreprise one is
the paid one the ops managers the
complex these are are the connectors
you
right so we among going here terminal
then
you
all right so it's saying there is no
data file
you
right so the XS we start Mongol with the
super users with administrative
privileges sometimes and it's not
actually able to access
that's what starting now let's start the
mangu now let's go to downloads again
downloads Mongo
you
cute ensue
all right so now we're actually logged
in
Oh No see the connection we're actually
only able to connect to the one internal
client it's the u.s. gives the
information about my OS nope that's
exactly what we need to do we're
actually able so that means the first
step is we download it based on which
machine that we are working on since I'm
using a MacBook so I downloaded my tar
file then once I downloaded the tar file
I unzipped it then inside the bin folder
I executed from been for I security
Mammootty command to run it off once I
started the server's using this pseudo
that means the administrative privileges
that and I started the Mongo instance
the Mongo client and once I actually
have the Mongo client in place now I can
run the Mongo specific commands in here
okay so let's see
TV's right so
this is the list of databases that we
have in mangu right now so if I say use
you do break it pulls so what it does
it's when you actually special these are
some standard commands that become
they're all really handy when we are
working with Mongo if we want to build
the list of the DB's that we have in
there so we say show devious and then if
we actually want to use a specific
database if the database does not exist
then we actually define and that
specific database within Mongo or it
actually points to it that specific
database if a database already exists
now if we need use show DBS
right so I see to move a car
okay so if you want to show TVs just
want to show the databases we say show
databases or just show DB's it will show
the babies in there say use ad man that
will use a twin
no sorry
this accuse different car
you switched America hold that's
something that will actually explore
next session Vernon will actually be
exporting more on the objects
what is there to you I mean how we
actually execute those commands the list
or face the various list of commands
that we have in place and how we
actually modify them off how we execute
the how we actually run those commands
that's something that we'll explore in
our next upcoming session and with that
we're wrapping up for this session so
thank you or until then have a nice time
right thanks I hope you have enjoyed
listening to this video please be kind
enough to like it and you can comment
any of your doubts and queries and we
will reply them at the earliest do look
out for more videos in our playlist and
subscribe to any rekha channel to learn
more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>