<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big Data Analytics in Python Programming | Edureka | Coder Coacher - Coaching Coders</title><meta content="Big Data Analytics in Python Programming | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big Data Analytics in Python Programming | Edureka</b></h2><h5 class="post__date">2014-08-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G8VvTp0zgC0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so why peyten you know fighter mm it's a
great language and it's very easy to
learn you know if for a starter who does
not know Python you will take at maximum
two to three days you know to start
writing programs in Python it's that
easy if if you have some kind of
scripting language experience you know
be it's bad scripting cell scripting it
makes a life much more easier or or for
that matter if if you know any language
you know fight them would be a cakewalk
it'll be very easy to adapt them and
understand the biggest strength of
Python and you know it's like I told you
right
it's is the most preferred language but
it a scientist it's because uh you know
they have a bulk of library which is
portable it also supports you know UI
based programming and and we used to
create applications on portable on Mac
Windows UNIX and so on and so forth and
and also you know point to be note that
it's it's open source it's free with
libraries like PI Roop and scifi it's
it's dream come true for Big Data you do
not have to write lines you know
hundreds and hundreds of lines of code
like you normally do in Java you know
let's say for example if if you are
writing two hundred lines of code in
Java you will be writing at max ten to
fifteen lines of code in Python so it's
it's that simple now the the growing
interest in Python over time right since
June July 2010 it has picked up and and
lately there's a surge of people you
know wanting to know how Python and
learning fight them so it's it's just
going to grow higher and higher now I'll
be taking you through a demo and web
scrapping using Python so just to give
you a brief as to what I have I plan to
do I plan to
you know extract some basic financial
data from google.com finance if if you
know folks some of you are I'm sure some
of you might be working in finance so
you'd know as to what google.com finance
website does right it it gives you all
the financial little information of
companies which I'm going to show it to
you shortly we shall use open source web
scrapping framework called beautifulsoup
you know there are other packages as
well which you can use but for sake of
simplicity I am using beautifulsoup here
and scrapping is used for a wide range
of purpose for data mining to monitoring
and automated testing so so let me
straight away get to the problem yet now
now see this this is Google Finance
right and let's say for example I want
to see data related to SAP all I do is
you know I certain SAP and then I get
all this information range 52 weeks open
volume market cap so I'm not interested
in this for now for my example what I
would be interested is in this data this
is what I want I want the net profit
margin of sa P I want the operating
margin I want Evi TD margin return on
average asset employees CDP's score you
know these are the only data I am
interested for now this is what I want
to scrap the data for demo purpose and
that's I want to scrap data not just for
IBM I want to scrape data for Facebook
as well right I just want to do a high
level comparison as to how this company
companies fare with each other you know
when when you look at this financial
data so all this data you know you can
see all this data so let's go to the
code and then you'll be surprised to see
it's only so many lines of code right so
like I told you I'll be using
beautifulsoup right you see here I'm
using beautifulsoup and I have listed a
few companies like SAP IBM TCS or actual
Facebook and Google's of whose data I
want to pull and and just note here
right see I am I am building in URL here
although you know if you are not from
you know Python you might not be able to
understand it right now I'm just giving
you exist it will take you maybe at max
two to three days you know to start
working with Python so so don't worry
about it I'll just walk you through the
logic
as to how I did it and you know how many
lines of code the is is needed here
if you look at the entire code you know
half of them is about formatting okay
and the actual code is hardly five to
six lines that's about it and then you
know I'm building a string I just say
google.com finance and then I've given
the name of the company here if you go
back to let's say you know here see this
is how you can build your query right
you see this financed Facebook let's say
for example GCS right you get this data
so this is what I took into picture
while you while creating my code and now
this is what I am building here the URL
I'm opening the URL using the you know
beautiful shoe and then I'm reading the
HTML I'm reading the entire HTML let's
say for example I want to read this if
you go and look at inspect element you
know you see all the code and then you
know that this is the net profit margin
that I want right so I go here
and then I load it in my beautiful soup
I load the HTML in my beautiful soup and
then I make a note here and then finally
I go and check for class equal to column
header if I go and search here column
its
EAV here you see this this is what I
have done I I have checked for class
equal to column header here which is
classical to column header and I pulled
all this information net profit margin
and so on and so forth here so finally
you know I just find all all the class
which is column header and then you know
using some some regular expression I
just finally print the data I also find
other information plus equal to LFT name
and so on and so forth right so let's
let's run this and then basically it's
cleaning data you see this re I abuse
regular expression is cleaning of detox
s2 you know in which format I want the
data to be to be there in my screen so
let's see give me 30 seconds so say this
data SAP IBM now let me stir
my program for around running now you
see this
so for SAP right I get this information
exactly as is it's here in this second
now let me go back to ACP and then let
me get this back validate whether the
data is correct or not net profit margin
twenty five point eight three twenty
five point eight three nineteen point
seven seven so on and so forth now you
know having this data in one place in
one terminal or maybe let's say in a
file in Excel see you know it's so easy
for you to identify as to you know you
know how the company stands these are
financial information this is what just
one part of it right
I can scrap give me any web site and I
can scrap anything that you want
so using Python and some of its packet
it's extremely easy for you to pull all
this data study about companies and and
make make a mission of your own so if
you were to be a financial guy right now
you'll find this information very useful
so so that's about it that this is about
beautiful soup and and a little bit of
Python now now let's let's go back to
the code right
I mean what's what's most important here
the most important things here are you
know you look you opening the page you
reading the page loading it beautiful
soup and then you know you finding the
class column header and then in the you
pulling all all these informations so
hardly there are ten lines of code you
know which is important here the rest
are same you know this this is copy
paste and then if you look re you know
whatever you see it's regular expression
saying that you know I just want to pull
everything you know that is there from
column had in in my HTML so that's about
it and this is about cleaning data a pop
pop means delete the data at the end so
so that's about it I mean you do not
have to worry a lot about this this is
all cleaning data like I told you it's
just five to ten lines of code that's
about it now let's go back to our slide
now this this was web scrapping using
Python
now let's go to you know collecting
tweets using Python now you might say
right off all these days you know social
media analytics is a big thing if you
have been watching news lately they do
so a lot of social media and analytics
they do sentiment endless analysis they
do all kinds of analysis I mean if if
there's an election going on that they
try to find a friend as to whether there
is a positive sentiment or negative
sentiment if there is a movie you know
if there is a you you have Oscars going
on you know then they do sentiment
analysis of movie and us you know just
to give you an example right last year
using big data you know someone
correctly predicted the first you know
the oscar-winning actor oscar-winning
movie and oscar-winning director it was
it was accurately predicted I mean it is
add to that extent we can predict the
data now you would ask me as to what
technology did he use in order to do
that you know you can use any technology
but you know but if you are a data
scientist or if you intend to be a data
scientist you'd definitely use Python so
let me give you a small example or demo
of you know collecting tweets using
Python you know all of you must might be
in tutor right you you write a lot of
things day in and day out I myself do it
a lot so so how do you pull those data
about something let's say forever some
individual or let's say for example some
shoes or some movies you know anything
whichever you want to analyze how do you
do that so so this example demonstrates
how to extract historical tweets for a
particular brand like Nike or Apple so
this is one thing like the code that I
have written is used to extract
historical tweets now if you want to
pull the streaming data you have other
tools available which are Hadoop
ecosystem tools called flume right using
flume you can pull those streaming data
but if you have to pull data from fast
you know you have no way to do do that
in in Facebook in in in Twitter and
Google Analytics you know and so on and
so forth you know that take any any
social media name
so in those cases we shall make use of
REST API to call Twitter and extract
wits 1 and this data can be further used
to perform like like I told you write it
it can be used to perform sentiment
analysis you can use natural language
processing that's one powerful algorithm
and whose package is again available in
Python right
just think about any algorithm just
think about anything that comes to your
mind you know when it comes to data
processing mining or or anything you
know related to data and and it is
available invite them so it is you know
if that's how much powerful it is so
let's let's go to my Twitter example so
this is one code that I have written you
don't have to worry about this if you go
to dev da twitter.com you get all this
information so you know nope no point
copying my my information and I have
just used one package Oh ay OH ay uth to
you there are many many packages
available in Twitter I mean people have
already worked very hard creating this
packages so there they can be reuse you
do not have to write them and then you
know these are again consumer token
client and nothing to worry about it
this is how the package comes and then
you have to provide this information the
important things here your log into
Twitter you are you know you are
creating you're putting this parameter
so right now I want to analyze the data
from Nike I want to pull data related to
Nike all those data related with 98 now
you'd asked what is this 200 you know
Peter is very smart
they would not provide you data for free
right and at max for free I mean without
paying any money you can pull three
thousand two hundred tweets and then on
top of that you cannot just pull three
thousand two hundred tweets in one shot
what you have to do is you know you have
to pull data in batches so 200 is the
batch size that has been defined so if
you have to pull data you have to first
pull 202 it's you know find a max ID you
know all these tweets I haven't have an
identification number it's like if you
have 3,200 to its it will start from one
to 3200 and then you pull the first
without 200 and then and the 201 - again
400 and then 401 to 600 so on and so
forth right so so you build a parameter
here using the user ID which is Nike and
then the count which is 200 you take
this you you create your URL which is
the fixed URL of the Twitter source and
then the parameter here and based on
that again here you know just don't
worry about all these things okay I I
created a program wherein you you can
pass dates and then only for that
particular date you can pull the streets
and so on and so forth now I have been
experimenting a lot with this so don't
worry about those things so basically if
you are interested to pull Twitter data
of yourselves you can write your name
here and if there are many people with
your name it will pull all of their data
as well or you can you can give your
user ID you know you have login user ID
and and it will pull your data right so
here I have created a request URL see
all you have to do is you have to pipe
pass that to a client dot request you
pass the URL nu get a response the
response is Jason okay it's in JSON
format but for folks who do not
understand JSON format it's it's
something like XML it's not exactly XML
XML has tags but it has a format but
it's not completely structured okay so
so you'll get a lot of response there
you will get the maximum ID minimum ID
user information the tweet count retweet
information in a host of other
information
hundreds of information you will get it
here and you will also get header
information the only thing that you need
here is you need the to it you need the
text right you don't need any other
thing so so here I am trying to create
the total pull count so you do not have
to worry about it so this is the most
important thing you got a response and
then what I do here is I go to the
response and then I pass it I parse the
response here properly and then finally
what I do is I I print the data that's
about it you see you see this here this
is the most important thing here you
know i i parse the data from
this date to this day that is what I
wanted to pull you know I passed this
information and then I print it so let
me run this in code for you
and and if you look at the lines of code
that I have written
don't worry till here it's a it's almost
up you have to write it always you know
just look at the number of lines of code
half of them is commented right hardly
fifteen fifteen lines of code it takes a
little time to run because it has to
connect to Twitter and then there is an
authentication process a handshake that
is done and then and then you start
getting the data that Twitter does
authentication and based on that they
send you data right so that's the reason
why it takes some time okay it's it's
it's not like it takes minutes and you
know minutes and hours it hardly takes
30 seconds to one minute at max but but
then you see you have all the data of
Nike I mean whoever has been writing
about Nike at at maximum I told you
right it can be 3200 tweets and not more
than that because beyond that you know
you have to pay in order to BA buy this
data because this is actually sold there
are companies who survive and selling
Twitter data so it's it's that big a
business but for you is simple you do
not have to do anything
five to ten lines of code and you are
done so that's what that's why I say
it's that it's the most preferred
language of data scientist if you would
want to grow into being a data scientist
someday this is one language you cannot
ignore you have to learn this because if
you start writing this code in Java or
any other languages then then you cannot
imagine as to how much time it will take
and how many lines of code you'll end up
writing so again going back so you have
seen this now now you know how hot
possibly you know how powerful Python is
but now I shall give you an example of
word count using Hadoop streaming API
okay
this example shows that simple word
count application written in Python we
shall use Hadoop streaming API to run
MapReduce code written
fight them okay I'm using Python to
write a MapReduce code and and
word-count application can be used to
index text documents files uh forgiving
search query let's do that now now let
me first show you as to what my inputs
are
okay so
so this is my data okay I took a very
very simple example in order to explain
this to you right I did not want to make
it complex now I have this let's say for
example deer beer river car car river I
just want to count the number of
occurrence of each of these words so the
the result that I want is okay just give
me a second success I just want to show
you the output before you know I
actually show you the problem statement
no okay fine I let's run the program and
then we will see here let me open one
terminal so this is how you you look
inside Hadoop right generally if I mean
if you have worked on linux shell
scripting you do an LS but I am NOT
doing LS Here I am doing you know Hadoop
FS minus LS so that's how you do now
this is more of a python class so i'm
not going to explain in detail back to
you which we can keep for later now
now you see this this is the output that
I want okay let me copy this for your
reference and all I wanted to do is I
would take this input and then I would
get this result I would count each of
the word that has benefited here is
repeated DW y this is Z W so it is if it
has been repeated only once the air is 1
V W is 1 VAR b r is has been repeated
twice so how is that done how how has
that been done so let me go to mine
install hadoop folder and let's see do
you see this map adore PYD i just show
you what this is okay I just take an
input the file as a as a standard input
I just split the words you know I just
strip whatever is there in their new
line characters and and whatever is
there
I just split the word and then finally I
password and one or four examples you
know this is how the logic is this is
how the mapper works like I said it's
it's a value pair every word would be
split and that one will be assigned to
it and when you go I'm sure many of you
would not be able to understand this
pardon me for that guys because you know
I'll have to the you know look into the
time as well right and here all it does
is it's it aggregates the data okay you
don't have to worry about they say I
have given comments for each of the code
here there's much comment for each of
this code and enhancer you know it's
looking big so how is it that you
execute that you execute it like this
you know you say Hadoop jar and then you
say you know this is all part of the
syntax so you do not have to worry all
you have to worry about they see Python
and then I am giving the location of the
Python method or py Python reducer that
py and that's it I run it like this okay
did I one second okay
it has started okay it says the folder
already exists that's why it failed and
I give 6 now it'll it will run
successfully so there is no 6 file here
so the mapper and reducer I started
right the map person base is zero and
reduce percentage is zero right now so
the whole idea of showing this to you
was not to explain the MapReduce per se
it was to show you that you know even
using the streaming API sand Python code
you know you can very well write your
MapReduce programs and then process you
you the amount of data
now it says map is drawn reduced started
so here I wanted to show you something
see it created this file it created at
8:28 like what's the what's my time now
it is a 29 so you just created this
folder just now it's still processing
the data and it says output is complete
so if I go here and then check in my
HDFS and whenever a Hadoop job is
completed it great C file it's one is a
success file with with 0 kb a log file
so if if there is any kind of failure
there will be something in law otherwise
there nothing in that and a part 0 0 0 5
so that part 0 0 0 file let's see what's
there in that part 3 0 file
so this is exactly what I wanted right
it's so easy it's it's that simple I
mean you might not find it so simple in
the first time itself of course I mean
it will take a couple of days learning
for you to be acquainted with all the
external lodgings and then start doing
things start writing covert but I would
I would guarantee you this that with
Python this is hundred times more easier
than Java or any other language but if
you are in Java expert then then then
good I mean you know you you know both
so let me go back to okay let me
minimize this so I've given a demo on
that you know this demo is done</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>