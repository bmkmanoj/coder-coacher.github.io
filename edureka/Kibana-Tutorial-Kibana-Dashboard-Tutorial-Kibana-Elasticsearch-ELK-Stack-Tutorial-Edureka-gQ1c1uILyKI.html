<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kibana Tutorial | Kibana Dashboard Tutorial | Kibana Elasticsearch | ELK Stack Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Kibana Tutorial | Kibana Dashboard Tutorial | Kibana Elasticsearch | ELK Stack Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kibana Tutorial | Kibana Dashboard Tutorial | Kibana Elasticsearch | ELK Stack Tutorial | Edureka</b></h2><h5 class="post__date">2018-01-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gQ1c1uILyKI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good evening ladies and gentlemen my
name is Martin and in behalf of Federica
I welcome you all to this session on
Cabana tutorial now a lot of you people
might be thinking like Ibarra right so
the reason why I'm having a session on
Cabana is because it is a very integral
part of the ALK stack and those of you
who know what the eoq a stack is don't
worry I will give you a quick
introduction to what a milky stack is
and then I will get to Cumorah okay that
would be my first topic of the day
so Akash mahi is Devraj all of you in
the session don't worry I'm going to
first show you the agenda and if you
have any doubts after this put them in
the comment box and we'll fill it out so
like I said the FK stack would be the
first topic that I'll be talking about
it'll be a small introduction and after
that I will guru Tyrians of what is the
role that Cabana plays in the haystack
okay and the next topic is going to be
about the key bona fide dashboard what
do we have you know in the dash would
have it give you a quick demonstration I
will give you an overview and then
finally I will give you a demonstration
of how to perform visualization and
analytics with cabana right so a
majority of this session is going to be
your hands on but I'm going to show you
a few slides before I get to that point
of time okay so guys I'm going to start
the session and if you have any doubts
you can put them in the comment box so
again to our topic
blk stack is basically a combination of
three open-source tools elasticsearch
log stash and Cabana and the reason we
use the elastic search is mainly for
performing log analysis now you can ask
me why do we need to do log analysis
right a lot of people can have that
question his analyzing logs was really
important because a lot of people don't
do the importance a lot of people think
that logs are a waste these get
generated for no reason and they are of
no use to us well if there are any of
you in this session with who has the
same opinion then guys that standard
change your opinion because logs are
actually one of the most important piece
of data that you can generate right so
whatever we invest you gonna grow your
business these logs are going to have a
higher than that now the reason is that
in today's world beat any business
okay it can be anything it can be a
backing or it can be banking or finance
domain and the ID it can be educational
domain any domain take it you know for
that matter of fact in these domains
computing devices play an important role
in fact even non computing devices like
routers switches bridges all these also
play an important role because of
Technology right we need internet we
need to use mobiles or these laptops or
systems so because of these reasons it's
really important that we also make sure
that these computing or non computing
devices are healthy all the time because
take for example a company that relies
completely on the Internet like say
there's a software solution provider and
he has a server running and basically
his product is sold on the cloud he may
be a software as a service provider at
that time if all the service is on the
cloud and if something happens to his
server where his hoof it is software
then there can be a big problem right if
there's downtime of the server then that
can lead to a lot of loss because if his
proper is not being used by his
customers then he is going to walk away
his business is down right so that's
just one example I gave you but take any
other example also it's really important
that you have to have your devices up
and running your computing north of
feeding devices your I or II devices if
you have any your smartphone
smartwatches all these device should be
up and running now how will you make
sure that these are healthy these are up
and running how you make sure that you
use them the least and make the most
profit in your business right so if you
want to do all these things you got to
use this VLK stack now ElkY stack you
got to use it because it's log analysis
tool there are known for the tools in
the market for the same purpose Splunk
similar object navi and a lot more now
the thing is that all these other tools
they are not open source I mean a few
are open source and few are free but the
elkay stack is the easiest the best tool
that you can use which is your license
free for performing log analysis and you
know like I said log analysis is really
important for all the reasons I
mentioned earlier
and if you want to perform these log
analysis you have to use a combination
of these three tools you have to use an
asset search you have to use log stash
and you have to use Cabana now whatever
you really need to use elastic search
because elastic search is an Apache
Lusine based search engine okay it is
develop using Java and that is basically
your database which is going to hold or
your data okay
and whatever analysis or searching you
do you do it on elastic search and what
about locks - so locks - is that food
which is used for collecting and from
monitoring these logs from remote
machines because your logs can be
generated from anywhere and we you know
generated from MA I already devices
maybe from smart bulbs smartwatches
Smart TVs it could be generated from
servers to be generated you might have
server logs system logs whatever logs
right and you there are lot of ways your
DW get generated
now since elastic search is where that
data should be for us to perform
analytics and your searching how you get
the data to elastic search right so
locks ash falls right there that's where
it locks us comes with the picture lock
- does it for you it creates a pipeline
from where data from multiple sources
can be combined and put into one
particular database to create one
particular database into stored in one
particular index and the index in our
case is in the elastic search so elastic
search stores data in the form of
indexes and logstash
is responsible for getting data in from
multiple sources to that particular
index correct so at this point of time
you can also ask this question if lost
ash can do the data collection and the
philosopher CH can do all the analysis
then why do we need Kabana well the
reason we need Kabala is because Cabala
is the most integral part i mean how
good is your elastic search if you
cannot interact with that right how good
is your EA k stack I mean it can your
elastic search can be able to searching
it could be able to do analytics but the
fact is that it does not have a UI like
I said Capanna is the tool that provides
the front
user interface to your LT stack so if
you beat searching the data or p8
performing analytics be it creating
reports or creating a dashboard with
Alaura for the reports in place all
these things are done on cabana right so
you do the searching via Cubana and then
Cubana actually goes and searches
elastic search for that particular data
so at the end of the name
it's Cubana which does the entire trick
so it's highly important that you know
kibarim because Kabara if you don't know
how to work with gabbana then you cannot
work with the lk stack as supplies that
i think that was a good enough
explanation if any of you have doubts
please put them in have an answer okay
so seems like no doubt I'm gonna move on
then now how do they work together I
mean like I said Knox ash will do the
log collection or the reader collection
it can get data in from system logs from
log files from Kafka views which is
again a messaging platform a distributed
messaging platform you can get data and
from there you can get from rabbitmq and
various other sources okay
and what Locke says would do is it would
create a pipeline and send data through
that pipeline to elasticsearch and
elasticsearch would solve that data in
different indices and finally if you
want to search that data on a data point
of time or if you want to perform
analytics if you want to get insights
based on that data which is collected
then all that is done with the help of
kibana right that is what it performs
the queries on elasticsearch the one
thing you should notice that logstash
can get a data send the data as an
output to other sources also to a lot of
other destinations but in our Ilkka
stack it's gonna be elastic search which
is gonna be the receiver of the data
okay okay since I've spoken now so much
about the LT stack and about Shobana let
me go forward and so what is the rule of
Cabana and ELT right so that's what I'm
going to cover over here there are
mainly three things which I again told
you earlier but I'm just recapping it
over here the first and foremost thing
is that it enables us to do the
searching and interact with the data in
elasticsearch right that's the first and
foremost thing and it also allows us to
perform advanced and Alex and help us
create reports based on that data and
create a dashboard of those reports and
share it with other people you know it's
called it our dynamic Dashwood's get
shared with a lot of other people Pete
your team you can share it with all of
those people and then it also enables us
to create the sharing of dynamic
dashboard so I mean just like I
explained a couple of seconds back and
this dynamic dashboard will also get
updated in real time so data can be
coming in at real time from various
sources it can be coming in to will
asset search with the help of log stash
and whatever repose that you create on
the dashboards those will get updated
with the newest data coming in right so
you might create a data of statistics of
the last 24 hours
right so data will be coming in
continuously and the analytics or the
reports would be based on the last 24
hours
data correct so at such times you don't
have to go and refresh or create and
unipod every time with the help of
kabbalah those dashboards and those
reports can get updated in real time and
basically that's how a lot of people
will use Kabana
I'll be your colleagues or video
superiors you can just open up your
dashboard give them the link to where
your dashboards are and then they can
see all the reports on your dashboard
and we'll get the answers correct so
that's the three big benefits with
kamana so before I go forward I will
just let you know that Kibera is again
it's a web-based tool in fact even
lassic searches right so you mean you
interact with Cubana
over the web and it's always hosted on
port number five six zero one by default
you can change it but however by default
the port
Bertrand zone is five six zero one okay
I have a question here he says what
about elasticsearch the port number
where does that run the port number on
which elasticsearch runs is for number
nine to double zero Akash all right okay
great that's a good question I'm sorry I
didn't measure it earlier
so anyways moving forward I just want to
quickly finish up with my peepees by
showing this one slide of the fact that
a lot of popular companies are using
Kabbalah let's say I mean take LinkedIn
for example or Stack Overflow Netflix at
some short HipChat
a lot of these people are automatic elk
is that compared to its competitors I
mean of course because it's one is it's
costly and the second thing is that with
Kabbalah and with elasticsearch you can
do just about anything that you can do
with us plan or the others it may be a
little difficult it might involve people
who are technically who can get hands
dirty but the only other benefit that
other tools have is that they can be
used by human non-technical users that's
the only benefit but otherwise LG stack
is up there right it's up there in the
market and it's growing and trust me in
the future the market share is going to
be dominated by LG stack in the log
analysis to me okay so another laugh
told you so much about Iligan stack
camera get started with my hands on and
show you give you some action and
instead of just boring you with PPD's
okay so data visualization with Gabbana
that's a topic and first let me open up
the port numbers right so like I said
your elasticsearch is who still on port
number nine to double zero okay now I
can type in fourth about zero over here
but it might not work because I haven't
initiated that process correct
same thing with Capano so that is active
usually on port number five six zero one
that also would not work so the first
thing that you go to do is that you got
to start the service so how will you run
the service you can simply go and start
it
from the extracted file so it's Windows
which I am using and whenever you've
extracted it you will have these three
tools right so I have four V DS versions
I have versions five point six point
five 6.00 and I have six point one point
one you can use anything I feel the
version number five point six point five
is the most stable so I am going to show
you demonstration on this particular
version so my three tools are your
classic search version five point six
point five
Giavanna and locks - okay so let's start
the services for each of these three
tools so let me start with elastic
search first let me go in here go to bin
and run the batch file so that's all you
got to do to start your elastic search
service the rest would be taken care of
let it happen so the meanwhile we can go
to cabana and open up the bass file
again I mean it's gonna start our
service all right okay my last xr2
started and even Mikey Bona is connected
it's not connected as yet but let's see
okay so even like your bar adds up and
ready so let me just quickly go back to
my browser and open up these pores now
this shows that my elasticsearch is up
and running and let me refresh this and
there you go
even Mike Ibarra services started now I
have not yet started my log stash
service that's because I don't need to
start my love service right now only
when I want to upload a send it beta
using the pipeline's to my elasticsearch
only then I need to use log so what that
for if I get start the service so for
now these two are good enough so lemme
Carter view okay now that my Cubana and
soul is here let me just quickly run
through each of these tabs so venema for
the first time when you install la
Cubana you would be taken to this
particular tab the management tab okay
so you would get a window something like
this in the beginning okay since I have
already created these through indexes I
did not get this particular screen okay
but since the first time when you
started since you won't have any index
which you need to map it with you will
get this kind of a screen and whatever
the name of the index that you want to
give in to where data is being stored
that has to be given here so take an
example in this case there's a
Shakespeare which I've written here
right so there's one index which I had
created which has videos about
Shakespeare's work literature to write
all his Norman's so there's a data set
and that data set I uploaded through
locks trash and the next name is
Shakespeare and similarly if you want to
create an index for Shakespeare or any
other purpose then you just go to hit
the name here just enter the index name
and you'd all be good okay so that's
what this tab is all about for the first
time you'll be doing it over here okay
so once you have your data in once
you've configured log sash to get data
into your Cabana you can easily ask that
data by choosing which you know by
choosing which index you want okay so
you have to first go to the discover tab
over here and then from here you have a
drop-down you need to choose your index
and the relay is in that index would be
thrown out
so if I choose demo which was an index
which I created recently then initially
it says no results found now that's
because of the fact that there is a time
range in the last 15 minutes
there's been no data that's added to my
this particular demo index ok so let me
just change the time right here let's
just say today or I can even save last 5
years so in that case all the previous
data which was F that comes up right so
in this particular demo index field I
have a customers dot CSV file ok that's
what I imported in this particular index
and now similarly in this particular
logstash index I have some system logs
ok so let me just change the your time
line again to last 5 years and you can
see that I have a lot of system logs
since these Riina's so unstructured you
can understand that these are logs being
generated by at some server or at some
system correct and it's tough to analyze
this logs and that is where Cabala come
into the picture or in fact the a
haystack comes into the picture and
makes our job easier right it gives us a
better view it can understand logs
better because it's understands isn't
forward and the whole purpose of coil he
sacked is to understand logs and help us
understand those logs right so that's
how our job gets simpler now getting
back to the previous act I will show you
what was my total next and third index
has this Shakespeare details with regard
to all this work whatever he's done
right all its place whatever dialogues
we used to all the speaker's and all
these details you can see the various
fields in all these indexes so similar
to I mean I'm pretty sure you would have
gone through the previous elasticsearch
tutorial video which was again recorded
by Erica now the thing is whether it's
explained with respect to how you'll
asset search compass or database so I'm
not going to go into that details now ok
I'm going to assume that you know
everything from the elastic source point
of view and what I'm going to teach you
now is this is that how to create
reports how to create dashboards how to
use this key Barra UI
because first-timers might have a little
bit of a problem okay so let me get
started by first of all going to this
particular square root tab in fact we're
already at that discover tab okay like I
told you this is where you get a view of
all your logs now if you want to create
a graph if you are a create a report or
look anything like that then you've got
to go to this visualize tab over here
okay you got to say create a
visualization and you have various
options if you want to create a media
chart you can do that if you want to
create a heat map you can do that too or
if it's a horizontal bar or a line chart
a pie chart or a vertical bar all these
can be done besides this you have no
more options for visualizing you can
sort data or you can create a data table
view report you can go
you have gone only a metric
elasticsearch can read coordinates with
the help of this coordinate map function
there is region map there is time Leon
which of course is used for time series
analytics of nodes which come over the
period of time right Long's be generated
every single second every millisecond
there is log being generated now those
logs over a period of time is tough man
died so time Leon is there for that
purpose to help us understand those logs
we have which will build oh you have mom
down and tagcloud and you have a lot of
other options okay but since this is
introductory session I'm going to talk
about the basic charts over here and in
my upcoming sessions I will talk about
this very exciting coordinate map and
timely on features okay now these two
are very exciting I will do it later at
a later point of time
today's is going to be a basic
introduction I will create probably if
you want I can create a pie chart for
you or vertical bar or a horizontal bar
anything which you would prefer okay and
the next step that we have here is that
of dashboard tab once you've created a
number of flow reports with the help of
these graphs you can get all of them all
to this one go to your dashboard so I
will show you this die good later when
I've run of creating a few of these
reports okay and then there is your time
Leon so time Leon is another amazing
feature that has come up with a cabana
the ESG and it's going to be very useful
in the future for us to analyze data
over a bit of time okay and then you
have the next tab which is the
development tools
now over here if there are draw any
details you want to add to your
elasticsearch you can do it besides just
doing it from logs - okay so this is
your dev tools it's right there on your
left you can use it to basically send
your data to search your data if you
want to analyze something if you want to
retrieve all the reader that's present
in index you can do that you can search
for it you can also post your data you
can put your data to your last search
from here you can do bulk API so if you
don't want to use log stash then there's
another option of bulk upload that is
this okay post underscore bulk so with
this whatever logs you have that can be
easily input in to your elastic search
so that's what I have done so a bit of
my Shakespeare logs in my Shakespeare
read a set which was an adjacent format
that I send using this method of cost
and a score bulb so that is about the
development tools okay so let's get back
to just keep on off let me go to
discover and do you talk a little more
about the data set that I'm gonna show
you a demonstration on okay let me open
up my DVD and explain that so like I
said when you insert for the first time
you will get this window where you will
have to set your index pattern and you
will have to create that index okay and
then whatever the other you have you can
discover that data interact with that
data from this discover tab like I
showed you
so here if you see I've chosen the demo
index and over there your data can be in
that lock format in JSON format or you
can choose your fields over here there's
this option of selected fields right
from the available fields if you choose
any of these fields that would come in
here and those would be available as a
separate column for you to are read it
so this improves readability right so
that's one good advantage with the
discover tab of Cabana similarly what
I've done with the Shakespeare data set
is that from the available fields I
picked up or pretty much all of the
fields which is nothing but ID a score
type line ID line above play name
speaker speech number and text entry
right so I've created a table view and B
as humans for us it's easier to
understand it when it's this way correct
for systems it wouldn't make a
difference it would probably feel that
logs are better that's a better way of
understanding but for us readability is
important right so for us we will create
the reader we can you know use Cabana to
view data like this also in a table form
and you can also set it to you know for
a particular time range you can do that
too so let me just quickly show you that
if you go to discover tab
okay so I am in the disco with that yeah
so you can choose whether you want to
search for the last 15 minutes if you
want to search for this year or whenever
and then you would have all your logs
over here and you would you know get
details about when these logs came into
your machine right into your elastic
search so if you just drill down here
you can see that it gives you more
idiots right
you can just over over the screen like
this and you can drill down under the
particular second depth of which
particular date that data came in so as
you can see here January 8th 2018 1224
alright and then January 2018 12 money
for again but the difference is in
milliseconds so in BCE to 58
milliseconds all the entire dataset
which is nothing but the customer
database CSV that came in to my
elasticsearch right so you can easily
drill down to a particular time range
also so since I did all this you notice
that it's no more last five years or it
should not last money or any more the
time range automatically changes over
here so that's the thing which you are
not and ok Akash is telling me that show
me a tabular format of these logs of
this data set right ok Akash so like I
showed you in the PPD I will give you
this kind of a view over there also ok
so basically if you know and if you want
a tableau view if you want to get fees
if you want to make it more readable you
can just go under the available fields
is options and you can just keep adding
which other fields you want so customer
type is a failure
timestamp is one customer F name is
running but first name customer version
custody all these areas right so let's
this add customer city customer first
name customer ID and customer type so
when we add all these when we just click
on add over here it gets added to
selected fields and whatever is there
under these selected fields option that
gets displayed in the tabular format for
us right so you have the time videos
over you're asked to when which deed
this particular document then whatever
value is stored in that particular field
like in this case this particular person
called Arlene is from New Orleans
the ID is 200 and the type of customer
this person is is that it's loyal so we
have various options you have loriel new
VIP again there is loyal there is a VVIP
and a couple of mode right so later on I
will show you how to perform analytics
or how to create reports based on this
data alright so in the meanwhile let me
also go to another
next which is nothing but the logstash
index over here and then let me show you
how to create a table rapport over here
also so okay
discover after in the log stash index
and are now correct we have these many
fields over here so these are actual
server logs system rows which was
generated back in 2015 and these were
the Venus fields that were tabulated so
to understand these logs better I can
start adding fields so let's say I add
boost I add IP I can add agent I can add
machine OS I can add response and yeah I
can pretty much higher anything else
right but this is good enough for me so
with this what it shows is that whatever
was in the form of events right now
those were called events the host for
this particular event is this okay the
IP address where this an I even got
generated is this - 10.1 5.11 3.62 and
these earlier is nothing what the
browser which was probably used when
this particular web page was accessed
right whatever was hosted on that server
whenever it was accessed Hiro's access
using Mozilla and similarly the machine
which the client used was Windows 7 it
was a Windows 7 machine which access
this and the response the personal
client or the particular user got was
200 now a lot of you people might not
understand this easily but forget this
if there is a response of 200 means that
whatever request was made by this end
user or by this client to the server
that was successful the server got back
with a success response of 200 which
means that whatever was requested was
serviced but however if you have some
other response or states like four not
four or five hundred then it means that
there was probably an error something to
not happen right okay so those are the
details that I have into this particular
index of mine and I can switch to my
next index which is nothing but the
Shakespeare index and over here again I
can just say discover this refresh all
those fields and I have all these
available fields over here from these
fields for you
think I might want a tab review of let's
say I want the value of score I want to
add type line number cleaning who was
the speaker who where the speakers in
that participate all these idioms by
just picking it it comes selected fields
and when it's here it gets displayed
over here correct so you can see that
the play which Shakespeare wrote who is
that of Henry Ford right and there are a
number of speakers what you see on the
screen here are just a few of them so
the first set of events have King Henry
being the speaker and the play name is
also Henry four so similarly if you keep
scrolling down you'll see that you have
all your events being displayed and of
course different event will have
different values right so here the
speaker is different its Falstaff and
you are it's Prince Henry not King Henry
anymore
that is Falstaff again and then you have
a lot of other rod areas which will come
in through your log stash right out
little three o'clock so all that would
be like or aligned Bureau here now
besides these tabs it'll besides these
next options I did not mention one more
thing there is this brilliant add a
special option over here in La Cabana
which can be used to filter out or
eliminate data so let me just click on
add filter here and show you what I will
talk about so let's say that I have
different speakers you know there is a
right now that's King Henry Ford right
and there is Westmoreland let's say I
would have filter only King Henry I want
to see only his events or his place then
what I can do is I can choose the
particular field which is nothing but a
speaker I can you choose it there go Cho
speaker or keyboard and I can say is not
and then okay I can simply say is if I
want to see every King Henry's I can
choose the condition as is and then the
value as King Henry four so when I hit
save then only those details with
respect to this particular speaker would
be displayed okay no more Westmoreland
no more Falstaff and all right holy King
Henry force materials are coming up and
you will also know the number of result
returned to you Oh
so the word hit is the keyword that gets
associated so to Turin and hits means to
dine and results then return now if it's
not King Henry then you have one line
seven two hits in total thousand nine
hundred and seventy two hits all right
so undocking Henry appears that many
number of times if you want to filter
out and view only this particular
speakers list then you can do that too
you can say again add filter choose that
particular field name say is and
Westmoreland and you get only his
details now so you can also modify it in
such a way that you get the night of
things suppose you want all the results
except for those of for Westmoreland if
you know that to edit it and the value
instead of saying is you can make it is
not best more land and when you hit on
save then all the triple-a number of
hits difference there right so all the
results which do not have a small and a
speaker those would be read on you so
that's how easy it is to search data
here and to interact with your data
right so this was just one of my indexes
so similarly if I go to the log stash
index I have my system log here
correct which I was just showing you
telling you earlier so let me just add a
few of those fields onto the dashboard
so let's say I want to add agent client
IP I want to add machine operating
system okay I don't want requests let's
say I want to add response I can add
that and yeah I am good enough so
initially it was in the form of events
and now it's broken down with separate
fields all right oh here you can see
that Mozilla is the agent but there will
be times when Mozilla on the agent so
guys all remember now there are sixty
six hits in total now if I want to
search for Mozilla for want events which
has you know only the agent has been
only was enough then I can simply type a
cure I just type and roses up then all
the events which has the word Mozilla is
returned correct
but that is the power of Kabbalah okay
so now that I have shown you how things
work how do they look so I think I can
go and create a small report or
particular visualization for you right
so this is the demo field discover so
this is my entire event and here I'm
gonna then that's here for a tabular
view I want first name last name and
customer type so I have customer type
field lab the customer city let's say
add customer first name also then the
customer ID now I've shown you how to
interact with the data right and make it
more readable for you now to get started
with visualization and creating reports
and graphs you can quickly just go to
the visualize tab over here click on
visualize and you can choose any graph
you want either pie chart or in your
chart or what at the bar or anything
okay so I am going to click on pie chart
and here we can enter which index I want
to get my data in from now if I want to
perform on my demo index so I can choose
demo here and over here I have to add
media so right now I'm not choosing
anything so my entire pie chart is
filled okay now let me filter out it to
the report I want to so in here let's
say the aggregation I wanna do is count
I want a count of the different customer
types that were there right so that's
what I was showing you earlier also yeah
so like I was telling you have we use
customer types right you have for some
customers are loyal some are new some of
VIP if you want the count of Harmony
each of these fields are then you can
get that and visualize that with an
thathe pie chart so let me just quickly
visualize that for you go ahead I'm
going to choose aggregation as
count and then I'm gonna say aggregate
as terms you have no further operations
for aggregation you have for data so
grandeur of histogram arrange these are
however not going to apply at this
profile because I just want to count the
number of fields in my entire index
I'm gonna choose the word thumbs okay
and ready to storms you want to choose
which field that you're talking about
then the one that I've talked about is
that of customer type right so customer
type is here so I'm going to choose this
and metric of course would be count
let's say I want the results in
descending order and I want results of
the top ten okay even though it might
not happen sighs you're basically fixes
the number of results that it would
aggregate so that's what this is and by
just clicking on this after I finish
here you're basically a spy in chart
changes right the view changes so now if
you see your view is better so winds of
having to count over there on your own
you can simply do this you can come here
and see that okay the number of loyal
customers that I have are forty all
right it's forty number and if I go here
the number of new customers and I have
they are 35 number my VIP are sixteen a
number my B VIPs are nine and number and
then I have one other different type
actually this may be a wrong field but
still this word is there in our data set
so this is how you can pull up the basic
pie chart and similarly if you want to
pull up probably some other index you
can do that too right but before that
when you exit this if you don't want to
lose this you can save it so you can
save it by getting on this button here
save you can hit or you can do the name
of the visualization you can say
customer types and you can say it's a
pie chart correct don't save so by now
this particular chart has been saved to
your visualize tab so if you go here
this is new newly created now if you
want to create one more bar chart for a
different index let's take Shakespeare
the next this time then we can do that
too let's say I want to visualize with
the help of a horizontal bar and
Shakespeare is index okay and from here
let me again say that I want the
aggregation to be based on the count so
let's understand a dataset first of all
let's go to the Shakespearean next and
if you choose a relevant of fields
there's plenum there is speaker
and score okay now more you can
understand from here is that you have a
lot of speakers who have been a part of
a lot of plays right so the same play
can have more than one speaker as you
can see from here the name of the play
is Henry four and King Henry was one of
the speakers
Westmoreland other own speakers and
similarly a lot of other speakers are
there so if you want or details with
respect to how many times this
particular speaker spoke in that place
then you can do it because in the logs
this particular speakers dialog is
present correct if you remember let me
just show to you so this is the speaker
this is the play name and this is what
this particular speaker spoke so if you
want to count of how many times each of
these speakers spoke in the play you can
do that and I actually want to visualize
that with the help of a bar chart so
let's say the aggregation again would be
a count aggregation I can add a label I
mean let's say that count of speakers
right and over here I can say the y-axis
I don't need a y-axis I already have one
so this time I can choose the x-axis the
aggregation this time would again be
with the help of terms right it's not
going to be data so Graham or histogram
or range or a TV for range right so
these are a little more use case
specific I can use a date histogram if I
have logs which is going to be disputed
over a period of time since I just want
to calculate the number of speakers that
I have in the play I can choose terms
and from the field I can again choose
the speaker right so I would get a count
of the speakers over here I would have
difference because your and their count
is what I would get them off a bar chart
correct and let's say I want to get a
result of for 10 different speakers who
spoke with that play and I wanted in
descending order let's say let me even
ask
speakers okay perfect let me hit create
so as you can see
Henry is the one who has the most number
of dialogues he spoken the most over of
times and the count is 372 lines correct
and similarly first off with 366 lines
Hotspur with 352 King Henry with 239
points with 88 Glenn power with 82 and
lot more lot just like that right so
this is how you pull up a basic repose
this is how easy it is to create reports
with the help of these graphs and charts
on kibana
correct it's as good as any of the
visualization tool so let me go to save
this and let me give it a name speakers
let's say count of speakers in place
okay and I can click on save now the
good thing with a stack is that even at
a later point of time if you've updated
this right you're going to create a new
one so you can just come to a count of
four speaker in place you can make your
modification over here with respect to
x-axis or y-axis you can say this give
me five you can generate the new report
and at this point of time however it's
not saved okay but if you click on save
then you can save it and overwrite what
the previous report which was created
okay so let it was 10 but now since I
change the value to 5 this has been
updated and the new report has been
saved and since the visualization is
ready I can add it to the dashboard at a
later point of time all right
so unless not lie click on save it will
not be saved but the moment I do it will
be so that's one of the options and as
you can see if I just keep on visualize
now I have these two visualizations
right counter of course because the
plays and customer types
so this was one Python I created and
this was a pass at which I created just
off so let's go back to this code tab
and see what else we have we have the
logstash index right so this is where we
have the server logs now let me do a
quick visualization of something that's
there over here so when I say discover
this is the entire event and let me
filter out certain fields okay let's say
I just want host
and I want to client IP okay now you can
see that all these events have come in
in a very fraction of for a second
difference right so some have come in at
23
some have come in on May 19 some on May
18th someone May 20th
but yeah feet or whatever they've all
come in a very short span of time now
what we can do is we can basically
create a date histogram I can trigger a
chart so you saw the option of data so
ROM and just Instagram right so I can
use that to show you how we can analyze
these logs by the time they came in so
for that let's again put a visualize and
create a new one
we're here on plus a line chart and let
me choose blocks at this time and in y
axis I have count okay and this time in
the x-axis bucket type let's choose the
aggregation to be date histogram and
when you do that it automatically picks
up the timestamp as one of the fields
right so based on the time stamp field
it would return me all those events so
let me just hit save
okay
yeah so since I've chosen date histogram
and the timestamp we have this dot over
here can you see this so there's nothing
wrong with our report exist that it's a
little too magnified so let's click
again and make it better for us so as
you can see by clicking on that dot
we've zoomed into a different time zone
it was earlier our last five years and
now it's come down to the particular
dates when these logs came in it was
between the 18th and the 19th so that's
how easy it is to basically analyze your
data look at your data from different
views understand your data in a better
fashion and work with the easy stack
right so this again I can just click on
save and I can say log history right I
can save this and even this
visualization which is in the form of
histogram is present with me now how
good is all this if I don't really use
it right
well another idea showed you the
visualization aspect let me just go back
to the discover tab and show you one
more thing so I can also filter data
from right here and that filter data I
can create a saved search so saved
search right so that's a brilliant thing
so let's say from here I want to filter
out particular some some kind of data so
let's include response to this field and
right now I have all kind of responses I
have 200 I have 4.4 response and I also
have a final three response correct so I
have three different kind of responses
of course I need 200 is a successful
response so what I can do is I can add a
filter I can say filter out the data
where response is and I can say 200 over
here and I can even give a name to this
particular filter I can say successful
response okay my bad have forgotten my
spelling
okay perfect spelling great so I can
just give it a name then I can save this
so right now you will not find a four
not four or five not 3 anymore in
responses because I have filtered out
this particular search to only those
events which have a successful response
correct now even this I can save it I
can say say when I can give it a name I
can say successful response is :
200 okay and I have basically a pinned 1
so next time I want to open some search
supposing there's a kind of search which
I do want to daily day to day basis my
admin does on a day to day basis so
instead of having to type and a filter
out 200 every time you can simply click
on open and now click on what he wants
if it's response and failure which he
wants or if it's speaker filter or if
it's successful responses whatever by
taking on successful responses you get
the reader of only the successful
responses and similarly if you click on
any other saved search like speak a
filter then in this case it says speaker
the keyword it does not have King Henry
right so that is a rule so here in this
case whenever King Henry was a speaker
those events have been filtered out so
you will find all the others because
your XF okay Henry
that's one thing and I can also open
another raw filter which is responding
failure they're only Chrome has been
used as an agent right so these are the
different filters that we can do and we
can save we can have a safe services and
all these things right so these are the
so much of opportunities that we have
with the discovered and the visualize
tab so now comes the dashboard aspect
which I was talking about which you can
share with a lot of people which would
get updated in real time and which you
know once you do it it's all set so that
could have not what you have with diodes
so let's create a dashboard by clicking
on Add button we can create whatever we
want right so I've already created a few
of these visualizations right I created
a few of these reports just in front of
you so let me add these visualizations
so my dashboard so count off four
speakers and plays count of types pie
chart and see
moment I do it it comes up over here so
this is how my dad word will look like
that similarly log history is another
reportage I created so once I add that
that also comes here and besides that
even the saved search of whatever you
are same right that can also come up so
let's say I want filtered answers to
speakers I can choose that and that
would come up here and successful
responses is another thing that I have
chosen so when I chose do all these
things all would come up in my dashboard
now what I can do is I can quickly save
this correct so if I click on save I can
give it a name a name in this diet but
now let's say this is my dashboard and
by just clicking on save your dashboard
is ready so this is how this dagger will
look like so at any point of time in a
hurry if you want to see your basic your
important metrics which you see on a
day-to-day basis that can be accessed
from this dashboard my dashboard option
correct so everything at one go at one
instance so your business decisions can
be made quickly if there's any emergency
if then they drop in the graph you get
know immediately if there's an unusual
spike anywhere that you will be caught
you'll visibly see all those things
right so here at of course says no
results found
so Akash had this / - no comment saying
no results were found for pie charts oh
well the reason for that is the time
range option so if I this make it last
five years the pie chart is back right
yeah so that's how things work so when I
mean when I change the time range over
there this of course went back to a dot
so as to change the time range for it to
be visible so that's how things work
with timanÃ¡ all right and that's how fun
it is to use coupon arrays and if you
want to like share with other people you
can do it by clicking on this button and
then you have these links over here if
you want to embed this dashboard
somewhere then you have an option of
embedded iframe
right you can embed your dashboard for
some particular webpage of yours and it
will be visible there but otherwise if
you want to give your team access to
this particular dashboard you can just
copy this and you know you can let them
use it
the only difference would be that in
this case it's localhost but otherwise
once you work on it on the server it
would be the IP address of the server :
the port number right so I'll show you
how that looks like so from incognito I
hit that same URL I will get to see the
same dashboard which I created so that's
how I can add people collaborate with me
and get to see what I've created correct
so that is about Kabbalah guys that's
about real K so I hope you guys had fun
correct so that pretty much is an end to
my session here I am Leon is one topic
which I will be talking about in my next
class alright so stay do to that and can
that let me just go back to my slides
and now see is there anything that's
left so I explained how to create a data
date histogram visualization and I
showed you how to create a bar chart and
also a pie chart so yeah that's pretty
much an end to this particular session
guys if there are any doubts please put
them on the chat box I'll answer them
right now
okay so seems like there are no doubts
Thank You Akash for bringing the session
thank you Mahesh hope you guys have a
good night and I'll catch you people in
the next session all right thank you I
hope you enjoyed listening to this video
please be kind enough to like it and you
can comment any of your doubts and
queries and we will reply to them at the
earliest do look out for more videos on
our playlist and subscribe to our Erica
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>