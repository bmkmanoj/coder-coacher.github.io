<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hive Architecture - A Deep Dive with Real Life Examples | Hive Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hive Architecture - A Deep Dive with Real Life Examples | Hive Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hive Architecture - A Deep Dive with Real Life Examples | Hive Tutorial | Edureka</b></h2><h5 class="post__date">2016-03-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AcpGl0TQIRM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's talk about hive architecture so
indeed we didn't have a specialized
architecture because big was most of a
tool but high although it is again a
tool it is an Indian but on top of that
we also talk about having an
architectural amount of what high needs
so I'll give you a quick run up in Owen
function resin over dig deep into the
top I'm not very good at drawing but I
hope you'll understand why I'm trying to
portray here so imagine that somebody is
looking from top okay somebody is
looking from the top so if you look from
the top and this is a 3d three
dimensional plane somebody is looking
from top and you have a 3d thing does it
make sense guys if you look from top it
looks like a 3d plane for you can we can
you visualize this thing ahead yes some
itself yes all right
let's this I have somebody who
understands my going okay now imagine
this is HDFS okay this is HDFS this is
where your data is written so I'm trying
to help you or not good before that it's
not how HDFS looks like though but just
imagine that alright so this is how your
data in HDFS is a blank sheet of paper
that's how you see if the FSA d'argent
now there now my question is if I put a
grid here if I have a great deal and the
grid is a table grid it has a table grid
so if I if I if I put a table grid in in
between the actual data in my eyes and
if I look through the grid on the HDFS
data you know do you think if the grid
is a table of structure how feel as if
the data in HDFS is the tubular
structure do you feel like that guys do
you understand what I am trying to say
here yes so this is the exact same
concept of this is the exact same
concept of height in height what you
have is your data still resides on HDFS
but what you create is you create click
the grid that you create is responsible
for viewing the data which is actually
stored in a black line in HDFS normal
texture
style it can be verbal and it can be you
know approached as a tabular structure
so what you need is you need a great
unit to define agreed on using directory
you can see different folders and inside
the folders the data that you see using
or through that great will almost look
like a tabular structure for you all
right
now this great information the grid will
happen mainly the grid we talked about
how many you know partitions are there
in the grid it will talk about what kind
of data can you see through the date is
that we engage I as a text day now what
kind of data all those data needs to be
saved somewhere and that is the reason
why you need a special kind of server in
hive and that's called a make a date
ourselves a metadata server is a small
database normally it it's created using
sequin or it is creatives in Postgres so
what you do with this is you store only
the metadata of the grid so that when
anybody wants to view the table what
happens if you just push or bring the
grid details on this and view the data
from HDFS using up through that rig and
that's how is all about all right so
it's a very simple concept the idea is
very suitable your data actually resides
on HDFS the data resides in HDFS and
when you try to process the data one you
want to do analytics on the data then
you'd have analysis should be done using
your research knowledge Gyan
this is the original concept of your
data storage and data processing nothing
changes what changes is you have
something called high on top of that
hive is a service the service what it
does is it only creates and maintains
different rates and the information
about the grids like what is the table
name what is a grid name which is
actually the table name what how many
columns are there what is the data type
of which column all those details are
stored into your metadata that's what
you make of it is all about and there is
something called a driver the driver
what it does it whenever you run a
sequel query whenever you create a
sequel query that sequel query gets
converted into MapReduce and pushed on
into your resource manager by the driver
and that is how you create a height
service so hive server settings door
narrator nor it actually passes Rita
what it does it only stores the grid
information and on top of that you also
have a driver view where whenever you
submit any single queries it gets
converted into normal MapReduce program
and it is pushed in or
not really is pushed into your resource
manager for processing like it always
have been
all right now the question is how do you
process very now what is how do you go
and access those drivers how do you
somewhat execute query well sequel query
can be submitted using three different
ways one is CLI so how you provides you
with the CLI it's more like you know pig
cruncher you have a CLI command line
interface and that's what hi winter face
so how you interface can be used for
opening up a command prompt and running
up queries there is a high web interface
you can open up a web interface and run
some small queries into the web
interface or you have a thrifty surface
it through free service is nothing but a
you know cumulation of a different kind
of service which is normally called as
northrup service that gives you the
flexibility to work with JDBC ODBC
connections now JDBC an ODBC our
universal connector so any any database
in the world which or any any service in
the world which actually you know
announced itself is a database either it
can be data warehouse date or no already
database any any service that announces
itself as a data storage system has to
provide the basic ways of connection so
JDBC would be good if you see a
universal connections available for
connection to any database so because
high winds you know or highways
approaching itself but you're clearing
itself on announcing itself as you know
data warehouse you need to provide those
services so that anybody can actually
access highways in getting zero emission
and that's what your thrift shop is all
about
all right so through service provides a
layer and your JDBC ODBC can be order a
PC ODBC connections can be established
from your external world now what if the
external world available all kind of BI
tools you people chemistry at all these
are you know examples of simple BI tools
so it can this can go all and on so you
can have any kind of external system
which can work with JDBC over these
connections they can definitely come
across and access your hype server using
three circles so if you is internally
working on top of JDBC an ODBC so that's
what the internal architecture of hive
is all about it doesn't store the data
the actual data is still in HDFS it will
suit process the data as MapReduce
program it's just that your sequel query
will be
pushed through the driver which gets
converted into not reduce program and
you on the other side in a meta store
the meta store is actually giving you a
way of representing of showing the grid
all right some it says CLI and it's
everywhere so CLI as I said the
command-line interface
HW is web interface right so this huge
web interface using hue you can see a
small street and I am using that screen
you can go and round some five queries
right all right so suffix is a hydrator
in a storage made a hive metadata store
in Modesto it has to leave the separate
data UCS as I said normally you
represent a meta store using a you know
four squares or a my sequel or an Oracle
database so when you create and set up
your hive Ethan you have to set up small
database and that database is actually
the meta store it stores only your
magnetic ocean when a request is made
from you would it go through is WI or go
to thrift service so if it is going
through you it will both wakes every
white right setting both raced every
right not certificate and that's how you
need to be on a different server yes you
can have sir - a different server so you
can install hive on a different server
so all of these are actually you know
you know you can have them on a separate
analytical service right machine so you
can have separate machines so you can
have one dedicated machine or couple of
dedicated machines which acts as a hive
and pig neurotics machine right but
those machines even though you run your
sequel queries or you run your pips
tricks through that machine invariably
those all queries and scripts through
the driver get cemented into the
resource managers so we know how those
architectures right how no matter where
you start making a job from it
eventually goes and in you know gets
gets emitted into your resource market
so she says how many metadata stored and
accessed from the height MapReduce
program so MapReduce program knows the
database passwords and information so if
you internally create a not progress
program when it creates the MapReduce
program all the data's databases data
structure and everything will be
maintained in the Metis - you pull the
data and you create the metadata or you
create those MapReduce programs so it is
very simple to understand right that if
I have a MapReduce program the MapReduce
will have to have you know that I need
to understand you know how the data out
per line of information is getting
stored so when I when I
go and you know check out the type of
the data that I have in my Mehta store I
can usually find out that yes my data
has the entire line is divided into
twelve
you know let's say columns and follow
one is a in a string data type column
two is a text data or a boolean data
type follow-throughs something else
so all those would be taken care of when
you go and fetch this inter metadata for
next row so whenever the engine creates
the MapReduce program the MapReduce
program will have to go connect the
rattan processor data it will become
easier if you already know what kind of
information or what end of data Apolo
how many pounds or what kind of columns
are stored in an opportunity like
shopping says why how do when we
separate my sequel database for height
can it not stored HDFS no so you do not
want to store the metadata in our it's
the FS right the meta data in HDFS will
be a very bad design that you already
know about it we have talked about named
load and I have seen that even in name
note the meta data gets stored in the
RAM here you want to persist that one of
course is that because you want query
those tables as as to T as you want so
you want to have a very small data base
that only stores a metadata right
similar concept for what a name node has
if you're storing metadata how is fault
power is taken care of
are you get the question lovingly I know
so why how is fault tolerance not taking
care of here you know because when you
submit a program the program gets
converted into MapReduce and we have had
three sessions discussing about how on
tolerance is maintained in a low
resource manager when whenever you
submit a job right so we know about it
orbitty if I understand it correctly
does have service typically get
installed or run on a master or any
other servers dedicated for hypochondria
no no you've got install high on any
separate box you know so there is no
restriction you can install it on any
more box you can install it on a you
know 108 a node you can install that on
our I know on a separate box you can
have an edge node dedicated only for
friggin height because that will act as
a service layer for your data analytics
right so you do not have to install this
I mean there is no restriction like
where we to install it but you can
install it anywhere you feel like you
can install it on a separate
only the restriction is you need to have
the data or down the road manuals in
same box apart from that height can be
installed and any boxes you want you
know because eventually whenever you get
in whenever you submit a job it gets
some way leading to your resource
manager right like it has always been in
the ANA difference between CLI and HWI
well CLI and HWI had different because
one is a command-line interface one is a
web interface is just giving you options
to connect to a database no it's this
giving options to panic to hide right so
there are different ways of connecting
at wearing height these are two weights
which component here in particular
converse high to matter to stroke up the
driver ever convert into markets program
become tropic tables right in height so
we can't drop it but let's not jump the
gun so why Cannot we drop right so we
can easily drop triples all right so
what we talked about is a hive normally
have these components is the shell you
have a CLI shell so CNC action is the
actual in a place where we're going to
go and buy some polls and execute our
narrative program and processes meta
story has told stores all your grid
based information execution engine the
execution engine is actually executing
your MapReduce program the compiler the
compiler which converts your you know
sequel script into a java-based
MapReduce program and compile it and the
driver driver is actually the one which
is converting a sequel into your actual
not reduce our program so these are all
components these are all conference
which can be used for processing design
hi how the large video files been
handling by the trail firstly you know
normally you do not process video data
using you know hi you cannot do it and
second thing is guys you know second
thing is you know if you think logically
you know the data in HDFS as I have been
saying it's already divided into blocks
and that's what I've been saying for
last so many classes the data is thrown
into blocks so even if you are seeing
the data through that grid you are not
seeing a single unified view of the data
the data has actually been shown to you
from different different sources right
so your data in video data is also in
HDFS store into different blocks of data
of 120mph
right so your data is actually scattered
into
multiple knowns across of HDFS right so
similar similar to here guys you know
even if you have a big file even even if
you have a big file and in restoring
this huge file into your let's see Drive
you know you can still go and check out
the file right internally the data might
not be located in a single place in your
heart right but if you go into a chec
right your file system is going to show
you unified view of the data it's going
to show you a small you know file icon
and say hey this is your fight that size
of the file can internally be known in
GBS but it's still going to show you the
iconic fight similarly in HDFS you know
no matter internally how many blocks are
divided and how the data is scattered
when you go into an HDFS layer and you
query your HDFS it's going to show you
that okay this is your single fight
so guys you know this is how we have
been talking about since class number
two or three right so you will go into
your HDFS and you say how do you access
- LS is going to reach down the content
of this it's going to list out all the
files in the folder it doesn't tell you
that ok this file is happenin to this
many blocks and this is loading
different machines right that's
internally how file system works right
so you only go and check out the data
and when you go and do an LS on the data
on your filesystem consisten is only
going to show you the content of the
file all of the folder irrespective of
how many number of blocks the data is
divided into and how that it blocks the
scattered right so we know that part
right shashi we already have covered so
many you know classes discussing about
the same thing over and over again right
that data so for us the file system will
internally manage the return for us no
matter how big or how small the Filene's
i'm going to see only the file name
given on that photo so if I'm looking
through even a you know grid doesn't
matter to me because I'm who you see in
that ok there is a file like this so can
you briefly tell what happens in driver
compiler and execution with it one more
time all right guys all right so first
first one is you know the meta store so
the major story is one that stores your
grading function right so the big
information talks about you know all the
grid level or a table level details the
name of the table you know which
location your team
so if you are saying that my table is in
you know drive or my table is in folder
slash and on data 0 1 then the table
name and the location of your data
should be managed and maintained
somewhere second part is execution
engine the execution engine is
responsible for executing a MapReduce so
the execution engine internally is
nothing but your yarn architecture so
yarn provides you with the execution
engine so why we are saying hive as a
separate execution because in hardened
works there is a new tradition engine
that's come into picture that's all
faiths this is a simple very similar
execution engine which is similar to
your yarn architecture so let's not go
into detail so execution engine for
normal discussions will always be on
compiler is the one that converts your
actual sequel program or actually non
Java program that has been converted
into MapReduce and it will compute
executable if you compile a program edge
we have seen that Java program which
should compile in converting the jar
that's what the compiler will compile
your Java program so the question is who
creates the Java program the Java
program will be created by the tracker
alright so these are the things that
considers your high confidence Lucas
says in the information which is which
is a name node stored into the name
which is in the meta struts loaded into
the name node absolutely not who's who
loads and maintains material in the
Modesto so it's normally when you create
those tables when you go across and
create a table right so you define the
table structure similar to what we do
for pick even we'll pick we create a
command saying the hey this is the
object storage and in your fixed organs
that is how your data will look like so
when you create those tables the tables
will be stored the in function of the
minaret of the table roll into your next
row so let's go into those two I know of
pointer skies when we will start doing
something will understand how it works
right do we need to do any kind of setup
for my sequel in modesto before using hi
so if you are into administrative work
and if you are setting up high then yes
you need to set up my sequel and connect
to sequel into your actual hype surface
if you're talking from a developer's
point of view know you know hybrid we
already it's set up and given to you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>