<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Recurrent Neural Networks (RNN) | RNN LSTM | Deep Learning Tutorial | Tensorflow Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Recurrent Neural Networks (RNN) | RNN LSTM | Deep Learning Tutorial | Tensorflow Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Recurrent Neural Networks (RNN) | RNN LSTM | Deep Learning Tutorial | Tensorflow Tutorial | Edureka</b></h2><h5 class="post__date">2017-08-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/y7qrilE-Zlc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is order from area
Rekha and today we will be focusing on a
recurrent neural networks so without any
further ado let us move forward and have
a look at the agenda for today so let's
begin by understanding why can't we use
feed-forward networks for various
problems we focus on various issues with
feed-forward networks after that will
understand how recurrent neural network
solve those issues and will understand
how exactly it works then we'll focus on
various issues with recurrent neural
networks namely vanishing and exploring
gradient after that will understand how
lsdm that is long short-term memory unit
solves this problem
and finally we will be looking at an
lsdm use case so guys let's begin we'll
understand why can't we use feed-forward
networks now let us take an example of a
feed-forward Network that is used for
image classification so we have trained
this particular Network for classifying
various images of animals now if you see
it is an image of a dog it will identify
that image and will provide a relevant
label to that particular image similarly
if you feed in an image of an elephant
it will provide a relevant label to that
particular image as well now if you
notice the new output that we have got
that is classifying an elephant has no
relation with the previous output that
is of a dog or you can say that the
output at time T is independent of
output at time t minus 1 as you can see
that there is no relation between the
new output and the previous output so we
can say that it feed-forward networks
outputs are independent to each other
now there are few scenarios where we
actually need the previous output to get
the new output let us discuss one such
scenario now what happens when you read
a book you will understand that book
only on the understanding of your
previous words alright so if I use a
feed-forward Network and try to predict
the next word in a sentence I can't do
that why can't I do that because my
output will actually depend on the
previous outputs but in the feed-forward
Network my new output is independent of
the previous output status output at t
plus 1 has no relation with output at t
minus 2 t minus 1 and at t so basically
we cannot use feed-forward networks for
predicting the next word in a sentence
similarly you can think of many other
examples where we need the previous
output some information from the
previous output so as to infer the new
output this is just one small example
there are many other examples that you
can think of
so we'll move forward and understand how
we can solve this particular problem so
over here what we have done we have
input at t minus 1 will feed it to our
network then we'll get the output at t
minus 1 then at the next time stamp that
is at time T we have input at time T
that will be given to a network along
with the information from the previous
time step that is T minus 1 and that
will help us to get the output at T
similarly as output for T plus 1 we have
two inputs one is a new input that we
give another is the information coming
from the previous timestamps that is T
in order to get the output at time T
plus 1 similarly it can go on so over
here I has just written a generalized
way to represent it there is a loop
where the information from the previous
timestamp is flowing and this is how we
can solve this particular challenge now
let us understand what exactly are
recurrent neural networks so for
understanding recurrent neural network I
take an analogic suppose your gym
trainer has made a schedule for you the
exercises are repeated after every third
day now this is the order of your
exercises firstly you'll be doing
shoulder secondly you'll be doing biceps
third day you'll be doing cardio and all
these exercises are repeated in a proper
order now what happens when we use a
feed-forward network for predicting the
exercise today so we'll provide in the
input such as day of the week month of
the year and health status all right and
we need to train our model or l network
on the exercises that we have done in
the past after that there'll be a
complex voting procedure involved that
will predict the exercise for us and
that procedure won't be that accurate
so whatever output we'll get would be as
accurate as we want it to be now what if
I change my inputs and I make my inputs
ask what exercise I have done yesterday
so if I have done shoulder then
definitely today I will be doing biceps
similarly if I have done by since
yesterday today I will be doing cardio
similarly if I've done cardio yesterday
today I will be doing shoulder now there
can be one scenario where you are unable
to go to gym for one day due to some
personal reasons you could not go to the
gym now what will happen at that time
the go done time stamp back and will
feed in what exercise that happened day
before yesterday so if the exercise that
happened day before yesterday was
shoulder then yesterday there were
biceps exercises all right similarly
biceps happened day before yesterday
then yesterday would have been cardio
exercises
similarly scar day would have happened
day before yesterday yesterday would
have been shoulder exercises all right
and this prediction the prediction for
the exercise that happened yesterday
will be fed back to our network and
these predictions will be used as inputs
in order to predict what exercise will
happen today similarly if you have
missed your gym say for two days three
days or one week so you need to roll
back you need to go to the last day when
you went to the gym you need to figure
out what exercise you did on that day
feed that as an input and then only
you'll be getting the relevant output as
to what exercise will happen today now
what I'll do I'll convert these things
into a vector now what is a vector
vector is nothing but a list of numbers
all right so this is a new information
guys along with the information from the
prediction at the previous time step so
we need both of these in order to get
the prediction at time T imagine if I
have done shoulder exercises yesterday
so this will be 1 this will be 0 this
will be 0 now the prediction that will
happen will be biceps exercise because
if I've done solo yesterday today and be
biceps so my output will be 0 1 and 0
and this is our vectors work guys
so I hope you have understood this guys
now this is our dealer network looks
like guys we have view information along
with the information from the previous
timestamp the output that we have got in
the previous time step and use certain
information from that will feed into our
network as inputs and then that will
help us to get the new output similarly
this new output that we have got will
take some information from that feed in
as an input to our network along with
the new information to get the new
prediction and this process keeps on
repeating now let me show you the math
behind the recurrent neural networks so
this is the structure of a recurrent
neural network guys let me explain you
what happens here now consider a time T
equals to 0 we have input X naught and
we need to figure out what is X naught
so call it to this equation H of 0 is
equal to W I weight matrix multiplied by
our input X of 0 plus W R into H of 0
minus 1 which is H of minus 1 and time
can never be negative so we this
particular equation cannot be applied
here plus a bias so W I into X of 0 plus
B H passes through a function G of H to
get H of 0 over here after that I want
to calculate Y naught so
why not I will multiply ho0 with the
weight matrix WI and I'll add a bias to
it and pass it through a function G of Y
to get why not now the next time stamp
that is at time equals to 1 things
become a bit tricky now let me explain
you what happens there so at time t
equals to 1 I have input X 1 I need to
figure out what is H 1 so for that I'll
use this equation so I'll multiply it WI
that is the weight matrix by the input X
1 plus W R into H of 1 minus 1 which is
0 H of 0 we know what we got from here
so W R into H of 0 plus the bias pass it
through a function G of H to get the
output as H 1 now this H 1 we'll use to
get y1 we'll multiply H one with WI plus
a bias and we'll pass it through a
function G of Y to get by one similarly
the next time stamp that is at time T
equals to 2 we have input X 2 we need to
figure out what will be H 2 so we'll
multiply the weight matrix WI with X of
2 plus W R into H of fun that we have
got here plus B of s and pass it through
a function G of X to get H of 2 from H
of 2 we'll calculate Y of 2 WI into H of
2 plus B Y that is the bias pass it
through a function G of Y to get white
and this is how recurrent neural network
works guys now you must be thinking how
to train a recurrent neural network so
recurrent neural network uses back
propagation algorithm for training but
back propagation happens for every
timestamp that is why it is commonly
called as back propagation through time
and I've discussed back propagation and
detail in artificial neural network
tutorial so you can go through that over
here I won't be discussing that
propagation in detail I'll just give you
a brief introduction of what it is now
with back propagation there are certain
issues namely vanishing and exploding
radians let us see those one by one so
in managing gradient what happens when
you use back propagation you tend to
calculate the error which is nothing but
the actual output that you already know
- the morn output output that you got
through a model and the square of that
so you figure out the error with that
error what do you do you tend to find
out the change in error with respect to
change in weight or any variable so we
will call it made here so change of
error with respect to weight multiplied
by learning rate will give you the
change in way
then you need to add that change in
weight to the old way to get the new
weight all right so obviously what we
are trying to do we are trying to reduce
the error so for that we need to figure
out what will be the change in error if
my variables are changed right so that
way we can get the change in the
variable and add it to our old variable
to get the new variable now over here
what can happen if the value de by DW
that is a gradient or you can say the
rate of change of error with respect to
our variable weight becomes very smaller
than one leg it is point zero zero
something so if you multiply that with
the learning rate which is definitely
smaller than one then you get the change
of weight which is negligible all right
so there might be certain examples where
you know you are trying to predict say
our next word in a sentence and the
sentence is pretty long for example if I
say I went to France - - - I went to
France then there are certain words then
I say few of them speak - now I need to
predict speak one come after speak so
for that I need to go back in time and
check what was the context which will be
very complex and due to that there will
be a lot of iterations and because of
that this error this change in weight
will become very small very small so the
new way that will get will be actually
almost equal to your old weight so there
won't be any updation of weight that
will be happening and that is nothing
but your vanishing gradient
all right I'll repeat it once forth so
what happens in back propagation you
first calculate the error this error is
nothing but the difference between the
actual output here or more loud put and
the square of that with that error we
figured out what will be the change in
error when we change a particular
variable say weight so de by DW
multiplied with learning rate to get the
change in the variable or change in the
weight now we'll add that change in the
way to our old wait to get the new
weight this is what the hi propagation
is going so rate I was just giving you a
small introduction to back propagation
now consider a scenario where you need
to predict the next word in sentence and
your sentence is something like this I
have been to France then there are lot
of words after that few people speak and
then you need to predict what comes
after speak now if I need to do that I
need to go back and understand the
context what is it talking about
and that is nothing but your long-term
dependencies so what happens during
long-term dependencies if this de by DW
becomes very small then when you
multiply it with n which is again
smaller than one you get Delta W which
will be very very small that will be
negligible so the new way that you will
get here
will be almost equal to your old weight
so I hope you are getting my point so
this new weight so there will be no
updation of weight scales this new
weight will definitely be will always be
almost equal to one old bit there won't
be any learning here so that is nothing
but your vanishing gradient problem
similarly when I talk about exploding
gradient it is just the appositive
vanishing gradient so what happens when
your gradient or de by DW becomes very
large becomes greater than greater than
one all right and you have some long
term dependencies so at that time your D
by DW will keep on increasing Delta W
will become large and because of the
idea of weights the new weight but that
will come will be very different from
your old weight so these two are the
problems with backpropagation now let us
see how to solve these problems now
exploding gradients can be solved with
the help of truncated BTD back
propagation through time so instead of
starting back propagation as the last
time stamp
we can choose a smaller time stamp like
10 or we can clip the gradients at the
threshold so there it can be a threshold
value where we can you know clip the
gradients and we can adjust the learning
rate as well now for vanishing gradient
we can use a railer activation function
we have discussed reloj activation
function and artificial neural network
tutorial guys similarly we can also use
lsdm and gr use in this tutorial we'll
be discussing LST aims at a long
short-term memory units now let us
understand what exactly are L escapes so
guys we saw what are the two limitations
with the recurrent neural networks now
we'll understand how we can solve that
with the help of LS themes now what are
LS diems long short term memory networks
usually called as LS themes are nothing
but a special kind of recurrent neural
network and these recurrent neural
networks are capable of learning long
term dependencies now what a long term
dependencies have discussed on the
previous slide well I'll just explain it
to you here as well now what happens
sometimes you only need to look at the
recent information to perform the
present task now let me give you an
example consider a language model trying
to predict the
next word based on the previous ones if
we are trying to predict the last word
in the sentence say the clouds are in
the sky so we don't need any further
context it's pretty obvious that the
next word is going to be sky now in such
cases where the gap between the relevant
information and the place that it's
needed is small RNAs can learn to use
the past information and at that time
there would be such problems like
vanishing and exploring gradient but
there are few cases where we need both
context consider trying to predict the
last word in the text I grew up in
France then there are some words after
that comes I speak fluent French now
recent information suggests that the
next word is probably the name of a
language but if we want to narrow down
which language we need the context of
France from further back and it's
entirely possible for the gap between
the relevant information and the point
where it is needed to become very large
and this is nothing but long term
dependencies and the elysium are capable
of handling such long-term dependencies
now LST M also have a chain like
structure like recurrent neural networks
now all the recurrent neural networks
have the form of a chain of repeating
modules of neural networks now in
standard RNA the repeating modules will
have a very simple structure such as a
single tannish layer that you can see
now this stanitch layer is nothing but a
squashing function now what I mean by
squashing function is to convert my
values between minus 1 and 1 all right
that's why we use damage and this is an
example of an ordinal now we will
understand what exactly are Ellis themes
now this is a structure of an LST or if
you notice lsdm also have a chain like
structure but the repeating module has
different structures instead of having
signal neural network here there are 4
interacting in a very special way now
the key to LST M is the cell state now
this particular line that I am
highlighting this is what what is called
the cell state the horizontal lines
running through the top of the diagram
so this is nothing but your cell state
now you can consider the cell state as a
kind of a conveyor belt it runs straight
down the entire chain with only some
minor linear interactions now what I'll
do I'll give you a walkthrough of lsdm
step by step alright so we'll start with
the first step all right guys so the
first step in our LS service to decide
what information we are
going to throw away from the cells tape
and you know what is the sale state
right have discussed in the previous
type now this decision is made by the
sigmoid here so the layer that I'm
highlighting with my cursor it is a
sigmoid layer called you forget gate
here it looks at HT minus one that is
the information from the previous
timestamp and XT which is the new input
and outputs a number between zero and
one for each number in the cell state C
D minus one which is coming from the
previous time stamp one represents
completely keep this while at zero
represents completely get rid of this
now if you go back to our example of a
language model trying to predict the
next word based on all the previous ones
in such a problem the cell state might
include the gender of the present
subject so that the correct pronounce
can be used when we see a new subject we
want to forget the gender of the old
subject right we want to use the gender
of the new subject so we'll forget the
gender of the previous subject here this
is just an example to explain you what
is happening here now let me explain you
the equations which I have written here
so SP will be combining with the cell
state later on that I'll tell you so
currently s T will be nothing but the
wave matrix multiplied by HT minus 1 and
XT and plus a bias and this equation is
passed through a sigmoid here alright
and we get an output that is widowed and
one zero means completely get rid of
this and one means completely keep this
alright so this is what basically is
happening in the first step now let us
see what happens in the next step so the
next step is to decide what information
we are going to store in the previous
step we decided what information we are
going to keep but here we are going to
decide what information we are going to
store here all right what new
information we are going to store in the
cell state now this has two parts first
a sigmoid here this is called a sigmoid
layer and which is also known as an
input gate layer decide which values
will update all right so what values we
need to update then there's also a
tannish layer that creates a vector of
the candidate values C power of t minus
1 that will be added to the state later
on alright so let me explain it to you
in a simpler terms
so whatever input that we are getting
from the previous time stamp and the new
input it will be passed through an
egg-white function which will give us I
of T alright and this I of T
will be multiplied by CP which is
nothing but the input coming from the
previous timestamp and give you input
with that is passed through at an ass
that will result in CT and this will be
later added on to ourselves state and
the next step will combine these two to
update the states now let me explain the
equations so I of T will be what weight
matrix and then we have HT minus 1 comma
X T multiplied by the weight matrix plus
the bias pass it through a sigmoid
function we get I of T C bar of T will
get by passing a weight matrix HT minus
1 XT plus by Stuart Tanner squashing
function and will get C bar of T alright
so as I've told you earlier is very the
next step will combine these two to
update the state let us see how we do
that so now is the time to update the
old cell states et minus 1 with a new
cell state C T all right
and the previous steps we have already
decided what to do we just need to
actually do it so what we will do will
multiply the old cell state C t minus 1
with s T that he got the first step for
getting the things that we decided to
forget earlier in the first step if you
can recall then what we do we added to I
T and C T then we add it by the tone
that will come after multiplication of I
T and C bar T and this new candidate
value scaled by how much we decided to
update each state value all right so in
the case of the language model that we
were discussing this is where we would
actually drop the information about the
old subject gender and add the new
information as we decided in the
previous steps so I hope you are able to
follow me going so right so let us move
forward and let's see what is the next
step now a last step is to decide what
we are going to output and this output
will depend on us cell state but it will
be a filtered version now finally what
we need to do is we need to decide what
we are going to output and this output
will be based on our cell state first we
need to pass HT minus 1 and X 3 through
a sigmoid activation function so that we
get output that is 40 all right and this
ot will be in turn multiplied by this
cell state of after passing it through
an image squashing function or an
activation function and why we do that
just to push the values between minus 1
and 1 so after multiplying your ot that
is this value and at an at CT will get
the output h2 which will be a new output
and that will only output the part that
we decided
to whatever we have decided in the
previous steps it will only output that
value alright now I'll take the example
of the language model again since I just
saw a subject it might want to output
information relevant to a verb and in
case that's what is coming next for
example it might output whether the
subject is singular or plural so that we
know what form of a book should be
conjugated into alright and you can see
from the or you can see the equations as
well again we have a sigmoid function
then that whatever output we get from
there we multiply it with standard CT to
get the new output alright guys so this
is basically lsdm in a nutshell so in
the first step it decided what we need
to forget in the next step we decided
what are we going to add to our Russell
State what new information we are going
to add to ourselves did and we were
taking an example of the gender
throughout this whole process
alright and in the third step what we do
we actually combined it to get the new
Cystic now in the fourth step what we
did we finally got the output that we
want and how we did that just by passing
HT minus 1 and XT through a sigmoid
function multiplying it with the tan @ct
the tan H new cell state and we get the
new output fine guys so this is what
basically lsdm is guys now we look at a
use case where we will be using lsdm to
predict the next word in a sentence
alright let me show you how we are going
to do that so this is what we are trying
to do in a use case guys will see dial
STM with correct sequences from the text
of three symbols for example had a
general and a label that is counsel in
this particular example eventually our
network will learn to predict the next
symbol correctly so obviously we need to
train it on something let us see what we
are going to train it off so we'll be
training our lsdm to predict the next
word using a sample short story that you
can see over here all right so it has
basically 112 unique symbols so even
comma and full stop or translated
symbols alright so this is what we are
going to Train at all so technically we
know that LS diems can only understand
real numbers alright so what we need to
do is we need to convert these unique
symbols into a unique integer value
based on the frequency of occurrence and
like that we'll create a dictionary for
example we have hardware that will have
value 20 a will have values
Gendron will have value 33 all right and
then what happens are lsdm will create a
hundred and twelve element vector that
will contain the probability of each of
these words or each of these unique
integer values all right so since point
six has the highest probability in this
particular vector it will pick the index
value of 0.6 then it will see in what
symbol is attached to that particular
integer value so 37 is attached to
counsel so this will be a prediction
which is absolutely correct as a label
is also counseled according to a
training data all right so this is what
we are going to do in our use case so
guys this is what we will be doing in
our today's use case now I'll quickly
open my PI charm and I'll show you how
you can implement it using Python will
be using tensorflow which is a popular
Python library for implementing a deep
urine networks or neural networks in
general all right so I'll quickly open
my Python now so that is my item and I
over here I've already written the code
in order to execute the use case that we
have so first we need to do is import
libraries numpy for arrays and the flow
we know attends a float contract from
that we need to import RNN in random
collections in time all right so this
particular block of code is used to
evaluate the time taken for the training
after that we have log underscore path
and this log in this code path is
basically telling us a path where the
graph will be stored all right there
will be a graph that will be created and
then that grass be launched then only
our errand and model will be executed
then that's how tend the flow work guys
so that graph will be created in this
particular part all right and we are
using summary writer so that will
actually create the log file that will
be used in order to display the graph
using ten support all right so then we
have defined a training underscore file
which will have a story on which will
train our model on then what we need to
do is read this file so how are we going
to do that first is read line by line
whatever content that we have in a file
then we are going to strip it that means
we are going to remove the first and the
last whitespace then again we are
splitting it just to remove all the
white spaces that are there after that
we're creating an array and then we are
reshaping it now in reading the reshape
if you notice this minus one value tells
us the compatibility all right so when
you are reshaping it you need to make
sure that you know we are providing in
the correct parameters
we ship so you can convert a 3 cross 2
matrix to a 2 cross 3 matrix like right
so just to make sure that it is
compatible enough we add this -1 and it
will be done automatically
all right then return content after that
what we are doing we are feeding in addy
training data that we have trailing
underscore file we are feeding in our
story and calling the function read
underscore data then what we are doing
we are creating a dictionary what is a
dictionary we all know key value pairs
based on the frequency of occurrences of
key symbol all right so from here
collections or counter words dot most
common so most common words with their
frequency of occurrence will be a
dictionary created and after that we'll
call this dick function and this dick
function or well feed in word and which
is equal to length of dictionary that
means or whatever the length of that
particular dictionary is how many time
it is repeated so we'll have the
frequency as well as the symbol that
will be our key value pair and we are
reversing it as well then what we are
doing we are calling it a build
underscore data set and we're feeding in
our training data there this is a
vocabulary size which is nothing but the
length of your dictionary then we are
defined various parameters such as
learning rate I iterations or air box
then we have display step and underscore
input now lonely great we all know what
it is the steps in which our variables
are updated training underscore
alliterations is nothing but a fox the
total number of iterations we have given
fifty thousand iterations here then we
have display underscore step that is
thousand this is basically or bad sites
so that size is what after every
thousand a box you'll see the output
alright so it will be processing it in
batches of thousand iterations then we
have n underscore input s3 now the
number of units in an RNN cell will keep
it as 512
then we need to define x and y so x will
be a placeholder that will have the
input values and Y will have all the
labels all right over tap size so X is a
placeholder will will be feeding in our
input dictionary similarly Y is also one
word placeholder and it will have a
shape of non comma vocab size vocab size
we have defined earlier as you can see
which is nothing but the length of the
dictionary then we are defining weights
as well as biases after that we have
defined our model all right so this is
how we are going to define it will
create a function RN n then we'll have X
weights and biases
after that we are calling in our n n dot
multi RM n cell function and this is
basically to create a two layer lsdm and
each layer has an underscore hidden
units after that what we are doing
they're generating the predictions but
once we have generated the prediction
there are n underscore input outputs but
we only want the last output alright so
for that we have written this particular
line and then finally we are making a
prediction we are calling this our NN
function feeding in X weights and biases
after that we are calculating we lost as
and then we are optimizing it for
calculating the rows we are using reduce
underscore me softmax cross-entropy
and this will give us basically the
probability of each symbol and then we
are optimizing it using RMS prop
optimizer alright and this gives
actually a better accuracy than atom
optimizer and that's the reason why we
are using it then we are going to
calculate the accuracy and after that we
are going to initialize the variables
that we have used as we exceed intensive
so that you need to initialize all the
variables
unlike constants and placeholders
intensify alright and once we are done
with that we are feeding in our values
then calculating the accuracy how
accurate it is and then when
optimization is done we are calculating
the elapsed time as well so that will
give us how much time it took in order
to train our model then this is just to
run the tensor board on a local host six
zero zero six and yeah and this
particular block of code is used in
order to handle the exceptions so
exceptions can be like whatever word
that we are putting in might not be
there in our dictionary or might not be
there in our training data so those
exceptions will be handled here and if
it is not there in our dictionary that
will print word not in a dictionary all
right so fine guys let's input some
values and we'll have some fun with this
mod all right so the first thing that
I'm going to feed in is had a general so
whenever I feed in these three values
hide our general there will be a story
that will be generated by feeding back
the predicted output as index symbol in
the inputs alright so when I feed in
harder general so it will predict the
correct output as counsel and this
counsel will be fed back as a part of
the new input and our new input will be
a general counsel so it will be our
general counsel all right so these three
words will become a new input to predict
the new output which is - all right and
so on so surprisingly LSD
actually create a story that you know
somehow make sense so let's just heat it
I had a general counsel to consider what
measures they could take to outwit their
common enemy the cat by this means we
should always know when she was about
and could easily all right so some how
it actually makes sense any fear in that
so what will happen when you feed in
these three inputs it will predict the
next word that is counsel after that it
will take counsel and it will feedback
as an input along with a jungle so our
general counsel will be your next input
to predict to similarly in the next
iteration it will take general counsel
to and predict consider for us and this
will keep on to repeat it so guys this
is it for today's session now let me
just give you a quick summary of court
on things we have discussed so guys we
started by understanding why can't we
use feed-forward networks for various
problems we understood few limitations
of feed-forward networks then we saw how
a recurrent neural networks all those
problems and what exactly it is after
that we saw various issues with
recurrent neural network as well
vanishing and exploring gradient and
both of these issues are due to long
term dependencies in order to solve
those issues because of long term
dependencies we use long short term
memory networks called LSD ends so we
understood what exactly LSD ms and after
that we even saw a use case of LSD m so
guys this is it for today's session now
if you are looking for live online
instructor led training for artificial
intelligence using tensor flow then you
can visit our website let me show you so
there's a website guide where you can
enroll ready record or course / AI deep
learning with tensor flow alright so it
is basically a structured program we
will be starting with the basics
the first model will be entirely
dedicated to how machine learning works
and few algorithms then we'll move
forward understand how tensor flow works
what are the fundamentals of fuel
networks convolutional neural networks
recurrent neural networks
auto-encoders things like that apart
from that we also give you a self-paced
course that is absolutely free of course
that comes along with this particular
course so that will be covering all your
bases of Python and statistics required
in order to understand deep learn so let
me tell you guys in indica you always
have the flexibility to change your
batch as and when required apart from
that we have 24/7 support team which are
ready to help you with any of your
queries in doubt cetera and finally let
me tell you that iterator provides
whatever
access that a guy gives you is for
lifetime guys all right so don t be it
guys thank you and have a great day I
hope you enjoyed listening to this video
please be kind enough to like it and you
can comment any of your doubts and
queries and we will reply to them at the
earliest do look out for more videos in
our playlist and subscribe to our Eddie
Rica channel to learn more happy
learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>