<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Installation Tutorial | Hadoop Cluster Setup | Hadoop Installation on CentOS | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hadoop Installation Tutorial | Hadoop Cluster Setup | Hadoop Installation on CentOS | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Installation Tutorial | Hadoop Cluster Setup | Hadoop Installation on CentOS | Edureka</b></h2><h5 class="post__date">2016-08-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l1QmEPEAems" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi folks this is working from a Drakkar
today you will learn how to install a
single node Hadoop cluster this is a
practical laboratory class Big Data and
Hadoop course so let's get started by
downloading the Hadoop package from the
internet and since it's free you can
just download from a batteries website
let me show you how it's done I'm going
to Google how to download so these are
the available versions of Hadoop which
we can download and I have downloaded
two point seven point two which is a
very stable release and I would also
request you to download the same and do
note that we need to download the binary
package of this so let me show you where
it's installed in my local system so the
Hadoop tar file you see here this is the
downloaded whole package we need to
first extract the contents from this
package and we can do it from the
terminal the command to do that is tar -
xvf package name
now that we have one TARDIS package
there are three important steps that we
need to follow to set up a Hadoop
cluster the first of them is bring to
review the configuration files these
configuration files contain properties
which are necessary for setting up a
Hadoop cluster second is we need to set
the environment variables these
environment variables they inform the
system as to well Hadoop and Java are
installed and finally we need to format
the name node we can and then start the
Hadoop demons so let's get started with
this so we need to go to the home
directory I'm going to list down the
contents here this is the whole package
we downloaded and this is the folder
that got created and has all the how-to
files so we need to go to the hadoop
folder I let's down the contents again
so we have the EDC folder let's change
directory to ET see there's a whole
folder inside that let's change rectory
to Hadoop again and let's down the
contents so guys these are the
configuration files that come in built
with Hadoop the file configuration files
that we are going to call straight on
our course ID or XML the Hadoop
environment or SH HDFS site or XML
MapReduce site or XML dot template and
the on site dot XML let's start updating
these configuration files with our
properties let's start off with the
course I wrote XML we run the geoid
command for that so the properties for
the course ID or XML are here with me
going to copy it and paste it right here
so these properties says that my Hadoop
must run on port number 9000 ok I'm
going to save it and close it
next let's edit my HD first side or
external by giving the command G edit
HDFS side or XML and the properties for
these are copy and paste so in here we
set the replication factor to 1 by
default Hadoop as a replication factor
of 3 we are choosing that and then
additionally we are updating the path
for our name node and a data node in
here ok let's save it and exit next
let's edit our yarn side dot X now we
will again to a G edit yarn sight dot
XML ok over here we have this property
we copy it and paste it so basically we
are just starting our auxilary services
for a yarn node manager with these
properties alright let me save it and
exit now that we are done with these
three configuration files we have two
other important configuration files our
MapReduce side dot XML dot template is
there and we need to first create a
MapReduce side dot XML configuration
file from here
we can do that by copying this template
and the command is CP MapReduce side or
XML dot template followed by the new
configuration file that we want to
create that is MapReduce side dot XML so
this is going to create a new
consolation file with this name
containing all the properties of the
template ok enter let me let's down the
content of the file so as you can see
here this is the new folder that's
created so let's go to this file and
edit the configuration files
here we set the properties for Ariane
and Map Reduce
then we save it and close so finally we
have our Hadoop environment or SH which
needs to be updated with the path of our
Java since the Hadoop framework is built
on Java Java is a prerequisite for for
running Hadoop so let me run a G edit
command to enter the environment or SH
file as you can see Java's path is not
set and let me check where Java is
installed in my folder from my root
directory I go here then to this folder
now let me search for the folder called
JVM alright here it is and there's
another folder called JDK so this is
where my Java is installed so I need to
feed the path of jdk in my environment
or SH file so I'll replace this part
with my actual path alright folks let me
save it and exit now that we've updated
our configuration files let's go set our
environment variables we first go to our
root directory here run the command G
edit dot bash RC so with this bash RC
file this contains all the environment
variables alright let's go here as you
can see the path of Hadoop and Java is
not set so I go to my folder where I
have everything I'm going to copy this
property and paste it right here
so this indicates that my Hadoop home is
set in this directory how dope is
installed in my home /e Drakkar
directory my configuration files are
under the EDC folder of Hadoop and my
MapReduce home is right here my hedge
DFS home is in this directory and also
my yarn home is in this directory
additionally we also see where our Java
said so the path of my GDK is this and
I'm saving my path here so let me save
it and exit this file we need to always
source this bash RC file informing the
terminal that the bash RC file has been
updated with you environment variables
so the command is source dot bash RC now
that we are through with setting the
environment variables we need to format
a new moon and start the Hadoop demons
let's go to the Hadoop directory and run
the command format the name node CD
change directory Hadoop and the command
format the name node is dot slash bin
bin slash / o name node - format so we
have the message that our name node has
been successfully formatted what we need
to do next is we to start our agreements
we have to demons the DFS demon and the
yarn demon and then we to start the
history server
let's start our head DFS demon first for
that we need to go to another folder
called has been which is inside the
huddle directory let me let's down the
contents of this folder we need to go
here so let me change the vector IDO as
well
and now let me start the DFS demon for
that the command is dot slash start -
DFS dot Sh and the password is this it
asks for a password again there you go
great now let's start the yawn demon and
the command is dot slash start - yawn
drawed Sh
okay there is the password we need to
now start out history server and the
command for that is dot slash mr - job
history - demon dot Sh start job history
start history so er enter so we've
started all our demons now we can check
us all our processes are running by
giving the command sudo JPS and the
password is this okay folks now you can
see that a name node or resource manager
job as we saw a secondary name node data
node and the node manager are all active
and these are the process IDs with
respective processes or Hadoop cluster
successfully setup and we can verify the
status of our Hadoop cluster by going to
the following URL let me go to my
browser
and here the URL is localhost colon five
double zero seven zero and up here you
go folks we set our Hadoop cluster at
this number and our intention in the
beginning was to set up a single node
Hadoop cluster and as you can see the
number of live nodes activist 1 this
shows that our cluster has been
successfully set up thank you for
watching the entire video guys please
like the video and comment your doubts
and queries do watch more videos from
our playlist and subscribe to add
raycast channel to learn more
happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>