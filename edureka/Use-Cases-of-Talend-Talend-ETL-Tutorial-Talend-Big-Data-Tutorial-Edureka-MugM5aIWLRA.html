<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Use Cases of Talend | Talend ETL Tutorial | Talend Big Data Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Use Cases of Talend | Talend ETL Tutorial | Talend Big Data Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Use Cases of Talend | Talend ETL Tutorial | Talend Big Data Tutorial | Edureka</b></h2><h5 class="post__date">2015-03-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MugM5aIWLRA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it really doesn't matter what level of
experience you come from aland for big
data will definitely embrace you and
indefinitely you will realize that time
in public data is such an easy area to
learn and gel and look provide the Big
Data solutions now this is an
introductory session where I want to you
know we demonstrate and showcase you the
kind of capabilities and the kind of
features that are and it has to offer in
the Big Data area now let us all
secretly what talent can do in minutes
reducing the man-hours effort estimation
in doing the MapReduce programming in
hand road shall we now what we have
planned during one of our project
session also ok I have taken up one from
one of my previous clients experience I
have taken up and I have asked the
banking industry data where the
challenge that we got from the business
was addressing the challenges in the
growing business with the use of the big
later what what what what this use case
is all about I will give you a quick
quick over all of you know round
experience in very shortly will use this
customer filled weblog data which is
collected by a bank to understand their
customer behavior and with the help of
take etl job we will answer the question
that this Bank KPI have asked us to
build in Allen for big data or more
predominantly they have asked us that
can we handle such a huge web block and
get some meaningful Delta or the
business data meaningful decision out of
it and the KPI of the requirement was
where should the bank hold the marketing
campaigns for the new product launch
in order to get more and more customer
and business we had designed it for them
in the EPL big data analytics type ok so
this use case was that this Bank has
collected many a huge web log file for
from their customers interaction and it
was it was not a very structured data of
course and they wanted to give us a
semi-structured data and they wanted to
make meaningful decisions out of this
wet log file which they can begin using
their decision so that they can have
increased their potential business or
their potential customer based on these
decisions that come out from the etl big
data analytics solution and I was saying
the environment which we will need from
our module 6 onwards till the end of the
course is for the big data is hot rocks
and Bob's 1.3 as i explained talent open
studio 5.5 and you guys may be running
on either mag or either on windows 7 so
that should be fine and I am running
minimum requirement what you need to do
is I am running trust me i am not
running on a very high configuration
model my personal computer is running
only on 4 gb ram and it's a i three core
i3 processor that's it neither it is i
fond memories I 7-8 key gram now with
that sort of sector we managed and we
figured out how to run big data really
fast so you imagine if you are even
having a better infrastructure of AG
gram or you are running on a vector
processor so you are doing much better
so it's really not hard we're even
constrained to have a hands-on ok and
there are some myths going on the market
you really need a very good
configuration model hardware in order
allah the greater knows it is just the
bay you need to figure out how to you
know get around the solution that's it
so our use case we had designed it you
know me that what does the use case
demonstrate which was also covered
during our project is because we because
of the confidentiality we cannot use any
claims data so we have to mask the data
and talent is full of advanced
capabilities where it can generate its
own data that is also one more reason
why this makes this product such a hot
cake in the market there are very few
akhil tool in the market which can
generate their own data I can generate
millions of record for load balance
testing for simulating my production
environment data based on the criteria
we will see that in our practical demo
session that how talent is so good in
generating Gil lifetime data whatever
amount of your data you wanted to
generate and once we generate that will
load the data file everything what's
step one step two step three that we are
doing we will be doing all these
operations using talent it's completely
automated in talent and not going to do
any cone we are not going to do any
manual step I will know that data on the
local file system today's DFS platform I
dope which will be done in seconds then
our primary job main job will read the
data from sdfm it will process by a pig
script and achieve the result as the
banking system we were saying now this
is a small window which showcase how
this connectivity of interfacing thing
happens in between the job or the talent
platform and the hortonworks data
platform this is the use case snapshot
right now or we see here it shows
different components used to
the desired result we read the weblog
then we filter out some of the weblog
depending on a requirement and since the
data is excessively huge whatever we do
not need we are going to these are all
advanced level of transformation and he
will cover them one by one with handsome
during our class and then we filter out
some of the columns which we are not
going to use during our result producing
and then we calculate some of the value
in a summarized format that we are
interested to know that which are the
potential area where his bank should you
know conduct the campaigns and then the
potential of those area probably have
getting you know more customers more and
more customers for this man and then we
present the data in a order format using
a shorter and then we store this result
again back in Hadoop or in HDFS so that
if you are running any big data
analytics technology or any dashboarding
on top of it that particular technology
can again be complementary and can use
this store result and if you see talent
on your right side this is the window
let me get a marker here and this is an
area which is my talents of the Italian
for big data is on Windows or any
operating system you can say macbook I'm
running on Windows 7 okay now this gets
connected to this sandbox if you see
here is the sandbox and this is what a
nose and then it interacts with the
sandbox I creatively sandbox talks with
the LM for big data and Alan public data
at all for the sandbox I get to do my
high area I get to do my Pig here
get to do my ex catalog here I get to do
all the s gfs command that I can
probably do from the command line as
well in the file rousing mode then I get
to do all the kind of settings need to
return here as admin all right I can do
all of that in this window and I also
get a parallel version of this in a
command line I get the same kind of
stuff in command as well so if you still
want to do scripting what denim for big
data or waterworks allows you is if you
still want to go screaming not using
talent for doing your big data job then
we can still do that from commonly so
during our technical demo during our
sessions I am going to teach you both
way of interacting with the big data
environment from the graphical user
interface starting from talent or big
data to the URL based interface which is
called we visit a bit of water works
who's a big data flavor traffic a user
interface which is very easy to you and
plus if you are also interested in using
from the command mode from the command
line so we get all the three interface
and this is actually the real job as I
said what weblog has been assumed that
weblog has been created and generated by
the claim by the bank bank is my client
is given with this weblog to make some
business out of it make some meaningful
business out of it and read this how do
we leave this we will see that in the
class then how do i frighten my data how
do I reduce that let alone here if there
are four volumes coming till here and I
know that my job will take a huge amount
of time if I don't cut short of my
required data if I only required to data
then it is go into the comma entry
barrier and it is going to be a severe
for me it will just give me tude it and
then i will use some summarized or
magicians to achieve the reserve then i
will arrange them in order
sorta and I will finally store so that
if any dashboarding report needs to be
generated here can regenerate on top of
this story listen as I was seeing what
is the advantage that you get with Talan
learning Italian Way learning the talent
for the big data hits to jackpot just in
a single shot wine number one most of
the enterprise struggled to integrate
their data which takes efforts n skills
and now here data integration skill of
talent comes handy and it's the
complementary to the big data
environment as I was saying by the end
of this course you guys are not only
going to be dictated champs and experts
but also you are also data integration
specialist so even if big data is also
in demand you also become that demanding
the source in the ETA lambda chi
integration platform market now number
two obviously no programming makes big
data area such a fun game that you get
to focus on so much demanding technology
and issues within the big later but
rather than only focusing on MapReduce
program right now when you do things not
in ours but in minutes so you have so
much of time to focus on advanced
concert which are coming in the market
like Scala spa loon mahout all right in
the big data area people are doing some
or other thing on day-to-day basis you
just need to know you just need to get
time to get a taste of it what exactly
are you know these things happening on
the Today market
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>