<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hadoop Multi Node Cluster Setup | Hadoop Installation | Hadoop Administration Tutorial | Edureka | Coder Coacher - Coaching Coders</title><meta content="Hadoop Multi Node Cluster Setup | Hadoop Installation | Hadoop Administration Tutorial | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hadoop Multi Node Cluster Setup | Hadoop Installation | Hadoop Administration Tutorial | Edureka</b></h2><h5 class="post__date">2015-11-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-YEcJquYsFo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so let's get started so okay thank
you guys for thank you for your time for
joining in the taking in time to join
into the session so in today's session
what we would be doing is let's see how
to set up a how to multi node cluster Oh
so this is kind of a little bit of a
kind of an advanced thing I would say
from if you are very new to it you might
find it a little bit difficult but just
try to follow the steps that I am doing
step-by-step and you can also go to the
recording once this is done and you
would be able to set up your cluster on
your own so you this is this is
something I'm I'm structuring this in
such a way that anybody can set up the
instances on their own now before going
on I have a quick question or maybe want
to check with you guys how many of you
oh well to set the agenda so what we'll
be seeing in the next 1 hour or the next
45 minutes is we will be talking about
various Hadoop components and then we'll
see how can we do the configure how can
we do the configuration on of these
various components so as part of this
what we would be doing is we would be
setting up a three node cluster so when
I say three node cluster in in
real-world sense when you talk about a
Hadoop cluster it is a physical grouping
of servers
so normally you talk about clusters
where you say it's a logical grouping on
application servers but in here in Hutto
we will be talking about a physical
grouping of servers up so what as part
of this what I would be doing is since
since I have to show the demo on a
single machine we would try to replicate
a real-world scenario on on a on a VMs
where we would be using Oh
Oracle's VirtualBox manager so the same
physical server that you would see would
be replicated on my instance on mice on
my desktop using your Oracle's
VirtualBox manager now before going
ahead so if we can have a quick poll how
many of you have heard of Hadoop how
many of you know
were the different versions of Hadoop
how many of you know or have seen Hadoop
set up and how many of you are new to
Hadoop so you just want to understand my
audience so that I can set the tone and
I can I can see where I can answer or
how they can address this man gender how
can we set up scoop in VMware a man
ginger that is out of scope of this
certain topic right now well certainly I
can help you I can talk about that if we
get enough time so I'll definitely
address that if you have enough time
towards the end of the session on scoop
what seem is asking can you parallel me
the construction in Hortonworks so
Hortonworks
I think maybe Waseem you need to reframe
the question you saying
can you parallely the construction in
Hortonworks not sure exactly on that Oh
Yogesh has a question for a small to
mid-size project how many nodes so we
have well for a small to mid-size
project a minimum size of a cluster it
would be fine old cluster that's the
starting of a start-up that's the best
way to have a minimum final cluster to
start off which Hadoop distribution are
you going to use for this session
oh we would be using open source Apache
the Hadoop version would be 2.71 Yogesh
is certified developer perfect wonderful
rishi case new to hadoop Swapnil heard
about hadoop run ratchet Kumar yes
you know Hadoop and cluster setup no
okay no problem muncha muncha is
familiar with your oop Shamel I do not
know how to but I have set up clusters
okay perfect
what seem is new to Hadoop soft knit
again beginner such an array now I know
how to snare hull but what is difference
between single node and multi node can
you explain definitely I will explain
that Hashima new to Hadoop okay peez
saying not able to hear my voice guys
can everybody hear my voice
okay so what seems question is can you
parallely discuss how to construct
multi-node in Hortonworks well guys we
are not using any GOI tools here what we
are going to show you is what happens
behind the GUI right so Haughton works
and cloudier or the ones which help you
to install to a wizard but what I am
going to show you is what what is the
wizard going to show you or what is a
wizard going to help you on behind the
scenes right so so that is what we are
going to tell you but yes Hortonworks
and cloudier is something which we can
cover in and maybe maybe in a far more
advanced session but this will be
tweaked or this will be primarily
revolving around your open source Apache
Hadoop so fine ode to mid size should be
15 to 30 notes so yes
yo geisha you can to start off with a
single cluster you say I mean if it's a
very small cluster fie notes even a 100
node cluster is not considered as a
large cluster even that is also
considered as a small Flynn stir because
when you talk in real-world sense it is
a you are talking about thousands and
thousands of servers
ok so others so saying a completed
Hadoop goes from a Drake I wanted to
understand more perfect so Nell new to
Hadoop AJ I'm not having Oracle VM into
vSphere will will it still work yeah
absolutely if you have VMware flavor of
for any flavor see what we need is just
a VM player it doesn't matter whether
you whether you are using a VMware
Player VMware Workstation or or a an
Oracle an Oracle VirtualBox manager so
you just need a player where you can
simulate a virtual working any Ron mint
silverstein sayings doing Hadoop
certification in it like a wonderful
service Tim good to know
ok so so I think or view are you going
to use any big data tool no soap nil
what is a lightweight harder
distribution Hortonworks map are etc
well a lightweight is open source Apache
is the more
slight weight distribution right now but
yeah but what you guys need to I mean
the reason why we chose open source is
it will tell you exactly what are the
configuration files that needs to be
changed and what changes get impacted
there can you share what is a default
block size in Hadoop 271 the default
block size is 128-bit 128 MB in your in
your Hadoop 1 you typically talk about
64 MB in Hadoop two respective versions
it is 128 MB I know enough to be
dangerous wonderful something that's
great to know
ok alright so the kind of response I am
getting I think up up or the kind of
response I'm getting is that people have
people who know Hadoop I have people who
does not know anything about how to just
people have heard of it and I have
people who have already implemented
Hadoop and people who are have already
set up a one node cluster already so
that's a good note good to know that I
have a mix of people so I can start from
from the scratch or from the basics and
let you help you understand each and
everything so everybody is on mute right
now any questions you have please type
into the chat window and please try to
keep your question to the point where I
am discussing one at topic okay in the
next few slides next five six slides
will be talking about multiple topics so
please keep your question relevant to
the topic we are asking so that we are
not jumping around here and there so
neeraj has a question let me answer this
question one of the intro question
difference between Hadoop architecture
and a client architecture
I think the nearer should a question
would be Hadoop server Hadoop cluster
architecture and your client
architecture
well basically on your cluster you'll
have all the demons running on your
client you will have the same
application or the same package but you
will not have any demons running so you
just have the api's so the communication
from the client to it's a client-server
architecture where the fly
we'll have the api's you have your HDFS
API you have your MapReduce API the
communication happens where the api's oh
how will you make sure not nodes in a
cluster on the same network you need to
make sure that's again you need to loop
in your Geisha look in your network team
and make sure that how you're building
it when you build a new cluster it's not
an individual responsibility there are
multiple themes involved except for if
it's a small company so if it's a small
company and if you are if you are a
one-man hero then you'll be doing
everything but oh but oh but that's
again goes from the network side on the
network switch okay how you configure
the network again will be dependent on
the network team can we have set up
discussion with three nodes in the
cluster yes
KP or we are going to talk about that
okay so let's get started to start off
with we need to see what are the Hadoop
components difference between Hadoop
cluster and Apache spark cluster well
it's not in the scope of this certain
it's a Hadoop cluster is different and
spark cluster would be different what
spark coming I can talk about a high
level view right now where spark does
not use your MapReduce but you mean it
could it just bypasses the MapReduce but
then the data will be still be residing
on your sha HDFS itself so that's a
different topic
okay so just don't want to diverge right
now let's move on and see let's stick to
the current topics and see what are the
core components so oh this diagram that
we have right now is primarily towards
talking about your tube to X I mean we
are I mean you might be knowing
different versions of Hadoop are
available so Hadoop one where there are
some disadvantages where wherein you do
not have h a high availability and you
have a single job tracker now with
Hadoop - the reason Hadoop - came into
picture is because of some of the
disadvantages with Hadoop one where some
of the concerns
some of the shortcomings have been
addressed and new features were
implemented now now to start off with
for the guys who are new whenever you
talk about Hadoop or whenever you're
talking learning about how to look into
this into this one is storage the second
one is processing always remember your
Hadoop is entirely revolved around a
storage and your processing
that is what Hadoop is an application
which is helping you to store the data
and process the data so so that's how
your Hadoop is evolved or Hadoop is
revolved around now you may have a
question a basic question or if you talk
about real-world use case take the
example of take the example of an
e-commerce website an e-commerce website
when e-commerce website you log in to
any e-commerce website try to search for
some item let's say a mobile or or a
digital SLR camera and you search for an
item and you get a listing you hit on
one link and towards the bottom of that
you will see that there are some
recommendation saying that people who
have purchased this have also bought a
different item along with this purchase
people have purchased an additional lens
okay so you short this to the camera
towards the bottom you will see that
people have purchased an additional cat
an additional lens with that so a
default camera might come in might be
coming up with an eighteen fifty five
lens right so you might go for an
upgraded lens also so there are chances
that you might be tempted to go with the
majority of the recommendation and you
end up buying an additional lens so
that's what a recommendation engine is
called it's called a recommendation
engine where Amazon implemented for the
first time and they claimed that when
they implemented the first month they
have seen a 30% revenue jump right so
the reason I am talking about this is
because we need to understand in a
real-world scenario what are the cases
where Hadoop is being implemented now
now how did how did any e-commerce site
comes to that conclusion because they
have to store the historic
data not just a one month data they have
to store last six months last last 12
months last 18 months last two two years
last three years when they store the
amount of data when the store when they
increase the duration of the data there
is there are chances that they will get
an accurate hit the more accurate to
accurately predictions can come up right
so based on historical information is
what the recommendations are coming in
now comes the actual problem when you
have to store say for example two months
data its well and good maybe it's a one
terabyte or two terabyte you can store
it maybe you want to store six months
data okay you can store it if you want
to store it for the last five years
there is a data problem you have huge
data there you have big data there that
is what the big data is the data for the
last five years people who have browsed
your website people who have purchased a
particular item
all this will be composed of the big
data so it's a huge data for the last
five years now you need to look at a way
to store the data if you go with the
traditional systems they are very
expensive right your traditional storage
is very expensive and if you are saying
that you want to store the data for the
last five years you need to spend a huge
huge amount of money to store to invest
in so many storage servers and then
store the data now that's where people
have started looking at a new solution
now now before even going that forward
let's step back a little bit
once you store the data what do you do
you analyze the data you store the data
first of all and then you run your
analysis when you do the analysis what
is that you are doing you are picking
out the value out of the data the value
is nothing but the recommendation engine
that is a recommendation your
recommendation is based on the last five
years historical data so that's where
you can get an accurate accurate
prediction or accurate information the
same analogy can be applied this is just
one example of an e-commerce website you
can apply it for your financials
on your stock trading on your stock
trading the companies gather huge amount
of data so anytime whenever you see a
news happening you will get some
accurate predictions saying that the
stock is going to go down the stock is
going to go up
how does they come to that conclusion
because they come to the conclusion
based on historical information and lot
many factors based on the company's
financial audit results so when you
store this data you have the challenge
of storing a huge amount of data that is
your big data now once you store the
data what you need to do is you need to
process the data so when they wind up
when the people were challenged with
these issues they look for a new
solution and they came up with Hadoop as
a solution which is providing you
storage plus processing now the storage
is termed as your HDFS ok storage is
called as HDFS and your processing is
called as MapReduce so the MapReduce
higher version is implementation is yan
ok I will explain these acronyms in a
short while now try to understand what
are the core components your storage and
your processing why do you store the
data it is because it has some business
value to store the data now in a
competitive world
each company wants to stay ahead of
other company so when you want to stay
ahead of others you need to have more
information and come up with good offers
or good campaigns so that's why you
store the data and you're leveraging
Hadoop you call it as HDFS which is
Hadoop distributed file system ok so
that's where you store the data now now
once you store the data how do you
process it the processing part you need
to process it the processing part is
called as a MapReduce historically or
legacy it is called as a MapReduce now
with a new version with Hadoop 2 you are
calling it as yan ok yan is
yet another resource negotiator okay
John is yet another resource negotiator
which is which is typically called as Mr
version - Mr version one is associated
with Hadoop one John is associated with
Mr version - and that's what you call it
as MapReduce version two so this is just
a John is a component which does the
resource negotiation and on top of it
you still have MapReduce running in here
now that you understand the storage and
processing now let's understand the sub
components within each the sub
components within HR okay it is a master
and a slave architecture so you have a
master and a slave
so for HDFS you have a master and a
slave for Yan you have a master and a
slave now what is the master component
called the master component call is for
HDFS is called as H your name node so as
a backup for name node you have
something called as a secondary name
node which acts as a backup for your
name node then the slave component for
your HDFS is called as a data node
similarly the master component for your
yarn is called as a resource manager and
the slave component for your yarn is
called as your node manager so it's a
master and a slave architecture where
you have a single single master and
multiple slaves so okay so click
questions coming in can we process data
locally and then pull it on production
yes hopefully that can be done Rohit or
Jean and M are version two same well mr
version two is yarn and Yammer version
two typically you need to understand the
yarn framework or Map Reduce so how does
yarn you need to understand the resource
management part so they're closely tied
together where the resource management
is taken care by resource management is
taken care by yarn with Hadoop - how can
you tell again how yarn is different
from MapReduce well see
and runs on top of sorry MapReduce runs
on top of yarn okay so so John does
cluster management John does cluster
management and your MapReduce runs on
top of yarn we're because when you Roo
it when you do a MapReduce processing
you need to have some kind of resources
allocation and that is taken care by Ori
on standby node yes standby node is
again a feature with Hadoop - so standby
node fits into the same place instead of
a if you with Hadoop - typically people
use companies use Hadoop - because it
has got the ability to do an automatic
failover so with Hadoop 1 there is no
secondary name node is a legacy is a
legacy term or a legacy of which comes
with Hadoop 1 now by default when you
are building a Hadoop cluster I mean as
we do in our practicals now you will
have only secondary name node when you
implement a standby nor your secondary
name node will not be available ok so
this is the entire architecture of this
entire architecture of how you have your
Hadoop components now setting up a tree
node cluster typical cluster I mean when
you set up when you talk about a typical
cluster now you know water then what are
the components now I think can you guys
tell me water what is it demon does
everyone know what is a demon is calumny
can we integrate spark inside how to
cluster absolutely as spark can be
integrated inside out of cluster that's
right so I'm just trying to answer
address some questions guys so why is
data node as slave yes data node is a
slave because it's the data node is
where all the data is going to decide
which Java collection is best to process
data Java 1.6 and ago is what is is
supported so right now it's 8 JDK 8 is
available you can use it 8 all so what
is HBase HBase is a no sequel database
it's a Hadoop database John look
like MapReduce library no Yan is
different and mapreduces different yarn
does only or resource management
MapReduce runs on top of yarn that is
does it follow instruction yes money
show slave node follows instructions
from master node what is the advantage
of yarn over MPP or map MapReduce okay
advantage of yarn over MapReduce is if
you have a data on your Hadoop cluster
with MapReduce you can write only a
MapReduce programming with Yan
implementation you implement something
called as a as a capacity of capacity
scheduler so you need to go a little
deeper I mean that's something which you
can understand once we go deeper but but
Mr MapReduce runs on top of yarn where
the resources are allocated or decided
by your resource manager that is your
yarn which is part of your young leo is
saying storage place process plus
analytics these are managed in data
where as to why we need Hadoop instead
of ETL tools right storage plus process
plus analytics absolutely leo they are
managed but there is a license coaster
you need to pay for license and and what
if the storage so storage you are not
storing the entire storage right so you
are you going to store I mean your your
archiving some data and only storing a
subset of data on your ETL because there
is a license cost and your disk is
expensive there so once you hit your
capacity you need to discard the disk go
for a new disk right but you don't have
that problem with your Hadoop where you
can expand your size of cluster to the
extent that you want perfect guys I
think everybody answered process a demon
is a process so which keeps on running a
process which is up and running all the
time let's write so the reason I asked
you what a daemon is because so how
question demons are like name node
that's white man ginger so the reason I
asked for daemon is because all this
processes all these names your
seeing here or nothing but demons your
name node is a demon secondary name node
is a demon data node is a demon data
node is a demon on this server so for
our practical purpose what we would be
doing right now is we will be trying to
have three nodes I am NOT I mean three
or four nodes but typically I will have
secondary name Lord running on the same
machine okay as my name node and I'll
have two data nodes so one master node
and two slave nodes the same thing will
be applicable this is what is your HDFS
es now when I say HDFS because the
naming convention you're seeing is only
related to your HDFS but I will also
have my yarn components running in the
same way on this name node I will have
my resource manager on this data node I
will have my node manager so in effect I
will be running everything on three
nodes I will replicate a real world
cluster on my local machine but typical
cluster use case if you look at here you
need to have excellent RAM for name node
hard disk use of not that important need
to have multiple multi course make sure
you have multiple Ethernet interfaces go
for a 64-bit operating system and make
sure there is a redundant power supply
you know I mean that is common with a
typical data center right so you'll have
two paths where in case one power supply
is gone you will have a second power
supply available so that's on the name
node and you'll have an identical
secondary name node when you are
implementing a secondary name node you
are saying that this is a backup for
your name node so the configuration of
your secondary name node should be
exactly identical to your name Lord now
coming to the slave processes the RAM is
not not that important here though the
processes run here the RAM is not that
important but your disk is very very
crucial because this is where the data
is getting stored so you can have
multiple multiple multiple disks of two
terabytes or multiple disks our small
sizes
and your course cases you know one with
to course and make sure there are
internet into multi into multiple
Internet's multiple Ethernet interfaces
and also go for a 64-bit OS one other
caution is always make sure you have a
you have a homogeneous software across
all the machines so that you can have
it's easier to debug or it's easier to
maintain as an administrator do not have
a true genius well you can have Center
yes you can have red hat you can have
Susie all this mix of variations you can
have but it is suggested that you have a
single flavor of OS running on all the
mesh all your machines
okay so shaman has a question as per
your slide you're showing a 64gb Welsh
ml that is again the metadata need to be
held in your name note that's why you
need to have more RAM that's why I need
to have more ramp beneath has a question
very Steve volume velocity big data and
not in warehouse license cause place no
scope Phyllis vertical horizontal
expansion answers to open it wonderful
law I think you summed it also what is
the function of secondary name node
ponies second the function of secondary
name node is to act as a backup okay so
can we have secondary name node and name
node on on different nodes yes you can
you can have it on different nodes name
node looks like look softer reading and
writing data from data node that's right
suppose I want to configure 100 node
cluster same way I can do any other or
any other conflicts require yup you can
do so I'll show you the steps where I
can show you one step on setting up a
master node and step on multiple ways on
setting up your slave nodes the same
Epting can be applied to almost all your
nodes
okay irrespective of one node or a few
fine notes or a hundred node you can
follow the same process just need to
have your linux skills make sure you can
push it push the same code or same
config files across all the machines
what factors need to be kept in mind
while setting up named node what amount
of data and how much RAM and Coase are
needed
right near a sure that's a good question
so that again depends on how much your
company is going to invest and how much
your how much sure how much you're going
to comment sure in capacity how much
data is going to come in and also
whether it's an i/o bound or a CPU bound
okay so I will pound this what amount of
data you store CPU bound is what the
amount of processing you do whether
cluster is going for more CPU bound or
more I will pound that again so all
these factors will need to be considered
um Puneet is asking does it mean
automatic failover well secondary name
node does not do an automatic failover
only the standby node will do an
automatic failover so it is not here on
this slide right now but you typically
talk about says standby node which does
an automatic failover I think MapReduce
processing will happen on absolutely
MapReduce will happen on data node where
you have a node manager running is the
configuration for how many nodes this is
the configuration that I am doing is for
three nodes so obvious yes such an now
we are doing try to do it on a
real-world configure in a real-time
environment exactly
yes subject please go ahead and shoot
your question no obviously unfortunately
you cannot speak we cannot harm you -
everybody's on mute so you need to type
into the chat window I just wait for you
to finish typing so in the meanwhile I
had where does name node store FS image
and edit locks in if in hard disk then
why need more RAM like in three node
cluster when we store any file on HDFS
how can you identify that on which node
it's being stored yeah manage inner what
happens is the metadata will be written
into two locations one is on to the disk
and the second one is on to the name
nodes Ram so you need to store it onto
the name it gets loaded onto the RAM so
that's why Ram is very important to the
second question yes there are a lot of
tools available with Hadoop where you
can upload it
with file and you can query your file
system and try to figure out on which
node that particular block is going and
residing soap nails question is
automatic failure causes main node to
get failed no automatic failure is where
if your name node crashes for some
reason with this secondary name node
you need to have manual intervention and
bring it up but with your with your
standby node if the name Lord crashes
the standby node will be automatic
failover you have two ways one is you
can do it
manual failover the second one is you
can do an automatic failover but without
losing your data so a sheet is asking I
have one machine in Ubuntu and two
machine in Raspberry Pi
yes you can you can configure absolutely
no problem why do we need two data nodes
right so yeah I think most of your
questions will get answered guys so I
mean just just hold off for some time
okay so if name node cannot get all
metadata info does the information get
spilled on to disk if the name node
cannot accommodate metadata that's where
you hit the capacity of your RAM or your
meta your metadata Ram is the capacity
is hit and you cannot upload any data
into the cluster okay
all right so let's let's start with
let's start guys so I think most your
questions will be answered as we do the
session so some of the configuration
files that we need to look into is well
I mean I'll explain to you so your
Hadoop env assess your yani and may
assess your code set XML HDFS map red
yarn site and your slaves file so what
these files are and what these are all
I'll show you in a short while so four
steps for creating a Hadoop multi node
cluster first thing is a Hadoop is a
Java based framework need to make sure
your Java is installed okay
need to make sure your java 1.6 endeavor
is installed or jdk your your jdk is
needs to be of above 6 and 2 and hadoop
need to download Haru package now the
next point is talk about is space
by the IP address of each machine
followed by their host names in host
files right so so this is required
because the nodes will be there a lot of
communication happening between the
nodes the data node will be talking to
the name node name node to the data node
lot of intercommunication keeps on
happening every now and then very very
often so for that communication to
happen the node should know each other
either you can add it into e.t.c host
files or you can rely on a DNS okay
normally you'll have a primary DNS and
you have a backup DNS so make sure your
DNS is properly configured if you don't
have your DNS configure then have your
EDC host file set up properly the next
one is configured so download set up
your IP addresses then configure your
configuration files and then you format
the name node and and you can check the
UI so this is the steps that I am going
to show you in the next few minutes okay
let's go and start and let's do that
alright so in my case what I'm showing
you here is I have n n1 ok typically I
do a lot of demos so n n2 is where I do
a second or standby name node where we
show H a demo ok where we talk about H a
so I am NOT as started only n n1 then I
have D n1 D n2 + d n3 so n n1 is my name
node ok so where I will have my look at
this here so what I am building a
cluster is master diamond's will run on
my name node so can you guys quickly
tell me what are the master demons so I
have a HDFS and I have my yarn so what
are the master demons for HDFS the
mastered even for HDFS is name node and
the master diamond for my Gianni's
resource manager
and my slave demons will be running on
my DN 1 and DN 2 now also here I have my
slave demons so HDFS my slave demon is
called as a data node and my jean demon
is called as node manager ok
so this is how this is how much my
cluster would be structured around where
I'll have three nodes one is a master
node and two slave nodes so right now
what I have is on my machine I have a
cent to s installed so this is the
center is here instance so you can see
it is C this is sent to s release six
point seven is what on my machine right
now so I will be my practicals are all
surrounded around this now the first
thing is make sure you have your working
Java version installed that is important
so just try to see you you have your
Java version make sure your Java version
is good okay there's the same thing on
all the machines so so okay now I will
be toggling always switching between
multiple machines so keep an eye on this
one where it's talks about my host name
this is my host name NN one so I have
two other host T n1 and my third host is
DN 2 ok three nodes so my nan one will
be my master my DN one and my DN 2 will
be my slave processes where I have my
slave nodes running so this is how we
have structured it YP address NN 1 my
master and my two slave nodes now what I
did was I also had a DNS setup so I can
easily ping
ok ping my DN 1 your ping should work
and your NS lookup should also work ok
your nslookup should also work so if I
say nslookup to my DN 1 that should be
working so it should come back with my
lookup ok so you need to have your
forwards zone and your reverse look up
zone so if I give up my nslookup of IP
address it should be coming back with a
resolution so if you do not have a
NS setup the alternate ways you can go
inside it is a host file and make the
entries in here okay it is a host file
make the entries in here where your
local host will be pointing in to local
host right on that machine and the rest
of the mapping should be done here
so my DN 1 is also pointing in my IP
address and my full qualified name the
fully qualified name so the fully
qualified name is what if I say host
name right if I say host name this is
what my fully qualified name is so here
it is only showing my host name and my
fully qualified name is anything after
your host nimac cluster 2.com is my
fully qualified domain name so I made
sure that my communication is happening
between my name node and my data nodes
so if I think DN 2 it should be
communicating I should get a response so
this is what the first check is and then
I verified my I verified Java is
installed
ok that's I'm not going to show you how
to install Java so ok so your Java is
already installed on here the next
important thing is download your Hadoop
ok so download your Hadoop where you
download your Hadoop typically you can
go into a open source hadoop
so you can just say hadoop download
choose a mirror sight always go for a
stable version so this will give you
almost all the versions available always
go for the stable version off of the
stable folder because that's where you
have a stable version because others may
have some bugs in there which will be
fixed in stable so this is a version I'm
downloading just click on this this will
download a tarball so the same tarball
is what I have here there's a first step
in your Hadoop cluster setup okay now
let me bring this to the top of the
screen quickly I now the first thing
this is what I do now just trying to see
okay let me let me remove this let me
remove this I have a soft cling created
here so let me quickly remove this and
then remove this folder as well so I
have tarball downloaded the next step is
to unter it so how do I enter it tar -
zxf okay
you
yes this would be uploaded to YouTube
post the end of a session so this is
right now entering it so similarly I
will do it on my DN one also I have the
tarball here so it is taking some time
zxf and on my DN 2 also so once I enter
it what I will have here is I will have
this folder created you see here I don't
have a Hadoop 271 folder right now when
I antar it I have it
Hadoop 271 folder created now if I go on
to go inside that I need to say CD
Hadoop - two point seven point one
I need to type everything so to avoid
that I will quickly create a soft link
in here I will quickly create a soft
link how do you create a soft link
Ln - s you give it a folder name and
then you do the soft link name now if I
say Ln - L of Hadoop this is actually
pointing into my 271 so the first step I
do create a soft link the reason I am
creating a soft link is tomorrow if you
get a newer version you can simply
unlink it and point it to that
particular link okay you can just map it
now let's go inside your Hadoop location
see here what all files and folders you
have so this is your directory structure
of your Hadoop package so this is
nothing but your Hadoop right now so
this you have downloaded the software
and you have installed it the next
logical steps would be to configure it
so where do you configure it with you
have the configuration files all the XML
files we just saw in my slide or
available on in my EDC folder within
that I have a Hadoop folder and within
that I have all my XML files these are
all my XML files
where
I need to configure here now now
important XML files that I need to be
concentrating on or my coresight XML
then my HDFS site XML then my map read
site XML okay
so map recite XML is not be there so
you'll have a template file and then my
yarn site XML so these are the important
various configuration files I need to
help now if you open these files all
these files are empty when I say empty
look at this configuration here in
between configuration you don't have
anything so that is what when I say it's
an empty file so you don't have any
configuration now this is what is a
standalone mode of installation is okay
so this installation is called as a
standalone mode where I can execute my
HDFS commands HDFS DFS if I say HDFS DFS
and - LS of let's say here now I am now
I am executing my Hadoop commands where
am i executing my hardwood commands on
my local file system I am executing the
Hadoop commands on my local file system
so this is what would be your viewer
your local your standalone mode of
installation right because it is giving
me it is giving me the output of the
same files so this is your standalone
mode of installation now building upon
this we will start with our multi
cluster mode multi node setup so first
thing I need to do is open my coresight
XML within this configuration
tags is what I need to add a property
the property that I need to add in here
is available here so copy this property
value it has a format ok the property
has a format where you open a property
and within that property you have a name
the name of the property is a first or
default FS where you have an IP address
okay and then a port number so I am
defining where my HDFS is running my
master my master H
DFS is running and I end the property
tag so this is the default confirming
minimal configuration is what I am doing
so they can quickly start my cluster and
I save this file there's a first step
then my HDFS site XML in my history of a
site XML is also by default it's empty
it's empty by default there is no
configuration in there now what I do
I'll go ahead and make these two entries
what these entries are the first entry
talks about DFS dot name no dot named or
dar this indicates where I am running or
where I am having my name nodes metadata
being stored so I need to give it disk
location this is my disk location this
is a disk location where my name node
metadata is being written similarly I
have a DFS dot data node dot data of the
air which indicates where all my data
nodes data will be residing so all your
blocks will go and residing in this
directory location so these are the only
two properties I am giving in here
let me save this file now I need to make
sure the directory structure is created
so I can simply create that in a single
one go ok m'kay day or - P I will create
a name node barratry similarly I am
going to create the data node directory
also ok once my data node directory is
created one other thing you need to
remember is you need to make sure the
permissions on your data node or seven
five five ok seven five five or the
permissions on your data node so your
HDFS ID XML is done the next thing is
your map read set XML so as you have
seen there is no map reside XML by
default so you have a map read sight XML
template so make a copy of this XML file
into map let's add XML and make the
entry into your map rested XML so one
property I need to add in my map let's
add XML is what is indicating what is
the framework it is
to use typically with Hadoop one you
will have to define where is a job
tracker his running so but here I am not
running any job tracker here I'm running
I'm running John okay I'm running on top
of yarn so that is what you're
indicating in here so we are indicating
MapReduce dot framework dot name which
is your yarn so save this file
the next important file is your yarn
side XML where it is also by default it
is also empty so some of the properties
that I am going to copy right now or
required property or required properties
so these properties I add in here okay
so my John site XML is also done and
once my a on site XML is done save this
file now once I saved all my property
files the next step if you go back into
your presentation the next step is it
talks about edit the slave files on
master right slave files on master so
you have something called as a slaves
here which will contain a list off which
will contain a list of servers that are
to run as your slave nodes now I know
that as part of my initial configuration
okay I am setting my dn1 as my slave
node so I will insert my first slave
node then my second slave node my second
slave node is my DN to get the IP
address and add the IP here so I say the
slaves file here now one thing to
remember is with Hadoop 2 you will not
have a master file only with Hadoop 1
you will have a masters file because
that's where you're going to configure
your secondary name node but with Hadoop
2 by default typically companies will do
go for Hadoop 2 is because they want to
have a standby node rather than a
secondary name node so whenever you are
implementing a Hadoop to implementation
you should definitely have a standby
node now these are the configuration
files I am done ok now I am
you do start my cluster right so format
the cluster so format name node and
start all Hadoop services so it says
format your cluster but before I format
hold on what I need to do I need to make
sure my slaves are also having the same
configuration files the slave should
know which is a master how does a slave
know the dn1 how does three and one know
that it has to it has to act as a slave
for your nn1 so what you need to do you
need to make sure you need to make sure
create a soft link similarly with what
you read with Hadoop one and copy the
XML files into your slave node so that's
what I'm going to do in the next step
steps so first thing create a soft link
here Ln - is point it to Hadoop so do
the same thing here on your DN - also if
I say Hadoop oops it is pointing into my
271 now for me to copy the XML files
what are the XML files I need to copy I
need to copy the XML files my important
XML files I showed you here right I
listed down my course site my HDFS ID my
map read and my jean side so what I will
do I will just do an SCP so SCP course
ID XML and my map read side dot XML then
your yarn so your HDFS site XML then
your John site XML copy DS into what
into my DN one home slash a Drakkar
slash hadoop slash ET c / hadoop and
copy in there that is one to my DN one
the next one is copy this into my DN 2
also
just compute dn1 and verify the files
are there so go inside your Hadoop
inside this et Cie inside this Hadoop so
you have your XML files right so these
are time-stamped right now so now I know
that these are my latest finds okay just
copied so the same will be copied onto
your D and to also now one important
thing I need to do here is because I'm
having my data node this is my data node
I need to create my data node structure
I don't have to create the data route
structure on my name nor I think I
created on my name node here so we don't
need to do it on name node if you are
doing a pseudo cluster then yes you can
do it on name node but I don't want to
have my slave processes running on my
name node so so I'm not giving it on my
name node so I need to create the
dietary structure on my slave one first
of all mkdir - P and I am creating the
data node structure and chmod 755 for
this directory same thing we just
execute the same thing on your DN 2 as
well and chmod 755 for this put
particular territory ok so you're all
good right now so the next step is it
says edit slave files on master node
format the name node and start all
services so the first thing you do when
you purchase a new hard disk or a new
computer is what you do you format it
now that I do I'm new to Hadoop I don't
know what commands to run I just run
HDFS ok I just want HDFS this gives me a
listing of all the commands I can run
here the first one is name node - format
so I will leverage the same thing I will
say HDFS name node - format
so this is where it is formatting your
cluster your name node typically so what
is a format do typically when you format
your hard disk it will create your inode
entries if you create tables all those
things the same thing is being done
right now now make sure you are seeing
this message make sure you are seeing
this message storage directory has been
successfully formatted you remember this
is the directory we have given on your
HDFS site XML where your HDFS dot named
named no dot name door there
this is the this is the value to that
property so your format is successful
the next important thing is start the
instances so how do you start the
instances so we go by two starts one is
a start your DFS door Sh
so this is study your name note
yes so what is happening here is it is
automatically logging into your slave
nodes so you need to set up SSH keys
okay so that is something which I
already done before the start of it or
else it will keep on prompting you to
enter the key in the password every time
you do this so you'll have the secondary
neighborhood starting here because I'm
not using your standby node because I'm
not using a standby note I will
obviously have my secondary name node on
the same machine where I have where I
have my name node running so this is
taking some time guys so 2 2 2 2 2 ok so
to verify if the processes have started
or not I just do a JPS and I see that my
master nodes are up my name node and my
secondary name node now log in to your
DN 1 and try to do it JP s JP s is what
JP s is a Java process monitor ok JP s
is indicating it's a Java process
monitor and try to do a GPS on your DN 2
so the data nodes have not started here
so let's quickly look at how to
troubleshoot this so how my master notes
upon my name node but mites they're not
started on my data node right so what
could be the reason here the reason
could be because let's quickly look at
the log files so where do you have the
log files you have the log files on your
Hadoop install location so this is
typically what you can do as a
troubleshooting step I have I should
have cleaned it up but I did not clean
it so where it says that ok data node
all specified Bera trees or failed to
load so it is not able to start the data
node process here right so there is a
lock in here ok there is a lock in here
on your iterator Hadoop data node - so
this is acquired by node ok so this is
not been acquired properly that is
because the cluster IDs are different so
let's quickly
overcome this how can you overcome this
you can go inside home ad wake up then
Hadoop store HDFS then you are data node
- right so you already have some
information here just remove this meta
data and then now now that I need to do
some kind of a troubleshooting start the
processes individually so how do I do
that usually I do - diamond or SH start
my data node process now if I say JPS
here my Java process monitor will
indicate to me might get a notice up so
similarly I will do the same thing in my
Hadoop store to store then HDFS data
node - I have something else here
previous I have the I have the ID from
the previous I have the metadata from
previous cluster which is available here
which I am just trying to remove right
now I have removed it and then I am
starting individual process demand dot
SH start my data node while the data
node is starting here come back into
your name node and start your John
demons john dot sh okay so let's come
back into your DN one here so I am on DN
one right now do it JPS so your data
node is up here do a JPS your data node
and your node manager should be up here
okay
so here it says here you see the message
it says node manager is running to find
nine four to five nine four was from a
previous I mean you can we can start at
looking at troubleshooting this okay
pipe grip for your to find nine four so
it is I mean the process is not running
but is something which you can explore
try to see Oh
happening in the back end so let's
quickly try to access the gy here 192
168 so you're accessing your your GOI
name node GOI admin on port five zero
zero seven zero so guys I know we are
short of time just give me two minutes
we should be able to see the setup here
okay so this is what will show you in
effect the cluster setup in total
configured capacities might that is it
GB and I have my to live nodes okay
two data nodes which are configured in
here whether I can go from here and see
what are all the nodes my DN one and my
DN 2 so 2 nodes set up for my cluster so
this is how you build a multi node
cluster and start working on this okay
so 2 2 2 2 2 2
Oh typically you can also run a Hadoop
DFS admin report you can do a HDFS DFS
admin - report which will give you the
same message or the same details as your
name node user interface now you may
have a question why we're accessing on
port 5 0 0 7 0 so what will happen here
is every process every demon has an
Associated port or an Associated web
service running in ok it's a small web
page or a JSP page runs associated with
each and every process so for example on
name node it runs on five zero zero
seven zero on your resource manager
it runs on it runs on both H zero eight
eight your resource manager user
interface can be accessed on eight zero
eight eight similarly your your data
node can be accessed on port number five
zero zero one zero so various ports are
associated with each of the processes so
this is what it is showing the same
report that I showed you from a name
node UI is what it is showing here my
configured capacity and my live nodes so
I have to live nodes D and one okay and
DN to the n1 here
and DN 2 so so this is how you set up a
multi node cluster and I think as part
of this I will show you how to
troubleshoot also I mean we got an
opportunity to look at the log files the
log files are always located into the
location where you have your hadoop
installed well you can change the log
location you remember in our Hadoop env
dot s which we talked about within that
file is what you can change the log
location and the process identifies
everything now to come to the course
there is a there's a batch coming up on
the weekend so for this 7th November so
we have it right 24 hours in total 24
hours module I mean over a period of 4
weeks 8 classes for 24 hours we cover go
index into it understand how to set up
how to set up lustre I think some of you
are asking how to set up a password
Leske as such everything will be showed
to you each and every step ok so we are
short of time right now so that's why I
cannot go into each and every topic but
as you mean I mean as we go through
these sessions so you can definitely I
mean be assured that we will definitely
show you each and every step and we will
explain to you how to troubleshoot where
to look at the log files where all you
can check and how to bring up an
instance up and running if they are not
coming up okay having said that let me
ask let me take some questions
I think a lot of questions came in it
okay so what is mean by name note format
well Mangena that's how you format
whenever you disk
whenever you format your cluster or your
hard disk that's how you do with the
format to create cluster on local
machine are we going to make changes in
the EDC folder say if you are creating
on your standalone mode no then you're
not going to make any changes to your
EDC folder
do I need to install my sequel as a
prerequisite no tsubasa you don't need
to install my sequel but yes if you are
doing with the cloud or installation
your my sequel is it important to two to
ten other questions I here is what do
you mean by client in host while your
client is again my client machine so I
have a separate machine configured here
where where I want to have it contact
with my cluster so the client is
anything any client which contacts to my
cluster does not DTC host find the
public IP address should come first
well it doesn't matter open it so you
can have the IP address just that
mapping should be there Swapnil is DN 1
and DN 2 a different machine that's
right so opening two different machines
how you're accessing NN 1 DN 1 10 nodes
from your single machine how will you
come to know in which h IP HDFS is
running yep that's again you need to
look into your course at XML shell har
sorry jaha Johar chuggy so oh you need
to you need to go with the or else you
can actually look into your look into
your name node UI and figure out where
your name node is running soap Nell I'm
getting confused with name node and data
no definitions can you explain data node
and name node with example well
obviously name node is a master node and
data node is this is a slave node name
node is where all your metadata is
stored data node is where all your data
will go into side that's your slave node
what you have to run em r1 and mr2 on
the same cluster well Nitin mr1 and mr2
on the same cluster is a is not a
requirement I think it's typically you
have MapReduce one applications written
in MapReduce version one which you need
to recompile when you run on MapReduce
version two but I don't think there is
any use case where you have both mr1 and
mr2 one running on the same cluster
because any application that you run for
MapReduce version one can be run on
MapReduce version two that's what you
call it as a backward compatibility how
do we know all what all convicts to be
set in different config files for
setting up cluster first time your first
time I think it's a it's an unwritten
rule you can say what all configuration
files you need to make so you need to
make sure that whatever files I have
have set or whatever properties have set
or the requisite properties where can
you configure standby node yes standby
node can be configured in your HTML site
XML you need to add a property DFS dot
name services which will indicate where
your standby node will be running how
can we access another cluster from
another machine
how can you access another clusters from
another machine typically I think your
question is how can you copy the data
between different machines so there is a
tool called disk CP which is a
distributed copy which will copy the
data from one cluster to another cluster
well no ma'am not format the whole data
in your disk well man gender the format
will dis the format name node format
will do what it will format this
particular location
okay so this particular location where
you are specifying where your storage
directory is where you want to write
your metadata only third disk or that
location will be formatted
what is the RAM and disk pays it has
been allotted a lot allocated to this
machine so well Samuel I have allocated
1gb RAM for each of the machines and
about 20 GB hard disk and we are using
native OS file system 1 or HDFS can you
use waste raw disks on your HDFS storage
well no no Nitin so OS should be
different so it's always good to have
you have multiple disks segregate your
OS from your HDFS so that there does not
interfere with each other guide me in
setting apart we are definitely bothered
you can enroll here will definitely
guide you as block size is configurable
then what will happen if block size is
less than 64 MB or 128 MB well neeraj
sure if the block size is less than 64
MB say for example if I have a hundred
MB file how many blocks will I have
going by 64 MB block size I'll have two
blocks one is a 64 MB the second one is
a 36 MB so don't confuse this block size
with your Linux file system block size
or your windows file system block size
because there is no corruption or there
is no padding happening for the rest of
the of the rest of your 24 bytes okay 24
MB so it will be 36 MB will be a block
size we haven't set up the replication
factor it is default to 3 no that's
right rajat it will be default to 3 as a
computer engineering fresher can I go
for absolutely sure you can definitely
go for this how can we gather some
sample data and process it yes I think
we are almost down the time guys so I
know I won't appreciate each and
everybody's time so just give me five
more minutes so we should be able to
wrap up do you mean job tracker and TAS
tracker dummy demons doesn't need to be
installed if you're an absolutist nail
life if you have if you have Hadoop to
running you have yarn running then yet
you don't need to install job tracker
and task record because it's all taken
care in the MapReduce again will be
whatever jobs you're submitting will be
intern ok run on your yarn framework ok
so it runs on top of unit it is inbuilt
into yarn
so when you start yarn your processes
your ApS also will come in so Krishna
I'm Joe our Java developer without your
experience can can this can take this
course and pursue kept solutely Krishna
I think developers will have far more
advantage because they can understand
the internal concepts right so if you
want to debug an issue because it's a
purely a java-based application so you
can understand main go inside and open
up your API s-- and figure out what is
happening so that's easiest way to
understand Rajat has what is the
replication that doesn't match with the
number of slave nodes will that okay so
yeah Roger that's a good point you
brought up so it's always good practice
to have the number of nodes in your
cluster should always be greater than
the replication factor if you go below
that you will have a problem there where
where the data if you have let's say for
example you have 3x replication and 3
slave nodes and 3x replication will make
sure what 3 copies of the data is copied
to 3 nodes and if you bring down one
node the master will try to replicate
one other node to some other node but
you don't have a sum of that node you
don't have three nodes you have only two
nodes so there it will get keeps it
tries to keep on copying it so good
practice is to have the number of nodes
should always be greater than your
default replication factor or the
replication factor you have set Leo
Hadoop version 1 demons are not running
in version 2 these are overwritten by es
absolutely early oh ok tu-tu-tu-tu-tu-tu
shredder how to get IP which you typed
in the browser that IP address is a IP
address you have allocated here so this
again again anything you need to go
inside deeper into this IP address which
I picked up to teach it how to manage
compute and how to take care of storage
with replication factor and what type of
switches open it up I think how to
manage compute and how to take care of
storage with replication factor yep so
you have you need to go
with the default properties so right now
if I don't use any properties the
default values will be taken the
replication factor will be 3 and the
block size will be when the block size
will be 128 MB so if you want to
overwrite these default proper default
values you need to explicitly call out
that property in the respective file
okay how can I add more node in the
existing cluster and can I use to name
node in one cluster
no man ginger you cannot have to name
nodes so when you say to name nodes it
should be 1 should be master and second
one would be a sorry 1 will be a active
node the second one will be a standby
node nearest yes if you have 50 MB block
size then yes it will occupy only 50 MB
on your on your on your actual cluster
if you have 50 MB then where will will
there be no there will not be any slow
processing thanks poet thanks for your
appreciate your words it's good how
about me 15 years in frustration
wonderful I think you beat my experience
Pune time just 12 and a half year
experience so it's good to know that
people are learning but yes it's a new
technology and you can absolutely learn
any reference book for admin yeah there
lot of books are available how many
nodes what configuration nodes oh yeah
will as part of this as part of the
course we do have one session dedicated
to the cluster configuration where we
talk about how to how to do how to
decide on what all what all nodes to
take and how much how can you define
decide on your your cluster capacity
planning the question is what is
zookeeper quorum orgy using any zuker
concept in haiku yes man gender
zookeeper is okay so zookeeper is what
is kind of a gatekeeper the reasons you
the zookeeper came into picture is
because we are talking about we are
talking about multiple nodes in the
cluster
and it's a distributed file system there
are chances that you end up into race
conditions where you have locks because
resources needs to be need to be
properly properly distributed across and
resources need to be shared across
multiple machines so to avoid running
into these race conditions and avoiding
multiple issues you have a zookeeper
implemented there so that's kind of a
gate keeper which will handle all your
client requests appropriately and route
them properly
oh I think the last question I will take
is where to route where to configure the
application factor the replication
factor will be configured on your HDFS
site XML TFS store application so guys
so hopefully I think this I will hope
this session is helpful for you guys
okay so try to try to I mean this video
will be uploaded soon after this in a
short while on to youtube so we can go
through that if you miss anything just
follow the steps I did maybe you can you
will be able to or you will be able to
set up your own cluster but there are
some steps which because of there are
some steps which I think I have done
before because of time limit in there so
so just try to go through that and you
should be able to set up your own
cluster any questions you can always go
back to a reach out to us or any queries
concerns yes the youtube link will be
shared in a short while immediately
after this session okay hopefully I hope
this is a this is helpful for you guys
please provide your feedback and also
try to go to the course reach out to our
support we will be able to guide you
appropriately on the course modules and
how it is being showed you okay so as I
said it's up so there's a batch running
on this weekend so try to I mean they'll
try to come as soon as possible cell
phone for you guys for us also okay
thank you and appreciate your time you
guys have a great night and a great day
talk to you soon hope to see us very
soon
they are definitely upon a trial I will
have support reach out to you okay
alright then so thanks guys and bye bye
talk to you guys very soon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>