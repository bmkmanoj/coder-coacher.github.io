<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Apache Cassandra Tutorial | Introduction to Cassandra | Apache Cassandra Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Apache Cassandra Tutorial | Introduction to Cassandra | Apache Cassandra Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Apache Cassandra Tutorial | Introduction to Cassandra | Apache Cassandra Training | Edureka</b></h2><h5 class="post__date">2018-04-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/y-ZqjhvFUhc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone hi I welcome you all this
is the ejectives and what are the basic
things that I will be walking you
through today we will have an overview
of what exactly Big Data is and how our
DBMS has comes up with the limitations
when we are talking about a very high
volume of data in there and then what
exactly no sequel comes up before I
believe I mean it's very important we if
we're actually jumping into casandra's
so it's very important that we have a
brief understanding of what exactly no
sequel is and what was the real need of
coming up to know sequel and what are
the specifications of a no sequel how we
represent our how we define that that
this is actually a part of the no sequel
stack now and then the cap theorem now
if it's once you guys have an
understanding of no SQL the basic
principle of how no SQL works is
primarily on the cap theorem the
consistency availability and
partitioning so these are the three
basic pillars on which every no SQL
works primarily and then which of these
three features are being extensively
used in Cassandra and how cassandra
actually behaves a response these three
features and what exactly cap theorem
says that's something that we'll have a
look in today and then probably we can
have before your Cassandra what is the
kind of structures and what are the
basic features a Cassandra gives us and
then we will have I will walk you
through the installation of Cassandra
for primarily for installing Cassandra
what we have done Erica is we have made
the life a little more easier and
simpler we have had we have a set of
installation documents that have an
existing virtual machine completely set
up with the required set of software is
already in place you just need to
download that specific virtual machine
and install it probably in you any of
your VirtualBox or virtual players
anyways I will walk you through when we
reached that point of time so until then
let's keep going
well once we the object you that is the
primary object you I mean but suppose I
mean if we have a conventional
I mean if we exactly we understand how a
conventional ID BMS works in a
conventional DBMS we always have various
sets of data stores and then those are
the stores we primarily refer to as the
databases and then inside the databases
it's probably it's primarily a very
structured we have various kinds of
relationships we will have tables in
there and then those tables will be
linked to each other are probably
Lydon's and then probably we have we
will have various unique identifiers
that define those relationships probably
based on the keys but that's a primary
key unique key or any other form of key
and then I will have those relationships
based on those keys primarily maybe a
foreign key or a unique key that defines
relationship between one or two multiple
different tables and then based on that
so but the basic key concept in every
our DBMS is that we always know what is
the kind of structure what is the kind
of information and what is the exact
format of data that we actually are
going to store in a specific storage
structure that's always a very basic
prerequisite in only our DBMS
implementation whether we talk from that
goes on from an my sequel store or to an
Oracle server for example that suppose
that we have an e-commerce application
and we have there is one of the features
is retail and in case retail we have
products so probably there will be a
products table for say and then from
products will have unique identifiers
probably a product ID and then probably
it will have a product name and stuff
and so on and so but the first the most
important thing there is as soon as we
actually define the structure in there
we primarily cannot or we primarily
don't change the structure that
extensivity obviously we can change the
table at any point of time but it's very
difficult especially when we have
different relationships first of all you
have to define a unique identifier
probably mostly the product ID in there
then we have defined the type of that ID
then same applies to the name that we
have specified and they are ugly it is
for example it suppose in case of name
it might be astraying
so that's a vodka in there and then if
it has any other attribute on the
description then it will have Valkyr or
text specified based on what kind store
we are using but in every form of every
form of an ID BMS we're always very
certain that what kind of data will
exactly store in there so the structure
of the table primarily does not change
it's it's mostly fixed and it's mostly
stringent that is something when we
actually have very high volume of data
and as soon now since the structure or
the behavior of the application the
interactions have have changed a lot now
now in case of an our DBMS we always
know that the data is always identified
in the form of rules or and these
specifications of the identifiers what
those rules are columns and the
combination of columns and rows
represents a table that is a unique
identifier for a specific set of rows
and columns and then probably we can
always have multiple number of rows and
multiple number of columns and those
columns were probably just like in in
our case just like the example I gave
you for the products of probably
products is a tableware and there might
be various different columns there might
be product ID there by a product name
and product description and then
probably the values in there will be the
rows and if ID in there and the rows is
the actual data that represents the more
valuable information inside that storage
but yes I mean a conventional our DBMS
is more optimal in a transactional
storage it extensively and in acutely
supports the acid the otama City the
consistency the data definitions most of
the basic principles of managing the
data is extensively
it's very enhanced but the biggest
drawback with any our DBMS is its
robustness its scaling up scaling up of
an our DBMS is always very complicated
and challenging because there's a very
different memory model that an our DBMS
plays and and every table has specific
set of logs and then there are a
different transaction announcement
systems the distributions and the the
operations how it writes to the actual
data
files in the backend internally it's a
very complicated process and so always
time-consuming and tedious and when we
have high volume of data it's always
very time consuming for NER NER DBMS to
actually specify these structures there
are that need to be written to files and
now since I mean as I told you that a
conventional possible storage since
there was now there's no limitation
there is no specific expectation what we
will be expecting now the when I say it
with respect to the data that we are
actually interacting with or the data
that we are actually talking with now
the expectations have changed now with a
diversification of the information
technology or the data management or the
data interactions
now the kinds of systems have grown up
there are various possibilities all of
the time where we always knew this the
data would always be in a specific
structure or then we have the concept of
semi structures there were specific
patterns or there were the specific
formats in which the data would be
actually specified then we have the
unstructured one now since the data has
actually grown extensively the industry
is actually becoming more stable and
stable and stable now
so things that are changing now and the
possibility of the structure of data is
relatively prime and visually getting
unidentified now now because since we
always want to make our systems more
robust and more optimal and acceptable
acceptable to change then susceptible to
failure so it's always very possible
that there is no specific structure of
the data that we go
we will have a predefined one or in the
free requisites so there's a possibility
that data might not have any specific
structure in place so there's a
possibility that we might want to store
the complete files in one go or there's
a possibility that we might not store
all the files in one go probably one
specific section of a file at once and
probably as we move on the structure of
the file might keep on evolving or might
keep on changing as we progress
or as the system grows on so I mean that
is why it makes more sense to have to
ensure that we're able soup to manage or
we're able to ensure that we have that
consistency in managing or we have the
acceptance or we have that leverage to
manage that kind of store where and we
don't have a very well-defined or very
predefined structure for managing that
very high volume of data or those on
structures are unexpected formats of
data and as we seem I mean if you see I
mean this is the this is the possibility
of the graphs now as we are going ahead
as we are moving up over the period of
times now now if the amount of
unstructured data is actually going on
the amount of unstructured data is
actually going is getting more as
compared to the amount of structured
data because since I as I told you that
everyone wants to be very robust and
very flexible so we cannot have a very
confined or a very very stringent set of
structured data inputs or data items
that will play around with so I mean
that's why the volume of unstructured
data moves on and now as soon as the
unstructured data moves on it's always
being identified as the one specific
term that's what we identify or what we
represent as the big data environment in
place in picture so I mean even though
primarily in Big Data there will always
be a high volume of data regardless of
what structure is what structure exactly
what is the structure of the data there
might be a possible structure or there
might not be a possible structure how
would the major or the biggest chunk of
big data stack is always the
unstructured structure since the volume
of unstructured structure is more and
it's scaling up it's a it's extensively
extra accelerating I mean if you see in
the past few years past five or six
years the volume of data is going in
exabytes that's what's the prediction
say I mean as per the predictions
probably by 2020 we might have actually
reached of approximately around fours
either by some unstructured store that's
a huge
of data and once we have that much
amount of data it's always very
important that we have we have places or
we have stores who are always very well
who are there to support or what they're
to manage those unstructured items or
unstructured sets but that's what Big
Data actually comprises off I mean since
no since when we have very high volume
of data we don't necessarily know that
how we will be processing so the
processing or the the manipulation are
utilizing the data is always very
complicated and very complex because now
since this is not something a
conventional store so there is no
specific set of predefined rules or
there is no specific mechanism to
process that data or to get more
something more information out of that
data and if you see especially if we
talk about the social media and the
possibility of the data can always vary
you can have someone for example if you
go in facebook or any social media app
the possibility of the data will always
vary there might be a person who
probably is pushing probably uploading
tens or thousands of posts on Facebook
every minute or second or then there are
people and the structure of that third
post can always vary there might be a
video there might be an audio there
might be a picture there might be a text
or there might be a combination of a
text or a picture there might be a
combination of a text or a video so the
possible structure of this store or the
data is actually it varies and same
applies to a meal for example if you see
something like Instagram where people
are always Instagram or YouTube wherein
there are extensive amount of heavy data
objects or heavy data items probably
like images or videos but people keep on
uploading them extensively
yeah probably per second I mean if you
see for hours there are probably
hundreds or thousands of videos that
everyone keeps on uploading per second
so the amount of data is actually
growing on the amount of data is
actually really not changing and the
structure of the data there is no
specific format there is no specific
structure of data in there and
for a conventional IDB Emmas it's almost
near to impossible to manage this much
amount of data because considering the
transactional structure or transactional
behavior of any our DBMS so amount of
time our DBMS actually requires to
process or to write or to perform basis
conversions for any specific data
formats
it's almost nearly impossible for any
our DBMS to take a look at that and do
the corresponding writes within that
data stored server now for Big Data
there are five V's or what I call as the
the five basic we've V fives the v5
engine I how I refer to it as that's
what the Big Data primarily talks about
whether the amounts the volume how big
data is actually growing out the amount
of data put in to the store or how much
amount of data needs to be processed
extensively is always very high so the
volume that's what the volume actually
talks about because since in big data
it's always about volume so always about
very big volume so that can be a
historical or real time that's always
different then the variety
now since primarily Big Data is mostly
talking about talking about the
unstructured stores so we never
necessarily really know that how exactly
are the data actually how or what kind
of data will be preserving in there so
this will always vary per the various
possibilities that this structure of the
data were always very there might be a
possibility that some might might be
only sending about structures to publish
a JSON objects there might be someone
else who might be just sending word
files there might be someone else who
buys who is just applauding who is only
talking about PDFs there might be
someone who's who's only talking about
movie files there might be someone who's
only talking about these the audio files
or there might be any other possibility
so the structure the possible forms of
data is always very varied it's very
diversified
that's what variety talks about and the
velocity now since we have an X
since you Marta for you so that means
there might be terabytes of data that is
actually being pushed in or that's that
needs to be managed that's where we talk
about the amount of data that's actually
already here now how the data is
actually being put into here so that
means there might be trillions of
records that are coming in to the store
or there are zillions of Records from
various different kinds of possible
store locations are actually being
pumped in or that need there that are
being sent to a different two different
or specific a data store that's what the
velocity talks about and that velocity
is always going up now since the amount
of systems keep on growing I mean if for
example if we just take an example of
Facebook and there was a time then where
probably they were under on probably a
10 million or 10 probably then it moved
on 200 million users then to 200 and so
on there was a time and right now it's
almost around 1 billion users 1 billion
hard users and imagine that if everyone
keeps on sending keeps on posting at
least once a day imagine how many number
of how much data is actually being sent
or what is the velocity at which the
data is actually being sent to Facebook
and how much of data needs to be
processed and how much of data needs to
be preserved in there no that is what
the velocity actually talks about now
since I told you that this is the
unstructured data so we exactly don't
know what is the exact possible way of
processing this data so there's always
there's always a possibility that we
might have lot of useful information in
there now the important thing is how
exactly we identify that how optimally
we decide that or how how smartly we
choose the best or the key and elements
are the key items for from a single data
set or multiple data set or across a
complete data store that we have and we
have now imagine that we have terabytes
of data and that that terabytes of data
is not just fixed it keeps on adding it
keeps on piling up since the it has an
extensive amount of velocity so there's
always someone who's always
pushing more data into that specific
storage so that means the amount of data
always keeps growing up and now since
its growing up so it's very important
that we don't just store the data for
for fun we always store data to get some
specific value to get some useful
information out of it that's what the
value taps out so it's very important
that we always identify or we always try
to leverage the maximum out of the data
that we have in hand right now that's a
very important thing and that is where
big data makes more sense that's why you
see a broad stack of tools and that's
why big data has been growing up that's
why there have been always law the
extensive amount of revisions and so
many smart tools that help us in
actually identifying what is the amount
of data or what is the optimal usage of
data that's already there or that's
present there but that's what makes
hurts that's what makes value very
important then when it becomes the
porosity porosity is primarily I mean to
ensure that and since we know that
there's always possibility when we
actually are when we have lot of
unstructured story so since there is no
concept of transactions in a high volume
store we don't primarily expect the
management of transaction so there's
always possibility that we might have
the same element multiple time or
there's a possibility that you might
lose a lot of data or this possibility
that there will be lot of loopholes or
there's there's a possibility that there
is law there's lot of inconsistency in
data for example but suppose if we are
actually if if we talk about a system
who keeps on sending his status
primarily every every second and let's
suppose we have ten tens or thousands of
those machines now since the structure
of every machine might be different
probably a machine might be as small as
probably a small microprocessor or or as
big as a ship now the structure of every
machine is different now but primarily
we do is we only have a DIF we only have
a single data store that's preserving
all the information that we are getting
from all the machines across the world
now the amount of data that we get in
there is it's huge it's her Mongols
right because we can have thousands or
millions of small micro processes and we
can have thousands or millions of those
big ships and the amount of data items
or data packets they send they will be
huge there will be huge amount of data
now what happens is now every time the
data is actually being sent there's
always a possibility now since when
you're sending high volume of data it's
a possibility that some of the elements
from the data gets lost while they were
being received while they were being
consumed or suddenly you do any of the
reasons probably connection failure
network wireless internet any other
possibility or probably that machine
going down at that point of time you
lose some information and while that was
being consumed by the consumer at that
specific place who's actually or that
specific sip system or application who's
actually interacting with that machine
fail laughs to consumer theater so that
means we were not able to receive all
the data from that machine right so that
is where you will get the inconsistency
in the data in there and then I mean
this is the biggest challenges we
exactly don't know what is what are the
possible what are the various
possibilities for those inconsistencies
a lot of possibilities now since it's
always a directly proportional thing
when we talk about the kind of devices
that we can have now since we there's
always there can be always n number of
possibilities for any possibilities in
the inconsistency in the data come
especially when we are communicating so
that's what the the five these of big
data are primarily talked about so the
volume the amount of data that we have
the variety the kinds the various
flavors of data that we have the
velocity that means the throughput that
we have for that data the value what is
the sensible data what makes sense and
what doesn't make sense in the voracity
that means are probably the
inconsistency that we are actually
getting in the destroyer well that's
what we are actually primarily talking
about now see from data data since I
told you that there is no unstructured
means I mean there is no specific
structure so there is no possibility we
don't primarily know whether we are
getting a string we are getting an int
we don't know we're getting an object
maybe we're getting an option right now
but
we're getting an object in there right
now we're getting a video tomorrow we
might get in you can argue in there so
the structure there's no specific
possibilities right so now there are the
possibilities there can be various
sources for Big Data right so I mean
there's no specific source or there's no
specific producer for for various kinds
of data or a huge amount of unstructured
data there's no specific store but
primarily I mean if you see we look
around so we can have various kinds of
stores I mean I I would say YouTube is a
very logical example because considering
now when we say the dam the amount of
data that we're actually sending in
YouTube actually does a lot of
processing extensively in a specific set
of times so it always makes sense to
give it to loop it in in case when we
are talking about the big data stores
now I told you that we are smoothing on
we're doing a transition from a
conventional I DBMS to a no sequel store
so but it's very big it's very important
what are the key component of I DBMS
that made us choose a new store because
why couldn't we why couldn't we optimize
something within the our DBMS that we're
at we actually really feel so that's why
it's very important first to understand
it what we have at hand and what is the
possibility what is the possible things
we are exactly this breaks off now I
mean the primary challenge with an
already BMS is we cannot store unstruck
we cannot have an unstructured
persistence within our DBMS we always
need the data to be very well defined
and the structure the consistency of the
data is very important then the scaling
of our DBMS servers is always very
difficult because the managing the
toppling of various servers it gets very
very costly and it's weights very it's
very difficult and there's a specific
limitation of every our DBMS if we talk
about MySQL Oracle so the amount of data
that we can store even within a specific
cluster and the amount of connections it
will be able to
open up or the amount of connections it
will be able to handle within a second
is is again very limited so there's
always limitations on scalability then
in case of the our DBMS the distributed
cache so I mean providing the extensive
reads that's always very difficult I
mean primarily I mean yes in case of
article its Oracle provides few feature
as well than that any other stores any
others are DBM store that we have in
there they don't primarily help us out
are to solve in that problem then the
biggest challenge with any our DBMS is
its cost
it's always very expensive our DBMS gets
very very costly so that is always a big
challenge in there so that's something
that always confines us off to ensure
that we try to part ways when we are
talking about putting huge amount of
data a la a large volume of data within
the IDB M is the managing of that data
that's always very difficult and then
when we are actually doing when we have
huge amount of data packet and that we
are that are actually coming in that we
are trying to write this we've always
seen that they're always the down
performance challenges with any our DBMS
even if even if that is an Oracle store
that will huge amount of write that
we're actually trying to do with our
DBMS it's always a big challenge it
always takes a lot of time a lot of
performance and Lord of a memory
utilization actually goes on dedicated
towards that area so that actually
impacts the overall performance of that
specific cluster for that our DBM was
stored then with sharding sharding is
always if we talk about the partitioning
in sharding that's that's right that
extensively very equal that's not very
smart because the biggest problem with
our DBMS is that it's a relational
stores so there's always lot of
relations defined in here so managing
sharding in there when we have high
volume of data that's always yes I mean
I don't say that sharding is not
possible and there yes
every bar DBA was told actually provide
lot of good features and sharding but
until a certain
extend until a certain amount of data as
soon as the amount of data actually goes
on it becomes very very difficult to
actually manage those and especially
when we have complex relationships that
are actually identified in there let's
suppose there is a user user has user is
linked to a company a company has lot of
different products and those products
have different set of clients and then
those clients probably might have their
own users as well so there a lot of
there's a tree change relationship
that's actually being identified in
there now if we actually want to short
the data at a specific user or a
specific company it becomes very
difficult because the amount of joins
that the server actually has to pick it
up from so becomes it's a lot of it's
very tedious and time consuming but when
we talk about Big Data the amount of
data will be huge so the the come number
of companies and there might be
thousands and then if we cross Sigma if
we cross multiply did those companies
with the the end users of their clients
it gets very high so managing those
things is always a big challenge with
our DPMS and if we try to break a cross
if we try to split this data across
multiple nodes within the servers within
that specific cluster and then if we
actually want to retrieve the data and
then when the the actual server actually
tries to retrieve the data from those
multiple nodes and accumulate all their
data it doesn't it's never at the best
performance in there but if we on the
contrast if we try to do a similar thing
from an unstructured soul
it's always very amazing it's always
very smart you will get an amazingly
extensive performance no this is this is
a sample use case and when we actually
talk trying to talk about the what was
the possibilities what were the problems
and that one of the organizations
actually failed to perform optimally but
where when they were actually trying to
do a similar amount of high volume data
within our DBMS structured store
Lemar with the data size the amount of
data was actually growing on extent
simply you know the amount of data
actually going on extensively and the
managing of that extensive amount of
data within in your DBMS is always a big
challenge in itself and then keeping in
order to have fast reads in order to
have very rapid Gries going on to the
database doing a database connection
interacting with the datastore
it's always considered a very very heavy
operation for a reason because it keeps
the channel open and it's always time
consuming now when you have millions of
users in there and who always want to
identify who always want to retrieve a
specific kind of data that specifies
only their needs so you cannot put you
you will not be able to put high volume
of that data you cannot keep on sending
sending those requests to achieve that
data at a rapid rate from an RDBMS store
that's always a big challenge and I
assume a similar problem was faced in
this use case and then you cannot keep
on adding nodes you cannot keep on
adding new instances within the cluster
its first of all it's very expensive
second managing and monitoring those
servers those individual instances
that's all that's always very difficult
and very complicated law DBMS is and
then if one of the nodes actually got
lost it takes a lot of time for the
alternate node to actually go on and
really himself and make that data sync
within it I'll give us information to
the remaining other nodes in that
cluster this thinking of a new node is
always very time consuming and very
complicated and then if we have a high
volume of right if there's an extensive
amount of writes as compared to the
reads
that's again always that's that's
another big challenge that comes with
this use case we are in your high volume
of writes millions of users actually are
trying to write their information or
write their data simultaneously at the
same amount of time and leave these are
the possible issues that instead pasted
at that point of time so now the
probability you see I mean there are
always possibilities that we will come
up with I mean there are always very
possibilities
that we can come up with solutions to
suffice with each but now our needs can
vary now if we consider the same use
case that we just saw on the front our
previous slide so as per them they have
a set of things that are add that they
actually require that they actually
exist or they're that they actually
expect to exist in the store that they
are actually trying to wear and they are
trying to preserve their data store one
is the scalability you should always be
very scalable it should be very easy to
scale up or scale down then it should
not run out of memory so that means it
should always be able to scale his
memory up and it it should always be
very easy to to enhance the memory of
the data store then the availability it
should be it should never be downs it
should ensure that now since I mean you
have millions of users you cannot do you
can't expect a downtime it's always very
important that you should have you
should be able to manage a very high
availability then sharding and then I
mean slipping across spreading the data
across multiple nodes and ensuring that
the partitioning has been done
automatically or the partitioning is
being done optimum ly
that's the others that's the another
feature that's that comes in they
expected list that fault tolerance I
mean let's suppose if there was a
feature if one of the doors actually got
down so what is the alternate what is
the solution or what is the alternate
way of solving that problem of how to
overcome that issue without giving a
downtime for the system or the cluster
and then shouting shouldn't create
problems I told you that there a lot of
complication that actually come come
across especially when you have huge
volumes of data and then the
interconnections in between those data
items are huge it's always very complex
and it's always very time consuming for
the clusters to actually perform the
sharding so to ensure that the
complexity those complex trees are
reduce that's rather expectation and
then
obviously everyone wants to make sure
that everyone wants that you you build
you build an optimal system at the
minimal possible cost it's always
cost-effective then the processing the
important the most important feature is
I mean when we are actually interacting
with the data whether we are writing
data or we are reading data
it's always very fast so it doesn't take
much time on the processing side that's
always very important now what exactly I
mean as an RDBMS
it's very obvious that these things are
not relatively with the Forte of a
conventional our DBMS that is what I
know sequels stored that's where no
sequel store actually comes in picture
and that's what no sequel actually helps
us out with no but with the convention
no SQL or a conventional unstructured
store now the first thing is that there
is no specific there is no specific
structure that needs to be followed so
the expectation the possibility is that
the data that are actually preserving
and I did in unstructured data store can
always change now so there is no
specific expectation on the schema side
so we don't need to predefined the
schema for any conventional store so the
structure can always change it can
always grow up or grow down now since
its keying or less so that means there
is no concept of relationships so there
won't be any relationships between
various set of data objects so it's very
important when we're actually working
with no SQL we ensure that we are very
smart enough to manage those relations
alternatively as we move ahead I will
explain how we can actually do that how
we can do a transition from a
conventional relational store to a no
SQL store and how we can handle those
relations how and where we can handle
those relationships and then I mean the
elasticity I mean of growing up or
growing down of the various nodes within
a cluster and the concept of sharding
how to divide or how to devise how to
spread the data across multiple shards
automatically without actually doing
without actually doing load of custom
operations or
management that's always a very helpful
feature that we're expecting and then
always the distributor environment the
distributor banishment off I mean
because now a since the basic feature of
the new SQL the basic expectation of the
no SQL was to manage high volume of data
so it's very very important that now
since a high volume of data cannot be
managed by one single man-machine so
that means it's always expected that
there will be always a group or multiple
set of machines bunch of machines that
are working together within a specific
echo system that we represent as a
cluster so that is what the distributed
environment comes in picture to ensure
that a one single source is actually
spread across multiple machines so that
it gives an effective performance in
place and then this scalability so I
mean there's a possibility that we might
be working on we might say deploy our
servers on Amazon right now then
probably we might do a transition on
Google Cloud or Azure or for say so the
transition is always relatively simple
and then it should always be scalable so
I mean we should always be able to
enhance the performance probably whether
that's the vertical pro enhancing the
performance vertically or horizontally
so that means adding more nodes to the
same cluster without affecting the
health of these the existing cluster or
scaling up in a way variance the amount
of variance if the velocity of the data
is actually increase that's the in
velocity or the out velocity regardless
of that the cluster is smart enough to
handle that much amount of floor that's
always a smart thing and then I mean a
seamless integration with the in-memory
stores I mean for whether we talk about
The Temper system source or the DES
caching what we identify is or reducing
the amount of expectations for
leveraging the caching features so
giving more optimum performance where we
don't primarily require much of X
caching features these are the basic
expectation
these are the basic features that we
expect an every new SQL now there are
various structures there are various
possible no SQL the structure of the
house destroyed the data how this
structure the data that always varies
there might be a key value store like a
map there might be a steward that stores
data in the form of documents there
might be a column based store or they
might be a graph store and then every
store every store has all the NOAA
skills towards always have a unique
identifier that is the index key or this
specific key that actually identifies
that gives the information on how the
data is actually being spread across a
board or multiple Lords and the
information about the information of how
how we can have how we can grow up and
that differs that depends there are some
stores who are optimum on the right side
then there are sorts who are optimum on
the read side then there are stores who
are optimum on both who have a balance
on both the things then there are stores
for optimal in managing relationships
then there are stores or more optimum in
managing the unstructured source so it
depends on the expectations and the
needs and they come out or the kind of
data that we try to preserve to
understand so it's very important that
first of all we should understand who
should be very well aware about our
needs and expectations before actually
choosing one before actually choosing a
data store for our needs or for our
implementations
it's sort it cannot be random it should
not be a random choice because there are
a lot of sort is we act as dynamodb
there is radius that is MongoDB there's
and there's elastic search and there's
Cassandra then there's the neo4j there's
Orion there's Titan there's lot of
stores that exist in the industry right
now for example in the in a key value
storage the key is is the is primarily
the Union defiers and then the values
actually
that actually specifies in here so a
based on the key so I mean it's it's
similar to a map the performance
I mean how exactly see the optimal way
of actually storing the data in here is
the one way is we actually store all the
data on one single Lord but the
challenge with that is knows no sense
right now we have only three three
values in here but now let's suppose if
if we are getting three hundred thousand
of those values per second so the amount
of that the amount of data that we're
actually trying to preserve in one
single store is since you know is is
whoremongers so what we do is we don't
necessarily tour all the data on one
single node we spread it across multiple
nodes within the same cluster and do a
more smart parallel processing in the
distributed environment so that we're
able to leverage the best out of every
node now on the document store now in
the document store there is their way as
possibility is we can have data in the
form of JSON we can have data in the
form of XML even though I mean the data
is primarily in the form of JSON that
you will see in there now in the JSON
there's always a key there's always a
key and fire and the actual data object
that actually represents the actual data
in there so the data in there might be
primarily a JSON or an XML format that's
where that's how a document in the store
works now in the column store column
sword is primarily Cassandra is is one
of the most smartest column stores in
there now in there the data is primarily
stored in in a bunch of columns as
compared to their worse rows that we use
to store so I mean all the columns they
are connected to each other what we
actually identify what we how we
represent them a bunch of columns always
represented by the column families and
then and that column family is always
identify it is being unique identified
by the rooky or the partition key the
partition key is always responsible that
the partition key is always
responsible to preserve the data across
multiple Nords within that specific
cluster to insure that how data and
which which row or which set of columns
will be going on which which specific
instance is being devised based on the
algorithm that that pod partition key
specifies and that's how a column store
works so that's how accounts short so I
mean there's there's column one it will
have ABCD and there's column 2 it will
have 1 by 2 and 3 so it has only three
different columns so that's very
important that's very smart so I mean
that is what I was trying to say so that
I mean there's a possibility that at the
columns for different elements might
differ so means it's like it's it's very
similar
how exactly how a table is our table
behaves in there however it's being
replaced in here at the problem in there
is that if if there are five different
columns within a specific table so all
the rows would have them would have the
values or would they will have the
information for all those columns that
are specified in there however there's a
possibility however in case of a column
store column based data store this like
Cassandra in here it's it's primarily
does not require to have any value for
that so it will probably be empty or no
for those columns that don't exist in
there and the remaining others would
continue to have so that means there
might be row DeCarlo families there
might be set of columns that have only
three three fields and there are there
might be another column family who have
who has 10 different fields so and there
might be and there's even possible that
all those ten fields might be completely
different and then the first three
fields in the first column family so
that's how a column store works now in
case of a graph graph is always is
always based on the Nords and how the
relationships actually work how the
relationships are being defined and then
the information some part of information
is being stored the node is the actual
entity and the know
we'll keep on growing and then the nord
label actually identifies the kind of
information that will be stored that
will be preserved on the specific nord
and the information on nord bill can
always vary and can always change so
know the node is actually the
information that actually exists in
there
the only difference of a graph store as
compute the remaining three stores the
key value document and column is that
graph actually stores the Ystad
principles it has better transaction
management as compared to the remaining
other unstructured sort so you can
actually store a lot of transactional
stores transactional data or the
transactional information on a graph
store there are various flavors we have
no neo4j we have Orient we have title we
have Cassandra's Cassandra graph from
data stacks so there are a lot of
possible features that we can exist in
there this is how a relationship looks
like this is the relationship
relationship from Nord
one and it has three different fields in
there that's ID name and age and then
there's another field there's another
node that has a relationship so these
are the attributes that are actually
being specified in there and then these
are the edges the relationship the edges
actually specify the information that
have that are linked to those things
that's how a graph database gets
represented in here now what are the
possible the use cases or what are the
possible things I feel how exactly which
stores actually get used more
extensively the key value stores and
column stores are primarily used
extensively alert because managing the
complexity and managing the complexity
and the performance wise it is always
very high and it's always very scalable
and then the least possible expectations
is always being expected from a graph
the graph oriented always has will God
give you the extensive amount of
performance like you will expect in a
key value for a key value store that's
always expected because since it always
maintains relationships in there but yes
obviously it still has if there's a huge
difference between the performance of
any graph store
as compared to any our DBMS any
relational data store so I mean since
considering the use case of Instagram
Instagram has primarily used column
based or Instagram actually started with
Cassandra the Cassandra was primarily
built at Facebook's it was there were a
few bunch of people now since the amount
of data actually started growing on
Facebook and there a lot of data
deadlocks that actually came in there so
in order to ensure in order to make sure
that Facebook has an extensive
performance up and running
that's why Cassandra was written
Cassandra was first written on Facebook
then it was actually made open source it
was made available on the open source
and then the Apache community was
actually there who was responsible to
enhance and who has been an effective
contributor and the open source
community has been there and that has
been helping us out in in optimizing and
adding new features to the Cassandra
store now if we talk about the
performance of any no sequel store as
compared to any our DBMS that we have in
there in picture or if we talk about the
no sequel stores as compared to within
them
now the operations that we are actually
trying to do I mean if we have if we
scale up now as soon as as we go out
nodes cassandra has always been a winner
has always been good at actually
managing a high volume of data and
extensive performance especially when it
comes to right there has not been any
other better performer in case of writes
when it comes to Cassandra that's a
similar example in here now if we
actually go almost to 32 lourdes 16 or
32 nodes you can yourself see the
difference you can see how many number
of operations will Cassandra be able to
perform it's close to 37 or 38 thousand
380,000 rights as well as reads that
Cassandra will be able to perform as
compared to Bongo a Mongo can primarily
perform only 50 to 55 thousand rights
nope Bongo has enhanced a bit as well
now with the mango tree in picture the
performance is probably gone up a bit as
well as compared to this slide now now
Cassandra now since the Cassandra was
actually true chosen so the best part of
our Cassandra is that that fault
tolerance has always been a very very
key feature of Cassandra it does not
have any single point of failure the
reason for not being a single point of
failure is that it does Cassandra works
on the gossip protocol so it does not
have any master slave concept in there
so there's always an advantage as always
gives a big edge since there is no
specific master in place so it's always
very easy that every node has
information of remaining other nodes
that are actually there and the second
best part in Cassandra it's completely
free and open source
anyone can come in and hop in and dig
deeper and and leverage the best that's
available in there and then the
scalability of Cassandra is very very
easy you can always add more multiple
Lords in Cassandra at any point of time
or you can downgrade the nodes at any
point of time without actually doing
Lord of complex configurations it sounds
will be very very easy the configuration
the management and configurations are
all administering the Cassandra cluster
is relatively very easy and then the
datasets are the high volume data's
whether we are talking about the reads
and writes rhythm Cassandra it's
relatively very very fast and very very
rapid you will not really see that they
won't be taken because how Cassandra
works actually it breaks off the actual
data into smaller chunks and then does
in the concept then actually performs
the operations or the micro batches so
the amount of processing is always very
extensive and with that it always gives
you a very extensive and very high
scalable performance in place and then
the reliability and cassandra store is
is always very very extensive and and
it's always and then the consistency how
cassandra actually behaves in case of
consistency whether we talk about the
consistency within the reads or there
is always relatively very very easy to
perform those operations in there now
Cassandra obviously certainly it's one
of the most popular snow SQL stores
that's there available in the industry
as of now and then the management of
concerned ISIL it's really very easy
it's primarily free even though there
are there is an honor price version of
managing and monitoring available with
data stacks but that's something that
has been built on top of the basic
feature that Cassandra itself provides
in and then the management of data the
high volume of data and the managing of
data whether that's reads or writes or
the managing of data is that's always
very easy within the Cassandra cluster
or across multiple servers within the
Cassandra environment itself then the
availability the possibility of failure
the fault tolerance and high
availability the a.cian DHA and FD are
always very good with Cassandra it's
very easy to manage both of those things
very smartly and very easily at at your
convenience the robustness
now since the best part about Cassandra
is that we can always dish Cassandra is
where you smile across our sharding and
data centers we can always have
different say data centers working
within the same cluster we can have
various servers that are pointing to s
single one data center and then there
might be another set of servers that are
pointing to another Center so I mean I
can give you a realistic example where
exactly this makes sense some time back
we had an application I was working for
one of our clients so whenever
application and there was a few over
servers were actually deployed in
Virginia and fewer servers were actually
applied and Singapore and suddenly the
data center stored that actually was
that was actually deployed in in the
Asian in Asia had one of the copies and
certainly one of these servers actually
got down so the challenge was that since
it was a Charlotte so there was there
was a lot of our sharding your
applications that was that had been
configured so that means
there was a copy of every other server
and probably one of the officers what we
had done is we had kept one copy in one
different Zorn that was indeed a central
one probably in Virginia and datacenter
two and Singapore so that one day that
it gave us was that we had there was no
single point of failure this was one of
the possibilities because since we don't
have everything within one single data
center so the advantage was that the
behaviors with respect to how it behaves
in Virginia would be different asking
with how it behaves in Singapore so now
that is something that is possible
specifically in Cassandra at the data
center level even though we can do it
even more granular at the rack level as
well and which rack of that data center
is this were actually being deployed so
so but see these things did the concept
data centers and the racks this makes
really more sense when we are talking in
the cloud environment because primarily
when we have the power whether that's
well that's a public network on a
private that book it makes more sense
and it always gives us more advantage of
scaling up within a different data
center across multiple across continent
or a cross that's clustered in itself
right so the advantage and the advantage
in there is that you can have probably
the performance of cassandra is always
very cold across multiple datacenters
because it works smartly tries to
identify which one is the closest one
inside that specific circle and based on
that it tries to send a request to that
specific Eurocentric or as compared to
going to the most farthest one this is
the basic one then I mean how exactly
the evolution of Cassandra actually
happened first it was developed at
Facebook of course the basic search
engine and then it was open source in
2008 then it went to the Apaches
incubation in 2009 then it went to the
Apache foundation in 2010 then it it
became quite popular and
then that's how the system the ecosystem
of the cassandra' actually moved on but
yes it has been a it actually there was
a huge impact of I mean how Cassandra
actually works this structure of
Cassandra is assumed or it's being
influenced that it was primarily
influenced by Google's BigTable and
Amazon Steiner I would say primarily was
BigTable because if you see how the
Cassandra architecture works that's
quite similar to how BigTable works even
though yes cassandra has done some more
applications primarily after after its
releases and it has evolved since it has
evolved a lot since then but the basic
architecture still remains the same and
then yes the p.m. the DynamoDB obviously
yes the Amazon has done a lot of work in
there as well so that has always that
has helped a lot in actually Cassandra's
evolution in itself so now I mean if we
if we just have we went through how
Cassandra behaves how how it was the
evolution what was the evolution the Big
Data how big it I well what makes sense
in what kind of store where do we where
doesn't make sense to have a key value
store where and does make sense to have
a graph store when the Baron it makes to
have column stores and then now it's
burden now then when we move on to the
evolution of Cassandra so it's very
important that we should know that what
are the most prominent features of
Katherine right so the most important
the feature of Cassandra is its rights
it's very very consistent in rights
cassandra is very powerful and rights
and then see the high availability and
fault tolerance it's very very smart and
high availability and fault tolerance as
well and then in the consistency it's
always very tunable and it's always very
reflects you in its nature and then with
the introduction of the cql as computer
to the previous thrift the sexual ascend
and it's very easy for anyone to
actually perform especially someone who
is actually doing a transition for man
I am a stork it actually makes your life
relatively very simple and very easy
then the data store it always has
it's always very flexible because since
it's an unstructured store and then it's
a column based so then the number of
columns can always scale up or scale
down and the behavior and the amount of
data or the kind of data that we can
store within those columns can always
change and then the distribution how the
sharding happens or how the partition of
data actually happens across multiple
nodes within a specific cluster can
always change as well the configuration
for that is relatively very simple and
easy and then the scalability now since
we can have various since we are
actually there's always a possibility
especially in low sequel it's a very
common thing we don't always start from
a tendril cluster we don't stop we don't
primarily start from a big I set up from
the first school we always tend to start
from small we always start a very small
cluster and as soon as the need goes up
as soon as the data coming in actually
increases the velocity increases the
volume increases the cluster spreads
across as well that's where this the
scalability is always a key so it's very
important that your system should be
able to scale horizontally as well as
work vertically right yeah so that is
the best part about the scalability so
it's important that we should know that
your store should always be able to
store horizontally as well as vertically
even though cassandra is primarily and a
horizontal scalable structure when you
have to keep on adding more nodes to
scale it up and then the cluster itself
will be smart enough to actually decide
which data will be going on to which
node I mean yes we can always do the
custom sets of to we Kings so with this
scalability structure it always gives
you a very high availability so there's
complete there's almost a hundred
percent high availability and there is
no downtime that's available in there
and then when it actually scales up the
read and writes the throughput for it
and write increases as well as soon as
it increases up as soon as the load that
chili jumps in the cluster at just that
point of time the amount of reason right
increases as well that's a very smart
thing because what cassava since in
Cassandra the best part in Cassandra is
there is no concept there is no master
and slave concept so every other node I
will be able to interact with every
other being that's available in there so
that means let's suppose that there were
only two different nodes in picture as
of now but as soon as we scale up theirs
if you add two more nodes if they were
actually expecting earlier they were
expecting only ten now they will be able
to perform twenty or thirty art because
the amount of node actually gets
distributed across at the amount of
lowered actually gets distributed across
multiple lords in there
then in the distribution now the
distribution is since the best part is
in essence there is no master slave
concept so the distribution is
relatively very easy there is no master
in there there who actually decides who
makes a decision to to understand to
ensure that which state I will be going
to which specific nord so it's since
it's a peer-to-peer so it's a gossip
protocol so as soon as the node actually
comes in here you get some data
yeah if no one feels it okay he says he
has actually enough data he goes and he
keeps on sending the remaining remaining
other data actually goes on to the node
two and without actually intervention of
any other nodes so there are smart
detectors that's that's what I call them
that is they're smarter every node is so
smart enough to make their own decisions
so that makes the performance of
Cassandra very high and very extends you
especially in a distributed environment
so the flexibility of distributing data
is always very cold so it always helps
us off in there now then the high
availability
now since the data that actually is
actually being stored at one store and
let's suppose that when we are working
in a crossroad environment so there is
always there is always a copy of that
data that's actually being replicated on
any other all that it node that might be
in within the same data center or across
euro center that depends on the
configurations or if we have done in it
tweaking sand
as if we have done any changes as well
and then in case of right Cassandra is
very effective in rights since Cassandra
stores a huge amount of extensive amount
of Rights its way it's very smart in
making right since it works on two-phase
commit so there's always there's always
attempts to rate and then it there's a
concept of batches where and Cassandra
keeps on going and writes keeps on
making the rights from in those batches
based on that and then the CQL this eq
was effectively introduced it was
upgraded from thrift cassandra mood by
mood ahead from the trip itself and CTO
is primarily very similar to any or DBMS
structure any or DBMS structure of curie
language so it makes relatively simple
transition for people for coming in from
not even national database store and the
flexible data store the best part is
that's always a good advantage of any
unstructured store whether you want to
store the data in a structured format
you want to store data in an
unstructured format regard like except
for the relationships at the possibility
of what kind of formats you want to
store and what kind of data do you want
to store and what there's a possibility
that you might actually change the
structure that you are actually storing
in there so those all these
possibilities all these flavor flav's
the behavior of the response of
cassandra to these changes and these
transitions is very very smart and it's
very very convenient to do those
transitions or to do those changes and
then the consistency consistency is is
primarily to ensure I mean how
consistent it actually performs the
rights or how consistency its performs
it performs the reads
so that actually depends I mean that is
primarily I mean what supports there
whether we are actually want to go more
on the availability side or on the
consistency side so now as I told you
now that actually primarily depends on
how we are actually trying to be here
how we are trying to behave
now writing on making sure whether it's
consistent or the availability so that
is primarily that primarily depends on
how exactly how the consistency actually
works in now that's that's what the cap
theorem actually primarily states on so
now see that's what I was trained so now
every I'm primarily every no SQL behaves
or responds to only two of the possible
structures at once it can have it can be
either z-- consistent available or it
can be consistent and partition or it
can be available and partition right now
based on a different noise no SQL stores
they respond differently to all of to
all three of these so I mean it depends
on the behavior of that no SQL store
just like at the big table DHBs Mongo or
radius they primarily work majorly on
the consistency and partitioning then
the dynamo DB or CouchDB they primarily
work on the availability and
partitioning and the our DBMS they
primarily work on the consistency in
availability they are not primary they
are not much on the partitioning side so
it depends now the consistency is
actually when I when I say I mean what
is actually consistency means
consistency is primarily I mean what is
the return what is the what is the most
recent value that was actually being
written whether that is in process or
that's actually being written and then
to ensure that the most recent write is
actually being returned across all the
loads so that means it might be that
even if that that node or that that data
is actually being just written or still
in that process so I mean to comes to
make sure that there's consistency
across all the nodes that's very very
important then the availability to
ensure that okay if the performance is
actually being to ensure that they be
the cluster the system is available 24/7
and all the operations are being handled
properly it's very very necessary that
everything is actually being responded
successfully without any failure that is
what they available that he talks about
and then the partitioning on tolerance
right so to ensure that a lot supports
know since the partitioning since
primarily the basic concept of every no
SQL is to manage high-volume data big
data and so big data is not majorly
deployed on one single load we always
have a bunch of ports a cluster in
itself and all together so now the devil
now since the data is actually being
spread across multiple loads in there so
it's always possible that a copy of one
one node there one node might not have
all the data and that it might have only
a chunk of that data and the remaining
remaining chunk of that data might be
available on the remaining other nodes
that means suppose if there are ten
different records and there are three
different node this node one node two
and three so there's a possibility that
node mine might contain four records and
node 2 and node 3 might contain the
remaining six records now somehow let
suppose that node six Lord three
actually got down so to make sure that
in case even in case of errors we or we
ought to make sure that we are still
available we are still able to handle
that failure and the data that was
actually partitioned to node 2 and node
three we don't lose that data that is
what the partitioning actually talks
about in case of failure or in case of
tolerance now these are the various
possible these are the major possible
things where in cassandra is actually
being primarily used in so I mean in
spotify is primarily for availability in
the product catalog primarily for at the
high availability across since it works
across multiple layer centers and then
the even bride is from mainly
on the read and write capacity because
it works on high available to read and
write and and they have NREL it's
primarily on the high volume since the
volume of data on that Cassandra can
perform on can always be very high and
very good so that's always a very good
thing and then comcast is again on high
availability and then a similar T it's
again on defensive measures to ensure
that the security means security
management's I mean managing security in
Cassandra is simply very easy so there
are various things that you can do to
ensure that you don't actually lose data
and that easily already there's no data
compromised any point of time there are
a lot of companies who have been using
Cassandra is honestly the most popular
no SQL stewards the Lord of big
companies actually prefer it and and it
makes sense as well and especially in
the recent times as we have actually
gone up as the data is actually moving
up so lot of people are actually
preferring to choose data these are some
these have been some problems that
Netflix are actually faced when because
Netflix actually use a lot of Cassandra
Schwartz so I mean there's there were a
few issues that actually Netflix has
faced earlier now that that's where
exactly that's the reason why Netflix
actually went on with Cassandra to
overcome those challenges especially
with the consistency the data integrity
and the more optimum ways of actually as
restoring and backing up its data stores
at any point of time very smartly and
then it's the best part is Cassandra is
chief it's almost for free because since
there is no specific info there's no
specific cause that actually way that's
being incurred by Cassandra other than
the installation so it's always very
easy and the scalability of the
scalability of
okay sunrise it's very easy and it's
very convenient we can always scale up
and the best part is across multiple
regions we can have a single cluster we
can have a multi region cluster
supported in Cassandra not not a lot of
what many NoSQL stores provide that's
kind of configuration and that actually
makes a smart thing because that reduces
actually the possibility of failures
default tolerance and the high
availability actually make a lot of
sense in here Netflix is actually being
used using Cassandra extensively the
number of clusters that they have
actually that they actually have
deployed in production is large it's
almost three or clusters and then how
many regions they're actually spread
across almost 12 months of regional
clusters and and and every region at
least has three clusters and the the
terabytes of data that's across clusters
is almost 65 terabytes and they're
almost four on the SEM for 70-yard Nords
so that's a huge amount of data and the
and the most interesting part than this
is the number of writes that are
actually being made the number of writes
is primarily more than number of reads
there's almost 250,000 writes that
Cassandra is make as I told you earlier
as well that Cassandra is more stronger
and writes than done reads so when we
have higher amounts of writes as
computer reads
that's where Cassandra makes more sense
I mean yes certainly I don't say that
the reads are not that extends you but
Cassandra gives a hundred percent
guarantee on missing a write and that is
a big thing because he reading data is
not that not that challenging yes the
performance makes make sense but losing
the data that actually is coming into
your store is always very difficult if
you lose a single packet of data that
means that can have a huge amount of
impact you don't necessarily know that
how and when you will be used utilizing
that data unless you exactly know the
kind of structure and because we don't
know who might be actually getting that
data input forms or any other external
source
third parties chores and we might not
get that data again so it's important
that we don't really lose those data
stores at any point of time this is
Netflix's architecture of Netflix is
actually being used so there is an there
is a specific memcache that actually
being a that's the that was that's what
that's what I was talking about this is
the intermediate layer for the
integration for Cassandra for actually
storing bits and in memory store I
bought a temp store is is relatively
very easy well that's all for the basic
introduction and the basic features that
we have in Cassandra what all things and
what why Cassandra makes sense now I
will try to walk you through in the
remaining part in session I will try to
walk you through on the installation of
the virtual machine as I told you
earlier that for your convenience and
for your ears we've ensured that we have
set up all the software's in a single
virtual machine you just need to
download that virtual machine and use
the same image copy in your any of the
virtual boxes and you are good to go
okay so this is the installation guide
so I mean you might actually be required
that the the virtual machines that
you'll actually be that words that will
be supposed that we are supposed to
install so I will explain you the
expectations I will explain to you what
are the basic steps that are required to
do the installations and to perform
these the basic levels of operations and
the guidelines that we need to keep in
mind while we're actually doing the
installations but before that I would
like to have an overview of a brief
brief summary on compiling up what we
learned today what is Big Data
so I mean the data a high volume of data
that is structured that's unstructured
or that semi structured that is what
primarily comprises of Big Data and then
the possible available stores I mean one
of the most problems with the possible
things the limitations of our DBMS we're
and our DBMS was not a is not really
able to surf
the high volumes of data and it's not
able to primarily manage high volumes of
data that's why and you saying came in
picture any of some analogy of which as
no SQL where and we have a lot of these
a bunch of important new features
because see now right now it's primarily
more on the data side then on and then
on the applications I know since lot of
systems have already been placed they're
already being set up so now the most
important part is that our application
is strong enough and then the
application data that we have in place
we are able to manage that manage the
data and the sense with the number of
users adding up to the existing systems
that we have in place and that always
piles up to the existing velocity as
well so that's why the that is what
that's where the evolution of no SQL
came in picture and certainly we have
various flavors of no SQL we have
aerospike we have MongoDB we have
elastic search we have Cassandra we have
graph databases we have dynamo DB we
have cow shreddies react so on and so
cool it's a pretty long chain and then I
mean Cassandra Cassandra certainly a
documents is is primarily a column base
data store where in the data is
literally stored in the form of columns
and the bunch of columns always refer to
as column families and then a major
advantages of Cassandra is its high
availability and durability
the scalability of Cassandra store
Cassandra stored within a specific
cluster and that cluster can always
expand itself to one or multiple regions
simultaneously at same point of time
cassandra behaves smartly in case of
across various data centers it's always
it always has a very effective way of
performing the writes and then in case
of consistency and durability cassandra
behaves very smartly then when we talk
about the cap theorem the most important
theorem when we primarily talk in case
of no excuse cassandra is phi merrily
more on the consistency as computer on
the availability so it's very important
that no single store no no SQL stuart
will be able to give
all the three features at the same point
of time so it's always a trade-off
between consistency and availability so
that means if there is a data store if
it does a data store a cluster of
probably three different nodes and
probably if there was there were
multiple there were two two different
writes that actually came in the same
place and there was a read request one
of the nodes at the same point of time
let's suppose and suddenly while these
three operations were performed by these
three different nodes two of the nodes
got down so now the responsibility of
that one node is to ensure that how it
actually will perform how it will be
able to make that right how we'll be
able to make a decision between to give
a consistency in the writes or to make
sure that the the availability is being
ensured so that means to make sure that
do we need to lose those rights or do we
need to make sure that the data that's
already available in there that needs to
be responded back smartly so it depends
even though I mean that depends so we
can always do a lot of tweaking and we
can always do a lot of configuration
setups that are required to make those
decisions behave the way we want them
yeah for future references there are few
tutorials and few useful links that you
can always refer to and there are a few
standard questions centered Q&amp;amp;A is for
the basic based on today's topic that we
have that we have deployed in here so
probably you can refer to this link and
probably just have a look at your
knowledge what you have learned till now
that's primarily what we have in place
now for installation there is the first
thing that you require is the downloads
be VirtualBox for downloading the
VirtualBox
you can refer to this document that's
mentioned in here and for that you can
go to this specific URL and you can
choose whichever actually applies to you
whether if you're using at the windows
one if you're working on Windows you can
download the windows installer for if
you have Mac you can download the dmg or
if you're working on any of the Linux
platforms you can choose which
dbn app applies to you it's a 32 GB or
32 bit or the 64 bits based on that once
you actually have downloads at the
VirtualBox then the installation
actually specifies the installation
guide is relatively simple it's a it's a
very easy guide this is actually the
same URL that I just pointed out too
once you have this downloaded you can
follow up the wizard to do the
installations and as soon as you go up
you can change the location on which you
actually want to install this location
this is primarily the look and feel
after Windows installation so the
whichever one actually applies to you
you can choose whichever one but
primarily the look and feel of the
Installer would be relatively similar
across multiple platforms once you have
the installation complete this is the
look and feel window that will be
actually defined to you that will be
available to you and as soon as you do
the installation it will ask you for
going orange you can choose now the next
step that you really require is go and
downloads the casandra's vm that erica
has created you can click on the link
that's actually been specified and as
soon as you click on the download link
it will download the corresponding
virtual machine for Cassandra that's
required in there once you have
downloads the corresponding virtual
image in there then you can go to import
alliance and as soon as you go to import
alliance you can import you can yard
stat in here you'll ask you you can you
can appoint the application to that
specific location for that file you can
use the same wizard in there once you
have added then you can add the specific
configuration that are is required in
there with respect to the network and
stuff once you have specified in there
then you can start the corresponding
VirtualBox in here make sure yeah so
this is my virtual box this is the
install one that I have in just install
so now if you click on start it should
start the casandra's vm
so it's going in an installation more
it's going into the startup board now it
forwards the Cassandra this has a Centro
s within itself this is Linux is
VirtualBox machine so it for Lords the
Linux server for you Cassandra is
already pre-installed on this so as soon
as it's as in Lords up it will it will
pop in video that this login window that
India with the 4 they tied you break a
user where in the password is ID Rica as
well see it has its yet to start as soon
as you login here you will be you will
be able to learn orange to the desktop
screen for the for your VirtualBox
so that means you first need to install
the VirtualBox you need download you
need to install it once you have
installed then you can specify me login
ID Rika once I log in the desktop for my
virtual machine will be displayed and
people who are already familiar with
Santos I'm sure if we primarily you see
it won't be that complicated for them
yeah so I'm logged in to the screen now
for actually going now for actually
going in and traversing through the
remaining else the remaining set of
features in there we can go into to
terminal go into terminal C cqms such as
the shell command for loading ins DC QL
now since the Cassandra is not started
so we go and we starts the Cassandra in
here using the Cassandra c'mon so it
will start the Cassandra for us
and as soon as the cassandra is started
with the cassandra start
as soon as the sunrise tartan you can
lord the cql SH short the cql so we're
logged into the CQ l cluster now if we
logins to the CQ l cluster so we are de
l'argent to the CQ l cluster now
yeah let's close that for now now as
soon as an axe evolved in now since
there's only one single Lord that's why
it actually shows you the Nords the name
of the Nord the URL in here and it shows
on but since by default Cassandra works
on the ring topology on the gossip
protocol so all the nodes that are
actually available in there in that ring
it will try to link it will try to
interact or if you try to connect to all
of the nodes is available in there but
since there's only one single node in
there that's why it finished in finished
joining but it tries to search it off it
tries to find out if there are any nodes
in there these are the set of tokens
that actually are already specified or
that are already defined in there so
these are the tokens based on which the
partition key is actually get created
for those column families that we have
specified and then it shows you the
other thing that actually shows you in
here is the rituals what is the
information about the Cassandra and what
is the information about the others the
other related information that we are
that we are supposed to understand
Cassandra's version in here is 3.13 dot
dots one and the thrift is 20.1 TRIF is
actually the the top-level Putra
protocol that is being used that
cassandra uses for text whole
communications trip is a protocol in
itself and it in order to use that
protocol we cassandra makes use of the
thrift api the sexual works on top of
that cql is primarily to give you a more
user-friendly and so cql cassandra
disgust and a package bundles in with be
sexual as well in here and the sequel
version for this is 3.4 now since by
default if this works on a native
protocol that's why actually lords at
the basic protocol in here by default it
tries to lower all the column families
that are available in there there are
three available in there so since we
have no
created any column family that's not
that's something we'll be doing in the
next upcoming sessions but as of now it
loads all the existing column families
that we just saw in the key that we just
went through previous terminal that I
just closed off when we had actually try
to log in and then these are at the JVM
arguments that Cassandra actually loads
in so there are a lot of since it
actually does a lot of work within a
memory so it's very important that we
actually allocate a huge bunch of a big
chunk of the end memory in there so that
cassandra is actually able to perform
any any linear levels of operations that
are required in there because when while
actually I told you that
Kisaragi performs two phase commits so
Cassandra doesn't actually really good
and right into the data source in one
single core it actually makes a right in
there and the first and then based on
that it actually makes a it actually
starts writing data first into a temp
file into log and then those log files
are actually being moved on to the
actual permanent source that is how the
BigTable used to work and that's symbol
that's how that's very similar that's
very identical how that's what cassandra
has adapted from BigTable and that self
cassandra actually tries to work
Cassandra has a configuration file
sandra not llamo and that actually has
the list of all the files that are
actually being created in here
okay okay let's schools is off so now
closing the instance back off you can
power off the machine you can close this
instance off you can close the
VirtualBox off now there are a few
settings that you might require for it's
primarily for the network ones in case
you might want to choose whichever that
network is applicable to you based on
that you can actually specify whether
it's if it's a bridged adapter in case
you what is the bridge adapter that you
are actually using or if you're using an
ad network then you might actually have
specified since I don't have any
networks that networks available in here
that's why it doesn't actually tell me
to specify in here because by default a
lot of times what happens primarily if
you are using Linux environment so the
the network is majorly disabled on the
virtual boxes in the Oracle one so you
won't be able to access Internet in
there on your world cell boxes so you
might have to explicitly configure that
on your own so that you will have to go
in there in the settings it's worth
those settings are all aboard then there
are a few more settings that you can
actually specify in there even though I
mean audio is not required in your case
it's primarily the ports and networks
that you might require if you want to
copy any stuff in there then you have
the shared folders I mean if you
actually want to share anything from
your local machine to the VirtualBox
that's where that shared folder is
actually about this shared folder
actually takes care of that information
in there that's all for today thank you
I hope you have enjoyed listening to
this video please be kind enough to like
it and you can comment any of your
doubts and queries and we will reply
them at the earliest do look out for
more videos in our playlist and
subscribe to Eddie Rica channel to learn
more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>