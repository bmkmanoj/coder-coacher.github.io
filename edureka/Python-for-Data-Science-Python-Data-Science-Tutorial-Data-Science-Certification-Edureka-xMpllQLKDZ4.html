<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python for Data Science | Python Data Science Tutorial | Data Science Certification | Edureka | Coder Coacher - Coaching Coders</title><meta content="Python for Data Science | Python Data Science Tutorial | Data Science Certification | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python for Data Science | Python Data Science Tutorial | Data Science Certification | Edureka</b></h2><h5 class="post__date">2018-03-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xMpllQLKDZ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey everyone this is from at Eureka and
this session we'll be learning Python
for data science now Python for data
science is definitely the next big thing
in industry and a must learn for
professionals in the data analytics
domain so let us move forward and look
at today's agenda so we start the
session by first understanding the need
of data science why there is sudden need
of it and then we'll understand what
exactly is data science after that we
understand why python is preferred or
used for data science over the other
programming languages once we're done
with that we'll directly jump onto the
hands-on part where I'll help you guys
with data manipulation that is how you
can load data analyze data and perform
some of its operations with different
libraries of Python
after that we'll understand and
implement machine learning using Python
and finally towards the end I'll be
doing a complete demo which includes
various libraries and using an algorithm
of machine learning we'll explore more
about data so I hope you guys are clear
with the agenda so let us begin with the
very first topic that is the need of
data science so traditionally or you can
say earlier thank the data that we had
was mostly structured and small in size
which could be analyzed by using the
simple BI tools but today most of the
data is unstructured or you can say semi
structured so by these terms I mean the
data isn't unorganized format and a lot
of data is being shipped read let's say
from a multimedia your text files your
sensors and even from an instrument so
if you look at today's science the data
is actually getting created faster than
you could ever imagine so let's say you
are traveling one tonne and you are
using Google Maps for navigation so
there is a lot of data being captured
through satellite and it is transmitted
real-time through your handle devices so
this basically tells you whether the
traffic is more or less in one locality
where exactly is traffic jam which road
you should prefer and all these things
so there is a huge amount of data which
is generated but the problem is we are
not making correct use of this data so
earlier there were no decision-making no
predictions were made about the data
more money was also spend and hence
we've not able to generate insights out
of the data but now what data science
has enabled us to do is that we are
moving from an approach wherein we have
the data which tells us a situation or
what exact
situation was and then how you can gain
insights from that data so now if we
look at the image here earlier we had
only structured data which is in the
organized form like DBMS or a DBMS or
Oracle but today we have both structured
as well as unstructured data which is
unorganized so we need to deal with that
as well then when this data grew a
bigger earlier we used to store it in
the database house where we had a
concept of historical data and
transaction data separately so data
warehouse is basically a large pool of
databases but today we have heard now
why because first of all now we have
tremendous amount of data which cannot
be stored in a single secondary device
it has to be distributed multiple
components should be there it should be
stored on cloud as well so that we can
access it in faster ways then in
traditional BIA acts on data warehouse
and generate some reports or you can say
graphics that is nothing but your
predetermined reports only so
month-on-month will display same kind of
reports and it does not go beyond that
but now what has happened we apply
various data science algorithms we just
not do simple reporting but we also
explore the data we visualize the data
and then we can build a story out of it
we can also perform some scientific
discovery based on that data as well so
that is how our data science has made
our life so easy now let me give you one
more example so how about if you could
understand the precise requirements of
your customers from the existing data
let's say the customers pass browsing
history or the purchase order or the
purchase history age and lets's income
as well so no now to add oils data
earlier do but now with the vast amount
and the variety of data that we have
been receiving you can train your model
more effectively and document the
product to the customer with more
precision wouldn't be amazing guys as it
will bring more business to your
organizations so this is where the data
science is actually needed now that
you've understood the need of data
science let's understand what is data
science so data science is known as
data-driven science which makes use of
scientific methods processes algorithms
and systems to gain insights by
discovering some hidden patterns from
your raw data so data science has been
primarily used to make decisions or
predictions based on your data now data
science is being applied by the data
scientists
let me discuss the roles of data
scientist as well so first of all data
scientist is responsible for designing
and creating processes for your complex
as well as your large-scale data sets so
these data sets are basically used for
your modeling your data mining and for
your research purposes as well so a data
scientist is involved in processing your
data cleaning your data verifying all
the integrity so data for your analysis
purpose and all these things are done by
data scientist also the major role of a
data scientist is to build predictive
models using machine learning algorithms
so don't worry guys we will be
discussing all about machine learning in
a further slides but for now I hope you
got a basic understanding of what data
science all about so let me discuss the
life cycle of data or what are the
phases data actually goes trough so a
data scientist provide a one-stop
solution to all these operations these
operations are discounting data
preparation model planning model
building or cache lies and then
communicating the results so let us
discuss all these operations one by one
so the first two steps are data
discovery and data preparation so in
discovery what you need to do you first
need to understand all the
specifications of a data you should also
know the requirements you should know
the priorities and the required budget
for the same you should understand each
and every aspect of data where you know
data is in which form how much data is
interesting format or how many numeric
values in there and all those things
then what you do you basically prepare
your data or the second step is data
preparation so in this you basically
ensure if there are any null values that
has to be removed or replaced with some
other dummy values or if there are some
of the data which is not relevant for
your analysis so that can also be
removed further you can also perform
easier that is extraction transform and
load and so you need to explore pre
process and condition your data prior to
your modeling path so third is your
model planning so hearing you will
basically determine the method and pick
looks to draw the relationship between
the variables so in this face you will
basically identify the type of algorithm
which we'll be applying to data and then
plan a model accordingly so this was all
about planning next you actually have to
build a model so here you will develop
data sets for training and testing
purposes so you will analyze various
learning techniques like classification
Association and clustering to build your
model and finally you operationalize the
data wearing you will deliver the final
reports briefings codes and all the
technical documents so as everything is
done you can then move into the
production so what you need to do so
afterwards just run they'll go them on
the data and communicate the results to
our end-users so what you're going to be
communicating is that what are the
insights you got from the data what are
the stories that you have found what are
the for the predictions
what is the forecasting based on the
data science that you have applied so
these are the steps which are there in
any data science project so if you pick
up any project from any domain this will
be the steps it will go through next we
have different programming languages for
data science we have our programming
language we have sequel we have Python
we have sass and we have many more like
that but here python is the most
preferred choice for data scientists
among all of these so let us understand
why Python for data science so python
has numerous feature so let me discuss
one by one so first of all it is simple
and is easy to learn so it's a very
powerful language and closely resembles
your English language now furthermore in
Python you don't have to deal with
complex index like you used to do in
Java or any other programming language
next it is fit for many platforms so
Python allows you to perform cross
language operations seamlessly so Python
is supported by most of the platforms
ranging from your Windows Linux
Macintosh solar is and many more like
these next it is high-level and
interpreted language so by high-level I
mean one does not need to bother about
low-level you can simply write your code
in English and Python will in turn
converted into low-level details then it
is interpreted or you can say a Python
is a scripting language as well it
basically translates or runs your code
one instruction at a time it is also
less code intensive as compared to any
traditional programming language then
you can perform data manipulation
analysis and visualization with Python
so for animation we use libraries such
as numpy and pandas we'll be discussing
these libraries as well and for
visualization we have matplotlib
Seabourn and many more like things but
these are the most common ones that we
use and finally Python comes with
powerful libraries for machine learning
applications and other scientific
computations so using Python you can
perform various machine learning
Gotham's but small and easy syndics and
you can gain insights based on the data
so this was all about Python and why I
should actually use Python for data
science next moving ahead with Python
let's understand data manipulation so
data manipulation is very with extract
filter and transform a data quickly and
efficiently so data manipulation can be
performed using libraries such as numpy
and pandas in python so let us
understand these two libraries one by
one so let us talk about numpy first so
now pi is the code library for
scientific computing which contains an
array object ID also provide tools for
integrating of c c++ as well so what
exactly is a numpy array now numpy array
is a powerful in dimensional array
object which is in the form of rows and
columns so we can initialize numpy eros
from nesting Python lists and accessing
elements now first what you need to do
you need to install this library so for
that it is very simple guys you just
need to go to a command prompt and type
in condign install numpy I have
installation will be started so in my
case I am using anaconda so if you're
not using that you can simply go to
command drop and type in pip install new
pipe now once the installation is
completed what you can do it and go to
your IDE let's say a pycharm
Jupiter or anything so in my case I will
be showing you all the practicals in
Jupiter notebook and for that I'm using
anaconda so you can use if anything else
as well so once everything is done what
you need to do just open your notebook
in your case it can be any ID and type
in import numpy and then you're ready to
work with number arrays similarly we
have pandas so pandas is built on top of
num pipe so it is used for data
manipulation as far as your analysis
pandas is also very suited for different
types of data such as your tabular data
your matrix data then your observation
or you can say statistical data and many
kind of heterogeneous data as well so
the installation process exactly same as
that of non pipe you can go ahead and
type in condensed all pandas and if
you're not using anaconda you can simply
use tip and finally go to our IDE and
type in input pandas so let me open my
Jupiter notebook and let's implement
some of these operations so I'll go to
my item I'll say to Peter notebook
so this will take some time and it will
open it in your default browser
as you can see here my default browser
has been opened so I will just delete
the previous ones and as you can see
here my Jupiter is running by now
Waterloo I go to new and I click on
Python 3 so here as you can see this is
my Jupiter notebooks we can give any
title to a notebook so in my case I say
basic operations and rename it now first
let us discuss few things about numpy so
I just make this as header so in Jupiter
you just need to go to markdown and you
can run this so this number is in my
heading 1 now I have to perform some of
the basic operations so as we've already
discussed Python stands for numerical
Python so python can also be used as an
efficient multi dimension container for
your generate data and what exactly is
multi-dimensional arrays with me just
print a single dimension array as well
as multi-dimensional array so for that
first you need to import numpy so what
I'll do i type in import numpy as NP and
i have just run this so basically inject
a notebook to run this you just need to
click Shift + Enter or you can simply
just click this button so I have already
imported numpy so this is not giving me
any errors next what I need to do I need
to print a single dimensional array now
for that what we'll do I type in a is
equal to NP which stands for your numpy
and then I'll type in array and let's
say I am giving some values let's say 1
2 3 and then I'll print a so I'll just
click on Shift + Enter just to run this
and if you see my array has been printed
so this is my single dimensional array
now whatever I want to print a
multi-dimensional array which is in the
form of rows and columns so what I'll do
you just need to go here or let me just
take another cell I say a used to NP dot
array
here I'll say 1 2 3 and put comma and I
specify the columns as well it's a four
five six and then I'll bring the a and
I'll say shift enter to run it and as
you can see here I have a multi
dimension array which is in the form of
rows and columns so I hope you got a
basic difference between a single
dimension array and multi-dimensional
array next let us perform some of the
operations on these numpy array so let
me just type in here number operations
I will make this code to markdown so
that it will come in my heading even if
you want to put in headed to what you
can do again specify one more hash and
then you can just run it so this is my
header one and this is my header - so
hope you go out the difference between
header 1 and heading 2
next let us talk about some numpy
operations so the first operation that
I'm going to discuss is end M so what
this function do it basically finds out
the dimension of the arrays so we have
now just performed a budget dimensional
array so what I'll do I just copy this
code over here and then in spite of
printing a what I'll do I print the
dimension of it so we know that it's a
multi-dimensional array so let me just
run this now so as we have just seen
here it is the two-dimensional array so
it is printing me - next what you can do
you think print the size as well so I
just copy the same example and I say
item size so this is basically print me
the size of each element present in the
array so let me just run this so as you
can see here it occupies 4 bytes in the
above numpy array next I want to know
the data type of this numpy operation so
what I'll do I'll copy the same thing
and I'll say D type so this will
basically pays me the beta type of the
adder so I'll just run this so as you
can see here the data type of the array
is integer and it is 32 bits so I want
to know the size and the shape of this
array so first of all I print a dot size
when I run this I know that there are
six elements then I also want to know
the shape of this so I will say shape a
dot shape so basically gives me the
shape as well now let us move forward
and see what are the other operations
that you can perform with this numpy
module so we can also perform reshaping
as well as your slicing operation with
this numpy module but first let us
understand what exactly is reshaping and
slicing so reshape is when you change
the number of rows and columns which
gives a new view to an object so let me
just take an example to explain this
better so let me first print this array
so I printed this array now what I need
to do for that I just need to type in a
dot reshape I wanted in the
three rows by two columns and if I run
this you can see the different guys so
these are my three rows and we have just
two columns so this is how you can
reshape your numpy array so now let us
go into the impurities of these array
now what happens let's say if you want
to pick up just one element of a given
array so what you can do you can simply
type in here let's say I'll just copy
the same thing so let's say we have this
array now so what you want to do you
need a particular element let's say you
want an element three out of this array
so what we're going to do you're going
to print a and then you see specifying a
zero
if apprentice gives me the result read
so what I have done I've just specified
my index and then the value I want so
this is my enix so here the array 1 2 3
is your index 0 and this is your index 1
so therefore we have printed the second
element from the zeroth element so here
this is my zeroth element and I have
printed the second element so 0 1 &amp;amp; 2 so
I want the 3 so I have printed 0th index
and 1 the second integer that is 0 1 2
and therefore it is printed 3 so taking
one step forward let's say we need the
second element from the zeroth element
and the first index of the array so
let's see how we can perform this for
pressure so for that what you can do you
can just copy this so for that what you
can do you can say a 0 and a colon and
then 2 so here : represents all your
rows including 0 so if you want to get
the second element we'll call the index
2 from both of the rows which includes
us the value 3 and 5 respectively so it
is not 5 it is 6 because the second
element is Messick's so here as you can
see my second element from both the
indexes has been printed
so here the Kulin represents your oil
rose including your 0 through next just
to remove the confusion let's say we
want one more row and we don't want to
get its second element printed just as
the above example so let me print that
is win now I just change this example I
save 1 2 4 5 I have
so I and I bring tail
and if I run this it gives me the value
two and five
so here what I have done I have simply
written 0 : 2 which means that it does
not include the second index of the
third row of an array so this will only
paint me 2 and 5 and not 7-5 otherwise
if I not written this : 2 it will bring
me all the values as 2 5 7 and 5 again
so just by specifying : - it was just
not include the second index of the
third row or it into the fourth row next
moving on let's understand another
operation that is linspace so this is
another operation in Python numpy which
basically returns all your even space
numbers over a specified interval so
then I just type in a is equals to n P
dot linspace and I'll specify the
numbers let's say 1 3 and 10 and if I
just run this also haven't printed this
so let me just print that so as you can
see here it has printed the 10 values
between 1 2 3 so I've written 1 3 and
then it will return me even space
numbers between a specified interval
type
so I've specified an interval 1 2 3 and
I want 10 numbers so I have 1 something
1.2 to 1.4 for and pill 3 then you can
also find out the maximum the minimum
and the some operations as well so let
me just type that as well so I say a is
equals to n P dot array
now what I need to do I need to print
the minimum of these numbers so I say a
dot min then I want the maximum as well
so I say a dot Max and then I want the
Sun so I'll say a dot Sun and let me
just run this or when only the Sun has
been printed so let me just do it
individually twelve say okay the max is
printed now I say a dot min which is 1
then I say a max and I just run this so
the maximum is 3 and if I just sunk this
it will give me 6 which is 3 plus 2 plus
12 is equal to 6 so that is how all
these operations are very easy in Nampa
so you must be finding these pretty
basic but with the help of this
knowledge you can actually perform a lot
bigger tasks as well she does understand
the concept of axes in numpy save me a
sprint in a day and let me tell you what
exactly are axes so yes a numpad 1 array
so as you can see here we have an umpire
a two by three so here the rows are
called as axes one and all the columns
are called as access zero
so basically my this is access zero
which is all the column that is 1 4 2 5
and 3 6 and my rows are access 1 that is
1 2 3 &amp;amp; 4 5 6 so now I must be wondering
what is the use of these axes now
suppose you want to calculate the sum of
all the columns then you make the use of
these axes so let me show you
practically how you can implement this
so just you want to know the Sun let's
say I can type in a dot some and I
specify the axis it say I want the axes
are 0 x equal to u if I run this so it
gives me the sum of this axis
that is 4 plus 1 is 5 then 2 plus 5 is 7
and 3 plus 6 is 9 that is how you got
the sum of all the columns present
similarly you can repeat the task with
axis one as well next we have various
other mathematical functions which you
can perform the numpy so you can find
out the square root you can find out the
standard deviation of the array so let
me just perform that is when so I'll say
let me just copy the same area and then
I want the square root of it so I'll say
a dot square root so I say NP dots is
QRT which is 4 square root and I'll then
pass on the variable that is a so I just
want this so as you can see here the
square root of all the elements are
printed
so basically standard deviation means
that how much each element varies from
the mean value of the Python numpy array
so that is how it in perfect square root
of it you can also perform some basic
operations such as addition subtraction
multiplication and division of two
matrices so let me just do that as well
so first of all I'll have to numpy
address for that I'll say X is equals to
n P dot array
then I'll initialize my secondary that
is why why this copy-paste the same
thing
I have to print let's say I want to
print the sum of this so I'll say print
X plus y if I run this you can see the
addition has been performed with these
numpy RS similarly you can perform
subtraction you can perform
multiplication so multiply so I am
getting this result and you can do for
other operation such as divisions and
all so next if you want to concatenate
two arrays and not just add them so what
you can do you can perform it by two
ways either by vertical stacking and
second is your horizontal stacking so if
you store it one by one so I'll just
create a head over here that is my
vertical stacking oh I haven't changed
the code to mom down so I'll just mark
down it and I will say it's my header 3
so I have done this and I understand
this so now I just need to erase why I
just copied it from here and show you
how you can perform vertical stacking
and horizontal step that is
contaminating two arrays and not just
add them so I'll type in here in P dot
vertical stab and I'll pass in x and y
so I just print that
as you can see here these areas has been
concatenated now I want it as an
horizontal sacking so I just remove V
and I'll type in HOA O so I just run
this you can see it is been stacked the
horizontally so guys we have many more
functions with numpy so you can just go
ahead and practice more and more about
it so we have discussed numpy now it is
move ahead and understand some of the
operations of pandas library so pandas
as I've already told you it is used for
fast analysis it is used for data
cleaning and this use for data
preparation as well so let me go ahead
and create our Jupiter notebook again
and let's perform some of the basic
operations there so let me just name my
notebook as pandas operations so we can
perform various functions and operations
on panda such as we can create a series
we can create data frames we can also
calculate the missing data and we can
perform various other functions such as
we can do bulging with data we can join
some of the columns we can join some
data frames we can concatenate and we
can perform various other functions to
pandas
so let me first go ahead and import
pandas so I'll type in import pandas as
PB so my input has been done so I've
just gone through my command prompt and
type in condi installed pandas and my
insertion has been done and after that I
have come to my children notebook and I
typed in input pandas so now what I'll
do I'll first create a list so let me
create a list let's say LST and I will
put in some values let's say 8
Electric 8:1 moralistic sale is - and
now I'll feed him some numeric values
let's say 1 2 3 now first it is create a
series in Panda so for creating the
series what you can do you can type in
PD dot C DS you can just click on tab
it's basically autocompletes it and
inside the series I can pass on my list
let's sell pass my list 1 if I run this
so this looks quite similar to a numpy
array but the only difference is we have
read index attached the left-hand side
so we have 0 1 2 here you can also
specify our index away also in spite of
that you can type in index so I can
basically past on my second list so I
will just copy this and I'll pass it
over here and now if I run this you will
see that first it was coming as 0 1 2
and now when I pass the second list the
index values has been changed to 1 2 3
similarly you can pass an array and
Biggs theory or any other thing is well
so let me now pass an array to it so for
a day first what I need to do I have to
import numpy so I'll type in here import
numpy as NP then I will be creating an
array and P dot array and I pass one one
list so let's say I'll pass on mainly
second so I'll pass my array so in spite
of having ABC as output I'll have the
elements of list - which are 1 2 &amp;amp; 3 so
let me just run this
so here I've just passed the list - to
both the arrays as well as the index so
I can just change my index to one so now
I will see the difference so here my
list is ABC so my index values ABC and
my array which has a list so the list
value contains one two three
similarly you can pass on a dictionary
which contains basically the key value
pairs so this was just the basics of
pandas so pandas can hold a variety of
data objects so you can pass in anything
to it now let me perform some of the
operations with pandas so let me type in
here operations and I'll say as markdown
let's say on the heading one Emily
so now what I'll do I perform an
operation with the list as well as a
series so what I will do I first print a
list so let me just create a list let's
say list is equals to
a B and C so let me just print this list
as well so here are my list is ABC now
what I will do I create a variable which
contains this list so I say list
variable and inside this I pass a
function let's say list x 2 so now if I
print this let me just first print this
so here as you can see I have my list as
ABC and if I multiply it by two the same
list has been repeated again
so with Sirius you to basically square
the values now let me implement that as
well
so although I create a series first a
series is equal to PD dot series
and inside CDs I can pass on the list so
let me now first bring the CDs so here
as you can see I have my CDs with an
index value of zero one two so now I
paint the same function which we have
done with the list so I'll create here a
series variable
inside that I say CDs x 2 and if I print
this variable let's see what does it
give me so as you can see here so to
basically square up the value so here I
will have an ABC and now I have a a BB
and CC
whereas in list I was just getting these
three numbers together so this is the
main difference between at lists and CDs
so you can perform any of the operation
with series you can have two series you
can add them up let me just create that
as well so I have series that same X 1
dot PT door series and inside that I can
pass on let's say 1 2 and my columns can
be let's say I'll take the example of
subjects let's say max then I have
English
let's say then I have Hindi
so now let me just print this series one
all right so I have just given two rows
so I will be giving two columns only so
I'll just run this alright so this is
done similarly I create one more series
while just copy this I will be creating
another series so I will be giving the
name s - I will printing it and in spite
of the subjects I can change it let's
say I want to change it to Hindi all
right
so now let me just print s to s well so
here as you can see my both of the
series have been printed now what i want
to do i want to perform an addition
operation on both of the series while
say s 1 plus s 2 so if i run this so now
as you can see here my match has been
calculated 2.0 and my English and Hindi
does not contain any value which is n n
so as you can see here my match has been
calculated - and English and Hindi
doesn't find any match so it will return
me
nn-never pandas hold a variety of data
objects and if you have noticed panda
automatically converts your integer
value to flu since we are giving matched
twice so we haven't got the output as
integer it automatically converts it to
the flow now why that's it it basically
do it because to retain all the
information which is possible so
similarly you can perform various of the
operations with pandas now let us see
some of the operations with data frames
so I'll say data frames
so this is my head you do know it what
exactly the data frame in pandas so data
frame is nothing but it is a panda
series which shares the indexes so for
that let me just import numpy and pandas
import numpy as NP and import pandas as
PT so next what I will be doing I will
be importing one more module which is
numpy dot random so this I will telling
you in a bit
I've done some mistake so it is import
random so my input has been done
successfully so now what we'll do I'll
first have NP dot random dot seed and
inside that I say a number it's 1 0 1 so
this is basically ensures that we get
some random numbers and that is why we
have imported this module as random now
what I'll do I create a data frame so
for that I'd say the F dot PD dot data
frame I can just click on Darwin and it
will show mixed suggestion and inside
that I can pass on the function that is
rnd and I'll pass which matrix I want I
want 3 by 3 matrix Y I will be having 3
rows and 3 columns so now I have to
specify those 3 rows and column so I
will be typing in let's say a B and C so
now you can just print my data frame
so as you can see here I have a
beautiful data frame with some random
values in it so negative is nothing but
it is a bunch of series which is sharing
the indexes so here we have a series
with a column name of B then we have a
series of column name E and similarly we
have four if so now if you want to print
a single series let's say I want to grab
a series of column B so for that what I
can do I can type in DF and I can
specify the column let's say I only want
to know the series which is in the
column D so if I run this alright so I
forgot to mention the square braces over
here so here I'm at this series has been
printed which is two point seven zero
zero point five and minus zero point
eight four similarly I can have it for
arrow key column so here this series has
been printed now what if I want to
confirm whether it is a series or not so
for that what I can do I can simply ask
for the type so what I will do and say
type data frame and inside this I want
to know whether it is a series or if
something else will fire on this so it
basically tells me that it is upon us
use similarly I can know the type of
anything else let's say I wanna know the
type of DF which is a data frame so let
me just run this so here it tells me
that pandas dataframe now you can
perform various of the operations on
this random number so you can see the
data frame that we have just created so
now it means printing the first two rows
of these data frames for that I will do
the F dot head so this is a function
which basically prints the value so here
I want just two rows that is a and B so
I will say two if I run this so as you
can see here I got two rows of the
entire data frame now let's say I have
just printed head over here now I want
the last row so for that what I can do I
can say D F dot tail and I can say one
so I just need the last row so if I
click on run I am getting the C 0 which
is the last row next you can also
perform various other functions such as
merging joining concatenation of two
data frames and all these things so now
let me perform one more operation of
data manipulation that is data cleaning
or you can say there is any null value
in your data frames what I'll do a type
in VF dot null and I'll save any so it
will basically tells me that if my day
the frame contains any null values so
here we can see that it does not contain
any null values let me just print it
all right status is null so as you can
see here I have D column which is false
so it does not contain any null value in
fact we have already provided with some
random numbers say there's some null
values it will give me a true value over
here similarly you can perform one more
operation that is you can basically drop
a column so let's say I want to drop one
column or if you want to drop a row so
what I'll do I'll type in D F dot drop
and inside that I can mention any other
row let's say I want to drop the per row
C so if I run this now my date of film
does not include the row C so that is
how you can perform various operations
on series on data frames on arrays or
anything so you can perform a lot more
things with us also with manipulation
with pandas and numpy you can clean your
data so just like I have done it here I
have dropped one table I have checked
whether it contains a null value or not
you can basically separate your element
arrays out of your data or frame then
you can separate then you can just
separate the ble emitters you can also
convert your series to strings or string
to series then you can handle the
missing values you can also detect
outliers and you can do many more things
with pandas so we want to know more
about this Panda series you can go to
their official website and you can lower
all the examples so just guys practice
more and more of these two libraries and
you will be perfect in these let's let
me go back to a presentation and let's
see what is my next topic so we have
discussed various operations on number
as well as pandas and else you'll be
able to manipulate the data according to
your needs
but again guys I'm saying I've just
showed you the basic operations you can
go ahead and you can perform many more
with it now let us move on to a next
topic
that is machine learning so machine
learning is the type of artificial
intelligence that allows the software
application to learn on its own so
basically what happens from quite a long
time we are creating a machine or a
program that follows a set of
instructions so what do we do we
basically frame a set of instructions or
rules which will be followed by the
program but a brain does not work in
this fashion so it learns how to behave
in different scenarios on its own and
how does it do that
it learns to the experience so this is
the main idea behind the machine
learning that is to mimic the way our
brain works so what we're doing machine
learning with create systems or programs
that learn how to behave when exposed to
a new data set so how can we achieve
this so what we do we allow machine
learning to detect patterns in data set
and create models out of it so and later
on we can adjust that model or going to
our needs and enhance the accuracy of
the model so if you look at the image
here so what you do we first play in
your data and based on any machine
learning algorithm which is built on
your data set you build a model and
there if you see there is this feedback
loop going on which is performed again
in the game until we achieve a good
amount of accuracy so this is the whole
idea behind machine learning and how
machine learning works also machine
learning can be used in various other
domains such as it analytics in weather
forecasting it is used for let's say
what will be the stock price next day
and all of this thing can be done using
machine learning where the system is
learning by its own or detecting pattern
so that it can take actions when it is
exposed to a new data set so now let us
move on to different types of machine
learning or you can say the different
categories of machine learning so it is
basically classified into three types
that is supervised learning unsupervised
learning and reinforcement learning so
let's talk about all these see taking
one at a time so let's start with
supervised learning so supervised
learning is where you have an input
variable X and an output variable Y and
you use an algorithm to reduce the
mapping function from the input to the
out so as you can see here we have a
function which is y is equals to F X so
basically you have to figure out this
function f X so the idea behind
supervised learning is fairly simple you
have a set of inputs
let's say for example we have a value
let's say X is equals to 1
two-and three-and similarly we have a
value of buy which is equals to two for
let's say an eight so here what we need
to do we need to figure out the mapping
between the two which is 2 to the power
n so here we have deduce a function that
is 2 to the power n so once this very
function is identified or once we know
the mapping what we can do for any new
number or for any new data or any new
input let's say we have a new input of 4
so we can predict the outcome or you can
say the Y value so here in our case the
value would be 2 to the power 4 which is
equals to 16 so here we know the mapping
function which is 2 to the power n and
from that we have deduced the value of y
that is 16 so I hope you got the
fundamental of supervised learning now
the reason why it is called supervised
learning it is the process of the
learning from a training data set so the
whole process to figure out the mapping
function can be thought as a teacher
supervising the learning process so for
example a teacher teaches a topic to a
bunch of students now if the student
understands the topic thoroughly even if
any new problem come or any new problem
arises to the prudent treated to the
same topic the student can go ahead and
solve the problem or take the necessary
action for the same so this is the whole
idea behind supervised learning
exhibition earning contains various
algorithms such as your linear
regression logistic regression decision
to a random forest and need Bayes
classifier so all these the Gordon's can
be used for different different use
cases so in case of linear regression it
is used to estimate real values such as
cost of house or number of cells or
number of calls or total number of sales
based on the continuous variables
whereas in logistic regression we
estimate discrete values that are the
binary value such as 0 1 yes or no true
or false based on set of independent
variables so don't worry guys I will be
implementing this practically as well
but let me just give you a basic
introduction of all these algorithms and
then we can proceed with the practical
wherein I will be implementing logistic
regression so then come to decision tree
so decision tree is basically used for
classification problems whenever you
want to classify your data set in
different categories we use decision
tree
and this works for both categorical and
dependent variables so don't worry guys
we will understand this dependent
variable independent variable or the
other key terms that I'm keep on using
during the practical I will explain
everything to you but for now you can
just understand as dependent variables
as your output pair as the independent
variable as your input that is your X so
next we have random forest random forest
is quite similar to a decision tree but
it gives better prediction and accuracy
than decision tree and last we have nave
Bayes classifier so this is based on
Bayes theorem wherein you have no
relation between different different
predictors so as to predict our output
so this was all about supervised
learning now let me just implement let's
say a logistic regression and let's say
how can you apply machine learning to it
so now let's understand what exactly is
logistic regression so logistically
gresham implicitly produces the results
in a binary format so we have already
established this fact that the outputs
would be yes or no it can be true for
high low pass and fail and all these
things so basically it should be
categorical it should be only either yes
or either no so therefore it predicts
the outcome of a categorical dependent
variable so whenever you apply a
logistic regression should be very
precise about which are your dependent
variable and which are your independent
variable so let us first understand the
relation between independent variables
and dependent variables so I hope all of
you must have come across this equation
which is the equation of a straight line
that is Y is equal to a plus BX wherein
we have Y as dependent variable X as
independent variable is the y-intercept
and B is the slope
so here the pending variable is
basically the unknown value of a
variable or the value which needs to be
predicted then we have X which is
independent variable that is also known
as known variable or the variable which
is related to the dependent variable so
this dependent variable will be
dependent on the independent variable
next we have a which is the y-intercept
so this is the value of dependent
variable when the independent variable
is 0 or the point at which the line cuts
the y-axis so that is why it has been
given the name that is why I intercept
and then we have B which is the slope
which is nothing but the change in
dependent variable for a unit increase
in the India
variable so it is also known as the
payment of the Angle made by the line
with the x-axis so now let us implement
this logistic regression so here I have
a problem statement wearing a car
company has released a new SUV in the
market and using the previous data about
the sales of their SUVs they want to
predict the category of people who might
be interested in buying this so here I
have a data set which includes user ID
it has gender it has the age it has the
estimated salary and whether the people
can purchase the SUV or not based on the
salary or based on any other factor so I
will telling that what are the dependent
variables over here and what are the
independent variables so let me just go
to my Jupiter and let us implement
statistic regression so here I'll create
a new file I can say the new notebook
let me just name this as demo
so here I want the head Alexei logistic
so here first of all we need a library
which is scikit-learn so this is a
library which is used to perform machine
learning in Python so scikit-learn is
nothing but it is an open-source library
which is licensed under BSD and provides
a range of supervised and unsupervised
learning algorithms in Python so
scikit-learn consists popular algorithms
and libraries and apart from this it
also contains the packages such as numpy
matplotlib sai pipe which is also called
a scientific Python so remember guys to
input scikit-learn you first need to
import all the parties such as numpy
patbot lib and sci-fi so we already
talked about numpy and pandas so we will
be importing scikit-learn as well as
matplotlib so how you can do that you
can simply go to a command prompt and
type in condi install scikit-learn or
you can say commander install matplotlib
and we're not using an account now you
can go to a command prompt and type pin
using the PIP come on so let me just
import all these libraries let's say
first I'll import numpy so you know how
it is done so I will say import numpy as
NP then I have to import pandas
then I will be importing one more
library that is matplotlib so this is a
visualization library which is used for
2d graphics so I say import matplotlib
dot fire plot as PLD and format for live
in Jupiter you have to type in
percentage matplotlib in life
so let me just run this all right so my
import has been done successfully so now
what you need to do the first step would
be importing your data set and
separating the dependent variable as
well as the independent variables so
first let me just import my data set so
I'll be needing pandas food so I'll type
in PV do not read CSV so my data set is
in the CSV format and then I'll mention
the name of my data set that is the car
purchases estimation
at CSP so now let me just print this
data set so as you have already seen in
the presentation I have a user ID I have
gender I have age estimated salary and
purchase so almost I have around 400
rows and I have five columns so now what
you need to do your first need to
identify which are your dependent
variable and which is your independent
variable so in our case our dependent
variable is purchased and the
independent variable is age and salary
so this is my unknown value and this is
the value we have already so we have the
salary and we have the estimated age and
we have a dependent variable which is
purchased so now let me just specify my
dependent variable as well as
independent variable so I will go ahead
I'll type in here capital X and I'll say
data set dot I lock so here I'm specify
my rows and columns so I will be taking
all the rows so here I'll be taking just
second and third column which are my age
and estimated salary so these are my
independent variable so I have specified
it over here that X is my independent
variable wearing I have all the rows and
I have second and third column as
independent variables so now what is
this I love function so I love function
is an index of a panda's data frame and
is used for integer based indexing or
you can say the selection by position so
I have taken the columns two angry
witches my age and estimated salary
based on the index position now let me
specify my dependent variable as well so
I say Y is equals to I lock
and specify all the rows and I just want
the fourth column that is the purchased
column so this is my dependent variable
and I will just take the value of it so
I say values now let me just print X all
right so I have just done a stupid
mistake I have just forgotten a now it
should print so here as you can see I
have both my independent variable which
is the age and the salary similarly I
can print Y as well so I will print in Y
which is the value of purchased so here
it's the value of purchased so this is
basically the last column which we need
to infer that is the purchased or you
can see the fourth column so the next
step would be training and testing the
data so here we'll be splitting the data
into two subsets that is your training
subset I are testing subsets so for that
what you need to do you need to import
cross-validation so here I'll type in
from SK learn dot cross-validation
and import train tests split
alright i underscore over here alright
so this is done now so now what you need
to do you have to define the variables
for your train subsets and your test
subsets so what are you doing i will be
writing x train then i will be writing x
x test and similarly for my variable
that is white rain and whitest
and then using this function name
testlet i will be passing in X variable
that is the independent and dependent
variable and then I mention a size to it
let's say test size is equals to 0.25 so
I'll just explain this in a minute and
specify a random state let's say you can
put in any value so in my case Excel I'm
putting 0 so this will basically insure
me that the same sample has been taken
every time whenever you run the code
satisfy you mentioned the random state
is zero now what is this test size so
here we are basically split the training
and testing subside in 0.25 percentage
now what I need to do next we need to
scale the input values for better
performance now how we can do that you
can use a standard scaler for that so
for that what we'll be doing will be
typing from SK learn that is the machine
learning library and we'll be using
pre-processing and here are the
importing standards Gaillard
all right so I do basically scale our
input values so if you see a data set we
are dealing with large numbers although
it is a very small data set so whenever
you'll be working with a production
environment you will be seeing large it
s is very will be having ten thousand or
hundred thousands of tuples
so they're scaling down the input values
can definitely affect the performance of
the whole program to a high extinct so
for that we need to scale our input
values and python we need to import
standard scaler for that so then we have
used something called as pre-processing
so pre-processing basically contains all
your methods or you can say
functionality which is required for
transforming your data or you can say it
feeding your data to our model so now
let me just scale down the input value
of test data as well as obtaining
subsets so I will be lighting in here is
C dot standard scaler then extreme
and here I'll be passing in the variable
that is extreme and similarly for X test
subset
and here also I'll pass the test subject
all right so here I have just typed in
the same line in this cell this is
running now what we need to do we have
to create a logical regression model
first of all I will be importing
logistic regression so I'll type in from
SK alone
in a model import logistic regression
all right now I need to do I need to
feed data into models so I will be using
classifier so here I will be typing in
classifier
logic supervision and specify a random
state let's say zero now I just fit this
model classifier dot fit and inside this
I'll be passing X train and white rain
so it is saying extreme and white three
so now we can use this and predict
desires of a test set so now instead of
printing the value of predicted outcome
and your actual dependent value what you
will use you can use something called as
confusion matrix so confusion matrix is
nothing but it is a two by two matrix
wherein you have columns which has a
predicted no Erik addicted wire and a
rose and Rose have actual no and actual
yes so let me show you the concept of
confusion matrix so first you need to
import this confusion matrix so for that
you need to type in from SK learn dot
matrix in poor confusion matrix
all right I did a spelling mistake know
what I've been doing I'll be creating a
variable CM which has the confusion
matrix of by values which is the widest
and the wire predictor so I will type in
my test and Y predict
so now I have a printing up infusion
matrix so here it is giving me that it
is white s is not defined so oops I just
forgot to mention over here
so here I'll be using this buy predict
to predict the results of the test
dataset so I will be typing in why why
predict I'll be saying classifier dot
predict and I'll pass on the X text
all right so Y is in my capital
so it's saying that why test is not
defined so if I go ahead all right
because if Y is in the upper case well
just copy-paste the same and now let me
just want this so here my confusion
matrix has been printed now let us
understand the concept of this confusion
matrix so as I've already explained you
that confusion matrix is a two by two
matrix and we have four outcomes to it
slick means explain that to you so here
in the columns we have predicted know
and predicted yes
whereas in rows we have actual no and we
have actual yes
so here let me just copy the same values
over here so here we have 65 that I'll
just write it over here
then we have three then we have eight
and we have 24 so as you can see here
the two fields which is 65 and 24 so
this is basically my actual know and
predicted no and actually yes and
predicted yes it's basically refers to
the predictions which were correct
now these fields which is eight and
three so these are the fields made the
model predicted yes but in reality the
output was no they were eight cases
which were predicted no and actually on
the data set it was yes so this is also
known as true negative it is known as
false positive this known as true
positive and this is called as false
negative so to calculate the accuracy
here what you need to do we need to add
the true negative and true positive and
divided by the total sum so what we need
to do so here 65 plus 24 is equal to 89
and the sum which is 65 plus three plus
eight plus 24 is your hundred so
basically we are getting the accuracy as
zero point eight nine
so therefore we are getting the accuracy
as 89% so here we have banned this
manually but in Python we have a
function for this as well so basically
our machine will calculate the same for
us so you can go ahead and input
accuracy Skol and then you will be
getting the same result as that we have
calculated so here we have implemented
largest aggression with the accuracy of
89% so now let me go back to my
presentation and let's see what else we
have so I this was cleared this up all
right so we have understood what exactly
is a large figuration now let us move
ahead and understand the second type
which is the answer provides learning so
using unsupervised learning we can train
a model based on the unlabeled data set
so in an unlabeled data set there is no
explanation or terms with respect to
each piece of data it just contains your
data and nothing else so are examples of
unlabeled data can be an unlabeled photo
or an audio which is scraped or grabbed
from Internet so now if you remember
when you have talked about supervised
learning we were dealing with those data
sets where a set of input and set of
output were clearly defined or clearly
specified but we cannot apply supervised
learning in these cases where data is
not defined or it is not been labeled
properly so unsupervised learning
basically draws influences from data
sets or unlabeled data sets on its own
therefore we can use this model so as to
cluster the input data into classes on
the basis of various statistical
properties so we have various algorithm
in unsupervised learning such as k-means
clustering hierarchical clustering and
many more so we will not be doing the
practical of unsupervised learning in
this session so I will just be briefing
you with the types of machine learning
algorithms and later we will come up
with series on these individual
algorithms so with advanced part of
machine learning we have next session
lined up so I don't want you guys to
miss any of those sessions so this was
all about unsupervised learning and the
third type of machine learning is your
reinforcement learning so reinforcement
learning basically is when your computer
tries to take decision on its own rather
than being taught explicitly so
computers aim is to maximize reward when
does an action so for example you ask a
child who completes over by evening and
who does that he'll give a reward to
them or let's say
and then the child will automatically do
it so what happens the child will keep
on repeating so as to get more
chocolates and finally and the son from
the past experiences that okay when I
did this I got an award so let me just
do something similar I get more rewards
so that is the whole idea behind
reinforcement learn now again there are
numerous algorithms in reinforcement
learning such as
q-learning then we have salsa then we
have D queuing which is related to deep
learning and we have many more like that
so we'll be discussing all the use cases
in our further sessions so that is it
for today guys so let me just recap what
we have started so far so first of all
we have started what is the mean of data
science why exactly data science came
into the picture then we have understood
what is data science the life cycle of
data after that we figure out why Python
is best for data science and its various
other features so once we have
established that pack that Python is
best for later science we have gone
ahead and we have performed some of the
data manipulation wherein I have
explained you how we can load a data how
we can perform some of the basic
operations with numpy and pandas then
what we have done we have took an
overview of machine learning variant I
have explained you the concept of
machine learning and the different types
of it so we have discussed about
supervised learning we have discussed
and supervised learning and a
reinforcement early and finally in the
demo part I have explained an algorithm
of supervised learning that is a
logistic regression wherein I have
implemented all the libraries appear
solid so far and how you can apply a
machine learning to it so in that demo
we have first imported the libraries
then you understood what are dependent
variables and independent variables then
we have also split the data set into
training data set and validating data
set and what we have and we have trained
a model or you can say create a model
and finally they have predicted the
output based on the test data set so
that is it for today guys so I hope you
liked the session so next if you have
any doubts or any queries here into any
of the topics that I discussed so far
you can comment below the video and I'll
be happy to help you with the same so
that's it guys I hope you have a great
day thank you bye bye I hope you have
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and
we will reply them at the earliest do
look out for more videos in our playlist
and subscribe to any Rekha channel to
learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>