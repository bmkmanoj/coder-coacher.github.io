<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Age Search Through Apache Solr | Edureka | Coder Coacher - Coaching Coders</title><meta content="New Age Search Through Apache Solr | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Age Search Through Apache Solr | Edureka</b></h2><h5 class="post__date">2015-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wC_rMVp96XM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I am atita I'm at aurora and today I
would be taking you through this some
small journey or from Apache Solr so
this is the new age search through
Apache Solr up topic which we would be
discussing today and just to give a you
know brief about myself I'm a software
professional being an industry for about
over nine years now and I have been
working on an Apache Solr since um I
think december two thousand eight and i
have been working on solaris since
version 2 dot 2 so we had lot of you
know challenges and a lot of struggles
which we had to go through when we were
using it initially so I would not go
into lot of details instead would like
to begin with this interesting topic
today and I hope this comes out to be a
pretty useful one for everyone who is
present on this forum today so I see
about 50 plus folks so let's quickly
begin with this I'll just minimize the
question window as of now and just a
small request we would take logical
breaks in between so I would try to
address each and every question you guys
some San across so I would be taking
each and every question and answering
everyone everyone's concerns so in case
if you post a question and you do not
you know see that I'm not answering the
question please wait for some time
before i take the logical break okay so
we have about 60 minutes so let's make
the most of it today so what we are
assuring you that what you will learn
today is that why exactly the search
engines are required in any enterprise
grade application which has been used
across some today what are the
objectives and challenges or for the
search engine
and what and how is the indexing and
searching handled and leucine again we
would be discussing a little bit of
leucine as well in today's session
because solar is you know it gets most
of its features from leucine only so
then we would step into solar we would
be discussing the architecture of solar
we would be discussing how the near
real-time search is supported through
solar how is the search enhanced through
Salar we would be learning about solar
cloud and we would also see a little bit
of how we can leverage missile our
capabilities with her tube so a quick
question for you guys I mean now I mean
everyone seems to be here for for a
purpose and everyone wants to learn
about solar and we know that salar is
something which enhances the search
functionality on any web applications so
a very simple question guys like why do
we really need a search engine what do
you guys think any any thoughts anyone
anyone from the attendees why do you
think we would need a search engine
imagine if you have to look for
something in the google is not there
okay so we have couple of answers from
opening it says it makes life easy
couple says its search files imagery as
PDFs saves time relevant information Wow
quick find a product information okay to
get the information quickly than a
chaise keyword-based searching in speed
get faster results deeper can manage
sales fabulous guys I mean you you guys
really know that why why we all are here
so just to sum this entire thing yes
Manoj save time as well so yes just to
sum up what we have just you know shared
so most of them were that we are trying
to find
the search results from the free text
and we are trying to get the quick
results and the other one was the
relevancy so if you if we want to you
know like you know narrow down the
answers which I have just received on my
answers window is two things the first
one is speed and the second one is the
relevancy anyone who has any other you
know point why we would need a search
engine I I narrow them down to two
things first one is the speed and second
one is the relevancy so I say that we
would need the search engine to get the
results faster and irrelevant the
accurate results so speed and relevancy
are two things solar or any search
engine for the matter of fact is popular
so we would need the search engine to
give us the speedy results the quick
results and the relevant ones right
everyone with me yes no maybe you can
send in these smileys as well so I like
it my no just using these smileys so you
can instead of typing in yes you can
send any smiley yeah good going up beep
again delish okay so all right great
yeah that works as well so just to you
know give you a small example of the
this snapshot which has been taken from
a very popular e-commerce website so
this is a you know up snapshot where and
we are searching for the quad core
mobile phones so this is how more or
less any e-commerce website some
interface looks like so we have the free
text search box here DD search box where
and you can send in your jump free text
um query and along with that query it
would give you the autosuggest you know
these suggestions like are you looking
for the quad core mobile phones with the
kitkat OS or the sony mobile phones or
the quad core mobile phones with a four
or five inch screen so so these are
something the suggestions which are um
you know given on the basis of
most used search strings by the other
users so these are basically the ways by
which it enables or this gives you the
nice experience that you don't have to
type too much you can just pick out one
of these suggestions if it suits you so
along with out on the left hand side if
you see that it provides you few
filtering criterias as well like um as
of now we are looking for the mobile
phones data so we might want to
categorize the mobile phones on the
basis of the price range they might
belong to the brand they belong to or
the operating system or maybe the screen
size or maybe the material of the body
or maybe the sim preferences like a
jewel same or the single same so they
could be several filtering criterias
which we might have and on the right
hand side if you notice these are drum
few of the related products which are
related to the product which you might
be looking for and in the center in the
in the middle of the screen if you
notice we have given these search
results so the search results basically
consists of the name of the product the
star rating maybe the consumer rating
which category does that belongs to what
is the discount you know discount which
is offered on this particular product
the price range and maybe the few of the
features like any side might prefer to
list the five key features or the five
most hot features of that particular
product so this is how each of the
product is listed so from here if you
look into there are few you know things
which we have gathered that we need a
text based search unlike any normal I
DBMS you know client we do have to you
know follow a particular semantics like
a field name or the name equals two
maybe the name of that product the
feature should be so and so this is how
the price should be it should belong to
this category Lake price should be maybe
more than three thousand less than five
thousand so
we'll have to follow a particular
semantics semantics to you know search a
particular product in any traditional I
DBMS instead on the contrary to this if
you are looking for such kind of feature
which enables your website to have a
free text search interface this is
something you should go for so any
e-commerce website or any you know web
application for the matter of fact which
is wanting to enhance the search
experience of the users of that site
would want to have this feature that
they would like to support a free text
box free text search box which can let
user type anything and it would
basically generate a complex query which
is sent to the system and the respective
results are being fetched along with
that I would also like to have the
features which might want to you know
narrow down my search results to a few
of the products so that I can quickly
list or I can quickly you know choose
from so these filter criteria salts are
called off a sets which you see on the
left-hand side of your screen along with
that they might be a chance that you
might be looking for a different search
string but it is related to something
you might be more interested to look
into like um I'm looking for a quad core
mobile phone but I see on the right-hand
side the related products are listing a
tablet which comes in the same range as
much as i'm wanting to spend on a mobile
phone so i might think that maybe you
know going for a mobile phone in that
price range i might go for a tablet so
this is something which might enhance
the user experience further as it can
give you these suggestions on the basis
of the popular products which belong to
the diner features you're looking for as
well and along with that it is listing
out all the search results in the form
of a document like all the information I
would like to want I would I would want
to see in one glance about a product has
been listed here so I can quickly see
that which brand is it what is he
consumer rating how much discount am I
getting on this website what are the key
you know features like how much is the
screen size
how much is the camera picks it pixels
and if it's a dual sim phone or not or
rub if my if the you know mobile phone
body is suitable to me or not so I can
get the quick inside of the product here
itself so this is the reason anyone
would want to prefer a search engine
that it makes her works easier plus it
gives me the speedy results and the
relevant ones so I get to see all the
relevant results on at one go and
they've been you know they can be
compared they can be sorted as per my
choice agree everyone anyone who would
like to add something to this okay so
how exactly it should be so how exactly
should a search engine be so if you need
a storage engine to search records or
the documents based on the free text you
know free text or keywords it should be
it should be able to support features
like it should be first of all it should
be optimized for the faster text
searches you should be able to support
free text and it should be faster as
well anyone who has worked on any
traditional I DBMS interfaces they would
know that you know you know the
numerical and any you know searches
which are related to any numbers they're
pretty fast on search interfaces but you
have to look for from any text related
searches they are very limited number of
operations available plus there are very
slow as well so most of the time on such
websites we look for the products and
they have been searched through the free
text so free text when I am saying I'm
saying that a user can type in anything
like taking an example from e-commerce
website maybe i can say that a silver
watch with a blue dial so this is an
example of a free text so i'm typing in
anything i'm wanting i'm not saying that
dial equals true blue
oh and the you know wristband should be
silver I'm not really specifying any
criterias but I'm just typing what
whatever comes to my mind
so along with that my you know my search
engine should have a flexible schema so
scheme I hope you guys do understand the
schema is the way my information has
been structured what all feels my record
is going to contain and in case of a
document how many fields or what all
feels my document is going to have so
imagine again taking an example from an
e-commerce website imagine if I have to
index a CI mobile phone data I wouldn't
it feels like the brand the screen size
the camera pixels the same preferences
dual or single I might say a price as
well
I might say the phone body I might say
the OS version so um my fields would be
different however imagine if I'm listing
say a book some a book data I would say
my fee my document is going to contain
he feels like who was the what is the
title of that book who is the author
which language is it written into what
type of binding it has and if it is
available in my country what is the
price what is the user rating so do you
see the difference do you see the
variation in the fields do you see how
the documents are being recorded they're
completely different the fields are not
going to be similar there is no
uniformity in the kind of data which has
been stored for these two different
products but my a search engine should
be able to ingest data for a mobile
phone as well as a book and it should be
able to store both of these informations
without any hassles without me having to
change the schema is that fine guys is
that making sense yes no maybe smilies
along with that it should be able to
support the sorting of the documents as
well like I should be able to sort them
on the basis of you know the the
features as well so like I would want to
sort say on the basis of price or on the
basis of something else so that should
be supported as well so sorting is very
important because I mean by any chance
if you might have used any e-commerce
website the first thing anyone would
want to hit as the price range from low
to high I'm sure you might have also
tried that I mean I do it all the time
when I'm looking for any product on the
e-commerce website I I do it all the
time the first thing after searching is
you know arranging my product list or
the search results from price low to
high so and this is a normal human
tendency so forth feature is that it
should be web scalable that is imagine
if you are drum
looking or you know catering to a web
application you can expect more number
of reads and less number of rights I
mean the data is going to be fed into
from a different route altogether your
data would be a you know read more
number of times instead of being written
more number of times so it should be
optimized to cater to huge number of
reads requests along with that it should
be document-oriented I mean each of the
fields or each of the information
pertaining to each of the product should
be listed in one single place so that
there is no hassle in looking for the
information whenever whenever I'm trying
to construct them UI is everyone fine
with it any other feature you feel or
search engine should have I mean this is
more or less a very exhaustive list so
this is another example which we have
taken from a travel website that if
you're looking for similarly eamon um we
were looking for the mobile phone data
similarly if you're looking for anything
on a travel website if you notice on
left inside the criterias the filter
criteria so they face it I mean in
technical jargon if I really have to
call them these are the faces so faces
are the filter windows so and these have
changed the price range is one of the
filter criterias here the star rating is
one of the criterias here what is the
like website rating what is the location
like in case if you're looking for any
destination or any particular hotel in a
city right so this is how the
prospective really changes so my search
engine should really let me control all
of these features so that I can have a
nice user experience agree everyone I'm
sure this is this is quite generic
things you might have seen in your real
life as well so coming back to this we
have been talking about solar but then
as I introduced leucine as well in the
beginning I said that everything which
is there in Seoul are like eighty five
percent of the features of salar are
based out of leucine so what exactly is
losing does any one of you recognize as
this guy in the in such doe cutting
anyone who recognizes him I'm sure a lot
of you word okay and he does is no okay
then a says yes cycle says big data
Hadoop absolutely so this is the guy who
was created Hadoop as well so this is
the guy who was created leucine as well
to begin with so he named her leucine
after his wife there's a very
interesting story in fact had this been
my real class I would have actually you
know shown you this video where does
there's been talking about how he you
know avoid the name of leucine so this
is very very interesting story in fact
you can find that on youtube as well so
a leucine is something which um you know
came as a in-house a project and later
on it was donated to apache and it
became open source so it's a very
powerful java source library which lets
you easily add the information retrieval
or these search capabilities to your web
application trusting if you're wanting
to trust the capabilities or from the
leucine you can check this link i think
i have opened this link somewhere yeah
this is the one so this is this these
are the giants who are using leucine so
you can get you know Hugh joins like a
well and you have apache you have apple
here you have apache IAM you have you
know netflix here you have linked in
here you have twitter here so these are
you know few of the joints to name
who are using leucine and they are using
the capabilities of leucine to enhance
the search experience of the users of
the website so this is okay any that has
a question here is solar using this
library yes any titles in fact when we
move across to the architecture of solar
you would you would see how excessively
you know this solar is using leucine in
fact the the core features of solar are
being you know extended from leucine
only so the basic indexing and searching
has been rendered from leucine Sosa very
I would say it's its heart of solar
okay so um it gives you the scalable and
high-performance indexing capabilities
and it's very powerful accurate and
efficient search algorithms it has
considering that term lot of big giants
have already been relying on it and have
been using it obviously you can you can
trust the capabilities plus the biggest
biggest advantage it has is that it
gives you the cross-platform solution so
which means that though it is based on
Java you can use it in dotnet you can
use it and Python you can use it in PHP
so there are implementations which are
available in other programming languages
as well which our index capable capable
and compatible so how does that really
works I mean what what makes it so fast
is what my next topic is so imagine guys
if you have to look for any information
considering that we have this example
that if we have three documents the
first one says I like at Eureka courses
the second one says at Eureka teaches
big data courses and the third one says
a break our hips learn new technologies
easily so imagine if you have to search
for rum maybe a single string maybe you
have to search for data out of these
three files and you have to write any
program or you have to use say the
traditional RDBMS how is it going to be
like
you would have to a flat file scan each
of these documents right is that is that
correct anyone who differs anyone was
any other way yeah okay so uh you know
we are saying that yes I could that's
right we can use no sequel as well so
solar is something which can be used as
no sequel as well so in fact that that
solution is correct that is what I'm
heading to as well so we are basically
defending that we should not be using
the flat file scan instead we can use
leucine so how does that really do is
that it maintains the indexes and how
that manages the indexes that we are
storing we are indexing the data of all
these three files in one single file
which is called the indexes and how is
it managers that each of the key terms
from this document from each of the
document they are being maintained as
the table and the document reference
like a Eureka and we are maintaining the
reference for the documents which
contain the word Eddie recomm so Eddie
rekha is there in d1 d2 and d3 so I I
store a d1 d2 d3 as the document
reference similarly courses is there in
d1 d2 d1 and d2 that's right teachers
are there in d2 big is there and d2 data
is there and d2 helps is there and III
so on and so forth so I break up break
up up each of the stream of data which
is there in each of the document into
the tokens or the words and I'm storing
them as key value pairs right so I'm
basically storing the word and the
document reference along with so what
happens in this case is in case I have
to look for say the word data i was
saying i don't have to go through each
of these files in that case i don't have
to go through and read be one I don't
have to go through and read III or d2
for the matter of fact I can simply skip
this the step and I can simply read this
in
X file what that does to me is that
instead of wasting my time in flat file
scan of all of these free files I just
spend my time in parsing this one file
and I simply get the results here that
data on the word data was there in d2 or
similarly the word at Eureka that was
there in d1 d2 and d3 all of these three
documents so that's right a chakra
through this this process is called
tf-idf that is term frequency inverse
document frequency that's right cyka
this is how MapReduce works as well that
that's absolutely fine I mean these all
of these Big Data technologies are
somewhere you know closely related to
each other the basically using the same
technique in most of them so I hope this
is clear to you guys that how that makes
the searching faster faster so that the
process of indexing does converts the
data into this this format this index
file has been generated from the chunk
of data which you give to your sellers
over so in the process of indexing it
breaks up the data into this format and
creates the indexes file which is then
being read by the search procedure okay
couple has another question a very
interesting one what if I misspelled at
Eureka to EDD you it you only will it
search it will definitely search couple
and we have again this is what leucine
probably would not be able to achieve if
you misspell the word leucine would not
be able to correct it but solar has got
the spell checker search component which
will correct your search the string and
it will ask you that did you mean Eddie
Rekha or did you mean anything which is
also close close to the word Eddie arica
as well so it can suggest you and it
then you can select that yes I meant
edge Oracle and then you can basically
look for the right number right results
yeah that's right like a Google search
so that is the spell checking feature
which is there it is called a search
component so we teach you how to
configure that on
salar instance okay chakra there has
another question Wilkie's be hashed up
I'm not sure what what does I mean how
how do you want to say it will be hashed
you mean no will they be encoded in some
format is that what you mean to ask okay
yeah yes you can choose to do that as
well normally it has not done that and
normally whenever he did has been
indexed it is not stored as the plain
text file it is anyway encoded in a
different format altogether so if you if
you basically look into any of the index
files which are being generated by solar
like over here if you look into so like
this is where I have stored the
collection so if you look into the data
folder this is where my Nexus are being
generated these are anyway the different
formats altogether how these indexes are
being stored there anyway encrypted in
some way which you cannot I'm in which
which are not human readable does that
answer your question chocolate ur ok all
right ok another question from bhisma is
how solar cheese better results than a
traditional search engine like very
verify Ernie okay it's a Verity I think
an idol I am Not sure of not really i've
heard of Verity i think but not really
have heard of idol so uh okay the
traditional search engines as I just
explained misma ism yeah very tk2 okay
so uh what I was trying to explain it
over here is that we have very limited
capabilities in terms of text searching
with the traditional search engines so
if they're not using they might be using
the traditional indexes which are being
generated on say Oracle Laura my sequel
so the the procedure of creating the
index is altogether different in case of
leucine and solar and plus it is faster
as well so this technique is something
which basically creates the
differentiation is that fine misma
another question from the perk is it
individual words are indexed or we can
index multiple words too so people just
to answer the question this is mostly
the configuration based so you can
choose how do you want to basically
break the tokens so each of the token
has been indexed and you can choose how
these tokens are supposed to be created
so you have various algorithms and you
have various some tokenizer switch come
handy in the configure in the
distribution itself so you can choose
the way by which you would want to break
the tokens so that's entirely on the
user's choice is that fine does answer
your question deeper okay so let me just
move ahead so we have we have the way in
which are we are writing the indexes
data how is it done is up so you can
think of a document as a record which
has been fetched term by a traditional I
DBMS sub you know table so like you use
search for a table record from a table
in the similar manner we search for a
document from a collection so a
collection is a table our document is
record and a field is a column so just
just to strike the similarities so
document is nothing but a record and
like a you know column like a record
that consists of several fields several
columns in the similar manner we have a
document which has several number of
fields right so these feels every time
you pass in a document with the numerous
fields in there that goes through the
process of analysis wherein it basically
breaks in the tokens as per the choice
of your rum algorithm so you can choose
these standard ones you can write your
own as well it's completely extendable
so you can extend the you know existing
functionality
so you can write your own way in which
the tokens are broken so then these
tokens are basically they go to through
the process of index writing and the
indexes are being created on each of the
fields and then this data has been
dumped as the indexes on the directory
so this directory could be the file
system this directory could be the HDFS
as well in case of using them with the
Big Data technologies so these are the
main classes which are used in case when
we are indexing the document using me
lucene so document fields and analyzer
index writer directory these are the
classes just to give you the context off
if you have to write a program in Java
to index similarly we have the query
parser guy who's the Magic Man so every
time you send in the expression or maybe
say a free text query that is been read
by the query parser which again performs
the analysis on this query which you
have given to the search given in the
search box and from this query and after
creating the text fragments by analysis
it would generate the complex query as a
knob whichever type of query parser you
have chosen and then this sends across
this complex query to be indexed
searcher and to the directory and the
search results are being returned in the
similar format so this query parser
basically translates your expressions
from the end into the arbitrarily
complex query which is then read by the
index searcher and has been sent to the
salon and the respective sources are
being fetched is that is that correct
nice is that right I mean any any
confusions hear anything you guys did
not follow okay I'm seeing a few
questions here so let me just cover few
more topics before I take up these
questions because we have a lot of slice
to cover I hope you guys don't mind that
I would not leave today's webinar with
any of the questions unanswered
so would like to cover a few topics
before I answer the questions I hope
everyone is fine with it so we have been
discussing about solar so what exactly
is so large so solar is the open-source
enterprise search server and also the
web application you can use it as a web
application as well you can use it as
the search or the index over as well you
can use it as in no sequel server as
well so it uses the leucine search
libraries and it is it extends the
features further along with that it
exposes the Lucy in some you know Java
API is as the restful services so you
can simply hit the you know the API is
directly from solar you can put in the
documents in the process of indexing
they have they're supposed to be they go
through the process of analysis and are
being a you know indexes are being
created on that particular document and
that has been dumped or stored into the
directory the index directory so these
documents can come across in XML JSON
CSV or binary and they can be sent
across over HTTP as well they can be
sent across through the coil request as
well so you can also query them through
coil you can also query them through
HTTP GET and receive this similar XML
JSON CSV or binary results so as an
whatever your choices you can also get
results in PHP as well you can get
results in in foil as well so this is
absolutely a choice so it supports a lot
of you know formats of data so that's
that's one good thing about it and it
gives you a very nice admin console as
well to see how things are basically
flowing around so what are the key
features of Solara so the so the first
thing which we have been discussing is
that it term gives you the advanced
full-text search capabilities which
which means that term the the text
searches in any format has been
supported free text or the full text or
any type of job you know
search which is related to the text or
the strings has been supported
completely then along with that it has
been optimized for the high volume of
web traffic like if you are a rum site
is expecting to have more number of
reads then writes definitely you should
go for solar so it is optimized for high
volume web traffic it you know exposes
the standard standard based open
interfaces like XML JSON and HTTP it
works on these standard interfaces it
provides you with a very comprehensive
HTML interface for administration we
would be taking a small you know demo a
basic demo for reading and writing
wherein I would give you the idea of how
the this administration console looks
like and how you can push in your data
and you can retrieve the data from that
admin console as well so we would see
that during the demo you can also
monitor your server start sub which can
be exposed over jmx for monitoring
purposes for administration purposes
another thing is that it gives you the
capability of the near real-time
indexing and searching which is
adaptable with the xml configuration so
you can change it you can change the
behavior you can choose that I mean how
quickly do you want to see your results
so it gives you like the near real-time
search and the Nexen capabilities so and
that too is absolutely configurable
along with that it is linearly scalable
like you can add in the memory similarly
in the similar format as Hadoop and the
auto index replication is there you can
choose the replication factor and that
with that using that factor the the data
would be replicated in case of any
disaster the replicated data can be used
to buy for recovering and also it
provides you with the auto extendable
plug-in architecture we have lot of
other plugins available like one of them
is to read data from different file
formats so one
them come some integrated in solar as
well you can add more plugins as well so
does this plugin is called Apache thika
which comes along with the Salar which
lets you read data from different file
formats is that fine guys let me take
few questions here before we move on to
the next topic I see lot of questions
are there ok I'll ok disperse another
question does it just takes paste or can
solar internally look for a metadata tag
based indexing so if you mean that you
want to index the HTML data or Rob
something close to that yes it can do
that as well ok ok ravindran has another
question in a typical solar search
architecture where the document is
usually stored it has been stored on the
file system ravindran the Salar uses NFS
files or MongoDB or HDFS to store the
original document so it is not really
using I mean you can choose to store the
indexes on any type of directory so the
directory which we just saw in the
diagram where we were showing how the
data has been in dec indexed we had this
one component called directory so you
can create the instance of the directory
object by choosing the type of directory
you wanting to store the data in so it
could be HDFS as well it could be ntfs
as well it could be a ram indexed
directory as well so wherever you are
wanting to store the indexes ok I would
have really wanted to share some more
business cases ravindran bathtub do
delete time because anyway we are
supposed to be ending the session at
ten-thirty and i'm pretty sure that i
would not be able to cover absolutely
all the slides that is why i'm kind of
rushing through them so i would
definitely would have shared some more
business cases in fact and not in my
classes I definitely you know encourage
everyone to share
business cases from their domains plus I
have lot of business cases I i generally
discuss a lot of case studies as well in
my classes okay then ash has another
question what are advantages of solar /
MongoDB okay so MongoDB again is similar
to this as well so they both run on the
similar you know i would say similar
principle of no sequel as well but again
with solari you can have joins as well
you can join the queries MongoDB would
not really let you do that okay nishith
has another question does having large
number of fields in a document slower on
the Nexus indexing procedure how do you
optimize it okay so yes it would
definitely affect the processing because
you would have to process more number of
fields so again to optimize it I would
say that you can basically write a
program by way of you can index or you
can analyze the fields in the customer
way I mean you can write that on your
own or I would say that I mean you would
need more processing power in that case
that that can also enhance or that can
also enable a quicker indexing procedure
so if you have more number of filters or
more number of stages of filtering
definitely that would slow down the
process but that's only the indexing
procedure that is going to slow down so
you can probably index the data or
whenever you are experiencing less
number of hits so you can give more
processing part of the indexing
procedure altogether yeah that's right
it's I could so um we comment down to
the soulers architecture here so this is
what I was talking about Anita when I
was saying that you would see that the
huge chunk the the majority Chung has
been occupied by apache Lucene so all
the cores searching the index reading
and the searching capabilities along
with the analysis capabilities along
with the indexing and the index writer
capabilities
they all are being rendered to solar
from Apache you seen so this is how
they've been extended extending in terms
of adding additional and the advanced
features so we have been we have been
speaking about that it exposes the
leucine API through you know rest
restful services so this is how these
services are being exposed like the
admin console has been you know
accessible through the / admin the /
select and these / spell so this is this
is where my spell checking component has
been enabled so the request handler is
is the guy which basically takes up the
request in in the way like a proper you
know you know how should I tell you that
like a request can be sent across to
this like a local host 89-83 is the
default term default URL which you would
definitely hit in case you're accessing
these solar so if you say that / admin
it would take you to the admin console
if you say / select it would take you to
the querying console so this is how this
entire thing goes about so all the
capabilities are accessible through this
so the request handler is basically the
way each of this request is going to be
handled so there is a different
implementation altogether which has been
rooted with respect to each of these
requests which are received on this
these particular rest api is so these
search components also a form a part of
a request handler like they can be used
individually as well like they can be
accessed individually you have the
configurations which are available to
monitor and control the behavior of each
of these search components like how
would you want to choose the way this
query search component to behave how
would you want this highlighting to
behave how would you want the spelling
to behave so on and so forth so then you
have the clustering and more like this
and facesitting and debugging these are
a few of these search components however
no one stops
so you can extend the behavior you can
create your own search components as
well which can help you achieve any any
particular business purpose so all of
these configurations are available in
solar config.xml so if you look here we
have listed you know in the middle two
configurations file to configuration
files which basically controls I would
say eighty-five percent of the behavior
of the solar instance the schema or XML
which basically lets you control the
schema of the documents which are coming
across in your solar instance and the
solar config.xml which basically let's
see you configure these request handlers
the search components and all of these
advanced features like faceting
filtering search caching highlighting
query parsing and analysis so you can
control almost all the advanced features
through this solar config.xml along with
that you can you have the request
writers which which are also present in
this architecture so request writers are
the ways in which you would want to
present your responses like how would
you want to write your request and how
would you want to you know get the
response like what all formats so it
could be XML binary JSON it could be a
Python as well it could be CSV as well
then also the update handlers which are
basically there to control if you are
wanting to send in the updates on the
already indexed data so you can send
that in the XML CSV or binary format in
the JSON format as well so you can send
in the updates in this format as well
which are basically there to update the
already stored data the update
processors are the processes which act
on these update handles whenever you are
updating the data you can choose to
maybe encrypt the data at the time of
creating the indexes you can also do a
different type of logging altogether or
different or the custom indexing as well
this is another thing which I would like
to discuss here I've just given you the
reference of Apache tikka this is also
um you know more pertinent to of the
update handlers like I might want to
send in the you know a word document or
a PDF document so my Apache tika which
also comes as the part of update handle
it basically extracts the data from the
from particular file format and indexes
that particular data in my solar
instance so that entire data has been
also indexed like data which has been
picked from any other you know text file
or CSV file then we have the data import
handler as well which is also a type of
a request handler which you can
configure to be accessible by any say /a
data import so you can configure that on
that particular REST API like / data
import and you can you have this another
configuration file which can let you
import the data from RSS feeds or any
traditional database you would like to
bring into your solar so this is this is
something which can also be achieved
through solar so you can configure this
data import handler which can import the
data from different sources then you
have the index replication as well which
is basically there to make sure that the
data has been replicated as per the
replication factor which has been
specified in the solar config.xml and
whatever the replication factor is that
many number of copies of data has been
maintained for that particular data set
is that fine guys we have few questions
meanwhile okay okay manisha the question
does it do data analytics as well like
most used text in search etc yes Manish
you can definitely have such kind of
implementation so we do have this plugin
which arm
you know gazes the popularity matrix of
each of the document like which is the
document which has been most searched
like you can calculate another matrix
which counts like how many times a
particular document appear then they
appeared in the search results so that
basically monitors the popularity index
of each of the document so such kind of
analytics you can definitely do okay are
there any vendors who distributes solar
ravindran and asking so solar is
open-source ravindran I mean um there
are vendors were extending the basic
functionalities so there are Hortonworks
and lucid works which provides
consultancy in such you know
applications which are using Salar so
otherwise salar is open source you can
happily download it and you can start
using it right away so it's as simple as
that it's just about executing a you
know shell script and you you get to go
okay couple is asking index replication
only replicates indexes naughty files
yeah that's right couple only the
indexes are being replicated to
supporting to support the search results
okay Anita has another question is data
moved physically physically to solar I
think you're asking or only creates
indexes on the remote documents okay
physically to solar okay oh well
whenever the document has been read from
any of these sources like I'm talking
about the reading the data from the
traditional I DBMS or from the feeds the
data stays where wherever it is we're
just maintaining the indexes at arrange
the data is not really moving okay can
we install solar in windows yes you can
do that finish so you have the
distribution available for Windows Linux
or drum whichever type of system you're
using okay is it possible for doing
custom ranking in
Solar Inc is asking yes that is possible
as well so you can overwrite the ranking
mechanism so you can you can definitely
do that so you can write in your own
class which can define that how do you
want to you know let the documents rank
and instead wherever we are you know
defining the ranking you know search
component you can define your own class
you can get the reference of your own
class and the ranking would be
calculated on the basis of your logic
instead of the default logic ok
ravindran says I'm sorry I missed the
distribution answer I'm saying that
abandoned and it's it's open source sore
eyes open source however the consultancy
of various other formats are being
provided by Hortonworks and lucid works
ok nishat says can we update indexes for
a partial set of documents yes you can
do that ok so let me just move ahead I'm
already running out of time is it fine
guys if we can extend for say 15-20
minutes will that be fine i'll try to
wrap up things as quickly as i can ok
thank you so much so this is another way
what is the procedure of search like
liquid i basically defines that what
type of query are you searching like you
can define the handlers the request
handlers here so more or less whenever
you send in the request it has been
given to the request handler you can
choose to define the definition type the
way your request should be read how it
should be broken like this act as the
query parcel so this query parcel is the
guy which breaks the free text into the
logically framed query which has been
sent across to solar for search results
then the query field is something which
act as like we you used to say that
select name
m-mom and department and say the age
from the employee table so this qf is we
is going to specify that what our fees
are we wanting to query right then this
start is going to specify like from
which record number are you wanting to
read the data from and how many rows are
you wanting to read the filter query is
little different from the qf so qf is
that we are selecting only those fields
from the data set
however fq is basically performing the
additional filtering from on the data
which has been fetched from the index
store so these basically does the little
more processing on the results which
have been fetched so more filtering has
been done and the WT Spence specifies
the writer type like you can choose in
which format are we wanting to read the
data so this is how the near real-time
search has been done which means that as
soon as you push in a document for
indexing and the next moment you search
for the document that is available for
that is available in the search results
so this is basically controlled by this
commit property auto soft comment
property anyone who is aware of MongoDB
would know that something similar to
this is also available in MongoDB as
well so this is the time where it you
can specify that in 100 drum or maybe
thousand milliseconds this particular
document would be available and it would
be synced up with the other drum indexes
and would be available in the search
results so we would take a quick example
of that as well okay similarly we have
the real-time get which basically
enables you to fetch a document with its
ID so real-time get is like as soon as
you push in a document and if you
wanting to fetch the document using its
ID you can do that by a you know
retrieving it from its unique key so
this is primarily very very useful
whenever you using salar in the no
sequel mode no sequel datastore mode and
not just as he search index so something
similar to like we used to do in you
know Oracle and the my sequel like we
used to fetch a particular record by its
unique ID
so we can take quick few examples on
that and you guys to want to take a
quick demo on this or rub should I just
move ahead with the topics and take the
demo in the last what do you guys
suggest okay demo first okay all right
I'll go with that so I quickly would
want to show you okay solar and imagine
I've just you know downloaded the
distribution of Solara for ten not for
and I'm simply i have just unzipped the
distribution here at this location and I
simply do this start it's as simple as
that ravindran absolutely no licenses or
anything per se is required so the
nature said that we can take them over
the last but looking at the pole and
when people wanted to see the demo first
so I hope you don't mind that Dinesh
okay all right so as soon as this has
been done it is going to read all the
schema or XML and the solar config.xml
and all the configurations and it simply
lets me access it so this is where this
is how my admin console looks like
increase the window size Anita says are
you not able to see the window Anita any
any issues with it that was just the
console I mean okay so this is how the
admin console looks like so you have the
dashboard which tells you about all the
JVM properties and stuff which tells you
about the system properties as well it
tells you about logging so we have
discussed about logging so how do you
want to really do that
so it tells you about all the components
which have been configured and if it
wasn't able to you know figure out any
component it would throw you an error
here you can choose the logging level
here as well like if you want to see the
info or the Warlocks this is the core
admin so core admin the core is the
collection so core is not really the
literal meaning core means the
collection so I have these many
collections or job just to understand it
better I have these many number of
tables like in a traditional database i
would say so I have these many you know
koers here I can further add any code
from here but for that I need to have
this directory present in my solara
folder and I need to have this
configuration and this data folder in
place and as soon as i refresh the
screen I would be able to see that code
added here so it is going to list out
all these things here like if I'm
wanting to optimize this or if I have
made any configuration changes i can
reload this core as well if I made any
subsequent changes here which are not
supported by solar it this this
particular Cora a cover is not going to
be loaded I can also swap it like if I
have enabled the replication here and i
would like to you recover it from the
replicated to a core i can swap it and i
can make the other replicated core as
the one who's going to receive the
request on behalf of this i can rename
it as as well and i can unload it as
well as in how it is required these are
the java properties again pertaining to
this particular java instance this is
the thread dump this is where i can
select each of the score like i am
wanting to use this particular core or
this particular collection to use the
data from this is going to tell me what
is the version like how many times i
have changed sleep data on this this is
going to tell me that how many documents
i have what is the heat memory and stuff
ok this is the analysis way like if i'm
wanting to see that i have so in so many
fields on the schema or xml how is this
going to
analyze the data which are sent across
so it is going to list out all the
properties so this is how I can in you
know configure my data import handler
like I have configured I think on this
one so this is how my data import is
going to look like like I have
configured a data import handler like
importing the data from a my sequel
table so I can configure this like this
so this is how it is going to look like
I'm simply going to say execute and it
is going to dump all the data from the
my sequel DB collection to this
particular collection it's as simple as
that no need to write any complex
scripts and stuff no code as well as
required this is where my documents can
be pushed into like just to give you an
example of how a normal document is
going to be indexed like I say webinar 8
and i say title is new age search
through solar and i say some mid
document status 0 means that the
document has been inserted successfully
now this is the query console so if you
look here this is where my request
handler is sending all my request to so
select is the guy by default this is the
star dot star which means select star
query on that particular table if q is
the guy which does the filter query
sorting if I am wanting to have any type
of sorting on my particular you know
result set the start in the end they
start any number of rows I would want to
see field listing is how many feels am I
wanting to see this is different from
the qf guys ok so field listing is
fetching all the data but I am just
wanting to see these many fields and
this is the writer type so you can see
the results in the JSON XML Python Ruby
PHP and csv format so for now I'm just
wanting to see the results in the JSON
format so i say that i'm looking for ID
equals two webinars eight and this is
the guy which I've just pushed in so if
you look here I've got this thing
these are the additional fields which I
have configured for each of the
documents just to maintain if I make any
changes this version he is going to be
changed is that fine guys any questions
on this this is as simple as that
however if you're wanting to see i say
i'm looking for simply research so i've
configured a default field here and i
can search my document through this as
well so all the documents here which
basically you know had the word search
in the body they are going to be listed
here is it clear guys it's as simple as
that it's very simple isn't it so can
you just resume mac too can we just
resume back to the presentation guys
let's cover few more topics before we
dispersed for the day so i'm not going
to take any more time I mean anyway I
have extended for seven minutes so I'm
not going to take any more time and I'm
just going to win it as soon as I can so
another important thing is the solar
cloud so solar cloud is basically the
newer which is available in the newer
versions of solar after version for dot
0 solar cloud was made available so this
is something to support the distribution
search functionality in a inner
application so this is something which
can help you set up a cluster of solar
servers and it can help you achieve the
foil tolerance and the high available of
you know sell our servers so indexes are
always available in case any one of them
goes down there's another one which can
take the place or which can take up the
request on behalf of that so which has
got down so it's a very flexible
distributed search and indexing
mechanism without having to have a
master node or allocate Nords charge and
replicas so this this does all by itself
so how how it achieves that is that
instead of using the master or something
similar to that it uses the zookeeper
guy so zookeeper is the cluster manager
again which reads and maintains the
configuration and as per the
configurations which we have provided it
it is going to manage all the time so
any time you send across see data the
zookeeper guy is going to figure out
that
which you know server this this guy
should go to so this is where my zoo
keeper is going to you know sit and this
is the architecture of the sera out so
this is the centralized configuration
management all the configuration files
are being read by this zookeeper I'm
preferring to have more than one
instance of zookeeper as well just to
ensure that in case zookeeper goes down
I have another one to take care of that
so this is this is where my overseer as
well as going to reside in too so
imagine I have millions of documents and
I have millions of users so considering
the first case the millions of documents
are there which are supposed to be
indexed through these solar cloud I am
going to hit a rest api which from where
I'm going to send the request for
indexing which can come into the XML
JSON or HTTP format as soon as these
documents has hits the main server by
load balancing it is going to assign
either server one or so over to to me so
just to ensure that we understand these
terms clearly we have if you notice we
have few key terms here the leader is
there the replica is there these
replication is there and the sharding is
there right so just to understand these
the leader is the guy who has the right
control of any particular shot so just
to understand what shard is before that
please understand I mean I hope you must
have understood by now a collection is
you know a search index which has been
distributed across multiple nodes with
the same configuration so we have said
that collection is like a table so
imagine what shard is going to be like a
a table has been distributed in small
chunks the data of that particular table
has been distributed in small chunks and
that has been placed on different nodes
so these chunks are called shards the
small piece of that table data is called
shards okay so sharding is a logical
slicing of that particular collection
data which has a name and it has a hash
range associated to it it has a leader
and it has a replica so what leader does
to assured is that it is the guy which
has got the right
roll on that particular shot so we have
we can have several replicas which can
read from that child but we just have
one leader who can ride on to that shard
and this is achieved through the process
of replication which makes sure that
whatever the replication factor is we're
going to maintain as many replicas of
that particular data so every time the
leader is going to write something to
this shot like any new document which is
coming across and we are hitting a
particular node so uh no diz nothing but
a jvm process of that solar which is
bound to a particular port number 89 84
in this case in 89 85 in this case so
this this is counted as a node 1 and
this is counted as node 2 we have solar
web application which has been deployed
on each of these so I can run multiple
instances of solar as well on a single
machine by binding them to a different
ports they would access different nodes
and I can enable the sharding and the
replication factor of 2 so as to achieve
this kind of structure ok so every time
you push in the data that has been you
know written by the leader and the copy
of that data has been sent across to
replica so as to make sure that if
anyone is because this replica is the
guy who's going to basically serve the
read request so this data should be
synced up with the replica so that the
consistent data can be sent across to
the reader is that fine guys in case
this leader node goes down in that case
the zookeeper is the guy who has the
control of the leader election algorithm
as well so this guys supposed to do the
centralized configuration management all
the configuration is supposed to be read
by this guy along with that it is
supposed to manage the cluster state
that it should be highly available none
of the nodes should go down in case the
leader has gone down this is the guy who
controls the leader election okay as
soon as this is done immediately a new
leader has been elected so that the
writing of data should not be affected
is that fine so this is a very highly
available architecture which you know
gives you the failsafe you know feature
to your application
early the reeds are also being
distributed so every read request has
been given to the replicas and every
time you have a write request for the
document that has been redirected or
delegated to the leader is that fine
guys any questions on this similarly we
have been reading about that how we can
leverage the soulers capability with
Hadoop so we know that if you have huge
chunk of data in your you know
application you can you would want to
you know optimize the storage by using
the MapReduce or the pic job so how if
you have lot of you know data in your
server and you wanting to achieve faster
searching as well because solar gives
you the fast and efficient and powerful
text searching capability so what you
can do is you can let the data come in
and you can create indexes and these
index files are going to sit on to HDFS
and on these HDFS you can run your pig
job or MapReduce job and then this data
would be organized or optimized for
storage and that can be used as the data
source that index some data store can be
used by solar to serve to serve the
search requests so in all the major
Hadoop distributions like Cloudera
Hortonworks map on you can easily
integrate the solar couple is asking me
that can leader do not serve read
request it's generally not advisable
couple because if we have the you know
consequent write request which is coming
across then we have the preference that
leader should be serving the right
request it has been preferred that
leader should only be taking the right
requests that is why we maintain high
replication factor to enable that the
data has been highly available if you
have more number of reads expected on
the application you should have higher
number of replicas anyway leader is
going to be any any way or is going to
be one only for each node
so this is how this HDFS thing is going
to look like I have data which is coming
in in different formats like PDF word or
HTML and this data is going to go
through the process of analysis and
stuff and the Nexus are going to be
created and these index files are going
to be stored on HDFS the raw files then
they go through the process of MapReduce
or indexing a job and these optimized
indexes are going to be stored on HDFS
as well and from my search application
the query and the response cycle from
this solar instance which is based out
of leucine they being served by these
optimized index files which are being
converted by MapReduce annexin job right
in the similar manner we have another
you know implementation wherein we are
using yarn I'm not sure how many of you
have really heard of this this is called
yet another resource negotiator so
considering a system in which there is
Hadoop which is there to perform the
deep analytics only data and solaris
there to leverage the anak query
capabilities and the data is supposed to
be analyzed and it is supposed to be
ingested as well from the real-time
tweets or maybe say real-time RSS feeds
so that can be absorbed through maybe
spark or storm so you you want three
processes to be running in parallel you
want the data to be ingested parallely
you want the data to be stored in HDFS
and you want the data to be easily
searchable as well so you have a data
which has been ingested through spark
and you have the data which is coming in
and storing in HDFS and you have the
data which have been served as the
search request through solar so three
different processes are running so to
support this multi-purpose data
processing platform we have something
which is called yarn so this is
basically separating the resource
management and the job scheduling from
the data processing in a Hadoop cluster
so this is how you can use yarn along
with salon and
maybe the spark distribution and HDFS in
a particular yarn cluster so in other
words it it basically allows you
different jobs to run in a Hadoop
cluster without affecting you know the
resource management and the job
scheduling this is basically segregated
so I mean this is something which might
be more relevant to you guys like what
is the job trend so we are seeing a huge
increase in the job trends off for solar
so I mean anyway a parte solarz
scalability is unmatched so currently it
has been my most widely used a search
solution on the only planet we have
approximately eight million plus
downloads every month and you have
approximately 2,500 plus open solar jobs
so it's been used by anyway a lot of big
giants like Netflix Apple Instagram
blinged in and I mean we anyway have
seen a lot a huge list and it's catering
to almost 100 million plus requests
every day so this is something which is
very much in demand okay we have before
giving in the final word I mean this is
the certification which you can go for
by eureka so we have the batch which is
starting on from 24th October so it is
going to be the weekend bash the classes
happen on Saturdays and Sundays three
hours each day so you have the option I
mean the batches basically you run I
mean Indian time in in morning and in
evening so it says the online life
course you have the live classes for 24
hours you have assignments for about 25
hours and you have the project location
for 20 years along with that you have
the lifetime access to all the resources
and 24 x seven support which is done by
a jew become so this is something you
can definitely add you have the
verifiable certificate as well which can
be added to your LinkedIn as well so
before this
pausing for the day the recording and
this presentation will be made available
to you within 24 hours so you can refer
to that in case some you have anything
any more doubts please feel free to
reach us in case of any questions
queries or feedback before that we have
another question from ravindran that
sort of have any limitations what are
those in any up you know ravindran the
limitations would be on a certain
perspective so I mean if you're wanting
to achieve the search capabilities like
Google definitely this is something you
can go for the first and the only
limitation i see on solaris that solar
works on the standing data I mean the
data needs to be stored on a particular
directory or you know the file system
and all the analysis is going to work on
that standing data that's sitting data
so you I mean it doesn't really works on
the flowing data like Google so that is
the only limitation I see from my
perspective okay reinke has another
question was sharding is it that
contains fewer info of the document um
sharding as I just explained drinky
imagine you have a table data which is
distributed on different nodes I mean
these smaller chunks of a table data
which is the which has been distributed
on different places so they all combine
all these share your shard Zephyr if
they are going to be combined they're
going to form a single table data so
imagine you are distributing the data of
a single table on different places so
that is charting nishith has a question
is it possible to form union of two
documents and solar and search on Union
results yes you can do that nishat
definitely you can do that krishna says
how can i use auto complete ok this is
configuration based their Krishna so had
this mean the real course I would have
asked you to do that I mean that's
that's part of the assignment in fact so
this is on the basis of the
configuration you can make the
configuration changes and you can
achieve this and you can let the solar I
decide that from where it is going to
maybe auto-suggest and autocomplete
these are two things which you can
achieve through Khan
figuration on solar any more question
guys so it was very nice interacting
with the very nice bunch i mean you know
lot of things so it was a nice knowledge
sharing thing ok Manoa says it was a
pretty heavy seminar for me because I'm
from telecom background ok alright
nevermind you have the recording
available to you know she would receive
it by tomorrow so you can watch it again
maybe to digest it further ok thank you
manish you've been a very nice audience
yeah thnkx dinesh glad you liked the
session and the demo yeah thnkx finish
yeah Thank You Anita you guys have been
great bunch thank you you have a good
night good day yes manoj ok i'm glad
anything you liked it thank you debug ok
good night guys take care hope to see
you in the class thanks bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>