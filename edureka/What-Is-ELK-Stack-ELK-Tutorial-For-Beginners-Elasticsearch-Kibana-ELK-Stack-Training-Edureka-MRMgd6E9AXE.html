<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Is ELK Stack | ELK Tutorial For Beginners | Elasticsearch Kibana | ELK Stack Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="What Is ELK Stack | ELK Tutorial For Beginners | Elasticsearch Kibana | ELK Stack Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>What Is ELK Stack | ELK Tutorial For Beginners | Elasticsearch Kibana | ELK Stack Training | Edureka</b></h2><h5 class="post__date">2017-10-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MRMgd6E9AXE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello guys thank you very much for
joining today's webinar session my name
is Satish and I'm your instructor for
today's webinar session for LG stack
this webinar session will give you an
idea about what LG stack is it's just an
introductory webinar session to give you
an idea about what exactly is LG stack
so I will be going through the slides
and giving you the idea about what
exactly it is so let's get started so y
lk stack lk stack is something it's a
basically a tool which is recently been
introduced and which is recently been
adopted by many many organizations all
across why do we need it what is the use
of LK stack in an ops environment in a
production environment so these are some
of the questions which will be coming
into your mind we'll be going through
the presentation and slowly slowly these
questions will be answered so let's get
started this is the agenda for today
this includes lead for log analysis
problem with the log analysis what is
ALK stack features of ALK stack and what
all companies are using LG stack need
for log analysis now we need to
understand why do we need to analyze the
logs now for someone who is currently
working in an IT organization as an ops
guy in the operation environment he
would be pretty much understanding and
he would be pretty much sure about what
role do logs play in his life and how do
it helps in troubleshooting in
day-to-day environment so logs in each
and every application generates some
logs which helps us to get an idea about
how the application is performing
whether the application is performing in
the way it is expected to or there is
some issue which will be going to come
in the near future all those things we
can very well get to know with the help
of logs so each and every application
will have its own way out to visualize
or to
take the logs so logs in each and every
application are written out in their own
manner so it is the first thing which
the administrator should learn that is
reading the logs or analyzing the logs
with the help of logs only we will be
able to analyze what exactly how the
application is performing or if it is
not performing in the expected way then
what error or what exactly is the
problem with the application so as I
said log analysis helps us to analyze
the logs in a better way earlier what
used to happen is I'll give you an
example to support the statement
regarding the log analysis suppose I'm
working in an organization where I'm
handling 500 web servers all of a sudden
someday I came across an issue regarding
one of the web application which is
hosted among one of the 500 web service
now the thing is should I try to drill
down the errors and try to find out on
which server it is actually running and
then evaluate the logs this will be a
very time-consuming process and also a
tedious one so what should we do there
should be a way out by which we can keep
all those logs in a central place and
analyze the logs from it so this is what
log analysis is log analysis can be
centralized or it can be decentralized
decentralized is something where the
logs are generated on each and every web
server and we have to log into each and
every web server to troubleshoot and
drill down the issue this is not an
ideal scenario this is not a good
approach because it's a time consuming
and also you know it becomes very hard
and very time-consuming to solve an
issue this is the reason we generally
recommend to have to store the logs at a
central place for analysis let's
understand how do LK stack helps us with
the log analysis log analysis is a
process of analyzing the computer or the
machine
data which includes collection of the
log data cleaning of the data conversion
of the structured form analyzing the
data and obtaining the result logs are
always unstructured form of data they
are not structured so we have to collect
them in a place we have to extract it
the important information from it and
then we have to convert and then we have
to analyze the data from that
information and then obtain the result
so this is the general flow which we
generally follow for getting the
information because from the logs I mean
there may be some information which is
not relevant to you during that time and
there may be some information which you
are actually looking for
so rather than wasting time in reading
from top to bottom it's generally a good
idea to you know drill down the keywords
and then analyze the data based on need
for log analysis so this is what we were
looking for issue debugging predictive
analysis security analysis performance
analysis Internet of Things and
debugging now let's quickly understand
what exactly these things are issue
debugging so whenever we come across an
issue in the production scenarios in
order to debug the issue we read the
logs so this is the important of
analyzing the logs it helps us to pee
bugs the issue moreover if we keep on
reading logs in a regular interval we
can also reduce the occurrence of a
certain issue or certain error which can
be a predictive analysis like suppose
some day you are reading across the logs
and you find out that there is some
error there is some issue which can
bring down the server or the web
application so this is something which
the predictive analysis is security
analysis logs helps us to I mean if
there is any attack going on on your
application these things are also very
much captured in the logs so on a server
we have something called the access logs
it's generally a routine job for a
sysadmin to go through these security
logs as well so that the c-segment can
very well
find out that whether there is ethos
attack or there is a penetration attack
going on on a particular application and
this can helps us to prevent the attack
from the external intruders performance
analysis yes logs helps us to analyze
the performance as well because it gives
you an idea about how well your
application is performing like suppose
you're going through the logs and you
figured out that the application is
performing slow and what is causing the
application to perform slow so through
logs through analyzing the logs we can
get to know what is actually causing the
slow performance or the degradation in
terms of performance of your application
Internet of Things is something which is
more or towards the application part so
like suppose we have an application
which is taking care which is feeding
the data from an external server and
based on that it is further performing
the action now if there is a case where
one application is dependent upon the
other application then in that case the
debugging of the IO T's comes into
picture IOT is something called the
Internet of Things where the data is
collected from a receiver or from a
sensor and it is being sent across to
the cloud for analysis or for further
actions so this is something called the
Internet of Things this is very popular
nowadays we have lot of devices which
are coming nowadays which are called
smart devices like the inverter or air
conditioners which are having their
smart sensors which are sending the data
to their respective vendors for analysis
for routine checkups and for their
performance whether their devices are
performing good or not so these are some
of the things which have been recently
coming up in the industry problem with
log analysis let's understand what
problem occur with log analysis so these
are some of the issues which we
generally you know come across we keep
on scratching our head these are some of
the challenges with log analysis number
one is non consistent log format number
two is non consistent time format number
three is decentralized logs number four
is expert knowledge requirement now
what non-consistent log format is each
and every application has its own way
out of producing logs like suppose we
have different web applications suppose
a tomcat a JBoss server or an IAS or an
Apache server will have their own syntax
for writing logs so this is what we call
the non consistent log format so we need
to be aware about the syntax like how to
read the syntax or how to read the log
of a particular application these are
some of the challenges which we have in
our environment where we have a
heterogeneous environment of multiple
applications and at times we keep on
scratching your head thinking about what
exactly is going on well there are at
times when we just don't understand non
consistent time format non consistent
time format means each and every
application may have different time
format configured like suppose some may
have the UTC time some may have the
Central Time some may have the Eastern
Time these are some of the things
decentralized logs means like I
explained before each and every server
has their own log directories so there
are located in a decentralized way we
have to log into each and every server
to troubleshoot expert knowledge
requirement means each and every guy in
the team will not have the access to
those log directories to visualize the
logs so these are some of the problems
with log analysis let's go through in
detail about each and every problem here
in this slide we are talking about the
logs which is being presented by the
Tomcat which is being presented by
apache as well as the is now if you
would see here each and every
application has its own syntax of
writing logs so this is a challenge non
consistent time format as I said before
each and every server each and every
application which is located in
different time zone may have their own
server times set up some may have the
UTC time some may have Eastern some may
have ISDN some may have Central time so
these are some of the challenges
decentralized logs means each and every
logs
are running on different servers which
are located all across for visualising
for digging down on troubleshooting
further we have to log into each and
every server and we have to you know
check what exactly is going on this is
also a challenge with log analysis
unless and until we don't find a way out
where we can overcome these challenges
we do have a way out actually expert
knowledge so everyone generally in the
team don't have access to logs generally
these are some of the l2 or l3 people
who have the technical expertise of
understanding and reading the
information from the logs so this can
also helps in reduce the time for
resolution of a problem so these are
some of the log management tools which
are available in the market Splunk gray
log lochley log entries and sumo logic
some of them are enterprise solutions
which are licensed based what exactly is
a lk stack lk stack is also doing the
same thing what these tools were doing
which I showed you in the previous slide
now let's understand what exactly LK
stack is LK stack is a combination of
three open-source tools named as
elasticsearch log stash and Cabana what
exactly elasticsearch locks tat and
Cabana are these are the three separate
products which were introduced or which
were developed earlier by three separate
development vendors later on it was
being acquired under a single umbrella
by a company called elastic search so
elastic search is a single company now
which is taking care of development and
releasing of these software products or
the packages of the e lk stack so
Yelchin stack is a combination as I said
it's a combination of three open-source
tool which form a log management tool
platform helps us in searching analyzing
and visualizing the logs generated from
different machines these are some of the
features of elastic search so let's
understand what
these three components do because these
three components are together making up
the ELCA stack each and every of these
three components have their own roles to
play let's understand what elasticsearch
is elasticsearch is a tool which plays
major role in storing the logs in the
JSON format indexing it and allowing the
searching of the logs now let's
understand what elasticsearch is this is
a tool which generally works on the data
which is been collecting the data which
is being collected is being converted
into or it is being indexed so that it
can be retrieved into are useful
information whenever we want to this is
the main role of elasticsearch features
it's a search engine like any other
search engine so it works upon data
convert the data into information it's a
no SQL based database that is the reason
it is kind of really fast it doesn't use
the SQL for queries it is based on
Apache Lucan and it provides a restful
api s-- so whatever communication that
happens with the rest of the components
this happened with the help of these
rest api and the best part is it is
based on the Apache Lu kill you see and
that is the reason it is fast it is
really fast in searching and indexing
the information from the backend it
provides the horizontal scalability
reliability and multi-tenant
capabilities for real-time searches it
uses the index to search which makes it
faster so the main idea and the main
concept behind this is the indexing the
way the elasticsearch index the data in
a very nice and very handy format so
that the data can further be evaluated
and taken out whenever required as per
the user and in the very fast manner as
compared to other products in the market
companies which use elastic search are
Guardian StumbleUpon wikipedia
soundcloud these are one of the most
famous github these are some of the
companies which use elastic search
a product in their application so
elasticsearch is not only working with
lk stack but it's a separate product
which helps us in doing the indexing
upon the information like suppose it's
more or less like a Big Data thing where
we are working upon a huge volume of
data and we are extracting information
out of it
what exactly is log stash log stash is
an open source tool which used to
collect parse and filter sis locks as
the input some of the features of this
is data pipeline tool centralize the
data processing it collects parse and
analyze the large variety of structured
and unstructured data in events it
provides plugin to connect to various
type of inputs sources and platform so
what do the log stash do whatever data
which is coming from the servers it is
being centrally taken or it is being
centrally pulled by this tool into a
central place and it is further kept at
a place where the elasticsearch work
upon that data so its main role is to
collect parts and filter the syslog data
as the input so it works as a pipeline
where from one end the data is input
from the servers which are there in the
server form or which is there in your
environment from the other end
elasticsearch takes out the data and
converted into useful information so it
works as a data pipeline it's
centralized the data processing and it
collects and parse and analyzed the
structured and unstructured data let's
understand what Cabana is kibana
is a web interface which is allowing us
to search display and compile the data
so this is the guy which is responsible
for presenting the data in the visual
format in your user interface so it has
a very handsome user interface which is
capable of designing the charts bar
graphs and whatever reports you want to
give it to your manager
it is very much capable of giving you
every kind of information in the form of
a report
so it's a very excellent graphical tool
whose capabilities can be extended with
the help of plugins which are further
developed by several development teams
so it has a capability of presenting the
data which is there at the backend in
the form of charts graphs or very
handsome reports which you can present
to your management for analysis so what
are the features it's a visualization
tool it provides the real-time access
summarization charting and debugging
capabilities as I said it provides the
instinctive and user-friendly interface
it allows the sharing of the snapshot of
log search through it permits the saving
of the dashboard and managing of
multiple dashboards so we do have an
option in Cabana which helps us to
customize the entire user interface into
the format in which you like to so
suppose you want to customize the user
interface where you want to add some
tabs into it or you want to add some
charts in the user interface so that
whenever you log in to the dashboard you
gets to know those valuable information
at the first sight so Cabana do have
that capability in it so we can very
well customize the dashboard according
to our requirements so whatever
information which is being collected by
elasticsearch is being presented to
Cabana let's understand how do the ELCA
stack works so these are some of the
servers which are maintaining their own
logs in their own directories how the
ELCA stack is helping here is it is
collecting the log at a central place
from these servers pulling out the logs
with the help of log stash of course
next elasticsearch is working upon that
data which is there in the data pipeline
which is being collected by log stash
and it uses the search and analysis to
index the data into useful information
later on Cabana is presenting the data
into the form of charts and graphs the
same data which is been collected by
elasticsearch collected index into
useful information so this is what I LK
stack
is there are a lot of products there in
the market which are licensed based
trust me guys I mean it is very much
competitive as compared to those
licensed product which are they in the
market plus being a advantage of an open
source product you can very well
customize it as per your requirement so
the best part about this product is it's
an open source so let's see what all
companies use LK stack so these are the
companies who use CLK stack Netflix
stack Flo Accenture LinkedIn medium
tripwire IFTTT OpenStack very famous
HipChat SWAT IO so these are some of the
companies which use e lk stack in their
environment from managing their
applications like suppose Netflix it's a
very famous company which provide the
media library so movie as a service or
what all things video as a service so we
will be giving you a high level
demonstration regarding the ELCA stack
how to the e lk stack look like in
production environment I do have a l ki
stack ready it's my end it's not a
production in one window it's just a
home lab and I've tried to simulate all
the possibilities which I could do from
my end so I'm going to jump to my jump
server from where I can access my lab
environment so these are just bunch of
some linux servers on which the lk stack
is running and i will be simulating the
logs from a tomcat server a puppet
master and a puppet node so let me just
see so this is the node from where i'm
actually i have the ELCA stack running
right now and if you could see here it
is able to fetch the queries from my
respective service which i have
configured jenkins puppet node let me
show you my dashboard environment
Gabanna dashboard will open through the
URL of the IP address of the server or
or the hostname fqdn the port number
will be five six zero one so in my case
it is 10 or 10.1 or 22 I don't have a
DNS at my place working right now so
I'll use the IP address instead of the
hostname with the port number five six
zero one so as soon as you enter the URL
it will take you to this window where
you have to configure your index pattern
index pattern is something which is been
used at the backend for extracting the
information here in my case I have
already configured the file bit as the
index pattern if you want I can delete
and I can recreate it for you so I have
deleted it from here so the first thing
which we will do is we will configure an
index pattern the index pattern will be
the same which will helps us in
visualizing the graphs which Cubana will
show in the front end so here I'm using
the file beat as the index pattern
because I am using file bit as a
back-end to fetch the logs from the
environment so I'm going to index based
on the time so I will be clicking on
this index contain the time based search
the time field which I will be using
would be the timestamp so I'll hit on
create one thing we have to keep in mind
whenever we are adding an index pattern
if at all whatever pattern you will
enter should be there in the elastic
search if at all these patterns will not
be there in the elastic search then it
had case it will not allow you to
proceed further and it will give you
this error that unable to face the
mapping do you have the index matching
the pattern so you have to take care of
these things
so whatever index you have at the back
end you can use that only so here I have
configured the index so as soon as you
have configured the index we will be
able to see these fields now let's go to
the discover page so it generally takes
some time
I already have some previous data
available so I share it to you using
that previous data so as soon as you
will go to the discover page you will
get these this dashboard this dashboard
contains few entries which we will go
through one by one few things which we
have to see basically if you see there
are four tabs up discover visualize
dashboard and settings discover this is
the main page which have access to most
of the information visualized will help
you to visualize the information in the
form of different visualizations like
charts like graphs like maps like line
charts vertical charts and all those
things all these presentation can be
saved and it can be presented on your
dashboard so as I said before you can
very well customize the dashboard as per
your requirement so there are certain
things in your environment which are
very important certain things which you
always want to have an eye on it so
those things you can present it on your
dashboard so that you can show it on
your big screen so that the ops team can
very well be having a look every time or
whenever something bad happens you can
just very well go to see the screen and
we can very well figure out how the
environment is performing settings we
have already gone through the settings
tab and we have configured the index
pattern as well so I'll go back to
discover again these are the logs which
are being collected if you could see
here so you can very well find based on
the search pattern it will be quick you
can do it as a relative search like from
what time duration you want the log
interval to be like suppose if you want
to search the log from a particular time
duration one day ago two day ago a week
ago so these are some of the default
which you can use here if there is some
specific time interval which you are
particularly looking for so this is the
tab which can help you where you can
just enter the time duration quick hit
go and then for that time duration only
you will be able to see the search here
these are the documents documents are
nothing but
these are you know those informations
which have been collected and they are
indexed into documents so documents are
some kind of what are you going to say a
JSON document which is running at the
backend so let me show you so each of
this column contains lot of information
so if I'll go here so this will give you
a tabular information regarding what all
fields are there if in case you want to
get the information in a JSON format
this is the tab which will help you with
this information right now field meat is
collecting the information and present
to read Qabbani
in the upcoming releases you may have an
option to upload the patterns to upload
the data directly through cabaña to the
elasticsearch so there will be a tab to
upload the format and then we will be
able to visualize the graphs so for a
situation where you know in the
production environment we cannot keep
the previous old data on the server all
the time because we do have space
constrained so that is the reason we
always try to export the data and we can
keep it for our later audit purposes so
if in case in future someday an auditor
comes and he wants to visualize the
previous data you can very well upload
it from the front-end these are
available fields so you can use these
fields to get more information about the
data like if I'll add the host
information if I'll add the source if
I'll add the messages and by the way if
you'll click on any of these available
field like suppose I'll give you an
example of process ID so this will give
you the count quick count of top 5
process IDs going further this is the
tab where you can search of a specific
pattern like suppose I want to search
about Tomcat logs
so here what I am doing is I am trying
to find out the Tomcat logs of
yesterday's till today as per the
current time if in case you want to
narrow down the search select the
specific time interval enter a query so
there are different queries which can be
entered here like suppose here if I am
looking for a Tomcat logs I can also
look for logs which are not Tom cast
based so I'll use not Tomcat so these
are non Tomcat based logs these are the
puppet node logs which I am looking here
if in case you want to enter the
messages where is it
timestamp will give you the timestamp it
will give you the logs source if in case
you want to see the log source so this
is how you can very well access the
information here one thing more which I
want to show you each and every search
you have an option to do a new search to
save the existing search and if in case
you want to load the saved search you
can also do that so if in case I want to
save this search
so this is basically puppet node what
logs are these let me see these are our
log messages okay so this says that it
is stopping the file bit so these are
some of the previous errors which I
would have got while configuring the
environment so if in case I want to save
this I can do that other than these what
all options we have let's have a look
quickly actually what happened day
before yesterday I was just trying to
configure these and one thing which you
always have to keep in mind you should
have the time synchrony in place with
all your nodes what I really mean to say
is all your node should be time
synchronous with respect to your
external ntp server or however you want
to do like so you should have or the
time synchronization in place they
should not be a major difference in the
time synchronization otherwise you may
not be able to get that data which you
are actually looking for in the ELCA
stack like suppose i have a web server
which is not running with the time
synchronization and it is having some
other server time and i'm trying to find
that by setting the time interval from
the top down then in that case since the
time interval is something specific
there on the server it will not be able
to show me here so some things which you
have to keep in mind so let's go to the
visualize tab so this is how we can very
well visualize the charts the
information which is there at the
backend so I'll show you how do we show
the information based on the pie chart
we have an option off from the save
search like suppose if you have saved a
particular search like I did for the
Tomcat if you want to create the reports
based on that you can select this option
or if you want to create the graphs
using the pie chart using the new search
you can hit here so this will give you a
pie chart where you can very well use
the aggregations but before that we need
to understand what aggregations are
aggregations are some way to present the
information and whatever all these
documents are stored it
and in the form of buckets so I am kind
of creating a bucket here for my
customization like how do I present the
backend information here like suppose
these are the options which I can use I
can use histogram range data range ipv4
range terms filters and significant
terms if I click on terms here so I'll
get these fields so these are those
field names which we saw when we were
doing the discovery so like suppose I
want to search or I want to present the
process IDs top five process IDs here it
is showing if you want to save it
process information
saved other than that if you want to
look around for messages these are the
log messages like the density what all
servers are creating what's messages
right now it is showing four Tomcat
because during that interval maybe the
Tomcat would be the only server from
where we are getting the logs because I
configured the entire setup in a
duration of time so what all things we
have like suppose the host you want to
search so if you would see here Tomcat
and puppet node these are the two hosts
from where we are getting defeats and
one more thing which you can do here you
can use the ascending or descending
order if in case you want to based on
how do you want to present the metric we
can use the size like if I am selecting
the top five here unfortunately I don't
have even five servers in my environment
which I can demonstrate to you in this
setup during that duration there were
only two servers if in case you want to
present only one it will show you only
the top it will show you the top two so
this is how it is one more thing what
all thing we have let's see in the
option tab we do have an option to
represent this as a donut form like
suppose this is the page if I select
donut it will show me the visualization
in the form of donut so process
information in the form of donut
saved so this is how we use the pie
charts let's go to the visualization tab
again like suppose what I can you do
here is the line chart so here also you
have those two options you can do it
from the safe search on you can do it
from a new search so save search is the
option like suppose in the production
environment if in case you already have
a search pattern and you want to be more
focusing upon those search patterns only
and your management wants to have the
report based on those search patterns
only because those patterns are the most
important for your production
environment so you can use that option
in that time I'll select x-axis here
I'll do a data histogram timestamp it
will be interval will be automatic this
means interval automatic means so if you
see here the axis it will resize itself
automatically so we need not have to
worry about so if you see here we have
the time stamps during which the logs
are collected and generated split lines
terms so you can also have a granular if
in case you want to input based on JSON
every tab has an option for adjacent
input so if in case you want to input
something in the JSON format it will
present it to you field like suppose I'm
using the log source
so for Tomcat and puppet note it is
showing the log source locks or screens
that will be I think the wire log only
so that is the reason we have only two
lines let's try to find some more graphs
maybe process ID can give us okay so
this is the process ID information if in
case you want to find so the entire
setup is based on plugins if you want
the elastic search or Cabana to present
you some more information you can add
plugins you can add functionalities if
you in case these strings they are not
suitable to you you can very well
customize these or add your own if you
in case you want to what I'll do I'll
save this line chart as well
line chart for process so this is how
you can use these visualization to
present your data I don't have very much
data because you know this is a lab
environment and you know the servers
from which I am getting the data is
itself not running any application so
log generations are not very frequent
but in the production environment you
will have hell lot of data and you'll
have very beautiful graphs so this is
the tab where we can create your own
dashboard want to add a visualization
suppose I want to add the donut which I
created I can do that here I can present
the chart I can present the line here
line graph so this is how I can
customize my own dashboard moreover you
can very well drag drop these like
suppose if in case you want to bring
here
yeah you can do that you can take this
if in case you want to take this here
you want to resize these you want to
make it more descriptive you can very
well do that you can save this dashboard
my dashboard
store time with dashboard
so these are some of the options which
we can use and present the data here
also it is giving you the option to save
the dashboard if in case you want to
load the save dashboard you can very
well do that different case you have
multiple dashboards so this is the
option for sharing adding the
visualization so this will give you
present the data during that interval
only for the interval with which we are
really important I think we have pretty
much covered all the tabs let me see if
in case I miss something
yes so these are the valuable fields
which we have already covered this tab
also we have covered so if you could see
here I can see the number of hits which
I am getting
so each and every page you have will
give you an option to save the search or
if in case you want to do a new search
you can click here do a new search like
suppose I want to look for I am not sure
if I have Apache with me do have so this
will give you the logs related to Apache
for that duration which I have selected
and based on this information you can
create the charts so there is a lot
which you can do with this dashboard
because the customization of the
dashboard depends from environment to
environment I cannot simulate each and
every internet but I can give you an
idea about to what level we can extract
the information and keep it and use it
for our requirement so the customization
of this dashboard is as per your
operation needs like suppose if you are
very much focused about Apache logs or
you are more focusing towards the puppet
logs so based on that you can very well
create the grass risen the graphs and
all those things you can do so that was
all for today I hope this webinar would
be informative for you and thank you
very much please leave the comments to
subscribe please visit our website
www.graebert.com off to like it and you
can comment any of your doubts and
queries and we will reply to them at the
earliest do look out for more videos now
playlists and subscribe to our in Raqqah
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>