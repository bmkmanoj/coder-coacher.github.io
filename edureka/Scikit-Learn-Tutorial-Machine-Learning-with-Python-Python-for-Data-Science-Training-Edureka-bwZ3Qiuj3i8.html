<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scikit Learn Tutorial | Machine Learning with Python | Python for Data Science Training | Edureka | Coder Coacher - Coaching Coders</title><meta content="Scikit Learn Tutorial | Machine Learning with Python | Python for Data Science Training | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scikit Learn Tutorial | Machine Learning with Python | Python for Data Science Training | Edureka</b></h2><h5 class="post__date">2018-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bwZ3Qiuj3i8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey guys this is Ephraim at Eureka and
welcome to this session on sidekick
learn tutorial now scikit-learn is an
attractive library for machine learning
so it's thought of this session by
understanding machine learning and how
cyclic learn helps us to implement that
in Python then we likely move on to the
hands-on part wherein I'll help you guys
with the installation of cyclic load and
then we implement various algorithms so
for that we'll be understanding
regression and classification techniques
followed by a black little devil so over
here I will be implementing a scream
classifier as well as logistic
regression and k-nearest nibbles so I
hope you guys are here with this agenda
so can you give me a quick confirmation
or you can just type in your chat box so
that we can proceed
all right submitting confirmations so
Anna is clear so is the big Jenny order
guys since most of you are clear let's
begin with the very first topic of
today's discussion that is what is
machine learning so machine learning is
a type of artificial intelligence that
allows software applications to launch
from the data without any human
intervention and this also helps you to
predict the outcomes as well so let me
take a very basic example to explain
this concept so have you guys ever
shopped online so I was checking for a
product did you notice it recommends
product similar to what you're looking
for or did you see the person bought
this product also got this that is the
combination of products and for every
user needs 15 set of recommendations now
have you ever wondered how are you doing
this recommendation or who does those
recommendations so one thing is clear
that there is no hard coding involved
but they must be something else for this
so this problem is solved using machine
learning algorithms so what happens
based on the users action they prepare a
smart machine algorithm which is able to
predict what exactly is the likely
product that I use it should buy and
this is something which can be done
using machine learning now coming to the
programming world you exactly say what
will be the input and what should be the
output that's where it stays but in
machine learning as in when the data
comes system adjusts itself to the
reality and then it behaves equality so
this is the dynamic nature of machine
learning Oregons an artificial
imperative that is it learns from a dual
meaning you have programmed it once but
every time it encounters a problem it
should not be programmed again which is
the main motive of it so what it does it
changes it code to the new scenarios and
discovers
so this is what machine learning is now
the question comes how we can achieve
this so what we do we provided with
scenarios we provided with the past
experiences we filled in the values and
based on those past experiences it comes
up with a new solution so machine
learning basically it helps us to detect
patterns in a data set and create models
over it and later on we can adjust that
models or you can say enhance the
accuracy of the model so as to predict
the action based on a new data set now
if you look at the image over here so in
machine learning what you do first you
train your model and
any machine learning algorithm which is
built on your data set you build a model
so by training data I mean the initial
stage that is every data is considered
like a training data and then if you see
there is this feedback loop going on
which is performed again and again until
we achieve a good amount of accuracy so
this is the whole idea behind machine
learning and how machine learning works
now machine learning can also be used in
various other domains such as in
analytics it can be used and can be
using weather forecasting it can also be
used to predict what will be the stock
price next day and all of this can be
done using machine learning where the
system is learning by its own or
detecting patterns so that it can take
actions whenever it is exposed to a new
leadership next machine learning is
classified into three types that is
supervised learning unsupervised
learning and reinforcement learning so
we have already discussed all these
three types of machine learning so you
can go back to the video what is machine
learning so for now let me just recap
all these Rilke rhythms so first is my
supervised learning so supervised
learning is a process of an algorithm
which is learning from the training data
set you can think it off as a teacher
supervising the learning process where
you know the correct answer with the
algorithm iteratively keeps predicting
the training data and respected by the
teacher
so this learning stops whenever the
algorithm achieves an acceptable level
of performance so here what you do you
basically generate a mapping function
between the input variable and the
output variable and using algorithms you
can treat that function so it is also
known as predictive modeling which
refers to a model of making predictions
using some data some of the popular
algorithms are linear regression then we
have large segregation so we will be
implementing this in some time then we
have decision tree random forests and
may Bayes classifier then comes your
unsupervised learning so in this process
of learning there is no one who
supervises the process or they have no
correct answer and there is no teacher
so what exactly happened this is a
process where a model is trained using
an information which is not labeled so
here the process can be used to cluster
the input data in classes on the basis
of the statistical properties so it is
also called as clustering analysis which
means you can group the objects based on
the information power and data
describing the objects or the
relationship between them so some of the
algorithms include
realistic ironical clustering etc then
we have reinforcement learning so
reinforcement learning is the learning
by implanting with a space or an
environment so this knowledge can be
thought as a hidden fly method where a
reward or a penalty point is given for
each action so let's say if a machine
chooses a current option so it gains a
reward word or vice versa
so what exactly happens a reinforcement
learning agent it learns from the
consequences of its action rather than
being taught explicitly so it selects
this action on the basis of the past
experiences and also by the new choice
or you can say exploration of data next
let's come to the main topic of a
discussion that is scikit-learn or what
exactly is like tickler or how does it
helps in machine learning so we know
that psychic learn is a library which is
used to perform machine learning in
python now it is an open source library
which is licensed under BSD and it is
reusable in various contexts encouraging
academic as well as your commercial use
which it is also built on popular
libraries such as numb by sci-fi and not
cotton also the best part about
scikit-learn
is that it has many curing parameters
along with a wonderful documentation and
a support community so let me show you
that as well so I just open my Google
and I just type in scikit-learn over
here the first link is my official
documentation of psychic loan so I just
click over it
so as you can see here this is my
official page of psychic load so over
here this is the introduction that
you've already discussed that it is
built on numpy sci-fi my plot live and
is licensed under BSD then we have
various other things like we have
classifications we have digression
clustering dimension reduction modern
search and pre-processing this is again
a part of machine learning so over here
we have some applications and some
algorithms to solve that so here let's
say in classification we have
applications such as spam detection or
image recognition in which the
algorithms use a ystem classifier then
they have nearest neighbors random
forest and similarly for all of them
nothing to go back to my presentation so
now the time we install scikit-learn
package so as I mentioned a sidekick
load is the most popular machine going
package and have a lot of awards already
built into it so what you need to do you
just need to go to your command line
answers
pimp it installs like Hitler but if you
are using an upon the distribution you
can simply type in Conda install
scikit-learn
now as I mentioned that it already
contains a lot of ego heads it has a
library such as numpy sci-fi which makes
your work easy with arrays and machine
learning techniques much much easier so
now if you go back to my psychic loan
documentation here you will see that
first we have the whole page then you
have installation guide and you have the
documentation is with set the click on
installation so this is the entire page
width will tell us how we can install
cipher so we have already discussed that
then we have a beautiful documentation
with guide see everything so you can
just explore this up and then we have
some examples of it as well so half of
that we have successfully installed
scikit-learn so now every algorithm is
exposed and scikit-learn via an
estimator so for that first you need to
import the model
so the general form to do that is from
st learn got family import model so
today to specially example let's say i
want to perform a linear regression
which is the first machine learning
algorithm that we have known so we'll
just go ahead and type in from st learn
dot linear and disco model import linear
regression so here my Lena a disco model
is the family and the linear regression
that's the model itself and then you
just need to instantiate that model so
this is how you can implement any
algorithm using psychical moving ahead
let us see the concept of regression and
clustering over in the documentation as
well we have seen various aspects of
scikit-learn where in the first two were
regression and clustering so we will be
going ahead and picking up these two for
this session
so here regression is the prediction of
a numerical values that often takes
input as a cog in his file so as you can
see here in the graph as well we have
continuous value of data points then we
have classification so classification is
the problem identifying to wit set of
categories and new observation belongs
let us understand this with an example
they will have a given set of mail and
you have to classify these mails into
two categories based on the fact that
whether the mail is a spam or not so
this is a problem of classification so
here basically the goal of
classification is to find boundaries or
to define boundaries that best separate
in categories of data now once this
decision boundaries have been framed if
we allow you to differentiate between
the classes of data and classify any new
value so in our case we'll be
segregating the male's whether it is
spam or not so you can go ahead pick up
any data and build your model to
classify them as well so this is the
whole idea behind clustering now apart
from this clustering is used in various
other domains such as it is used in
medical where you can predict whether
the patient is sick or not it can also
be lose an animal recognition we can
classify a set of animal images into
different different categories then you
can also use it in machine vision where
you can specify faces or whenever you
use a camera you can see over there it
can clearly identify different phases
present so therefore a lot of things can
be done using classification so we will
be implementing both of this
classification and regression techniques
but first of all let us explore the data
set that we're going to use here we have
the most popular that is the iris data
set now why iris data set because it is
one of the oldest data sets and carries
out easy supervised learning tasks now
this data set contains three classes of
50 instances needs with each class
refers to a type of iris clan practiced
iris setosa
Ida's virginica and iris versicolor so
it has a strong measurements among all
these species that is the sepal length
then we have sepal width we have better
than and we have better with this width
so there are four features which are
measured from each sample that is the
length and width of sepals and petals in
centimeters so now let us pick up
classification so here in classification
we have different algorithms we have
decision tree we have random forests we
have named Bayes classifier and there is
even classifier this width so we have
already discussed all these three of
them so in this session I will be
focusing on support vector machine which
is also called as SVM classifier so let
us understand the support vector machine
classifier so SVM is a supervised
machine learning algorithm which can be
used for both classification or
regression challenges but in general s
diem is used for classification problem
so what it does it tries to define a
hyperplane which can split the data in
the most optimal way such that there is
a wide
among the hyperplane and the observation
so this is your data points plotted in
this very graph so what the SDM is doing
it is defining a hyperplane which is
able to segregate data into different
different categories so data present at
the right side of the hyperplane will
belong to a different category
whereas data point present to the left
side of the hyperplane will belong to a
different category so it is considered
one of the most efficient algorithm in
machine learning for performing
classification all right so I have a
Koshiro video ok so be big ask me what
is a hyperplane so hyperplane is just a
generalization of a pain so if you talk
about one dimension a hyperplane is
called a point so let me show you as
well so here we talking about one guy
mentioned it see we have some data
points over here
so this is the point or you can call it
as a hyperplane which will categorize or
segregate the other points in two halves
so this very point is considered as a
hyperplane in one dimension now if you
talk about hyperplane in two dimension
equal to that is with
so I have some data points which I will
take a different color this time
so these are my data points now I want
to draw 18 so I want to create a hyper
thing which will they segregate these
data point so I am a hyper pain would be
a line so this is my line which is
basically segregate your data points
into two halves so basically a
hyperplane allows you to categorize the
data points into different classes or
you can simply say it segregates the
data points into different different
categories so I hope I answered your
question you bake
okay the bake is saying yes certainly
great bit so let's move ahead now so now
moving ahead we have an SVM use case so
now what we'll be doing will be
performing an escape classification on
the iris dataset so they've already
understood what exactly in the iris
dataset and what features as it contains
so here is this problem statement so
over here we have to use the ischium
classifier on the iris data set and we
have create a model because classify the
class based on the features so we have
features such as equal length sepal
width better length and we have better
with this way and we have to classify
the flaws so here we know that we have
different species of flag that is iris
the Tosa virginica and mercy column so
now let us go ahead and build our model
for that so I just go to my jupiter
notebook alright so this is my jupiter
notebook so I just go to Python 3 and
I'll give a name to it let's say one
name - scikit-learn tutorial
okay so now let me just brush up the
basics of Jupiter notebook supposed to
foil if you want to type in any headers
or if you want to give heading to your
Jupiter notebook you can simply type in
hash and King let's say let's say you
want to type in heading one and you can
go to the markdown and you can just run
this so shortcut to run this is Shift +
Enter or it can just die to go over here
and click the cell then if you want it
in this header - so we can just type in
- hash and writing any header so let's
say header 2 and run this already have
to change it to markdown and we just run
this so this is my header - similarly
you can do it for header 3 so it can you
just need to type in 3 hashes and you
can say header 3
now if you want to boil your heading so
for that what you need to do you just
need to type in two asterisk and conceal
its say I wanna say it's summer type
side kit I have in markdown so it's
markdown it and I want you just run this
so this is my boil next now similarly if
you want some bullet points on it so now
first I just set it to Malcolm
so I say markdown so over here I'll say
next 8.1 and then I want point so this
will basically show me the bullet point
so if we just run this so as you can see
I have bullet points over them so this
is that the basics of Jupiter notebook
and also for the shortcuts you have
everything over here so you can go over
here and you can click on keyboard
shortcuts so here it will show you the
list so you can just go over this once
so now that is implement a steam
classifier so I just give it a heading 1
that is s team classifier
should be captain switches capitalize it
and life now the first thing you have to
do for machine learning is you have to
import the library so what we'll be
doing his frog sq learn input SVM that
is my SVM classifier and I just run this
alright so just working fine because
I've already inputted a scalar then we
don't need to do I need to import data
sets as well so I will say from SK learn
again SQL on input data sets so this is
the default data set that we have talked
about which is the iris data say so here
we have seen that
Escalon is my sidekick learned library
and we have a steel glass file which we
are going to use then we have this
imported and data set so these are the
two models which will also the SQL and
library now what you need to do you just
need to load the data set so for that I
will pick a variable let's say iris
arises using the data sets I just load
my iris dataset so I'll say load iris
method values Laura this method to
loaded
so here it should be equal to and it
should good a license so using this load
IRS function we have just imported my a
district that's it so now what I need to
do I exist first let me just print the
type of this iris so I just run this now
as you can see here we have data type
called s bunch so this bunch basically
contains the iris dataset and all the
attributes so one of those attributes is
data so let me just type in IRS store
data so this will basically bring all
the features or gives all the
specifications of your iris dataset now
as you can see here I have four columns
and I have some 150 rows now but these
are the measurements so I've already
told you that iris dataset contains some
specifications of the flower so we have
simple length we have sepal width we
have people length and we have fitted
with as well so now you want to know the
feature names we can just type in I
restore feature named Fito
underscore names and just run this so
here we have CP length in centimeters
then second column stands for sepal
width in centimeter then better length
and the petal width so now we have these
measurements now there is one more
attribute called s target so I will just
say IRS dot argot and I will sprint this
so now what exactly is target so target
is what we are going to predict so here
we are getting the values as 0 1 &amp;amp; 2 so
here 0 represents Osric dosa or you can
sit idle sit dosa then 1 stands for your
rosy color and cosines for virginica
so you can also type in here is dot
target means so as you can see here I
have zero for Sentosa then one for Bercy
color and do for virginica
so this is how I can explore any dataset
so now I am in is run our s named
classification so till now we have just
imported her SVM and we have imported
the data set so now what we need to do
we need to create different errors for
storing our dependent variable and
independent variable so for independent
variable let's take it as X and will
store the value that is the features of
it so I will say I restore data and
we'll say colon comma 2 so this is
similar to I log which is used
rose election or column selection now
four columns will be considering the
first two features of it so now it means
go ahead and create my dependent
variable as well so I take it as Y so
I'll just say I distort target which is
the value I need to predict or you can
say which is the species that the flag
longs to so here the class of each
observation or you can say the data
point is stored in dot target that is
the attribute of the data set so we have
assigned this tour date dependant Mahima
that is y and we have already understand
what exactly is my target so we need to
specify whether it is a set dosa it is
over sequoia oil is over Jenica so here
I am just set my independent variables
and dependent variables now what I need
to do I need to split my data so I like
this little data into two subsets that
is my training data and my validation or
you can say the testing data so I will
just typing in extreme X underscore
drink
then X underscore test led by
underscoring and by underscore test
so first so before their first let's go
ahead and import cross validation so I
just insert a syllable search syllabub
and over here I agree writing from s key
alone dot cross validation import might
train listed so as I told you that we
have to split the data so I'll be
splitting the data in my training subset
advocating subset
so then I will be just specifying the
size of it so I'll just use this
function that is trained xsplit and I'll
pass in some arguments to it so I have a
possible independent variable that is X
then the dependent variable that is why
and I specify a size to it and says I
want to split the data let's say zero
point and then what I'll be doing
l beginning a random state through it so
here random state is nothing but it just
ensure that each time we run this for
theta getting the same sampling so you
can specify any number do it in our case
let's take it as 4 so here I have sugar
in an under scope all right so this runs
fine now so now are you to do you need
to use HTM to be a classifier which
helps to classify whenever we are
providing a new data or any data
regarding a flaw you can conclude to
which species belongs to you can say
which fly it belongs
then what is we're doing and creating a
model for that so I say model equals 2
SB m dot s PC and inside that I will be
passing colonel this is equal to linear
and then finally I will be just fitting
my model so I'll say model dot fit
okay so here I just need to fit my model
that is model dot fit and I have to pass
in my parameters that is my independent
variable that is my extreme and
dependent variable that is by train so
now I must be having a potion that what
is this curly so Sdn has a technique
called as kernel that adds a new feature
which is required so as to define a
hyperplane which can basically segregate
your data into different classes so now
let me just explain this with example so
I'll go back to my pain I just create a
new file
suppose we have some data points let's
see
so suppose we have data points plotted
somewhat like this then we have a
different set of data points that have
been plotted around it so I will take it
with a different color let's say let's
take green this time
so what I need to do you need to define
a hyperplane that segregate these two
classes very well that is a tedious task
over here so let's suppose you have X
values over here
so this is my x-axis
and I have by the way
all right so what your colon will do it
will extract a new feature such as the
whole data points can be plotted and can
clearly get a hyperplane segregating
different classes and data points so you
go ahead with some features such as it
says Z is equals to X to the power of 2
plus y to the power of 2 again and if
one water will be reformed as let's say
and check it as black
so let's say this is your new model now
let's say this is your z-axis and X is
this I'll take it if you narrow for this
so this is my x-axis now once you apply
this transformation you can clearly see
your data points plotting something like
this so let me just plot some data
points
and I've got one more set of data points
let's say yellow this time so now you
have one set of data points plotted like
this and another plotted like this so
over here I mean clearly define your
hyperplane this replicating your data
said that is your main goal so this is
what it kernel does
so basically kernel to achieve the
functionality which takes the low
dimensional input space and transforms
it into a higher space in order to
convert not separable problems into
separable problems so that's what we are
trying to do over here so now we have
set our kernel parameters as linear
because we are going ahead with a linear
hyperplane for a problem so since our
model is ready we can go ahead and
perform some predictions to predict a
class of a particular class based on the
test data set and therefore we can check
that accuracy there so now what we need
to do you just need to type in accuracy
and say modern not school and inside
this I will be giving the independent
variable of this subset that is X test I
will compare the predicted outcome that
is widest and then I spring my accuracy
so they'd be saying and you have to
import it first so what I'll be doing
I've been inserting when sale above
I can see below that I just need to type
in wrong dot metrics in both atlases
school
sorry this happens sometimes in Jupiter
notebook whereas if you run the same
code in Python it will run fine but now
on Jupiter notebook it gives a problem
of space so what I have done here I have
just converted all my errors into one
dimension so as you can see here I have
just converted my X train X test using
this function of reshape then what I
have done I just fitted my model over
here
so this I have already discussed that we
have used SVM that SPC and I've told you
the usage of Kernersville and then we
applied linear model to it then we have
just picked my model with the new values
that is X train mode and by doing good
after that we have just used a variable
that is vibrate mode and inside that
I've used the function of predict and
finally was in we have calculated the
accuracy using this function accuracy
score and then we have passed the
variable of this subset so now we have
calculated an accuracy of 96% which is
quite good so let me just recap all of
this once again
so here I'll explain you the basic
subscription notebook how you can type
in heading one header to Hideo 3 then a
bullet points to it then you've learned
about SDM classifier where we are first
imported SVM and data sets then you have
just load my iris dataset we have
printed the type of it which is Bunch
then I've just printed one of the
attribute that is data so this will
basically tell me all the specifications
for an iris flower then I've just
printed the feature name of it that is
the sepal length sepal width petal
length and bit width
after that I have printed one more
attribute that is target and then I have
print the name of it as well so that is
my is Atossa
versicolor and virginica then I'll just
set my dependent and independent
variable so I have an independent
variable as X which say is the features
2 where I just considered my hosting
features and then I have my dependent
variable that is my wife or you can say
the value which means to be predicted
after that I have just imported cross
validation and input this function that
is three nested so I split the data up
between the training subset and a
testing subset then after that I have
used this function train test split and
have passed on the variables and given a
size of 0.2 and I'm given a random state
value a spool so this value can be
anything and is used whenever we need
the same sampling every time then what
I've done I just converted all my errors
into one dimension you don't need it
sometime but sometimes you two throws
this error so what I have done I have
just imported all these arrays in
do my one dimension so use the function
a t-shape and then I simply use a model
that is SDM not SVC and I've used the
functionality of kernel to linear bottom
then I'll just fit my model and pass on
my independent variable and dependent
variable then I have just calculated my
accuracy so over here I have first
imported my accuracy school that is the
predefined function
so using this predefined function I have
just calculated my accuracy so this was
all about my SVM classifier next let us
implement K nearest neighbors so here
first of all I need to select the value
of K which you basically define your
nearest neighbors so here you have to
search for K observation or a number in
training day rather than the mirrors to
the measurement of unknown iris so I'll
go back to my Jupiter and let us
increment this as well so it says insert
some cell below so now we'll be
understanding caning that is so I give
it a heading of head or two that is my
gain and or you can say K nearest
neighbors
so let's go to mark town and I just run
this
so again we're trying to do I need to
load my data so I have already loaded my
iris dataset next what I'll be doing so
I just do it once again so I just load
my iris dataset alright so I have to
first comport it Leslie from SQL Allah
so I'll just import this load I
dysfunction so I'll say from sq learn
the data sets import load is so SK
longer data sets is my module and load
iris is the function from the SK module
so here I have just loaded my iris data
set so now what we'll be doing and I'm
having a variable called as X which have
Ida store data and I have Y which is
equal to I this dot packet so we have
already discussed this above let me
explain the shape of this so I'll just
type in spring or you can simply type in
X dot shape
so as you can see here I have got a 2d
array with 150 rows and 4 columns
similarly you can do it for my as well
so you can say wired sheep so here we
have one be added of length 150 or you
can say we need just one response value
so now as you have discussed to create a
model you first need to collect the data
we need to clean that data or that is
something which is not requiring my iris
dataset then we need to train and test
the data then what we need to do we need
to split the data packets into training
subsets and testing subsets and finally
we can calculate the accuracy Shrek -
implement this so I'll say from SK alone
dot neighbors
input k neighbors classifier so next
what you need to do you need to
instantiate the estimator that is you
have to make it instance of the model so
for that what I will be doing a typing
in KN n Keaney book classifier and
inside this I'll be passing it and NATO
I'm gonna pass in some value to it let's
say my value is 1 so this value will
basically look for my one nearest
neighbor so now let me just bring this
cane in value
Michell has a spelling mistake
okay so here was just a spelling mistake
of nimble's alright so here as you can
see I have a gardener my have leave
sighs I am metric metric panic meters
and all these things
next what I am doing as you train in the
data or you can see it basically it will
learn the relationship between the
feature and the response so what I need
to do I need to fit the model so I said
KN n dot v and inside this and passing
the features and response that is my X
and by then once your training is done
you need to predict your it as well
so will be predicting it for new
observation so I will say it KN n dot
predict and I'll pass on some dummy
values to it so let's say 4 5 6 2
so as you can see here we are just
getting the same matter that we got in a
previous example that is reshaping your
data using the idly shape function so
this error we have already encountered
in the last example when we are
discussing about a steam classifier so
over there what we can do we can simply
convert this into a number array so for
that let me just import mine umpire so I
say import numpy as NP then I'll be
taking the variable let's say a and I'll
say n P dot added and I'll pass some
values to it let's say the same value
that was 4 5 6 2 so now let me just
print the shape of it to see whether
it's working or not
let me just print it a Restless is
working so now I'll just repeat the same
step that is canaan and using the
predict function l to take the outcome
of it you can see the response of it so
I will pass in the value that is a and I
just run this code so as you can see I
am getting an array that has a two value
so over your site it does not know what
the story visits but here we have
already know that 2 represents 4 IDs for
Jenica so that is how we have computed
that for these set of values we getting
the output as - oya Genson is virginica
now similarly again try it with
different key values so here you have
given the value of 1 you change it to
any other value let's say if I give it a
value of let's say food and just run
this and now if I run the last statement
I have to fit this one and then let me
just run this now on it so it again it's
giving me the same about good that is
glue which basically represents my iris
virginica similarly we change the value
of n neighbors okay
Anna's asking me a question this time so
she is asking me how can you find an
optimal value of K or the nearest
neighbors okay this is a very good
question Anna so there are different
parameters for doing it so for that what
we need to do we need to tune your model
with different values of K so we will be
taking this forward in my next session
Anna wherein we work with bigger data
sets and we create clusters as well so
all your doubts regarding this question
will be covered in my next session makes
any other doubt guys you have regarding
what I've explained till now all right
since you guys don't have any questions
so let me take one more example so here
we have taken an example of K nearest
neighbors so it is implement logic
regression on the same model or you can
say on the same data set so for that
what I can do I can I have to first
import my model so I have to type in
from F killer
dot linear model input law justification
so next I'm going to instantiate this
model so what I'll be doing and sale log
are I can say log regression equals to
logical creation
then I just need to fit my model so I
will say Lockhart logging dot fit and
pass on the variables that is my x and
pi so this is my RC regression that is
so now what I need to do I need to just
type in the variable name that is log
read and use it to predict function I
can predict the value so I think the
same example so I have a over here and I
just run the code so here again I am
getting the output as to which basically
represents your is virginica so that is
how we can implement it same classifier
then you have implemented K nearest
neighbors and then finally we have
implemented the same lot celebration in
the same model using the either status F
so that is all for this session guys so
guys it's time for questions now so
three of you guys have any questions so
you can just type in your chat box if
you want I get an you do as well okay so
Anna doesn't have any question okay
Jenny is also saying no okay so I don't
see any questions right now no are you
guys even if you have questions later
you can just come up the doors in my
next session or you can just contact our
support team with this is 24/7 available
for you alright so let me just recap the
things that we have covered in today's
training so first of all be understood
what is machine learning and then you
got into sidekick load which is a fiber
package for implementing machinery then
we took a deep dive into it and they
understood the installation and the
various steps involved in machine
learning such as loading the data
analyzing it splitting the data into
clean intense subsets and finally
finding out the accuracy of it then
given the regression and classification
techniques they will start with the
implementation using the ischium
classifier then we moved on to the K
nearest neighbor and finally be
implemented and evolve the Hoff logistic
regression so that's all for today guys
hope you have enjoyed this session so if
you guys have any doubts of any queries
a little any of the topics that I've
discussed today please feel free to ask
me or I can come up with your motions in
my next class so there's that time I
would take your leave
thank you so much bye-bye I hope you
have enjoyed listening to this video
please be kind enough to like it and you
can comment any of your doubts and
queries and we will reply them at the
earliest do look out for more videos in
our playlist and
scribe to ed Eureka channel to learn
more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>