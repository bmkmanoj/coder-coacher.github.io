<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Demo on Hadoop Installation | Edureka | Coder Coacher - Coaching Coders</title><meta content="Demo on Hadoop Installation | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Demo on Hadoop Installation | Edureka</b></h2><h5 class="post__date">2014-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0RYNpfW-b38" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so these are topics for today basically
so we are going to look at Hadoop
installation so let's get started
alright so now let's get back to our
virtual manage virtualization
environment so this is how we are going
to basically do the Hadoop installation
so so everybody would also basically
need need to have the software
VirtualBox
or you can have vmware from windows so
what you would do is you have to
basically we are going to create one
virtual machine for each of the daemon
that we run in the system so so what you
do is you basically click on new and
then you can basically create a virtual
machine for example I am trying to
create a machine i'll say say lab 1 and
you give the type of ways that you want
so because virtual box lets you is a
virtualization environment so it you can
basically install any kind of OS so
since Hadoop runs on Linux so we are
going to go with Linux and then you can
have different flavors of Linux you can
have Ubuntu you can have basically Red
Hat you can have Linux you can have sent
to it so we are going to go I am going
to do it with sent to us so for sent to
us you see like basically other Linux
and 64-bit then you click on continue it
tells you how much memory you want so
you can go with any basically value
based on the size of your laptop or your
miss machine so I have a 16 gigs
basically RAM so I can make it 2 MC 512
or I can make it 1 MB so you can just
increase the number you can move this
bar left to right so while for time
being I will let it stay 512 MB and then
I continue and then 3 tells that it is
going to create a virtual hard drive so
you can either say no or yes so so
basically just keep on creating this and
then it tells ask you what type of drive
you want so you can see these different
options so usually it's recommended to
use VMDK because VMDK is interoperable
between the much but should basically be
mV
Claire and this way city goes to for
example you want to move this VM image
to a Windows machine if you have VMDK it
would work but if you use basic VDI it
won't work so you did advisable to use
VMDK and then it's saying the hard drive
you want to make it fixed size or
dynamically so usually it's good to
leave it dynamically because it will
just then shrink or basically increase
based on the space when it's needed and
then you can give the name of the hard
drive and then you can say what with the
maximum size of the hard drive so it is
saying 8 right now and then you just
click create so you can see that now it
has created a new virtual machine for me
which is called lab 1 and since I have
not done anything it just it was very
quick machine and so it just created it
so now I can go here and look at the
settings so it will have all the
settings that I basically initially
created so you can say that I specified
a Linux machine and I said the Linux is
sent to us and then on the system side
it tells you how much memory you have
allocated to it on the storage side it
tells you how much storage you have you
have set maximum 8gb and then on the
network side you can basically tell what
kind of network adapter it will have so
you can you have the options of nat
bridge adapter and then you have
host-only adapter so that means that
when you have a vm on that basically you
cannot talk to internet because it will
be behind a firewall and it will
basically assign the net IP to it and
then bridged means that it can basically
talk to Internet and can talk to your
laptop also and host only means it can
only talk to your laptop it cannot talk
to Internet again so in mostly it's good
to make it bridged so that you can
basically get files from your laptop to
your virtual machine so that's so we
will make it brushed and and then we go
under storage so here we can see that
the controller so you have to basically
now you have to basically tell what
image you are going to use to
making this VM so if you are making a
sent to us seriously you go to send to s
website and download the image so I
download a minimal image and there's a
DVD image also so I'll say that okay
please install this send OS server on
this so basically I am now just
specifying the parameters that I need
for this virtual machine and once I do
this now I can basically go here and so
that'll say press ok and then I can
start this virtual machine so when I
start this machine now this is a new
machine it will start installing the
Centaurs because I just gave my ma just
sent to us so you can click on enter and
it will start the installation so you
can see that it is going to just now
start the OS and basically install the
file system and then try to make the
hostname and I'll ask you certain
questions just just continue on those
questions
so to begin testing you can say skip or
just sauce insulation so I'll say skip
and so it will start installation so
says welcome to sent to us it will tell
me what language we're want so I select
English and keyboard selection I say us
so I'll just go by default and is saying
that it's asking me to reinitialize
asian addy initialize everything so I
will just press my arrow key and go to
reach initialize click enter and
timezone is I will stay leave it as it
is press ok
tab to move to ok button and then press
ENTER and then it's asking me password
so I will enter a password for this
machine so make sure you remember this
password click OK and then it's asking
that is going to replace everything or
existing on this machine because it is a
new VM so I can just say yes I'll just
press tab and go to ok button and then
it is saying that it's trying going to
write this changes to disk so I again
say enter on this and now you can see it
is started creating the filesystem
and starts basically installing the OS
so this is how basically you will start
do s installation so this process is
going to take some time so what we did
was I did these steps and now I already
have a basic send OS virtual machine up
alright so now basically we have our
basic VM up and now we will do few
things so what first of all what we need
to do is need to make sure that we have
a host name and we have it configured so
so what again I can do is on this
machine I can do like host name and give
a host name to it for example I can say
lab 1 dot cluster 1 calm and then when I
run host name command it will have that
name but if I go out and come back again
this host name won't be saved so this
host name is just for this particular
session so to save this what we do is we
go to it see sis config and under
network so when you open this file
basically you write your host name here
so I'll say my name as lab 1 dot cluster
one comm so now when this machine
reboots it will still have this host
name so that's the first thing that you
should do to save the persistent host
name and once you do that then it's good
to modify your HC host file and
basically
a you instead of localhost you basically
just say the name that you have for this
host and then we'll also see how to
assign an IP so once
let's first assign the IP to this
machine so on UNIX basically you run if
config - a it tells you whether you have
any IP or not so right now we haven't
configured any interface for this so
right now it's just a loopback which is
one twenty seven dot zero dot zero dot
one so what I will do is I will now
assign an IP for this so what I'll for
that what you have to do is you have to
go to its see sis config then you have
network scripts so you go in the network
scripts all right network's scripts and
then here you'll have like all the
scripts for interface so let's go with
our basic interface which is eth0 and
there we'll say that on boot will make
it yes that whatever IP is assigned to
this machine what we are saying is on
boot just use basically store that IP
and it so that it won't change that IP
so once we do this now we will basically
just do a reboot of this machine so what
I will do is if I cross it will tell me
whether I want to save the machine see
it want to power it off so I will say
power off and then I will just start it
again so it is booting
you
so needs us is asking me the usernames
well say root and at the passport that I
entered for this machine and now when I
do if config - a it has assigned an IP
so you can see then under eg at 0 the I
know I net address is 10.0.0.0 so that's
the IP that has been assigned to the
machine so now we have an IP for this
machine and now we have a hostname so
what we will do is now go under it see
host file and update this IP say
10.0.0.0 and we'll give the name
hostname which is cluster one.com so
that means if some any other node for
example this is a data node and name
node refers to basically this name node
this data node then this data nodes it C
host will basically be able to resolve
it so that's why we need to add the IP
here so you save it so this is one way
plus most in production people do like a
DNS you have a name server and you
basically do the resolution from there
so so that's also something that you
guys can try in your setup so so this
way basically now now we have a machine
where we have a hostname and when we
have a IP assigned to it so now we can
actually go and start the installation
so basically this is my similar machine
and now because I set it up as a bridge
adapter so now you can see the typing in
this shell is really difficult so what I
do is once I have an IP assigned for
example in this case I had 10 dot 0 dot
0 23 so now I can go to my terminal and
what I can say is this Lee open a new
tab and say SSH me basically increase
the font so that everybody can see
so now I can say SSH root at 10.0.0.0
and now I have like my terminal which is
much easier to type and it's a better
interface than this guy so now I can
just do it everything here so so this is
basically the this is how why it by you
will do it because we set a bridge
adapter that means this VM cannot talk
to my host so when in my host machine
from my Mac terminal I open a terminal I
can just give the IP of my VM and just
log in and then it would take me to the
medium so now I have entered to my
entered in my VM so that's how basically
you can go from your host system so so
Mohammed is saying I can do this in bare
metal Linux also yes you can do it so as
you don't have a Windows box yes so like
my Mac so I'm also using Mac right so I
don't have Windows so I'm doing it on
Mac and I'm just getting a Linux so
muhammad' if your question is that you
have a Linux OS already set up and you
just want to use that on directly on a
Linux machine yes you can do it
basically what we have done at this
point is just set up a Linux machine on
our laptops so in Windows we'll use
something like a VMware and do the same
things or our VirtualBox virtual box
comes both for Mac as well as Windows so
that's why we are going with VirtualBox
so so at this point of time we have set
up a Linux machine which is basically
now our machine machine to install
Hadoop so now we will start with the
installation of a tube so now I will go
to my this name node and so this was
basically just to show you how to set it
up and now I'm going to go to a machine
that I have already set up so that we
don't spend time in doing this so so I
will again go to my nn1 see where it is
and
is like really difficult to see anything
here or type anything so that's why your
Linux terminal is basically much easier
to work with so that's why as I said a
bridge adapter so that I can just type
everything easily here so I will again
go to my machine 10.0.0.0 password and
this machine I have named nn1 so same
way that I just showed you how to make
name and have the machines hostname so I
had the host name as this and I have the
IP defined in it C host so right now you
can just look at this one we'll discuss
about these later on so right now I have
one machine where I have set up my IP
and I have configured my file it see sis
config it sees this config Network
scripts might so basically as you can
see that I'm typing something and if I
don't find a file it gives me a list
right so if I'm typing something I press
tab button it will tell me all the files
that start with I so after pressing I I
press it tab and I see all the files
available because I don't remember the
name of the file so then I can look at
the list and I need this first one so I
say IFC FG - ETA zero so that's like
autocomplete feature that now when I
press tab UNIX will article or shell
will automatically complete find the
file with this name and give an
autocomplete it so this is what we did
rights on on boot we said yes that
whenever this machine gets rebooted we
are saying that use basically get the
same ip from DHCP for this machine so
that when we don't have to redo these
steps of changing or ip's are in our it
C host file so so now this IP 10.0.0.0
dot 0.13 is setup for this machine and
i'm i made this the hostname nn1 which i
am saying that no name node and I've
used a domain called cluster one so
that's what my testing so now I have
this machine setup and I have setup the
hostname for it
so sham is don't you give a static IP
10.0 to anything seems to be dhcp yes so
you can you can have a static IP also or
or you can just use this way of doing it
under Network scripts under it's the
config where you say that on board
basically just use this IP so mama I
think this is name load right so right
now this is nothing we are just saying
that we have one machine where we have a
sent to s machine and now we are
basically going to install Hadoop on
this so sentencing from my Windows
machine what do I need to do to SSS from
Windows or Linux host so so basically
yeah so with Mac becomes it comes with
this app called terminal and Windows has
something called putty
it's called PU TTY so you download putty
and then you just type the host name and
you just sssu that host on for 22 so
that would be putty so Mohammed is
saying name n n1 is name one yeah mama
so n n1 is the name I have used for this
machine I am going to make this machine
named node so I called it n n1 yes right
so first thing for Hadoop what you need
is basically have Java so so for Hadoop
you need Java 1.6 and higher so first so
so what you have to do is you have to go
like you install send to us you have to
basically get you have to download Java
so again you look at that and then
basically you go to Java website and you
get the download Java and what I do is
whenever I have to download I basically
go to something like index of Shaba and
Slean desktop i will say same jar 1.7
so it will basically take me to
to find out a place where I can have an
IP of basically a location where I can
just directly download the Java so I can
even say JDK and so basically you
basically get the download so you have
to download Java 1.6 and higher so in my
case I already downloaded it on under my
practice so I got JDK this RPM so what
you do is you can either download it
your host machine and then SCP to this
machine or you can just do something
like a W gate so let me try to show you
how to do a W gift so what I'm trying to
do is find this RPM so you can go to
archives and this is not right so this
index right so what I do is I go here so
this is basically a location where I
have this Java so what I do is I'll say
W get copy this complete path and I copy
the RPM for Java at the end of the path
so basically copied at this end of the
path and copy do a double you get on
this complete path so maybe what we
do this thing on this lab 1 which is a
new machine so what I can do is go under
opt to a W get on this so now since this
was a minimal sent to s so I might my
command might fail with W get not found
so what I will do is I will do young
install
w get so whatever thing you need you can
basically install so you will see that
it will start getting it from start
downloading it so so to download this
package is you need to have W gate so
you can just say yes and W gate will be
installed now you can just type W get on
this complete URL and you will see that
it will start downloading this RPM so
that's one way of basically not going to
your Mac machine and then try to get it
on this machine so you can just run
directly W get on this command line and
it will start copying it so since this
is going to take some time so I already
downloaded this RPM here so next we are
going to do is we are going to try to
install this java so that's the first
step you need to have on the system that
java should be installed so what I will
do is I'll run something like rpm - I V
H so that's the way to install an RPM so
I'll say so you can see that it feels
basically since I already installed it
so it will say it's already installed
otherwise it will install and now if I
run Java it will basically have Java in
the path I mean it won't have in the
path but then you have to basically now
you have Java in your system so you can
basically now say go under user Java and
the version that you installed and you
can go under bin and you can run
commands like JPS this JPS is the
command just tells you about Java
processes so a socio shock is saying do
I need to have Linux OS to do this so
should we try it on send to us right so
send to us is a flavor of Linux which is
so we are using sent over so you can use
same thing that I am using you can use
Ubuntu or you can use Red Hat also so
Ryan says can you pace the downloading
into the chat window yes
so this is the link so let me see if I
can paste it in the chat window so let
me know if everybody saw this chat
download link
but then this could be like any JVM init
so I am going with Java 1.7 so you can
try this and Sunday saying are these
instructions in elements also yes
they're also in LMS so so you can follow
them from there also
great so now we have Java and now what
we are going to do is that we are going
to basically set up our so now we have
to decide on a user that we want to run
Hadoop as so you could be any user it
could be Hadoop user Hadoop say your own
user ID it could be John so we have to
make sure that we have basically the
user added so what we will do is you
have to run a UNIX command called user
add to run so before that buttocks if I
do ID so IDs basically identity so if I
do ID tells me what ID I am Swiss T
right now I am root so what I'm going to
do is I am going to create another user
Hadoop so in this case I already have
this user so so you so it basically did
not do it so you can also do like user
let's try this userdel ID du and try to
create it so now I am trying to create
user Hadoop so now it is saying that it
says create a user Hadoop so when I do
ID Hadoop I will have a UNIX user ID
assigned to this user called 502 so
that's the user which we are going to
use for our Hadoop installation so I
think you may also want to save all
these commands in a file so well this
commands are also available on LMS in
document so that's why I am not saving
them here because you can always follow
them and then you will also have my
recording so you can even go through
these steps so so these these are
already present on LMS as a file so you
can refer it from there so says the
first step so once you have set this
user so now what you can do is you have
to basically set up a password for this
user so you can basically run password
dupe and it will basically ask you for
the password setup so I can say type a
password if it is too simple it will
fail so so obviously you should make
sure you have a good password and now I
will basically go to this user sudo su -
Hadoop so basically what I am saying is
some actually I do not have to do sudo
because I'm already root sudo is to
basically go as routes run command s
rule so I guess what I'm saying is su
- Hadoop su is switch user so I'm
switching user to Hadoop and I'm giving
a - basically - means that please get
the profile of this user Hadoop also so
now basically now I'm as the user Hadoop
if I type ID now I'm user Hadoop so now
first thing that I have to do is I have
to make sure that I have Java in my path
so Ryan is saying should we make lab
wonder cluster comm as our name node yes
we can use that also but you you can see
that it is still downloading Java it's
going to take some time so that's why
I'm trying to I've already downloaded
all these things here so I'm basically
just trying to show the steps because we
want to finish the demo within our class
frame and you want to discuss a lot of
other things also so if you wait for
this that one is going to take some time
so that's why I'm doing the same things
that you would do on a new VM on this
machine we have already just downloaded
the RPMs and basically I just need Java
rpm and I need Hadoop basically a tar
file so once that's done then we are
just going to go with the configuration
so right now we are working on setting
up Java so you have to make sure that
Java is in your path so as you saw that
Java was installed here right so what I
will do is I will go in my bash profile
so I'll do something like dot dash
underscore profile and there I will set
up a Java home which will point to the
directory where I just install Java and
then I will just say path that path will
be bin for that and that at export that
path so that every time when you go
because right now we just install Java
is there but if this path is not added
next time we will come then we type Java
it will fail so basically let's try to
let me try to just remove this from err
right
this looks like this right so when I go
here I go as user I do when I type Java
it is still able to find it from the
path but when I try to say
JPS it would not find it all the
commands that are present under Java bin
directory
they are they will not be available so
the issues saying you're not audible are
people not able to hear me Sanjay Singh
if you will help if you open the
document with all commands from LMS and
have it open side side as you got
executing them all right okay so people
are able to hear so Sanjay okay let's
try to do that so Cindy is saying he
wants me to open the document and tell
us side by side what am i doing so let's
do that
so let's go to edit a cut out in and go
to my courses and I'm going under say
installation using sent to us so I'm
going to download this document quickly
I'm going to open this right Hadoop's
lab setting so this is what we are
basically following so Ryan is saying do
we installer to version one or two Ryan
so yeah so once I finished Java
installation I am going to come to
Hadoop installation so yes so we are
going to start with our do one and then
we are going to discuss how to do in our
further sessions so you can see that
this is the RPN that we just downloaded
right so now we have downloaded the RPM
for this java and now we are basically
setting up the java park so what I will
do is I will go in my bash profile and I
set up basically a Java home parameter
which points to the directory where I
just install Java right and
and then I'll type my path is equal to
include Java home slash bin directory
and the existing path so right now let's
forget about this this one so this is
how we basically now adding we are
adding Java path to it right so now when
we save this file and we type JPS it
will still complain about the same thing
because we have changed something in our
profiles but we haven't sourced it so
what you have to do is dot bash profile
so this is called sourcing of patch
profile so it will get all the changes
that you have done to your per bash
profile in your environment and once I
do this now the Java now my environment
my shell environment knows where Java is
and I type JPS it will work and if I try
Java commands they will also work so
this is something that we have to do on
all the machines now my joshan is the
path that I just sent and it works so
now we have Java setup so next thing is
to basically get Hadoop download so
again this is 1.1 dot o setup so again
I'll go to the Internet and I'll usually
again type index of Hadoop so it will
basically take me to this location where
I have all the Hadoop basically
directories so you can see that we have
this versions here and
all the older raises are available here
so this will basically have everything
here right so we have starting from
Hadoop 0.10 14 it goes to 0.19 2023 and
goes to I do 1.0 so you can see that
naming convention is bit started with
zero point ten so everything under zero
point three basically less than Hadoop
0.23 is high do one and after 0.23 is I
do do basically so we obviously going to
choose the one of the relation so if we
go with y via with our LMS basically you
can see it is talking about 2025 so
we'll go to to 2025 and we are going to
download this tar.gz so again we'll go
the same way we'll do a double you get
on this so let's go to my session so
this Java is here so just so that
everybody wants to see how to do it so
what i will do is i'll type RPM dead -
IV h that will basically install my java
i don't want to waste time on Java
installation because we need to
configure it so that's why I didn't show
it
so because is just an RPM command it
will install java and then again we will
do something like this and then it will
get the Hadoop version so so again I
have already downloaded Hadoop on my
this machine so you can see that this is
under MS route so I will exit out and
again go as route from Hadoop user and I
have already downloaded this few
versions for our test environment so
what we so once you do this W gate and
this is not getting downloaded so I just
have this downloaded version here so now
again we will try to basically install a
dupe so for that what we will do is we
will try to extract this star file so so
this is star Z Z file right this is a
dot and a gz gzip compressed file so we
have to give this option star minus X G
size e V F so X
means extract Z means is a compressed
file V means give verbose output and F
is like I'm giving the file options and
the file that I want to uncompress is
Hadoop 0 20 specially I will give the
complete path I say root I do 0 20.2 a
5.0 tar.gz and then I can give the
directory where I want to basically
unter it so in my case I want to run
everything as Hadoop user so I am going
to copy this under the home directory of
Hadoop user so once I do this basically
now it is now extracting this tar file
under the home directory of Hadoop user
so now this is extracted there so now
again I will go back to my directory and
I'm in my home directory and if I see I
should have this directory here if I do
a less than cell TR let me remove this
link for the time being because we want
to start fresh that is from my previous
class so once you extract you will have
something like Hadoop 0 20.2 0 v dot is
so if we do go into this directory what
we will see is will have all the hadoop
jar files we have contract rhe we have
basically Lib directory so if I go under
Lib I will see libraries the jar files
that Hadoop needs and then n go under
conf I have the con files at her tube
needs so there was a question somebody
that how do jar files basically come so
basically now when you have installed
her to so these libraries are the
libraries which have the Hadoop
libraries and these are the lines that
going to be used so there is a one
library for free scheduler there are
common tools common CLI so all these are
the basically jar or basically which are
library files which are coming as part
of Hadoop installation and now like we
did the Java path we have to do the same
thing for Hadoop also because if I type
Hadoop now it will say come on not fine
because my Hadoop is under home I do
hadoop 0:20 and then
directory and then Hadoop is there so
once I do this my Hadoop command succeed
so what I have to do is I have to type
this Hadoop home also in the path
environment so now as you can see that
this is a version right
Hadoop 0 20 and then when you stall next
version or dupe this will change
so how usually people do is they make a
soft link to the current version that
they are having so what I will do is
will do NS Ln - s which is a soft link
and I will say make a softening of two
Hadoop 0:20 and the name of the link is
I do so now if I do LS - LT r so now I
have a soft link which is pointing to my
Hadoop current installation that I have
in anytime I install a new version of
Hadoop what I will do is I will just
remove this link RM hadoop so when I say
RM / - it's not going to delete this
directly because it is just pointing to
it right so it will is a 18 byte pointer
when I do RMA - you will see that just
this Hadoop link is gone and when I get
the new basically release of a dupe I
will download it and then I just create
the link to that new version of the same
ado because then you don't have to
change your system utilities all the
programs that you have there's you don't
have to winning when you get a new
version of I do we don't have to change
0 22 21 in all the different programs
you will just say home Hadoop a dupe and
that will basically point to the latest
version of Hadoop so now what what I can
do is previously I was for going to
Hadoop I was typing this big command
right so now I can just say home Hadoop
Hadoop bin Hadoop so now it's easier for
me because I don't have to remember that
big version numbering that Hadoop has
used and now again like Java path I will
also add this path in my bash profile
because I don't want
to type this whole thing again I just
want to type a dupe and once this whole
path is in my environment then shell
will know that when I'm typing Hadoop it
needs to look into my profile and my
path variable and if there's any path
where Hadoop is it will automatically
find it from there and I don't have to
type this complete big name so like I
said Java so in this path variable I'll
just add another thing for Hadoop I'll
say that in my path in so just include
all the Hadoop binaries path also in
your path variable so that when I have
to use Hadoop I don't have to type big
name now I'll just type a dupe and it
will work so again it won't work till
the time I don't source this profile so
I have to do a sourcing of this so once
I do this now I type a - it will work I
don't have to type this big name so now
basically we have set up the paths and
Hadoop and Java are now available in
that path so so much here's a question
when your chance can you please show
where is yesterday's recording so the
yesterday's recording should be under
the LMS for your account so basically
somebody you will log in and you go
under my courses and then there will be
LMS right
so in LMS if you go into module 1 you
will have class 1 recording so that's
where you should have your recording as
well as presentation and this is where I
also got this document downloaded right
now so this is where you should have it
all right so now
basically we have our Hadoop set up so
now what we will do is now we will start
configuring Hadoop so we set the path we
have the bash profile and now if I go
back to my deck you can see we have a
something called standalone mode right
in standalone mode what we have is what
we have right now we have JVM running we
have Java running we have the Hadoop
installed and now I do Hadoop
I can basically run Hadoop commands well
say Hadoop FS LS it will just give me
the listing of my current right if it's
a root it will run root so obviously I
am NOT actually running any Hadoop
daemon I have not started name but I
have not started data node I have not
started job tracker I'm just installed
basica tube and this mode is called
basically standalone mode in this in
case we are not running anything you are
just dev basically Jeff installed Hadoop
and you are trying to basically use this
word element so in this case what would
happen is some developer or somebody who
is making some MapReduce program they
will use this kind of environment to
write basically a file and try to really
run Hadoop commands on that so so
usually this won't be used by an
administrator ever because you there's
no basic use of it's mostly for like
developing some programs or basically
trying to test some programs because it
has no file system it can only basically
just run her two commands basically
commands
so so first file that we have to look at
is basically will go under home Hadoop
Hadoop conf
and Hadoop - env dot Sh
sorry not serial do a VI so you can see
that first thing that we have to setup
here is we have to define our Java home
so everything by default is basically
you can see that they are coming these
things are coming commented so what I
will do is I will copy my Java home and
I will uncomment this basically oops and
I will replace this by the path of Java
where I have Java installed and you can
see something like Hadoop heap size
right so this is the heap size it should
be used for any job any hadoop process
that you start so by default it is one
one gigs so I'm going to basically
reduce this because this is this I'm
running a virtualization environment
right so this can be changed to any
value so I will make it 512 for example
and then you see this log directory so
you can change log directory you can
change Hadoop so you have to now define
a Hadoop home parameter so that's
another parameter that we are going to
define so we're going to say export
Hadoop underscore home to the path where
we have installed Hadoop so which is
home Hadoop a dupe so that's the pathway
here we have installed Hadoop so that's
Hadoop home so now this parameter will
be used by all this log directory for
example it will be use by slaves so this
is the path basically where it will go
for finding out all the Hadoop
installation so we have to define this
Hadoop home you and once you do that now
you have this like basic definition of
Hadoop can be set up so we basically did
define a Hadoop home parameter and then
be changed the heap size
and then we specified Java home so this
is the first file that you will change
so I'll save this file can't open the
file for writing let me try to save this
at another location and see why I'm not
able to okay so let's see so another
thing that looks like yeah I forgot one
step so we have to make sure that we
change the permission so right now it is
root right so we are running as Hadoop
so it is not letting me run so once you
install you have to make sure that you
make Hadoop as the owner of everything
in the directory where you downloaded
hadoop so I'm going to make CH own
Hadoop for everything so I made chmod it
should be CH own CH own mean basically
we are changing the owner so I do CH own
and then I'll go back as this file and
now I will try to make these changes so
Java home
so
Java home then heap sighs I'm going to
make it
512mb and then you are going to define a
Hadoop or Hadoop underscore home is
equal to home I do a dupe
so this basically completes our Hadoop
env setup so now we are going to look at
the configuration files which is called
coresight so by default whenever you get
Hadoop configuration they are basically
all empty so you have to basically setup
the configuration so Muhammad is asking
what is the use of heap size in Hadoop
env dot s s so basically heap size is
the basically heap value that every
process that's started on this machine
will you so if I had a name node so name
node will use 512 MB of RAM if I start
data node it will use 512 MB of RAM for
this process and root permissions here
we changed that so our mother's also
suggesting that that was issue so now
what I will do is I will do a basic
configuration so what I will do is under
course ID what you have to do is you
have to specify the location of your
name node so we'll do so in Hadoop you
can see that we have to specify every
property as a property tag which is a
name and value pair so you have to
specify FS dot default dot name which is
the name of the name node and you
specify the URI of the name node you say
HDFS colon slash slash host name colon
the port which is 80 20 so 80 to 20 is
the name node RPC port so that's where
we are defining the name node port so
once you do this now you are you have
specified basically the location of the
name node so next step that you have to
do is you have to look at the coresight
sorry now we have to look at
hdfs site so we did the core site where
we specified the location of name node
and then we go at HDFS site
so in the HDFS site we specify the
location of name node metadata so again
I'll say DFS dot name dot d R is the
name of the property which tells where
the name node metadata would be stored
so I am saying it is under /data /index
directly exists so let's see data is not
there so first of all I will make this
directory again I'll go as root mkdir
data NN 1 and I'll make sure the
ownership of this directories with the
user who is trying to run which is
Hadoop user in our case so now this
territories created so now again go back
to user so now we have defined the three
files right we have defined a doob env
dot s H which is environment file which
needs to be done defined on every node
and then we have defined the core site
and the HDFS site so now at this point
we are ready to basically start a name
node because name node for name that you
just need to specify that metadata
directory and spell the name of the URI
where the name node is present so the
first step that you have to do before
doing a name node starting a name note
is you have to do a name node format
because HDFS is a file system and
whenever you try to use a file system
you have to format that file system
first of all so that's what you will do
is you will do Hadoop name node - format
it will basically make Hadoop name node
metadata directory ready for a dupe
system so we'll press that and we'll ask
that do you want to because we have
specified the directory for metadata for
name node is slash data and N 1 so it
automatically picks it up from this HDFS
side file and it is saying that do you
want to reformat the data here so I'll
say yes and at this point it has
basically basically
formatted this directory so now let's go
to slash data and n1 and Cu we just
created this directly let's see what all
has been created in this directory so we
have two directories here one is image
and one is current so let's go to
current right so we will have these
files 0 sorry so we have FS image that
we talked about so that's the image of
the file system right now and then they
are edits so right now we don't have any
changes to the file system so we have
this basic edits file and then we have
file system time and when we have a
version file so let's look at this
version file so this version file
basically has a namespace ID so
basically what this namespace ID is
going to be this namespace for this
cluster so whenever any new node is
added to this cluster it will have the
same name a namespace ID so so you get
just one namespace ID for the lifetime
of a cluster which is after you do a
formatting so when you do name name node
format that's when this namespace ID
gets assigned and and it tells that the
storage type for this name this host is
named node because we are trying to
configure a name node here so this is
basically the version file and so this
namespace ID is going to be shared by
all the returners if the data node don't
have this namespace ID name node will
not accept that data node as a node for
this cluster so now we have basically
get this we have everything ready to
start the name node so how you start the
name node is you do Hadoop daemon dot SH
start name node so once you do this
load will basically be starting and then
again it needs a permission things so
let's look at that because it's stem
directory is /tmp and for some reason
the data node this is not owned by the I
do
user so what I'm going to do is CH own -
are dupe the dupe /tmp this directory
and then I'm going to start and then you
can run jps which is a Java process now
you can see that the name node has come
up so now your name node is up and if I
do Hadoop FS - LS you will see that now
we don't see everything that you are
seeing it previously because now we have
a file system and your Hadoop file
system is blank right now so if I do
Hadoop FS LS it does not show me
anything
so now name node is up and what we can
do is we can actually also go to the
name node web UI what we can do is
10.0.0.0 in five zero zero seven zero so
that's a name node web UI port five zero
zero eight zero twenty was the RPC port
so this eight zero 20 is the RPC port
which data nodes what going to use to
contour to name node and five zero five
zero zero seven zero is the web use web
server web UI port so now you can see
that now it says it has one file it has
zero blocks because no data has been
written to the cluster and it has node
space right now it says hundred-person
use and there are no data nodes so next
step is to now add a data node right so
now right now we can say that we have no
live nodes you have no dead nodes right
now we just have a basic name node up so
and you can also see the heap size we
define heap sizes 512 MB so it used 494
MB out of it because some memory is also
used by a Java process coming up so so
this is where this is the values curve
bill is coming from hadoop env dot SS
where we defined it so named node
basically used all the values from there
so next would be now to set up a data
node so for setting up data node we have
to basically again look at the HDFS site
and add a parameter so now what we are
discussing is a sorry a pseudo
distributed mode we started name node
and now we are also starting a data node
on the same machine
so what I will do is I'll again go to
HDFS site and now I will add a parameter
which tells the the location of data
direct directory so I'll add this
property
DFS dot data or dir and I will say slash
data / 1 that's the place where I want
to store the data so this is the
property for data and I need to make
sure that this directory exists so I'll
do em k di are on this I'll make sure
that it is owned by Hadoop user slash
data / DN 1 and then I'll go back to my
user and now I try to start the data
node so like I did Hadoop
- Damon dot Sh start so when I start a
data node I say start data node when I
started name node I gave name node when
I type this so again the ownership
issues so we have to make sure that we
have the right ownership so I'll say own
- r / TM I'll do make sure that I make
all the Hadoop things owned by CHR so we
don't have to do this again and again so
once I do this I will start the data
node so now when I do JPS we have a data
load and then I can what I'm going to do
is I can go back to web UI initially
there was no live node live node is 0 I
will refresh this and now we have one
data node and when we have a space also
in the clusters this data node has 4 GB
of space so that 4 GB of space got added
to this cluster so now basically we have
a cluster where we have one data node up
so let me see if we have questions so
Sunday's
can you tell the significance of each of
these files so Cindy I've actually
briefly discussed the significance of
those files let me see so we discussed
file system image edits version and and
then the other directory that we have is
the image directory so there it just has
like the file system image and it's not
needed so so that directory is not
important so all the directories are
important in Hadoop daemons are the ones
that stays current so that's the one
that we are looking to so now I am
saying each time do we need to format
only one so hopefully nine answer that
question you will only format name node
once in the lifetime of the Questor
because if you do a reformatting you
will lose everything on that all the
data will be lost because basically
that's like reformatting an OS on your
Linux machine right when you reformat
everything is lost same is true for name
node so steeple is in question is at
this point everything on the cluster is
running on the same machine that's
correct
math is saying while starting hadoo why
are we getting warning how do they
forget right so that's another thing
because this warning is in Hadoop to
basically this variable is deprecated so
they are just giving us a warning that
Hadoop underscore home is not used so we
can suppress this warning also so I will
show you in next class how to suppress
this so but since it's a warning we
don't really care because halfway done
JPS we can see that actually the name
node ended on it they do come up so we
can ignore these warnings it's just that
this is a deprecated parameter so may be
saying you should not use it there since
supporting it but maybe somewhere future
down the lane they might not support
Hadoop underscore home and then your
process might fail so they are just
warning us so Muhammad's saying could
you type that property in here so that
we could copy the place copy and paste
sure so what I'll do is I'll actually
this should be in the LMS also so let's
go back to the document oh sorry it's in
here right so we did Hadoop and then the
base machine virtual manager so I think
there was the question bye Ashok also so
you can basically see everything here
whatever I'm doing you can see all of it
here it also tells you how to set up a
DNS servers
there's additional steps here also so
the DNS setup and then you are adding
the user then you are doing the RPM - IV
s then all the steps that we just did
you extracting the file then you have CH
own the file then s you do Hadoop and
you start the process you add Java home
to the export as Hadoop home to Hadoop
env then you look at coresight so all
these parameters are there so you don't
you will get everything from Ayers so so
squish it beside this heap size we need
to keep on changing as the filing
creases so heap size you will define as
Hadoop and env dot SSO when name node
starts it's going to use that value so
if you change that value you have to
basically restart name not because name
would cannot dynamically pick up the
value that you change in head okay NV
dot Sh so when name would has started it
already has the heap size off in this
case 512 MB so till the time you don't
increase in Hadoop env and restart name
node
it won't take into effect all right so
obviously everything is here so I will
just go back to my demo because I think
doing it life is more important so now
we have the data node added so that's
how basically now what I can do is this
now basically I have HDFS system up so
now if I have to start or start a task
tracker then I will now look at my third
configuration file which is map red site
so what I will do is I'll go in the
corner rectory and then I look at map
red dash site dot XML so again this
would be blank so what I will do is for
map red I will copy so you have to just
specify the jobtracker name so I will
just say job tracker has the same
machine and if you don't say it will
just take the local so I will type this
here and then you can also start
tasktracker so this is obviously on this
machine we have started everything they
started name node we have started task
trackers so now let's we can also see
starting a job tracker so now I also
jobtracker here and then I can also do
tasktracker so it is starting
tasktracker when I do JPS so now I have
everything on this machine and now this
is like a my cluster which is basically
a pseudo distributed mode cluster where
everything runs on this machine and now
I can basically run Hadoop commands I do
Hadoop FS LS enroute I don't I won't see
anything now temp directory is created
so what I can do is I can try to copy
some file for example I have this test
file so I can do Hadoop FS - put test on
root so what I am saying here is I have
a file on my UNIX filesystem and I want
to copy that to HDFS so source is this
file and I am trying to copy it in root
directory so once I do this it comes
back and then I do Hadoop FS LS - so now
apart from just TMP I will also this
suit can see this test file so thus test
file is now copied and now this file is
now is there so if I go back to this
here so I had zero blocks right so now I
will refresh this now I have two blocks
one basically for this TMP and then for
this test file then I can even do run
other commands I can do cat loop FS so
basically Hadoop FS means Hadoop file
system and then the command so when I do
Hadoop FS - LS I am doing a filesystem
listing when I do Hadoop FS - cat I'm
doing a cat command which is just like a
UNIX command so I will do this it will
just type whatever is in this file this
is test this is how basically you have a
pseudo distribute mode so now then I
have to go to fully distributed mode
what I will do is I will go back to my
VirtualBox and I'll have to repeat these
same steps on different basically watch
motion so I will create each of the
slightly created NN one we created lab
one I created DN one I created DN two so
I created all of these machines
separately and I had followed exactly
Saints
epps I installed Java I installed Hadoop
I set up I basically set up the bash
profile and then I went to that node n
now if I start this I did all this steps
and now I can basically go here and see
that what is the IP on this machine
Holloway 1 let's start the installation
of Hadoop 2.2 101 - first of all what we
need we need a VMware Player for that we
can download it from the Google and just
install in our window system once the
VMware Player is downloaded we have to
download the ubuntu image we once that
is downloaded we have to configure our
ubuntu image into our VMware Player once
it is done we will get a window like I
have in my screen this is called a
terminal on which we can write that
commands how we can get the terminal we
can just go to our - this right term we
can write terminal and we can get the
terminal over here okay this is used to
type the command first of all what we
need over here we need to update the
repositories first for that we have a
command sudo space apt hyphen get update
it asking me to put a password the
password is password PA SS wo RT it
starts working now it is updating the
repositories of my open though it will
depend upon the internet speed it will
not take more than one or two minutes as
you can see that it's done in my machine
now I can proceed further with the other
commands what we have to do we have to
download the open JDK now for that we
have a command sudo space apt space so I
get install space open JDK - 6 - JD I'm
downloading open JDK 6 to download or
not I will give a permission of here why
you start downloading as you can see
that it's 0% done it's 30 T remains 33
minutes remaining
now we utilize our time we can download
the Hadoop jar file as well we can say
at our file
by sigh what we can do for that you can
just right click on it and open a new
terminal a terminal will be open and we
can download the tough allure of Hadoop
2.2 for that we have a command
I have already copied the command so I
will just copy and paste it over in the
terminal but for that we need a we need
to put a command W get what is the
purpose of W get it is used to download
the file directly into our Ubuntu or
Linux operating system from the server
after who have written W get after that
we have put a space now I have given the
path of Shore download to Phi I will
press ENTER I would start downloading
now I have two things downloading side
by side one is JDK 6 second is Hadoop
2.2 let's wait for that till it
downloads because till it downloads we
cannot proceed further with the other
commands so we need to wait for that the
size of our open JD gates near about 170
MB and of Hadoop 2.2 we have a size of
around one point 1 0 4 MB if you want to
download JDK 7 like I have used the
command open JDK - six - JDK instead of
you 6 you can use 7 and the version 7
will be downloaded in your system but I
will recommend you to go with 6 as of
now for the Hadoop purpose
hope to point to ISM you can say a
second generation of Hadoop in which we
are dealing with the latest versions of
Hadoop earlier we are dealing with
Hadoop generation one that is out of one
point two point one the one point two
point zero we have different different
versions for that but now we have
switched to second generation that is
Hadoop 2 so we are dealing with that did
not open through is an open source in
which we can say a vanilla software in
which we have to download each and
everything from our own starting from
the sketch
nothing will be present over here
as you can see that Java is downloaded
88% and Hadoop file is downloaded 66% so
let's wait for the Java to download and
wait for the Hadoop also to download it
I think Java will download first because
it is 98% done 99% hundred percent done
now it will enter the packages of that
automatically we don't have to do
anything we don't have to write any
command for that it will automatically
do it as you can see that it's unpacking
judge Java files and all let's see side
by side what is the status of Hadoop
it's 77 percent done Wow
so we have a remaining 22 percent
remaining speed is also quite good so
it's showing 94 seconds left let's wait
for that till it downloads in my second
terminal it since entering the packages
of Java 6 Java is done its untaught the
packages as well not with the command we
can check it out whether it is installed
successfully or not for that we have a
command Java space hyphen Virgin put
enter it will show the version of Java
download it in your system right now I
have downloaded one point 6.0 underscore
31 so this is the functionality of Java
now let's wait for the hadoop it
downloads so that we can proceed further
with the commands of it it's 18 90% done
it showing 44 percent remaining sorry 44
seconds remaining
93% on any foe only 6% left after that
we will see the real scenario affect any
7% no 1997 okay okay it's done now with
the help of LS command we can check it
out
whether it's downloaded or not we can
see that it is downloaded fully now what
we have to do we have to enter it so for
that we have a command ta our space -
xvf space name of the file name of my
file is hadoop - two point two point
zero dot r dot ZZ let's press enter it
will enter the packages of that all the
files of a hadoop two point two point
zero
it's done now again we will go for the
command LS to check it see it is done
out of two point two point zero a folder
has been created with the name now we
will see the configuration files of it
to configure the path and all so what is
the status of that folder it's I do I do
- two point two point zero ok let's see
what all file it contains it contains
bin folder include Lib exe notice dot
txt s bin EDC Lib and all the files are
not so the configuration files are
present inside a TC directory for that
we use a command CD space a TC soy I
place SDC ok now I haven't I droped to
point to point 0 and EDC directory let's
see what all file it contains now it
contains only one folder with the name
huddle let's go inside that and see what
all it contains I'm in hydric directory
see all the files are present inside
this Adobe NV dot SH kosai dot XML map
it eme dot s search method rights word
XML these are the important file which
we need to configure it and we will get
the outputs after that means we will see
the demons running after it and yeah
this is also important file yawn - side
dot XML which we'll be using to
configure the path okay let me come out
of the older directories by using a
command CD ok now what we have to do we
have to set the path of Hadoop and Java
which we have downloaded so that will be
done inside that dot bash RC file dot
bash RC file is a hidden file which we
cannot see by using a command LS see I
put I have given the command LS I am NOT
able to see that file but this
one more command as well to see that
file LS space - al space be a string it
will display all the hidden files
starting with the B name okay there is
some error in the command LS - al okay
so this is the bosch RC file which we
can which we have to configure the path
okay instead I have not used a dot
before be that side giving me an error
let me use it over now LS space - al
space dot be a straight so I okay so we
have only three files with the name
starting dot B bash RC bash underscore
logout bash underscore history now we
have to edit the file dot bash RC to
edit the file we have a command sudo
space G edit space dot bash RC it will
give a pass it will ask a password
password is password that I put earlier
so same password will work for you a new
file will be created we can say not
created a file will be opened bash RC
file will be opened now we have to set
the path inside down let's go at last I
will show you what all paths we have to
configure in it it will be hydrogenated
path first of all we will be setting
Hadoop related paths
first of all we have a command export
space I do
okay sorry Auto sorry
export a'dope underscore I'm setting the
I do pompe I have set the home park now
now after that I will set the part of
configure directory means the files
which we have seen course ID dot
products ml map it dot XML for that we
need this we need to set the partner I'd
open the score come from in the score
directories for that we have a path I do
- two point two point zero it's the name
of the folder which we have downloaded
after that we have to proceed further to
e.t.c directory and after that we have
to move to hadoop folder inside that all
the directories are present so it will
take the part automatically from there
now we have to set the path for map read
home for that we have a command hadoop
underscore ma p RI t in this Cove home I
do - to point to point to m dot C now
the map at home is set now we have to
set the common home now
instead of writing in the commands you
can also copy paste the command from the
installation guide which you have we can
easily copy paste the command in ubuntu
there's no restrictions in that let me
close this now proceed further with the
commands common home is done now we have
to set the path for SDF s export now
Hadoop under Skol
SDF s and let's call home to home
directory okay SDF s home it said now we
have to set the home of yan yan
underscore home he goes to home
directory and the folder where it's
present its present Hadoop - 2 point 2
point 0 ok so these are the paths we
have set for the Hadoop purposes now we
have to set the path of Java which we
have downloaded for that we have a
command expert Java underscore home
directly which in which it automatically
downloads its user Li be JVM Java - was
6 - I will show you by the tammana also
the position of a hardened JDK 6 you can
verify the file name of the file from
there also so that it should be present
at the same means the name should
matches with the same now let's set the
path of Hadoop bin directory this is the
last part we have to set
in the bash RC fight I do path home
directory home Wow before that we have
to use but : so okay so the other part
of her to be upset this is the Java home
we have set and this is the part of
Hadoop to pine - we have set so these
are the only configurations we have to
do in dot bash RC file after that we can
this click on the Save button and close
this out it will take you to the command
we can type another command in that now
before proceeding further let me show
you the location of Java which we have
downloaded the location I have driven CD
space us our live JVM inside that let me
put LS again so I have given this song
we have JDK 6 &amp;amp; 7 but I've been used
I've downloaded Jadakiss 6 and it's
present over there and I have set the
path for it
come out of the sea directory and let me
clear this out sorry
okay now let's proceed further with the
other command dot bash RC means if you
are setting any path for that we can use
in any I mean suppose we are working
with the other installations as well so
it will work for all dot bash RC now we
have to set the Hadoop path as well
means we have to set the whole Java home
in Hadoop env dot s H as well so that we
can use it for Hadoop also so for that
we have a command CD space I do
we can use Hadoop underscore conf
directory means in which all the
directories are present we can give a
path as well
CD let me take you to the longer path
add up to point two point zero after
that et Cie after that I do okay we are
inside that now we have to edit the file
first of all Hadoop in V dot sh for that
we have a command sudo space G added
hello - e NV dot dot SH okay so the file
has been opened now I have to give the
path of Hadoop inside that as you can
see a Hadoop underscore home as has been
given now we have to comment this or we
can utilize this line only to set the
path let me just comment this and write
a new line to set the Java
sorry Java - six - open JDK - i386 okay
so this is the name of the file and this
is a directory let's click on the save
and close this out home now we have set
the Hadoop in V dot SH let's proceed
further with the other directories that
we have to create now let's work with
the course I dot XML we are already in
Hadoop directly so we can directly give
the command sudo space G added space
Corps - side dot XML a co side file has
been opened now we can give the
configuration inside that so what all
the configuration we have to give first
of all we have to define each and
everything inside the property I hope I
have given the spelling correct P ro PE
@t y height sky okay
in fact that we have some name of it and
after that it contains some values so
name is FS default name FS dot default
dot name after that we have to close
that as well let me just increase the
size of it so that everyone can see it
easily okay now I have to set the value
well you in this we are setting the SDF
s path so for that as BFS localhost
it will automatically take the IP
address of a local host and 9,000 is a
port number on which it will be running
okay and now we have to close the
property okay so it is closed so these
are the configurations we have to do in
course i dot examine now let's click on
the save and close this out after that
we have to edit the file of SDF inside
dot XML for that we have a command sudo
space G a date
SDF s - side dot examine s gfs side has
been open now we have to set the
configuration inside that so let's see
what all configurations we have to do
inside that first of all as I have done
in the course side we have to start with
the property after that it should
contain some name and it should contain
some values so name is DF s dot R
application I am setting their
application effect and inside that by
giving a name now in the value we have
to give the replication factor what
application factor I have to give one
two or three values one I am giving as a
replication factor again so this is done
now let's close the property of this now
we have to set the path of a name one as
well as a data node means SDF is contain
these two things let's set the path for
it so for that we have to again open the
property we have
to give some name name is DFS dot name
node dot name dot directory okay
so inside that we have to give the value
means the location of the file I think I
have rinsed one step I have to create
the directories so let me create let me
do one thing let me just pause this step
over here and open a new terminal and
set the directory side-by-side ok so I
open a new terminal to set the
directories we have to make a directory
we have to use a command mkdir - p and
we have to give the location where it
has to be created okay I am giving the
home directory of inside that a folder
with the name of Hadoop to underscore
data inside that a folder with the name
SPFs and inside that we have a file a
folder with the name name no okay so
what directly has been created the same
way we have to create the directories
for the data node as well I can use this
upper cursor and just name the chain the
name of data node instead of name bro
okay a directory has been created now we
have to use the same path inside the
HDFS side dot XML let's give the value
over here file will be file double colon
okay I've already given also now the
location of it home underscore user hope
to point 2.0
I do too under school let's code data -
s DFS - name Lord now after that we have
to close the way okay so a buck of this
property is done now we have to close
this property as well okay so we have
set the property of and items I named
not now now we have to set the property
of data node so we can just copy these
four lines select copy and paste it out
over here this we can change the name of
data node to sorry name no - dot in the
name we have to change and in the value
we have to change
apart from that each and everything will
be same we can cross check once because
it's we have said lot of configurations
so we can just cross check once before
closing the file properties open name we
have open after that we have given the
name we are given the value there is one
mistake I have done so I will just
rectify it
properties close now again a property
has been opened a name name has been
closed value it has been closed property
has been closed again a property has
been opened okay each and every thing is
fine now we can close it and proceed
further with the other files core side
is done let me just close it out and go
to the same domain we have set the path
of Hadoop e NV dot SH or site dot XML is
done as GFSI dot xml is done now let's
set the path in young site dot eczema
for that we have a command sudo space g
added space
Eon - site dot XML okay
a young site has been opened now inside
that we have to set the properties of a
one known manager you must be thinking
that in the previous two files we have
not received this line so it's it's not
a issue much if you want you can delete
this line as well or just keep it as a
same it will not get any issues to you
okay now first of all we have to set the
path of a node manager again what we
have to do we have to open the property
close this out we have to give some name
yarn dot node ma na GE r dot aux -
services ok a name has been done now we
have to give some values what is the
value of it let's see well you will be
MapReduce underscore shuffle about given
the spelling correctly gets correct okay
our value has been given now we can
close the properties and proceed further
okay we have to set one more property
inside dot of hadoop mapreduce part
which contains which will be linked with
yarn dot note manager only a name has
been opened what we the name Yan sorry
dot node manager dot aux - service sound
dot map
reduce dot shuffle dot class so the
value will be our G dot a patch a dot I
do dot map ride home but shuffle handler
this is the value we have given now we
have to close this value and we have to
close the property no ok so this has
been said we can cross check once
whether each and everything is fine and
not okay let's look like each and
everything is fine as should be capital
over here oh I did charge Apache Hadoop
dot map reduction enter ok it's fine we
can close it out and we can proceed
further now what we have to do we have
to edit the map it's on light dot XML
file this is the last file we have to
set up you have to set now we after that
we will be going for the installation we
will we'll be studying the demons and
check it out so let's open this file
with okay before we open this file
because by default map it's ID dot XML
will be empty so what we have to do we
have to copy the template of map it's I
dot XML dot template to map inside Road
examine let me show you one thing LS so
inside that we have to
so where are these okay okay so one file
is this mapping - side dot XML dot
template okay so we have to copy the
template to map it - side dot XML
because by default map it's I dot XML
will be empty let me show you one thing
sudo space GID
map braid - side dot XML okay as you can
see that by default map right side is
empty so we have to copy the template of
Muppets side dot XML dot template to
this file and let me show you the file
let me close it and open the map it
template dot XML okay so as you can see
that it cuz somewhat like the previous
files that we have open HDFS side dot
XML core side or dive XML and yawns I
dot XML so we have to use basically in
further for starting the demons and all
we have to use map it's I dot XML but by
default it will be empty so it has
created the template file - computer
template into that okay so to copy the
template we have a command CP map right
- one side dot XML dot template to map
right - side dot XML what we'll do it
has copied all the template from map it
five dot XML dot template to map it side
dot excellent now let's open the file of
map it's I dot xml to configure the
properties
okay now let's see the file of Muppets I
don't examine so that we can configure
the path inside that for that you will
use a command sudo space G added space
map paid - site dot examine earlier you
have seen that map read side dot XML was
blind now once we have copied the
configurations and all the templates -
from map it's ID dot XML dot template to
this file
it has copied each and every content to
it now we can set the path and
properties inside that easily so what we
have to set inside dot first of all
again we have to open the property after
that we have to give some name name is
Map Reduce so dot framework dot name
after that we have to give the value
value is yeah once it is done we have to
close a property now okay so this is the
only thing we have to set in map right
side dot XML Map Reduce dot framework
dot name it is the name after that the
value will be yeah save the file and
close it out okay so this has been done
we have set all the configurations that
are necessary for the Hadoop to point to
point 0 now let me just come out of etc'
and hadoop directory
let me do one thing let me just come out
of the whole directory harder to run
that demons and all we have to use
subdirectory Hadoop's means the file I
do - two point two point zero after that
it should be SVN okay so what we have to
do we have to father come on Shadow -
name node Brown we have to format the
name node first it just come and not
Fung
so let me check okay before starting the
demons and all we need to set the
configuration of a SSH key means so that
you should not ask the password while
starting the demons for that we need to
come out of Hadoop directory to send SSH
key we have a command SSH space not
space - key gen hi space hyphen T space
RS a space hyphen P we should be in
uppercase space double quotes they
should not be a space between the double
quotes this step should be followed very
clearly let me just repeat it once again
SSH - key gen after that day the space
after that - T space RS a space - P
space double quotes let's say it will
generate a key it will ask enter file in
which we have to save it so by default
the location is Hadoop underscore Hadoop
user SSH so let it be the same let's we
have to just press ENTER and it has been
created now we have to move that file to
authentication key so for that we have a
command
card space location of a home directory
now we have to move the ssh dot SSH ID
and the skull RS a dot pub to home after
that authorized key authorized under
skull keys okay so it has been copied
now now what we have to do let's
cross-check once all the files that we
have configured co site s DF s yarn and
map rate so for that we have command
sudo sorry CD space are the two point to
point - 0 after that ATC after that I do
so let's start with the course I dot XML
know we have configured Hadoop e NV dot
SH first so let's go with that first I
dope - II and V dot SH what we have done
in this we have just set the home path
of a Java so it has been correct now we
can close this out let's check for code
site not examine generally we need to
cross check that I mean we have closed
all the demons or not okay we have found
one mistake over here that instead of
value we have given name so let's
rectify it property name and all okay
rushed each and everything is fine save
it and close it after
let's open HDFS oh I was okay property
name value fine fine okay it's quick so
we can just close it
save and close it out after that we have
to use young okay
we found one mistake over here as well
we have not closed the property by
putting the backslash let's say close it
now the last file we have to cross-check
is map red dot method side got eczema
property name value property okay it's
fine so we have cross-checked also each
and everything now we can proceed
further with the formatting of a name
node and then at the last we will start
the demons of it so for that this come
out of the Hadoop directory directly
give the command I do name load - format
ok choose come on not form so let me
check what either we have to move some
directory to it to make it run so this
lets me let me cross check once ok
to format the name node we have to move
to Hadoop folder that is Hadoop to point
to point 0 so I it should be CD ok after
that we have to move to bin folder
inside that we have to format the name
node for that we have a command add a
space
space name no space - format okay so
it's formatting now okay it's done okay
to start the demons we have to move to s
been directly demons are present inside
that so let me come out of this and move
to s been okay so we are in has been
directly now so let's start the demon of
data node first for that we have a
command adult space
I find demon dot SH start Titan all okay
so our data node is starting okay it's
we can give the command JPS to check
whether it's turning or not okay it's
running fine now we will proceed further
to start the other demon now let's start
name node for that we have a command
idle - demon dot SH space start name no
ok so our name mode is starting now okay
it started let's go to the JPS to check
whether it's running
okay so dad I notice started earlier now
we have study name node so both are
running fine now let's start resource
manager as resource manager is a demon
of a yarn so for that we have a command
key on space demon dot SH start
resource manager let's go check the
spelling first ok I missed hard over
here ok let's start again starting
resource manager let's see okay so this
was manager I started that I notice
studied Nemo death started now let's
start node manager now for that we have
a command Yan - demon dot SH space start
node manager okay so let's press ENTER
and check it out
after that only one d1 is left that is
history server to start before going
positing further let's go JPS and check
the status of it okay so for all the
four demons are running fine let's go
with history server now to start with we
have a command mr - job history
I find demon dot SH space start history
server ok history server is also
starting let's go with JPS and check it
out but so the source manager I started
dead I know they started name or that
started non manager started job is to
serve I started ok these are the five
demons that we have to start in I do two
point two point zero so it means that
our cluster setup is ready and we can
proceed further wither and so on path we
can check one more thing to the browser
whether the health is fine or not so for
that we have a you let's go to Mozilla
Firefox first
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>