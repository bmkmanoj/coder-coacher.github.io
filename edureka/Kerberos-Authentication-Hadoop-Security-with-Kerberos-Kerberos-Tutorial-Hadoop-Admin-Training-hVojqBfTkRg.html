<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kerberos Authentication - Hadoop Security with Kerberos | Kerberos Tutorial | Hadoop Admin Training | Coder Coacher - Coaching Coders</title><meta content="Kerberos Authentication - Hadoop Security with Kerberos | Kerberos Tutorial | Hadoop Admin Training - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kerberos Authentication - Hadoop Security with Kerberos | Kerberos Tutorial | Hadoop Admin Training</b></h2><h5 class="post__date">2015-06-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hVojqBfTkRg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey guys good morning and good evening
guys this is Kumar here will be the
instructor for today's session
okay so Lee Joo is saying I'm new to
this so okay that's good to know
that's fine no problem so guys so can
you confirm if you can see my screen now
my oh the one with the elephant logo and
an instructor okay perfect
all right so if I know this module the
primary objectives would be of what we
would try to explain to you as as a
quick cluster introduction then we see
our what is the recommended
configuration for a cluster and we'll
see what are the different modes of
cluster running right your huddled
cluster nodes then we'll look introduce
you to the concept of Kerberos security
infante Kerberos and also primarily talk
about the hadoop administrator roles and
responsibilities is currently in the
market and finally I will do is most
like a small demo so this demo is
basically I mean they're multiple steps
involved but I think these steps are not
something which we cannot squeeze enough
of maybe in one hour so so some of the
steps which we already configured and
some of the basic steps where we I will
show you for an automated way and also
it I mean the steps will be for multiple
ways and it requires both them but
manual installation as well as an
automated installation so primarily the
automated steps is what I will be
showing you right now and we would be
liberation the the leading vendors
product Hortonworks HTP okay so they
have a very tool and using that embody
tool is what we'll be setting up to the
rest of the demo now any questions you
have up guys please I mean you can even
type into the chat window or raise your
hand okay any time you know the
questions since some of the since I
think some of the people are new to this
of Queen some of the people or new let's
circuit we go to meets like to quick
huddle conference up so
for the guys with experience I would say
it is a quick recap of what you guys or
I mean what you guys have been learning
till now or what you guys have been
doing but for the guys with no
experience with no experience or more
exposure maybe you maybe just to give
you a picture
oh you are kicked up quick inside view
of how they do wake up our head wake up
does the does the sessions and does
online sessions and how it wake us
training delivery mode will be okay so
this is good opportunity to learn all
right
Raja's Austin can you get the PPT so you
need to reach over the support the
support should be able to provide you
the DVDs okay so these DVDs or I think
should be available of only a limited
set of limited set of four slides will
be available I think once you enroll for
the course is what it can get a full
fledged content now now coming to a
quickly coming to Hadoop code components
the reason we see here is a herd of two
dot X is I mean I mean there are
multiple versions of Hadoop so out of
one and hundred to the current version
is Karuk tube which has got far more
advanced features than your Hadoop one
now Hadoop one is historical now I think
maybe outside people are moving away
from r1 to r2 and and how do one has
some of the limitations with earth one
like your name node like a single name
node there is no H a and and I mean that
has long to Lachman a name node has
limitations on how hard one where
whereas the third of - we don't have any
such limitation so you can have multiple
name nodes available no cluster now
that's one thing that's called
Federation the second one is a H high
availability where without one you talk
something called a second renamed node
file with two you talk about something
called ism or standby anymore so this is
a little bit advanced board and talking
right now
so now if I step back a little bit oh
excuse me guys
so if I step back a little bit and talk
about what are the core components of
kuru your Hadoop is made up of two calm
one is your storage layer and the second
one is your processing part right so
anytime you talk about Hadoop what it is
sort of is your storage your HDFS which
we called as a Hadoop distributed file
system and the processing part the
processing part is nothing but your
MapReduce which is version one get
MapReduce is version one we call a
MapReduce version one and the higher
version of MapReduce with version two is
something called yawn okay now what is
yawn yawn is a processing frame
framework introduced with ro 2 and this
is your drop this is nothing but yet
another resource negotiator so one has a
question coming in the Scarecrow's goes
with Windows or UNIX er so yeah so
Kerberos is a concept which is nothing
which is nothing to do with our route
guys okay it has it is not a packaged or
it is not something which comes with
Haru care process it is a completely a
separate concept which can be installed
okay which is used to secure your
systems secure I mean lock down your
operating system or your file system so
that there is no misuse of any of your
so I mean any of your Rob or files or
directories now now this is this is
nothing to do with Hadoop when I say
nothing to do with the Hadoop it is not
something which has inbuilt into Hadoop
it is a separate concept and it is
primarily an operating system concept
okay when I say an obvious concept it is
it can be installed on a windows it can
be installed in Linux machine on any
flavor okay so this is completely
separate so it is actually an operating
system concept it is not a whole concept
so so there are some advantages which I
will show you of during the session or
the next term next 30 to 40 minutes when
we talk about there are some primary
advantages like why or why people have
started a including Kerberos with your
car oh okay hope that answers your
question enough now now coming back to
the heart of core components primarily
there are two components your HDFS part
and there is a crossing part
now HDFS there is no from there's no
major change okay in your storage or
HDFS okay let's step back a little bit
further more and say you're too
confident the storage components on the
processing components processing is
nothing but your MapReduce part right so
you might be hearing about MapReduce so
what is MapReduce that is a crossing
that is the processing framework now now
on the stories you have HDFS HDFS is
nothing but your Hadoop distributed file
system and on your processing side you
see here yarn
Yanni's can be considered as a next
version of MapReduce which is we
typically coded as a question 2 of
MapReduce right now with yarn I mean
there's a lot of advantages also their
main which is what you'll learn in depth
when you I mean that is completely out
of the scope or just trying to give you
a quick heads up or heads on on that so
yawn yawn what yawn does is it
completely segregates your cluster
resource management right your resource
management wear it up on you talk about
job tracker which is a know which is
lower burden now you see utility on your
job there is no job tracker this I mean
this concept is replaced there's
something called resource manager and
and and the job of a job tracker has
been delegated to multiple of nodes here
ok that is something which sure you guys
would recommend that is primarily
towards your Hadoop admin or admin
topics is what you can talk about or you
can discuss more in there now primarily
importantly we seen the core components
your storage path which is your HDFS and
your processing power which is your yarn
and admitting days you see something
here the master and slave so what is the
master so every every process or every
each component will have a master
process and a slave process okay now now
slave process and a master process
typically is what we call it as a given
what is the demon price you should be
knowing that we even demons or any
process which is so running is called a
demon right any background process or
any process which is up and running so
anytime on a cluster on your horrid
cluster you
all these nodes or all these processes
up and running okay so moving on moving
on a typical hope cluster this is how
your heart crystal would look like
typically of your name no right look at
this name node your name node and your
standby node should be of identical
configuration so what we're seeing right
now is there a hardware and software
configuration primarily this is targeted
towards so what hardware you need to
choose and what OS you can choose okay
now now if you look at here if you look
at here the memory is very very
important on the name node okay so you
you might be seeing the name node and
the standby node or of equal
configuration right so you have a hard
disk is of not importance here but the
memory is very very important and and
operating system is any 64-bit version
can be supported hi Ankush we just got
started a couple of maybe ten five
minutes back so welcome to this is
continue receiving to listening so this
is being recorded guys I think the
recording will be shared with your
people and any questions you have you
can always put in put it put in there
alright oops now now you also have a
secondary name node now I mean secondly
name node is not required when you have
a standby node you will not be requiring
a secondary name node all right so this
is something which your typical how to
cluster would look like so people who
have already got experience they should
be more familiar with this so this is
actually a hundred to two that to
architect huddle to architecture
representation
now now now if you look at that if you
look at the slave power so I the slave
parts are worn the slave parts of your
data nodes right the data nodes will
what is important at the taken note
Caesar is a disk in the hard disk is
what is important Ram is of not that
importance here because we are talking
about your distributive systems here so
so on your on your name node
your RAM is important on the
datanodes Ram is of not that importance
ok hard disk is of importance then you
have you have a Ethernet interfaces you
need to have multiple interfaces and
then you take care center is any 64-bit
operating system extent OS is just an
example we are giving here but typically
you can also have work now you can also
go with top o with Linux with Red Hat
Linux RL or suzay or oil or any of the
flavors available ok so so these are all
open source available and you only need
to you don't need to purchase your
license for it only cuz the support is
what you be requiring up now requiring
to fail therefore the mean business
support requirements Kunal is asking
worst essence of three hard is for data
nodes 2 2 2 2 2 2 3 hard disk for the
data nodes so I think they own the data
nodes nothing we are talking about 6
into 2 terabytes right so that is again
our deeper concept guys why we go with a
hard disk of multiple disks right see
see what we talk about is something
called the commodity Hardware is right
so these are your commodity Hardware
where where which are cheaper hardware
but not cheap okay
these are mean data nodes are cheaper
hardware but not cheap but your after
your name node should be your high-end
plus and PI plus service so so what are
the essence of three hard disk or data
nodes and why multiple me concentrate on
roads and not named nodes right so if
multiple link cards are here right so
you have cleaning cards here here also
3d cards so name nodes we need to have
3d cards disc is of not I mean I mean
see primarily entirely everything is
being written on to your hard disk
all right your hardest competitor nodes
where your data is being recited right
your data is residing in this location
but here on the name nor your metadata
right whatever metadata you see the
metadata is being written into ran right
it yeah obviously it will be written
into the disk as well as the RAM but so
the RAM is very important so that the
may when the data nodes want to write a
file or read a file from the cluster
they will typically contact the name
node and the name node will in turn
query the metadata from within
and pull it as soon as possible okay so
the essence here is to to to get the to
get the metadata as soon as possible and
that is why everything will be loaded
into the RAM now on the on the Cajon
also you have 16 GB RAM because so the
processing is happening here the map
read processing is happening here
this is distributed is parallel
processing ok multiple data nodes
participate in doing your MapReduce
process so I mean so so you can go with
a lesser lesser configuration on your
Pam okay so that is again a deeper
concept guys so if you were into Hadoop
administration that is something would
show when you need to go deeper in there
so which is not which is not working of
touching right now and the question is
coming in is odd name node since a bit
cluster no no up when you see when I say
I think so so you can I talk about see
these are all it this is entirely is one
cluster okay so in this case I have 1 2
3 4 5 6 7 8 9
ok so this is my 9 node cluster ok this
is a 9 node cluster right now and this
is secondly node name node is optional
optional in the sense if you don't have
a standby name node you will definitely
need a secondary name node now if you
have a standby name node you will you
will be eliminating your second inning
no you don't need a secondary name node
right so so this is this is something
what what I mean second a name node does
a lot of things that people call it as a
checkpointing node also okay so you can
do a google search of what checkpoint
Noor is so this functionality is
replaced by the standby node so the
spies mentioned as optional here but in
fact if you have a if you have a standby
node there is no requirement for the
second inning though so this is your
entire cluster okay this is an entire
cluster is what we are talking about
right now so the name nodes will be
internal to the cluster the name nodes
cannot be external to the cluster
what is the what is redundant power
supply a random power supply is if you
have ever done a datacenter walkthrough
I know some of you are from fresh out of
college but people who have done it I
mean when you see a data center why if
you see if you I mean if you can
you can not do a Google search and go on
search for a data center and you'll see
you will see multiple servers stacked
into racks okay service was packed into
racks and each server will have two or
three power lines okay power lines
nuisance power supplies so given power
supply is what your power supply is if
one power supply goes down the second
power supply can pick it up I mean in
the sense your your machine is still up
because you have a two power supplies so
one can be only UPS the first one can be
on something else right I mean the first
one can be on your PC then the second
one can be on your work ups so if the
first one goes down you you still have
your server up and running because of
your power supplies okay oh I see some
raised hands coming in now shoe-bomber
please go ahead any question from you I
see you raised hand so guys so Jerry
shutting I answered a question now
hopefully now I have another question
coming in now is there is there new
edition of standby named Noreen her to
some need for see some deeper see our
stand when a note is yes that is with
Horeb to write so see so it's a standby
Nimrod easy is part of a concept called
H a so Hadoop HDFS a chip primarily HDFS
HH h TF sha is what is not available
with up with your Hadoop one HT of the
sketch a is what is available with your
out of - okay so that is so so for H it
to work you are not requiring a
secondary name node you'll be needing
something called a standby node so that
is what it is referring here so standby
language yes it has been interval
interviews with ro - that's right
now now moving on now something when
something interesting that you typically
would be saying is I mean a quick a
quick representation indicating how you
can LA you can you can you can
approximately figure out how much data
is coming at Y so
of the cluster growth on storage
capacity is often a good method to use
right when you say if you have a deck up
if you have a cluster of say oh now say
if you have a cluster of five terabytes
right so how much sure how much I make
not find out about you say oh you're de
cow going into the cluster is one
terabyte so how much space is required
guys how much space is required on my
cluster to store one terabyte data can
you quickly answer this people who have
experienced or people who have only
worked on this website yeah so you
typically would be requiring 3t or
terabytes right kunal not 30% but yeah
so so typically say there is a something
called a default replication that if
their application factor is something
which of sois you guys this keeps going
out soluble des see the oops see
whatever data you upload into the
cluster should be replicated three times
right so will be by default replicated
three times so that is one of the
important features of your product
cluster right so any guitar you upload
to the cluster is replicated twice so
you have three times for the same data
replicated now now now there is also
have something called a block size might
so blocks a is something which you can
learn later but three terabytes is if
I'm if I am uploading one terabyte you
to my sister then obviously I would
require three terabytes of the cluster
space now as an approximation if we have
five terabytes per week okay HDFS try to
replicate each block three times now we
are requiring 15 terabytes of disk space
per week okay and as your 30 percent
overhead and in total I would require I
would require 5 into 3 terabyte hard
drives okay so that means that means
this is coming back to a new server
every week right
a new server every week is what I would
be building it so that is something
which has a Hadoop administrator you
typically be needing to understand at
some point of time off you need to you
need to keep forecasting okay so if you
are primarily targeting towards
administration as an Hardware
illustrator you need to continuously see
what is the cluster capacity in what is
a usage and how much data is coming into
my cluster on a week on week basis or on
a day on day to day basis so the cool
thing is you don't want to end up in a
situation where where your plastic
capacity is full okay your custom
capacity has reached 98% and you don't
have a way to expand it so so you need
to forecast it and make sure you you
keep on increasing the cluster as for
the requirements now that's what I think
we talked about slave nodes okay so here
is what we're talking about the
recommendations on what is what is
required for your slave node stepping
the put it configuration a Kunal's
question why not use right it's
mentioned in the right so I launched the
islands good question what was the max
replication factor we can do the max
replication fact you can do is quite
well so 512 replications is what it in
cake okay so actually now high
performance versus low performance
components so the recommended
configuration for slave nodes so the
general configuration and you can you
get a special configuration right so
general dependent on depends on
requirement based consolation for the
slave node okay so 4 into 1 terabyte or
2 terabytes in a jaybird constellation
now what is about J both J board is a
just a bunch of disks okay just a bunch
of this do not use right okay to
quad-core CPUs and RAM is worth maybe 24
and Gigabit Ethernet
now guys with experience can you she'll
say why it's not no rate is quite can
you stable the chat window why why it's
say not wait for slave nodes exactly so
so see what will happening you say on
the pink unknown okay so now what is
right wave is an AB redundant array of
independent disks point if you look at
radar picture how will it happen is rate
yes go hit the fight where is is the
block size okay 128 is our 64 mph
default and 128 and fight filter blocks
the question was one of the maximum
applications you can set okay that is
something which I mean obviously will
not be going that forward but what vital
is something which you can say but by
default three X's are default
replication and people want to move
above the 3 X replication okay so it's
of no use if you go beyond that because
you are unnecessarily wasting space on
the disk now going back to the question
on a JBoss so J bar is nothing but a
just a bunch of disks okay that is that
is come that is completely against your
rate okay radius what we deserve is a
set of disks okay so if you if you want
to visualize how a rate looks if you
look at a CD track right CD track tray
where maybe you can say CD straight
right in a CD tray you you stack
multiple exit 50 CDs on top of one
another or say 10 CDs now what will
happen is when the right is happening on
to the disk right in the spindle right
when the spindle argument when you want
to write something on to the disk in a
raid architecture and mirror image will
be copied okay so a mirror image will be
copied so whatever you write onto the
base there will be a mirror copy
available so in case one of the disk
fails even can always recover the data
from the mirror copy
okay now you should be getting why we
don't need a right now and there is a by
default on a cluster we have three X
replication now you have 3 X replication
one terabyte of data you're occupying
three times if you have raid also what
will happen three more times right so
whatever data you're writing it will be
multiplied so so because the replication
is only there you you should be avoid
using the using your rate here and
that's why I say
j-bot you what so nothing but you're
just a bunch of this so just one of
interview questions guys ok so typically
people will ask about this and also
stratification questions it might come
up okay so Jake watts and right and this
very very important okay now so this is
where you're saying do not use weight
and if you look at a special
configuration multiples of hard drives
right so you always go for a multiple
hard disk rather than a single hardest
okay the reason being of
Oh reason being girl there isn't being
ma sorry so I guess the reason being go
see if you have this everything being
written into a single hard disk go into
a single hardest what will happen is if
that risk crashes okay that this crashes
everything is gone when you have
multiple discs you can keep on
recovering I mean one disk you can give
it you can safely have one or two discs
going down another question that we talk
indicating is to teach children is
Djebbour a new name no no no Vicki J
board is completely different not a new
name okay maybe because redundancies
already taken care that's right Kunal
explains the storage so J board is
completely opposite to your rate okay
rate is expensive J boards are cheaper
rate rate is you have multiple copies of
the same detail the rate version if
you're talking can be anything rate one
to ten okay so the rate zero to nine I
think like so any of those right so yeah
Rohit we use rate for only name note not
for not for data notes okay so the the
one we should talk about here is a slave
notes right so only for the slave nodes
we use right and not for the not for the
data nodes okay that's right with some
it is say you can do better read white
with multiple risk so again one more
thing is multiple disk is not for
redundancy but for performance so a
multiple disks say multiple disks is not
for relevancy but foot performance okay
so um it's the amicus say you can do
better weight white with multiple disks
on the data nodes the other side so we
see multiple disks happens okay see
reasonably assigned multiple disks is
okay if you have a single disk okay if
you have a single disk
I mean if so if the data
you
for you guys so if you have a single
disc and if that disc is crash your
entire data on that disc on that server
is lost if you have multiple discs the
data is split across multiple discs even
if one disc is down okay one disc you
splash you can simply you can safely
take out the disc and replace it right
and then get the mean I mean format the
disk and put it back and then load the
data but again there is also the concept
of 3x replication right I think one of
the question that came is why only three
X so that is again one more important
things why only three X sub because the
3 X replication is there's a reason why
we have three X replication in the first
beam yure the first application we'll go
into as the window to the to the data
node which is closest to the client okay
the first replication the first blocks
objective is to make sure the data hits
the cluster the moment as soon as
possible this so there is no leader
Talos the second one is for redundancy
so that it copies into a second rack
okay on it on a node on a different node
in a second rack and the third one will
be on the same rack okay in case of a
rack failure okay in case of data or
multiple data nodes failure okay so
first one is on closest to the client
the second one is to make sure there is
redundancy if one bracket goes down the
block is copied into a different node
and a different rack and the third block
is copied onto a third node on the same
rack on the second where the second
block is available okay so the fourth I
mean you can also have a for replication
for X so if you have more than three X
replication the fourth and the
succeeding your the subsequent blocks
can be written by default okay they can
go into any default locations which the
name node will orbit truly decide oh
there's a couple of more questions
coming in how retry it happens in j-pod
CJ boy is nothing but a discs okay
multiple disks so readwrite is something
which is a mean it drives only one time
one to the J about this okay hello
Crossing is enabled
thanks Tommy I think Sameer and wiki
saying that raid is used for named nodes
you yes Vicky raid is used for name
notice right
data is never replicated between discs
in the same node yes obviously data will
never be repeated a block ok block is
what we are talking about block will
never be replicated on the same note
block will be replicable see the third
replication what I said will be on a
different data node on a separate rack
sorry ok so so one block will be on one
data node on a one rack and two
subsequent blocks look a for how creates
replication the few subsequent blocks
will go into a second rack okay or okay
or a third rack but both the blocks will
be on the same rack so at any point of
time your three blocks will be on two
racks okay two racks on different on
three different nodes the data is now
replicated between disks in the same
node that's right yes summit that's
right how it finds out the distance
clothes off for see the the distance
between you see if you have a see if you
have a if you have a client machine on
the same rack right see the
communication the communication between
the nodes within the rack is it is very
faster okay because it doesn't require
to come out of the rack say oh so on the
top of the rack there is something on
the top of the rack switch right through
the switch is what they communicate and
and that goes on the between the
switches that is intercommunication but
in trevean beak we can inside the switch
okay written inside that you the client
is inside the switch the communication
will be faster oh okay so i'm it is
asking so failure of a disk even if you
have a failure of the desk even if we
have three disk will mark to take a note
then failure of a disk even if you have
three disks see if the if the entire if
the entire all the disks are down on it
on a node yes that is gone a row it is
asking how each finds a distance which
one is closer the same rat why should we
have the client see it doesn't require
to be on the same machine okay so you
may have it or you don't have it okay so
so the client can be on the same rack or
it can be on a different track but the
objective is when the client says I want
to upload a file
when the client says I want to upload a
file the name node what the name node
does is the name notes objective here is
to make sure give the closest data node
or the detainer which is closest to the
client location
it looks into VLAN okay look at the VLAN
and see which is in the same VLAN it was
in the same VLAN they're asking to write
in the same on the node which is closest
to it you get based on its IPO or VLAN
configuration is what it will look or
look and provide the details to the
signal sorry to the client of the data
node details to to to to to to to to
help things or distance matrix air is
used to find the distance one two four
eight okay so I'm it is saying the
distance matrix array is used to find
the distance one two four eight here
kinds of it okay so this about top of
Slater configuration now going on slave
nodes on the more details we see of
slave nodes ram ram is tough for serum
is safe ram is required on this round
the slave node because the processing is
happening on the slave node right the
slave node is what the processing part
that we talked about is the task vector
the task tracker is what runs on the
slave node right so so so what is what
is the processing part the processing
components are your map and reduced
components okay so you take about 1 gb
or 2 gb of ram and slave should not be
using virtual memory right so enough
ensure enough ram is present to run the
pass place the data node tasks record
demons place the OS okay so so on your
on your machine on your data node what
what are the demons running is you see
you will have an operating system
machine you have an opening OS running
that would require some ram then you
have a data node female running that
would require ram then you have a task
record demon that would require ram okay
and make sure make sure the thumb rule
is the total number of tasks either a
map task or reduce task is 1.5 into
number of process of course okay so if
the dual-core
so how many you will have this is a
dual-core processor how many tasks you
get
that's right guys yeah we'll just I mean
we almost done 30 minutes oh yeah so
we'll definitely go into the today's
topic so before I go there so I just
want to bring up some I mean just want
to talk about name node and although I
mean all the recommended configurations
right so I think we are almost 30
minutes into the session so let's
actually jump into our actual core core
Tinga Calabro spot some of the important
things some of the important things or
unity know is the cluster multiple modes
of cluster installations right so one of
them is standalone mode the next one is
sort of more the third one is a fully
distributed mode so you guys will all be
or you might already be familiar with
all these modes to suit I mean stand
alone is something which show how it
doesn't have an HDFS why it doesn't have
a distributed file system and pseudo
mode is where you have all the demons up
and running on a single machine right so
the quarter what are the demons now now
my dear our demons are named no data
node then your job tracker in the task
tracker all the demons will be running
on a single machine and a fully
distributed more is where for today
we'll see that where we have all the
demons running on multiple machines so
typically in a typical production
cluster what you would have is in a
collection cluster what you would have
is so what you have is your name your
master daemons your name node and your
job that it will be running on one
machine or and your slave components
right you're getting out in a stretcher
or taker or your truck data node and
node manager would be running on
different machines so so let's get the
slave components or slave is something
which we call as a commodity hardware
and the community hardware so we'll have
also there the cheaper hardware than the
expensive expensive service and what
important thing is only for the money
for the data node so only for the slave
nodes is what we would use we'll be
using your a community hardware but
primarily for your rock for your name
note you need you still need to go for
an expensive high-end server
quickly some of the consolation files
through the confirmation files your
environmental files your course site
your HT f SE on site map red slaves so
these are the these are the config files
which are familiar I mean which are
primarily important with your family
important with Hadoop to get out of
Washington so with Hadoop on you will
not have some of these files you have
something in here called the master file
will be available right so this is how
the compilation files would look look
like all right now to to GT now let's
look at the security concepts right so
this is what we are talking about today
now why we are talking about security
and why your we need to look at a
solution or a tandem and not rent
mechanism or how you secure your cluster
right so typically when people say
secure a cluster the first thing that
will be done is implemented is your
carry Bruce okay now Kerberos is
something which versus something which
is not which it does not come with a
huddle okay it is completely separate
component of huddle and it is actually
an OS concept okay
so it's an OS concept which which
typically anyone can enable it so that
they can secure their operating systems
okay either Linux or Windows anywhere or
Witcher it is now the reason why care
approach has been chosen the reason why
Kerberos has been chosen is because
softer or because of the because of of
the size of the cluster okay so how do
we are talking about thousands hundreds
and thousands of nodes of the cluster so
so so typically this the current
security systems or the current security
packages of software's will not be able
to scale up to the level where your
hadoop scales right so that's why you
have chosen Kerberos and Kairos is
nothing but an open source so it's a
it's an open up it's an open source
available fully available from MIT okay
so the MIT Institute guys came up with
this cameras concept and they
it is enabling security on the voice
there now what does it do it does
service level authorization and the
proxy cab build is in yarn okay and this
is what you're talking about most
security tools failed to scare and
perform with meteor and walnuts right
now why the security risks what why
we're talking about sake I mean what
security risk we're talking about
insufficient authentication okay do not
authenticate users services now the
client says I want to run a MapReduce
program to the resource manager the
resource management blindly goes and
goes around ran store runs the program
so it can be a find it I mean the
resource manager will ask I mean in fact
your scale applications manager and your
scheduler will not even look work with
coming in it's simply goes ahead and
runs the program on the node managers
right so it doesn't matter what who is
asking and who is coming in so no
privacy and no integrity insecure
Network transport and no message level
security right so arbitrary code
execution so they will not be any user
verification for MapReduce code and
malicious users can submit a simple job
right so if you mean somebody who wants
to who wants to play around and see what
is available on a different users
territory you can simply log in and
still run this MapReduce program now now
we have Kerberos to the rescue of
Network authentication protocol and
developer at MIT or available as open
source and something we already talked
about so what it is doing is the
interaction between the host and the
client should be encrypted okay and it
should be easy for users to use and
protect against intercepted credentials
right so with Kerberos or typically when
you say when you send your user ID and
password right so what it is saying is
nobody should be intercepting this I
nobody should be on the network hacking
on to the network and trying to read
your username and the password
so we care across you're not sending
your password you'll be sending it token
okay you'll be sending an encrypted
token which is what will be decrypted
and the other end and and yeah this
worked is talking about so it is based
on a secret key distribution model okay
keys are the basis of authentication in
Kerberos and typically a
the short sequence of bytes we can use
to both encrypt and decrypt now when you
look at encryption keep plain text with
encryption text okay you have a cipher
and you need when you want to decrypt
this you have a cipher text description
key which is nothing but your plain text
right so here you are encrypting it and
sending a cipher text on the network
rather than sending your password okay
the same way at the decryption when you
are decrypting it you take your
ciphertext and you have a depletion or
you have a decryption key and you get a
plain I mean the plaintext output now
two important things are the three
important things you need to look at
here is and how does how is it how care
Bruce has been integrated with your
huddle right so there is a user
authentication user and group access
control list at cluster level okay oh
and then tokens so delegation white
token is delegation is what delegation
is between the the client machine and
your and your KDC
so if you look at this part here
something like you're coming you in your
drop in your Naomi when you conflict get
across you look something you have
something called at KDC okay there is
your key distribution center
now within the KDC you will have two
components one is an authentication
server in the second one is a ticket
granting server now now when the when
the client says he wants to authenticate
before that if you step back a little
bit or if you see here three important
things that you need to learn here is so
one is the principle the next one is a
authentication the final one is the
opera's ation right what is the
principle the principle can be a client
or a user ok user or any service now
let's say that's a principle well
authentication is authentication come it
comes into picture when say when you're
authenticating when you say your F
indicating it is indicating you or who
you are
right you are typing in a password and
saying yes am the person I am
representing and it is with my password
and identifying myself now the final
thing the authorization authorization is
what authorization is indicating what
all services or what all love components
you have access okay we're all you can
navigate through what all files you can
open what all folders you can open so
three important things principle the
application and authorization okay now
here the tokens is what is used okay in
stock for sending in the password so
tokens is what family used so delegation
between the client machine okay the job
is what then between your between your
job tracker and your date and your task
trackers right and when the job to gain
the block tokens will be or between your
name node and the data nodes now when
our client wants to say he wants to
authenticate he would typically up
sending a message to authentic Asian
server okay and then he would get a you
get a token back using the token you
would be contacting or starting the
session so with natok in what will
happen in the token you'll have a
timestamp or you will have you will have
the authorized details and also the
timestamp validity for how long you can
exude those comments so what are
applications we talked about an
authentication authorization of
confidentiality and within networks and
small sets of petals okay so real
quickly go inside the demo part our
demos for today I would be using hope
you guys have seen the screen it spawns
today so primarily this is a upcoming
demo I believe I am doing is on a on a
VM box which has which already has my
own in which already has some code of
install on it and I mean I have I have a
embody installed it so I'm body is
nothing about field offices from what
works for most framework and is a so I
mean in the competitors right so you
have a CDH third row manager then you
have embody their main become bar you
you you deploy your HTTP you have hot we
work so data platform right so typically
I mean a lot of steps P I mean a lot of
prerequisite steps order a lot of steps
that has to be done okay now when you
want to authenticate or when you want to
enable lock when you will love security
you simply the simplest step that you
can do is go in sir Edmond and say go
inside security okay and from here you
click on
right now it says calbro security is
disabled right so you simply say enable
security here and this will give a list
of steps okay a list of steps what you
need to perform as an administrator
before you can start enabling Kerberos
right so so install and configure and
start your KDC right your Kerberos KDC
and install and configure the Kerberos
client on every host in the cluster
right so this is what the first two
steps we need to do now the first two
step that I am doing right now are Oh
once again nice let me show this also
right the first the first two steps look
let me share the complete she is screen
completely that's my thing right I met
here it says install and configure and
start your Kerberos KDC ok install and
configure the Kerberos client on every
host right so primarily I have already
done that part here where you would be
installing the ok so how do you install
it you would you you'd simply run this
command where you can do a yum install
ok so primarily your your downloading
your cabbies I mean can process server a
library files it a not dial-up file and
workstation so these are different
packages which you need to download ok
now which is what I have already done so
when I did a yum install right so um
installed it has so it has got me easier
it has got me the packages downloaded
okay so the workstation is something
which is a client machine so when I say
oh this is work I have my name not
running here and from here I want to
enable my Kerberos so there's all the
installation steps in so you run this
command you have all the installations
done ok there's your first step the next
step is you need to configure right so
you're copying you're copying your care
your menu
- Oh what is this one once again yeah
you need to edit your configuration file
ok you need to edit the configuration
file ok edit your configuration file and
set a real um here the real um that I'm
setting here is example.com please real
um and it might carry see my KDC is
running on the node 1 okay and my admin
servers I'm giving a server name so node
1 is the host name here so before you
swing for one if you show you this one
play this so they do hostname this my
node 1 is my host name right now now
here I have a user by the name wait
right so when I say LS how do I invoke
my commands on my HDFS H how do is - LS
I just do
so there's nothing right now let me do
is go back and type in LS / I am looking
at the root filesystem root HDFS
filesystem so guys for the guys who are
new or what they're doing right now is
I'm logged in with the user ID edge
rekha okay onto a server by the name
node 1 and right now I am executing my
Hadoop commands okay so this Hadoop
commands is what I am executing against
Maya on HDFS yes Rohit is a slower I
think my machine issues my limousine so
I have a level very small Ram right so
so I have user dead tree here now
looking at user directory I want to see
how do that this LS user and I want to
see what is there inside the user
well this is an 8gb ram borrow it so
that's very less for the kind of
applications we run here so this is a
user ID
now now now when we at the end of this
when we enable cat gross if I run this
command you would see that it will throw
it will throw an error message okay
let's see how it is now here see I when
I did a LS of user it was showing me
everything anybody I mean so it showing
me embody QA ed Rekha all the user
Beltran is in train me so this is what
it should not show okay so if you are if
you are I mean if you enable Karos only
the users who are who who should win who
can authenticate and who have got the
privileges to look into that directory
will be able to look at that right right
now I am NOT authenticated guys okay
it's still not authenticated now I'm
just trying to show you the difference
now if I am able to I'm able to run the
commands now but it's still not that so
the first step is install and configure
okay and start your KDC
so so when i say oops right so
installation is done okay
so inst install part is done within this
part here let me missile at all so how
do I start my KDC I start my KO kv-5
server and also my k admin okay so
you're starting your kid KDC
so so right now I have both these
services up and running
like honestly so here we get out I have
my therapy mmm and clear Kenny Caesar on
thumb up and running okay
and also my o admin so cared minestrone
now for the next steps let's go here now
it says create principles for Hadoop
services and host generate key tabs for
each principle and place at the
appropriate host so to host is what I am
using one is my node one and the second
one is my note - okay nor to look at is
here is my note - so node 2 is what I am
I'm saying it will be the client machine
and note 1 is where I have my KDC
running and from node 1 is what I want
to authenticate ok so
once that is done your care gross you
have a confirmation is done now go back
into your your embody here and say oh
you have already done these two parts
are done okay so you just do it next
year okay so what is this a is all you
can always you just go with the default
settings here don't change anything for
embody security wizard let's say next
oh yeah so from here if you're creating
right so what you're doing is you're
creating the principles and and
principles and the key taps no issues
Rohit I was trying to expand this
because I'm I'm more comfortable with
one two are sorry on my Oracle VM so if
you can let me know I'll be happily
taking that out let's pan this having
your meaning back right so you create
the down I create these the principles
download this CSV and you see the file
you what happens with your creating a
scene
let me open this nice please bear with
me for some time come on
so you have it here all right now we
copy this don't have done till now once
again guys okay so what happens we'll
send key caps at your creature right now
that you're copying this copy this and
in to into up five okay so we're a copy
yeah thanks for hipster thanks to ignore
yeah thanks for and appreciate you guys
patience again now oh there I had to
spin up this in the last minute so my
Oracle VM starter I mean there's an
issue at my Oracle VM now so I had to
use this I had something in my in my
note 1 right so in my note 1 root
scripts have clip exposed or sh-show
sorry logged in as what ID root so we I
wrote scripts and create schools for
this one so this is
what will create the principles so I
need to use route scripts and Kairos see
if I can paste it here no I'm not be
able to paste it so the easiest thing is
so let me copy this quickly maternal
here and Carlos Oh
so why is this so this is I already open
here I'll become out of this so insert
and paste it here okay so whatever
principals and key taps you are just
created you're pasting into hell kevro's
file here now I have a simple script to
create a principle okay let me clear
this here so I simply run create
principles okay so the path that I would
be entering here is my CSV file this is
a little bit of manual as well as
automated way guys
so typically typically this is what I
mean
I mean basically right now everybody is
going with a Turin body or Florida so
the same steps would be you be quickly
using with either either of these two
flavors of four of flavor softly Barack
heard of installation okay these things
you cannot ignore ignore for now right
now
yes banal it can be implemented
open-source as well love spnego is I
think it's a negotiated authentication
typically family use with Windows so
just try oh let's try to stop cat gross
and start the services to do to do so
just give me five minutes guys
so I think yes we are almost down the
time I mean it's almost the time to do
it
thanks.thanks or opposing you that keeps
the spirits high but yeah I mean I agree
yeah on the beams and this one first
time I'm using not a first time but used
it but then not way I do not have a way
to expand its sorry about that guys next
one probably I would be doing on our
Oracle's VM but anyway people are wrong
and if your people are pressed for time
sorry about this we are exceeding the
time so if you people can I mean people
have to log off you can rub off so
basically what will happen is right now
once we enable a cheer frog once we
enable it just start to start the
services and job and whatever commands
we have run right now here you'll not be
able to excuse commands okay this I am
running as the user ed wake alright so I
will not be able to execute these
commands so you basically need to or you
basically need to configure the user
permissions okay so you need to have
something called of a admin dot local
and with that k admin dot local you you
need to create a principal create a
principal key generate a key for this
user okay and and add it to the key tap
file okay so once you add the key tap
file you just need to give permissions
and and once the permissions are
available you can simply log in and he
can execute these commands right the
steps in for in mexico bros security and
security serve for those in Florida
almost the same steps of villain so
similar steps where you need to do both
okay so I mean manual step right so
manual steps needs to be done as well
answer as well as a automation so in the
sense automated in the sense you need to
do it from a cloud room manager I'm not
sure how many of you are using your can
come in Florida and home you're using
your say Omni I'm know - where do we
have the Kerberos installed see on note
- we don't need to have you don't need
to have that bro server installed you
just need to install of Rose client
machine
okay so the Kerberos client his word I
have installed just before the start of
this session so you just run this
command see I'm logged in - no - and I
install care Bruce workstation so so the
camera server is installed primarily on
your main server
slice is taking too much time
okay so oh no no I'm ill and this
scripts are all something which we have
to write okay so not this same thing
which yeah ah yes so hit the okay so
this scripts or something which I have
here oh yeah
I mean now basically or from different
locations is what I just described will
here these are what I mean after I start
the controls so this is what I need to
do I mean configure the user permissions
and you set the permissions okay so the
user is what I'll set for the user ID in
rekha right so a director user ID and my
my example.com is the real right if you
see my you can see it has seen the
caveat on file right if you see in the
care they don't I see if care beyond 5
by default when you download it your
everything will be under example so here
is what I have changed in my real MCS
what I changed just a node 1 ok and node
1 is what I changed and and then I need
to start ok I have to start my boat to
carry my and my care I mean KDC and left
a admin server arm is asking so if you
have say 150 nodes will you install it
manually
see what will happen is if you have 150
nodes so typically you will have
something called a something called that
sir well tools ok you not be doing it
manually you need pushing it through
tools either rsync or rough or you have
puppet and you have chip okay so using
those tools is what you may take if any
time you talk about how to cluster right
you're not talking about one node
you know you're always talking about top
hundreds of Norse and and what is the
biggest nor in the cluster biggest
cluster in the currently ice how many
nodes are there anybody the knowledge
can help me so handling thousands of
notes is begin impossible
Amit is saying 4,200 can all say a
thousand men are asking any reference
links for Kara Bruce part that can help
our territory to deter annum is saying
4,000 no yeah 4,000 is what I have heard
guys so I think your who has a port of
the node cluster but now this 8 6,000
node that's the largest cluster maybe
that's a limitation okay so we talked
about name node right name or limitation
on 4000 knows but with how true we don't
have a limitation there because you are
talking about multiple name nodes as for
the steps are okay yeah and is saying
now they move to 10,000 nodes so perfect
I think that's because they thought up
do you have multiple name was fine the
reference links it's also average from
different perturbations Milan or if you
want maybe your I can just write up this
and
send across to you guys you can share it
across or from we can show it to or
maybe send across into the or uploading
though it refers block site okay I'm it
is saying the most common clusters are
less than two thousand nodes yeah that's
what I mean eBay also has about thousand
node cluster so maybe less than that
lesson Vidya probably less than two
thousand nodes but having this ten nodes
spend ten thousand nodes and whatever
notes we talking why they they did it
for testing purpose because because you
know right Yahoo is a primary
contributor to work towards Hadoop or
the other guys who funded or no funded
dog cutting and mica for Allah so
initially so they I think they do it for
more testing purposes so they want to
see the true put and also want to see
how the test will work for me
okay yahoo has total in 200 notes and
i'm it is saying yahoo has 4,200 nodes
and 10,000 in federation perfect tens
omit okay guys so i think so basically
the next step would be a quantitative
permissions okay and this is something
which show the permissions is what we
are to set or this is gonna take some
time now in case people are hard pressed
for time you can can drop off oh and and
and also please provide any available
feedback any ways we can improve yes one
thing your as one thing i just want to
point out was so from Rohit thanks for
your solutions so I make sure will will
improve will try to improve on and what
conversations you have provided yes
recording to definitely be available it
will be posted after this session
definitely will be available guys and
let's check yeah okay this is gonna take
some time guys so while I will reach out
to the support and as promised if you
want I can post these into a headwaiter
blog site okay so although is the
example some of the steps and
step-by-step instructions this is just
something which I have Scrabble here so
it does not make sense looking at this
for guys work of freshly learning or who
wants to set up or enable their gross so
I'll post this off take my word I will
help I will reach out to the support in
for a to you guys okay so any other
questions guys and yeah thanks Rohit
thanks for all the wishes thank you
okay no other questions then thank you
guys for all your time and appreciate
your time here and and yeah I mean take
time to provide the feedback any way you
can I mean any feedback is important for
us so the way we can improve ourselves
very very important please try to spend
some time on that okay thank you
you guys have a great night and for the
folks abroad now great evening or gate
day and talk to you hopefully or try to
enroll guys or try to enroll for
Administration stuff will definitely be
showing you a lot of in-depth hands-on
and we can we'll definitely walk you
through the step-by-step instruction and
I mean although although everything will
be explained to you so all the
presentation that we showed will is part
of the administration course so I think
finally we need to talk about one five
on the admin roles and responsibilities
which is a primarily important for a lot
of people why so as an administrator
you'd be responsible for most of our
ecosystem projects and this is very
important rights you see here recovery
upgrading and etching is what you'll be
doing and performance tuning is is very
very important here nowadays as the
number of clusters increase your
performance tuning is what is people
would look at so it's a learning thing
which comes out of experience and
obviously you need to monitor right you
need to monitor and then deploy your
security
oh yes whip see children comes with
parcels and they also come with packages
okay either ways you can do it
installations but yes with parcels so
matching matching with the pastures I
think you just look upgrade right so not
exactly sure on that okay I don't want
to give a wrong answer but and to check
on that with either
oops oh you guys okay this is still
going on now stunning the services duty
to let me see if it started the services
here easiest way to check yourself for
Java so right now is in safe mode okay
so it's starting to services so my name
is AB my kitten was a sleeves a secure
data no 32 tu-tu-tu-tu-tu-tu is our node
one am I not to see what is there yeah
these are starting up here on my note 2
also guys how to more minutes will write
for two more minutes guys
see
who's asking why we do the braid or
patch doing to bring down the service of
guru is it mandatory oh yes and then
when you're doing your grater attached
or obviously yes you have to do it bring
it down or you put some you put the
cluster into something called a
maintenance mode okay so so that nobody
is so nobody is trying to write into the
cluster C upgrade and a patch is what
requires downtime okay but what you want
to add additional nodes and you want to
remove additional nodes of the cluster
you follow a process something called
the decommission and a commission okay
that doesn't require you to bring down
any of the cluster of any of the cluster
so this is okay now is there any scope
refresher horrified min developers just
curious about how much experience they
are looking for big firms so it's me
Lance question from 2.4 and ago can we
do rolling yes yeah I think um it's
question is the average is a stranger
yeah we can do a rolling update so I
think in fact with the two points each
oh yeah that's right oops this has
failed I think some problem with the
just configuration so so fully you so
why does why does it fail you cannot go
forward
oh but it still says a startup as fail
let me see why the startup as frame oh
so the question is for after the
freshness yes there's a decreasing huge
demand for the freshest guys right now
in the market the freshers would
definitely there is a you need a lot of
market is there for freshers definitely
required and and especially in the
administrators and on the developers
okay so if you mean I mean from for a
company looking for freshers who's got
experience if I mean if you claim and if
you say you got really if you have
really experienced on this so they are
definitely going to hire you and right
now people are looking for minimum two
years experience okay so hope has been
very very new maybe two and people
finding people with three days
experience is very very difficult
okay so two years and one years is
very common now so so I think the
industry is maturing as of now how many
same shape for this cube with x cubed
materials oh okay that could be the
problem now let me see once again it
says and enable so let me want no yes um
it just want to verify that because so
right now it should typically say it's
enable but some of the services have not
started so let me see what services are
not started HDFS yarn okay so these are
fine oh okay the node managers are not
come up I think okay so this is where
the node maps are still down mmm just a
minute guys well note matter is also up
let me do this su into a breaker and let
me try to do this of slash
there you go see we have an able
security now it says the user group for
this is for this particular user which
is process in April mind so you will not
be able to connect so the same thing is
what we are done here
see earlier we have Hadoop FS LS and I
get the output here right now I'm not
able to get there I mean I'm not able to
into the results here right so this is
where Rob this is how you enable
security now okay so yeah so yeah I will
do it can it right so how will be
appended yeah that's what the next steps
would be right so how do we how do we
attend ticket price so that is where the
user per drop the user permission so up
till here is what we have done right now
you started the third growth and
configure user permissions right so how
do you control the user permissions are
you need to start the K admin tool yes
Amit I just come to that commit just in
a minute
any questions
sorry yeah Amit I will do a cane it in a
short way how for this I need to log in
as root okay so so my security is
enabled so what I do K admin ok so you
have invoked your drop your key I mean
right now you are interacting with your
CAD Mineo Kerberos admin what do I need
to do let me copy this from here the
first thing is so I had a principal key
okay for the user in this domain right
so add quinces
when I'm generating a random key
defaulting to know for C so there the
principal has been created one the
principal is created you need to create
a key tap file okay so where do you
create a key tap file the key tap file
should be created in your PC so wetting
the step is here once in nice
you
convinced
guys so you wouldn't yeah okay so now
the next step is your rock you need to
create a key tap file so use this
schematic or clinical just copy this
command guilty of okay now just rinse it
out of this now you need to set of
permission so I permission should be set
for the curve for that user so change so
this file should be created let's go and
see this file is already there so you
have a lady raker head let's keep that
right so this one now what the
permissions I said I just change it to a
to a kind the user I do we change it
next one is changed move set it to four
four zero the same file first thing is I
change the ownership now I change the
permissions okay now initialize right so
how do initialize you set up everything
properly so let's login to often you can
make sure oh come on eyes okay now use
this km8
okay now I'm locking the tidy iterator
here there you go okay so you have
generated you have created a principle
for this user for this user and you have
now we have set up our security now if
you look at this CTC
KRV Phi dot com right so this ticket
lifetime by this ticket lifetime is got
will be what 24 hours okay
so this ticket lifetime will be 24 hours
now I can submit my chops here I can
submit all my jobs here and I think
still I mean when that's where I am
indicating here right now so if we do a
list here there you go okay so this time
stamp right 24 hours is what they said
so this is where is it for this
particular user
okay all right guys so so this is what
setup is now hope and request you guys
to get this I mean get it enrolled and
we'll show you more in deeper in depth
and also the manual steps or
step-by-step instruction will be shown
once you get into interest it so so
which over the support for any queries
and and this I think this will be made
available right J Fichtner this will be
made available you can always view this
okay right then so thanks guys you guys
have a great day and night thank you bye
bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>