<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Mining using R | Data Mining Tutorial for Beginners | R Tutorial for Beginners | Edureka | Coder Coacher - Coaching Coders</title><meta content="Data Mining using R | Data Mining Tutorial for Beginners | R Tutorial for Beginners | Edureka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/edureka/">edureka!</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Mining using R | Data Mining Tutorial for Beginners | R Tutorial for Beginners | Edureka</b></h2><h5 class="post__date">2017-11-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BB2O4VCu5j8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello people this is harini from in
Eureka and this session is all about
data mining we'll start off by
understanding the core concepts of data
mining following which we'll have a
hands-on session with our let's have a
look at the agenda we'll begin this
session by knowing what is the need of
data mining thereafter we'll
comprehensively understand what exactly
is data mining next we'll learn about
the sequential steps involved in
extracting knowledge from the data
further we'll go through some of the
most common data mining techniques and
then we'll look at the list of
programming languages which can be used
for data mining finally we'll arrive at
the most interesting power of the
session which is case study using our
why data mining as they say this is the
age of information and for information
we need data
let's take an example you go out with
your friends for a party take some
selfies and then upload these selfies on
Facebook look at the bigger picture now
not just you there are millions of
people uploading millions of selfies
every single T and this is the data
deluge that I am talking about data is a
viewer and it is expanding exponentially
and this data is being generated through
multiple sources this is not just coming
from social media
it comes from sectors such as healthcare
sector financial sector telecom sector
and so on and this data is also
available in multiple formats let's have
a quick look at the different data types
we have broadly speaking we have
structured semi-structured was he
structured and unstructured structured
data type has a defined format and since
it has a defined format extracting
information from it is quite easy semi
structured data does not have a rigid
pattern but it does have a noticeable
pattern an example of semi structured
data would be the XML file quasi
structured data it neither does have
rigid structure nor does have a
noticeable pattern but a little bit of
tweaking information can be extracted
from it unstructured data has no
inherent data format at all and that has
one extracting information
from an and B quite a cumbersome task he
wonders to that DITA comes from multiple
sources and it is also available in
different formats now let's see what can
be done with the data which we have with
us this is John and he comes across
financial data and he is trying to
understand how many of these financial
transactions are fraudulent
now John stumbles upon the email data
and he's trying to figure out how many
of the mails are spam John notices
telecom data and he wants to find out
how many of the customers will churn out
John does have the data with him
how does he obtain information from it
so his friend Jim is a data scientist
who suggests him to use data mining
techniques to find solutions to each of
the three problem statements that US
John can use data mining techniques to
find out how many of the financial
transactions were fraudulent
similarly he can also use data mining
techniques to find out if the mail was
spam honored and again John can again
use data mining techniques to find out
how many of the customers will churn out
if understood the need of data mining
let's understand what exactly is data
mining let's look at the definition data
mining is the computing process of
discovering patterns in large data sets
involving methods at the intersection of
machine learning statistics and database
systems now that is quite a complicated
definition so what do we exactly mean by
that in simple terms data mining as
mining of information from data which is
new correct and potentially useful now
let's understand why should the mined
information be new let's take an example
we have a dataset with us we apply some
data mining techniques find out a
pattern and this pattern tells us that
smoking causes cancer now this is not
something new we already know this we've
invested time we've invested effort and
we found out information which we
already know so it doesn't really so
good push let's look at the next
parameter
the correctness of the information let's
again take an example we have a deed
asset which comprises of the stock
prices of all the companies and we
predict that a company X its stock price
will increase by 20% in the next week
but on the contrary what actually
happens is its stock price decreases by
10% what happened to you we found out
information which eventually proved to
be wrong and that is why and is always
necessary to check the validity of the
information before we reach to any sort
of conclusion you understood that the
information is new and correct now it's
time to see if the information is useful
or not as we saw in the first example we
found out an information which told us
that smoking causes cancer now that
information did not serve a purpose we
instead of finding that information what
we can do is you can find out patterns
which would tell us the longevity of a
person who smokes and the longevity of a
person who does not smoke this
information will be more useful to us
one listed what is data mining now let's
look at the sequential steps in
discovering knowledge from the data kdd
has these steps selection of data
pre-processing of data mining of data
evaluation of patterns and
representation of knowledge let's start
with the first step which is selection
of data we already know that theta comes
from multiple sources and that is also
available in multiple formats so our
first task would be to integrate all of
this data and store it in one single
location which is the data warehouse so
we've integrated the data stored it into
a data warehouse now we can select one
particular dataset on which we would
want to do our analytical tasks the data
selection has been done now it's time to
do some pre-processing pre-processing
tasks involves such as understanding the
structure of data you can also use
visualization technique
you understand how are the variables in
the dataset related to each other the
correlation among them can also apply
simple operations which are summarizing
aggregation and normalization the
solution has been done we've also done a
bit of pre-processing now it's time for
the most important stab and Kaleigh
process which is mining of data you
already know what is data mining now
let's understand how can we do it so
data mining is applying of intelligent
operations such as clustering
classification regression and so on we
apply these intelligent operations and
extract patterns so we'll applied
intelligent operations with obtain
patterns now it's time to check for the
validity of these patterns that as we do
the three parameter check we need to see
if the obtained information is correct
useful and new once the information is
validated now it's time finally to
present the information using simple and
ascetic graphs so those roses keep
sequential steps involved in kdd process
now let's look at the data mining
techniques in this session we'll be
looking at anomaly detection Association
rule mining clustering classification
and regression let's proceed with the
first EDA mining technique which is
anomaly detection this technique helps
us to identify unusual patterns or
outliers in the data let's take an
example there is a movie theater and the
average salary of people who go to watch
a movie in that theater is around rupees
50,000 per month one fine day it just
happens that mr. Mukesh Ambani decides
to watch a movie in this theater
so what happens the average salary
shoots up and it shoots up sky-high so
in this case mr. Mukesh Ambani would be
the outlier or the anomaly prior to
processing of the data we need to make
sure that we do some processing on the
outliers that as a Dobie remove the
outliers or
do a bit of adjustment to the outliers
and then we come to the main analysis
pile with the data the next technique is
Association rule mining this is also
known as market basket analysis this has
a very interesting example and it goes
by the name of the Bo diaper syndrome so
you know so we done by a supermarket it
was found out that whenever a single
father came to the store to buy a diaper
there was very high likelihood that he
would also buy a bottle of pure now
that's quite an uncanny relationship
isn't it a single father coming to a
store to buy a diaper
also buys a bottle of pure white strange
but this is what the supermarket was
able to find out with the help of
Association rule mining and this is also
the basis for many of the recommender
system let's take Netflix suppose you
watch the TV show friends on Netflix the
recommender system also suggests you to
watch How I Met Your Mother and Big Bang
Theory why does it do it the recommender
system understands that all of these
three TV shows belong to the comedy
genre and therefore if you watch friends
there is also a high likelihood that you
might watch Big Bang Theory and How I
Met Your Mother
next comes clustering this comes under
the purview of unsupervised learning so
with the help of clustering if you have
a set of observations we can segregate
these observations into clusters on the
basis of their similarity let's have a
look at this light we have the set of
observations and we are obtaining two
clusters from these so the first cluster
comprises of all the animals and the
second cluster comprises of all the
Bhoots
and this clustering has been done on the
similarity of the observations that as
there is high interest or similarity so
what do I mean by high intra cluster
similarity that us all the observations
in class in one cluster are similar to
each other so if we see over here all of
these are animals and the similar to
each other and in the second cluster all
of them are birds and are quite similar
to each other but when we compare these
2+2 is what happens as there is very low
similarity or there is quite a bit of
dissimilarity and this has the basis for
clustering now I've said that clustering
comes under the purview of unsupervised
learning so what is unsupervised
learning
now again unsupervised learning comes
under the domain of machine learning and
machine learning algorithms are
algorithms which learn on their own she
coming to unsupervised learning you do
not have predefined cables so what we
see over here are we just have
observations and there are no labels
which tell us that this is an animal or
this is a bird but even then the
unsupervised learning algorithm is able
to cluster them and the clustering has
been done on the basis of similarities
of these observations it is only after
the clustering has been done that we
understand that all of the observations
in the first cluster animals and all of
the observations in the second cluster
upwards the next technique is
classification and classification comes
under the purview of supervised learning
and unsupervised learning
we do have predefined labels let's take
a look at the slide we have an
observation and we have two levels which
are cat and dog and our task is to
assign the label to it so what are we
doing over here so what happens in
classification is we have an observation
and we try to classify it into one of
the categories so for this slide if we
assign the observation the label of cat
then the classification would be wrong
if we assign the label of dog to it then
the classification would be right the
next technique is regression with the
help of regression we can find out how
does one variable change with respect to
another variable regression again comes
under the purview of supervised learning
and therefore we have labels there are
many types of regression techniques such
as linear regression logistic regression
Poisson regression and so on but for the
sake of this session I'll give a brief
intro to linear regression now all of
you must have come across the equation
of a straight line which
y is equal to MX plus C over here Y will
be the dependent variable and X will be
the independent variable and we are
trying to understand how does Y vary
with respect to X that is the basic idea
behind regression we have one dependent
variable and we try to understand how
does this dependent variable change with
respect to the independent variables
we've looked at some of the most
commonly used data mining techniques now
it's time to have a look at some of the
programming languages used for the
purpose of data mining we have Jim again
and Jim tells us that we can use our
Python Julia and SAS for a data mining
tasks and for the hands-on since I've
used our let's understand some of the
features of our but why am i using R so
R is a programming language which is
used for many statistical modeling and
data science tasks and I believe one of
the best feature of RS that it provides
more than ten thousand three packages so
whatever you need is if you want to do
data visualization are provides a
package for that if you want to do
statistical analysis or provides a
package for that if you want to do data
manipulation are also provides a package
for that isn't it great the R is also a
dynamically typed programming language
what do I mean by it
so the focus is on the value of the
variable and not on the data type of the
variable when because of this we can
make statements in R such as so the
first statement would be e is equal to
10 and the next statement would be a is
equal to an
so we see over here that the focus is
not on the datatype for the first case
we are assigning an integer and for the
second case we are assigning a string
and this has the basic idea of a
dynamically typed programming language
another feature of our as it can be
easily integrated with other popular
software's
some of the examples are tableau and SQL
server wonders should why let's look at
who uses our so this slide tells us that
the bigshots the top-notch companies use
our Facebook uses our for behavior
analysis Google uses our for advertising
effectiveness and Twitter uses are
petite a visualization so if your idea
has to do some data science tasks then
our is definitely the programming
language that you need to use so one
listed wire who uses her let's finally
go ahead and install our so this is the
site from where you'll be installing her
Krannert our - project let's go to the
client repository so I'll just type out
download Roo you click on the first link
if you have a window system you can
download off windows if you have a Mac
you can download off Mac if you have a
liner you can download out for Linux in
so I have a Windows system I'm
downloading it for Windows and this is
the latest version of our three point
four point two right click on download
and the download starts now if you also
have an IDE
which is our studio now I'd recommend
all of you to also download the IDE
before you perform programming because
our studio makes our task easier let's
go ahead and also download our studio
I'll just type out download our studio
over here I click on the first link and
it redirects me to our studio comm I
download with de of three version and
these are the options
Windows Mac and Ubuntu I download it for
the windows system download starts now
since I've already installed our in our
studio I don't need to install it again
so we'll directly go ahead to the key
study this will be a key study we have
the houses for sealed data set which
comprises of price of house large size
number of rooms living area and so on
and our task is to understand the data
set and design a model which will help
us to predict the prices of houses with
respect to other variables let's look at
the tasks which we'll be performing in
the key study we'll start off by
importing the data set and then we'll do
a bit of pre-processing the
pre-processing will consist of
understanding the structure of data and
it will also involve bit of data
cleaning because the data would be
untidy so we're supposed to make it tidy
then comes the important part which is
data mining and the data mining
technique which we'll be using this
linear regression so we'll use linear
regression to predict the prices of
houses and we'll be building two models
actually and are those two models we'll
try to understand which model is more
accurate which model will give us better
results let's start with the hands-on
session this is our studio and this is
how it looks like
this is the stripped window this is the
console window let's start with a first
step three dot CSV this is the function
with which we are inputting the CSV file
and that CSV file consists of the houses
data set let's look at this data set
this is the data set guys so these two
columns are sue book which widgets are
numbering so we would have to delete
them this is the price of the house this
is the lot size this column tells us if
the house has a waterfront or not this
column tells us how old is the house
this is the land value this column tells
us if the house is newly constructed or
not and this has fir'd if the house has
centralized air conditioning or not
these two are the fuel types and the
heat types used in house this is the
sewer system living area the number of
fireplaces number of bathrooms and
number of rooms whereas the D desert
let's also look at the structure of the
data set
Str function with the help of sto
function we can assign the structure now
this tells us there are 17 28
observations and 16 variables in total
and most of these variables are of the
type integer but who is the structure
now if I understood that this data is
not clean so let's go ahead and do the
pre-processing or a bit of cleaning so
the first task would be to delete these
two columns after that now these values
seem to be a bit ambiguous so if the
house has a waterfront you can change it
to yes instead of one and if the house
does not have a waterfront you can
change it to no instead of zero
similarly we can change the values to
yes and no for construction similarly
for the fuel type instead of two three
and four we can change it to be electric
gas and air and again for the heat type
we can change it to hot air hot water
and electric for the sewer system it
again has the values two three and four
instead of two three and four we can
make the sewer system pipe to be private
public and none so none will say that
the house does not have a sewer system
associated with it these are the basic
pre-processing tasks that we would need
to do and for that we would have to load
the deployer package I'm loading it
prior to loading any package we are
supposed to install it at UConn
installer with you at iPod apply of
selected now when I say install the
package will be installed and since I've
already installed the package I don't
need to do it again I loaded the package
now it's time to eat the first two
columns
the select function comes from the
deployer package and this is the data
set and this symbol overview that you
see is the pipe operator the pipe
operator helps us to connect things so
from the houses data set we are
selecting all other variables except the
first and the second column and we are
storing the result again in the houses
data set so you apply the command and we
see that the changes have taken place
the food's two columns have been deleted
let's go ahead with the pre-processing
so we saw that wherever the values for 0
&amp;amp; 1
they have been changed to no and yes
let's again have a look at these
commands so dollar symbol helps us to
select the column so with the help of
houses dollar aircon
what am i doing is I'm selecting the
air-conditioned column of the houses
dataset so I select this column and I
change it to a factor that as initially
the values for numerical now I'm
changing it to a categorical variable
I've changed it to a categorical
variable and now it's time to change the
labels of it so initially the labels for
0 and 1 and I'm changing the labels to
no and yes I've done the same for
construction column and waterfront
column so I've selected the construction
column changed it to a categorical
variable and change the labels to no
India's are down the same for the
waterfront column as well now let's go
ahead and also change the fuel type heat
type and the sewer system let's go ahead
and do that so we have changed fuel and
sewer so the fuel type has been changed
to gas electric and oil and the type of
sewer system has been changed to non
private and public we also need to
change the heat column let's also do
that
so I am selecting the eighth column will
help up a dollar symbol
I will change to hot
what do
and electric and the changes will be
applied yes poutine is having
successfully applied we're done with the
pre-processing now let's start
visualization for visualization our
provides us with a package which goes by
the name of ggplot2 and again prior to
loading the package we supposed to
install it I just type our ggplot2 over
here selected and when I click on
install the package will be installed
and since I've already installed it I
don't need to install it again now
ggplot2 is based on grammar of graphics
now whenever we are trying to form a
sentence we are supposed to follow the
grammatical rules similarly whenever we
are making graphs we have something
known as grammar of graphics and we need
to follow this grammar of graphics let's
understand what exactly is grammar of
graphics so for this command grammar of
graphics has three parts the first is
the data byte
next is the aesthetic next is the
geometry so first we supposed to select
the dataset on which we are supposed to
do the plotting so over here I've
selected the house's data set next are
the east headings so aesthetics tell us
on what variable are we supposed to map
notice we can select the east set x such
as x axis y axis or the color or the
fill or the size of it
so these are the East attics and we
assign variables to these East attics so
for this case I've assigned the price to
the x axis and then finally it's time to
decide the geometry that is the type of
graph that you want to plot we can
either select a histogram box plots can
applaud or a line plot so whatever is a
need we can select the geometry for that
so now I would want to know what is the
distribution of price let's plot this
I've plotted the graph now this is the
price distribution you see this values
over here so this is 2 e + 0 5 4 e + 0 5
6 e + 0 5 that basically stands for 2
into 10 power 5 4 into 10 power 5 6 into
10 power 5 that again means the value is
around 2 lakhs 4 lakhs or 6 lakhs and 8
lakhs
so from this graph we can infer that the
average salary of the houses is around 2
lakhs and the maximum price of the house
could be around somewhere close to seven
point five lakhs down with us let's have
a bit of color to the plot so and it
looks prettier in the geometry I've
added two more attributes I have felt
and I have color so the film is of light
blue and color which is the boundary is
of dark blue is the same graph I just
added color to it now let's understand
how this price vary with respect to
waterfront that is if the house has a
waterfront what is the price of the
house and if the house does not have a
waterfront what is the price of the
house so the data is again houses the y
axis denotes price x axis denotes
whether the house has a waterfront or
not and the color is again determined by
whether the house has a waterfront or
not and the geometry used so here is box
plot so the color or the fill is
automatically determined by the variable
since waterfront has two categories
it gives two colors let's look at this
so we can easily infer that if the house
has a waterfront it will have a higher
price and if the house does not have a
waterfront it will have a low price so
we'll be comparing the median values
just to be sure so this the median value
of the house which has a waterfront is
higher and the median value of a house
which does not have a waterfront is
lower now let's see how does the price
vary with respect to the air
conditioning again data set is houses
y-axis is price x-axis is determined by
whether the house has air conditioning
or not the fill is determined by again
whether the house has air conditioning
barnard geometry used is boxplot again
we can very easily infer that if the
house has air conditioning its price
will be higher and
the house does not have air conditioning
its price will be lower let's see how
this price vary with respect to living
area
so here the geometry is used our
scatterplot and a line plot so Jian
point s for scatterplot John smooth is
for Lion Floyd and again x-axis is for
living area y-axis determines the price
so we see over here that it is a linear
relationship that is as the size of
living area increases the price of the
house also increases now let's see how
does the price vary with respect to the
age of the house now this is inverse
relationship that is if the age of the
house increases that is if the house is
cooled its price will be low and if the
house is new its price will be high
this will be a very interesting plot
quite a pretty plot doesn't it again
y-axis determines price x-axis
determines the size of living area and
this over here that you see is that the
colors are determined by the number of
rooms so if the house has five or six
rooms then the price of the house would
be around 1.5 likes to 4.5 likes because
the color of the house if it has five or
six rooms is green and all of these
green dots come between 1.5 likes to 4.5
lakhs
similarly what we can see is if the
house has 12 rooms so it will have the
maximum price these are the sort of
inferences which we can make from this
graph
who selected the data we've done a bit
of cleaning we've also done a bit of
visualization now it's time to come to
the most important part of the key study
which is applying the data mining
technique now prior to building the
linear regression model we are supposed
to split our data into training set and
testing set we do this because this
helps us to measure the accuracy of the
model built so we build our model on the
training set and check for its accuracy
on the test set to split the data set is
supposed to load the C it will slide
ready to see you to his package and
again prior to loading we're supposed to
install the package
sceeto's package provides us with a
function which goes by the name of
sample dot split now this has a
parameter split ratio split ratio
determines how many of the observations
get true label and how many of the
observations get the false label so we
have given the split ratio to be 0.65
that is 65 percent of the observations
will have the true label and 35 percent
of the observations will have the false
label and I've stored the result and
split index splitting is done so happy
to remove you now wherever the value or
the label of the observation is true we
are selecting all of the sub survey
shion's
and storing them in the training set and
wherever the value of all the
observations is false selecting them and
storing them in the test setting your
training set comprises of 65% of the
observations and testing set comprises
of 35% of the observations
trains had hatched while 73 Rose test
set has 455 rose but as the 6535 split
you split the data now it's time to
build the model you're building in your
regression as we saw the pieces of
linear regression is a linear equation
which is y is equal to MX plus C over
here this is the formula so the tilde
symbol on the left side of this tilde
symbol we have the dependent variable
and on the right side of the tilde
symbol we have all the independent
variables so the price is the dependent
variable and when we put a dot over here
this basically determines that we are
selecting all other columns except the
price that is we are trying to
understand how does price vary with
respect to all other columns in the data
set and I will build the model and
stored it in the object mod 1 now it's
time to predict on the test data set so
we check for the accuracy of it on the
test set and store the result into an
object which has the name of result I do
this now let me bind the actual values
and the predicted values so the test set
has the price column which are the
actual values and the predicted values
are in the result object so I'm naming
it as predicted and I am combining these
two columns and storing it in an object
whose name is compare result let me have
a look at this these are the actual
values and these are the predicted
values let's find out the error
introduction reference compare isn't is
a matrix we're supposed to convert it to
a DITA frame using a store data frame
function I've changed it to a dieter
frame
I found out the arrow so we find out the
error by just subtracting actual values
- predicted values so actual values -
predicted values gives us the error in
prediction that is what I've done with
you let me also bind the error
introduction to the same data set and
store it and compare result that we have
a look at compare result so actual
values predicted values and arrow in
prediction we have something known as
root mean square error to check for its
accuracy we showed it in our MSE one
let's look at its value so this is the
error in the prediction 52 eight thirty
seven point five seven is the arrow
introduction now let's also have a look
at the summary of the model built so
this was the model which we build on a
training site and this is the summary so
these stars which you see over here
these tell us how much is the impact of
this independent variable on the
dependent variable greater the number of
stars greater is the impact so wherever
the variable has three stars that would
mean it has a greater impact on the
dependent variable so if the house has a
waterfront then it will have a greater
impact on the price of the house if the
land value again has a greater impact on
the price of the house and if its newly
constructed it again has a greater
impact on the price now let's look at
the opposite side that which do not have
an impact at all so heed sewer number of
fireplaces
so type of heat used or the type of fuel
used or the number of fireplaces used
they do not have an impact on the
dependent variable at all we have a
little check for the accuracy of the
model which is the adjusted R squared so
if this value is closer to one then that
would mean this model is perfect so the
accuracy of this model is around 0.65
one now let's build another model and
compare that model with this this is the
second model now since you've understood
that fireplace is sewer and fuel do not
impact the dependent variable at all so
I selected all other variables except
these three variables you see this minus
symbol so I am removing five
is sewer and fueled by putting a prefix
- before it comes to him result in mod -
I predict the model on the testing set
and store it and result - now it's time
to bind the actual values and the
predicted values and I store it in
compare result - let's have a look at
comparison - this is comparison - these
are the actual values and these are the
predicted values let's have a look at
the yeah if you're supposed to change it
to a data frame since its matrix so
change it to a data frame using a store
data frame now it's time to find out the
error in prediction so I subtracted the
predicted values from the actual values
and store the result in arrow now also
let's combine the error in prediction
the same data set
let's have a look at this
tactual values predicted values and
arrow introduction let's go ahead and
also calculate the root mean square
error of it root mean square arrow is
simple
so we square the error and then we find
out its mean and then we find out its
square root
let's compare root mean square error of
both the models built so RMS C 1 is 50
2837 RMS C 2 is 50 2773
what we observe from this is the error
of the second model is lesser than the
error in the first model and that is why
the second model is better than the
first model let's also have a look at
its summary summary of mod 2 so it's R
squared value is 0.65 to 2 and the R
squared value of the first model was 0.6
y1 and since we see that this value is
greater than the first model again so
model 2 is better than the fluish model
so this was it guys for the key study we
started off by importing the data set we
did a bit of pre-processing and then we
used ggplot2 package to visualize the
correlation among the different data
entities and then we split the dataset
after splitting the data set we finally
built the linear regression model we
actually build two models after that we
have found out the accuracy of these two
models and evaluate which performs
better thank you for watching it hope
you enjoyed it thank you I hope you
enjoyed listening to this video please
be kind enough to like it and you can
comment any of your doubts and queries
and we will reply to them at the
earliest do look out for more videos now
playlist and subscribe to our in Raqqah
channel to learn more happy learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>